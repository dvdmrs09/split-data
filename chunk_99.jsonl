{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a callable instance that will convert a value to a Child object.", "response": "def to_child_field(cls):\n    \"\"\"\n    Returns an callable instance that will convert a value to a Child object.\n\n    :param cls: Valid class type of the Child.\n    :return: instance of ChildConverter.\n    \"\"\"\n\n    class ChildConverter(object):\n\n        def __init__(self, cls):\n            self._cls = cls\n\n        @property\n        def cls(self):\n            return resolve_class(self._cls)\n\n        def __call__(self, value):\n            try:\n                # Issue #33: if value is the class and callable, then invoke\n                if value == self._cls and callable(value):\n                    value = value()\n\n                return to_model(self.cls, value)\n            except ValueError as e:\n                error_msg = CHILD_ERROR_MSG.format(value, self.cls, str(e))\n                raise ValueError(error_msg)\n\n    return ChildConverter(cls)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a callable instance that will convert a value to a Sequence.", "response": "def to_sequence_field(cls):\n    \"\"\"\n    Returns a callable instance that will convert a value to a Sequence.\n\n    :param cls: Valid class type of the items in the Sequence.\n    :return: instance of the SequenceConverter.\n    \"\"\"\n    class SequenceConverter(object):\n\n        def __init__(self, cls):\n            self._cls = cls\n\n        @property\n        def cls(self):\n            return resolve_class(self._cls)\n\n        def __call__(self, values):\n            values = values or []\n            args = [to_model(self.cls, value) for value in values]\n            return TypedSequence(cls=self.cls, args=args)\n\n    return SequenceConverter(cls)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a callable instance that will convert a value to a Sequence.", "response": "def to_set_field(cls):\n    \"\"\"\n    Returns a callable instance that will convert a value to a Sequence.\n\n    :param cls: Valid class type of the items in the Sequence.\n    :return: instance of the SequenceConverter.\n    \"\"\"\n    class SetConverter(object):\n\n        def __init__(self, cls):\n            self._cls = cls\n\n        @property\n        def cls(self):\n            return resolve_class(self._cls)\n\n        def __call__(self, values):\n            values = values or set()\n            args = {to_model(self.cls, value) for value in values}\n            return TypedSet(cls=self.cls, args=args)\n\n    return SetConverter(cls)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_mapping_field(cls, key):  # pragma: no mccabe\n    class MappingConverter(object):\n\n        def __init__(self, cls, key):\n            self._cls = cls\n            self.key = key\n\n        @property\n        def cls(self):\n            return resolve_class(self._cls)\n\n        def __call__(self, values):\n            kwargs = OrderedDict()\n\n            if isinstance(values, TypedMapping):\n                return values\n\n            if not isinstance(values, (type({}), type(None))):\n                raise TypeError(\"Invalid type : {}\".format(type(values)))\n\n            if values:\n                for key_value, item in values.items():\n                    if isinstance(item, dict):\n                        item[self.key] = key_value\n                        item = to_model(self.cls, item)\n                    kwargs[key_value] = item\n\n            return TypedMapping(cls=self.cls, kwargs=kwargs, key=self.key)\n\n    return MappingConverter(cls, key)", "response": "Returns a callable instance that will convert a value to a Mapping."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a str if the value is not None.", "response": "def str_if_not_none(value):\n    \"\"\"\n    Returns an str(value) if the value is not None.\n\n    :param value: None or a value that can be converted to a str.\n    :return: None or str(value)\n    \"\"\"\n    if not(value is None or isinstance(value, string_types)):\n        value = str(value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef str_to_uuid(value):\n    if isfunction(value):\n        value = value()\n\n    return UUID(value) if isinstance(value, string_types) else value", "response": "Converts a str to a UUID object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_date_field(formatter):\n    class DateConverter(object):\n\n        def __init__(self, formatter):\n            self.formatter = formatter\n\n        def __call__(self, value):\n            if isinstance(value, string_types):\n                value = datetime.strptime(value, self.formatter).date()\n\n            if isinstance(value, datetime):\n                value = value.date()\n\n            return value\n\n    return DateConverter(formatter)", "response": "Returns a callable instance that will convert a string to a Date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_datetime_field(formatter):\n    class DateTimeConverter(object):\n\n        def __init__(self, formatter):\n            self.formatter = formatter\n\n        def __call__(self, value):\n            if isinstance(value, string_types):\n                value = parser.parse(value)\n\n            return value\n\n    return DateTimeConverter(formatter)", "response": "Returns a callable instance that will convert a string to a DateTime."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a callable instance that will convert a string to a Time.", "response": "def to_time_field(formatter):\n    \"\"\"\n    Returns a callable instance that will convert a string to a Time.\n\n    :param formatter: String that represents data format for parsing.\n    :return: instance of the TimeConverter.\n    \"\"\"\n    class TimeConverter(object):\n\n        def __init__(self, formatter):\n            self.formatter = formatter\n\n        def __call__(self, value):\n            if isinstance(value, string_types):\n                value = datetime.strptime(value, self.formatter).time()\n\n            return value\n\n    return TimeConverter(formatter)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_dict(obj, **kwargs):\n\n    # if is_related, then iterate attrs.\n    if is_model(obj.__class__):\n        return related_obj_to_dict(obj, **kwargs)\n\n    # else, return obj directly. register a custom to_dict if you need to!\n    #   reference: https://pypi.python.org/pypi/singledispatch\n    else:\n        return obj", "response": "Convert an object into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef related_obj_to_dict(obj, **kwargs):\n\n    # Explicitly discard formatter kwarg, should not be cascaded down.\n    kwargs.pop('formatter', None)\n\n    # If True, remove fields that start with an underscore (e.g. _secret)\n    suppress_private_attr = kwargs.get(\"suppress_private_attr\", False)\n\n    # if True, don't store fields with None values into dictionary.\n    suppress_empty_values = kwargs.get(\"suppress_empty_values\", False)\n\n    # get list of attrs fields\n    attrs = fields(obj.__class__)\n\n    # instantiate return dict, use OrderedDict type by default\n    return_dict = kwargs.get(\"dict_factory\", OrderedDict)()\n\n    for a in attrs:\n\n        # skip if private attr and flag tells you to skip\n        if suppress_private_attr and a.name.startswith(\"_\"):\n            continue\n\n        metadata = a.metadata or {}\n\n        # formatter is a related-specific `attrs` meta field\n        #   see fields.DateField\n        formatter = metadata.get('formatter')\n\n        # get value and call to_dict on it, passing the kwargs/formatter\n        value = getattr(obj, a.name)\n        value = to_dict(value, formatter=formatter, **kwargs)\n\n        # check flag, skip None values\n        if suppress_empty_values and value is None:\n            continue\n\n        # field name can be overridden by the metadata field\n        key_name = a.metadata.get('key') or a.name\n\n        # store converted / formatted value into return dictionary\n        return_dict[key_name] = value\n\n    return return_dict", "response": "Covert a known related object to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncoercing a value into a model object based on a class - type.", "response": "def to_model(cls, value):\n    \"\"\"\n    Coerce a value into a model object based on a class-type (cls).\n    :param cls: class type to coerce into\n    :param value: value to be coerced\n    :return: original value or coerced value (value')\n    \"\"\"\n\n    if isinstance(value, cls) or value is None:\n        pass  # skip if right type or value is None\n\n    elif issubclass(cls, Enum):\n        value = cls(value)\n\n    elif is_model(cls) and isinstance(value, dict):\n        value = convert_key_to_attr_names(cls, value)\n        value = cls(**value)\n\n    else:\n        value = cls(value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_key_to_attr_names(cls, original):\n    attrs = fields(cls)\n    updated = {}\n    keys_pulled = set()\n\n    for a in attrs:\n        key_name = a.metadata.get('key') or a.name\n        if key_name in original:\n            updated[a.name] = original.get(key_name)\n            keys_pulled.add(key_name)\n\n    if getattr(cls, '__related_strict__', False):\n        extra = set(original.keys()) - keys_pulled\n        if len(extra):\n            raise ValueError(\"Extra keys (strict mode): {}\".format(extra))\n\n    return updated", "response": "convert key names to their corresponding attribute names"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_yaml(obj, stream=None, dumper_cls=yaml.Dumper, default_flow_style=False,\n            **kwargs):\n    \"\"\"\n    Serialize a Python object into a YAML stream with OrderedDict and\n    default_flow_style defaulted to False.\n\n    If stream is None, return the produced string instead.\n\n    OrderedDict reference: http://stackoverflow.com/a/21912744\n    default_flow_style reference: http://stackoverflow.com/a/18210750\n\n    :param data: python object to be serialized\n    :param stream: to be serialized to\n    :param Dumper: base Dumper class to extend.\n    :param kwargs: arguments to pass to to_dict\n    :return: stream if provided, string if stream is None\n    \"\"\"\n\n    class OrderedDumper(dumper_cls):\n        pass\n\n    def dict_representer(dumper, data):\n        return dumper.represent_mapping(\n            yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,\n            data.items())\n\n    OrderedDumper.add_representer(OrderedDict, dict_representer)\n\n    obj_dict = to_dict(obj, **kwargs)\n\n    return yaml.dump(obj_dict, stream, OrderedDumper,\n                     default_flow_style=default_flow_style)", "response": "Serialize a Python object into a YAML stream with OrderedDict and defaulted to False."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a YAML stream into a class via the OrderedLoader class.", "response": "def from_yaml(stream, cls=None, loader_cls=yaml.Loader,\n              object_pairs_hook=OrderedDict, **extras):\n    \"\"\"\n    Convert a YAML stream into a class via the OrderedLoader class.\n    \"\"\"\n\n    class OrderedLoader(loader_cls):\n        pass\n\n    def construct_mapping(loader, node):\n        loader.flatten_mapping(node)\n        return object_pairs_hook(loader.construct_pairs(node))\n\n    OrderedLoader.add_constructor(\n        yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,\n        construct_mapping)\n\n    yaml_dict = yaml.load(stream, OrderedLoader) or {}\n    yaml_dict.update(extras)\n    return cls(**yaml_dict) if cls else yaml_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_json(obj, indent=4, sort_keys=True, **kwargs):\n    obj_dict = to_dict(obj, **kwargs)\n    return json.dumps(obj_dict, indent=indent, sort_keys=sort_keys)", "response": "Convert an object to json and then output to json\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_json(stream, cls=None, object_pairs_hook=OrderedDict, **extras):\n    stream = stream.read() if hasattr(stream, 'read') else stream\n    json_dict = json.loads(stream, object_pairs_hook=object_pairs_hook)\n    if extras:\n        json_dict.update(extras)  # pragma: no cover\n    return to_model(cls, json_dict) if cls else json_dict", "response": "Convert a JSON string or stream into a specified class."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates new boolean field on a model.", "response": "def BooleanField(default=NOTHING, required=True, repr=True, cmp=True,\n                 key=None):\n    \"\"\"\n    Create new bool field on a model.\n\n    :param default: any boolean value\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, bool)\n    return attrib(default=default, validator=validator, repr=repr, cmp=cmp,\n                  metadata=dict(key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ChildField(cls, default=NOTHING, required=True, repr=True, cmp=True,\n               key=None):\n    \"\"\"\n    Create new child field on a model.\n\n    :param cls: class (or name) of the model to be related.\n    :param default: any object value of type cls\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, None)\n    converter = converters.to_child_field(cls)\n    validator = _init_fields.init_validator(\n        required, object if isinstance(cls, str) else cls\n    )\n    return attrib(default=default, converter=converter, validator=validator,\n                  repr=repr, cmp=cmp, metadata=dict(key=key))", "response": "Create a new child field on a node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a date field on a node.", "response": "def DateField(formatter=types.DEFAULT_DATE_FORMAT, default=NOTHING,\n              required=True, repr=True, cmp=True, key=None):\n    \"\"\"\n    Create new date field on a model.\n\n    :param formatter: date formatter string (default: \"%Y-%m-%d\")\n    :param default: any date or string that can be converted to a date value\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, date)\n    converter = converters.to_date_field(formatter)\n    return attrib(default=default, converter=converter, validator=validator,\n                  repr=repr, cmp=cmp,\n                  metadata=dict(formatter=formatter, key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new datetime field on a node.", "response": "def DateTimeField(formatter=types.DEFAULT_DATETIME_FORMAT, default=NOTHING,\n                  required=True, repr=True, cmp=True, key=None):\n    \"\"\"\n    Create new datetime field on a model.\n\n    :param formatter: datetime formatter string (default: \"ISO_FORMAT\")\n    :param default: any datetime or string that can be converted to a datetime\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, datetime)\n    converter = converters.to_datetime_field(formatter)\n    return attrib(default=default, converter=converter, validator=validator,\n                  repr=repr, cmp=cmp,\n                  metadata=dict(formatter=formatter, key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef TimeField(formatter=types.DEFAULT_TIME_FORMAT, default=NOTHING,\n              required=True, repr=True, cmp=True, key=None):\n    \"\"\"\n    Create new time field on a model.\n\n    :param formatter: time formatter string (default: \"%H:%M:%S\")\n    :param default: any time or string that can be converted to a time value\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, time)\n    converter = converters.to_time_field(formatter)\n    return attrib(default=default, converter=converter, validator=validator,\n                  repr=repr, cmp=cmp,\n                  metadata=dict(formatter=formatter, key=key))", "response": "Create a new time field on a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates new float field on a node.", "response": "def FloatField(default=NOTHING, required=True, repr=True, cmp=True,\n               key=None):\n    \"\"\"\n    Create new float field on a model.\n\n    :param default: any float value\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, float)\n    return attrib(default=default, converter=converters.float_if_not_none,\n                  validator=validator, repr=repr, cmp=cmp,\n                  metadata=dict(key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate new int field on a CNF object.", "response": "def IntegerField(default=NOTHING, required=True, repr=True, cmp=True,\n                 key=None):\n    \"\"\"\n    Create new int field on a model.\n\n    :param default: any integer value\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, int)\n    return attrib(default=default, converter=converters.int_if_not_none,\n                  validator=validator, repr=repr, cmp=cmp,\n                  metadata=dict(key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates new mapping field on a Sequence object.", "response": "def MappingField(cls, child_key, default=NOTHING, required=True, repr=False,\n                 key=None):\n    \"\"\"\n    Create new mapping field on a model.\n\n    :param cls: class (or name) of the model to be related in Sequence.\n    :param child_key: key field on the child object to be used as the map key.\n    :param default: any mapping type\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, OrderedDict())\n    converter = converters.to_mapping_field(cls, child_key)\n    validator = _init_fields.init_validator(required, types.TypedMapping)\n    return attrib(default=default, converter=converter, validator=validator,\n                  repr=repr, metadata=dict(key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new str field on a model object.", "response": "def RegexField(regex, default=NOTHING, required=True, repr=True, cmp=True,\n               key=None):\n    \"\"\"\n    Create new str field on a model.\n\n    :param regex: regex validation string (e.g. \"[^@]+@[^@]+\" for email)\n    :param default: any string value\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, string_types,\n                                            validators.regex(regex))\n    return attrib(default=default, converter=converters.str_if_not_none,\n                  validator=validator, repr=repr, cmp=cmp,\n                  metadata=dict(key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef SequenceField(cls, default=NOTHING, required=True, repr=False, key=None):\n    default = _init_fields.init_default(required, default, [])\n    converter = converters.to_sequence_field(cls)\n    validator = _init_fields.init_validator(required, types.TypedSequence)\n    return attrib(default=default, converter=converter, validator=validator,\n                  repr=repr, metadata=dict(key=key))", "response": "Create new sequence field on a model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating new set field on a model.", "response": "def SetField(cls, default=NOTHING, required=True, repr=False, key=None):\n    \"\"\"\n    Create new set field on a model.\n\n    :param cls: class (or name) of the model to be related in Set.\n    :param default: any TypedSet or set\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, set())\n    converter = converters.to_set_field(cls)\n    validator = _init_fields.init_validator(required, types.TypedSet)\n    return attrib(default=default, converter=converter, validator=validator,\n                  repr=repr, metadata=dict(key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef URLField(default=NOTHING, required=True, repr=True, cmp=True, key=None):\n    cls = ParseResult\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, cls)\n    return attrib(default=default, converter=converters.str_to_url,\n                  validator=validator, repr=repr, cmp=cmp,\n                  metadata=dict(key=key))", "response": "Create new UUID field on a model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates new UUID field on a model.", "response": "def UUIDField(default=NOTHING, required=False, repr=True, cmp=True, key=None):\n    \"\"\"\n    Create new UUID field on a model.\n\n    :param default: any value\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    cls = UUID\n    default = _init_fields.init_default(required, default, uuid4)\n    validator = _init_fields.init_validator(required, cls)\n    return attrib(default=default, converter=converters.str_to_uuid,\n                  validator=validator, repr=repr, cmp=cmp,\n                  metadata=dict(key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate new decimal field on a node.", "response": "def DecimalField(default=NOTHING, required=True, repr=True, cmp=True,\n                 key=None):\n    \"\"\"\n    Create new decimal field on a model.\n\n    :param default: any decimal value\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, Decimal)\n    return attrib(default=default, converter=lambda x: Decimal(x),\n                  validator=validator, repr=repr, cmp=cmp,\n                  metadata=dict(key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns draco and return stderr and stdout", "response": "def run_clingo(\n    draco_query: List[str],\n    constants: Dict[str, str] = None,\n    files: List[str] = None,\n    relax_hard=False,\n    silence_warnings=False,\n    debug=False,\n) -> Tuple[str, str]:\n    \"\"\"\n    Run draco and return stderr and stdout\n    \"\"\"\n\n    # default args\n    files = files or DRACO_LP\n    if relax_hard and \"hard-integrity.lp\" in files:\n        files.remove(\"hard-integrity.lp\")\n\n    constants = constants or {}\n\n    options = [\"--outf=2\", \"--quiet=1,2,2\"]\n    if silence_warnings:\n        options.append(\"--warn=no-atom-undefined\")\n    for name, value in constants.items():\n        options.append(f\"-c {name}={value}\")\n\n    cmd = [\"clingo\"] + options\n    logger.debug(\"Command: %s\", \" \".join(cmd))\n\n    proc = subprocess.Popen(\n        args=cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n\n    program = \"\\n\".join(draco_query)\n    file_names = [os.path.join(DRACO_LP_DIR, f) for f in files]\n    asp_program = b\"\\n\".join(map(load_file, file_names)) + program.encode(\"utf8\")\n\n    if debug:\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as fd:\n            fd.write(program)\n\n            logger.info('Debug ASP with \"clingo %s %s\"', \" \".join(file_names), fd.name)\n\n    stdout, stderr = proc.communicate(asp_program)\n\n    return (stderr, stdout)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(\n    draco_query: List[str],\n    constants: Dict[str, str] = None,\n    files: List[str] = None,\n    relax_hard=False,\n    silence_warnings=False,\n    debug=False,\n    clear_cache=False,\n) -> Optional[Result]:\n    \"\"\" Run clingo to compute a completion of a partial spec or violations. \"\"\"\n\n    # Clear file cache. useful during development in notebooks.\n    if clear_cache and file_cache:\n        logger.warning(\"Cleared file cache\")\n        file_cache.clear()\n\n    stderr, stdout = run_clingo(\n        draco_query, constants, files, relax_hard, silence_warnings, debug\n    )\n\n    try:\n        json_result = json.loads(stdout)\n    except json.JSONDecodeError:\n        logger.error(\"stdout: %s\", stdout)\n        logger.error(\"stderr: %s\", stderr)\n        raise\n\n    if stderr:\n        logger.error(stderr)\n\n    result = json_result[\"Result\"]\n\n    if result == \"UNSATISFIABLE\":\n        logger.info(\"Constraints are unsatisfiable.\")\n        return None\n    elif result == \"OPTIMUM FOUND\":\n        # get the last witness, which is the best result\n        answers = json_result[\"Call\"][0][\"Witnesses\"][-1]\n\n        logger.debug(answers[\"Value\"])\n\n        return Result(\n            clyngor.Answers(answers[\"Value\"]).sorted,\n            cost=json_result[\"Models\"][\"Costs\"][0],\n        )\n    elif result == \"SATISFIABLE\":\n        answers = json_result[\"Call\"][0][\"Witnesses\"][-1]\n\n        assert (\n            json_result[\"Models\"][\"Number\"] == 1\n        ), \"Should not have more than one model if we don't optimize\"\n\n        logger.debug(answers[\"Value\"])\n\n        return Result(clyngor.Answers(answers[\"Value\"]).sorted)\n    else:\n        logger.error(\"Unsupported result: %s\", result)\n        return None", "response": "Run clingo to compute a completion of a partial spec or violations."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks a task. draco_query returns True if the task is valid False otherwise", "response": "def is_valid(draco_query: List[str], debug=False) -> bool:\n    \"\"\" Check a task.\n        Args:\n            draco_query: a list of facts\n        Returns:\n            whether the task is valid\n    \"\"\"\n    _, stdout = run_clingo(\n        draco_query,\n        files=[\"define.lp\", \"hard.lp\", \"hard-integrity.lp\"],\n        silence_warnings=True,\n        debug=debug,\n    )\n\n    return json.loads(stdout)[\"Result\"] != \"UNSATISFIABLE\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_data_to_asp(file: str) -> List[str]:\n    if file.endswith(\".json\"):\n        with open(file) as f:\n            data = json.load(f)\n            return schema2asp(data2schema(data))\n    elif file.endswith(\".csv\"):\n        df = pd.read_csv(file)\n        df = df.where((pd.notnull(df)), None)\n        data = list(df.T.to_dict().values())\n        schema = data2schema(data)\n        asp = schema2asp(schema)\n        return asp\n    else:\n        raise Exception(\"invalid file type\")", "response": "Reads the given file and generates the ASP definition."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget one item from the deque.", "response": "def get(self):\n        \"\"\"\n        Get one item. This will rotate the deque one step. Repeated gets will\n        return different items.\n\n        \"\"\"\n\n        self.index += 1\n\n        # If we've gone through the entire deque once, shuffle it again to\n        # simulate ever-flowing random. self.shuffle() will run __init__(),\n        # which will reset the index to 0.\n        if self.index == len(self):\n            self.shuffle()\n\n        self.rotate(1)\n        try:\n            return self[0]\n        except:\n            return \"wow\""}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshuffle the deque Deques themselves do not support this, so this will make all items into a list, shuffle that list, clear the deque, and then re-init the deque.", "response": "def shuffle(self):\n        \"\"\"\n        Shuffle the deque\n\n        Deques themselves do not support this, so this will make all items into\n        a list, shuffle that list, clear the deque, and then re-init the deque.\n\n        \"\"\"\n\n        args = list(self)\n        random.shuffle(args)\n\n        self.clear()\n        super(DogeDeque, self).__init__(args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self):\n        if len(self) < 1:\n            return \"wow\"\n\n        if self.index >= len(self):\n            self.index = 0\n\n        step = random.randint(1, min(self.step, len(self)))\n\n        res = self[0]\n        self.index += step\n        self.rotate(step)\n        return res", "response": "Get one item and prepare to get an item with lower\n        rank on the next call."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef onscreen_len(s):\n\n    if sys.version_info < (3, 0) and isinstance(s, str):\n        return len(s)\n\n    length = 0\n    for ch in s:\n        length += 2 if unicodedata.east_asian_width(ch) == 'W' else 1\n\n    return length", "response": "Calculate the length of a unicode string on screen"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup_seasonal(self):\n\n        # If we've specified a season, just run that one\n        if self.ns.season:\n            return self.load_season(self.ns.season)\n\n        # If we've specified another doge or no doge at all, it does not make\n        # sense to use seasons.\n        if self.ns.doge_path is not None and not self.ns.no_shibe:\n            return\n\n        now = datetime.datetime.now()\n\n        for season, data in wow.SEASONS.items():\n            start, end = data['dates']\n            start_dt = datetime.datetime(now.year, start[0], start[1])\n\n            # Be sane if the holiday season spans over New Year's day.\n            end_dt = datetime.datetime(\n                now.year + (start[0] > end[0] and 1 or 0), end[0], end[1])\n\n            if start_dt <= now <= end_dt:\n                # Wow, much holiday!\n                return self.load_season(season)", "response": "Setup appropriate Holidayal words."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies text around doge", "response": "def apply_text(self):\n        \"\"\"\n        Apply text around doge\n\n        \"\"\"\n\n        # Calculate a random sampling of lines that are to have text applied\n        # onto them. Return value is a sorted list of line index integers.\n        linelen = len(self.lines)\n        affected = sorted(random.sample(range(linelen), int(linelen / 3.5)))\n\n        for i, target in enumerate(affected, start=1):\n            line = self.lines[target]\n            line = re.sub('\\n', ' ', line)\n\n            word = self.words.get()\n\n            # If first or last line, or a random selection, use standalone wow.\n            if i == 1 or i == len(affected) or random.choice(range(20)) == 0:\n                word = 'wow'\n\n            # Generate a new DogeMessage, possibly based on a word.\n            self.lines[target] = DogeMessage(self, line, word).generate()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the doge from the file.", "response": "def load_doge(self):\n        \"\"\"\n        Return pretty ASCII Shibe.\n\n        wow\n\n        \"\"\"\n\n        if self.ns.no_shibe:\n            return ['']\n\n        with open(self.doge_path) as f:\n            if sys.version_info < (3, 0):\n                if locale.getpreferredencoding() == 'UTF-8':\n                    doge_lines = [l.decode('utf-8') for l in f.xreadlines()]\n                else:\n                    # encode to printable characters, leaving a space in place\n                    # of untranslatable characters, resulting in a slightly\n                    # blockier doge on non-UTF8 terminals\n                    doge_lines = [\n                        l.decode('utf-8')\n                        .encode(locale.getpreferredencoding(), 'replace')\n                        .replace('?', ' ')\n                        for l in f.xreadlines()\n                    ]\n            else:\n                doge_lines = [l for l in f.readlines()]\n            return doge_lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_real_data(self):\n\n        ret = []\n        username = os.environ.get('USER')\n        if username:\n            ret.append(username)\n\n        editor = os.environ.get('EDITOR')\n        if editor:\n            editor = editor.split('/')[-1]\n            ret.append(editor)\n\n        # OS, hostname and... architechture (because lel)\n        if hasattr(os, 'uname'):\n            uname = os.uname()\n            ret.append(uname[0])\n            ret.append(uname[1])\n            ret.append(uname[4])\n\n        # Grab actual files from $HOME.\n        files = os.listdir(os.environ.get('HOME'))\n        if files:\n            ret.append(random.choice(files))\n\n        # Grab some processes\n        ret += self.get_processes()[:2]\n\n        # Prepare the returned data. First, lowercase it.\n        # If there is unicode data being returned from any of the above\n        # Python 2 needs to decode the UTF bytes to not crash. See issue #45.\n        func = str.lower\n        if sys.version_info < (3,):\n            func = lambda x: str.lower(x).decode('utf-8')\n\n        self.words.extend(map(func, ret))", "response": "Grab actual data from the system and store it in self. words."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget words from stdin.", "response": "def get_stdin_data(self):\n        \"\"\"\n        Get words from stdin.\n\n        \"\"\"\n\n        if self.tty.in_is_tty:\n            # No pipez found\n            return False\n\n        if sys.version_info < (3, 0):\n            stdin_lines = (l.decode('utf-8') for l in sys.stdin.xreadlines())\n        else:\n            stdin_lines = (l for l in sys.stdin.readlines())\n\n        rx_word = re.compile(\"\\w+\", re.UNICODE)\n\n        # If we have stdin data, we should remove everything else!\n        self.words.clear()\n        word_list = [match.group(0)\n                     for line in stdin_lines\n                     for match in rx_word.finditer(line.lower())]\n        if self.ns.filter_stopwords:\n            word_list = self.filter_words(\n                word_list, stopwords=wow.STOPWORDS,\n                min_length=self.ns.min_length)\n\n        self.words.extend(word_list)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngrab a shuffled list of all currently running processes", "response": "def get_processes(self):\n        \"\"\"\n        Grab a shuffled list of all currently running process names\n\n        \"\"\"\n\n        procs = set()\n\n        try:\n            # POSIX ps, so it should work in most environments where doge would\n            p = sp.Popen(['ps', '-A', '-o', 'comm='], stdout=sp.PIPE)\n            output, error = p.communicate()\n\n            if sys.version_info > (3, 0):\n                output = output.decode('utf-8')\n\n            for comm in output.split('\\n'):\n                name = comm.split('/')[-1]\n                # Filter short and weird ones\n                if name and len(name) >= 2 and ':' not in name:\n                    procs.add(name)\n\n        finally:\n            # Either it executed properly or no ps was found.\n            proc_list = list(procs)\n            random.shuffle(proc_list)\n            return proc_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the current terminal size without using a subprocess.", "response": "def get_tty_size(self):\n        \"\"\"\n        Get the current terminal size without using a subprocess\n\n        http://stackoverflow.com/questions/566746\n        I have no clue what-so-fucking ever over how this works or why it\n        returns the size of the terminal in both cells and pixels. But hey, it\n        does.\n\n        \"\"\"\n        if sys.platform == 'win32':\n            # stdin, stdout, stderr = -10, -11, -12\n            ret = self._tty_size_windows(-10)\n            ret = ret or self._tty_size_windows(-11)\n            ret = ret or self._tty_size_windows(-12)\n        else:\n            # stdin, stdout, stderr = 0, 1, 2\n            ret = self._tty_size_linux(0)\n            ret = ret or self._tty_size_linux(1)\n            ret = ret or self._tty_size_linux(2)\n\n        return ret or (25, 80)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsignaling lazy load when QuerySet. _fetch_all fetches rows.", "response": "def signalify_fetch_all(queryset, parser=None, **context):\n    \"\"\"Signal lazy load when `QuerySet._fetch_all` fetches rows. Note: patch\n    `_fetch_all` instead of `iterator` since, as of Django 1.11, the former is\n    used for all fetches while the latter is not.\n    \"\"\"\n    func = queryset._fetch_all\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        if queryset._result_cache is None:\n            signals.lazy_load.send(\n                get_worker(),\n                args=args,\n                kwargs=kwargs,\n                ret=None,\n                context=context,\n                parser=parser,\n            )\n        return func(*args, **kwargs)\n    return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef watch_docs(ctx):\n    try:\n        import sphinx_autobuild  # noqa\n    except ImportError:\n        print('ERROR: watch task requires the sphinx_autobuild package.')\n        print('Install it with:')\n        print('    pip install sphinx-autobuild')\n        sys.exit(1)\n    docs(ctx)\n    ctx.run('sphinx-autobuild {} {}'.format(docs_dir, build_dir), pty=True)", "response": "Run build the docs when a file changes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of lists of coordinates of the polygon.", "response": "def _coords(shape):\n    \"\"\"\n    Return a list of lists of coordinates of the polygon. The list consists\n    firstly of the list of exterior coordinates followed by zero or more lists\n    of any interior coordinates.\n    \"\"\"\n\n    assert shape.geom_type == 'Polygon'\n    coords = [list(shape.exterior.coords)]\n    for interior in shape.interiors:\n        coords.append(list(interior.coords))\n    return coords"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndropping degenerate inners from the polygon.", "response": "def _drop_degenerate_inners(shape):\n    \"\"\"\n    Drop degenerate (zero-size) inners from the polygon.\n\n    This is implemented as dropping anything with a size less than 0.5, as the\n    polygon is in integer coordinates and the smallest valid inner would be a\n    triangle with height and width 1.\n    \"\"\"\n\n    assert shape.geom_type == 'Polygon'\n\n    new_inners = []\n    for inner in shape.interiors:\n        # need to make a polygon of the linearring to get the _filled_ area of\n        # the closed ring.\n        if abs(Polygon(inner).area) >= 0.5:\n            new_inners.append(inner)\n\n    return Polygon(shape.exterior, new_inners)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _union_in_blocks(contours, block_size):\n\n    n_contours = len(contours)\n    for i in range(0, n_contours, block_size):\n        j = min(i + block_size, n_contours)\n\n        inners = []\n        for c in contours[i:j]:\n            p = _contour_to_poly(c)\n            if p.type == 'Polygon':\n                inners.append(p)\n            elif p.type == 'MultiPolygon':\n                inners.extend(p.geoms)\n        holes = unary_union(inners)\n        assert holes.is_valid\n\n        yield holes", "response": "Generator which yields a valid shape for each block_size multiple of\n    input contours. This yields a valid shape for each block_size multiple of\n    input contours."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_valid_pyclipper(shape):\n\n    # drop all degenerate inners\n    clean_shape = _drop_degenerate_inners(shape)\n\n    pc = pyclipper.Pyclipper()\n\n    try:\n        pc.AddPaths(_coords(clean_shape), pyclipper.PT_SUBJECT, True)\n\n        # note: Execute2 returns the polygon tree, not the list of paths\n        result = pc.Execute2(pyclipper.CT_UNION, pyclipper.PFT_EVENODD)\n\n    except pyclipper.ClipperException:\n        return MultiPolygon([])\n\n    return _polytree_to_shapely(result)", "response": "Make a valid pyclipper polygon from a shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a polygon valid.", "response": "def make_valid_polygon(shape):\n    \"\"\"\n    Make a polygon valid. Polygons can be invalid in many ways, such as\n    self-intersection, self-touching and degeneracy. This process attempts to\n    make a polygon valid while retaining as much of its extent or area as\n    possible.\n\n    First, we call pyclipper to robustly union the polygon. Using this on its\n    own appears to be good for \"cleaning\" the polygon.\n\n    This might result in polygons which still have degeneracies according to\n    the OCG standard of validity - as pyclipper does not consider these to be\n    invalid. Therefore we follow by using the `buffer(0)` technique to attempt\n    to remove any remaining degeneracies.\n    \"\"\"\n\n    assert shape.geom_type == 'Polygon'\n\n    shape = make_valid_pyclipper(shape)\n    assert shape.is_valid\n    return shape"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_it_valid(shape):\n\n    if shape.is_empty:\n        return shape\n\n    elif shape.type == 'MultiPolygon':\n        shape = make_valid_multipolygon(shape)\n\n    elif shape.type == 'Polygon':\n        shape = make_valid_polygon(shape)\n\n    return shape", "response": "Attempt to make any polygon or multipolygon valid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode a linear MVT geometry into a list of Lines.", "response": "def _decode_lines(geom):\n    \"\"\"\n    Decode a linear MVT geometry into a list of Lines.\n\n    Each individual linestring in the MVT is extracted to a separate entry in\n    the list of lines.\n    \"\"\"\n\n    lines = []\n    current_line = []\n    current_moveto = None\n\n    # to keep track of the position. we'll adapt the move-to commands to all\n    # be relative to 0,0 at the beginning of each linestring.\n    x = 0\n    y = 0\n\n    end = len(geom)\n    i = 0\n    while i < end:\n        header = geom[i]\n        cmd = header & 7\n        run_length = header // 8\n\n        if cmd == 1:  # move to\n            # flush previous line.\n            if current_moveto:\n                lines.append(Line(current_moveto, EndsAt(x, y), current_line))\n                current_line = []\n\n            assert run_length == 1\n            x += unzigzag(geom[i+1])\n            y += unzigzag(geom[i+2])\n            i += 3\n\n            current_moveto = MoveTo(x, y)\n\n        elif cmd == 2:  # line to\n            assert current_moveto\n\n            # we just copy this run, since it's encoding isn't going to change\n            next_i = i + 1 + run_length * 2\n            current_line.extend(geom[i:next_i])\n\n            # but we still need to decode it to figure out where each move-to\n            # command is in absolute space.\n            for j in xrange(0, run_length):\n                dx = unzigzag(geom[i + 1 + 2 * j])\n                dy = unzigzag(geom[i + 2 + 2 * j])\n                x += dx\n                y += dy\n\n            i = next_i\n\n        else:\n            raise ValueError('Unhandled command: %d' % cmd)\n\n    if current_line:\n        assert current_moveto\n        lines.append(Line(current_moveto, EndsAt(x, y), current_line))\n\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreorder the list of lines so that the distance from the end of one to the beginning of the next is minimised.", "response": "def _reorder_lines(lines):\n    \"\"\"\n    Reorder lines so that the distance from the end of one to the beginning of\n    the next is minimised.\n    \"\"\"\n\n    x = 0\n    y = 0\n    new_lines = []\n\n    # treat the list of lines as a stack, off which we keep popping the best\n    # one to add next.\n    while lines:\n        # looping over all the lines like this isn't terribly efficient, but\n        # in local tests seems to handle a few thousand lines without a\n        # problem.\n        min_dist = None\n        min_i = None\n        for i, line in enumerate(lines):\n            moveto, _, _ = line\n\n            dist = abs(moveto.x - x) + abs(moveto.y - y)\n            if min_dist is None or dist < min_dist:\n                min_dist = dist\n                min_i = i\n\n        assert min_i is not None\n        line = lines.pop(min_i)\n        _, endsat, _ = line\n        x = endsat.x\n        y = endsat.y\n        new_lines.append(line)\n\n    return new_lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding a sequence of bytes as an MVT tile and reorder the string table of the layers and the order of its multilinestrings.", "response": "def optimise_tile(tile_bytes):\n    \"\"\"\n    Decode a sequence of bytes as an MVT tile and reorder the string table of\n    its layers and the order of its multilinestrings to save a few bytes.\n    \"\"\"\n\n    t = tile()\n    t.ParseFromString(tile_bytes)\n\n    for layer in t.layers:\n        sto = StringTableOptimiser()\n\n        for feature in layer.features:\n            # (multi)linestrings only\n            if feature.type == 2:\n                optimise_multilinestring(feature.geometry)\n\n            sto.add_tags(feature.tags)\n\n        sto.update_string_table(layer)\n\n    return t.SerializeToString()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the coordinates of the snap in the grid with integer coordinates", "response": "def coords_on_grid(self, x, y):\n        \"\"\" Snap coordinates on the grid with integer coordinates \"\"\"\n\n        if isinstance(x, float):\n            x = int(self._round(x))\n        if isinstance(y, float):\n            y = int(self._round(y))\n        if not self._y_coord_down:\n            y = self._extents - y\n        return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_messages(self, email_messages):\n        if not email_messages:\n            return\n        with self._lock:\n            try:\n                stream_created = self.open()\n                for message in email_messages:\n                    if six.PY3:\n                        self.write_message(message)\n                    else:\n                        self.write_message(message)\n                    self.stream.flush()\n                if stream_created:\n                    self.close()\n            except:\n                if not self.fail_silently:\n                    raise\n        return len(email_messages)", "response": "Write all the messages to the stream in a thread - safe way."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsignaling handler for SIGTERM.", "response": "def sigterm(self, signum, frame):\n        \"\"\"\n        These actions will be done after SIGTERM.\n        \"\"\"\n        self.logger.warning(\"Caught signal %s. Stopping daemon.\" % signum)\n        sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exit(self):\n        self.logger.warning(\"Stopping daemon.\")\n        os.remove(self.pid)\n        sys.exit(0)", "response": "Cleanup pid file at exit."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts the daemonization process.", "response": "def start(self):\n        \"\"\"\n        Start daemonization process.\n        \"\"\"\n        # If pidfile already exists, we should read pid from there; to overwrite it, if locking\n        # will fail, because locking attempt somehow purges the file contents.\n        if os.path.isfile(self.pid):\n            with open(self.pid, \"r\") as old_pidfile:\n                old_pid = old_pidfile.read()\n        # Create a lockfile so that only one instance of this daemon is running at any time.\n        try:\n            lockfile = open(self.pid, \"w\")\n        except IOError:\n            print(\"Unable to create the pidfile.\")\n            sys.exit(1)\n        try:\n            # Try to get an exclusive lock on the file. This will fail if another process has the file\n            # locked.\n            fcntl.flock(lockfile, fcntl.LOCK_EX | fcntl.LOCK_NB)\n        except IOError:\n            print(\"Unable to lock on the pidfile.\")\n            # We need to overwrite the pidfile if we got here.\n            with open(self.pid, \"w\") as pidfile:\n                pidfile.write(old_pid)\n            sys.exit(1)\n\n        # skip fork if foreground is specified\n        if not self.foreground:\n            # Fork, creating a new process for the child.\n            try:\n                process_id = os.fork()\n            except OSError as e:\n                self.logger.error(\"Unable to fork, errno: {0}\".format(e.errno))\n                sys.exit(1)\n            if process_id != 0:\n                if self.keep_fds:\n                    # This is the parent process. Exit without cleanup,\n                    # see https://github.com/thesharp/daemonize/issues/46\n                    os._exit(0)\n                else:\n                    sys.exit(0)\n            # This is the child process. Continue.\n\n            # Stop listening for signals that the parent process receives.\n            # This is done by getting a new process id.\n            # setpgrp() is an alternative to setsid().\n            # setsid puts the process in a new parent group and detaches its controlling terminal.\n            process_id = os.setsid()\n            if process_id == -1:\n                # Uh oh, there was a problem.\n                sys.exit(1)\n\n            # Add lockfile to self.keep_fds.\n            self.keep_fds.append(lockfile.fileno())\n\n            # Close all file descriptors, except the ones mentioned in self.keep_fds.\n            devnull = \"/dev/null\"\n            if hasattr(os, \"devnull\"):\n                # Python has set os.devnull on this system, use it instead as it might be different\n                # than /dev/null.\n                devnull = os.devnull\n\n            if self.auto_close_fds:\n                for fd in range(3, resource.getrlimit(resource.RLIMIT_NOFILE)[0]):\n                    if fd not in self.keep_fds:\n                        try:\n                            os.close(fd)\n                        except OSError:\n                            pass\n\n            devnull_fd = os.open(devnull, os.O_RDWR)\n            os.dup2(devnull_fd, 0)\n            os.dup2(devnull_fd, 1)\n            os.dup2(devnull_fd, 2)\n            os.close(devnull_fd)\n\n        if self.logger is None:\n            # Initialize logging.\n            self.logger = logging.getLogger(self.app)\n            self.logger.setLevel(logging.DEBUG)\n            # Display log messages only on defined handlers.\n            self.logger.propagate = False\n\n            # Initialize syslog.\n            # It will correctly work on OS X, Linux and FreeBSD.\n            if sys.platform == \"darwin\":\n                syslog_address = \"/var/run/syslog\"\n            else:\n                syslog_address = \"/dev/log\"\n\n            # We will continue with syslog initialization only if actually have such capabilities\n            # on the machine we are running this.\n            if os.path.exists(syslog_address):\n                syslog = handlers.SysLogHandler(syslog_address)\n                if self.verbose:\n                    syslog.setLevel(logging.DEBUG)\n                else:\n                    syslog.setLevel(logging.INFO)\n                # Try to mimic to normal syslog messages.\n                formatter = logging.Formatter(\"%(asctime)s %(name)s: %(message)s\",\n                                              \"%b %e %H:%M:%S\")\n                syslog.setFormatter(formatter)\n\n                self.logger.addHandler(syslog)\n\n        # Set umask to default to safe file permissions when running as a root daemon. 027 is an\n        # octal number which we are typing as 0o27 for Python3 compatibility.\n        os.umask(0o27)\n\n        # Change to a known directory. If this isn't done, starting a daemon in a subdirectory that\n        # needs to be deleted results in \"directory busy\" errors.\n        os.chdir(self.chdir)\n\n        # Execute privileged action\n        privileged_action_result = self.privileged_action()\n        if not privileged_action_result:\n            privileged_action_result = []\n\n        # Change owner of pid file, it's required because pid file will be removed at exit.\n        uid, gid = -1, -1\n\n        if self.group:\n            try:\n                gid = grp.getgrnam(self.group).gr_gid\n            except KeyError:\n                self.logger.error(\"Group {0} not found\".format(self.group))\n                sys.exit(1)\n\n        if self.user:\n            try:\n                uid = pwd.getpwnam(self.user).pw_uid\n            except KeyError:\n                self.logger.error(\"User {0} not found.\".format(self.user))\n                sys.exit(1)\n\n        if uid != -1 or gid != -1:\n            os.chown(self.pid, uid, gid)\n\n        # Change gid\n        if self.group:\n            try:\n                os.setgid(gid)\n            except OSError:\n                self.logger.error(\"Unable to change gid.\")\n                sys.exit(1)\n\n        # Change uid\n        if self.user:\n            try:\n                uid = pwd.getpwnam(self.user).pw_uid\n            except KeyError:\n                self.logger.error(\"User {0} not found.\".format(self.user))\n                sys.exit(1)\n            try:\n                os.setuid(uid)\n            except OSError:\n                self.logger.error(\"Unable to change uid.\")\n                sys.exit(1)\n\n        try:\n            lockfile.write(\"%s\" % (os.getpid()))\n            lockfile.flush()\n        except IOError:\n            self.logger.error(\"Unable to write pid to the pidfile.\")\n            print(\"Unable to write pid to the pidfile.\")\n            sys.exit(1)\n\n        # Set custom action on SIGTERM.\n        signal.signal(signal.SIGTERM, self.sigterm)\n        atexit.register(self.exit)\n\n        self.logger.warning(\"Starting daemon.\")\n\n        try:\n            self.action(*privileged_action_result)\n        except Exception:\n            for line in traceback.format_exc().split(\"\\n\"):\n                self.logger.error(line)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef js_reverse_inline(context):\n    if 'request' in context:\n        default_urlresolver = get_resolver(getattr(context['request'], 'urlconf', None))\n    else:\n        default_urlresolver = get_resolver(None)\n    return mark_safe(generate_js(default_urlresolver))", "response": "Outputs a string of javascript that can generate URLs via the use\n    of the names given to those URLs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare_url_list(urlresolver, namespace_path='', namespace=''):\n    exclude_ns = getattr(settings, 'JS_REVERSE_EXCLUDE_NAMESPACES', JS_EXCLUDE_NAMESPACES)\n    include_only_ns = getattr(settings, 'JS_REVERSE_INCLUDE_ONLY_NAMESPACES', JS_INCLUDE_ONLY_NAMESPACES)\n\n    if exclude_ns and include_only_ns:\n        raise ImproperlyConfigured(\n            'Neither use JS_REVERSE_EXCLUDE_NAMESPACES nor JS_REVERSE_INCLUDE_ONLY_NAMESPACES setting')\n\n    if namespace[:-1] in exclude_ns:\n        return\n\n    include_only_allow = True  # include_only state varible\n\n    if include_only_ns != []:\n        # True mean that ns passed the test\n        in_on_empty_ns = False\n        in_on_is_in_list = False\n        in_on_null = False\n\n        # Test urls without ns\n        if namespace == '' and '' in include_only_ns:\n            in_on_empty_ns = True\n\n        # check if nestead ns isn't subns of include_only ns\n        # e.g. ns = \"foo:bar\" include_only = [\"foo\"] -> this ns will be used\n        # works for ns = \"lorem:ipsum:dolor\" include_only = [\"lorem:ipsum\"]\n        # ns \"lorem\" will be ignored but \"lorem:ipsum\" & \"lorem:ipsum:..\" won't\n        for ns in include_only_ns:\n            if ns != \"\" and namespace[:-1].startswith(ns):\n                in_on_is_in_list = True\n                break\n\n        # Test if isn't used \"\\0\" flag\n        # use \"foo\\0\" to add urls just from \"foo\" not from subns \"foo:bar\"\n        if namespace[:-1] + '\\0' in include_only_ns:\n            in_on_null = True\n\n        include_only_allow = in_on_empty_ns or in_on_is_in_list or in_on_null\n\n    if include_only_allow:\n        for url_name in urlresolver.reverse_dict.keys():\n            if isinstance(url_name, (text_type, str)):\n                url_patterns = []\n                for url_pattern in urlresolver.reverse_dict.getlist(url_name):\n                    url_patterns += [\n                        [namespace_path + pat[0], pat[1]] for pat in url_pattern[0]]\n                yield [namespace + url_name, url_patterns]\n\n    for inner_ns, (inner_ns_path, inner_urlresolver) in \\\n            urlresolver.namespace_dict.items():\n        inner_ns_path = namespace_path + inner_ns_path\n        inner_ns = namespace + inner_ns + ':'\n\n        # if we have inner_ns_path, reconstruct a new resolver so that we can\n        # handle regex substitutions within the regex of a namespace.\n        if inner_ns_path:\n            args = [inner_ns_path, inner_urlresolver]\n\n            # https://github.com/ierror/django-js-reverse/issues/65\n            if StrictVersion(django.get_version()) >= StrictVersion(\"2.0.6\"):\n                args.append(tuple(urlresolver.pattern.converters.items()))\n\n            inner_urlresolver = urlresolvers.get_ns_resolver(*args)\n            inner_ns_path = ''\n\n        for x in prepare_url_list(inner_urlresolver, inner_ns_path, inner_ns):\n            yield x", "response": "Prepare the list of urls for the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a jsmin version of the even Taxonomy.", "response": "def _make_jsmin(python_only=False):\n    \"\"\"\n    Generate JS minifier based on `jsmin.c by Douglas Crockford`_\n\n    .. _jsmin.c by Douglas Crockford:\n       http://www.crockford.com/javascript/jsmin.c\n\n    :Parameters:\n      `python_only` : ``bool``\n        Use only the python variant. If true, the c extension is not even\n        tried to be loaded.\n\n    :Return: Minifier\n    :Rtype: ``callable``\n    \"\"\"\n    # pylint: disable = R0912, R0914, W0612\n\n    if not python_only:\n        try:\n            import _rjsmin  # pylint: disable = F0401\n        except ImportError:\n            pass\n        else:\n            return _rjsmin.jsmin\n    try:\n        xrange\n    except NameError:\n        xrange = range  # pylint: disable = W0622\n\n    space_chars = r'[\\000-\\011\\013\\014\\016-\\040]'\n\n    line_comment = r'(?://[^\\r\\n]*)'\n    space_comment = r'(?:/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/)'\n    space_comment_nobang = r'(?:/\\*(?!!)[^*]*\\*+(?:[^/*][^*]*\\*+)*/)'\n    bang_comment = r'(?:/\\*![^*]*\\*+(?:[^/*][^*]*\\*+)*/)'\n\n    string1 = \\\n        r'(?:\\047[^\\047\\\\\\r\\n]*(?:\\\\(?:[^\\r\\n]|\\r?\\n|\\r)[^\\047\\\\\\r\\n]*)*\\047)'\n    string2 = r'(?:\"[^\"\\\\\\r\\n]*(?:\\\\(?:[^\\r\\n]|\\r?\\n|\\r)[^\"\\\\\\r\\n]*)*\")'\n    strings = r'(?:%s|%s)' % (string1, string2)\n\n    charclass = r'(?:\\[[^\\\\\\]\\r\\n]*(?:\\\\[^\\r\\n][^\\\\\\]\\r\\n]*)*\\])'\n    nospecial = r'[^/\\\\\\[\\r\\n]'\n    regex = r'(?:/(?![\\r\\n/*])%s*(?:(?:\\\\[^\\r\\n]|%s)%s*)*/)' % (\n        nospecial, charclass, nospecial\n    )\n    space = r'(?:%s|%s)' % (space_chars, space_comment)\n    space_nobang = r'(?:%s|%s)' % (space_chars, space_comment_nobang)\n    newline = r'(?:%s?[\\r\\n])' % line_comment\n\n    def fix_charclass(result):\n        \"\"\" Fixup string of chars to fit into a regex char class \"\"\"\n        pos = result.find('-')\n        if pos >= 0:\n            result = r'%s%s-' % (result[:pos], result[pos + 1:])\n\n        def sequentize(string):\n            \"\"\"\n            Notate consecutive characters as sequence\n\n            (1-4 instead of 1234)\n            \"\"\"\n            first, last, result = None, None, []\n            for char in map(ord, string):\n                if last is None:\n                    first = last = char\n                elif last + 1 == char:\n                    last = char\n                else:\n                    result.append((first, last))\n                    first = last = char\n            if last is not None:\n                result.append((first, last))\n            return ''.join(['%s%s%s' % (\n                chr(first),\n                last > first + 1 and '-' or '',\n                last != first and chr(last) or ''\n            ) for first, last in result])\n\n        return _re.sub(\n            r'([\\000-\\040\\047])',  # \\047 for better portability\n            lambda m: '\\\\%03o' % ord(m.group(1)), (\n                sequentize(result)\n                .replace('\\\\', '\\\\\\\\')\n                .replace('[', '\\\\[')\n                .replace(']', '\\\\]')\n            )\n        )\n\n    def id_literal_(what):\n        \"\"\" Make id_literal like char class \"\"\"\n        match = _re.compile(what).match\n        result = ''.join([\n            chr(c) for c in xrange(127) if not match(chr(c))\n        ])\n        return '[^%s]' % fix_charclass(result)\n\n    def not_id_literal_(keep):\n        \"\"\" Make negated id_literal like char class \"\"\"\n        match = _re.compile(id_literal_(keep)).match\n        result = ''.join([\n            chr(c) for c in xrange(127) if not match(chr(c))\n        ])\n        return r'[%s]' % fix_charclass(result)\n\n    not_id_literal = not_id_literal_(r'[a-zA-Z0-9_$]')\n    preregex1 = r'[(,=:\\[!&|?{};\\r\\n]'\n    preregex2 = r'%(not_id_literal)sreturn' % locals()\n\n    id_literal = id_literal_(r'[a-zA-Z0-9_$]')\n    id_literal_open = id_literal_(r'[a-zA-Z0-9_${\\[(!+-]')\n    id_literal_close = id_literal_(r'[a-zA-Z0-9_$}\\])\"\\047+-]')\n\n    dull = r'[^\\047\"/\\000-\\040]'\n\n    space_sub_simple = _re.compile((\n        # noqa pylint: disable = C0330\n\n        r'(%(dull)s+)'\n        r'|(%(strings)s%(dull)s*)'\n        r'|(?<=%(preregex1)s)'\n            r'%(space)s*(?:%(newline)s%(space)s*)*'\n            r'(%(regex)s%(dull)s*)'\n        r'|(?<=%(preregex2)s)'\n            r'%(space)s*(?:%(newline)s%(space)s)*'\n            r'(%(regex)s%(dull)s*)'\n        r'|(?<=%(id_literal_close)s)'\n            r'%(space)s*(?:(%(newline)s)%(space)s*)+'\n            r'(?=%(id_literal_open)s)'\n        r'|(?<=%(id_literal)s)(%(space)s)+(?=%(id_literal)s)'\n        r'|(?<=\\+)(%(space)s)+(?=\\+)'\n        r'|(?<=-)(%(space)s)+(?=-)'\n        r'|%(space)s+'\n        r'|(?:%(newline)s%(space)s*)+'\n    ) % locals()).sub\n    #print space_sub_simple.__self__.pattern\n\n    def space_subber_simple(match):\n        \"\"\" Substitution callback \"\"\"\n        # pylint: disable = R0911\n\n        groups = match.groups()\n        if groups[0]:\n            return groups[0]\n        elif groups[1]:\n            return groups[1]\n        elif groups[2]:\n            return groups[2]\n        elif groups[3]:\n            return groups[3]\n        elif groups[4]:\n            return '\\n'\n        elif groups[5] or groups[6] or groups[7]:\n            return ' '\n        else:\n            return ''\n\n    space_sub_banged = _re.compile((\n        # noqa pylint: disable = C0330\n\n        r'(%(dull)s+)'\n        r'|(%(strings)s%(dull)s*)'\n        r'|(%(bang_comment)s%(dull)s*)'\n        r'|(?<=%(preregex1)s)'\n            r'%(space)s*(?:%(newline)s%(space)s*)*'\n            r'(%(regex)s%(dull)s*)'\n        r'|(?<=%(preregex2)s)'\n            r'%(space)s*(?:%(newline)s%(space)s)*'\n            r'(%(regex)s%(dull)s*)'\n        r'|(?<=%(id_literal_close)s)'\n            r'%(space)s*(?:(%(newline)s)%(space)s*)+'\n            r'(?=%(id_literal_open)s)'\n        r'|(?<=%(id_literal)s)(%(space)s)+(?=%(id_literal)s)'\n        r'|(?<=\\+)(%(space)s)+(?=\\+)'\n        r'|(?<=-)(%(space)s)+(?=-)'\n        r'|%(space)s+'\n        r'|(?:%(newline)s%(space)s*)+'\n    ) % dict(locals(), space=space_nobang)).sub\n    #print space_sub_banged.__self__.pattern\n\n    def space_subber_banged(match):\n        \"\"\" Substitution callback \"\"\"\n        # pylint: disable = R0911\n\n        groups = match.groups()\n        if groups[0]:\n            return groups[0]\n        elif groups[1]:\n            return groups[1]\n        elif groups[2]:\n            return groups[2]\n        elif groups[3]:\n            return groups[3]\n        elif groups[4]:\n            return groups[4]\n        elif groups[5]:\n            return '\\n'\n        elif groups[6] or groups[7] or groups[8]:\n            return ' '\n        else:\n            return ''\n\n    def jsmin(script, keep_bang_comments=False):  # pylint: disable = W0621\n        r\"\"\"\n        Minify javascript based on `jsmin.c by Douglas Crockford`_\\.\n\n        Instead of parsing the stream char by char, it uses a regular\n        expression approach which minifies the whole script with one big\n        substitution regex.\n\n        .. _jsmin.c by Douglas Crockford:\n           http://www.crockford.com/javascript/jsmin.c\n\n        :Parameters:\n          `script` : ``str``\n            Script to minify\n\n          `keep_bang_comments` : ``bool``\n            Keep comments starting with an exclamation mark? (``/*!...*/``)\n\n        :Return: Minified script\n        :Rtype: ``str``\n        \"\"\"\n        if keep_bang_comments:\n            return space_sub_banged(\n                space_subber_banged, '\\n%s\\n' % script\n            ).strip()\n        else:\n            return space_sub_simple(\n                space_subber_simple, '\\n%s\\n' % script\n            ).strip()\n\n    return jsmin"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef jsmin_for_posers(script, keep_bang_comments=False):\n    if not keep_bang_comments:\n        rex = (\n            r'([^\\047\"/\\000-\\040]+)|((?:(?:\\047[^\\047\\\\\\r\\n]*(?:\\\\(?:[^\\r\\n]'\n            r'|\\r?\\n|\\r)[^\\047\\\\\\r\\n]*)*\\047)|(?:\"[^\"\\\\\\r\\n]*(?:\\\\(?:[^\\r\\n]'\n            r'|\\r?\\n|\\r)[^\"\\\\\\r\\n]*)*\"))[^\\047\"/\\000-\\040]*)|(?<=[(,=:\\[!&|?'\n            r'{};\\r\\n])(?:[\\000-\\011\\013\\014\\016-\\040]|(?:/\\*[^*]*\\*+(?:[^/*'\n            r'][^*]*\\*+)*/))*(?:(?:(?://[^\\r\\n]*)?[\\r\\n])(?:[\\000-\\011\\013\\0'\n            r'14\\016-\\040]|(?:/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/))*)*((?:/(?![\\r'\n            r'\\n/*])[^/\\\\\\[\\r\\n]*(?:(?:\\\\[^\\r\\n]|(?:\\[[^\\\\\\]\\r\\n]*(?:\\\\[^\\r'\n            r'\\n][^\\\\\\]\\r\\n]*)*\\]))[^/\\\\\\[\\r\\n]*)*/)[^\\047\"/\\000-\\040]*)|(?<'\n            r'=[\\000-#%-,./:-@\\[-^`{-~-]return)(?:[\\000-\\011\\013\\014\\016-\\04'\n            r'0]|(?:/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/))*(?:(?:(?://[^\\r\\n]*)?['\n            r'\\r\\n])(?:[\\000-\\011\\013\\014\\016-\\040]|(?:/\\*[^*]*\\*+(?:[^/*][^'\n            r'*]*\\*+)*/)))*((?:/(?![\\r\\n/*])[^/\\\\\\[\\r\\n]*(?:(?:\\\\[^\\r\\n]|(?:'\n            r'\\[[^\\\\\\]\\r\\n]*(?:\\\\[^\\r\\n][^\\\\\\]\\r\\n]*)*\\]))[^/\\\\\\[\\r\\n]*)*/)['\n            r'^\\047\"/\\000-\\040]*)|(?<=[^\\000-!#%&(*,./:-@\\[\\\\^`{|~])(?:[\\000'\n            r'-\\011\\013\\014\\016-\\040]|(?:/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/))*(?'\n            r':((?:(?://[^\\r\\n]*)?[\\r\\n]))(?:[\\000-\\011\\013\\014\\016-\\040]|(?'\n            r':/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/))*)+(?=[^\\000-\\040\"#%-\\047)*,.'\n            r'/:-@\\\\-^`|-~])|(?<=[^\\000-#%-,./:-@\\[-^`{-~-])((?:[\\000-\\011\\0'\n            r'13\\014\\016-\\040]|(?:/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/)))+(?=[^\\00'\n            r'0-#%-,./:-@\\[-^`{-~-])|(?<=\\+)((?:[\\000-\\011\\013\\014\\016-\\040]'\n            r'|(?:/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/)))+(?=\\+)|(?<=-)((?:[\\000-'\n            r'\\011\\013\\014\\016-\\040]|(?:/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/)))+(?'\n            r'=-)|(?:[\\000-\\011\\013\\014\\016-\\040]|(?:/\\*[^*]*\\*+(?:[^/*][^*]'\n            r'*\\*+)*/))+|(?:(?:(?://[^\\r\\n]*)?[\\r\\n])(?:[\\000-\\011\\013\\014\\0'\n            r'16-\\040]|(?:/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/))*)+'\n        )\n\n        def subber(match):\n            \"\"\" Substitution callback \"\"\"\n            groups = match.groups()\n            return (\n                groups[0] or\n                groups[1] or\n                groups[2] or\n                groups[3] or\n                (groups[4] and '\\n') or\n                (groups[5] and ' ') or\n                (groups[6] and ' ') or\n                (groups[7] and ' ') or\n                ''\n            )\n    else:\n        rex = (\n            r'([^\\047\"/\\000-\\040]+)|((?:(?:\\047[^\\047\\\\\\r\\n]*(?:\\\\(?:[^\\r\\n]'\n            r'|\\r?\\n|\\r)[^\\047\\\\\\r\\n]*)*\\047)|(?:\"[^\"\\\\\\r\\n]*(?:\\\\(?:[^\\r\\n]'\n            r'|\\r?\\n|\\r)[^\"\\\\\\r\\n]*)*\"))[^\\047\"/\\000-\\040]*)|((?:/\\*![^*]*\\*'\n            r'+(?:[^/*][^*]*\\*+)*/)[^\\047\"/\\000-\\040]*)|(?<=[(,=:\\[!&|?{};\\r'\n            r'\\n])(?:[\\000-\\011\\013\\014\\016-\\040]|(?:/\\*(?!!)[^*]*\\*+(?:[^/*'\n            r'][^*]*\\*+)*/))*(?:(?:(?://[^\\r\\n]*)?[\\r\\n])(?:[\\000-\\011\\013\\0'\n            r'14\\016-\\040]|(?:/\\*(?!!)[^*]*\\*+(?:[^/*][^*]*\\*+)*/))*)*((?:/('\n            r'?![\\r\\n/*])[^/\\\\\\[\\r\\n]*(?:(?:\\\\[^\\r\\n]|(?:\\[[^\\\\\\]\\r\\n]*(?:'\n            r'\\\\[^\\r\\n][^\\\\\\]\\r\\n]*)*\\]))[^/\\\\\\[\\r\\n]*)*/)[^\\047\"/\\000-\\040]'\n            r'*)|(?<=[\\000-#%-,./:-@\\[-^`{-~-]return)(?:[\\000-\\011\\013\\014\\0'\n            r'16-\\040]|(?:/\\*(?!!)[^*]*\\*+(?:[^/*][^*]*\\*+)*/))*(?:(?:(?://['\n            r'^\\r\\n]*)?[\\r\\n])(?:[\\000-\\011\\013\\014\\016-\\040]|(?:/\\*(?!!)[^*'\n            r']*\\*+(?:[^/*][^*]*\\*+)*/)))*((?:/(?![\\r\\n/*])[^/\\\\\\[\\r\\n]*(?:('\n            r'?:\\\\[^\\r\\n]|(?:\\[[^\\\\\\]\\r\\n]*(?:\\\\[^\\r\\n][^\\\\\\]\\r\\n]*)*\\]))[^/'\n            r'\\\\\\[\\r\\n]*)*/)[^\\047\"/\\000-\\040]*)|(?<=[^\\000-!#%&(*,./:-@\\[\\\\'\n            r'^`{|~])(?:[\\000-\\011\\013\\014\\016-\\040]|(?:/\\*(?!!)[^*]*\\*+(?:['\n            r'^/*][^*]*\\*+)*/))*(?:((?:(?://[^\\r\\n]*)?[\\r\\n]))(?:[\\000-\\011'\n            r'\\013\\014\\016-\\040]|(?:/\\*(?!!)[^*]*\\*+(?:[^/*][^*]*\\*+)*/))*)+'\n            r'(?=[^\\000-\\040\"#%-\\047)*,./:-@\\\\-^`|-~])|(?<=[^\\000-#%-,./:-@'\n            r'\\[-^`{-~-])((?:[\\000-\\011\\013\\014\\016-\\040]|(?:/\\*(?!!)[^*]*\\*'\n            r'+(?:[^/*][^*]*\\*+)*/)))+(?=[^\\000-#%-,./:-@\\[-^`{-~-])|(?<=\\+)'\n            r'((?:[\\000-\\011\\013\\014\\016-\\040]|(?:/\\*(?!!)[^*]*\\*+(?:[^/*][^'\n            r'*]*\\*+)*/)))+(?=\\+)|(?<=-)((?:[\\000-\\011\\013\\014\\016-\\040]|(?:'\n            r'/\\*(?!!)[^*]*\\*+(?:[^/*][^*]*\\*+)*/)))+(?=-)|(?:[\\000-\\011\\013'\n            r'\\014\\016-\\040]|(?:/\\*(?!!)[^*]*\\*+(?:[^/*][^*]*\\*+)*/))+|(?:(?'\n            r':(?://[^\\r\\n]*)?[\\r\\n])(?:[\\000-\\011\\013\\014\\016-\\040]|(?:/\\*('\n            r'?!!)[^*]*\\*+(?:[^/*][^*]*\\*+)*/))*)+'\n        )\n\n        def subber(match):\n            \"\"\" Substitution callback \"\"\"\n            groups = match.groups()\n            return (\n                groups[0] or\n                groups[1] or\n                groups[2] or\n                groups[3] or\n                groups[4] or\n                (groups[5] and '\\n') or\n                (groups[6] and ' ') or\n                (groups[7] and ' ') or\n                (groups[8] and ' ') or\n                ''\n            )\n\n    return _re.sub(rex, subber, '\\n%s\\n' % script).strip()", "response": "Minify javascript based on Douglas Crockford."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over each record in the archive", "response": "def _iterate_records(self):\n        \"\"\" iterate over each record\n        \"\"\"\n        raise_invalid_gzip = False\n        empty_record = False\n\n        while True:\n            try:\n                self.record = self._next_record(self.next_line)\n                if raise_invalid_gzip:\n                    self._raise_invalid_gzip_err()\n\n                yield self.record\n\n            except EOFError:\n                empty_record = True\n\n            self.read_to_end()\n\n            if self.reader.decompressor:\n                # if another gzip member, continue\n                if self.reader.read_next_member():\n                    continue\n\n                # if empty record, then we're done\n                elif empty_record:\n                    break\n\n                # otherwise, probably a gzip\n                # containing multiple non-chunked records\n                # raise this as an error\n                else:\n                    raise_invalid_gzip = True\n\n            # non-gzip, so we're done\n            elif empty_record:\n                break\n\n        self.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _raise_invalid_gzip_err(self):\n        frmt = 'warc/arc'\n        if self.known_format:\n            frmt = self.known_format\n\n        frmt_up = frmt.upper()\n\n        msg = self.GZIP_ERR_MSG.format(frmt, frmt_up)\n        raise ArchiveLoadFailed(msg)", "response": "Raise an exception if the gzip file with multiple ARC records is not in the known format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _consume_blanklines(self):\n        empty_size = 0\n        first_line = True\n\n        while True:\n            line = self.reader.readline()\n            if len(line) == 0:\n                return None, empty_size\n\n            stripped = line.rstrip()\n\n            if len(stripped) == 0 or first_line:\n                empty_size += len(line)\n\n                if len(stripped) != 0:\n                    # if first line is not blank,\n                    # likely content-length was invalid, display warning\n                    err_offset = self.fh.tell() - self.reader.rem_length() - empty_size\n                    sys.stderr.write(self.INC_RECORD.format(err_offset, line))\n                    self.err_count += 1\n\n                first_line = False\n                continue\n\n            return line, empty_size", "response": "Consume blank lines from the file and return them as a string and the size of the line that can be read."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the next digester from the stream and update the record with the data read.", "response": "def read_to_end(self, record=None):\n        \"\"\" Read remainder of the stream\n        If a digester is included, update it\n        with the data read\n        \"\"\"\n\n        # no current record to read\n        if not self.record:\n            return None\n\n        # already at end of this record, don't read until it is consumed\n        if self.member_info:\n            return None\n\n        curr_offset = self.offset\n\n        while True:\n            b = self.record.raw_stream.read(BUFF_SIZE)\n            if not b:\n                break\n\n        \"\"\"\n        - For compressed files, blank lines are consumed\n          since they are part of record length\n        - For uncompressed files, blank lines are read later,\n          and not included in the record length\n        \"\"\"\n        #if self.reader.decompressor:\n        self.next_line, empty_size = self._consume_blanklines()\n\n        self.offset = self.fh.tell() - self.reader.rem_length()\n        #if self.offset < 0:\n        #    raise Exception('Not Gzipped Properly')\n\n        if self.next_line:\n            self.offset -= len(self.next_line)\n\n        length = self.offset - curr_offset\n\n        if not self.reader.decompressor:\n            length -= empty_size\n\n        self.member_info = (curr_offset, length)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the next record from the reader stream and return the record object.", "response": "def _next_record(self, next_line):\n        \"\"\" Use loader to parse the record from the reader stream\n        Supporting warc and arc records\n        \"\"\"\n        record = self.loader.parse_record_stream(self.reader,\n                                                 next_line,\n                                                 self.known_format,\n                                                 self.no_record_parse,\n                                                 self.ensure_http_headers)\n\n        self.member_info = None\n\n        # Track known format for faster parsing of other records\n        if not self.mixed_arc_warc:\n            self.known_format = record.format\n\n        return record"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, length=None):\n        all_buffs = []\n        while length is None or length > 0:\n            self._fillbuff()\n            if self.empty():\n                break\n\n            buff = self.buff.read(length)\n            all_buffs.append(buff)\n            if length:\n                length -= len(buff)\n\n        return b''.join(all_buffs)", "response": "Read some number of bytes from the internal buffer and return them as a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef readline(self, length=None):\n        if length == 0:\n            return b''\n\n        self._fillbuff()\n\n        if self.empty():\n            return b''\n\n        linebuff = self.buff.readline(length)\n\n        # we may be at a boundary\n        while not linebuff.endswith(b'\\n'):\n            if length:\n                length -= len(linebuff)\n                if length <= 0:\n                    break\n\n            self._fillbuff()\n\n            if self.empty():\n                break\n\n            linebuff += self.buff.readline(length)\n\n        return linebuff", "response": "Read a full line from the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iso_date_to_datetime(string):\n\n    nums = DATE_TIMESPLIT.split(string)\n    if nums[-1] == '':\n        nums = nums[:-1]\n\n    if len(nums) == 7:\n        nums[6] = nums[6][:6]\n        nums[6] += PAD_MICRO[len(nums[6]):]\n\n    the_datetime = datetime.datetime(*(int(num) for num in nums))\n    return the_datetime", "response": "Convert ISO date to datetime."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef datetime_to_http_date(the_datetime):\n    timeval = calendar.timegm(the_datetime.utctimetuple())\n    return formatdate(timeval=timeval,\n                      localtime=False,\n                      usegmt=True)", "response": "Convert a datetime to an HTTP date."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning ISO 8601 date string for the given datetime.", "response": "def datetime_to_iso_date(the_datetime, use_micros=False):\n    \"\"\"\n    >>> datetime_to_iso_date(datetime.datetime(2013, 12, 26, 10, 11, 12))\n    '2013-12-26T10:11:12Z'\n\n    >>> datetime_to_iso_date(datetime.datetime(2013, 12, 26, 10, 11, 12, 456789))\n    '2013-12-26T10:11:12Z'\n\n    >>> datetime_to_iso_date(datetime.datetime(2013, 12, 26, 10, 11, 12), use_micros=True)\n    '2013-12-26T10:11:12Z'\n\n    >>> datetime_to_iso_date(datetime.datetime(2013, 12, 26, 10, 11, 12, 456789), use_micros=True)\n    '2013-12-26T10:11:12.456789Z'\n\n    >>> datetime_to_iso_date(datetime.datetime(2013, 12, 26, 10, 11, 12, 1), use_micros=True)\n    '2013-12-26T10:11:12.000001Z'\n\n    \"\"\"\n\n    if not use_micros:\n        return the_datetime.strftime(ISO_DT)\n    else:\n        return the_datetime.isoformat() + 'Z'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pad_timestamp(string, pad_str=PAD_6_UP):\n\n    str_len = len(string)\n    pad_len = len(pad_str)\n\n    if str_len < pad_len:\n        string = string + pad_str[str_len:]\n\n    return string", "response": "Pads a timestamp string to fit in the specified length."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef timestamp_to_datetime(string):\n\n    # pad to 6 digits\n    string = pad_timestamp(string, PAD_6_UP)\n\n    def clamp(val, min_, max_):\n        try:\n            val = int(val)\n            val = max(min_, min(val, max_))\n            return val\n        except:\n            return max_\n\n    def extract(string, start, end, min_, max_):\n        if len(string) >= end:\n            return clamp(string[start:end], min_, max_)\n        else:\n            return max_\n\n    # now parse, clamp to boundary\n    year = extract(string, 0, 4, 1900, 2999)\n    month = extract(string, 4, 6, 1, 12)\n    day = extract(string, 6, 8, 1, calendar.monthrange(year, month)[1])\n    hour = extract(string, 8, 10, 0, 23)\n    minute = extract(string, 10, 12, 0, 59)\n    second = extract(string, 12, 14, 0, 59)\n\n    return datetime.datetime(year=year,\n                             month=month,\n                             day=day,\n                             hour=hour,\n                             minute=minute,\n                             second=second)", "response": "Converts a timestamp to a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_record_stream(self, stream,\n                            statusline=None,\n                            known_format=None,\n                            no_record_parse=False,\n                            ensure_http_headers=False):\n        \"\"\" Parse file-like stream and return an ArcWarcRecord\n        encapsulating the record headers, http headers (if any),\n        and a stream limited to the remainder of the record.\n\n        Pass statusline and known_format to detect_type_loader_headers()\n        to faciliate parsing.\n        \"\"\"\n        (the_format, rec_headers) = (self.\n                                     _detect_type_load_headers(stream,\n                                                               statusline,\n                                                               known_format))\n\n        if the_format == 'arc':\n            uri = rec_headers.get_header('uri')\n            length = rec_headers.get_header('length')\n            content_type = rec_headers.get_header('content-type')\n            sub_len = rec_headers.total_len\n            if uri and uri.startswith('filedesc://'):\n                rec_type = 'arc_header'\n            else:\n                rec_type = 'response'\n\n        elif the_format in ('warc', 'arc2warc'):\n            rec_type = rec_headers.get_header('WARC-Type')\n            uri = self._ensure_target_uri_format(rec_headers)\n            length = rec_headers.get_header('Content-Length')\n            content_type = rec_headers.get_header('Content-Type')\n            if the_format == 'warc':\n                sub_len = 0\n            else:\n                sub_len = rec_headers.total_len\n                the_format = 'warc'\n\n        is_err = False\n\n        try:\n            if length is not None:\n                length = int(length) - sub_len\n                if length < 0:\n                    is_err = True\n\n        except (ValueError, TypeError):\n            is_err = True\n\n        # err condition\n        if is_err:\n            length = 0\n\n        # limit stream to the length for all valid records\n        if length is not None and length >= 0:\n            stream = LimitReader.wrap_stream(stream, length)\n\n\n        http_headers = None\n\n        # load http headers if parsing\n        if not no_record_parse:\n            http_headers = self.load_http_headers(rec_type, uri, stream, length)\n\n        # generate validate http headers (eg. for replay)\n        if not http_headers and ensure_http_headers:\n            http_headers = self.default_http_headers(length, content_type)\n\n        return ArcWarcRecord(the_format, rec_type,\n                             rec_headers, stream, http_headers,\n                             content_type, length)", "response": "Parse a record stream and return an ArcWarcRecord object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetecting type of record from stream and return the header of the record.", "response": "def _detect_type_load_headers(self, stream,\n                                  statusline=None, known_format=None):\n        \"\"\" If known_format is specified ('warc' or 'arc'),\n        parse only as that format.\n\n        Otherwise, try parsing record as WARC, then try parsing as ARC.\n        if neither one succeeds, we're out of luck.\n        \"\"\"\n\n        if known_format != 'arc':\n            # try as warc first\n            try:\n                rec_headers = self.warc_parser.parse(stream, statusline)\n                return 'warc', rec_headers\n            except StatusAndHeadersParserException as se:\n                if known_format == 'warc':\n                    msg = 'Invalid WARC record, first line: '\n                    raise ArchiveLoadFailed(msg + str(se.statusline))\n\n                statusline = se.statusline\n                pass\n\n        # now try as arc\n        try:\n            rec_headers = self.arc_parser.parse(stream, statusline)\n            return self.arc_parser.get_rec_type(), rec_headers\n        except StatusAndHeadersParserException as se:\n            if known_format == 'arc':\n                msg = 'Invalid ARC record, first line: '\n            else:\n                msg = 'Unknown archive format, first line: '\n            raise ArchiveLoadFailed(msg + str(se.statusline))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ensure_target_uri_format(self, rec_headers):\n        uri = rec_headers.get_header('WARC-Target-URI')\n        if uri is not None and uri.startswith('<') and uri.endswith('>'):\n            uri = uri[1:-1]\n            rec_headers.replace_header('WARC-Target-URI', uri)\n        return uri", "response": "Checks the value for the WARC - Target - URI header field to see if it is a valid value for the field. If it is corrects and updates the field to the correct value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps the given stream in a LimitReader if it is not already a LimitReader.", "response": "def wrap_stream(stream, content_length):\n        \"\"\"\n        If given content_length is an int > 0, wrap the stream\n        in a LimitReader. Otherwise, return the stream unaltered\n        \"\"\"\n        try:\n            content_length = int(content_length)\n            if content_length >= 0:\n                # optimize: if already a LimitStream, set limit to\n                # the smaller of the two limits\n                if isinstance(stream, LimitReader):\n                    stream.limit = min(stream.limit, content_length)\n                else:\n                    stream = LimitReader(stream, content_length)\n\n        except (ValueError, TypeError):\n            pass\n\n        return stream"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef headers_to_str_headers(headers):\n    '''\n    Converts dict or tuple-based headers of bytes or str to\n    tuple-based headers of str, which is the python norm (pep 3333)\n    '''\n    ret = []\n\n    if isinstance(headers, collections_abc.Mapping):\n        h = headers.items()\n    else:\n        h = headers\n\n    if six.PY2:  #pragma: no cover\n        return h\n\n    for tup in h:\n        k, v = tup\n        if isinstance(k, six.binary_type):\n            k = k.decode('iso-8859-1')\n        if isinstance(v, six.binary_type):\n            v = v.decode('iso-8859-1')\n        ret.append((k, v))\n    return ret", "response": "Converts dict or tuple - based headers of bytes or str to\n    tuple - based headers of str which is the python norm"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open(filename, mode='r', **kwargs):  #pragma: no cover\n    if six.PY3 or 'x' not in mode:\n        return sys_open(filename, mode, **kwargs)\n\n    flags = os.O_EXCL | os.O_CREAT | os.O_WRONLY\n    if 'b' in mode and hasattr(os, 'O_BINARY'):\n        flags |= os.O_BINARY\n\n    fd = os.open(filename, flags)\n    mode = mode.replace('x', 'w')\n    return os.fdopen(fd, mode, 0x664)", "response": "open file with exclusive mode x"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the value of a header.", "response": "def get_header(self, name, default_value=None):\n        \"\"\"\n        return header (name, value)\n        if found\n        \"\"\"\n        name_lower = name.lower()\n        for value in self.headers:\n            if value[0].lower() == name_lower:\n                return value[1]\n\n        return default_value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreplacing header with new value or add new header", "response": "def replace_header(self, name, value):\n        \"\"\"\n        replace header with new value or add new header\n        return old header value, if any\n        \"\"\"\n        name_lower = name.lower()\n        for index in range(len(self.headers) - 1, -1, -1):\n            curr_name, curr_value = self.headers[index]\n            if curr_name.lower() == name_lower:\n                self.headers[index] = (curr_name, value)\n                return curr_value\n\n        self.headers.append((name, value))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a header from the list of available headers.", "response": "def remove_header(self, name):\n        \"\"\"\n        Remove header (case-insensitive)\n        return True if header removed, False otherwise\n        \"\"\"\n        name_lower = name.lower()\n        for index in range(len(self.headers) - 1, -1, -1):\n            if self.headers[index][0].lower() == name_lower:\n                del self.headers[index]\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_statusline(self, valid_statusline):\n        code = self.get_statuscode()\n        try:\n            code = int(code)\n            assert(code > 0)\n            return True\n        except(ValueError, AssertionError):\n            self.statusline = valid_statusline\n            return False", "response": "Check that the statusline is valid eg. starts with numeric\n            code."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_range(self, start, part_len, total_len):\n        content_range = 'bytes {0}-{1}/{2}'.format(start,\n                                                   start + part_len - 1,\n                                                   total_len)\n\n        self.statusline = '206 Partial Content'\n        self.replace_header('Content-Range', content_range)\n        self.replace_header('Content-Length', str(part_len))\n        self.replace_header('Accept-Ranges', 'bytes')\n        return self", "response": "Add range headers indicating that this is a partial response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_ascii_bytes(self, filter_func=None):\n        try:\n            string = self.to_str(filter_func)\n            string = string.encode('ascii')\n        except (UnicodeEncodeError, UnicodeDecodeError):\n            self.percent_encode_non_ascii_headers()\n            string = self.to_str(filter_func)\n            string = string.encode('ascii')\n\n        return string + b'\\r\\n'", "response": "Convert the RFCscoal headers block as ascii."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes any non - ascii headers in the message.", "response": "def percent_encode_non_ascii_headers(self, encoding='UTF-8'):\n        \"\"\" Encode any headers that are not plain ascii\n            as UTF-8 as per:\n            https://tools.ietf.org/html/rfc8187#section-3.2.3\n            https://tools.ietf.org/html/rfc5987#section-3.2.2\n        \"\"\"\n        def do_encode(m):\n            return \"*={0}''\".format(encoding) + quote(to_native_str(m.group(1)))\n\n        for index in range(len(self.headers) - 1, -1, -1):\n            curr_name, curr_value = self.headers[index]\n            try:\n                # test if header is ascii encodable, no action needed\n                curr_value.encode('ascii')\n            except:\n                new_value = self.ENCODE_HEADER_RX.sub(do_encode, curr_value)\n                if new_value == curr_value:\n                    new_value = quote(curr_value)\n\n                self.headers[index] = (curr_name, new_value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, stream, full_statusline=None):\n\n        # status line w newlines intact\n        if full_statusline is None:\n            full_statusline = stream.readline()\n\n        full_statusline = self.decode_header(full_statusline)\n\n        statusline, total_read = _strip_count(full_statusline, 0)\n\n        headers = []\n\n        # at end of stream\n        if total_read == 0:\n            raise EOFError()\n        elif not statusline:\n            return StatusAndHeaders(statusline=statusline,\n                                    headers=headers,\n                                    protocol='',\n                                    total_len=total_read)\n\n        # validate only if verify is set\n        if self.verify:\n            protocol_status = self.split_prefix(statusline, self.statuslist)\n\n            if not protocol_status:\n                msg = 'Expected Status Line starting with {0} - Found: {1}'\n                msg = msg.format(self.statuslist, statusline)\n                raise StatusAndHeadersParserException(msg, full_statusline)\n        else:\n            protocol_status = statusline.split(' ', 1)\n\n        line, total_read = _strip_count(self.decode_header(stream.readline()), total_read)\n        while line:\n            result = line.split(':', 1)\n            if len(result) == 2:\n                name = result[0].rstrip(' \\t')\n                value = result[1].lstrip()\n            else:\n                name = result[0]\n                value = None\n\n            next_line, total_read = _strip_count(self.decode_header(stream.readline()),\n                                                 total_read)\n\n            # append continuation lines, if any\n            while next_line and next_line.startswith((' ', '\\t')):\n                if value is not None:\n                    value += next_line\n                next_line, total_read = _strip_count(self.decode_header(stream.readline()),\n                                                     total_read)\n\n            if value is not None:\n                header = (name, value)\n                headers.append(header)\n\n            line = next_line\n\n        if len(protocol_status) > 1:\n            statusline = protocol_status[1].strip()\n        else:\n            statusline = ''\n\n        return StatusAndHeaders(statusline=statusline,\n                                headers=headers,\n                                protocol=protocol_status[0],\n                                total_len=total_read)", "response": "parse stream for status line and headers returning a StatusAndHeaders object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit key string into prefix and remainder for first matching prefix from a list", "response": "def split_prefix(key, prefixs):\n        \"\"\"\n        split key string into prefix and remainder\n        for first matching prefix from a list\n        \"\"\"\n        key_upper = key.upper()\n        for prefix in prefixs:\n            if key_upper.startswith(prefix):\n                plen = len(prefix)\n                return (key_upper[:plen], key[plen:])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an iterator to produce an SQL dump of the database.", "response": "def _iterdump(connection):\n    \"\"\"\n    Returns an iterator to the dump of the database in an SQL text format.\n\n    Used to produce an SQL dump of the database.  Useful to save an in-memory\n    database for later restoration.  This function should not be called\n    directly but instead called from the Connection method, iterdump().\n    \"\"\"\n\n    cu = connection.cursor()\n    yield('BEGIN TRANSACTION;')\n\n    # sqlite_master table contains the SQL CREATE statements for the database.\n    q = \"\"\"\n        SELECT \"name\", \"type\", \"sql\"\n        FROM \"sqlite_master\"\n            WHERE \"sql\" NOT NULL AND\n            \"type\" == 'table'\n            ORDER BY \"name\"\n        \"\"\"\n    schema_res = cu.execute(q)\n    for table_name, type, sql in schema_res.fetchall():\n        if table_name == 'sqlite_sequence':\n            yield('DELETE FROM \"sqlite_sequence\";')\n        elif table_name == 'sqlite_stat1':\n            yield('ANALYZE \"sqlite_master\";')\n        elif table_name.startswith('sqlite_'):\n            continue\n        # NOTE: Virtual table support not implemented\n        #elif sql.startswith('CREATE VIRTUAL TABLE'):\n        #    qtable = table_name.replace(\"'\", \"''\")\n        #    yield(\"INSERT INTO sqlite_master(type,name,tbl_name,rootpage,sql)\"\\\n        #        \"VALUES('table','{0}','{0}',0,'{1}');\".format(\n        #        qtable,\n        #        sql.replace(\"''\")))\n        else:\n            yield('{0};'.format(sql))\n\n        # Build the insert statement for each row of the current table\n        table_name_ident = table_name.replace('\"', '\"\"')\n        res = cu.execute('PRAGMA table_info(\"{0}\")'.format(table_name_ident))\n        column_names = [str(table_info[1]) for table_info in res.fetchall()]\n        q = \"\"\"SELECT 'INSERT INTO \"{0}\" VALUES({1})' FROM \"{0}\";\"\"\".format(\n            table_name_ident,\n            \",\".join(\"\"\"'||quote(\"{0}\")||'\"\"\".format(col.replace('\"', '\"\"')) for col in column_names))\n        query_res = cu.execute(q)\n        for row in query_res:\n            yield(\"{0};\".format(row[0]))\n\n    # Now when the type is 'index', 'trigger', or 'view'\n    q = \"\"\"\n        SELECT \"name\", \"type\", \"sql\"\n        FROM \"sqlite_master\"\n            WHERE \"sql\" NOT NULL AND\n            \"type\" IN ('index', 'trigger', 'view')\n        \"\"\"\n    schema_res = cu.execute(q)\n    for name, type, sql in schema_res.fetchall():\n        yield('{0};'.format(sql))\n\n    yield('COMMIT;')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ask_questions(self):\n        out_config = {}\n\n        for key in c.CONFIG_QUESTIONS:\n            section = key[1]\n            print\n            for question in section:\n                answer = utils.ask(question[1], question[2])\n                out_config[\"{0}_{1}\".format(key[0], question[0])] = answer\n\n        for key in c.CONFIG_MULTIPLE_CHOICE_QUESTIONS:\n            section = key[1]\n            print\n            # keep going until we get what we want\n            while True:\n                input = utils.ask(section[1], \"\")\n                if (input.isdigit() and\n                        int(input) > 0 and\n                        int(input) <= section[0]):\n                    answer = input\n\n                    break\n                print\n\n            # store the answer as 1 less than it is, we're dealing with\n            # a list to select the number which is 0 indexed\n            answer = int(input) - 1\n\n            if key[0] == \"license\":\n                out_config[\"license_type\"] = c.LICENSE_TYPES[answer][0]\n                out_config[\"license_url\"] = c.LICENSE_TYPES[answer][1]\n\n        # merge in defaults without asking questions\n        merged_config = dict(c.CONFIG_DEFAULTS.items() + out_config.items())\n\n        return merged_config", "response": "Ask questions and return config settings from user."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a message with a specific color.", "response": "def log(color, *args):\n    \"\"\"\n    Print a message with a specific color.\n    \"\"\"\n    if color in c.LOG_COLOR.keys():\n        out_color = c.LOG_COLOR[color]\n    else:\n        out_color = color\n\n    for arg in args:\n        print clr.stringc(arg, out_color)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the usage for the help command.", "response": "def usage():\n    \"\"\"\n    Return the usage for the help command.\n    \"\"\"\n    l_bracket = clr.stringc(\"[\", \"dark gray\")\n    r_bracket = clr.stringc(\"]\", \"dark gray\")\n    pipe = clr.stringc(\"|\", \"dark gray\")\n\n    app_name = clr.stringc(\"%prog\", \"bright blue\")\n    commands = clr.stringc(\"{0}\".format(pipe).join(c.VALID_ACTIONS), \"normal\")\n    help = clr.stringc(\"--help\", \"green\")\n    options = clr.stringc(\"options\", \"yellow\")\n\n    guide = \"\\n\\n\"\n    for action in c.VALID_ACTIONS:\n        guide += command_name(app_name, action,\n                              c.MESSAGES[\"help_\" + action])\n\n    # remove the last line break\n    guide = guide[:-1]\n\n    return \"{0} {1}{2}{3} {1}{4}{3} {1}{5}{3}\\n{6}\".format(app_name,\n                                                           l_bracket,\n                                                           commands,\n                                                           r_bracket,\n                                                           help,\n                                                           options,\n                                                           guide)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef epilogue(app_name):\n    app_name = clr.stringc(app_name, \"bright blue\")\n    command = clr.stringc(\"command\", \"cyan\")\n    help = clr.stringc(\"--help\", \"green\")\n\n    return \"\\n%s %s %s for more info on a command\\n\" % (app_name,\n                                                        command, help)", "response": "Return the epilogue for the help command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef command_name(app_name, command, help_text):\n    command = clr.stringc(command, \"cyan\")\n    help = clr.stringc(\"--help\", \"green\")\n\n    return \"{0} {1} {2}\\n{3}\\n\\n\".format(app_name, command,\n                                         help, help_text)", "response": "Return a snippet of help text for this command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint the role information.", "response": "def role(name, report, role_name_length):\n    \"\"\"\n    Print the role information.\n    \"\"\"\n    pad_role_name_by = 11 + role_name_length\n\n    defaults = field_value(report[\"total_defaults\"], \"defaults\", \"blue\", 16)\n    facts = field_value(report[\"total_facts\"], \"facts\", \"purple\", 16)\n    files = field_value(report[\"total_files\"], \"files\", \"dark gray\", 16)\n    lines = field_value(report[\"total_lines\"], \"lines\", \"normal\", 16)\n\n    print \"{0:<{1}} {2} {3} {4} {5}\".format(\n        clr.stringc(name, c.LOG_COLOR[report[\"state\"]]),\n        pad_role_name_by, defaults, facts, files, lines)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints a specific field s stats.", "response": "def field_value(key, label, color, padding):\n    \"\"\"\n    Print a specific field's stats.\n    \"\"\"\n    if not clr.has_colors and padding > 0:\n        padding = 7\n\n    if color == \"bright gray\" or color == \"dark gray\":\n        bright_prefix = \"\"\n    else:\n        bright_prefix = \"bright \"\n\n    field = clr.stringc(key, \"{0}{1}\".format(bright_prefix, color))\n    field_label = clr.stringc(label, color)\n\n    return \"{0:>{1}} {2}\".format(field, padding, field_label)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef totals(report, total_roles, role_name_length):\n    roles_len_string = len(str(total_roles))\n    roles_label_len = 6  # \"r\" \"o\" \"l\" \"e\" \"s\" \" \"\n\n    if clr.has_colors:\n        roles_count_offset = 22\n    else:\n        roles_count_offset = 13\n\n    roles_count_offset += role_name_length\n\n    # no idea honestly but it fixes the formatting\n    # it will probably break the formatting if you have 100+ roles\n    if roles_len_string > 1:\n        roles_count_offset -= 2\n\n    pad_roles_by = roles_count_offset + roles_len_string + roles_label_len\n\n    roles = field_value(total_roles, \"roles\", \"normal\", 0)\n    defaults = field_value(report[\"defaults\"], \"defaults\", \"blue\", 16)\n    facts = field_value(report[\"facts\"], \"facts\", \"purple\", 16)\n    files = field_value(report[\"files\"], \"files\", \"dark gray\", 16)\n    lines = field_value(report[\"lines\"], \"lines\", \"normal\", 16)\n\n    print \"\".join(clr.stringc(\"-\", \"black\") * 79)\n    print \"{0} {2:>{1}} {3} {4} {5}\".format(\n        roles, pad_roles_by, defaults, facts, files, lines)", "response": "Print the totals for each role."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint the gen totals.", "response": "def gen_totals(report, file_type):\n    \"\"\"\n    Print the gen totals.\n    \"\"\"\n    label = clr.stringc(file_type + \" files   \", \"bright purple\")\n\n    ok = field_value(report[\"ok_role\"], \"ok\", c.LOG_COLOR[\"ok\"], 0)\n    skipped = field_value(report[\"skipped_role\"], \"skipped\",\n                          c.LOG_COLOR[\"skipped\"], 16)\n    changed = field_value(report[\"changed_role\"], \"changed\",\n                          c.LOG_COLOR[\"changed\"], 16)\n\n    # missing_meta = field_value(report[\"missing_meta_role\"],\n    #                            \"missing meta(s)\",\n    #                            c.LOG_COLOR[\"missing_meta\"], 16)\n\n    # print \"\\n{0} {1} {2} {3}\".format(ok, skipped, changed, missing_meta)\n\n    print \"\\n{0} {1} {2} {3}\".format(label, ok, skipped, changed)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting the scan totals.", "response": "def scan_totals(report):\n    \"\"\"\n    Print the scan totals.\n    \"\"\"\n    ok = field_value(report[\"ok_role\"], \"ok\", c.LOG_COLOR[\"ok\"], 0)\n    missing_readme = field_value(report[\"missing_readme_role\"],\n                                 \"missing readme(s)\",\n                                 c.LOG_COLOR[\"missing_readme\"], 16)\n\n    missing_meta = field_value(report[\"missing_meta_role\"],\n                               \"missing meta(s)\",\n                               c.LOG_COLOR[\"missing_meta\"], 16)\n\n    print \"\\n{0} {1} {2}\".format(ok, missing_readme, missing_meta)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine if the terminal supports ANSI colors.", "response": "def has_colors(stream):\n    \"\"\"\n    Determine if the terminal supports ansi colors.\n    \"\"\"\n    if not hasattr(stream, \"isatty\"):\n        return False\n    if not stream.isatty():\n        return False  # auto color only on TTYs\n    try:\n        import curses\n        curses.setupterm()\n        return curses.tigetnum(\"colors\") > 2\n    except:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string with terminal colors.", "response": "def stringc(text, color):\n    \"\"\"\n    Return a string with terminal colors.\n    \"\"\"\n    if has_colors:\n        text = str(text)\n\n        return \"\\033[\"+codeCodes[color]+\"m\"+text+\"\\033[0m\"\n    else:\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute_command(self):\n        stderr = \"\"\n        role_count = 0\n        for role in utils.roles_dict(self.roles_path):\n            self.command = self.command.replace(\"%role_name\", role)\n            (_, err) = utils.capture_shell(\"cd {0} && {1}\".\n                                           format(os.path.join(\n                                                  self.roles_path, role),\n                                                  self.command))\n\n            stderr = err\n            role_count += 1\n\n        utils.exit_if_no_roles(role_count, self.roles_path)\n\n        if len(stderr) > 0:\n            ui.error(c.MESSAGES[\"run_error\"], stderr[:-1])\n        else:\n            if not self.config[\"options_quiet\"]:\n                ui.ok(c.MESSAGES[\"run_success\"].replace(\n                    \"%role_count\", str(role_count)), self.options.command)", "response": "Execute the shell command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nemulates the behavior of mkdir - p.", "response": "def mkdir_p(path):\n    \"\"\"\n    Emulate the behavior of mkdir -p.\n    \"\"\"\n    try:\n        os.makedirs(path)\n    except OSError as err:\n        if err.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            ui.error(c.MESSAGES[\"path_unmakable\"], err)\n            sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef string_to_file(path, input):\n    mkdir_p(os.path.dirname(path))\n\n    with codecs.open(path, \"w+\", \"UTF-8\") as file:\n        file.write(input)", "response": "Write a file from a given string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef file_to_string(path):\n    if not os.path.exists(path):\n        ui.error(c.MESSAGES[\"path_missing\"], path)\n        sys.exit(1)\n\n    with codecs.open(path, \"r\", \"UTF-8\") as contents:\n        return contents.read()", "response": "Return the contents of a file when given a path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the contents of a file as a list when given a path.", "response": "def file_to_list(path):\n    \"\"\"\n    Return the contents of a file as a list when given a path.\n    \"\"\"\n    if not os.path.exists(path):\n        ui.error(c.MESSAGES[\"path_missing\"], path)\n        sys.exit(1)\n\n    with codecs.open(path, \"r\", \"UTF-8\") as contents:\n        lines = contents.read().splitlines()\n\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef url_to_string(url):\n    try:\n        page = urllib2.urlopen(url)\n    except (urllib2.HTTPError, urllib2.URLError) as err:\n        ui.error(c.MESSAGES[\"url_unreachable\"], err)\n        sys.exit(1)\n\n    return page", "response": "Return the contents of a web site url as a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a jinja2 template instance with extends support.", "response": "def template(path, extend_path, out):\n    \"\"\"\n    Return a jinja2 template instance with extends support.\n    \"\"\"\n    files = []\n\n    # add the \"extender\" template when it exists\n    if len(extend_path) > 0:\n        # determine the base readme path\n        base_path = os.path.dirname(extend_path)\n        new_base_path = os.path.join(base_path, \"README.{0}.j2\".format(out))\n\n        if os.path.exists(new_base_path):\n            path = new_base_path\n\n        if os.path.exists(extend_path):\n            files = [path, extend_path]\n        else:\n            ui.error(c.MESSAGES[\"template_extender_missing\"])\n            ui.error(extend_path)\n            sys.exit(1)\n    else:\n        files = [path]\n\n    try:\n        # Use the subclassed relative environment class\n        env = RelEnvironment(trim_blocks=True)\n\n        # Add a unique dict filter, by key.\n        # DEPRECATION WARNING: This is only used for backwards compatibility,\n        #                      please use the unique filter instead.\n        def unique_dict(items, key):\n            return {v[key]: v for v in items}.values()\n\n        env.filters[\"unique_dict\"] = unique_dict\n\n        def unique(a):\n\n            # Don\u2019t use the following commented out optimization which is used\n            # in ansible/lib/ansible/plugins/filter/mathstuff.py in Ansigenome\n            # as it resorts the role dependencies:\n            # if isinstance(a,collections.Hashable):\n            #     c = set(a)\n\n            c = []\n            for x in a:\n                if x not in c:\n                    c.append(x)\n            return c\n\n        env.filters[\"unique\"] = unique\n\n        # create a dictionary of templates\n        templates = dict(\n            (name, codecs.open(name, \"rb\", 'UTF-8').read())\n            for name in files)\n        env.loader = DictLoader(templates)\n\n        # return the final result (the last template in the list)\n        return env.get_template(files[len(files) - 1])\n    except Exception as err:\n        ui.error(c.MESSAGES[\"template_error\"], err)\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all files in a path but exclude git folders.", "response": "def files_in_path(path):\n    \"\"\"\n    Return a list of all files in a path but exclude git folders.\n    \"\"\"\n    aggregated_files = []\n\n    for dir_, _, files in os.walk(path):\n        for file in files:\n            relative_dir = os.path.relpath(dir_, path)\n\n            if \".git\" not in relative_dir:\n                relative_file = os.path.join(relative_dir, file)\n                aggregated_files.append(relative_file)\n\n    return aggregated_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exit_if_path_not_found(path):\n    if not os.path.exists(path):\n        ui.error(c.MESSAGES[\"path_missing\"], path)\n        sys.exit(1)", "response": "Exit if the path is not found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef yaml_load(path, input=\"\", err_quit=False):\n    try:\n        if len(input) > 0:\n            return yaml.safe_load(input)\n        elif len(path) > 0:\n            return yaml.safe_load(file_to_string(path))\n    except Exception as err:\n        file = os.path.basename(path)\n        ui.error(\"\",\n                 c.MESSAGES[\"yaml_error\"].replace(\"%file\", file), err,\n                 \"\")\n\n        if err_quit:\n            sys.exit(1)\n\n        return False", "response": "Load a yaml dict from a file or string with error handling."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_nice_yaml(yaml_input, indentation=2):\n    return yaml.safe_dump(yaml_input, indent=indentation,\n                          allow_unicode=True, default_flow_style=False)", "response": "Return condensed yaml into human readable yaml."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn stdout and stderr from a command.", "response": "def capture_shell(command):\n    \"\"\"\n    Return the stdout/stderr from a command.\n    \"\"\"\n    proc = subprocess.Popen(command,\n                            stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE,\n                            shell=True)\n\n    return proc.communicate()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef keys_in_dict(d, parent_key, keys):\n    for key, value in d.iteritems():\n        if isinstance(value, dict):\n            keys_in_dict(value, key, keys)\n        else:\n            if parent_key:\n                prefix = parent_key + \".\"\n            else:\n                prefix = \"\"\n\n            keys.append(prefix + key)\n\n    return keys", "response": "Recursively creates a list of keys from a dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef swap_yaml_string(file_path, swaps):\n    original_file = file_to_string(file_path)\n    new_file = original_file\n    changed = False\n\n    for item in swaps:\n        match = re.compile(r'(?<={0}: )([\"\\']?)(.*)\\1'.format(item[0]),\n                           re.MULTILINE)\n\n        new_file = re.sub(match, item[1], new_file)\n\n    if new_file != original_file:\n        changed = True\n\n    string_to_file(file_path, new_file)\n\n    return (new_file, changed)", "response": "Swap a string in a yaml file without touching the existing formatting."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exit_if_no_roles(roles_count, roles_path):\n    if roles_count == 0:\n        ui.warn(c.MESSAGES[\"empty_roles_path\"], roles_path)\n        sys.exit()", "response": "Exit if there are no roles found."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict of role names and repo paths.", "response": "def roles_dict(path, repo_prefix=\"\", repo_sub_dir=\"\"):\n    \"\"\"\n    Return a dict of role names and repo paths.\n    \"\"\"\n    exit_if_path_not_found(path)\n\n    aggregated_roles = {}\n\n    roles = os.walk(path).next()[1]\n\n    # First scan all directories\n    for role in roles:\n        for sub_role in roles_dict(path + \"/\" + role, repo_prefix=\"\",\n                                   repo_sub_dir=role + \"/\"):\n            aggregated_roles[role + \"/\" + sub_role] = role + \"/\" + sub_role\n\n    # Then format them\n    for role in roles:\n        if is_role(os.path.join(path, role)):\n            if isinstance(role, basestring):\n                role_repo = \"{0}{1}\".format(repo_prefix, role_name(role))\n\n                aggregated_roles[role] = role_repo\n\n    return aggregated_roles"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_role(path):\n    seems_legit = False\n    for folder in c.ANSIBLE_FOLDERS:\n        if os.path.exists(os.path.join(path, folder)):\n            seems_legit = True\n\n    return seems_legit", "response": "Determines if a path is an ansible role."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the stripped version of the arguments.", "response": "def stripped_args(args):\n    \"\"\"\n    Return the stripped version of the arguments.\n    \"\"\"\n    stripped_args = []\n    for arg in args:\n        stripped_args.append(arg.strip())\n\n    return stripped_args"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_role(role, config):\n    if role.startswith(config[\"scm_repo_prefix\"]):\n        role_name = role.replace(config[\"scm_repo_prefix\"], \"\")\n    else:\n        if \".\" in role:\n            galaxy_prefix = \"{0}.\".format(config[\"scm_user\"])\n            role_name = role.replace(galaxy_prefix, \"\")\n        elif \"-\" in role:\n            role_name = role.replace(\"-\", \"_\")\n        else:\n            role_name = role\n\n    return role_name", "response": "Normalizes a role name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a meta template.", "response": "def create_meta_main(create_path, config, role, categories):\n    \"\"\"\n    Create a meta template.\n    \"\"\"\n    meta_file = c.DEFAULT_META_FILE.replace(\n        \"%author_name\", config[\"author_name\"])\n    meta_file = meta_file.replace(\n        \"%author_company\", config[\"author_company\"])\n    meta_file = meta_file.replace(\"%license_type\", config[\"license_type\"])\n    meta_file = meta_file.replace(\"%role_name\", role)\n\n    # Normalize the category so %categories always gets replaced.\n    if not categories:\n        categories = \"\"\n\n    meta_file = meta_file.replace(\"%categories\", categories)\n\n    string_to_file(create_path, meta_file)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_version(path, default=\"master\"):\n    version = default\n    if os.path.exists(path):\n        version_contents = file_to_string(path)\n        if version_contents:\n            version = version_contents.strip()\n\n    return version", "response": "Get the version from a VERSION file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the config to a file.", "response": "def write_config(path, config):\n    \"\"\"\n    Write the config with a little post-converting formatting.\n    \"\"\"\n    config_as_string = to_nice_yaml(config)\n\n    config_as_string = \"---\\n\" + config_as_string\n\n    string_to_file(path, config_as_string)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef limit_roles(self):\n        new_roles = {}\n        roles = self.options.limit.split(\",\")\n\n        for key, value in self.roles.iteritems():\n            for role in roles:\n                role = role.strip()\n                if key == role:\n                    new_roles[key] = value\n\n        self.roles = new_roles", "response": "Limit the roles being scanned."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scan_roles(self):\n        for key, value in sorted(self.roles.iteritems()):\n            self.paths[\"role\"] = os.path.join(self.roles_path, key)\n            self.paths[\"meta\"] = os.path.join(self.paths[\"role\"], \"meta\",\n                                              \"main.yml\")\n            self.paths[\"readme\"] = os.path.join(self.paths[\"role\"],\n                                                \"README.{0}\"\n                                                .format(self.readme_format))\n            self.paths[\"defaults\"] = os.path.join(self.paths[\"role\"],\n                                                  \"defaults\", \"main.yml\")\n\n            self.report[\"roles\"][key] = self.report_role(key)\n\n            # we are writing a readme file which means the state of the role\n            # needs to be updated before it gets output by the ui\n            if self.gendoc:\n                if self.valid_meta(key):\n                    self.make_meta_dict_consistent()\n                    self.set_readme_template_vars(key, value)\n                    self.write_readme(key)\n            # only load the meta file when generating meta files\n            elif self.genmeta:\n                self.make_or_augment_meta(key)\n                if self.valid_meta(key):\n                    self.make_meta_dict_consistent()\n                    self.write_meta(key)\n            else:\n                self.update_scan_report(key)\n\n            if not self.config[\"options_quiet\"] and not self.export:\n                ui.role(key,\n                        self.report[\"roles\"][key],\n                        self.report[\"stats\"][\"longest_role_name_length\"])\n\n        self.tally_role_columns()\n\n        # below this point is only UI output, so we can return\n        if self.config[\"options_quiet\"] or self.export:\n            return\n\n        ui.totals(self.report[\"totals\"],\n                  len(self.report[\"roles\"].keys()),\n                  self.report[\"stats\"][\"longest_role_name_length\"])\n\n        if self.gendoc:\n            ui.gen_totals(self.report[\"state\"], \"readme\")\n        elif self.genmeta:\n            ui.gen_totals(self.report[\"state\"], \"meta\")\n        else:\n            ui.scan_totals(self.report[\"state\"])", "response": "Iterate over each role and report its stats."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexport the roles to one of the export types.", "response": "def export_roles(self):\n        \"\"\"\n        Export the roles to one of the export types.\n        \"\"\"\n        # prepare the report by removing unnecessary fields\n        del self.report[\"state\"]\n        del self.report[\"stats\"]\n        for role in self.report[\"roles\"]:\n            del self.report[\"roles\"][role][\"state\"]\n\n            defaults_path = os.path.join(self.roles_path, role,\n                                         \"defaults\", \"main.yml\")\n            if os.path.exists(defaults_path):\n                defaults = self.report[\"roles\"][role][\"defaults\"]\n                self.report[\"roles\"][role][\"defaults\"] = \\\n                    utils.yaml_load(\"\", defaults)\n\n        Export(self.roles_path, self.report, self.config, self.options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef report_role(self, role):\n        self.yaml_files = []\n\n        fields = {\n            \"state\": \"skipped\",\n            \"total_files\": self.gather_files(),\n            \"total_lines\": self.gather_lines(),\n            \"total_facts\": self.gather_facts(),\n            \"total_defaults\": self.gather_defaults(),\n            \"facts\": self.facts,\n            \"defaults\": self.defaults,\n            \"meta\": self.gather_meta(),\n            \"readme\": self.gather_readme(),\n            \"dependencies\": self.dependencies,\n            \"total_dependencies\": len(self.dependencies)\n        }\n\n        return fields", "response": "Return the fields gathered by the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gather_meta(self):\n        if not os.path.exists(self.paths[\"meta\"]):\n            return \"\"\n\n        meta_dict = utils.yaml_load(self.paths[\"meta\"])\n\n        # gather the dependencies\n        if meta_dict and \"dependencies\" in meta_dict:\n            # create a simple list of each role that is a dependency\n            dep_list = []\n\n            for dependency in meta_dict[\"dependencies\"]:\n                if type(dependency) is dict:\n                    dep_list.append(dependency[\"role\"])\n                else:\n                    dep_list.append(dependency)\n\n            # unique set of dependencies\n            meta_dict[\"dependencies\"] = list(set(dep_list))\n\n            self.dependencies = meta_dict[\"dependencies\"]\n        else:\n            self.dependencies = []\n\n        return utils.file_to_string(self.paths[\"meta\"])", "response": "Gather the meta file and set the dependencies"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the readme file.", "response": "def gather_readme(self):\n        \"\"\"\n        Return the readme file.\n        \"\"\"\n        if not os.path.exists(self.paths[\"readme\"]):\n            return \"\"\n\n        return utils.file_to_string(self.paths[\"readme\"])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngather the number of default variables.", "response": "def gather_defaults(self):\n        \"\"\"\n        Return the number of default variables.\n        \"\"\"\n        total_defaults = 0\n        defaults_lines = []\n\n        if not os.path.exists(self.paths[\"defaults\"]):\n            # reset the defaults if no defaults were found\n            self.defaults = \"\"\n            return 0\n\n        file = open(self.paths[\"defaults\"], \"r\")\n        for line in file:\n            if len(line) > 0:\n                first_char = line[0]\n            else:\n                first_char = \"\"\n\n            defaults_lines.append(line)\n\n            if (first_char != \"#\" and first_char != \"-\" and\n                    first_char != \" \" and\n                    first_char != \"\\r\" and\n                    first_char != \"\\n\" and\n                    first_char != \"\\t\"):\n                total_defaults += 1\n        file.close()\n\n        self.defaults = \"\".join(defaults_lines)\n\n        return total_defaults"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngathers the number of facts.", "response": "def gather_facts(self):\n        \"\"\"\n        Return the number of facts.\n        \"\"\"\n        facts = []\n\n        for file in self.yaml_files:\n            facts += self.gather_facts_list(file)\n\n        unique_facts = list(set(facts))\n        self.facts = unique_facts\n\n        return len(unique_facts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of facts.", "response": "def gather_facts_list(self, file):\n        \"\"\"\n        Return a list of facts.\n        \"\"\"\n        facts = []\n\n        contents = utils.file_to_string(os.path.join(self.paths[\"role\"],\n                                        file))\n        contents = re.sub(r\"\\s+\", \"\", contents)\n        matches = self.regex_facts.findall(contents)\n\n        for match in matches:\n            facts.append(match.split(\":\")[1])\n\n        return facts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gather_files(self):\n        self.all_files = utils.files_in_path(self.paths[\"role\"])\n\n        return len(self.all_files)", "response": "Return the number of files in the role"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngathering the number of lines in the file.", "response": "def gather_lines(self):\n        \"\"\"\n        Return the number of lines.\n        \"\"\"\n        total_lines = 0\n        for file in self.all_files:\n            full_path = os.path.join(self.paths[\"role\"], file)\n\n            with open(full_path, \"r\") as f:\n                for line in f:\n                    total_lines += 1\n\n            if full_path.endswith(\".yml\"):\n                self.yaml_files.append(full_path)\n\n        return total_lines"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsum up all of the stat columns.", "response": "def tally_role_columns(self):\n        \"\"\"\n        Sum up all of the stat columns.\n        \"\"\"\n        totals = self.report[\"totals\"]\n        roles = self.report[\"roles\"]\n\n        totals[\"dependencies\"] = sum(roles[item]\n                                     [\"total_dependencies\"] for item in roles)\n        totals[\"defaults\"] = sum(roles[item]\n                                 [\"total_defaults\"] for item in roles)\n        totals[\"facts\"] = sum(roles[item][\"total_facts\"] for item in roles)\n        totals[\"files\"] = sum(roles[item][\"total_files\"] for item in roles)\n        totals[\"lines\"] = sum(roles[item][\"total_lines\"] for item in roles)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns whether or not the meta file being read is valid.", "response": "def valid_meta(self, role):\n        \"\"\"\n        Return whether or not the meta file being read is valid.\n        \"\"\"\n        if os.path.exists(self.paths[\"meta\"]):\n            self.meta_dict = utils.yaml_load(self.paths[\"meta\"])\n        else:\n            self.report[\"state\"][\"missing_meta_role\"] += 1\n            self.report[\"roles\"][role][\"state\"] = \"missing_meta\"\n\n            return False\n\n        is_valid = True\n\n        # utils.yaml_load returns False when the file is invalid\n        if isinstance(self.meta_dict, bool):\n            is_valid = False\n            sys.exit(1)\n\n        return is_valid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_or_augment_meta(self, role):\n        if not os.path.exists(self.paths[\"meta\"]):\n            utils.create_meta_main(self.paths[\"meta\"], self.config, role, \"\")\n            self.report[\"state\"][\"ok_role\"] += 1\n            self.report[\"roles\"][role][\"state\"] = \"ok\"\n\n        # swap values in place to use the config values\n        swaps = [\n            (\"author\", self.config[\"author_name\"]),\n            (\"company\", self.config[\"author_company\"]),\n            (\"license\", self.config[\"license_type\"]),\n        ]\n\n        (new_meta, _) = utils.swap_yaml_string(self.paths[\"meta\"], swaps)\n\n        # normalize the --- at the top of the file by removing it first\n        new_meta = new_meta.replace(\"---\", \"\")\n        new_meta = new_meta.lstrip()\n\n        # augment missing main keys\n        augments = [\n            (\"ansigenome_info\", \"{}\"),\n            (\"galaxy_info\", \"{}\"),\n            (\"dependencies\", \"[]\"),\n        ]\n\n        new_meta = self.augment_main_keys(augments, new_meta)\n\n        # re-attach the ---\n        new_meta = \"---\\n\\n\" + new_meta\n\n        travis_path = os.path.join(self.paths[\"role\"], \".travis.yml\")\n        if os.path.exists(travis_path):\n            new_meta = new_meta.replace(\"travis: False\", \"travis: True\")\n\n        utils.string_to_file(self.paths[\"meta\"], new_meta)", "response": "Create or augment a meta file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the main keys to the file if they are missing.", "response": "def augment_main_keys(self, keys, file):\n        \"\"\"\n        Add the main key if it is missing.\n        \"\"\"\n        nfile = file\n        ansigenome_block = \"\"\"\nansigenome_info:\n  galaxy_id: ''\n\n  travis: False\n\n  synopsis: |\n    Describe your role in a few paragraphs....\n\n  usage: |\n    Describe how to use in more detail...\n\n  #custom: |\n  #  Any custom output you want after the usage section..\n\"\"\"\n\n        for key in keys:\n            if key[0] not in nfile:\n                if key[0] == \"ansigenome_info\":\n                    # make sure ansigenome_info is always on the bottom\n                    nfile = nfile + \"\\n{0}\".format(ansigenome_block)\n                else:\n                    nfile = \"\\n{0}: {1}\\n\\n\".format(key[0], key[1]) + nfile\n\n        return nfile"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_readme(self, role):\n        j2_out = self.readme_template.render(self.readme_template_vars)\n\n        self.update_gen_report(role, \"readme\", j2_out)", "response": "Write out a new readme file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites out a new meta file.", "response": "def write_meta(self, role):\n        \"\"\"\n        Write out a new meta file.\n        \"\"\"\n        meta_file = utils.file_to_string(self.paths[\"meta\"])\n\n        self.update_gen_report(role, \"meta\", meta_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the state and adjust the scan totals.", "response": "def update_scan_report(self, role):\n        \"\"\"\n        Update the role state and adjust the scan totals.\n        \"\"\"\n        state = self.report[\"state\"]\n\n        # ensure the missing meta state is colored up and the ok count is good\n        if self.gendoc:\n            if self.report[\"roles\"][role][\"state\"] == \"missing_meta\":\n                return\n\n        if os.path.exists(self.paths[\"readme\"]):\n            state[\"ok_role\"] += 1\n            self.report[\"roles\"][role][\"state\"] = \"ok\"\n        else:\n            state[\"missing_readme_role\"] += 1\n            self.report[\"roles\"][role][\"state\"] = \"missing_readme\""}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the gen report with the new entry.", "response": "def update_gen_report(self, role, file, original):\n        \"\"\"\n        Update the role state and adjust the gen totals.\n        \"\"\"\n        state = self.report[\"state\"]\n\n        if not os.path.exists(self.paths[file]):\n            state[\"ok_role\"] += 1\n            self.report[\"roles\"][role][\"state\"] = \"ok\"\n        elif (self.report[\"roles\"][role][file] != original and\n                self.report[\"roles\"][role][\"state\"] != \"ok\"):\n            state[\"changed_role\"] += 1\n            self.report[\"roles\"][role][\"state\"] = \"changed\"\n        elif self.report[\"roles\"][role][file] == original:\n            state[\"skipped_role\"] += 1\n            self.report[\"roles\"][role][\"state\"] = \"skipped\"\n            return\n\n        utils.string_to_file(self.paths[file], original)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_meta_dict_consistent(self):\n        if self.meta_dict is None:\n            self.meta_dict = {}\n\n        if \"galaxy_info\" not in self.meta_dict:\n            self.meta_dict[\"galaxy_info\"] = {}\n\n        if \"dependencies\" not in self.meta_dict:\n            self.meta_dict[\"dependencies\"] = []\n\n        if \"ansigenome_info\" not in self.meta_dict:\n            self.meta_dict[\"ansigenome_info\"] = {}", "response": "Make the meta_dict consistent with the ones in the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_readme_template_vars(self, role, repo_name):\n        # normalize and expose a bunch of fields to the template\n        authors = []\n\n        author = {\n            \"name\": self.config[\"author_name\"],\n            \"company\": self.config[\"author_company\"],\n            \"url\": self.config[\"author_url\"],\n            \"email\": self.config[\"author_email\"],\n            \"twitter\": self.config[\"author_twitter\"],\n        }\n        scm = {\n            \"host\": self.config[\"scm_host\"],\n            \"repo_prefix\": self.config[\"scm_repo_prefix\"],\n            \"type\": self.config[\"scm_type\"],\n            \"user\": self.config[\"scm_user\"],\n        }\n        license = {\n            \"type\": self.config[\"license_type\"],\n            \"url\": self.config[\"license_url\"],\n        }\n\n        role_name = utils.normalize_role(role, self.config)\n\n        normalized_role = {\n            \"name\": role_name,\n            \"galaxy_name\": \"{0}.{1}\".format(self.config[\"scm_user\"],\n                                            role_name),\n            \"slug\": \"{0}{1}\".format(self.config[\"scm_repo_prefix\"],\n                                    role_name),\n        }\n\n        if \"authors\" in self.meta_dict[\"ansigenome_info\"]:\n            authors = self.meta_dict[\"ansigenome_info\"][\"authors\"]\n        else:\n            authors = [author]\n\n            if \"github\" in self.config[\"scm_host\"]:\n                self.config[\"author_github\"] = \"{0}/{1}\".format(\n                    self.config[\"scm_host\"],\n                    self.config[\"scm_user\"])\n\n        self.readme_template_vars = {\n            \"authors\": authors,\n            \"scm\": scm,\n            \"role\": normalized_role,\n            \"license\": license,\n            \"galaxy_info\": self.meta_dict[\"galaxy_info\"],\n            \"dependencies\": self.meta_dict[\"dependencies\"],\n            \"ansigenome_info\": self.meta_dict[\"ansigenome_info\"]\n        }\n\n        # add the defaults and facts\n        r_defaults = self.report[\"roles\"][role][\"defaults\"]\n        self.readme_template_vars[\"ansigenome_info\"][\"defaults\"] = r_defaults\n\n        facts = \"\\n\".join(self.report[\"roles\"][role][\"facts\"])\n        self.readme_template_vars[\"ansigenome_info\"][\"facts\"] = facts", "response": "Set the readme template variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexits early if the path cannot be found.", "response": "def exit_if_path_exists(self):\n        \"\"\"\n        Exit early if the path cannot be found.\n        \"\"\"\n        if os.path.exists(self.output_path):\n            ui.error(c.MESSAGES[\"path_exists\"], self.output_path)\n            sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_skeleton(self):\n        utils.string_to_file(os.path.join(self.output_path, \"VERSION\"),\n                             \"master\\n\")\n\n        for folder in c.ANSIBLE_FOLDERS:\n            create_folder_path = os.path.join(self.output_path, folder)\n            utils.mkdir_p(create_folder_path)\n\n            mainyml_template = default_mainyml_template.replace(\n                \"%role_name\", self.role_name)\n            mainyml_template = mainyml_template.replace(\n                \"%values\", folder)\n\n            out_path = os.path.join(create_folder_path, \"main.yml\")\n\n            if folder not in (\"templates\", \"meta\", \"tests\", \"files\"):\n                utils.string_to_file(out_path, mainyml_template)\n\n            if folder == \"meta\":\n                utils.create_meta_main(out_path,\n                                       self.config, self.role_name,\n                                       self.options.galaxy_categories)", "response": "Create the role s directory and file structure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_travis_config(self):\n        test_runner = self.config[\"options_test_runner\"]\n\n        role_url = \"{0}\".format(os.path.join(self.config[\"scm_host\"],\n                                             self.config[\"scm_user\"],\n                                             self.config[\"scm_repo_prefix\"] +\n                                             self.normalized_role))\n\n        travisyml_template = default_travisyml_template.replace(\n            \"%test_runner\", test_runner)\n        travisyml_template = travisyml_template.replace(\n            \"%basename\", test_runner.split(\"/\")[-1])\n        travisyml_template = travisyml_template.replace(\n            \"%role_url\", role_url)\n\n        utils.string_to_file(os.path.join(self.output_path, \".travis.yml\"),\n                             travisyml_template)", "response": "Create a travis test setup."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npicking the correct default format.", "response": "def set_format(self, format):\n        \"\"\"\n        Pick the correct default format.\n        \"\"\"\n        if self.options.format:\n            self.format = self.options.format\n        else:\n            self.format = \\\n                self.config[\"default_format_\" + format]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating the format of the resource.", "response": "def validate_format(self, allowed_formats):\n        \"\"\"\n        Validate the allowed formats for a specific type.\n        \"\"\"\n        if self.format in allowed_formats:\n            return\n\n        ui.error(\"Export type '{0}' does not accept '{1}' format, only: \"\n                 \"{2}\".format(self.type, self.format, allowed_formats))\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef graph_dot(self):\n        default_graphviz_template = \"\"\"\ndigraph role_dependencies {\n        size=\"%size\"\n        dpi=%dpi\n        ratio=\"fill\"\n        landscape=false\n        rankdir=\"BT\";\n\n        node [shape = \"box\",\n              style = \"rounded,filled\",\n              fillcolor = \"lightgrey\",\n              fontsize = 20];\n\n        edge [style = \"dashed\",\n              dir = \"forward\",\n              penwidth = 1.5];\n\n%roles_list\n\n%dependencies\n}\n\"\"\"\n\n        roles_list = \"\"\n        edges = \"\"\n\n        # remove the darkest and brightest colors, still have 100+ colors\n        adjusted_colors = c.X11_COLORS[125:-325]\n        random.shuffle(adjusted_colors)\n        backup_colors = adjusted_colors[:]\n\n        for role, fields in sorted(self.report[\"roles\"].iteritems()):\n            name = utils.normalize_role(role, self.config)\n            color_length = len(adjusted_colors) - 1\n\n            # reset the colors if we run out\n            if color_length == 0:\n                adjusted_colors = backup_colors[:]\n                color_length = len(adjusted_colors) - 1\n\n            random_index = random.randint(1, color_length)\n            roles_list += \"        role_{0}              [label = \\\"{1}\\\"]\\n\" \\\n                          .format(re.sub(r'[.-/]', '_', name), name)\n\n            edge = '\\n        edge [color = \"{0}\"];\\n' \\\n                   .format(adjusted_colors[random_index])\n            del adjusted_colors[random_index]\n\n            if fields[\"dependencies\"]:\n                dependencies = \"\"\n\n                for dependency in sorted(fields[\"dependencies\"]):\n                    dependency_name = utils.role_name(dependency)\n                    dependencies += \"        role_{0} -> role_{1}\\n\".format(\n                        re.sub(r'[.-/]', '_', name),\n                        re.sub(r'[.-/]', '_',\n                               utils.normalize_role(dependency_name,\n                                                    self.config)\n                               )\n                    )\n\n                edges += \"{0}{1}\\n\".format(edge, dependencies)\n\n        graphviz_template = default_graphviz_template.replace(\"%roles_list\",\n                                                              roles_list)\n        graphviz_template = graphviz_template.replace(\"%dependencies\",\n                                                      edges)\n        graphviz_template = graphviz_template.replace(\"%size\",\n                                                      self.size)\n        graphviz_template = graphviz_template.replace(\"%dpi\",\n                                                      str(self.dpi))\n\n        if self.out_file:\n            utils.string_to_file(self.out_file, graphviz_template)\n        else:\n            print graphviz_template", "response": "Export the data in a dot format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef graph_png(self):\n        if not self.out_file:\n            ui.error(c.MESSAGES[\"png_missing_out\"])\n            sys.exit(1)\n\n        cli_flags = \"-Gsize='{0}' -Gdpi='{1}' {2} \".format(self.size, self.dpi,\n                                                           self.flags)\n        cli_flags += \"-o {0}\".format(self.out_file)\n\n        (out, err) = utils.capture_shell(\n            \"ansigenome export -t graph -f dot | dot -Tpng {0}\"\n            .format(cli_flags))\n\n        if err:\n            ui.error(err)", "response": "Export a graph of the data in png format using graphviz."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexits if the dot utility is missing.", "response": "def exit_if_missing_graphviz(self):\n        \"\"\"\n        Detect the presence of the dot utility to make a png graph.\n        \"\"\"\n        (out, err) = utils.capture_shell(\"which dot\")\n\n        if \"dot\" not in out:\n            ui.error(c.MESSAGES[\"dot_missing\"])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexporting a requirements file in txt format.", "response": "def reqs_txt(self):\n        \"\"\"\n        Export a requirements file in txt format.\n        \"\"\"\n        role_lines = \"\"\n\n        for role in sorted(self.report[\"roles\"]):\n            name = utils.normalize_role(role, self.config)\n            galaxy_name = \"{0}.{1}\".format(self.config[\"scm_user\"], name)\n\n            version_path = os.path.join(self.roles_path, role, \"VERSION\")\n            version = utils.get_version(version_path)\n            role_lines += \"{0},{1}\\n\".format(galaxy_name, version)\n\n        if self.out_file:\n            utils.string_to_file(self.out_file, role_lines)\n        else:\n            print role_lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport a requirements file in yml format.", "response": "def reqs_yml(self):\n        \"\"\"\n        Export a requirements file in yml format.\n        \"\"\"\n        default_yml_item = \"\"\"\n- src:     '%src'\n  name:    '%name'\n  scm:     '%scm'\n  version: '%version'\n\"\"\"\n\n        role_lines = \"---\\n\"\n        for role in sorted(self.report[\"roles\"]):\n            name = utils.normalize_role(role, self.config)\n            galaxy_name = \"{0}.{1}\".format(self.config[\"scm_user\"], name)\n            yml_item = default_yml_item\n\n            if self.config[\"scm_host\"]:\n                yml_item = yml_item.replace(\"%name\",\n                                            \"{0}\".format(galaxy_name))\n                if self.config[\"scm_repo_prefix\"]:\n                    role = self.config[\"scm_repo_prefix\"] + name\n\n                src = os.path.join(self.config[\"scm_host\"],\n                                   self.config[\"scm_user\"], role)\n            else:\n                src = galaxy_name\n                yml_item = yml_item.replace(\"  name: '%name'\\n\", \"\")\n                yml_item = yml_item.replace(\"  scm: '%scm'\\n\", \"\")\n\n            yml_item = yml_item.replace(\"%src\", src)\n\n            if self.config[\"scm_type\"]:\n                yml_item = yml_item.replace(\"%scm\", self.config[\"scm_type\"])\n            else:\n                yml_item = yml_item.replace(\"  scm: '%scm'\\n\", \"\")\n\n            version_path = os.path.join(self.roles_path, role, \"VERSION\")\n            version = utils.get_version(version_path)\n            yml_item = yml_item.replace(\"%version\", version)\n\n            role_lines += \"{0}\".format(yml_item)\n\n        if self.out_file:\n            utils.string_to_file(self.out_file, role_lines)\n        else:\n            print role_lines"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump(self):\n        report_as_json_string = utils.dict_to_json(self.report)\n        if self.out_file:\n            utils.string_to_file(self.out_file, report_as_json_string)\n        else:\n            print report_as_json_string", "response": "Dump the output to json."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fast_float(\n    x,\n    key=lambda x: x,\n    nan=None,\n    _uni=unicodedata.numeric,\n    _nan_inf=NAN_INF,\n    _first_char=POTENTIAL_FIRST_CHAR,\n):\n    \"\"\"\n    Convert a string to a float quickly, return input as-is if not possible.\n\n    We don't need to accept all input that the real fast_int accepts because\n    natsort is controlling what is passed to this function.\n\n    Parameters\n    ----------\n    x : str\n        String to attempt to convert to a float.\n    key : callable\n        Single-argument function to apply to *x* if conversion fails.\n    nan : object\n        Value to return instead of NaN if NaN would be returned.\n\n    Returns\n    -------\n    *str* or *float*\n\n    \"\"\"\n    if x[0] in _first_char or x.lstrip()[:3] in _nan_inf:\n        try:\n            x = float(x)\n            return nan if nan is not None and x != x else x\n        except ValueError:\n            try:\n                return _uni(x, key(x)) if len(x) == 1 else key(x)\n            except TypeError:  # pragma: no cover\n                return key(x)\n    else:\n        try:\n            return _uni(x, key(x)) if len(x) == 1 else key(x)\n        except TypeError:  # pragma: no cover\n            return key(x)", "response": "Convert a string to a float quickly return input as - is if not possible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a string to a int quickly return input as - is if not possible.", "response": "def fast_int(\n    x,\n    key=lambda x: x,\n    _uni=unicodedata.digit,\n    _first_char=POTENTIAL_FIRST_CHAR,\n):\n    \"\"\"\n    Convert a string to a int quickly, return input as-is if not possible.\n\n    We don't need to accept all input that the real fast_int accepts because\n    natsort is controlling what is passed to this function.\n\n    Parameters\n    ----------\n    x : str\n        String to attempt to convert to an int.\n    key : callable\n        Single-argument function to apply to *x* if conversion fails.\n\n    Returns\n    -------\n    *str* or *int*\n\n    \"\"\"\n    if x[0] in _first_char:\n        try:\n            return long(x)\n        except ValueError:\n            try:\n                return _uni(x, key(x)) if len(x) == 1 else key(x)\n            except TypeError:  # pragma: no cover\n                return key(x)\n    else:\n        try:\n            return _uni(x, key(x)) if len(x) == 1 else key(x)\n        except TypeError:  # pragma: no cover\n            return key(x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a natural sort on entries given on the command-line. Arguments are read from sys.argv.", "response": "def main(*arguments):\n    \"\"\"\n    Performs a natural sort on entries given on the command-line.\n\n    Arguments are read from sys.argv.\n    \"\"\"\n\n    from argparse import ArgumentParser, RawDescriptionHelpFormatter\n    from textwrap import dedent\n\n    parser = ArgumentParser(\n        description=dedent(main.__doc__), formatter_class=RawDescriptionHelpFormatter\n    )\n    parser.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=\"%(prog)s {}\".format(natsort.__version__),\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--paths\",\n        default=False,\n        action=\"store_true\",\n        help=\"Interpret the input as file paths.  This is not \"\n        \"strictly necessary to sort all file paths, but in cases \"\n        'where there are OS-generated file paths like \"Folder/\" '\n        'and \"Folder (1)/\", this option is needed to make the '\n        'paths sorted in the order you expect (\"Folder/\" before '\n        '\"Folder (1)/\").',\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--filter\",\n        nargs=2,\n        type=float,\n        metavar=(\"LOW\", \"HIGH\"),\n        action=\"append\",\n        help=\"Used for keeping only the entries that have a number \"\n        \"falling in the given range.\",\n    )\n    parser.add_argument(\n        \"-F\",\n        \"--reverse-filter\",\n        nargs=2,\n        type=float,\n        metavar=(\"LOW\", \"HIGH\"),\n        action=\"append\",\n        dest=\"reverse_filter\",\n        help=\"Used for excluding the entries that have a number \"\n        \"falling in the given range.\",\n    )\n    parser.add_argument(\n        \"-e\",\n        \"--exclude\",\n        type=float,\n        action=\"append\",\n        help=\"Used to exclude an entry that contains a specific number.\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--reverse\",\n        action=\"store_true\",\n        default=False,\n        help=\"Returns in reversed order.\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--number-type\",\n        \"--number_type\",\n        dest=\"number_type\",\n        choices=(\"int\", \"float\", \"real\", \"f\", \"i\", \"r\"),\n        default=\"int\",\n        help='Choose the type of number to search for. \"float\" will search '\n        'for floating-point numbers.  \"int\" will only search for '\n        'integers. \"real\" is a shortcut for \"float\" with --sign. '\n        '\"i\" is a synonym for \"int\", \"f\" is a synonym for '\n        '\"float\", and \"r\" is a synonym for \"real\".'\n        \"The default is %(default)s.\",\n    )\n    parser.add_argument(\n        \"--nosign\",\n        default=False,\n        action=\"store_false\",\n        dest=\"signed\",\n        help='Do not consider \"+\" or \"-\" as part of a number, i.e. do not '\n        \"take sign into consideration. This is the default.\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--sign\",\n        default=False,\n        action=\"store_true\",\n        dest=\"signed\",\n        help='Consider \"+\" or \"-\" as part of a number, i.e. '\n        \"take sign into consideration. The default is unsigned.\",\n    )\n    parser.add_argument(\n        \"--noexp\",\n        default=True,\n        action=\"store_false\",\n        dest=\"exp\",\n        help=\"Do not consider an exponential as part of a number, i.e. 1e4, \"\n        'would be considered as 1, \"e\", and 4, not as 10000.  This only '\n        \"effects the --number-type=float.\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--locale\",\n        action=\"store_true\",\n        default=False,\n        help=\"Causes natsort to use locale-aware sorting. You will get the \"\n        \"best results if you install PyICU.\",\n    )\n    parser.add_argument(\n        \"entries\",\n        nargs=\"*\",\n        default=sys.stdin,\n        help=\"The entries to sort. Taken from stdin if nothing is given on \"\n        \"the command line.\",\n    )\n    args = parser.parse_args(arguments or None)\n\n    # Make sure the filter range is given properly. Does nothing if no filter\n    args.filter = check_filters(args.filter)\n    args.reverse_filter = check_filters(args.reverse_filter)\n\n    # Remove trailing whitespace from all the entries\n    entries = [e.strip() for e in args.entries]\n\n    # Sort by directory then by file within directory and print.\n    sort_and_print_entries(entries, args)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_filters(filters):\n    if not filters:\n        return None\n    try:\n        return [range_check(f[0], f[1]) for f in filters]\n    except ValueError as err:\n        raise ValueError(\"Error in --filter: \" + py23_str(err))", "response": "Execute range_check for every element of an iterable."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if an entry falls into a desired range.", "response": "def keep_entry_range(entry, lows, highs, converter, regex):\n    \"\"\"\n    Check if an entry falls into a desired range.\n\n    Every number in the entry will be extracted using *regex*,\n    if any are within a given low to high range the entry will\n    be kept.\n\n    Parameters\n    ----------\n    entry : str\n    lows : iterable\n        Collection of low values against which to compare the entry.\n    highs : iterable\n        Collection of high values against which to compare the entry.\n    converter : callable\n        Function to convert a string to a number.\n    regex : regex object\n        Regular expression to locate numbers in a string.\n\n    Returns\n    -------\n    True if the entry should be kept, False otherwise.\n\n    \"\"\"\n    return any(\n        low <= converter(num) <= high\n        for num in regex.findall(entry)\n        for low, high in zip(lows, highs)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef keep_entry_value(entry, values, converter, regex):\n    return not any(converter(num) in values for num in regex.findall(entry))", "response": "Checks if an entry does not match a given value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sort_and_print_entries(entries, args):\n\n    # Extract the proper number type.\n    is_float = args.number_type in (\"float\", \"real\", \"f\", \"r\")\n    signed = args.signed or args.number_type in (\"real\", \"r\")\n    alg = (\n        natsort.ns.FLOAT * is_float\n        | natsort.ns.SIGNED * signed\n        | natsort.ns.NOEXP * (not args.exp)\n        | natsort.ns.PATH * args.paths\n        | natsort.ns.LOCALE * args.locale\n    )\n\n    # Pre-remove entries that don't pass the filtering criteria\n    # Make sure we use the same searching algorithm for filtering\n    # as for sorting.\n    do_filter = args.filter is not None or args.reverse_filter is not None\n    if do_filter or args.exclude:\n        inp_options = (\n            natsort.ns.FLOAT * is_float\n            | natsort.ns.SIGNED * signed\n            | natsort.ns.NOEXP * (not args.exp)\n        )\n        regex = regex_chooser(inp_options)\n        if args.filter is not None:\n            lows, highs = ([f[0] for f in args.filter], [f[1] for f in args.filter])\n            entries = [\n                entry\n                for entry in entries\n                if keep_entry_range(entry, lows, highs, float, regex)\n            ]\n        if args.reverse_filter is not None:\n            lows, highs = (\n                [f[0] for f in args.reverse_filter],\n                [f[1] for f in args.reverse_filter],\n            )\n            entries = [\n                entry\n                for entry in entries\n                if not keep_entry_range(entry, lows, highs, float, regex)\n            ]\n        if args.exclude:\n            exclude = set(args.exclude)\n            entries = [\n                entry\n                for entry in entries\n                if keep_entry_value(entry, exclude, float, regex)\n            ]\n\n    # Print off the sorted results\n    for entry in natsort.natsorted(entries, reverse=args.reverse, alg=alg):\n        print(entry)", "response": "Sort the entries and print them to stdout."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef regex_chooser(alg):\n    if alg & ns.FLOAT:\n        alg &= ns.FLOAT | ns.SIGNED | ns.NOEXP\n    else:\n        alg &= ns.INT | ns.SIGNED\n\n    return {\n        ns.INT: NumericalRegularExpressions.int_nosign(),\n        ns.FLOAT: NumericalRegularExpressions.float_nosign_exp(),\n        ns.INT | ns.SIGNED: NumericalRegularExpressions.int_sign(),\n        ns.FLOAT | ns.SIGNED: NumericalRegularExpressions.float_sign_exp(),\n        ns.FLOAT | ns.NOEXP: NumericalRegularExpressions.float_nosign_noexp(),\n        ns.FLOAT | ns.SIGNED | ns.NOEXP: NumericalRegularExpressions.float_sign_noexp(),\n    }[alg]", "response": "Selects an appropriate regex for the type of number of interest."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _normalize_input_factory(alg):\n    normalization_form = \"NFKD\" if alg & ns.COMPATIBILITYNORMALIZE else \"NFD\"\n    wrapped = partial(normalize, normalization_form)\n    if NEWPY:\n        return wrapped\n    else:\n        return lambda x, _f=wrapped: _f(x) if isinstance(x, py23_str) else x", "response": "Returns a function that will normalize unicode input data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef natsort_key(val, key, string_func, bytes_func, num_func):\n\n    # Apply key if needed\n    if key is not None:\n        val = key(val)\n\n    # Assume the input are strings, which is the most common case\n    try:\n        return string_func(val)\n    except (TypeError, AttributeError):\n\n        # If bytes type, use the bytes_func\n        if type(val) in (bytes,):\n            return bytes_func(val)\n\n        # Otherwise, assume it is an iterable that must be parsed recursively.\n        # Do not apply the key recursively.\n        try:\n            return tuple(\n                natsort_key(x, None, string_func, bytes_func, num_func) for x in val\n            )\n\n        # If that failed, it must be a number.\n        except TypeError:\n            return num_func(val)", "response": "This function sorts the string and numbers in a tree structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a function that will format a *bytes* object into a tuple.", "response": "def parse_bytes_factory(alg):\n    \"\"\"\n    Create a function that will format a *bytes* object into a tuple.\n\n    Parameters\n    ----------\n    alg : ns enum\n        Indicate how to format the *bytes*.\n\n    Returns\n    -------\n    func : callable\n        A function that accepts *bytes* input and returns a tuple\n        with the formatted *bytes*. Intended to be used as the\n        *bytes_func* argument to *natsort_key*.\n\n    See Also\n    --------\n    natsort_key\n\n    \"\"\"\n    # We don't worry about ns.UNGROUPLETTERS | ns.LOCALEALPHA because\n    # bytes cannot be compared to strings.\n    if alg & ns.PATH and alg & ns.IGNORECASE:\n        return lambda x: ((x.lower(),),)\n    elif alg & ns.PATH:\n        return lambda x: ((x,),)\n    elif alg & ns.IGNORECASE:\n        return lambda x: (x.lower(),)\n    else:\n        return lambda x: (x,)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a function that will format a number into a tuple.", "response": "def parse_number_factory(alg, sep, pre_sep):\n    \"\"\"\n    Create a function that will format a number into a tuple.\n\n    Parameters\n    ----------\n    alg : ns enum\n        Indicate how to format the *bytes*.\n    sep : str\n        The string character to be inserted before the number\n        in the returned tuple.\n    pre_sep : str\n        In the event that *alg* contains ``UNGROUPLETTERS``, this\n        string will be placed in a single-element tuple at the front\n        of the returned nested tuple.\n\n    Returns\n    -------\n    func : callable\n        A function that accepts numeric input (e.g. *int* or *float*)\n        and returns a tuple containing the number with the leading string\n        *sep*. Intended to be used as the *num_func* argument to\n        *natsort_key*.\n\n    See Also\n    --------\n    natsort_key\n\n    \"\"\"\n    nan_replace = float(\"+inf\") if alg & ns.NANLAST else float(\"-inf\")\n\n    def func(val, _nan_replace=nan_replace, _sep=sep):\n        \"\"\"Given a number, place it in a tuple with a leading null string.\"\"\"\n        return _sep, _nan_replace if val != val else val\n\n    # Return the function, possibly wrapping in tuple if PATH is selected.\n    if alg & ns.PATH and alg & ns.UNGROUPLETTERS and alg & ns.LOCALEALPHA:\n        return lambda x: (((pre_sep,), func(x)),)\n    elif alg & ns.UNGROUPLETTERS and alg & ns.LOCALEALPHA:\n        return lambda x: ((pre_sep,), func(x))\n    elif alg & ns.PATH:\n        return lambda x: (func(x),)\n    else:\n        return func"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_string_factory(\n    alg, sep, splitter, input_transform, component_transform, final_transform\n):\n    \"\"\"\n    Create a function that will split and format a *str* into a tuple.\n\n    Parameters\n    ----------\n    alg : ns enum\n        Indicate how to format and split the *str*.\n    sep : str\n        The string character to be inserted between adjacent numeric\n        objects in the returned tuple.\n    splitter : callable\n        A function the will accept a string and returns an iterable\n        of strings where the numbers are separated from the non-numbers.\n    input_transform : callable\n        A function to apply to the string input *before* applying\n        the *splitter* function. Must return a string.\n    component_transform : callable\n        A function that is operated elementwise on the output of\n        *splitter*. It must accept a single string and return either\n        a string or a number.\n    final_transform : callable\n        A function to operate on the return value as a whole. It\n        must accept a tuple and a string argument - the tuple\n        should be the result of applying the above functions, and the\n        string is the original input value. It must return a tuple.\n\n    Returns\n    -------\n    func : callable\n        A function that accepts string input and returns a tuple\n        containing the string split into numeric and non-numeric\n        components, where the numeric components are converted into\n        numeric objects. The first element is *always* a string,\n        and then alternates number then string. Intended to be\n        used as the *string_func* argument to *natsort_key*.\n\n    See Also\n    --------\n    natsort_key\n    input_string_transform_factory\n    string_component_transform_factory\n    final_data_transform_factory\n\n    \"\"\"\n    # Sometimes we store the \"original\" input before transformation,\n    # sometimes after.\n    orig_after_xfrm = not (alg & NS_DUMB and alg & ns.LOCALEALPHA)\n    original_func = input_transform if orig_after_xfrm else _no_op\n    normalize_input = _normalize_input_factory(alg)\n\n    def func(x):\n        # Apply string input transformation function and return to x.\n        # Original function is usually a no-op, but some algorithms require it\n        # to also be the transformation function.\n        x = normalize_input(x)\n        x, original = input_transform(x), original_func(x)\n        x = splitter(x)  # Split string into components.\n        x = py23_filter(None, x)  # Remove empty strings.\n        x = py23_map(component_transform, x)  # Apply transform on components.\n        x = sep_inserter(x, sep)  # Insert '' between numbers.\n        return final_transform(x, original)  # Apply the final transform.\n\n    return func", "response": "Create a function that will split and format a string into a tuple of numeric and non - numeric components."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sep_inserter(iterable, sep):\n    try:\n        # Get the first element. A StopIteration indicates an empty iterable.\n        # Since we are controlling the types of the input, 'type' is used\n        # instead of 'isinstance' for the small speed advantage it offers.\n        types = (int, float, long)\n        first = next(iterable)\n        if type(first) in types:\n            yield sep\n        yield first\n\n        # Now, check if pair of elements are both numbers. If so, add ''.\n        second = next(iterable)\n        if type(first) in types and type(second) in types:\n            yield sep\n        yield second\n\n        # Now repeat in a loop.\n        for x in iterable:\n            first, second = second, x\n            if type(first) in types and type(second) in types:\n                yield sep\n            yield second\n    except StopIteration:\n        # Catch StopIteration per deprecation in PEP 479:\n        # \"Change StopIteration handling inside generators\"\n        return", "response": "A generator that yields the values of the iterable in order with the given separator."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a function to transform a string into a tree structure.", "response": "def input_string_transform_factory(alg):\n    \"\"\"\n    Create a function to transform a string.\n\n    Parameters\n    ----------\n    alg : ns enum\n        Indicate how to format the *str*.\n\n    Returns\n    -------\n    func : callable\n        A function to be used as the *input_transform* argument to\n        *parse_string_factory*.\n\n    See Also\n    --------\n    parse_string_factory\n\n    \"\"\"\n    # Shortcuts.\n    lowfirst = alg & ns.LOWERCASEFIRST\n    dumb = alg & NS_DUMB\n\n    # Build the chain of functions to execute in order.\n    function_chain = []\n    if (dumb and not lowfirst) or (lowfirst and not dumb):\n        function_chain.append(methodcaller(\"swapcase\"))\n\n    if alg & ns.IGNORECASE:\n        if NEWPY:\n            function_chain.append(methodcaller(\"casefold\"))\n        else:\n            function_chain.append(methodcaller(\"lower\"))\n\n    if alg & ns.LOCALENUM:\n        # Create a regular expression that will remove thousands separators.\n        strip_thousands = r\"\"\"\n            (?<=[0-9]{{1}})  # At least 1 number\n            (?<![0-9]{{4}})  # No more than 3 numbers\n            {nodecimal}      # Cannot follow decimal\n            {thou}           # The thousands separator\n            (?=[0-9]{{3}}    # Three numbers must follow\n             ([^0-9]|$)      # But a non-number after that\n            )\n        \"\"\"\n        nodecimal = r\"\"\n        if alg & ns.FLOAT:\n            # Make a regular expression component that will ensure no\n            # separators are removed after a decimal point.\n            d = get_decimal_point()\n            d = r\"\\.\" if d == r\".\" else d\n            nodecimal += r\"(?<!\" + d + r\"[0-9])\"\n            nodecimal += r\"(?<!\" + d + r\"[0-9]{2})\"\n            nodecimal += r\"(?<!\" + d + r\"[0-9]{3})\"\n        strip_thousands = strip_thousands.format(\n            thou=get_thousands_sep(), nodecimal=nodecimal\n        )\n        strip_thousands = re.compile(strip_thousands, flags=re.VERBOSE)\n        function_chain.append(partial(strip_thousands.sub, \"\"))\n\n        # Create a regular expression that will change the decimal point to\n        # a period if not already a period.\n        decimal = get_decimal_point()\n        if alg & ns.FLOAT and decimal != \".\":\n            switch_decimal = r\"(?<=[0-9]){decimal}|{decimal}(?=[0-9])\"\n            switch_decimal = switch_decimal.format(decimal=decimal)\n            switch_decimal = re.compile(switch_decimal)\n            function_chain.append(partial(switch_decimal.sub, \".\"))\n\n    # Return the chained functions.\n    return chain_functions(function_chain)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a function to either transform a string or convert to a number.", "response": "def string_component_transform_factory(alg):\n    \"\"\"\n    Create a function to either transform a string or convert to a number.\n\n    Parameters\n    ----------\n    alg : ns enum\n        Indicate how to format the *str*.\n\n    Returns\n    -------\n    func : callable\n        A function to be used as the *component_transform* argument to\n        *parse_string_factory*.\n\n    See Also\n    --------\n    parse_string_factory\n\n    \"\"\"\n    # Shortcuts.\n    use_locale = alg & ns.LOCALEALPHA\n    dumb = alg & NS_DUMB\n    group_letters = (alg & ns.GROUPLETTERS) or (use_locale and dumb)\n    nan_val = float(\"+inf\") if alg & ns.NANLAST else float(\"-inf\")\n\n    # Build the chain of functions to execute in order.\n    func_chain = []\n    if group_letters:\n        func_chain.append(groupletters)\n    if use_locale:\n        func_chain.append(get_strxfrm())\n    kwargs = {\"key\": chain_functions(func_chain)} if func_chain else {}\n\n    # Return the correct chained functions.\n    if alg & ns.FLOAT:\n        # noinspection PyTypeChecker\n        kwargs[\"nan\"] = nan_val\n        return partial(fast_float, **kwargs)\n    else:\n        return partial(fast_int, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a function to transform a tuple.", "response": "def final_data_transform_factory(alg, sep, pre_sep):\n    \"\"\"\n    Create a function to transform a tuple.\n\n    Parameters\n    ----------\n    alg : ns enum\n        Indicate how to format the *str*.\n    sep : str\n        Separator that was passed to *parse_string_factory*.\n    pre_sep : str\n        String separator to insert at the at the front\n        of the return tuple in the case that the first element\n        is *sep*.\n\n    Returns\n    -------\n    func : callable\n        A function to be used as the *final_transform* argument to\n        *parse_string_factory*.\n\n    See Also\n    --------\n    parse_string_factory\n\n    \"\"\"\n    if alg & ns.UNGROUPLETTERS and alg & ns.LOCALEALPHA:\n        swap = alg & NS_DUMB and alg & ns.LOWERCASEFIRST\n        transform = methodcaller(\"swapcase\") if swap else _no_op\n\n        def func(split_val, val, _transform=transform, _sep=sep, _pre_sep=pre_sep):\n            \"\"\"\n            Return a tuple with the first character of the first element\n            of the return value as the first element, and the return value\n            as the second element. This will be used to perform gross sorting\n            by the first letter.\n            \"\"\"\n            split_val = tuple(split_val)\n            if not split_val:\n                return (), ()\n            elif split_val[0] == _sep:\n                return (_pre_sep,), split_val\n            else:\n                return (_transform(val[0]),), split_val\n\n        return func\n    else:\n        return lambda split_val, val: tuple(split_val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string containing all the characters in x making doubled letters lowercase.", "response": "def groupletters(x, _low=lower_function):\n    \"\"\"\n    Double all characters, making doubled letters lowercase.\n\n    Parameters\n    ----------\n    x : str\n\n    Returns\n    -------\n    str\n\n    Examples\n    --------\n\n        >>> groupletters(\"Apple\")\n        {u}'aAppppllee'\n\n    \"\"\"\n    return \"\".join(ichain.from_iterable((_low(y), y) for y in x))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchains a list of single - argument functions together and return.", "response": "def chain_functions(functions):\n    \"\"\"\n    Chain a list of single-argument functions together and return.\n\n    The functions are applied in list order, and the output of the\n    previous functions is passed to the next function.\n\n    Parameters\n    ----------\n    functions : list\n        A list of single-argument functions to chain together.\n\n    Returns\n    -------\n    func : callable\n        A single argument function.\n\n    Examples\n    --------\n    Chain several functions together!\n\n        >>> funcs = [lambda x: x * 4, len, lambda x: x + 5]\n        >>> func = chain_functions(funcs)\n        >>> func('hey')\n        17\n\n    \"\"\"\n    functions = list(functions)\n    if not functions:\n        return _no_op\n    elif len(functions) == 1:\n        return functions[0]\n    else:\n        # See https://stackoverflow.com/a/39123400/1399279\n        return partial(reduce, lambda res, f: f(res), functions)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit a string into its path components and extensions.", "response": "def path_splitter(s, _d_match=re.compile(r\"\\.\\d\").match):\n    \"\"\"\n    Split a string into its path components.\n\n    Assumes a string is a path or is path-like.\n\n    Parameters\n    ----------\n    s : str | pathlib.Path\n\n    Returns\n    -------\n    split : tuple\n        The path split by directory components and extensions.\n\n    Examples\n    --------\n\n        >>> tuple(path_splitter(\"this/thing.ext\"))\n        ({u}'this', {u}'thing', {u}'.ext')\n\n    \"\"\"\n    if has_pathlib and isinstance(s, PurePath):\n        s = py23_str(s)\n    path_parts = deque()\n    p_appendleft = path_parts.appendleft\n    # Continue splitting the path from the back until we have reached\n    # '..' or '.', or until there is nothing left to split.\n    path_location = s\n    while path_location != os_curdir and path_location != os_pardir:\n        parent_path = path_location\n        path_location, child_path = path_split(parent_path)\n        if path_location == parent_path:\n            break\n        p_appendleft(child_path)\n\n    # This last append is the base path.\n    # Only append if the string is non-empty.\n    # Make sure the proper path separator for this OS is used\n    # no matter what was actually given.\n    if path_location:\n        p_appendleft(py23_str(os_sep))\n\n    # Now, split off the file extensions using a similar method to above.\n    # Continue splitting off file extensions until we reach a decimal number\n    # or there are no more extensions.\n    # We are not using built-in functionality of PathLib here because of\n    # the recursive splitting up to a decimal.\n    base = path_parts.pop()\n    base_parts = deque()\n    b_appendleft = base_parts.appendleft\n    while True:\n        front = base\n        base, ext = path_splitext(front)\n        if _d_match(ext) or not ext:\n            # Reset base to before the split if the split is invalid.\n            base = front\n            break\n        b_appendleft(ext)\n    b_appendleft(base)\n\n    # Return the split parent paths and then the split basename.\n    return ichain(path_parts, base_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a format string construct the regex with class attributes.", "response": "def _construct_regex(cls, fmt):\n        \"\"\"Given a format string, construct the regex with class attributes.\"\"\"\n        return re.compile(fmt.format(**vars(cls)), flags=re.U)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef natsort_keygen(key=None, alg=ns.DEFAULT):\n    try:\n        ns.DEFAULT | alg\n    except TypeError:\n        msg = \"natsort_keygen: 'alg' argument must be from the enum 'ns'\"\n        raise ValueError(msg + \", got {}\".format(py23_str(alg)))\n\n    # Add the NS_DUMB option if the locale library is broken.\n    if alg & ns.LOCALEALPHA and natsort.compat.locale.dumb_sort():\n        alg |= NS_DUMB\n\n    # Set some variables that will be passed to the factory functions\n    if alg & ns.NUMAFTER:\n        if alg & ns.LOCALEALPHA:\n            sep = natsort.compat.locale.null_string_locale_max\n        else:\n            sep = natsort.compat.locale.null_string_max\n        pre_sep = natsort.compat.locale.null_string_max\n    else:\n        if alg & ns.LOCALEALPHA:\n            sep = natsort.compat.locale.null_string_locale\n        else:\n            sep = natsort.compat.locale.null_string\n        pre_sep = natsort.compat.locale.null_string\n    regex = utils.regex_chooser(alg)\n\n    # Create the functions that will be used to split strings.\n    input_transform = utils.input_string_transform_factory(alg)\n    component_transform = utils.string_component_transform_factory(alg)\n    final_transform = utils.final_data_transform_factory(alg, sep, pre_sep)\n\n    # Create the high-level parsing functions for strings, bytes, and numbers.\n    string_func = utils.parse_string_factory(\n        alg, sep, regex.split, input_transform, component_transform, final_transform\n    )\n    if alg & ns.PATH:\n        string_func = utils.parse_path_factory(string_func)\n    bytes_func = utils.parse_bytes_factory(alg)\n    num_func = utils.parse_number_factory(alg, sep, pre_sep)\n\n    # Return the natsort key with the parsing path pre-chosen.\n    return partial(\n        utils.natsort_key,\n        key=key,\n        string_func=string_func,\n        bytes_func=bytes_func,\n        num_func=num_func,\n    )", "response": "Generates a key that sorts strings and numbers naturally."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef natsorted(seq, key=None, reverse=False, alg=ns.DEFAULT):\n    key = natsort_keygen(key, alg)\n    return sorted(seq, reverse=reverse, key=key)", "response": "Returns a list of natural sort order of the elements in the input iterable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef humansorted(seq, key=None, reverse=False, alg=ns.DEFAULT):\n    return natsorted(seq, key, reverse, alg | ns.LOCALE)", "response": "Returns a list of non - numeric characters in a random order."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of the sorted elements in the order of the elements in the input.", "response": "def realsorted(seq, key=None, reverse=False, alg=ns.DEFAULT):\n    \"\"\"\n    Convenience function to properly sort signed floats.\n\n    A signed float in a string could be \"a-5.7\". This is a wrapper around\n    ``natsorted(seq, alg=ns.REAL)``.\n\n    The behavior of :func:`realsorted` for `natsort` version >= 4.0.0\n    was the default behavior of :func:`natsorted` for `natsort`\n    version < 4.0.0.\n\n    Parameters\n    ----------\n    seq : iterable\n        The input to sort.\n\n    key : callable, optional\n        A key used to determine how to sort each element of the sequence.\n        It is **not** applied recursively.\n        It should accept a single argument and return a single value.\n\n    reverse : {{True, False}}, optional\n        Return the list in reversed sorted order. The default is\n        `False`.\n\n    alg : ns enum, optional\n        This option is used to control which algorithm `natsort`\n        uses when sorting. For details into these options, please see\n        the :class:`ns` class documentation. The default is `ns.REAL`.\n\n    Returns\n    -------\n    out : list\n        The sorted input.\n\n    See Also\n    --------\n    index_realsorted : Returns the sorted indexes from `realsorted`.\n\n    Examples\n    --------\n    Use `realsorted` just like the builtin `sorted`::\n\n        >>> a = ['num5.10', 'num-3', 'num5.3', 'num2']\n        >>> natsorted(a)\n        [{u}'num2', {u}'num5.3', {u}'num5.10', {u}'num-3']\n        >>> realsorted(a)\n        [{u}'num-3', {u}'num2', {u}'num5.10', {u}'num5.3']\n\n    \"\"\"\n    return natsorted(seq, key, reverse, alg | ns.REAL)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of the indexes used to sort a sequence naturally.", "response": "def index_natsorted(seq, key=None, reverse=False, alg=ns.DEFAULT):\n    \"\"\"\n    Determine the list of the indexes used to sort the input sequence.\n\n    Sorts a sequence naturally, but returns a list of sorted the\n    indexes and not the sorted list itself. This list of indexes\n    can be used to sort multiple lists by the sorted order of the\n    given sequence.\n\n    Parameters\n    ----------\n    seq : iterable\n        The input to sort.\n\n    key : callable, optional\n        A key used to determine how to sort each element of the sequence.\n        It is **not** applied recursively.\n        It should accept a single argument and return a single value.\n\n    reverse : {{True, False}}, optional\n        Return the list in reversed sorted order. The default is\n        `False`.\n\n    alg : ns enum, optional\n        This option is used to control which algorithm `natsort`\n        uses when sorting. For details into these options, please see\n        the :class:`ns` class documentation. The default is `ns.INT`.\n\n    Returns\n    -------\n    out : tuple\n        The ordered indexes of the input.\n\n    See Also\n    --------\n    natsorted\n    order_by_index\n\n    Examples\n    --------\n\n    Use index_natsorted if you want to sort multiple lists by the\n    sorted order of one list::\n\n        >>> a = ['num3', 'num5', 'num2']\n        >>> b = ['foo', 'bar', 'baz']\n        >>> index = index_natsorted(a)\n        >>> index\n        [2, 0, 1]\n        >>> # Sort both lists by the sort order of a\n        >>> order_by_index(a, index)\n        [{u}'num2', {u}'num3', {u}'num5']\n        >>> order_by_index(b, index)\n        [{u}'baz', {u}'foo', {u}'bar']\n\n    \"\"\"\n    if key is None:\n        newkey = itemgetter(1)\n    else:\n\n        def newkey(x):\n            return key(itemgetter(1)(x))\n\n    # Pair the index and sequence together, then sort by element\n    index_seq_pair = [[x, y] for x, y in enumerate(seq)]\n    index_seq_pair.sort(reverse=reverse, key=natsort_keygen(newkey, alg))\n    return [x for x, _ in index_seq_pair]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\norder a given sequence by an index sequence. The output of `index_natsorted` is a sequence of integers (index) that correspond to how its input sequence **would** be sorted. The idea is that this index can be used to reorder multiple sequences by the sorted order of the first sequence. This function is a convenient wrapper to apply this ordering to a sequence. Parameters ---------- seq : sequence The sequence to order. index : iterable The iterable that indicates how to order `seq`. It should be the same length as `seq` and consist of integers only. iter : {{True, False}}, optional If `True`, the ordered sequence is returned as a iterator; otherwise it is returned as a list. The default is `False`. Returns ------- out : {{list, iterator}} The sequence ordered by `index`, as a `list` or as an iterator (depending on the value of `iter`). See Also -------- index_natsorted index_humansorted index_realsorted Examples -------- `order_by_index` is a convenience function that helps you apply the result of `index_natsorted`:: >>> a = ['num3', 'num5', 'num2'] >>> b = ['foo', 'bar', 'baz'] >>> index = index_natsorted(a) >>> index [2, 0, 1] >>> # Sort both lists by the sort order of a >>> order_by_index(a, index) [{u}'num2', {u}'num3', {u}'num5'] >>> order_by_index(b, index) [{u}'baz', {u}'foo', {u}'bar']", "response": "def order_by_index(seq, index, iter=False):\n    \"\"\"\n    Order a given sequence by an index sequence.\n\n    The output of `index_natsorted` is a\n    sequence of integers (index) that correspond to how its input\n    sequence **would** be sorted. The idea is that this index can\n    be used to reorder multiple sequences by the sorted order of the\n    first sequence. This function is a convenient wrapper to\n    apply this ordering to a sequence.\n\n    Parameters\n    ----------\n    seq : sequence\n        The sequence to order.\n\n    index : iterable\n        The iterable that indicates how to order `seq`.\n        It should be the same length as `seq` and consist\n        of integers only.\n\n    iter : {{True, False}}, optional\n        If `True`, the ordered sequence is returned as a\n        iterator; otherwise it is returned as a\n        list. The default is `False`.\n\n    Returns\n    -------\n    out : {{list, iterator}}\n        The sequence ordered by `index`, as a `list` or as an\n        iterator (depending on the value of `iter`).\n\n    See Also\n    --------\n    index_natsorted\n    index_humansorted\n    index_realsorted\n\n    Examples\n    --------\n\n    `order_by_index` is a convenience function that helps you apply\n    the result of `index_natsorted`::\n\n        >>> a = ['num3', 'num5', 'num2']\n        >>> b = ['foo', 'bar', 'baz']\n        >>> index = index_natsorted(a)\n        >>> index\n        [2, 0, 1]\n        >>> # Sort both lists by the sort order of a\n        >>> order_by_index(a, index)\n        [{u}'num2', {u}'num3', {u}'num5']\n        >>> order_by_index(b, index)\n        [{u}'baz', {u}'foo', {u}'bar']\n\n    \"\"\"\n    return (seq[i] for i in index) if iter else [seq[i] for i in index]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disassemble(co, lasti=-1):\n    # Taken from dis.disassemble, returns disassembled code instead of printing\n    # it (the fuck python ?).\n    # Also, unicodified.\n    # Also, use % operator instead of string operations.\n    # Also, one statement per line.\n    out = StringIO()\n    code = co.co_code\n    labels = dis.findlabels(code)\n    linestarts = dict(dis.findlinestarts(co))\n    n = len(code)\n    i = 0\n    extended_arg = 0\n    free = None\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i in linestarts:\n            if i > 0:\n                print(end=u'\\n', file=out)\n            print(u'%3d' % linestarts[i], end=u' ', file=out)\n        else:\n            print(u'   ', end=u' ', file=out)\n\n        if i == lasti:\n            print(u'-->', end=u' ', file=out)\n        else:\n            print(u'   ', end=u' ', file=out)\n        if i in labels:\n            print(u'>>', end=u' ', file=out)\n        else:\n            print(u'  ', end=u' ', file=out)\n        print(u'%4i' % i, end=u' ', file=out)\n        print(u'%-20s' % dis.opname[op], end=u' ', file=out)\n        i = i + 1\n        if op >= dis.HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i + 1]) * 256 + extended_arg\n            extended_arg = 0\n            i = i + 2\n            if op == dis.EXTENDED_ARG:\n                extended_arg = oparg * 65536\n            print(u'%5i' % oparg, end=u' ', file=out)\n            if op in dis.hasconst:\n                print(u'(%r)' % co.co_consts[oparg], end=u' ', file=out)\n            elif op in dis.hasname:\n                print(u'(%s)' % co.co_names[oparg], end=u' ', file=out)\n            elif op in dis.hasjrel:\n                print(u'(to %r)' % (i + oparg), end=u' ', file=out)\n            elif op in dis.haslocal:\n                print(u'(%s)' % co.co_varnames[oparg], end=u' ', file=out)\n            elif op in dis.hascompare:\n                print(u'(%s)' % dis.cmp_op[oparg], end=u' ', file=out)\n            elif op in dis.hasfree:\n                if free is None:\n                    free = co.co_cellvars + co.co_freevars\n                print(u'(%s)' % free[oparg], end=u' ', file=out)\n        print(end=u'\\n', file=out)\n    return out.getvalue()", "response": "Disassemble a code object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _iterOutFiles(self):\n        out = StringIO()\n        self.callgrind(out, relative_path=True)\n        yield (\n            'cachegrind.out.pprofile',\n            out.getvalue(),\n            'application/x-kcachegrind',\n        )\n        for name, lines in self.iterSource():\n            lines = ''.join(lines)\n            if lines:\n                if isinstance(lines, unicode):\n                    lines = lines.encode('utf-8')\n                yield (\n                    os.path.normpath(\n                        os.path.splitdrive(name)[1]\n                    ).lstrip(_ALLSEP),\n                    lines,\n                    'text/x-python',\n                )\n        sql_name_template = 'query_%%0%ii-%%i_hits_%%6fs.sql' % len(\n            str(len(self.sql_dict)),\n        )\n        for index, (query, time_list) in enumerate(\n            sorted(\n                self.sql_dict.iteritems(),\n                key=lambda x: (sum(x[1]), len(x[1])),\n                reverse=True,\n            ),\n        ):\n            yield (\n                sql_name_template % (\n                    index,\n                    len(time_list),\n                    sum(time_list),\n                ),\n                b'\\n'.join(b'-- %10.6fs' % x for x in time_list) + b'\\n' + query,\n                'application/sql',\n            )\n        if self.zodb_dict:\n            yield (\n                'ZODB_setstate.txt',\n                '\\n\\n'.join(\n                    (\n                        '%s (%fs)\\n' % (\n                            db_name,\n                            sum(sum(x) for x in oid_dict.itervalues()),\n                        )\n                    ) + '\\n'.join(\n                        '%s (%i): %s' % (\n                            oid.encode('hex'),\n                            len(time_list),\n                            ', '.join('%fs' % x for x in time_list),\n                        )\n                        for oid, time_list in oid_dict.iteritems()\n                    )\n                    for db_name, oid_dict in self.zodb_dict.iteritems()\n                ),\n                'text/plain',\n            )\n        if self.traverse_dict:\n            yield (\n                'unrestrictedTraverse_pathlist.txt',\n                tabulate(\n                    ('self', 'path', 'hit', 'total duration'),\n                    sorted(\n                        (\n                            (context, path, len(duration_list), sum(duration_list))\n                            for (context, path), duration_list in self.traverse_dict.iteritems()\n                        ),\n                        key=lambda x: x[3],\n                        reverse=True,\n                    ),\n                ),\n                'text/plain',\n            )", "response": "Yields path data mimetype for each file involved on or produced by cachegrind."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef asMIMEString(self):\n        result = MIMEMultipart()\n        base_type_dict = {\n            'application': MIMEApplication,\n            'text': MIMEText,\n        }\n        encoder_dict = {\n            'application/x-kcachegrind': encode_quopri,\n            'text/x-python': 'utf-8',\n            'text/plain': 'utf-8',\n        }\n        for path, data, mimetype in self._iterOutFiles():\n            base_type, sub_type = mimetype.split('/')\n            chunk = base_type_dict[base_type](\n                data,\n                sub_type,\n                encoder_dict.get(mimetype),\n            )\n            chunk.add_header(\n                'Content-Disposition',\n                'attachment',\n                filename=path,\n            )\n            result.attach(chunk)\n        return result.as_string(), result['content-type']", "response": "Return a mime - string representation of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the cache entry as a zip archive.", "response": "def asZip(self):\n        \"\"\"\n        Return a serialised zip archive containing:\n        - callgrind profiling statistics (cachegrind.out.pprofile)\n        - any SQL query issued via ZMySQLDA (query_*.sql)\n        - any persistent object load via ZODB.Connection (ZODB_setstate.txt)\n        - any path argument given to unrestrictedTraverse\n          (unrestrictedTraverse_pathlist.txt)\n        - all involved python code, including Python Scripts without hierarchy\n          (the rest)\n        \"\"\"\n        out = BytesIO()\n        with zipfile.ZipFile(\n            out,\n            mode='w',\n            compression=zipfile.ZIP_DEFLATED,\n        ) as outfile:\n            for path, data, _ in self._iterOutFiles():\n                outfile.writestr(path, data)\n        return out.getvalue(), 'application/zip'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef runfile(fd, argv, fd_name='<unknown>', compile_flags=0, dont_inherit=1,\n        filename=None, threads=True, verbose=False):\n    \"\"\"\n    Run code from given file descriptor with profiling enabled.\n    Closes fd before executing contained code.\n    \"\"\"\n    _run(threads, verbose, 'runfile', filename, fd, argv, fd_name,\n        compile_flags, dont_inherit)", "response": "Runs code from given file descriptor with profiling enabled."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef runpath(path, argv, filename=None, threads=True, verbose=False):\n    _run(threads, verbose, 'runpath', filename, path, argv)", "response": "Run code from a file path with profiling enabled."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _relpath(name):\n    return os.path.normpath(os.path.splitdrive(name)[1]).lstrip(_allsep)", "response": "Return relative path from name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprofiles a line of text into a single cell.", "response": "def pprofile(line, cell=None):\n    \"\"\"\n    Profile line execution.\n    \"\"\"\n    if cell is None:\n        # TODO: detect and use arguments (statistical profiling, ...) ?\n        return run(line)\n    return _main(\n        ['%%pprofile', '-m', '-'] + shlex.split(line),\n        io.StringIO(cell),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hit(self, code, line, duration):\n        entry = self.line_dict[line][code]\n        entry[0] += 1\n        entry[1] += duration", "response": "A line has finished executing."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call(self, code, line, callee_file_timing, callee, duration, frame):\n        try:\n            entry = self.call_dict[(code, line, callee)]\n        except KeyError:\n            self.call_dict[(code, line, callee)] = [callee_file_timing, 1, duration]\n        else:\n            entry[1] += 1\n            entry[2] += duration", "response": "A method to update the entry in the call_dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getFilenameSet(self):\n        result = set(self._mergeFileTiming())\n        # Ignore profiling code. __file__ does not always provide consistent\n        # results with f_code.co_filename (ex: easy_install with zipped egg),\n        # so inspect current frame instead.\n        # Get current file from one of pprofile methods. Compatible with\n        # implementations that do not have the inspect.currentframe() method\n        # (e.g. IronPython).\n        # XXX: Assumes that all of pprofile code is in a single file.\n        # XXX: Assumes that _initStack exists in pprofile module.\n        result.discard(inspect.getsourcefile(_initStack))\n        return result", "response": "Returns a set of profiled file names."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndumps statistics for a given set of source files.", "response": "def callgrind(self, out, filename=None, commandline=None, relative_path=False):\n        \"\"\"\n        Dump statistics in callgrind format.\n        Contains:\n        - per-line hit count, time and time-per-hit\n        - call associations (call tree)\n          Note: hit count is not inclusive, in that it is not the sum of all\n          hits inside that call.\n        Time unit: microsecond (1e-6 second).\n        out (file-ish opened for writing)\n            Destination of callgrind profiling data.\n        filename (str, collection of str)\n            If provided, dump stats for given source file(s) only.\n            By default, list for all known files.\n        commandline (anything with __str__)\n            If provided, will be output as the command line used to generate\n            this profiling data.\n        relative_path (bool)\n            When True, absolute elements are stripped from path. Useful when\n            maintaining several copies of source trees with their own\n            profiling result, so kcachegrind does not look in system-wide\n            files which may not match with profiled code.\n        \"\"\"\n        print(u'# callgrind format', file=out)\n        print(u'version: 1', file=out)\n        print(u'creator: pprofile', file=out)\n        print(u'event: usphit :microseconds/hit', file=out)\n        print(u'events: hits microseconds usphit', file=out)\n        if commandline is not None:\n            print(u'cmd:', commandline, file=out)\n        file_dict = self._mergeFileTiming()\n        if relative_path:\n            convertPath = _relpath\n        else:\n            convertPath = lambda x: x\n        if os.path.sep != \"/\":\n            # qCacheGrind (windows build) needs at least one UNIX separator\n            # in path to find the file. Adapt here even if this is probably\n            # more of a qCacheGrind issue...\n            convertPath = lambda x, cascade=convertPath: cascade(\n                '/'.join(x.split(os.path.sep))\n            )\n        code_to_name_dict = {}\n        homonym_counter = {}\n        def getCodeName(filename, code):\n            # Tracks code objects globally, because callee information needs\n            # to be consistent accross files.\n            # Inside a file, grants unique names to each code object.\n            try:\n                return code_to_name_dict[code]\n            except KeyError:\n                name = code.co_name + ':%i' % code.co_firstlineno\n                key = (filename, name)\n                homonym_count = homonym_counter.get(key, 0)\n                if homonym_count:\n                    name += '_%i' % homonym_count\n                homonym_counter[key] = homonym_count + 1\n                code_to_name_dict[code] = name\n                return name\n        for current_file in self._getFileNameList(filename, may_sort=False):\n            file_timing = file_dict[current_file]\n            print(u'fl=%s' % convertPath(current_file), file=out)\n            # When a local callable is created an immediately executed, this\n            # loop would start a new \"fn=\" section but would not end it before\n            # emitting \"cfn=\" lines, making the callee appear as not being\n            # called by interrupted \"fn=\" section.\n            # So dispatch all functions in a first pass, and build\n            # uninterrupted sections in a second pass.\n            # Note: cost line is a list just to be mutable. A single item is\n            # expected.\n            func_dict = defaultdict(lambda: defaultdict(lambda: ([], [])))\n            for lineno, code, hits, duration in file_timing.iterHits():\n                func_dict[getCodeName(current_file, code)][lineno][0].append(\n                    (hits, int(duration * 1000000)),\n                )\n            for (\n                lineno,\n                caller,\n                call_hits, call_duration,\n                callee_file, callee,\n            ) in file_timing.iterCalls():\n                call_ticks = int(call_duration * 1000000)\n                func_call_list = func_dict[\n                    getCodeName(current_file, caller)\n                ][lineno][1]\n                append = func_call_list.append\n                append(u'cfl=' + convertPath(callee_file))\n                append(u'cfn=' + getCodeName(callee_file, callee))\n                append(u'calls=%i %i' % (call_hits, callee.co_firstlineno))\n                append(u'%i %i %i %i' % (lineno, call_hits, call_ticks, call_ticks // call_hits))\n            for func_name, line_dict in func_dict.iteritems():\n                print(u'fn=%s' % func_name, file=out)\n                for lineno, (func_hit_list, func_call_list) in sorted(line_dict.iteritems()):\n                    if func_hit_list:\n                        # Multiple function objects may \"reside\" on the same\n                        # line of the same file (same global dict).\n                        # Sum these up and produce a single cachegrind event.\n                        hits = sum(x for x, _ in func_hit_list)\n                        ticks = sum(x for _, x in func_hit_list)\n                        print(\n                            u'%i %i %i %i' % (\n                                lineno,\n                                hits,\n                                ticks,\n                                ticks // hits,\n                            ),\n                            file=out,\n                        )\n                    for line in func_call_list:\n                        print(line, file=out)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndumping annotated source code with profiling statistics to out.", "response": "def annotate(self, out, filename=None, commandline=None, relative_path=False):\n        \"\"\"\n        Dump annotated source code with current profiling statistics to \"out\"\n        file.\n        Time unit: second.\n        out (file-ish opened for writing)\n            Destination of annotated sources.\n        filename (str, collection of str)\n            If provided, dump stats for given source file(s) only.\n            If unordered collection, it will get sorted by decreasing total\n            file score (total time if available, then total hit count).\n            By default, list for all known files.\n        commandline (anything with __str__)\n            If provided, will be output as the command line used to generate\n            this annotation.\n        relative_path (bool)\n            For compatibility with callgrind. Ignored.\n        \"\"\"\n        file_dict = self._mergeFileTiming()\n        total_time = self.total_time\n        if commandline is not None:\n            print(u'Command line:', commandline, file=out)\n        print(u'Total duration: %gs' % total_time, file=out)\n        if not total_time:\n            return\n        def percent(value, scale):\n            if scale == 0:\n                return 0\n            return value * 100 / scale\n        for name in self._getFileNameList(filename):\n            file_timing = file_dict[name]\n            file_total_time = file_timing.getTotalTime()\n            call_list_by_line = file_timing.getCallListByLine()\n            print(u'File: %s' % name, file=out)\n            print(u'File duration: %gs (%.2f%%)' % (file_total_time,\n                percent(file_total_time, total_time)), file=out)\n            print(_ANNOTATE_HEADER, file=out)\n            print(_ANNOTATE_HORIZONTAL_LINE, file=out)\n            last_line = file_timing.getLastLine()\n            for lineno, line in LineIterator(\n                self._getline,\n                file_timing.filename,\n                file_timing.global_dict,\n            ):\n                if not line and lineno > last_line:\n                    break\n                hits, duration = file_timing.getHitStatsFor(lineno)\n                print(_ANNOTATE_FORMAT % {\n                    u'lineno': lineno,\n                    u'hits': hits,\n                    u'time': duration,\n                    u'time_per_hit': duration / hits if hits else 0,\n                    u'percent': percent(duration, total_time),\n                    u'line': (line or u'').rstrip(),\n                }, file=out)\n                for (\n                    _,\n                    call_hits, call_duration,\n                    callee_file, callee,\n                ) in call_list_by_line.get(lineno, ()):\n                    print(_ANNOTATE_CALL_FORMAT % {\n                        u'hits': call_hits,\n                        u'time': call_duration,\n                        u'time_per_hit': call_duration / call_hits,\n                        u'percent': percent(call_duration, total_time),\n                        u'callee_file': callee_file,\n                        u'callee_line': callee.co_firstlineno,\n                        u'callee_name': callee.co_name,\n                    }, file=out)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump_stats(self, filename):\n        if _isCallgrindName(filename):\n            with open(filename, 'w') as out:\n                self.callgrind(out)\n        else:\n            with io.open(filename, 'w', errors='replace') as out:\n                self.annotate(out)", "response": "Dump the current state of the user to a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef runctx(self, cmd, globals, locals):\n        with self():\n            exec(cmd, globals, locals)\n        return self", "response": "Similar to profile. Profile. runctx but with the same arguments as self."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _disable(self):\n        self.total_time += time() - self.enabled_start\n        self.enabled_start = None\n        del self.stack", "response": "Disables the current log entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, cmd):\n        import __main__\n        dikt = __main__.__dict__\n        return self.runctx(cmd, dikt, dikt)", "response": "Similar to profile. Profile. run but returns a dictionary of the result."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stop(self):\n        if self.is_alive():\n            self._can_run = False\n            self._stop_event.set()\n            self._profiler.total_time += time() - self._start_time\n            self._start_time = None", "response": "Request thread to stop.\n        Does not wait for actual termination (use join() method)."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_expanded_field_serializer(\n        self, name, nested_expand, nested_fields, nested_omit\n    ):\n        \"\"\"\n        Returns an instance of the dynamically created nested serializer.\n        \"\"\"\n        field_options = self.expandable_fields[name]\n        serializer_class = field_options[0]\n        serializer_settings = copy.deepcopy(field_options[1])\n\n        if name in nested_expand:\n            serializer_settings[\"expand\"] = nested_expand[name]\n\n        if name in nested_fields:\n            serializer_settings[\"fields\"] = nested_fields[name]\n\n        if name in nested_omit:\n            serializer_settings[\"omit\"] = nested_omit[name]\n\n        if serializer_settings.get(\"source\") == name:\n            del serializer_settings[\"source\"]\n\n        if type(serializer_class) == str:\n            serializer_class = self._import_serializer_class(serializer_class)\n\n        return serializer_class(**serializer_settings)", "response": "Creates an expanded field serializer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _import_serializer_class(self, location):\n        pieces = location.split(\".\")\n        class_name = pieces.pop()\n\n        if pieces[len(pieces) - 1] != \"serializers\":\n            pieces.append(\"serializers\")\n\n        module = importlib.import_module(\".\".join(pieces))\n        return getattr(module, class_name)", "response": "Imports a dotted - notation string to serializer class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _clean_fields(self, omit_fields, sparse_fields, next_level_omits):\n        sparse = len(sparse_fields) > 0\n        to_remove = []\n\n        if not sparse and len(omit_fields) == 0:\n            return\n\n        for field_name in self.fields:\n            is_present = self._should_field_exist(\n                field_name, omit_fields, sparse_fields, next_level_omits\n            )\n\n            if not is_present:\n                to_remove.append(field_name)\n\n        for remove_field in to_remove:\n            self.fields.pop(remove_field)", "response": "Remove fields that are not found in omit list and are not present in sparse list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the given field exists in the given order.", "response": "def _should_field_exist(\n        self, field_name, omit_fields, sparse_fields, next_level_omits\n    ):\n        \"\"\"\n            Next level omits take form of:\n            {\n                'this_level_field': [field_to_omit_at_next_level]\n            }\n            We don't want to prematurely omit a field, eg \"omit=house.rooms.kitchen\"\n            should not omit the entire house or all the rooms, just the kitchen.\n        \"\"\"\n        if field_name in omit_fields and field_name not in next_level_omits:\n            return False\n\n        if len(sparse_fields) > 0 and field_name not in sparse_fields:\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the current request object can be accessed.", "response": "def _can_access_request(self):\n        \"\"\"\n        Can access current request object if all are true\n        - The serializer is the root.\n        - A request context was passed in.\n        - The request method is GET.\n        \"\"\"\n        if self.parent:\n            return False\n\n        if not hasattr(self, \"context\") or not self.context.get(\"request\", None):\n            return False\n\n        return self.context[\"request\"].method == \"GET\""}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_expand_input(self, passed_settings):\n        value = passed_settings.get(\"expand\")\n\n        if len(value) > 0:\n            return value\n\n        if not self._can_access_request:\n            return []\n\n        expand = self._parse_request_list_value(\"expand\")\n\n        if \"permitted_expands\" in self.context:\n            permitted_expands = self.context[\"permitted_expands\"]\n\n            if \"~all\" in expand or \"*\" in expand:\n                return permitted_expands\n            else:\n                return list(set(expand) & set(permitted_expands))\n\n        return expand", "response": "Returns the expand value from the passed settings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn boolean of whether the passed key is expanded.", "response": "def is_expanded(request, key):\n    \"\"\" Examines request object to return boolean of whether\n        passed field is expanded.\n    \"\"\"\n    expand = request.query_params.get(\"expand\", \"\")\n    expand_fields = []\n\n    for e in expand.split(\",\"):\n        expand_fields.extend([e for e in e.split(\".\")])\n\n    return \"~all\" in expand_fields or key in expand_fields"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split_levels(fields):\n    first_level_fields = []\n    next_level_fields = {}\n\n    if not fields:\n        return first_level_fields, next_level_fields\n\n    if not isinstance(fields, list):\n        fields = [a.strip() for a in fields.split(\",\") if a.strip()]\n    for e in fields:\n        if \".\" in e:\n            first_level, next_level = e.split(\".\", 1)\n            first_level_fields.append(first_level)\n            next_level_fields.setdefault(first_level, []).append(next_level)\n        else:\n            first_level_fields.append(e)\n\n    first_level_fields = list(set(first_level_fields))\n    return first_level_fields, next_level_fields", "response": "Split a list of names into a list of first - level fields and next - level fields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all objects from iterable that have at least one of the keys", "response": "def filter_dict(iterable, keys):\n  \"\"\"\n\tfilters keys of each element of iterable\n\t$.(a,b) returns all objects from array that have at least one of the keys:\n\t[1,\"aa\",{\"a\":2,\"c\":3},{\"c\":3},{\"a\":1,\"b\":2}].(a,b) -> [{\"a\":2},{\"a\":1,\"b\":2}]\n\t\"\"\"\n  if type(keys) is not list:\n    keys = [keys]\n  for i in iterable:\n    try:\n      d = {}\n      for a in keys:\n        try:\n          d[a] = i[a]\n        except KeyError:\n          pass\n      if d != {}:\n        yield d\n    except Exception:\n      pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a datetime object from a list of datetime objects", "response": "def dateTime(arg):\n  \"\"\"\n\td may be:\n\t\t- datetime()\n\t\t- [y,m,d,h[,m[,ms]]]\n\t\t- [date(),time()]\n\t\t- [[y,m,d],[h,m,s,ms]]\n\t\tand permutations of above\n\t\"\"\"\n  l = len(arg)\n  if l is 1:\n    dt = arg[0]\n    typed = type(dt)\n    if typed is datetime.datetime:\n      return dt\n    if typed in (tuple, list) and len(dt) in [5, 6, 7]:\n      return datetime.datetime(*dt)\n  if l is 2:\n    date = time = None\n    typeArg0 = type(arg[0])\n    typeArg1 = type(arg[1])\n    if typeArg0 in STR_TYPES:\n      return datetime.datetime.strptime(arg[0], arg[1])\n    if typeArg0 is datetime.date:\n      d = arg[0]\n      date = [d.year, d.month, d.day]\n    if typeArg0 in (tuple, list):\n      date = arg[0]\n    if typeArg1 is datetime.time:\n      t = arg[1]\n      time = [t.hour, t.minute, t.second, t.microsecond]\n    if typeArg1 in (tuple, list):\n      time = arg[1]\n    return datetime.datetime(*date + time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef color_replace(image, color):\n    pixels = image.load()\n    size = image.size[0]\n    for width in range(size):\n        for height in range(size):\n            r, g, b, a = pixels[width, height]\n            if (r, g, b, a) == (0,0,0,255):\n                pixels[width,height] = color\n            else:\n                pixels[width,height] = (r,g,b,color[3])", "response": "Replace black with other color"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nproduces QR code from image.", "response": "def produce(txt,img,ver=5,err_crt = qrcode.constants.ERROR_CORRECT_H,bri = 1.0, cont = 1.0,\\\n        colourful = False, rgba = (0,0,0,255),pixelate = False):\n    \"\"\"Produce QR code\n\n    :txt: QR text\n    :img: Image path / Image object\n    :ver: QR version\n    :err_crt: QR error correct\n    :bri: Brightness enhance\n    :cont: Contrast enhance\n    :colourful: If colourful mode\n    :rgba: color to replace black\n    :pixelate: pixelate\n    :returns: list of produced image\n\n    \"\"\"\n    if type(img) is Image.Image:\n        pass\n    elif type(img) is str:\n        img = Image.open(img)\n    else:\n        return []\n    frames = [produce_impl(txt,frame.copy(),ver,err_crt,bri,cont,colourful,rgba,pixelate) for frame in ImageSequence.Iterator(img)]\n    return frames"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef produce_impl(txt,img,ver=5,err_crt = qrcode.constants.ERROR_CORRECT_H,bri = 1.0, cont = 1.0,\\\n        colourful = False, rgba = (0,0,0,255),pixelate = False):\n    \"\"\"Produce QR code\n\n    :txt: QR text\n    :img: Image object\n    :ver: QR version\n    :err_crt: QR error correct\n    :bri: Brightness enhance\n    :cont: Contrast enhance\n    :colourful: If colourful mode\n    :rgba: color to replace black\n    :pixelate: pixelate\n    :returns: Produced image\n\n    \"\"\"\n    qr = qrcode.QRCode(version = ver,error_correction = err_crt,box_size=3)\n    qr.add_data(txt)\n    qr.make(fit=True)\n    img_qr = qr.make_image().convert('RGBA')\n    if colourful and ( rgba != (0,0,0,255) ):\n        color_replace(img_qr,rgba)\n    img_img = img.convert('RGBA')\n\n    img_img_size = None\n    img_size = img_qr.size[0] - 24\n    if img_img.size[0] < img_img.size[1]:\n        img_img_size = img_img.size[0]\n    else:\n        img_img_size = img_img.size[1]\n\n    img_enh = img_img.crop((0,0,img_img_size,img_img_size))\n    enh = ImageEnhance.Contrast(img_enh)\n    img_enh = enh.enhance(cont)\n    enh = ImageEnhance.Brightness(img_enh)\n    img_enh = enh.enhance(bri)\n    if not colourful:\n        if pixelate:\n            img_enh = img_enh.convert('1').convert('RGBA')\n        else:\n            img_enh = img_enh.convert('L').convert('RGBA')\n    img_frame = img_qr\n    img_enh = img_enh.resize((img_size*10,img_size*10))\n    img_enh_l = img_enh.convert(\"L\").resize((img_size,img_size))\n    img_frame_l = img_frame.convert(\"L\")\n\n    for x in range(0,img_size):\n        for y in range(0,img_size):\n            if x < 24 and (y < 24 or y > img_size-25):\n                continue\n            if x > img_size-25 and (y < 24 ):\n                continue\n            if (x%3 ==1 and  y%3 == 1):\n                if (img_frame_l.getpixel((x+12,y+12)) > 70 and img_enh_l.getpixel((x,y)) < 185)\\\n                        or (img_frame_l.getpixel((x+12,y+12)) < 185 and img_enh_l.getpixel((x,y)) > 70) :\n                    continue\n            img_frame.putpixel((x+12,y+12),(0,0,0,0))\n    pos = qrcode.util.pattern_position(qr.version)\n    img_qr2 = qr.make_image().convert(\"RGBA\")\n    if colourful and ( rgba != (0,0,0,0) ):\n        color_replace(img_qr2,rgba)\n    for i in pos:\n        for j in pos:\n            if (i == 6 and j == pos[-1]) or (j == 6 and i == pos[-1])\\\n                or (i == 6 and j == 6):\n                continue\n            else:\n                rect = (3*(i-2)+12,3*(j-2)+12,3*(i+3)+12,3*(j+3)+12)\n                img_tmp = img_qr2.crop(rect)\n                img_frame.paste(img_tmp,rect)\n\n    img_res = Image.new(\"RGBA\",(img_frame.size[0]*10,img_frame.size[1]*10),(255,255,255,255))\n    img_res.paste(img_enh,(120,120),img_enh)\n    img_frame = img_frame.resize((img_frame.size[0]*10,img_frame.size[1]*10))\n    img_res.paste(img_frame,(0,0),img_frame)\n    img_res = img_res.convert('RGB')\n    if pixelate:\n        return img_res.resize(img_qr.size).resize((img_img_size,img_img_size))\n    return img_res", "response": "Produce an image object from a QR code and a text object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps the argument in a numpy. ndarray.", "response": "def wrap_in_ndarray(value):\n    \"\"\"Wraps the argument in a numpy.ndarray.\n\n       If value is a scalar, it is converted in a list first.\n       If value is array-like, the shape is conserved.\n\n    \"\"\"\n\n    if hasattr(value, \"__len__\"):\n        return np.array(value)\n    else:\n        return np.array([value])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the finite difference coefficients for given derivative order and accuracy order.", "response": "def coefficients(deriv, acc):\n    \"\"\"\n    Calculates the finite difference coefficients for given derivative order and accuracy order.\n    Assumes that the underlying grid is uniform. This function is available at the `findiff`\n    package level.\n    \n    :param deriv: int > 0: The derivative order.\n          \n    :param acc:  even int > 0: The accuracy order. \n          \n    :return: dict with the finite difference coefficients and corresponding offsets \n    \"\"\"\n\n    if acc % 2 == 1:\n        acc += 1\n\n    ret = {}\n\n    # Determine central coefficients\n\n    num_central = 2 * math.floor((deriv + 1) / 2) - 1 + acc\n    num_side = num_central // 2\n\n    ret[\"center\"] = _calc_coef(num_side, num_side, deriv)\n\n    # Determine forward coefficients\n\n    if deriv % 2 == 0:\n        num_coef = num_central + 1\n    else:\n        num_coef = num_central\n\n    ret[\"forward\"] = _calc_coef(0, num_coef - 1, deriv)\n\n    # Determine backward coefficients\n\n    ret[\"backward\"] = _calc_coef(num_coef - 1, 0, deriv)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the finite difference coefficients for a given derivative order and accuracy order.", "response": "def coefficients_non_uni(deriv, acc, coords, idx):\n    \"\"\"\n    Calculates the finite difference coefficients for given derivative order and accuracy order.\n    Assumes that the underlying grid is non-uniform.\n\n    :param deriv: int > 0: The derivative order.\n\n    :param acc:  even int > 0: The accuracy order.\n     \n    :param coords:  1D numpy.ndarray: the coordinates of the axis for the partial derivative\n    \n    :param idx:  int: index of the grid position where to calculate the coefficients\n\n    :return: dict with the finite difference coefficients and corresponding offsets \n    \"\"\"\n\n    if acc % 2 == 1:\n        acc += 1\n\n    num_central = 2 * math.floor((deriv + 1) / 2) - 1 + acc\n    num_side = num_central // 2\n\n    if deriv % 2 == 0:\n        num_coef = num_central + 1\n    else:\n        num_coef = num_central\n\n    if idx < num_side:\n        matrix = _build_matrix_non_uniform(0, num_coef - 1, coords, idx)\n\n        rhs = _build_rhs(0, num_coef - 1, deriv)\n\n        ret = {\n            \"coefficients\": np.linalg.solve(matrix, rhs),\n            \"offsets\": np.array([p for p in range(num_coef)])\n        }\n\n    elif idx >= len(coords) - num_side:\n        matrix = _build_matrix_non_uniform(num_coef - 1, 0, coords, idx)\n\n        rhs = _build_rhs(num_coef - 1, 0, deriv)\n\n        ret = {\n            \"coefficients\": np.linalg.solve(matrix, rhs),\n            \"offsets\": np.array([p for p in range(-num_coef + 1, 1)])\n        }\n\n    else:\n        matrix = _build_matrix_non_uniform(num_side, num_side, coords, idx)\n\n        rhs = _build_rhs(num_side, num_side, deriv)\n\n        ret = {\n            \"coefficients\": np.linalg.solve(matrix, rhs),\n            \"offsets\": np.array([p for p in range(-num_side, num_side + 1)])\n        }\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing the equation system matrix for the finite difference coefficients", "response": "def _build_matrix(p, q, deriv):\n    \"\"\"Constructs the equation system matrix for the finite difference coefficients\"\"\"\n    A = [([1 for _ in range(-p, q+1)])]\n    for i in range(1, p + q + 1):\n        A.append([j**i for j in range(-p, q+1)])\n    return np.array(A)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_rhs(p, q, deriv):\n\n    b = [0 for _ in range(p+q+1)]\n    b[deriv] = math.factorial(deriv)\n    return np.array(b)", "response": "Builds the right hand side of the equation system matrix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct the matrix for the finite difference coefficients of non - uniform grids at location k", "response": "def _build_matrix_non_uniform(p, q, coords, k):\n    \"\"\"Constructs the equation matrix for the finite difference coefficients of non-uniform grids at location k\"\"\"\n    A = [[1] * (p+q+1)]\n    for i in range(1, p + q + 1):\n        line = [(coords[k+j] - coords[k])**i for j in range(-p, q+1)]\n        A.append(line)\n    return np.array(A)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the accuracy order of the finite difference scheme.", "response": "def set_accuracy(self, acc):\n        \"\"\" Sets the accuracy order of the finite difference scheme.\n            If the FinDiff object is not a raw partial derivative but a composition of derivatives\n            the accuracy order will be propagated to the child operators.\n        \"\"\"\n        self.acc = acc\n        if self.child:\n            self.child.set_accuracy(acc)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply the finite differences only to slices along a given axis", "response": "def _apply_to_array(self, yd, y, weights, off_slices, ref_slice, dim):\n        \"\"\"Applies the finite differences only to slices along a given axis\"\"\"\n\n        ndims = len(y.shape)\n\n        all = slice(None, None, 1)\n\n        ref_multi_slice = [all] * ndims\n        ref_multi_slice[dim] = ref_slice\n\n        for w, s in zip(weights, off_slices):\n            off_multi_slice = [all] * ndims\n            off_multi_slice[dim] = s\n            if abs(1 - w) < 1.E-14:\n                yd[ref_multi_slice] += y[off_multi_slice]\n            else:\n                yd[ref_multi_slice] += w * y[off_multi_slice]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the metadata about the current user as supplied by the anchore. io service.", "response": "def get_current_user_info(anchore_auth):\n    \"\"\"\n    Return the metadata about the current user as supplied by the anchore.io service. Includes permissions and tier access.\n\n    :return: Dict of user metadata\n    \"\"\"\n\n    user_url = anchore_auth['client_info_url'] + '/' + anchore_auth['username']\n    user_timeout = 60\n    retries = 3\n    result = requests.get(user_url, headers={'x-anchore-password': anchore_auth['password']})\n    if result.status_code == 200:\n        user_data = json.loads(result.content)\n    else:\n        raise requests.HTTPError('Error response from service: {}'.format(result.status_code))\n    return user_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing list of Anchore data policies.", "response": "def show(details):\n    \"\"\"\n    Show list of Anchore data policies.\n\n    \"\"\"\n\n    ecode = 0\n    try:\n        policymeta = anchore_policy.load_policymeta()\n\n        if details:\n            anchore_print(policymeta, do_formatting=True)\n\n        else:\n            output = {}\n\n            name = policymeta['name']\n            output[name] = {}\n            output[name]['id'] = policymeta['id']\n            output[name]['policies'] = policymeta['policies']\n            output[name]['whitelists'] = policymeta['whitelists']\n            output[name]['mappings'] = policymeta['mappings']\n\n            anchore_print(output, do_formatting=True)\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exists(path, with_expected_db_version=None):\n        dbmetafile = os.path.join(path, AnchoreImageDB._db_metadata_filename)\n\n        if not os.path.exists(path) or not os.path.exists(dbmetafile):\n            return False\n\n        if with_expected_db_version:\n            with open(dbmetafile, 'r') as f:\n                json_dict = json.load(f)\n\n            if json_dict['anchore_db_version'] != with_expected_db_version:\n                return False\n        else:\n            return True", "response": "Checks if the database exists and if it has the expected version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbase on the click.help_option code. Adds a ``--extended-help`` option which immediately ends the program printing out the extended extended-help page. Defaults to using the callback's doc string, but can be given an explicit value as well. This is intended for use as a decorator on a command to provide a 3rd level of help verbosity suitable for use as a manpage (though not formatted as such explicitly). Like :func:`version_option`, this is implemented as eager option that prints in the callback and exits. All arguments are forwarded to :func:`option`.", "response": "def extended_help_option(extended_help=None, *param_decls, **attrs):\n    \"\"\"\n    Based on the click.help_option code.\n\n    Adds a ``--extended-help`` option which immediately ends the program\n    printing out the extended extended-help page. Defaults to using the\n    callback's doc string, but can be given an explicit value as well.\n\n    This is intended for use as a decorator on a command to provide a 3rd level\n    of help verbosity suitable for use as a manpage (though not formatted as such explicitly).\n\n    Like :func:`version_option`, this is implemented as eager option that\n    prints in the callback and exits.\n\n    All arguments are forwarded to :func:`option`.\n    \"\"\"\n\n    def decorator(f):\n        def callback(ctx, param, value):\n            if value and not ctx.resilient_parsing:\n                if not extended_help:\n                    ctx.command.help = ctx.command.callback.__doc__\n                    click.echo(ctx.get_help(), color=ctx.color)\n                else:\n                    ctx.command.help = extended_help\n                    click.echo(ctx.get_help(), color=ctx.color)\n                ctx.exit()\n\n        attrs.setdefault('is_flag', True)\n        attrs.setdefault('expose_value', False)\n        attrs.setdefault('help', 'Show extended help content, similar to manpage, and exit.')\n        attrs.setdefault('is_eager', True)\n        attrs['callback'] = callback\n        return click.option(*(param_decls or ('--extended-help',)), **attrs)(f)\n\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefault simple string format. Dumps block - style indented yaml for dicts.", "response": "def std_formatter(msg):\n    \"\"\"\n    Default simple string format. Dumps block-style indented yaml for dicts if found. Otherwise no formatting\n    :param msg:\n    :return:\n    \"\"\"\n\n    if isinstance(msg, dict):\n        return yaml.safe_dump(msg, indent=True, default_flow_style=False)\n    return str(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nformats the output in JSON", "response": "def json_formatter(obj):\n    \"\"\"\n    Format the output in JSON\n    :param obj:\n    :return:\n    \"\"\"\n    if isinstance(obj, str):\n        # Make a list of size 1\n        return json.dumps([obj], indent=True)\n    else:\n        return json.dumps(obj, indent=True, sort_keys=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints to stdout using the proper formatting for the command.", "response": "def anchore_print(msg, do_formatting=False):\n    \"\"\"\n    Print to stdout using the proper formatting for the command.\n\n    :param msg: output to be printed, either an object or a string. Objects will be serialized according to config\n    :return:\n    \"\"\"\n    if do_formatting:\n        click.echo(formatter(msg))\n    else:\n        click.echo(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive an image file and a list of image ids construct a list of image ids.", "response": "def build_image_list(config, image, imagefile, all_local, include_allanchore, dockerfile=None, exclude_file=None):\n    \"\"\"Given option inputs from the cli, construct a list of image ids. Includes all found with no exclusion logic\"\"\"\n\n    if not image and not (imagefile or all_local):\n        raise click.BadOptionUsage('No input found for image source. One of <image>, <imagefile>, or <all> must be specified')\n\n    if image and imagefile:\n        raise click.BadOptionUsage('Only one of <image> and <imagefile> can be specified')\n\n    filter_images = []\n    if exclude_file:\n        with open(exclude_file) as f:\n            for line in f.readlines():\n                filter_images.append(line.strip())\n\n    imagelist = {}\n    if image:\n        imagelist[image] = {'dockerfile':dockerfile}\n\n    if imagefile:\n        filelist = anchore_utils.read_kvfile_tolist(imagefile)\n        for i in range(len(filelist)):\n            l = filelist[i]\n            imageId = l[0]\n            try:\n                dfile = l[1]\n            except:\n                dfile = None\n            imagelist[imageId] = {'dockerfile':dfile}\n\n    if all_local:\n        docker_cli = contexts['docker_cli']\n        if docker_cli:\n            for f in docker_cli.images(all=True, quiet=True, filters={'dangling': False}):\n                if f not in imagelist and f not in filter_images:\n                    imagelist[f] = {'dockerfile':None}\n        else:\n            raise Exception(\"Could not load any images from local docker host - is docker running?\")\n\n    if include_allanchore:\n        ret = contexts['anchore_db'].load_all_images().keys()\n        if ret and len(ret) > 0:\n            for l in list(set(imagelist.keys()) | set(ret)):\n                imagelist[l] = {'dockerfile':None}\n\n    # Remove excluded items\n    for excluded in filter_images:\n        docker_cli = contexts['docker_cli']\n        if not docker_cli:\n            raise Exception(\"Could not query docker - is docker running?\")\n        for img in docker_cli.images(name=excluded, quiet=True):\n            imagelist.pop(img, None)\n\n    return imagelist"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the output formatters for the current console.", "response": "def init_output_formatters(output_verbosity='normal', stderr=sys.stderr, logfile=None, debug_logfile=None):\n    \"\"\"\n    Initialize the CLI logging scheme.\n\n    :param output_verbosity: 'quiet','normal','verbose', or 'debug' controls the output to stdout and its format\n    :param stderr: stream for stderr output, default=stderr, pass a file path/string to have stderr in a file\n    :return:\n    \"\"\"\n\n    if output_verbosity not in console_verbosity_options:\n        raise ValueError('output_verbosity must be one of: %s' % console_verbosity_options.keys())\n\n    # Initialize debug log file, 'anchore-debug.log'. This log has stack-traces and is expected to be human read\n    # and intended for developers and debugging, not an operational log.\n\n    # Configure stderr behavior. All errors go to screen\n    stderr_handler = logging.StreamHandler(stderr)\n\n    if output_verbosity == 'quiet':\n        stderr_handler.setLevel(level='ERROR')\n        stderr_handler.setFormatter(NoTracebackFormatter(fmt=NORMAL_FORMAT, err_fmt=ERR_FORMAT))\n        logging.root.setLevel('ERROR')  # Allow all at top level, filter specifics for each handler\n\n    elif output_verbosity == 'normal':\n        # The specific console logger\n        stderr_handler.setLevel('INFO')\n        stderr_formatter = NoTracebackFormatter(fmt=NORMAL_FORMAT, err_fmt=ERR_FORMAT)\n        stderr_handler.setFormatter(stderr_formatter)\n        stderr_handler.addFilter(LoggerNamePrefixFilter(prefix='anchore', non_match_loglevel='ERROR'))\n        logging.root.setLevel('INFO')\n\n    elif output_verbosity == 'verbose':\n        stderr_handler.setLevel('INFO')\n        stderr_handler.setFormatter(NoTracebackFormatter(fmt=NORMAL_FORMAT, err_fmt=ERR_FORMAT))\n        logging.root.setLevel('INFO')\n\n    elif output_verbosity == 'debug':\n        stderr_handler.setLevel(level='DEBUG')\n        stderr_handler.setFormatter(logging.Formatter(fmt=DEBUG_FORMAT))\n        logging.root.setLevel('DEBUG')\n\n    logging.root.addHandler(stderr_handler)\n\n    if debug_logfile:\n        debug_filehandler = logging.FileHandler(debug_logfile)\n        debug_filehandler.setLevel('DEBUG')\n        formatter = logging.Formatter(fmt=DEBUG_LOGFILE_FORMAT)\n        debug_filehandler.setFormatter(formatter)\n        logging.root.addHandler(debug_filehandler)\n        logging.root.setLevel('DEBUG')\n\n    if logfile:\n        filehandler = logging.FileHandler(logfile)\n        filehandler.setLevel('INFO')\n        filehandler.setFormatter(NoTracebackFormatter(fmt=LOGFILE_FORMAT, err_fmt=LOGFILE_FORMAT))\n        logging.root.addHandler(filehandler)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the specified record as text.", "response": "def format(self, record):\n        \"\"\"\n        Modified from logging/__init__.py in python 2.7 lib\n\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n\n        if record.levelno >= logging.getLevelName('ERROR'):\n            s = self._err_format % record.__dict__\n        else:\n            s = self._fmt % record.__dict__\n\n        if record.exc_info:\n            record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            # Trim the trailing newline\n            if s[-1:] == \"\\n\":\n                s = s[:-1]\n\n            try:\n                # Delimit with the colon\n                s = s + ': ' + record.exc_text\n            except UnicodeError:\n                # Sometimes filenames have non-ASCII chars, which can lead\n                # to errors when s is Unicode and record.exc_text is str\n                # See issue 8924.\n                # We also use replace for when there are multiple\n                # encodings, e.g. UTF-8 for the filesystem and latin-1\n                # for a script. See issue 13232.\n                s = s + record.exc_text.decode(sys.getfilesystemencoding(),\n                                               'replace')\n            # Reset to avoid using the cache since this logger changes the exception format\n            record.exc_text = None\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck the path and construct if not found and init_if_missing = True", "response": "def check(self, init_if_missing=False):\n        \"\"\"\n        Check the path and construct if not found and init_if_missing=True\n        :param init_if_missing: create the path if not found\n        :return: true if exists false if not\n        \"\"\"\n        for d in self.path_overrides + [self.inputdir]:            \n            if os.path.exists(d):\n                self.allpaths.append(d)\n        if len(self.allpaths) > 0:\n            return(True)\n\n        if init_if_missing:\n            os.makedirs(self.inputdir)\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the scripts at the path in sorted order as set in the module properties", "response": "def get_scripts(self):\n        \"\"\"\n        Get the scripts at the path in sorted order as set in the module properties\n        :return: a sorted list of scripts\n        \"\"\"\n        ret = list()\n        for d in self.allpaths:\n            scripts = filter(lambda x: x.startswith(self.prefix), os.listdir(d))\n            scripts.sort(reverse=(not self.sort_ascending))\n            ret = ret + [os.path.join(d, x) for x in scripts]\n\n        return(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute all the scripts in the current directory and return a list of the output of the scripts.", "response": "def execute(self, capture_output=False, fail_fast=False, cmdline=None, lastcsums=None, **kwargs):\n        \"\"\"\n        Pass in the kwargs as --<name> <value> pairs.\n        :param capture_output: if True then return output of script in return dict. If False, only return code\n        :param fail_fast:if True then quit execution after first non-zero return code, else execute all\n        :returns list of (script, returncode, stdout) tuples in order of execution\n        \"\"\"\n\n        currcsums = self.csums()\n        \n        scripts = self.get_scripts()\n        output = []\n\n        for script in scripts:\n            if not os.access(script, os.R_OK ^ os.X_OK) or re.match(\".*~$\", script):\n                # Skip any that are not executable (e.g. README or txt files)\n                continue\n\n            if lastcsums:\n                if (script in currcsums and script in lastcsums) and currcsums[script] == lastcsums[script]:\n                    # skip if the analyzer has not changed since the last time it was run\n                    self._logger.debug(\"Skipping analyzer %s since this analyzer version has already executed in the past\" % script)\n                    continue\n\n            self._logger.debug('Executing script: %s' % script)\n\n            try:\n                csum = currcsums[script]\n            except:\n                csum = \"N/A\"\n\n            if cmdline:\n                cmd = [script] + cmdline.split()\n            else:\n                cmd = [script]\n\n            if capture_output:\n                try:\n                    outstr = subprocess.check_output(cmd, stderr=subprocess.STDOUT, **kwargs)\n                    outstr = outstr.decode('utf8')\n                    #outstr = rawstr.encode('utf8')\n                    output.append((' '.join(cmd), 0, outstr))\n                except subprocess.CalledProcessError as e:\n                    outstr = e.output.decode('utf8')\n                    #outstr = rawstr.encode('utf8')\n                    output.append((' '.join(cmd), e.returncode, outstr))\n            else:\n                output.append((' '.join(cmd), subprocess.call(cmd, **kwargs), None))\n\n            self._logger.debug('%s return code %d' % (output[-1][0], output[-1][1]))\n            if output[-1][2]:\n                self._logger.debug('%s output:' % (script))\n                for l in output[-1][2].splitlines():\n                    l = l.strip()\n                    self._logger.debug('%s | %s' % (script, l))\n\n            if fail_fast and output[-1][1]:\n                break\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct and informative user - agent string.", "response": "def get_user_agent():\n    \"\"\"\n    Construct and informative user-agent string.\n    Includes OS version and docker version info\n\n    Not thread-safe\n    :return:\n    \"\"\"\n\n    global user_agent_string\n    if user_agent_string is None:\n        # get OS type\n        try:\n            sysinfo = subprocess.check_output('lsb_release -i -r -s'.split(' '))\n            dockerinfo = subprocess.check_output('docker --version'.split(' '))\n\n            user_agent_string = ' '.join([sysinfo.replace('\\t','/'),\n                                          dockerinfo.replace(\" version \", \"/\").replace(\", \", \"/\").replace(' ', '/')]).replace('\\n','')\n        except:\n            user_agent_string = None\n\n    return user_agent_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_headers(headers):\n    for (k, v) in headers.items():\n        headers.pop(k)\n        headers[k.lower()] = v\n\n    result = {}\n    if 'content-length' in headers:\n        result['content-length'] = headers['content-length']\n    elif 'contentlength' in headers:\n        result['content-length'] = headers['contentlength']\n\n    if 'content-type' in headers:\n        result['content-type'] = headers['content-type']\n    elif 'contenttype' in headers:\n        result['content-type'] = headers['contenttype']\n\n    if 'last-modified' in headers:\n        result['last-modified'] = headers['last-modified']\n    elif 'lastmodified' in headers:\n        result['last-modified'] = headers['lastmodified']\n    else:\n        result['last-modified'] = datetime.datetime.utcnow().strftime(RFC1123_TIME_FORMAT)\n\n    # Normalize date\n    if isinstance(result['last-modified'], datetime.datetime):\n        result['last-modified'] = result['last-modified'].strftime(RFC1123_TIME_FORMAT)\n\n    if 'ETag' in headers:\n        result['etag'] = headers['etag'].strip('\"')\n    elif 'etag' in headers:\n        result['etag'] = headers['etag'].strip('\"')\n\n    return result", "response": "Convert useful headers to a normalized type and return in a new dict\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_resource_retriever(url):\n\n    if url.startswith('http://') or url.startswith('https://'):\n        return HttpResourceRetriever(url)\n    else:\n        raise ValueError('Unsupported scheme in url: %s' % url)", "response": "Returns the appropriate retriever object for the specified url."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfetches the content of the current object and save it to the given path.", "response": "def fetch(self, output_path, last_etag=None, lastmodified=None, progress_bar=False):\n        \"\"\"\n        :param output_path:\n        :param last_etag:\n        :param lastmodified:\n        :param progress_bar:\n        :return:\n        \"\"\"\n        req_headers = {}\n        usr_agent = get_user_agent()\n        if usr_agent is not None:\n            req_headers['User-Agent'] = usr_agent\n\n        if last_etag is not None:\n            req_headers['If-None-Match'] = '\"' + last_etag + '\"'\n        if lastmodified is not None:\n            req_headers['If-Modified-Since'] = lastmodified\n\n        response = requests.get(self.url, headers=req_headers, stream=progress_bar)\n\n        if response.status_code == 304:\n            # Conditions didn't match, no content returned\n            return None\n        if response.status_code != 200:\n            raise requests.HTTPError\n\n        with open(output_path, 'wb') as f:\n            if progress_bar:\n                total_size = long(response.headers.get('content-length'))\n                for chunk in progress.bar(response.iter_content(chunk_size=self.chunk_size),\n                                          expected_size=(total_size / self.chunk_size) + 1):\n                    if chunk:\n                        f.write(chunk)\n                        f.flush()\n            else:\n                f.write(response.content)\n\n        headers = normalize_headers(response.headers)\n\n        if 'etag' not in headers:\n            headers['etag'] = fs_util.calc_file_md5(output_path)\n\n        return headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clear(self):\n        for f in [x['content'] for x in self.metadata.values()]:\n            os.remove(f)\n\n        self.metadata = {}\n        self._flush()", "response": "Remove all entries from the cache and delete all data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load(self, url):\n        self._logger.debug('Loading url %s into resource cache' % url)\n        retriever = get_resource_retriever(url)\n        content_path = os.path.join(self.path, self._hash_path(url))\n\n        try:\n            if url in self.metadata:\n                headers = retriever.fetch(content_path, last_etag=self.metadata[url]['etag'], progress_bar=self.progress_bars)\n                if headers is None:\n                    # no update, return what is already loaded\n                    self._logger.info('Cached %s is up-to-date. No data download needed' % url)\n                    return self.metadata[url]\n            else:\n                headers = retriever.fetch(content_path, progress_bar=self.progress_bars)\n                if headers is None:\n                    raise Exception('Fetch of %s failed' % url)\n\n            if 'etag' not in headers:\n                # No Etag returned, so generate it\n                headers['etag'] = fs_util.calc_file_md5(content_path)\n\n            # Populate metadata from the headers\n            self.metadata[url] = headers.copy()\n            self.metadata[url]['content'] = content_path\n            return self.metadata[url]\n        except:\n            self._logger.error('Failed getting resource: %s')\n\n            # forcibly flush local entry if found\n            if url in self.metadata:\n                self.metadata.pop(url)\n            raise\n        finally:\n            if url not in self.metadata:\n                self._logger.debug('Cleaning up on failed load of %s' % url)\n                # Cleanup on failure\n                if content_path is not None and os.path.exists(content_path):\n                    os.remove(content_path)", "response": "Load from remote object and populate the metadata dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nflushing the metadata to the backing file", "response": "def _flush(self):\n        \"\"\"\n        Flush metadata to the backing file\n        :return:\n        \"\"\"\n        with open(self.metadata_file, 'w') as f:\n            json.dump(self.metadata, f)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tarzip_data(root_dir, output_file):\n    if root_dir is None:\n        raise ValueError('Must be a valid directory path', root_dir)\n\n    t = tarfile.open(output_file, 'w:gz')\n    for child in os.listdir(root_dir):\n        t.add(os.path.join(root_dir,child), arcname=child)\n    t.close()\n    return t.name", "response": "Adds all its children to a tarball"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating directories in the specified path.", "response": "def createpath(path, mode, exists_ok=True):\n    \"\"\"\n    Create directories in the indicated path.\n    :param path:\n    :param mode:\n    :param exists_ok:\n    :return:\n    \"\"\"\n    try:\n        os.makedirs(path, mode)\n    except OSError, e:\n        if e.errno != errno.EEXIST or not exists_ok:\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates a file s md5 checksum. Use the specified chunk_size for IO or the default 256KB.", "response": "def calc_file_md5(filepath, chunk_size=None):\n    \"\"\"\n    Calculate a file's md5 checksum. Use the specified chunk_size for IO or the\n    default 256KB\n\n    :param filepath:\n    :param chunk_size:\n    :return:\n    \"\"\"\n    if chunk_size is None:\n        chunk_size = 256 * 1024\n    md5sum = hashlib.md5()\n    with io.open(filepath, 'r+b') as f:\n        datachunk = f.read(chunk_size)\n        while datachunk is not None and len(datachunk) > 0:\n            md5sum.update(datachunk)\n            datachunk = f.read(chunk_size)\n\n    return md5sum.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nunpack and Squash image to local filesystem", "response": "def unpack(destdir):\n    \"\"\"Unpack and Squash image to local filesystem\"\"\"\n\n    if not nav:\n        sys.exit(1)\n\n    ecode = 0\n    try:\n        anchore_print(\"Unpacking images: \" + ' '.join(nav.get_images()))\n        result = nav.unpack(destdir=destdir)\n        if not result:\n            anchore_print_err(\"no images unpacked\")\n            ecode = 1\n        else:\n            for imageId in result:\n                anchore_print(\"Unpacked image: \" + imageId)\n                anchore_print(\"Unpack directory: \"+ result[imageId])\n    except:\n        anchore_print_err(\"operation failed\")\n        ecode = 1\n\n    contexts['anchore_allimages'].clear()\n    \n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_module_dev(destdir):\n\n    if not nav:\n        sys.exit(1)\n\n    ecode = 0\n    try:\n        anchore_print(\"Anchore Module Development Environment\\n\")\n        helpstr = \"This tool has set up an environment that represents what anchore will normally set up before running an analyzer, gate and/or query module.  Each section below includes some information along with a string that you can use to help develop your own anchore modules.\\n\"\n        anchore_print(fill(helpstr, 80))\n        anchore_print(\"\")\n\n        anchore_print(\"Setting up environment...\")\n        anchore_print(\"\")\n\n        result = nav.unpack(destdir=destdir)\n        if not result:\n            raise Exception(\"unable to unpack input image\")\n\n        for imageId in result:\n            unpackdir = result[imageId]\n\n            # copy anchore imageDB dir into unpacked environment\n            imgdir = '/'.join([config.data['image_data_store'], imageId])\n            tmpdatastore = '/'.join([unpackdir, 'data'])\n            dstimgdir = '/'.join([tmpdatastore, imageId])\n\n            if not os.path.exists(imgdir):\n                anchore_print_err(\"Image must exist and have been analyzed before being used for module development.\")\n                break\n            if not os.path.exists(tmpdatastore):\n                os.makedirs(tmpdatastore)\n            shutil.copytree(imgdir, dstimgdir, symlinks=True)\n\n            # copy examples into the unpacked environment            \n            examples = {}\n            basedir = '/'.join([unpackdir, \"anchore-modules\"])\n            if not os.path.exists(basedir):\n                os.makedirs(basedir)\n\n                # copy the shell-utils\n                os.makedirs('/'.join([basedir, 'shell-utils']))\n                for s in os.listdir('/'.join([config.data['scripts_dir'], 'shell-utils'])):\n                    shutil.copy('/'.join([config.data['scripts_dir'], 'shell-utils', s]), '/'.join([basedir, 'shell-utils', s]))\n                            \n            # copy any examples that exist in the anchore egg into the unpack dir\n            for d in os.listdir(config.data['scripts_dir']):\n                scriptdir = '/'.join([basedir, d])\n\n                if os.path.exists(config.data['scripts_dir'] + \"/examples/\" + d):\n                    if not os.path.exists(scriptdir):\n                        os.makedirs(scriptdir)\n                    for s in os.listdir(config.data['scripts_dir'] + \"/examples/\" + d):\n                        thefile = '/'.join([config.data['scripts_dir'], \"examples\", d, s])\n                        thefiledst = '/'.join([scriptdir, s])\n                        if re.match(\".*(\\.sh)$\", thefile):\n                            examples[d] = thefiledst\n                            shutil.copy(thefile, thefiledst)\n\n            # all set, show how to use them\n            anchore_print(\"\\tImage: \" + imageId[0:12])\n            anchore_print(\"\\tUnpack Directory: \" +result[imageId])\n            anchore_print(\"\")\n            analyzer_string = ' '.join([examples['analyzers'], imageId, tmpdatastore, dstimgdir, result[imageId]])\n            anchore_print(\"\\tAnalyzer Command:\\n\\n\\t\" +analyzer_string)\n            anchore_print(\"\")\n\n            anchore_utils.write_plainfile_fromstr(result[imageId] + \"/queryimages\", imageId+\"\\n\")\n\n            queryoutput = '/'.join([result[imageId], \"querytmp/\"])\n            if not os.path.exists(queryoutput):\n                os.makedirs(queryoutput)\n\n            query_string = ' '.join([examples['queries'], result[imageId] + \"/queryimages\", tmpdatastore, queryoutput, \"passwd\"])\n            anchore_print(\"Query Command:\\n\\n\\t\" + query_string)\n            anchore_print(\"\")\n \n            anchore_print(\"Next Steps: \")\n            anchore_print(\"\\tFirst: run the above analyzer command and note the RESULT output\")\n            anchore_print(\"\\tSecond: run the above query command and note the RESULT output, checking that the query was able to use the analyzer data to perform its search\")\n            anchore_print(\"\\tThird: modify the analyzer/query modules as you wish, including renaming them and continue running/inspecting output until you are satisfied\")\n            anchore_print(\"\\tFinally: when you're happy with the analyzer/query, copy them to next to existing anchore analyzer/query modules and anchore will start calling them as part of container analysis/query:\\n\")\n            anchore_print(\"\\tcp \" + examples['analyzers'] + \" \" + config.data['scripts_dir'] + \"/analyzers/99_analyzer-example.sh\")\n            anchore_print(\"\\tcp \" + examples['queries'] + \" \" + config.data['scripts_dir'] + \"/queries/\")\n            anchore_print(\"\\tanchore analyze --force --image \" + imageId + \" --imagetype none\")\n            anchore_print(\"\\tanchore query --image \" + imageId + \" query-example\")\n            anchore_print(\"\\tanchore query --image \" + imageId + \" query-example passwd\")\n            anchore_print(\"\\tanchore query --image \" + imageId + \" query-example pdoesntexist\")\n            \n    except:\n        anchore_print_err(\"operation failed\")\n        ecode = 1\n\n    contexts['anchore_allimages'].clear()\n    \n    sys.exit(ecode)", "response": "Setup an anchore module development environment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show_dockerfile():\n\n    if not nav:\n        sys.exit(1)\n\n    ecode = 0\n    try:\n        result = nav.run_query(['show-dockerfile', 'all'])\n        if result:\n            anchore_utils.print_result(config, result)\n\n    except:\n        anchore_print_err(\"operation failed\")\n        ecode = 1\n\n    contexts['anchore_allimages'].clear()\n    sys.exit(ecode)", "response": "Generate or display actual image Dockerfile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshow history of all known repo tags for image", "response": "def show_taghistory():\n    \"\"\"Show history of all known repo/tags for image\"\"\"\n\n\n    if not nav:\n        sys.exit(1)\n\n    ecode = 0\n    try:\n        result = nav.get_taghistory()\n        if result:\n            anchore_utils.print_result(config, result)\n\n    except:\n        anchore_print_err(\"operation failed\")\n        ecode = 1\n\n    contexts['anchore_allimages'].clear()\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_analyzer_status():\n\n    ecode = 0\n    try:\n        image=contexts['anchore_allimages'][imagelist[0]]\n        analyzer_status = contexts['anchore_db'].load_analyzer_manifest(image.meta['imageId'])\n        result = {image.meta['imageId']:{'result':{'header':['Analyzer', 'Status', '*Type', 'LastExec', 'Exitcode', 'Checksum'], 'rows':[]}}}\n        for script in analyzer_status.keys():\n            adata = analyzer_status[script]\n            nicetime = datetime.datetime.fromtimestamp(adata['timestamp']).strftime('%Y-%m-%d %H:%M:%S')\n            try:\n                row = [script.split('/')[-1], adata['status'], adata['atype'], nicetime, str(adata['returncode']), adata['csum']]\n                result[image.meta['imageId']]['result']['rows'].append(row)        \n            except:\n                pass\n        if result:\n            anchore_utils.print_result(config, result)\n    except:\n        anchore_print_err(\"operation failed\")\n        ecode = 1\n\n    contexts['anchore_allimages'].clear()\n    sys.exit(ecode)", "response": "Show analyzer status for specified image"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport image anchore data to a JSON file.", "response": "def export(outfile):\n    \"\"\"Export image anchore data to a JSON file.\"\"\"\n\n    if not nav:\n        sys.exit(1)\n\n    ecode = 0\n    savelist = list()\n    for imageId in imagelist:\n\n        try:\n            record = {}\n            record['image'] = {}\n            record['image']['imageId'] = imageId\n            record['image']['imagedata'] = contexts['anchore_db'].load_image_new(imageId)\n        \n            savelist.append(record)\n        except Exception as err:\n            anchore_print_err(\"could not find record for image (\"+str(imageId)+\")\")\n            ecode = 1\n\n    if ecode == 0:\n        try:\n            if outfile == '-':\n                print json.dumps(savelist, indent=4)\n            else:\n                with open(outfile, 'w') as OFH:\n                    OFH.write(json.dumps(savelist))\n        except Exception as err:\n            anchore_print_err(\"operation failed: \" + str(err))\n            ecode = 1\n\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kubesync():\n\n    ecode = 0\n\n    try:\n        images = anchore_utils.get_images_from_kubectl()\n        if images:\n            anchore_print(\"Writing image IDs to ./anchore_imageIds.kube\")\n            with open(\"anchore_imageIds.kube\", 'w') as OFH:\n                for imageId in images:\n                    OFH.write(imageId + \"\\n\")\n            anchore_print(\"Writing image names to ./anchore_imageNames.kube\")\n            with open(\"anchore_imageNames.kube\", 'w') as OFH:\n                for imageId in images:\n                    OFH.write(images[imageId] + \"\\n\")\n                    \n    except Exception as err:\n        anchore_print_err(\"operation failed: \" + str(err))\n        ecode = 1\n\n    sys.exit(ecode)", "response": "Communicate with kubernetes deployment via kubectl and save image names and IDs to local files"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef image_import(infile, force):\n    ecode = 0\n    \n    try:\n        with open(infile, 'r') as FH:\n            savelist = json.loads(FH.read())\n    except Exception as err:\n        anchore_print_err(\"could not load input file: \" + str(err))\n        ecode = 1\n\n    if ecode == 0:\n        for record in savelist:\n            try:\n                imageId = record['image']['imageId']\n                if contexts['anchore_db'].is_image_present(imageId) and not force:\n                    anchore_print(\"image (\"+str(imageId)+\") already exists in DB, skipping import.\")\n                else:\n                    imagedata = record['image']['imagedata']\n                    try:\n                        rc = contexts['anchore_db'].save_image_new(imageId, report=imagedata)\n                        if not rc:\n                            contexts['anchore_db'].delete_image(imageId)\n                            raise Exception(\"save to anchore DB failed\")\n                    except Exception as err:\n                        contexts['anchore_db'].delete_image(imageId)\n                        raise err\n            except Exception as err:\n                anchore_print_err(\"could not store image (\"+str(imageId)+\") from import file: \"+ str(err))\n                ecode = 1\n\n    sys.exit(ecode)", "response": "Import an image anchore data from a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef show():\n\n    ecode = 0\n    try:\n        o = collections.OrderedDict()\n        inimage = imagelist[0]\n        anchoreDB = contexts['anchore_db']\n        image = anchoreDB.load_image(inimage)\n        if image:\n            mymeta = image['meta']\n            alltags_current = image['anchore_current_tags']\n            distrodict = anchore_utils.get_distro_from_imageId(inimage)\n            distro = distrodict['DISTRO']\n            distrovers = distrodict['DISTROVERS']\n            base = image['familytree'][0]\n\n            o['IMAGEID'] = mymeta.pop('imageId', \"N/A\")\n            o['REPOTAGS'] = alltags_current\n            o['DISTRO'] = distro\n            o['DISTROVERS'] = distrovers\n            o['HUMANNAME'] = mymeta.pop('humanname', \"N/A\")\n            o['SHORTID'] = mymeta.pop('shortId', \"N/A\")\n            o['PARENTID'] = mymeta.pop('parentId', \"N/A\")\n            o['BASEID'] = base\n            o['IMAGETYPE'] = mymeta.pop('usertype', \"N/A\")\n\n            for k in o.keys():\n                if type(o[k]) is list:\n                    s = ' '.join(o[k])\n                else:\n                    s = str(o[k])\n                print k+\"='\"+s+\"'\"\n        else:\n            raise Exception(\"cannot locate input image in anchore DB\")\n\n    except Exception as err:\n        anchore_print_err(\"operation failed\")\n        ecode = 1\n\n    contexts['anchore_allimages'].clear()\n\n    sys.exit(ecode)", "response": "Show image summary information"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_orig():\n\n    if not nav:\n        sys.exit(1)\n\n    ecode = 0\n    try:\n        image=contexts['anchore_allimages'][imagelist[0]]\n\n        o = collections.OrderedDict()\n        mymeta = {}\n        mymeta.update(image.meta)\n        o['IMAGEID'] = mymeta.pop('imageId', \"N/A\")\n        o['REPOTAGS'] = image.get_alltags_current()\n        o['DISTRO'] = image.get_distro()\n        o['DISTROVERS'] = image.get_distro_vers()\n        o['HUMANNAME'] = mymeta.pop('humanname', \"N/A\")\n        o['SHORTID'] = mymeta.pop('shortId', \"N/A\")\n        o['PARENTID'] = mymeta.pop('parentId', \"N/A\")\n        o['BASEID'] = image.get_earliest_base()\n        o['IMAGETYPE'] = mymeta.pop('usertype', \"N/A\")\n\n        for k in o.keys():\n            if type(o[k]) is list:\n                s = ' '.join(o[k])\n            else:\n                s = str(o[k])\n            print k+\"='\"+s+\"'\"\n\n    except:\n        anchore_print_err(\"operation failed\")\n        ecode = 1\n\n    contexts['anchore_allimages'].clear()\n\n    sys.exit(ecode)", "response": "Show image summary information"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef audit(anchore_config, ctx, image, imagefile, include_allanchore):\n\n    global config, imagelist, nav\n    ecode = 0\n    success = True\n    config = anchore_config\n\n    #include_allanchore = True\n\n    if image and imagefile:\n        raise click.BadOptionUsage('Can only use one of --image, --imagefile')\n\n    #if image or imagefile:\n    #    include_allanchore = False\n\n    try:\n        imagedict = build_image_list(anchore_config, image, imagefile, not (image or imagefile), include_allanchore)\n        imagelist = imagedict.keys()\n        try:\n            ret = anchore_utils.discover_imageIds(imagelist)\n        except ValueError as err:\n            raise err\n        else:\n            imagelist = ret\n\n    except Exception as err:\n        anchore_print_err(\"could not load input images\")\n        sys.exit(1)", "response": "Image IDs can be specified as hash ids, repo names (e.g. centos), or tags (e.g. centos:latest)."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows an analysis report of the specified images.", "response": "def report():\n    \"\"\"\n    Show analysis report of the specified image(s).\n\n    The analysis report includes information on:\n\n    \\b\n    Image Id - The image id (as a hash)\n\n    Type - The type of image (--imagetype option used when anchore analyze was run)\n\n    CurrentTags - The current set of repo tags on the image\n\n    AllTags - The set of all repo tags that have been on the image during analysis passes\n\n    GateStatus - The overall aggregate gate output status: GO|STOP|WARN\n\n    Size - The size in bytes of the image on disk\n    \n    Counts - The counts for various attributes of the images such as packages, files, and suid files\n\n    BaseDiffs - Differences of this image from its base image\n\n    Report outputs these entries in a table format by default.\n    \"\"\"\n    ecode = 0\n\n    try:\n        nav = init_nav_contexts()\n        result = nav.generate_reports()\n\n        #result = generate_reports(imagelist, showall=all, showdetails=details)\n\n        if result:\n            anchore_utils.print_result(config, result)\n\n    except:\n        anchore_print_err(\"operation failed\")\n        ecode = 1\n\n    contexts['anchore_allimages'].clear()\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef status(conf):\n\n    ecode = 0\n    try:\n        if conf:\n            if config.cliargs['json']:\n                anchore_print(config.data, do_formatting=True)\n            else:\n                anchore_print(yaml.safe_dump(config.data, indent=True, default_flow_style=False))\n        else:\n            result = {}\n            if contexts['anchore_db'].check():\n                result[\"anchore_db\"] = \"OK\"\n            else:\n                result[\"anchore_db\"] = \"NOTINITIALIZED\"\n\n            if anchore_feeds.check():\n                result[\"anchore_feeds\"] = \"OK\"\n            else:\n                result[\"anchore_feeds\"] = \"NOTSYNCED\"\n\n            afailed = False\n            latest = 0\n            for imageId in contexts['anchore_db'].load_all_images().keys():\n                amanifest = anchore_utils.load_analyzer_manifest(imageId)\n                for module_name in amanifest.keys():\n                    try:\n                        if amanifest[module_name]['timestamp'] > latest:\n                            latest = amanifest[module_name]['timestamp']\n                        if amanifest[module_name]['status'] != 'SUCCESS':\n                            analyzer_failed_imageId = imageId\n                            analyzer_failed_name = module_name\n                            afailed = True\n                    except:\n                        pass\n\n            if latest == 0:\n                result[\"analyzer_status\"] = \"NODATA\"\n            elif afailed:\n                result[\"analyzer_status\"] = \"FAIL (\"+analyzer_failed_imageId+\")\"\n                result[\"analyzer_latest_run\"] = time.ctime(latest)\n            else:\n                result[\"analyzer_status\"] = \"OK\"\n                result[\"analyzer_latest_run\"] = time.ctime(latest)\n   \n            anchore_print(result, do_formatting=True)\n\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)", "response": "Show anchore system status."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show_schemas(schemaname):\n\n    ecode = 0\n    try:\n        schemas = {}\n        schema_dir = os.path.join(contexts['anchore_config']['pkg_dir'], 'schemas')\n        for f in os.listdir(schema_dir):\n            sdata = {}\n            try:\n                with open(os.path.join(schema_dir, f), 'r') as FH:\n                    sdata = json.loads(FH.read())\n            except:\n                anchore_print_err('found schema file but failed to parse: ' + os.path.join(schema_dir, f))\n\n            if sdata and (not schemaname or f in schemaname):\n                schemas[f] = sdata\n\n        if not schemas:\n            anchore_print_err(\"no specified schemas were found to show\")\n        else:\n            anchore_print(json.dumps(schemas, indent=4))\n\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)", "response": "Show anchore document schemas."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef backup(outputdir):\n\n    ecode = 0\n    try:\n        anchore_print('Backing up anchore system to directory '+str(outputdir)+' ...')\n        backupfile = config.backup(outputdir)\n        anchore_print({\"anchore_backup_tarball\":str(backupfile)}, do_formatting=True)\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)", "response": "Backup an anchore installation to a tarfile."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrestore an anchore installation from a previously backed up tar file.", "response": "def restore(inputfile, destination_root):\n    \"\"\"\n    Restore an anchore installation from a previously backed up tar file.\n    \"\"\"\n\n    ecode = 0\n    try:\n        anchore_print('Restoring anchore system from backup file %s ...' % (str(inputfile.name)))\n        restoredir = config.restore(destination_root, inputfile)\n        anchore_print(\"Anchore restored.\")\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexports all anchore images to JSON files", "response": "def exportdb(outdir):\n    \"\"\"Export all anchore images to JSON files\"\"\"\n    ecode = 0\n    try:\n        imgdir = os.path.join(outdir, \"images\")\n        feeddir = os.path.join(outdir, \"feeds\")\n        storedir = os.path.join(outdir, \"storedfiles\")\n\n        for d in [outdir, imgdir, feeddir, storedir]:\n            if not os.path.exists(d):\n                os.makedirs(d)\n\n        anchore_print(\"exporting images...\")\n        imagelist = anchore_utils.get_image_list().keys()\n        for imageId in imagelist:\n            thefile = os.path.join(imgdir, imageId+\".json\")\n            if not os.path.exists(thefile):\n                with open(thefile, 'w') as OFH:\n                    OFH.write(json.dumps(contexts['anchore_db'].load_image_new(imageId)))\n\n            stored_namespaces = contexts['anchore_db'].load_files_namespaces(imageId)\n            for namespace in stored_namespaces:\n                stored_files = contexts['anchore_db'].load_files_tarfile(imageId, namespace)\n                if os.path.exists(stored_files):\n                    thedir = os.path.join(storedir, imageId, namespace)\n                    if not os.path.exists(thedir):\n                        os.makedirs(thedir)\n                    thefile = os.path.join(thedir, \"stored_files.tar.gz\")\n                    shutil.copy(stored_files, thefile)\n\n        anchore_print(\"exporting feeds...\")\n        feedmeta = contexts['anchore_db'].load_feedmeta()\n        thefile = os.path.join(feeddir, \"feedmeta.json\")\n        with open(thefile, 'w') as OFH:\n            OFH.write(json.dumps(feedmeta))\n\n        for feed in feedmeta:\n            feedobj = feedmeta[feed]\n            for group in feedobj['groups']:\n                groupobj = feedobj['groups'][group]\n                datafiles = groupobj.pop('datafiles', [])\n                for datafile in datafiles:\n                    thedir = os.path.join(feeddir, feed, group)\n                    if not os.path.exists(thedir):\n                        os.makedirs(thedir)\n                    thefile = os.path.join(thedir, datafile)\n                    if not os.path.exists(thefile):\n                        with open(thefile, 'w') as OFH:\n                            OFH.write(json.dumps(contexts['anchore_db'].load_feed_group_data(feed, group, datafile)))\n\n    except Exception as err:\n        anchore_print_err(\"operation failed: \" + str(err))\n        ecode = 1\n\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef importdb(indir):\n    ecode = 0\n    try:\n        imgdir = os.path.join(indir, \"images\")\n        feeddir = os.path.join(indir, \"feeds\")\n        storedir = os.path.join(indir, \"storedfiles\")\n\n        for d in [indir, imgdir, feeddir, storedir]:\n            if not os.path.exists(d):\n                raise Exception (\"specified directory \"+str(indir)+\" does not appear to be complete (missing \"+str(d)+\")\")\n\n                \n        anchore_print(\"importing images...\")\n        #imagelist = []\n        for ifile in os.listdir(imgdir):\n            patt = re.match(\"(.*)\\.json\", ifile)\n            if patt:\n                imageId = patt.group(1)\n\n                if contexts['anchore_db'].is_image_present(imageId):\n                    anchore_print(\"\\timage (\"+str(imageId)+\") already exists in DB, skipping import.\")\n                else:\n                    #imagelist.append(patt.group(1))\n                    thefile = os.path.join(imgdir, ifile)\n                    with open(thefile, 'r') as FH:\n                        imagedata = json.loads(FH.read())\n                    try:\n                        rc = contexts['anchore_db'].save_image_new(imageId, report=imagedata)\n                        if not rc:\n                            contexts['anchore_db'].delete_image(imageId)\n                            raise Exception(\"save to anchore DB failed\")\n                    except Exception as err:\n                        contexts['anchore_db'].delete_image(imageId)\n                        raise err\n\n                    thedir = os.path.join(storedir, imageId)\n                    if os.path.exists(thedir):\n                        for namespace in os.listdir(thedir):\n                            thefile = os.path.join(thedir, namespace, \"stored_files.tar.gz\")\n                            if os.path.exists(thefile):\n                                contexts['anchore_db'].save_files_tarfile(imageId, namespace, thefile)\n\n                    anchore_print(\"\\timage (\"+str(imageId)+\") imported.\")\n\n        anchore_print(\"importing feeds...\")\n        thefile = os.path.join(feeddir, \"feedmeta.json\")\n        with open(thefile, 'r') as FH:\n            feedmeta = json.loads(FH.read())\n\n        if feedmeta:\n            contexts['anchore_db'].save_feedmeta(feedmeta)\n\n        for feed in feedmeta:\n            feedobj = feedmeta[feed]\n            for group in feedobj['groups']:\n                groupobj = feedobj['groups'][group]\n                datafiles = groupobj.pop('datafiles', [])\n                for datafile in datafiles:\n                    thedir = os.path.join(feeddir, feed, group)\n                    thefile = os.path.join(thedir, datafile)\n                    if not os.path.exists(thefile):\n                        pass\n                    else:\n                        with open(thefile, 'r') as FH:\n                            contexts['anchore_db'].save_feed_group_data(feed, group, datafile, json.loads(FH.read()))\n                    anchore_print(\"\\tfeed (\"+feed+\" \" + group + \" \" + datafile + \") imported\")\n\n        #TODO import stored files\n\n    except Exception as err:\n        anchore_print_err(\"operation failed: \" + str(err))\n        ecode = 1\n\n    sys.exit(ecode)", "response": "Import a previously exported anchore DB"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconsumes the next token of the type expected_type and return the type found and the updated buffer with the found token removed", "response": "def next_token(expected_type, data):\n    \"\"\"\n    Based on the expected next type, consume the next token returning the type found and an updated buffer with the found token\n    removed\n\n    :param expected_type:\n    :param data:\n    :return: (TokenType, str) tuple where TokenType is the type of the next token expected\n\n    \"\"\"\n    next_data = copy.copy(data)\n\n    next_type = TokenType.INVALID\n\n    if len(next_data) == 0 or next_data[0] == None:\n        next_type = TokenType.END\n    elif (expected_type == TokenType.DIGIT or expected_type == TokenType.DIGIT_OR_ZERO) and next_data[0].isalpha():\n        next_type = TokenType.LETTER\n    elif expected_type == TokenType.LETTER and next_data[0].isdigit():\n        next_type = TokenType.DIGIT\n    elif expected_type == TokenType.SUFFIX and next_data[0].isdigit():\n        next_type = TokenType.SUFFIX_NO\n    else:\n        if next_data[0] == '.':\n            next_type = TokenType.DIGIT_OR_ZERO\n        elif next_data[0] == '_':\n            next_type = TokenType.SUFFIX\n        elif next_data[0] == '-':\n            if len(next_data) > 1 and next_data[1] == 'r':\n                next_type = TokenType.REVISION_NO\n                # Pop leading char off\n                next_data = next_data[1:]\n            else:\n                next_type = TokenType.INVALID\n\n        next_data = next_data[1:]\n\n    if next_type < expected_type:\n        if not ((next_type == TokenType.DIGIT_OR_ZERO and expected_type == TokenType.DIGIT) or\n                (next_type == TokenType.SUFFIX and expected_type == TokenType.SUFFIX_NO) or\n                (next_type == TokenType.DIGIT and expected_type == TokenType.LETTER)):\n            next_type = TokenType.INVALID\n\n    return next_type, next_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive the expected token type and the data return the next token and the updated string.", "response": "def get_token(expected_type, data):\n    \"\"\"\n    Given the expected token type and the data, grab the next token and return a tuple of (token, token_type, updated_string)\n    :param expected_type: Parts enum value for tthe expected type\n    :param data: the str from which to pull next token starting at the beginning\n    :return: (token, token_type, new working str)\n    \"\"\"\n\n    token_value = i = 0\n    next_token_type = TokenType.INVALID\n    d_len = len(data)\n\n    if len(data) <= 0:\n        return 0, TokenType.END, data\n\n    if expected_type == TokenType.DIGIT_OR_ZERO and data[i] == '0':\n        # Handled leading zeros\n        while i < d_len and data[i] == '0':\n            i += 1\n        next_token_type = TokenType.DIGIT\n        token_value = -i\n    elif expected_type in [TokenType.DIGIT_OR_ZERO, TokenType.DIGIT, TokenType.SUFFIX_NO, TokenType.REVISION_NO]:\n        # Handle numbers\n        dig_val = ''\n        while i < d_len and data[i].isdigit():\n            dig_val += data[i]\n            i += 1\n        token_value = int(dig_val) if dig_val else 0\n    elif expected_type == TokenType.LETTER:\n        # Handle letter values\n        token_value = ord(data[i])\n        i += 1\n    elif expected_type == TokenType.SUFFIX:\n        idx = -1\n        # Is this is a pre-release suffix?\n        for s in pre_release_suffixes:\n            i = len(s)\n            if data.startswith(s):\n                idx = pre_release_suffixes.index(s)\n                break\n\n        if idx >= 0:\n            token_value = idx - len(pre_release_suffixes)\n        else:\n            idx = -1\n            # Try post-release suffixes for a match\n            for s in post_release_suffixes:\n                i = len(s)\n                if data.startswith(s):\n                    idx = post_release_suffixes.index(s)\n                    break\n            if idx < 0:\n                # No match found\n                return -1, TokenType.INVALID, data\n            else:\n                token_value = idx\n    else:\n        return -1, TokenType.INVALID, data\n\n    data = data[i:]\n    if len(data) == 0:\n        next_token_type = TokenType.END\n    elif next_token_type != TokenType.INVALID:\n        pass\n    else:\n        next_token_type, data = next_token(expected_type, data)\n\n    return token_value, next_token_type, data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_version_relationship(ver_str1, ver_str2):\n\n    # Expect first type to be a digit, per Gentoo spec (used by apk)\n    v1_type = TokenType.DIGIT\n    v2_type = TokenType.DIGIT\n    v1_tok = 0\n    v2_tok = 0\n\n    if ver_str1 is None and ver_str2 is None:\n        return ComparisonResult.equal_to\n\n    if ver_str1 is None and ver_str2 is not None:\n        return ComparisonResult.less_than\n\n    if ver_str1 is not None and ver_str2 is None:\n        return ComparisonResult.greater_than\n\n    # Find either the end of one string or the first invalid token of first non-equal token pair.\n    while v1_type == v2_type and v1_type != TokenType.END and v1_type != TokenType.INVALID and v1_tok == v2_tok:\n        v1_tok, v1_type, ver_str1 = get_token(v1_type, ver_str1)\n        v2_tok, v2_type, ver_str2 = get_token(v2_type, ver_str2)\n\n    # Check the value of the current token\n    if v1_tok < v2_tok:\n        return ComparisonResult.less_than\n\n    if v1_tok > v2_tok:\n        return ComparisonResult.greater_than\n\n    if v1_type == v2_type:\n        return ComparisonResult.equal_to\n\n    if v1_type == TokenType.SUFFIX and get_token(v1_type, ver_str1)[0] < 0:\n        return ComparisonResult.less_than\n\n    if v2_type == TokenType.SUFFIX and get_token(v2_type, ver_str2)[0] < 0:\n        return ComparisonResult.greater_than\n\n    if v1_type > v2_type:\n        return ComparisonResult.less_than\n\n    if v2_type > v1_type:\n        return ComparisonResult.greater_than\n\n    return ComparisonResult.equal_to", "response": "This function returns the comparison of alpine package version numbers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query(anchore_config, image, imagefile, include_allanchore, module):\n\n    global config, imagelist, nav\n    ecode = 0\n    success = True\n    config = anchore_config\n\n    if module:\n        if image and imagefile:\n            raise click.BadOptionUsage('Can only use one of --image, --imagefile')\n\n        try:\n            imagedict = build_image_list(anchore_config, image, imagefile, not (image or imagefile), include_allanchore)\n            imagelist = imagedict.keys()\n\n            try:\n                ret = anchore_utils.discover_imageIds(imagelist)\n            except ValueError as err:\n                raise err\n            else:\n                #imagelist = ret.keys()\n                imagelist = ret\n\n        except Exception as err:\n            anchore_print_err(\"could not load input images\")\n            sys.exit(1)\n\n    try:\n        nav = init_nav_contexts()\n\n        result = nav.run_query(list(module))\n        if result:\n            anchore_utils.print_result(config, result)\n\n        if nav.check_for_warnings(result):\n            ecode = 2\n\n    except:\n        anchore_print_err(\"query operation failed\")\n        ecode = 1\n\n    contexts['anchore_allimages'].clear()\n    sys.exit(ecode)", "response": "Query an anchore archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a given file and applies defaults as specified to any values not present in the file. Returns a new dict with the merged values.", "response": "def load_and_merge(file_path=None, defaults=None):\n    \"\"\"\n    Load a given file and apply defaults as specified to any values not present in the file.\n    Does a merge of the file and defaults and returns a dict with the results\n    :param file_path: file to load. Supports .yaml and .json files\n    :param defaults: dict with defaults in correct structure to compare and overlay with the values from the file\n    :return: a new dict with the merged results\n    \"\"\"\n\n    ret = None\n    if defaults is not None:\n        ret = defaults.copy()\n\n    file_data = None\n    if file_path is not None:\n        with open(file_path) as f:\n            if file_path.endswith('.yaml') or file_path.endswith('.yml'):\n                file_data = yaml.safe_load(f)\n            elif file_path.endswith('.json'):\n                file_data = json.load(f.read())\n    else:\n        file_data = {}\n\n    if file_data is not None:\n        if ret is not None:\n            ret.update(file_data)\n            return ret\n        else:\n            return file_data\n    else:\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show(feed):\n    ecode = 0\n    try:\n        feedmeta = anchore_feeds.load_anchore_feedmeta()\n        if feed in feedmeta:\n            result = {}\n            groups = feedmeta[feed].get('groups',{}).values()\n            result['name'] = feed\n            result['access_tier'] = int(feedmeta[feed].get('access_tier'))\n            result['description'] = feedmeta[feed].get('description')\n            result['groups'] = {}\n            if 'subscribed' not in feedmeta[feed]:\n                result['subscribed'] = False\n            else:\n                result['subscribed'] = feedmeta[feed]['subscribed']\n\n            for g in groups:\n                result['groups'][g['name']] = {\n                    'access_tier': int(g.get('access_tier')),\n                    'description': g.get('description'),\n                    'last_sync': datetime.datetime.fromtimestamp(g.get('last_update')).isoformat() if 'last_update' in g else 'None'\n                }\n\n            anchore_print(result, do_formatting=True)\n        else:\n            anchore_print_err('Unknown feed name. Valid feeds can be seen withe the \"list\" command')\n            ecode = 1\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)", "response": "Show detailed feed information"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(showgroups):\n\n    ecode = 0\n    try:\n        result = {}\n        subscribed = {}\n        available = {}\n        unavailable = {}\n        current_user_data = contexts['anchore_auth']['user_info']\n        feedmeta = anchore_feeds.load_anchore_feedmeta()\n\n        for feed in feedmeta.keys():\n            if feedmeta[feed]['subscribed']:\n                subscribed[feed] = {}\n                subscribed[feed]['description'] = feedmeta[feed]['description']\n                if showgroups:\n                    subscribed[feed]['groups'] = feedmeta[feed]['groups'].keys()\n\n            else:\n                if current_user_data:\n                    tier = int(current_user_data['tier'])\n                else:\n                    tier = 0\n\n                if int(feedmeta[feed]['access_tier']) > tier:\n                    collection = unavailable\n                else:\n                    collection = available\n\n                collection[feed] = {}\n\n                collection[feed]['description'] = feedmeta[feed]['description']\n                if showgroups and collection == available:\n                    collection[feed]['groups'] = feedmeta[feed]['groups'].keys()\n\n        if available:\n            result['Available'] = available\n        if subscribed:\n            result['Subscribed'] = subscribed\n        if unavailable:\n            result['Unavailable/Insufficient Access Tier'] = unavailable\n\n        anchore_print(result, do_formatting=True)\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)", "response": "Show list of Anchore data feeds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsubscribing to the specified feeds.", "response": "def sub(feednames):\n    \"\"\"\n    Subscribe to the specified feed(s).\n    \"\"\"\n\n    ecode = 0\n    current_user_data = contexts.get('anchore_auth', {}).get('user_info', None)\n    if not current_user_data:\n        current_user_tier = 0\n    else:\n        current_user_tier = int(current_user_data['tier'])\n\n    try:\n        for feed in feednames:\n            rc, msg = anchore_feeds.subscribe_anchore_feed(feed, current_user_tier)\n            if not rc:\n                ecode = 1\n                anchore_print_err(msg)\n            else:\n                anchore_print(msg)\n\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unsub(feednames, delete, dontask):\n\n    ecode = 0\n    try:\n        for feed in feednames:\n            rc, msg = anchore_feeds.unsubscribe_anchore_feed(feed)\n            if not rc:\n                ecode = 1\n                anchore_print_err(msg)\n            else:\n                anchore_print(msg)\n                if delete:\n                    dodelete = False\n                    if dontask:\n                        dodelete = True\n                    else:\n                        try:\n                            answer = raw_input(\"Really delete feed data (\"+str(feed)+\"'? (y/N)\")\n                        except:\n                            answer = \"n\"\n                        if 'y' == answer.lower():\n                            dodelete = True\n                        else:\n                            anchore_print(str(feed) + \": skipping delete.\")\n\n                    if dodelete:\n                        anchore_print(str(feed) + \": deleting feed.\")\n                        rc = anchore_feeds.delete_anchore_feed(feed)\n\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)", "response": "Unsubscribe from the specified feeds."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sync(since, do_compact):\n\n    ecode = 0\n    try:\n        rc, ret = anchore_feeds.sync_feedmeta()\n        if not rc:\n            anchore_print_err(ret['text'])\n            ecode = 1\n        else:\n            rc, ret = anchore_feeds.sync_feeds(force_since=since, do_combine=do_compact)\n            if not rc:\n                anchore_print_err(ret['text'])\n                ecode = 1\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)", "response": "Syncs the latest data for all subscribed feeds from the Anchore service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning an anchore gate on the specified image.", "response": "def gate(anchore_config, force, image, imagefile, include_allanchore, editpolicy, rmpolicy, listpolicy, updatepolicy, policy, run_bundle, bundlefile, usetag, resultsonly, show_gatehelp, show_policytemplate, whitelist, global_whitelist, show_triggerids, show_whitelisted):\n    \"\"\"\n    Runs gate checks on the specified image(s) or edits the image's gate policy.\n    The --editpolicy option is only valid for a single image.\n\n    The --image and --imagefile options are mutually exclusive.\n\n    Image IDs can be specified as hash ids, repo names (e.g. centos), or tags (e.g. centos:latest).\n    \"\"\"\n\n    ecode = 0\n    success = True\n\n    # special option, does not need any image inputs\n    if show_gatehelp:        \n        try:\n            gate_info = anchore_utils.discover_gates()\n            anchore_print(gate_info, do_formatting=True)\n        except Exception as err:\n            anchore_print_err(\"operation failed: \" + str(err))\n            sys.exit(1)\n        sys.exit(0)\n\n    if show_policytemplate:\n        try:\n            outstr = \"\\n\"\n            gate_info = anchore_utils.discover_gates()\n            for g in gate_info.keys():\n                for t in gate_info[g].keys():\n                    params = list()\n                    if 'params' in gate_info[g][t] and gate_info[g][t]['params'] and gate_info[g][t]['params'].lower() != 'none':\n                        for p in gate_info[g][t]['params'].split(','):\n                            params.append(p+\"=<a,b,c>\")\n                        \n                    outstr += ':'.join([g, t, \"<STOP|WARN|GO>\", ' '.join(params)]) + \"\\n\"\n            \n            anchore_print(outstr, do_formatting=False)\n        except Exception as err:\n            anchore_print_err(\"operation failed: \" + str(err))\n            sys.exit(1)\n        sys.exit(0)\n\n    # the rest require some form of image(s) be given as input\n    if image and imagefile:\n        raise click.BadOptionUsage('Can only use one of --image, --imagefile')\n\n    if policy and (editpolicy or whitelist or listpolicy or updatepolicy or rmpolicy):\n        raise click.BadOptionUsage('Cannot use other policy options when --policy <file> is specified.')\n\n    if (policy and run_bundle):\n        raise click.BadOptionUsage('Cannot use both --policy and --run_bundle at the same time.')\n\n    if (run_bundle and (editpolicy or whitelist or listpolicy or updatepolicy or rmpolicy)):\n        raise click.BadOptionUsage('Cannot use other policy options when --run_bundle is specified.')\n\n    if (run_bundle and (usetag and resultsonly)):\n        raise click.BadOptionUsage('Cannot use --resultsonly if --usetag is specified.')\n\n    if (run_bundle and (usetag and not image)):\n        raise click.BadOptionUsage('Cannot specify --usetag unless gating a single image (using --image)')\n\n    try:\n        imagedict = build_image_list(anchore_config, image, imagefile, not (image or imagefile), include_allanchore)\n        imagelist = imagedict.keys()\n        inputimagelist = list(imagelist)\n\n        try:\n            ret = anchore_utils.discover_imageIds(imagelist)\n        except ValueError as err:\n            raise err\n        else:\n            imagelist = ret\n\n    except Exception as err:\n        anchore_print_err(\"could not load any images\")\n        sys.exit(1)\n\n    try:\n        con = controller.Controller(anchore_config=anchore_config, imagelist=imagelist, allimages=contexts['anchore_allimages'], force=force)\n    except Exception as err:\n        anchore_print_err(\"gate operation failed\")\n        ecode = 1\n    else:\n        if editpolicy:\n            if not con.editpolicy():\n                ecode = 1\n        elif whitelist:\n            if not con.editwhitelist():\n                ecode = 1\n        elif rmpolicy:\n            if not con.rmpolicy():\n                ecode = 1;\n            else:\n                anchore_print(\"policies successfully removed.\", do_formatting=True)\n        elif updatepolicy:\n            if not con.updatepolicy(updatepolicy):\n                ecode = 1;\n            else:\n                anchore_print(\"policies successfully updated.\", do_formatting=True)\n        elif listpolicy:\n            result = con.listpolicy()\n            record = {}\n            if not result:\n                ecode = 1\n            else:\n                try:\n                    for imageId in result.keys():\n                        record[imageId] = list()\n                        pol = result[imageId]\n                        for gate in pol.keys():\n                            for trigger in pol[gate].keys():\n                                if str(pol[gate][trigger]['params']):\n                                    outstr = \":\".join([gate, trigger, str(pol[gate][trigger]['action']), str(pol[gate][trigger]['params'])])\n                                else:\n                                    outstr = \":\".join([gate, trigger, str(pol[gate][trigger]['action'])])\n                                record[imageId].append(outstr)\n                    if record:\n                        anchore_print(record, do_formatting=True)\n                except Exception as err:\n                    anchore_print_err(\"failed to list policies: \" + str(err))\n                    ecode = 1\n        elif run_bundle:\n            try:\n                if not anchore_policy.check():\n                    anchore_print_err(\"run-bundle specified, but it appears as though no policy bundles have been synced yet: run 'anchore policybundle sync' to get your latest bundles from anchore.io\")\n                    ecode = 1\n                else:\n                    bundle = anchore_policy.load_policymeta(policymetafile=bundlefile)\n                    if not bundle:\n                        raise Exception(\"could not load stored bundle - run 'anchore policybundle sync' and try again\")\n\n                    bundleId = bundle['id']\n                    \n                    inputimage = inputimagelist[0]\n\n                    allresults = {}\n                    for inputimage in inputimagelist:\n                        result, image_ecode = anchore_policy.run_bundle(anchore_config=anchore_config, image=inputimage, matchtags=usetag, bundle=bundle, show_whitelisted=show_whitelisted, show_triggerIds=show_triggerids)\n                        allresults.update(result)\n\n                        if image_ecode == 1:\n                            ecode = 1\n                        elif ecode == 0 and image_ecode > ecode:\n                            ecode = image_ecode\n\n                    if not resultsonly:\n                        if anchore_config.cliargs['json']:\n                            anchore_print(json.dumps(allresults))\n                        else:\n                            for image in allresults.keys():\n                                for gate_result in allresults[image]['evaluations']:\n                                    _logger.info(\"Image=\"+image + \" BundleId=\"+bundleId+\" Policy=\"+gate_result['policy_name']+\" Whitelists=\"+str(gate_result['whitelist_names']))\n                                    anchore_utils.print_result(anchore_config, gate_result['results'])\n                    else:\n                        final_result = {}\n                        for image in allresults.keys():\n                            for gate_result in allresults[image]['evaluations']:\n                                final_result.update(gate_result['results'])\n                        anchore_utils.print_result(anchore_config, final_result)\n            except Exception as err:\n                anchore_print_err(\"failed to run gates\")\n                ecode = 1\n\n        else:\n            try:\n                # run the gates\n                result = con.run_gates(policy=policy, global_whitelist=global_whitelist, show_triggerIds=show_triggerids, show_whitelisted=show_whitelisted)\n                if result:\n                    anchore_utils.print_result(anchore_config, result)\n                    success = True\n                    ecode = con.result_get_highest_action(result)\n            except Exception as err:\n                anchore_print_err(\"failed to run gates\")\n                ecode = 1\n\n    contexts['anchore_allimages'].clear()\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nanalyze the specified image and returns a list of images that can be used to build the image.", "response": "def analyze(anchore_config, force, image, imagefile, include_allanchore, dockerfile, imagetype, skipgates, layerstrategy, excludefile):\n    \"\"\"\n    Invokes the anchore analyzer on the specified image(s).\n\n    To include multiple images use the --imagefile, no option, or --include-allanchore options.\n    To exclude specific images from analysis, use the --excludefile option.\n\n    One of --imagetype or --dockerfile should be supplied for an analysis run. Use --dockerfile whenever possible as the inclusion\n    of the dockerfile for an image associates the dockerfile and image for later use in queries etc. The --dockerfile option\n    is only valid in combination with the --image option.  If neither --dockerfile and --imagetype is supplied, then \n\n    When using --imagetype, use 'none' to specify that the image(s) is an unknown or user image and use 'base' to specify\n    that the image(s) are approved base images to be used to build other images or it is useful to mark the image one from which\n    other images are meant to be derived.\n\n    Image IDs can be specified as hash ids, repo names (e.g. centos), or tags (e.g. centos:latest).\n\n    \"\"\"\n\n    success = True\n    ecode = 0\n\n    args = {}\n\n    if image and imagefile:\n        raise click.BadOptionUsage('Can only use one of --image, --imagefile')\n\n    if dockerfile and not image:\n        raise click.BadOptionUsage('Must specify --image option when using --dockerfile option')\n\n    if not imagefile:\n        if imagetype:\n            if imagetype == \"anchorebase\":\n                args['anchorebase'] = True\n            elif imagetype == \"base\":\n                args['isbase'] = True\n            elif imagetype == \"none\":\n                pass\n            else:\n                raise click.BadOptionUsage(\"Invalid imagetype specified: valid types are 'none' or 'base'\")\n\n    try:\n        imagedict = build_image_list(anchore_config, image, imagefile, not (image or imagefile), include_allanchore, exclude_file=excludefile, dockerfile=dockerfile)\n        imagelist = imagedict.keys()\n\n        try:\n            ret = anchore_utils.discover_imageIds(imagelist)\n        except ValueError as err:\n            raise err\n        else:\n            #imagelist = ret.keys()\n            imagelist = ret\n\n    except Exception as err:\n        anchore_print_err(\"could not load any images\")\n        ecode = 1\n    else:\n\n        step = 1\n        count = 0\n        allimages = {}\n        success = True\n        for imageId in imagedict.keys():\n\n            if count % step == 0:\n                allimages.clear()\n                allimages = {}\n                count = 0\n\n            args.update({'dockerfile': imagedict[imageId]['dockerfile'], 'skipgates': skipgates, 'selection_strategy': layerstrategy})\n\n            inlist = [imageId]\n            try:\n                anchore_print(\"Analyzing image: \" + imageId)\n                rc = analyzer.Analyzer(anchore_config=anchore_config, imagelist=inlist, allimages=allimages, force=force, args=args).run()\n                if not rc:\n                    anchore_print_err(\"analysis failed.\")\n                    success = False\n                    ecode = 1\n\n            except:\n                anchore_print_err('failed to run analyzer')\n                allimages.clear()\n                success = False\n                ecode = 1\n                break\n\n            count = count + 1\n\n        allimages.clear()\n\n        if not success:\n            anchore_print_err(\"analysis failed for one or more images.\")\n            ecode = 1\n\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nevaluates the strategy for the given family tree and return a dict of images to analyze that match the strategy", "response": "def evaluate_familytree(self, family_tree, image_set):\n        \"\"\"\n        Evaluate strategy for the given family tree and return a dict of images to analyze that match the strategy\n\n        :param family_tree: the family tree to traverse and evaluate\n        :param image_set: list of all images in the context\n        :return:\n        \"\"\"\n\n        if family_tree is None or image_set is None:\n            raise ValueError('Cannot execute analysis strategy on None image or image with no familytree data')\n\n        toanalyze = OrderedDict()\n        tree_len = len(family_tree)\n        for i in family_tree:\n            image = image_set[i]\n            if self._should_analyze_image(image, family_tree.index(i), tree_len):\n                toanalyze[image.meta['imageId']] = image\n\n        return toanalyze"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_mapping_actions(image=None, imageId=None, in_digests=[], bundle={}):\n\n    if not image or not bundle:\n        raise Exception(\"input error\")\n\n    if not verify_policy_bundle(bundle=bundle):\n        raise Exception(\"input bundle does not conform to bundle schema\")\n\n    ret = []\n    \n    image_infos = []\n\n    image_info = anchore_utils.get_all_image_info(image)\n    if image_info and image_info not in image_infos:\n        image_infos.append(image_info)\n\n    for m in bundle['mappings']:\n        polname = m['policy_id']\n        wlnames = m['whitelist_ids']\n\n        for image_info in image_infos:\n            #_logger.info(\"IMAGE INFO: \" + str(image_info))\n            ii = {}\n            ii.update(image_info)\n            registry = ii.pop('registry', \"N/A\")\n            repo = ii.pop('repo', \"N/A\")\n\n            tags = []\n            fulltag = ii.pop('fulltag', \"N/A\")\n            if fulltag != 'N/A':\n                tinfo = anchore_utils.parse_dockerimage_string(fulltag)\n                if 'tag' in tinfo and tinfo['tag']:\n                    tag = tinfo['tag']\n\n            for t in [image, fulltag]:\n                tinfo = anchore_utils.parse_dockerimage_string(t)\n                if 'tag' in tinfo and tinfo['tag'] and tinfo['tag'] not in tags:\n                    tags.append(tinfo['tag'])\n\n            digest = ii.pop('digest', \"N/A\")\n            digests = [digest]\n            for d in image_info['digests']:\n                dinfo = anchore_utils.parse_dockerimage_string(d)\n                if 'digest' in dinfo and dinfo['digest']:\n                    digests.append(dinfo['digest'])\n                                \n            p_ids = []\n            p_names = []\n            for p in bundle['policies']:\n                p_ids.append(p['id'])\n                p_names.append(p['name'])\n\n            wl_ids = []\n            wl_names = []\n            for wl in bundle['whitelists']:\n                wl_ids.append(wl['id'])\n                wl_names.append(wl['name'])\n                \n            if polname not in p_ids:\n                _logger.info(\"policy not in bundle: \" + str(polname))\n                continue\n\n            skip=False\n            for wlname in wlnames:\n                if wlname not in wl_ids:\n                    _logger.info(\"whitelist not in bundle\" + str(wlname))\n                    skip=True\n            if skip:\n                continue\n\n            mname = m['name']\n            mregistry = m['registry']\n            mrepo = m['repository']\n            if m['image']['type'] == 'tag':\n                mtag = m['image']['value']\n                mdigest = None\n                mimageId = None\n            elif m['image']['type'] == 'digest':\n                mdigest = m['image']['value']\n                mtag = None\n                mimageId = None\n            elif m['image']['type'] == 'id':\n                mimageId = m['image']['value']\n                mtag = None\n                mdigest = None\n            else:\n                mtag = mdigest = mimageId = None\n\n            mregistry_rematch = mregistry\n            mrepo_rematch = mrepo\n            mtag_rematch = mtag\n            try:\n                matchtoks = []\n                for tok in mregistry.split(\"*\"):\n                    matchtoks.append(re.escape(tok))\n                mregistry_rematch = \"^\" + '(.*)'.join(matchtoks) + \"$\"\n\n                matchtoks = []\n                for tok in mrepo.split(\"*\"):\n                    matchtoks.append(re.escape(tok))\n                mrepo_rematch = \"^\" + '(.*)'.join(matchtoks) + \"$\"\n\n                matchtoks = []\n                for tok in mtag.split(\"*\"):\n                    matchtoks.append(re.escape(tok))\n                mtag_rematch = \"^\" + '(.*)'.join(matchtoks) + \"$\"\n            except Exception as err:\n                _logger.error(\"could not set up regular expression matches for mapping check - exception: \" + str(err))\n\n            _logger.debug(\"matchset: \" + str([mregistry_rematch, mrepo_rematch, mtag_rematch]) + \" : \" + str([mregistry, mrepo, mtag]) + \" : \" + str([registry, repo, tag, tags]))\n\n            if registry == mregistry or mregistry == '*' or re.match(mregistry_rematch, registry):\n                _logger.debug(\"checking mapping for image (\"+str(image_info)+\") match.\")\n\n                if repo == mrepo or mrepo == '*' or re.match(mrepo_rematch, repo):\n                    doit = False\n                    matchstring = mname + \": N/A\"\n                    if tag:\n                        if False and (mtag == tag or mtag == '*' or mtag in tags or re.match(mtag_rematch, tag)):\n                            matchstring = mname + \":\" + ','.join([mregistry, mrepo, mtag])\n                            doit = True\n                        else:\n                            for t in tags:\n                                if re.match(mtag_rematch, t):\n                                    matchstring = mname + \":\" + ','.join([mregistry, mrepo, mtag])\n                                    doit = True\n                                    break\n                    if not doit and (digest and (mdigest == digest or mdigest in in_digests or mdigest in digests)):\n                        matchstring = mname + \":\" + ','.join([mregistry, mrepo, mdigest])\n                        doit = True\n                    \n                    if not doit and (imageId and (mimageId == imageId)):\n                        matchstring = mname + \":\" + ','.join([mregistry, mrepo, mimageId])\n                        doit = True\n\n                    matchstring = matchstring.encode('utf8')\n                    if doit:\n                        _logger.debug(\"match found for image (\"+str(image_info)+\") matchstring (\"+str(matchstring)+\")\")\n\n                        wldata = []\n                        wldataset = set()\n                        for wlname in wlnames:\n                            wldataset = set(list(wldataset) + extract_whitelist_data(bundle, wlname))\n                        wldata = list(wldataset)\n\n                        poldata = extract_policy_data(bundle, polname)\n                        \n                        wlnames.sort()\n                        evalstr = ','.join([polname] + wlnames)\n                        evalhash = hashlib.md5(evalstr).hexdigest()\n                        ret.append( ( poldata, wldata, polname,wlnames, matchstring, m, evalhash) )\n                        return(ret)\n                    else:\n                        _logger.debug(\"no match found for image (\"+str(image_info)+\") match.\")\n                else:\n                    _logger.debug(\"no match found for image (\"+str(image_info)+\") match.\")\n\n    return(ret)", "response": "Returns a list of policy_data whitelist_data policy_name whitelist_names matchstring mapping_rule_json obj and evalhash for each policy_id in in_digests."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef login(anchore_config, user, passfile):\n    config = anchore_config\n    ecode = 0\n\n    try:\n        anchore_creds_file = os.path.join(anchore_config.config_dir, 'anchore_creds.json')\n        anchore_stored_username = None\n        anchore_stored_password = None\n        if os.path.exists(anchore_creds_file):\n            try:\n                with open(anchore_creds_file, 'r') as FH:\n                    anchore_stored_creds = json.loads(FH.read())\n                    anchore_stored_username = anchore_stored_creds.pop('username', None)\n                    anchore_stored_password = anchore_stored_creds.pop('password', None)\n            except Exception as err:\n                raise err\n\n        if user:\n            anchore_print(\"Using user from cmdline option: \" + str(user))\n            username = user\n        elif os.getenv('ANCHOREUSER'):\n            anchore_print(\"Using user from environment (ANCHOREUSER)\")\n            username = os.getenv('ANCHOREUSER')\n        elif anchore_stored_username:\n            anchore_print(\"Using stored username from anchore_creds.json\")\n            username = anchore_stored_username\n        else:\n            username = raw_input(\"Username: \")\n\n        if passfile:\n            anchore_print(\"Using password from cmdline option: \" + str(passfile))\n            with open(passfile, \"r\") as FH:\n                password = FH.read().strip()\n        elif os.getenv('ANCHOREPASS'):\n            anchore_print(\"Using password from environment (ANCHOREPASS)\")\n            password = os.getenv('ANCHOREPASS')\n        elif anchore_stored_password:\n            anchore_print(\"Using stored password from anchore_creds.json\")\n            password = anchore_stored_password\n        else:\n            password = getpass.getpass(\"Password: \")\n            \n        aa = contexts['anchore_auth']\n\n        new_anchore_auth = anchore_auth.anchore_auth_init(username, password, aa['auth_file'], aa['client_info_url'], aa['token_url'], aa['conn_timeout'], aa['max_retries'])\n        rc, ret = anchore_auth.anchore_auth_refresh(new_anchore_auth)\n        if not rc:\n            anchore_print(\"Failed to log in: check your username/password and try again!\")\n            raise Exception(\"Login failure - message from server: \" + str(ret['text']))\n        else:\n            contexts['anchore_auth'].update(new_anchore_auth)\n            anchore_print(\"Login successful.\")\n\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)", "response": "Log into Anchore using your username and password from anchore. io."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlogging out of Anchore service", "response": "def logout(anchore_config):\n    \"\"\"\n    Log out of Anchore service\n    \"\"\"\n    ecode = 0\n    try:\n        aa = contexts['anchore_auth']\n        if aa:\n            anchore_auth.anchore_auth_invalidate(aa)\n            if 'auth_file' in aa:\n                os.remove(aa['auth_file'])\n        print \"Logout successful.\"\n    except Exception as err:\n        anchore_print_err('operation failed')\n        ecode = 1\n\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshow user data for current user if available", "response": "def whoami(anchore_config):\n    \"\"\"\n    Show user data for current user if available\n    :param anchore_config:\n    :return:\n    \"\"\"\n    ecode = 0\n    try:\n        aa = contexts['anchore_auth']\n        if aa and 'username' in aa and 'password' in aa:\n            info = {'Current user': aa['user_info'] if aa['user_info'] else 'anonymous'}\n\n            anchore_print(info, do_formatting=True)\n        else:\n            anchore_print_err('No anchore auth context found. Cannot get user info. Try logging in first')\n            ecode = 1\n\n    except Exception as err:\n        anchore_print_err('Cannot get user info')\n        ecode = 1\n\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def _create_redis_pool(*args, **kwargs):\n\n        if StrictVersion(aioredis.__version__) >= StrictVersion('1.0.0'):  # pragma no cover\n            return await aioredis.create_redis_pool(*args, **kwargs)\n        else:  # pragma no cover\n            return await aioredis.create_pool(*args, **kwargs)", "response": "A wrapper for aioredis. create_redis_pool."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconnecting to the instance of the AIOHTTP server.", "response": "async def connect(self):\n        \"\"\"\n        Get an connection for the self instance\n        \"\"\"\n\n        if isinstance(self.connection, dict):\n            # a dict like {'host': 'localhost', 'port': 6379,\n            #              'db': 0, 'password': 'pass'}\n            kwargs = self.connection.copy()\n            address = (\n                kwargs.pop('host', 'localhost'),\n                kwargs.pop('port', 6379)\n            )\n            redis_kwargs = kwargs\n        elif isinstance(self.connection, aioredis.Redis):\n            self._pool = self.connection\n        else:\n            # a tuple or list ('localhost', 6379)\n            # a string \"redis://host:6379/0?encoding=utf-8\" or\n            # a unix domain socket path \"/path/to/redis.sock\"\n            address = self.connection\n            redis_kwargs = {}\n\n        if self._pool is None:\n            async with self._lock:\n                if self._pool is None:\n                    self.log.debug('Connecting %s', repr(self))\n                    self._pool = await self._create_redis_pool(\n                        address, **redis_kwargs,\n                        minsize=1, maxsize=100)\n\n        return await self._pool"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncloses connection and resets pool", "response": "async def close(self):\n        \"\"\"\n        Closes connection and resets pool\n        \"\"\"\n        if self._pool is not None and not isinstance(self.connection, aioredis.Redis):\n            self._pool.close()\n            await self._pool.wait_closed()\n        self._pool = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets a lock on a resource.", "response": "async def set_lock(self, resource, lock_identifier, lock_timeout):\n        \"\"\"\n        Lock this instance and set lock expiration time to lock_timeout\n        :param resource: redis key to set\n        :param lock_identifier: uniquie id of lock\n        :param lock_timeout: timeout for lock in seconds\n        :raises: LockError if lock is not acquired\n        \"\"\"\n\n        lock_timeout_ms = int(lock_timeout * 1000)\n\n        try:\n            with await self.connect() as redis:\n                await redis.eval(\n                    self.set_lock_script,\n                    keys=[resource],\n                    args=[lock_identifier, lock_timeout_ms]\n                )\n        except aioredis.errors.ReplyError as exc:  # script fault\n            self.log.debug('Can not set lock \"%s\" on %s',\n                           resource, repr(self))\n            raise LockError('Can not set lock') from exc\n        except (aioredis.errors.RedisError, OSError) as exc:\n            self.log.error('Can not set lock \"%s\" on %s: %s',\n                           resource, repr(self), repr(exc))\n            raise LockError('Can not set lock') from exc\n        except asyncio.CancelledError:\n            self.log.debug('Lock \"%s\" is cancelled on %s',\n                           resource, repr(self))\n            raise\n        except Exception as exc:\n            self.log.exception('Can not set lock \"%s\" on %s',\n                               resource, repr(self))\n            raise\n        else:\n            self.log.debug('Lock \"%s\" is set on %s', resource, repr(self))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nunlocking this instance :param resource: redis key to set :param lock_identifier: uniquie id of lock :raises: LockError if the lock resource acquired with different lock_identifier", "response": "async def unset_lock(self, resource, lock_identifier):\n        \"\"\"\n        Unlock this instance\n        :param resource: redis key to set\n        :param lock_identifier: uniquie id of lock\n        :raises: LockError if the lock resource acquired with different lock_identifier\n        \"\"\"\n        try:\n            with await self.connect() as redis:\n                await redis.eval(\n                    self.unset_lock_script,\n                    keys=[resource],\n                    args=[lock_identifier]\n                )\n        except aioredis.errors.ReplyError as exc:  # script fault\n            self.log.debug('Can not unset lock \"%s\" on %s',\n                           resource, repr(self))\n            raise LockError('Can not unset lock') from exc\n        except (aioredis.errors.RedisError, OSError) as exc:\n            self.log.error('Can not unset lock \"%s\" on %s: %s',\n                           resource, repr(self), repr(exc))\n            raise LockError('Can not set lock') from exc\n        except asyncio.CancelledError:\n            self.log.debug('Lock \"%s\" unset is cancelled on %s',\n                           resource, repr(self))\n            raise\n        except Exception as exc:\n            self.log.exception('Can not unset lock \"%s\" on %s',\n                               resource, repr(self))\n            raise\n        else:\n            self.log.debug('Lock \"%s\" is unset on %s', resource, repr(self))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def is_locked(self, resource):\n\n        with await self.connect() as redis:\n            lock_identifier = await redis.get(resource)\n        if lock_identifier:\n            return True\n        else:\n            return False", "response": "Checks if the resource is locked by any redlock instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nattempt to set the lock on all the redis instances with the given resource and lock_identifier. Returns the elapsed time that took to lock the instances with the given resource.", "response": "async def set_lock(self, resource, lock_identifier):\n        \"\"\"\n        Tries to set the lock to all the redis instances\n\n        :param resource: The resource string name to lock\n        :param lock_identifier: The id of the lock. A unique string\n        :return float: The elapsed time that took to lock the instances\n            in seconds\n        :raises: LockError if the lock has not been set to at least (N/2 + 1)\n            instances\n        \"\"\"\n        start_time = time.time()\n        lock_timeout = self.lock_timeout\n\n        successes = await asyncio.gather(*[\n            i.set_lock(resource, lock_identifier, lock_timeout) for\n            i in self.instances\n        ], return_exceptions=True)\n        successful_sets = sum(s is None for s in successes)\n\n        elapsed_time = time.time() - start_time\n        locked = True if successful_sets >= int(len(self.instances) / 2) + 1 else False\n\n        self.log.debug('Lock \"%s\" is set on %d/%d instances in %s seconds',\n                       resource, successful_sets, len(self.instances), elapsed_time)\n\n        if not locked:\n            raise LockError('Can not acquire the lock \"%s\"' % resource)\n\n        return elapsed_time"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the lock from all the redis instances.", "response": "async def unset_lock(self, resource, lock_identifier):\n        \"\"\"\n        Tries to unset the lock to all the redis instances\n\n        :param resource: The resource string name to lock\n        :param lock_identifier: The id of the lock. A unique string\n        :return float: The elapsed time that took to lock the instances in iseconds\n        :raises: LockError if the lock has not matching identifier in more then\n            (N/2 - 1) instances\n        \"\"\"\n        start_time = time.time()\n\n        successes = await asyncio.gather(*[\n            i.unset_lock(resource, lock_identifier) for\n            i in self.instances\n        ], return_exceptions=True)\n        successful_remvoes = sum(s is None for s in successes)\n\n        elapsed_time = time.time() - start_time\n        unlocked = True if successful_remvoes >= int(len(self.instances) / 2) + 1 else False\n\n        self.log.debug('Lock \"%s\" is unset on %d/%d instances in %s seconds',\n                       resource, successful_remvoes, len(self.instances), elapsed_time)\n\n        if not unlocked:\n            raise LockError('Can not release the lock')\n\n        return elapsed_time"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def is_locked(self, resource):\n\n        successes = await asyncio.gather(*[\n            i.is_locked(resource) for\n            i in self.instances\n        ], return_exceptions=True)\n        successful_sets = sum(s is True for s in successes)\n\n        locked = True if successful_sets >= int(len(self.instances) / 2) + 1 else False\n\n        return locked", "response": "Checks if the resource is locked by any redlock instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shell(ipython_args):\n    import IPython\n    from IPython.terminal.ipapp import load_default_config\n    from traitlets.config.loader import Config\n    from flask.globals import _app_ctx_stack\n\n    app = _app_ctx_stack.top.app\n\n    if 'IPYTHON_CONFIG' in app.config:\n        config = Config(app.config['IPYTHON_CONFIG'])\n    else:\n        config = load_default_config()\n\n    config.TerminalInteractiveShell.banner1 = '''Python %s on %s\nIPython: %s\nApp: %s [%s]\nInstance: %s''' % (sys.version,\n                   sys.platform,\n                   IPython.__version__,\n                   app.import_name,\n                   app.env,\n                   app.instance_path)\n\n    IPython.start_ipython(\n        argv=ipython_args,\n        user_ns=app.make_shell_context(),\n        config=config,\n    )", "response": "Runs an interactive Python shell in the app context."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _clean(value):\n    if isinstance(value, np.ndarray):\n        if value.dtype.kind == 'S':\n            return np.char.decode(value).tolist()\n        else:\n            return value.tolist()\n    elif type(value).__module__ == np.__name__:\n        # h5py==2.8.0 on windows sometimes fails to cast this from an np.float64 to a python.float\n        # We have explicitly cast in Albacore (merge 488) to avoid this bug, since casting here could be dangerous\n        # https://github.com/h5py/h5py/issues/1051\n        conversion = np.asscalar(value)\n        if sys.version_info.major == 3 and isinstance(conversion, bytes):\n            conversion = conversion.decode()\n        return conversion\n    elif sys.version_info.major == 3 and isinstance(value, bytes):\n        return value.decode()\n    else:\n        return value", "response": "Convert numpy numeric types to their python equivalents."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the prior alignment that was used for 2D basecalling.", "response": "def get_prior_alignment(self):\n        \"\"\" Return the prior alignment that was used for 2D basecalling.\n\n        :return: Alignment data table.\n        \"\"\"\n        data_group = '{}/HairpinAlign'.format(self.group_name)\n        data = self.handle.get_analysis_dataset(data_group, 'Alignment')\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the alignment and model states from the 2D basecall.", "response": "def get_2d_call_alignment(self):\n        \"\"\" Return the alignment and model_states from the 2D basecall.\n\n        :return: Alignment data table.\n        \"\"\"\n        data_group = '{}/BaseCalled_2D'.format(self.group_name)\n        data = self.handle.get_analysis_dataset(data_group, 'Alignment')\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the alignment and model_state table to be written.", "response": "def add_2d_call_alignment(self, data):\n        \"\"\" Add the alignment and model_state data table..\n        \n        :param data: Alignment and model_state table to be written.\n        \"\"\"\n        path = 'Analyses/{}'.format(self.group_name)\n        if 'BaseCalled_2D' not in self.handle.handle[path]:\n            self.handle.add_analysis_subgroup(self.group_name, 'BaseCalled_2D')\n\n        path = '{}/BaseCalled_2D'.format(self.group_name)\n        self.handle.add_analysis_dataset(path, 'Alignment', data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_called_sequence(self, section=None, fastq=False):\n        if section != \"2D\":\n            warnings.warn(\"Basecall2DTools.get_called_sequence() should specify section='2D'\", DeprecationWarning)\n            # Backwards compatibilty to 0.3.3, if no \"2D\" section, bump args by 1 and pass to super\n            if section == None:\n                # We assume that a named arg or no-arg was given\n                return super(Basecall2DTools, self).get_called_sequence(\"2D\", fastq)\n            # We assume that a single unnamed arg was given for fastq\n            return super(Basecall2DTools, self).get_called_sequence(\"2D\", section)\n        return super(Basecall2DTools, self).get_called_sequence(section, fastq)", "response": "Return either the called sequence data or the full sequence string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_results(self):\n        summary = self.handle.get_summary_data(self.group_name)\n        results = {'template': {'status': 'no data'},\n                   'complement': {'status': 'no data'},\n                   '2d': {'status': 'no data'}}\n        if 'genome_mapping_template' in summary:\n            results['template'] = self._get_results(summary['genome_mapping_template'])\n        if 'genome_mapping_complement' in summary:\n            results['complement'] = self._get_results(summary['genome_mapping_complement'])\n        if 'genome_mapping_2d' in summary:\n            results['2d'] = self._get_results(summary['genome_mapping_2d'])\n        return results", "response": "Get the details about the alignment that have been performed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_alignment_data(self, section):\n        subgroup = '{}/Aligned_{}'.format(self.group_name, section)\n        sam = self.handle.get_analysis_dataset(subgroup, 'SAM')\n        fasta = self.handle.get_analysis_dataset(subgroup, 'Fasta')\n        if sam is None or fasta is None:\n            return None\n        sequence = fasta.split('\\n')[1]\n        return sam, sequence", "response": "Get the alignment data for the specified section."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_alignment_data(self, section, sam, sequence):\n        subgroup = 'Aligned_{}'.format(section)\n        if not subgroup in self.handle.handle['Analyses/{}'.format(self.group_name)]:\n            self.handle.add_analysis_subgroup(self.group_name, subgroup)\n        sam_arr = np.array(sam, dtype=str)\n        self.handle.add_analysis_dataset('{}/{}'.format(self.group_name, subgroup), 'SAM', sam_arr)\n        fasta_arr = np.array('>{}\\n{}\\n'.format(section, sequence), dtype=str)\n        self.handle.add_analysis_dataset('{}/{}'.format(self.group_name, subgroup), 'Fasta', fasta_arr)", "response": "Add the SAM and Fasta alignment data for a basecall."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the speed of a specific section of the fast5 file.", "response": "def calculate_speed(self, section, alignment_results=None):\n        \"\"\" Calculate speed using alignment information.\n\n        :param section: The section (template or complement) we're calculating\n            speed for.\n        :param alignment_results: Optional dictionary of the alignment summary,\n            so that speed can be calculated without having to write the summary\n            out to the fast5 file first.\n        :return: Speed in bases per second or zero if the speed could not be\n            calculated.\n\n        The only reliable way we have of finding out how many bases have gone through the pore is by\n        looking at how much of the reference the sequence aligned to. This takes that information and\n        uses it to calculate speed in reference-bases-per-second.\n        \"\"\"\n        speed = 0.0\n        if alignment_results:\n            results = self._get_results(alignment_results)\n        else:\n            results = self.get_results()[section]\n        if results['status'] != 'match found':\n            return 0.0\n        ref_span = results['ref_span']\n        ref_len = ref_span[1] - ref_span[0]\n        seq_span = results['seq_span']\n        seq_len = seq_span[1] - seq_span[0]\n        total_len = results['seq_len']\n\n        sample_rate = self.handle.get_channel_info()['sampling_rate']\n\n        # We need the duration from the segmentation results\n        chain = self.handle.get_chain(self.group_name)\n        if chain is not None:\n            segmentation_group = dict(chain).get('segmentation')\n        else:\n            segmentation_group = None\n        duration = 0\n        if segmentation_group is not None:\n            with SegmentationTools(self.handle, group_name=segmentation_group) as seg:\n                summary = seg.get_results()\n                if summary is not None:\n                    duration = summary['duration_{}'.format(section)]\n        if duration == 0:\n            return 0.0\n\n        normalized_duration = duration * seq_len / float(total_len)\n        speed = sample_rate * ref_len / normalized_duration\n        return speed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_raw_data(self, data, attrs):\n        self.assert_writeable()\n        if \"Raw\" not in self.handle:\n            self.handle.create_group(\"Raw\")\n        if \"Signal\" in self.handle['Raw']:\n            msg = \"Fast5 file already has raw data for read '{}' in: {}\"\n            raise KeyError(msg.format(self.read_id, self.filename))\n        self.handle['Raw'].create_dataset('Signal', data=data, compression='gzip', shuffle=True, dtype='i2')\n        self._add_attributes(\"Raw\", attrs, clear=True)", "response": "Add raw data for a read."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd channel info data to the channel_id group.", "response": "def add_channel_info(self, attrs, clear=False):\n        \"\"\" Add channel info data to the channel_id group.\n\n        :param data: A dictionary of key/value pairs. Keys must be strings.\n            Values can be strings or numeric values.\n        :param clear: If set, any existing channel info data will be removed.\n        \"\"\"\n        self.assert_writeable()\n        if 'channel_id' not in self.handle:\n            self.handle.create_group('channel_id')\n        self._add_attributes('channel_id', attrs, clear)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_analysis(self, component, group_name, attrs, config=None):\n        if \"Analyses\" not in self.handle:\n            self.handle.create_group(\"Analyses\")\n        super(Fast5Read, self).add_analysis(component, group_name, attrs, config)", "response": "Add an analysis group to the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_event_data(self, section):\n        event_group = '{}/BaseCalled_{}'.format(self.group_name, section)\n        data = self.handle.get_analysis_dataset(event_group, 'Events')\n        return data", "response": "Return either the template or complement event data if present."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_event_data(self, section, data):\n        event_group = 'BaseCalled_{}'.format(section)\n        if not event_group in self.handle.handle['Analyses/{}'.format(self.group_name)]:\n            self.handle.add_analysis_subgroup(self.group_name, event_group)\n        self.handle.add_analysis_dataset('{}/{}'.format(self.group_name, event_group), 'Events', data)", "response": "Add template or complement basecalled event data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning either the called sequence data or the single multiline fastq string.", "response": "def get_called_sequence(self, section, fastq=False):\n        \"\"\" Return either the called sequence data, if present.\n        \n        :param section: ['template', 'complement' or '2D']\n        :param fastq: If True, return a single, multiline fastq string. If\n            False, return a tuple of (name, sequence, qstring).\n        :return: Either the fastq string or the (name, sequence, qstring) tuple.\n        \"\"\"\n\n        event_group = '{}/BaseCalled_{}'.format(self.group_name, section)\n        data = self.handle.get_analysis_dataset(event_group, 'Fastq')\n        if data is None:\n            raise KeyError(\"No fastq data in: {} {}\".format(event_group, self.filename))\n        if fastq:\n            return data\n        name, sequence, _, qstring = data.strip().split('\\n')\n        name = name[1:]\n        return name, sequence, qstring"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding called sequence to the data.", "response": "def add_called_sequence(self, section, name, sequence, qstring):\n        \"\"\" Add basecalled sequence data\n        \n        :param section: ['template', 'complement' or '2D']\n        :param name: The record ID to use for the fastq.\n        :param sequence: The called sequence.\n        :param qstring: The quality string.\n        \"\"\"\n        event_group = 'BaseCalled_{}'.format(section)\n        if not event_group in self.handle.handle['Analyses/{}'.format(self.group_name)]:\n            self.handle.add_analysis_subgroup(self.group_name, event_group)\n        fastq_text = '@{}\\n{}\\n+\\n{}\\n'.format(name, sequence, qstring)\n        fastq_arr = np.array(fastq_text, dtype=str)\n        self.handle.add_analysis_dataset('{}/{}'.format(self.group_name, event_group), 'Fastq', fastq_arr)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting a Strand object to the stream.", "response": "def write_strand(self, strand):\n        \"\"\" Writes a Strand object to the stream. \"\"\"\n        if strand['channel'] != self._current_channel \\\n           or self._strand_counter == self._reads_per_file:\n            self._start_new_file(strand)\n        fname = self._write_strand(strand)\n        self._index.write('{}\\t{}\\t{}\\t{}\\n'.format(strand['channel'],\n                                                    strand['read_attrs']['read_number'],\n                                                    self._current_file, fname))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the event data for the specified read.", "response": "def set_event_data(self, data, read_attrs):\n        \"\"\" Set event data with the specied attributes.\n        \n        :param data: Event data table.\n        :param read_attrs: Attributes to put on the read group. This must include\n            the read_number, which must refer to a read present in the object. The\n            attributes should not include the standard read attributes:\n\n             * read_id\n             * start_time\n             * duration\n             * start_mux\n\n            Those will be pulled from the read information already present in the\n            object for the specified read.\n        \"\"\"\n        if self.handle.mode == 'r':\n            raise Exception('File is not open for writing.')\n        read_number = read_attrs['read_number']\n        read_group = '{}/Reads/Read_{}'.format(self.group_name, read_number)\n        read_info = self.handle.status.read_info\n        read_number_map = self.handle.status.read_number_map\n        index = read_number_map.get(read_number)\n        if index is None:\n            raise Exception('Cannot add event detection data for a read that does not exist.')\n        info = read_info[index]\n        read_attrs.update({'read_id': info.read_id,\n                           'start_time': info.start_time,\n                           'duration': info.duration,\n                           'start_mux': info.start_mux,\n                           'median_before': info.median_before})\n        attrs = self.handle.get_analysis_attributes(read_group)\n        if attrs is None:\n            self.handle.add_analysis_subgroup(self.group_name, 'Reads/Read_{}'.format(read_number),\n                                              attrs=read_attrs)\n            self.handle.add_analysis_dataset(read_group, 'Events', data)\n        else:\n            raise Exception('Event detection data already exists for this analysis and read.')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the event data for the specified read.", "response": "def get_event_data(self, read_number=None, time_in_seconds=False):\n        \"\"\" Get event data for the specified (or only) read.\n        \n        :param read_number: The read number to grab event data for. If this\n            is None, and there is only one read, it will grab event data for\n            that read.\n        :param time_in_seconds: If True, this will convert (if necessary) the\n            start and length fields from samples to seconds. If they are already\n            in seconds, this option has no effect.\n        :return: A tuple containing the event data, and the read attributes.\n        \"\"\"\n        read_info = self.handle.status.read_info\n        if read_number is None:\n            if len(read_info) != 1:\n                raise Exception('Must specify a read number if there is not exactly 1 read.')\n            read_number = read_info[0].read_number\n        else:\n            read_numbers = [info.read_number for info in read_info]\n            if read_number not in read_numbers:\n                raise Exception('Specified read does not exist.')\n        group = '{}/Reads/Read_{}'.format(self.group_name, read_number)\n        attrs = self.handle.get_analysis_attributes(group)\n        dataset = self.handle.get_analysis_dataset(group, 'Events', proxy=True)\n        if dataset is None:\n            raise Exception('Read number {} has no event data.'.format(read_number))\n        if time_in_seconds and dataset['start'].dtype.kind in ['i', 'u']:\n            channel_info = self.handle.get_channel_info()\n            sample_size = 1.0 / channel_info['sampling_rate']\n            descr = [(x[0], 'float64') if x[0] in ('start', 'length') else x\n                     for x in dataset.dtype.descr]\n            with dataset.astype(np.dtype(descr)):\n                data = dataset[()]\n            data['start'] *= sample_size\n            data['length'] *= sample_size\n        else:\n            data = dataset[()]\n        return data, attrs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has_event_data(self, read_number=None):\n        read_info = self.handle.status.read_info\n        if read_number is None:\n            if len(read_info) != 1:\n                raise Exception('Must specify a read number if there is not exactly 1 read.')\n            read_number = read_info[0].read_number\n        else:\n            read_numbers = [info.read_number for info in read_info]\n            if read_number not in read_numbers:\n                raise Exception('Specified read does not exist.')\n        group = '{}/Reads/Read_{}'.format(self.group_name, read_number)\n        dataset = self.handle.get_analysis_dataset(group, 'Events', proxy=True)\n        return dataset is not None", "response": "Returns True if the specified read has event data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_results(self):\n        summary = self._get_summary_data()\n        if summary is None:\n            results = {'has_template': False,\n                       'has_complement': False}\n        else:\n            results = {}\n            if 'has_template' in summary:\n                results['has_template'] = bool(summary['has_template'])\n            else:\n                results['has_template'] = True if summary['num_temp'] > 0 else False\n            if 'has_complement' in summary:\n                results['has_complement'] = bool(summary['has_complement'])\n            else:\n                results['has_complement'] = True if summary['num_comp'] > 0 else False\n            need_raw_info = False\n            if results['has_template']:\n                if 'start_index_temp' in summary:\n                    summary['start_event_template'] = summary['start_index_temp']\n                    summary['end_event_template'] = summary['end_index_temp']\n                if 'first_sample_template' not in summary:\n                    need_raw_info = True\n            if results['has_complement']:\n                if 'start_index_comp' in summary:\n                    summary['start_event_complement'] = summary['start_index_comp']\n                    summary['end_event_complement'] = summary['end_index_comp']\n                if 'first_sample_complement' not in summary:\n                    need_raw_info = True\n            if need_raw_info:\n                self._get_raw_info(summary)\n            if results['has_template']:\n                results['first_sample_template'] = summary['first_sample_template']\n                results['duration_template'] = summary['duration_template']\n                if 'start_event_template' in summary:\n                    results['start_event_template'] = summary['start_event_template']\n                    results['end_event_template'] = summary['end_event_template']\n            if results['has_complement']:\n                results['first_sample_complement'] = summary['first_sample_complement']\n                results['duration_complement'] = summary['duration_complement']\n                if 'start_event_complement' in summary:\n                    results['start_event_complement'] = summary['start_event_complement']\n                    results['end_event_complement'] = summary['end_event_complement']\n        return results", "response": "Returns the segmentation summary data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the event data for the specified section.", "response": "def get_event_data(self, section, time_in_seconds=False):\n        \"\"\" Get the template or complement event data.\n        \n        :param section: Either template, complement, or both.\n        :param time_in_seconds: Return the start and length fields\n            in seconds, rather than samples.\n        :return: The event dataset for the section. If section=both\n            then it returns a tuple with both sections. Returns None\n            if the section does not exist.\n        \"\"\"\n        if section not in ['template', 'complement', 'both']:\n            raise Exception('Unrecognized section: {} Expected: \"template\", \"complement\" or \"both\"'.format(section))\n        results = self.get_results()\n        if results is None:\n            return None, None if section is 'both' else None\n        if section == 'both':\n            sections = ['template', 'complement']\n        else:\n            sections = [section]\n        evdet_group, _ = self._find_event_data()\n        with EventDetectionTools(self.handle, group_name=evdet_group) as evdet:\n            event_data, _ = evdet.get_event_data(time_in_seconds=time_in_seconds)\n        datasets = [None, None]\n        for n, this_section in enumerate(sections):\n            if not results['has_{}'.format(this_section)]:\n                continue\n            ev1 = results['start_event_{}'.format(this_section)]\n            ev2 = results['end_event_{}'.format(this_section)]\n            datasets[n] = event_data[ev1:ev2]\n        if section == 'both':\n            return tuple(datasets)\n        return datasets[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_raw_data(self, section, scale=False):\n        results = self.get_results()\n        datasets = [None, None]\n        if section == 'both':\n            sections = ['template', 'complement']\n        else:\n            sections = [section]\n        for n, this_section in enumerate(sections):\n            if not results['has_{}'.format(this_section)]:\n                continue\n            start = results['first_sample_{}'.format(this_section)]\n            dur = results['duration_{}'.format(this_section)]\n            datasets[n] = self.handle.get_raw_data(start=start, end=start+dur, scale=scale)\n        if section == 'both':\n            return tuple(datasets)\n        return datasets[0]", "response": "Get the raw data for the specified section."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary of tracking - id key - value pairs.", "response": "def get_tracking_id(self):\n        \"\"\" Returns a dictionary of tracking-id key/value pairs.\n        \"\"\"\n        self.assert_open()\n        tracking = self.handle[self.global_key +'tracking_id'].attrs.items()\n        tracking = {key: _clean(value) for key, value in tracking}\n        return tracking"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_tracking_id(self, data, clear=False):\n        self.assert_writeable()\n        self._add_attributes(self.global_key + 'tracking_id', data, clear)\n        return", "response": "Add tracking - id data to the tracking - id group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of channel information key value pairs.", "response": "def get_channel_info(self):\n        \"\"\" Returns a dictionary of channel information key/value pairs.\n        \"\"\"\n        self.assert_open()\n        channel_info = self.handle[self.global_key + 'channel_id'].attrs.items()\n        channel_info = {key: _clean(value) for key, value in channel_info}\n        channel_info['channel_number'] = int(channel_info['channel_number'])\n        return channel_info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding channel info data to the channel_id group.", "response": "def add_channel_info(self, data, clear=False):\n        \"\"\" Add channel info data to the channel_id group.\n        \n        :param data: A dictionary of key/value pairs. Keys must be strings.\n            Values can be strings or numeric values.\n        :param clear: If set, any existing channel info data will be removed. \n        \"\"\"\n        self.assert_writeable()\n        self._add_attributes(self.global_key + 'channel_id', data, clear)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of context tag key value pairs.", "response": "def get_context_tags(self):\n        \"\"\" Returns a dictionary of context tag key/value pairs.\n        \"\"\"\n        self.assert_open()\n        if self.has_context_tags:\n            tags = self.handle[self.global_key + 'context_tags'].attrs.items()\n            return {key: _clean(value) for key, value in tags}\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreplace any existing context tag data with the provided values.", "response": "def add_context_tags(self, data, clear=False):\n        \"\"\" Replaces any existing context tag data with the provided values.\n        \n        :param data: A dictionary of key/value pairs. Keys must be strings.\n            Values can be strings or numeric values.\n        :param clear: If set, any existing context tag data will be removed. \n        \"\"\"\n        self.assert_writeable()\n        if self.has_context_tags:\n            self._add_attributes(self.global_key + 'context_tags', data, clear)\n        else:\n            self._add_group(self.global_key + 'context_tags', data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets raw data from the Fast5 file.", "response": "def get_raw_data(self, read_number=None, start=None, end=None, scale=False):\n        \"\"\" Pull raw data from the file.\n        \n        :param read_number: The read number you want raw data from. Pass None\n            if there is only one read, and will grab that read's raw data.\n        :param start: The first sample to pull. Default is to pull from the\n            beginning.\n        :param end: One past the last sample to pull. Default is to pull until\n            the end.\n        :param scale: If set, the returned data will be scaled floating point\n            values, in pA. Otherwise raw DAQ values are returned as 16 bit\n            integers.\n        :returns: Raw data as either 32 bit floats, or 16 bit integers.\n        \"\"\"\n        self.assert_open()\n        if read_number is None:\n            read_number = self._get_only_read_number()\n        read_index = self.status.read_number_map[read_number]\n        if not self.status.read_info[read_index].has_raw_data:\n            msg = 'Fast5 file has no raw data for read {} in {}'.format(read_number, self.filename)\n            raise KeyError(msg)\n        if end is None:\n            end = self.status.read_info[read_index].duration\n        if start is None:\n            start = 0\n        dataset_name = 'Raw/Reads/Read_{}/Signal'.format(read_number)\n        return self._load_raw(dataset_name, start, end, scale)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding raw data for a read.", "response": "def add_raw_data(self, read_number, data):\n        \"\"\" Add raw data for a read.\n        \n        :param read_number: The number of the read the raw data is for.\n        :param data: The raw data DAQ values (16 bit integers).\n        \n        The read must already exist in the file. It must not already\n        have raw data.\n        \"\"\"\n        self.assert_writeable()\n        read_index = self.status.read_number_map[read_number]\n        if self.status.read_info[read_index].has_raw_data:\n            msg = 'Fast5 file already has raw data for read {} in {}'\n            raise KeyError(msg.format(read_number, self.filename))\n        self._save_raw(read_number, data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_analyses(self, component=None):\n        self.assert_open()\n        analyses = []\n        if 'Analyses' not in self.handle:\n            return analyses\n        ana_groups = self.handle['Analyses'].keys()\n        for group_name in ana_groups:\n            group_attrs = self.handle['Analyses/{}'.format(group_name)].attrs\n            if 'component' in group_attrs:\n                comp = _clean(group_attrs['component'])\n            elif group_name[:-4] in LEGACY_COMPONENT_NAMES:\n                comp = LEGACY_COMPONENT_NAMES[group_name[:-4]]\n            else:\n                # We don't know anything about this component!\n                comp = None\n            if comp is not None and (component is None or comp == component):\n                analyses.append((comp, group_name))\n        return analyses", "response": "Provides a list of all analyses groups."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_chain(self, group_name, component_map):\n        self.assert_writeable()\n\n        for component, path in component_map.items():\n            if not path.startswith('Analyses/'):\n                path = 'Analyses/{}'.format(path)\n            component_map[component] = path\n\n        self.add_analysis_attributes(group_name, component_map)", "response": "Adds the chain of components to the group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the component and group names for the last step in the analysis chain e. g. Basecall_1D_000.", "response": "def get_chain(self, group_name):\n        \"\"\" Provides the component and group names for an analysis chain.\n        \n        :param group_name: The group name of the last step in the analysis\n            chain e.g. 'Basecall_1D_000'\n        :returns: A list of component-name/group-name pairs (tuples). This\n            will include each component of the chain, in order.\n        \"\"\"\n        self.assert_open()\n        endgroup = 'Analyses/{}'.format(group_name)\n        attr = self.handle[endgroup].attrs\n        if 'component' in attr:\n            component = attr['component']\n        else:\n            component = LEGACY_COMPONENT_NAMES[group_name[:-4]]\n        chain = deque()\n        chain.append((component, group_name))\n        groups_to_check = deque()\n        groups_to_check.append(endgroup)\n        while len(groups_to_check) > 0:\n            group = groups_to_check.popleft()\n            attr = self.handle[group].attrs\n            for key, value in attr.items():\n                if str(value).startswith('Analyses/'):\n                    chain_entry = (key, value[9:])\n                    # We need to maintain the order of the components, so\n                    # we'll move any we see again to the end of the chain.\n                    if chain_entry in chain:\n                        chain.remove(chain_entry)\n                    chain.append(chain_entry)\n                    groups_to_check.append(value)\n        return list(chain)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new read to the file.", "response": "def add_read(self, read_number, read_id, start_time, duration, mux, median_before):\n        \"\"\" Add a new read to the file.\n\n        :param read_number: The read number to assign to the read.\n        :param read_id: The unique read-id for the read.\n        :param start_time: The start time (in samples) of the read.\n        :param duration: The duration (in samples) of the read.\n        :param mux: The mux set at the time of the read.\n        :param median_before: The median level of the data before the read.\n\n        Note that most tools assume a file contains only one read.\n        Putting multiple reads into a file severely limits the\n        ability to operate on those reads with standard tools.\n        \"\"\"\n        self.assert_writeable()\n        read_info = ReadInfo(read_number, read_id, start_time, duration, mux=mux, median_before=median_before)\n        self.status.read_info.append(read_info)\n        n = len(self.status.read_info) - 1\n        self.status.read_number_map[read_number] = n\n        self.status.read_id_map[read_id] = n\n        group_name = 'Raw/Reads/Read_{}'.format(read_number)\n        attrs = {'read_number': read_number,\n                 'read_id': read_id,\n                 'start_time': start_time,\n                 'duration': duration,\n                 'start_mux': mux,\n                 'median_before': median_before}\n        self._add_group(group_name, attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new analysis group to the file.", "response": "def add_analysis(self, component, group_name, attrs, config=None):\n        \"\"\" Add a new analysis group to the file.\n        \n        :param component: The component name.\n        :param group_name: The name to use for the group. Must not already\n            exist in the file e.g. 'Test_000'.\n        :param attrs: A dictionary containing the key-value pairs to\n            put in the analysis group as attributes. Keys must be strings,\n            and values must be strings or numeric types.\n        :param config: A dictionary of dictionaries. The top level keys\n            should be the name of analysis steps, and should contain\n            key value pairs for analysis parameters used.\n        \"\"\"\n        self.assert_writeable()\n        group = 'Analyses/{}'.format(group_name)\n        cfg_group = '{}/Configuration'.format(group)\n        group_attrs = attrs.copy()\n        group_attrs['component'] = component\n        self._add_group(group, group_attrs)\n        self.handle[group].create_group('Summary')\n        self.handle[group].create_group('Configuration')\n        if config is not None:\n            self._add_attribute_tree(cfg_group, config)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a log to the file.", "response": "def add_log(self, group_name, field_name, log_string):\n        \"\"\" Add a log the file.\n        :param group_name: Global group name. Cave: No 'Analyses' is prepended.\n        :param field_name: A field {group_name}/{field_name} will be written.\n        :param log_string: content to be written.\n        \"\"\"\n        self.assert_writeable()\n        if group_name not in self.handle:\n            self._add_group(group_name, {})\n        sanitized_data = _sanitize_data_for_writing(log_string)\n        self.handle[group_name].create_dataset(field_name, data=sanitized_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_summary_data(self, group_name, section_name, data):\n        self.assert_writeable()\n        group = 'Analyses/{}/Summary/{}'.format(group_name, section_name)\n        self._add_group(group, data)", "response": "Set the summary data for an analysis group."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the configuration data for the analysis group.", "response": "def set_analysis_config(self, group_name, config):\n        \"\"\" Set Configuration data for analysis group.  The ``config`` can\n            be passed as a dict of dicts e.g. ``{'section': {'key': 'value'}}``\n            or can be passed directly as a ConfigParser object.\n\n        :param group_name: The name of the analysis group e.g. Example_000\n        :param config: Representation of configuration as ConfigParser or\n            dict of dicts.\n        :raises KeyError: if group_name does not exist in fast5 or\n            fast5 is not open for writing\n        :raises TypeError: if ``config`` is not ConfigParser or dict obj.\n        \"\"\"\n        self.assert_writeable()\n        if 'Analyses/{}'.format(group_name) not in self.handle:\n            msg = 'Dataset cannot be added to non-existent group: Analyses/{} in {}'\n            raise KeyError(msg.format(group_name, self.filename))\n        if isinstance(config, ConfigParser):\n            config_dict = {}\n            for section in config.sections():\n                config_dict[section] = {k: v for k, v in config.items(section)}\n        elif isinstance(config, dict):\n            config_dict = config\n        else:\n            raise TypeError('config must be a ConfigParser or dict not {}'.format(type(config)))\n\n        config_path = 'Analyses/{}/Configuration'.format(group_name)\n        self._add_attribute_tree(config_path, config_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget any config data saved for the analysis.", "response": "def get_analysis_config(self, group_name):\n        \"\"\" Gets any config data saved for the analysis.\n\n        :param group_name: The name of the analysis group.\n        :returns: A dictionary of dictionaries. Each key represents\n            an analysis step. Each value is a dictionary containing the\n            analysis parameters as key/value pairs. Returns None if no\n            configuration exists for the analysis.\n        \"\"\"\n        self.assert_open()\n        group = 'Analyses/{}/Configuration'.format(group_name)\n        config = None\n        if group in self.handle:\n            config = self._parse_attribute_tree(group)\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_summary_data(self, group_name):\n        self.assert_open()\n        group = 'Analyses/{}/Summary'.format(group_name)\n        summary = None\n        if group in self.handle:\n            summary = self._parse_attribute_tree(group)\n        return summary", "response": "Get the summary data for an analysis group."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new subgroup to an existing analysis group.", "response": "def add_analysis_subgroup(self, group_name, subgroup_name, attrs=None):\n        \"\"\" Add a new subgroup to an existing analysis group.\n        \n        :param group_name: The name of the analysis group you are adding to.\n        :param subgroup_name: The name of the new subgroup.\n        :param attrs: A dictionary representing the attributes to assign to\n            the subgroup.\n        \n        The new subgroup must not already exist.\n        \n        The subgroup name can be a nested name, such as \"Template/Data\". This\n        will create the \"Template\" subgroup (if it does not exist), and the\n        \"Data\" subgroup below it.\n        \"\"\"\n        self.assert_writeable()\n        group = 'Analyses/{}/{}'.format(group_name, subgroup_name)\n        self._add_group(group, attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_analysis_attributes(self, group_name, attrs, clear=False):\n        self.assert_writeable()\n        group = 'Analyses/{}'.format(group_name)\n        self._add_attributes(group, attrs, clear)", "response": "Add attributes to the group or dataset specified."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the attributes for the specified group or dataset.", "response": "def get_analysis_attributes(self, group_name):\n        \"\"\" Returns the attributes for the specified group or dataset.\n        \n        :param group_name: The path of the group or dataset, relative to the\n            \"Analyses\" group.\n        :returns: A dictionary representing the attributes (if any).\n        \"\"\"\n        self.assert_open()\n        group = 'Analyses/{}'.format(group_name)\n        attr = None\n        if group in self.handle:\n            attr = self.handle[group].attrs.items()\n            attr = {key: _clean(value) for key, value in attr}\n        return attr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_analysis_dataset(self, group_name, dataset_name, data, attrs=None):\n        self.assert_writeable()\n        group_path = 'Analyses/{}'.format(group_name)\n        if group_path not in self.handle:\n            msg = 'Dataset cannot be added to non-existent group: Analyses/{} in {}'\n            raise KeyError(msg.format(group_name, self.filename))\n\n        sanitized_data = _sanitize_data_for_writing(data)\n        if np.shape(sanitized_data) == ():  # We can't compress scalar datasets\n            self.handle[group_path].create_dataset(dataset_name,\n                                                   data=sanitized_data)\n        else:\n            self.handle[group_path].create_dataset(dataset_name,\n                                                   data=sanitized_data,\n                                                   compression='gzip')\n        if attrs is not None:\n            path = '{}/{}'.format(group_path, dataset_name)\n            self._add_attributes(path, attrs)", "response": "Add a new dataset to the specified group."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_analysis_dataset(self, group_name, dataset_name, proxy=False):\n        self.assert_open()\n        dataset_name = 'Analyses/{}/{}'.format(group_name, dataset_name)\n        data = None\n        if dataset_name in self.handle:\n            data = self.handle[dataset_name]\n            if not proxy:\n                data = _sanitize_data_for_reading(data[()])\n        return data", "response": "Returns the data for the specified analysis dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates a fast5 file from an older version to the new standard.", "response": "def update_legacy_file(fname):\n        \"\"\" Update a fast5 file from an older version to the new standard.\n        \n        :param fname: The filename of the fast5 file.\n        \"\"\"\n        status = Fast5Info(fname)\n        if not status.valid:\n            raise IOError('Cannot update invalid file: {}'.format(fname))\n        with h5py.File(fname, 'r+') as handle:\n            # Add Raw/Read/Read_## groups for reads if they are missing.\n            for read_info in status.read_info:\n                read_group_name = 'Raw/Reads/Read_{}'.format(read_info.read_number)\n                if read_group_name in handle:\n                    rgh = handle[read_group_name]\n                else:\n                    rgh = handle.create_group(read_group_name)\n                rgh.attrs['read_number'] = read_info.read_number\n                rgh.attrs['read_id'] = read_info.read_id\n                rgh.attrs['duration'] = read_info.duration\n                rgh.attrs['start_time'] = read_info.start_time\n                rgh.attrs['start_mux'] = read_info.start_mux\n\n            # Add the Analyses and tracking_id groups, if they are missing.\n            if not 'Analyses' in handle:\n                handle.create_group('Analyses')\n            if not 'tracking_id' in handle['UniqueGlobalKey']:\n                handle.create_group('UniqueGlobalKey/tracking_id')\n\n            # Update the EventDetection_000 created by old versions of MinKNOW, if needed.\n            if status.version < 1.1:\n                if 'Analyses/EventDetection_000' in handle:\n                    reads_group = handle['Analyses/EventDetection_000/Reads']\n                    data_group_names = reads_group.keys()\n                    for data_group_name in data_group_names:\n                        read_group = reads_group[data_group_name]\n                        read_number = read_group.attrs['read_number']\n                        read_info = status.read_info[status.read_number_map[read_number]]\n                        read_group.attrs['read_id'] = read_info.read_id\n                        if 'Events' in read_group:\n                            dataset = read_group['Events']\n                            if 'variance' in dataset.dtype.names:\n                                old_data = read_group['Events'][()]\n                                new_data = np.empty(old_data.size, dtype=[('mean', float), ('stdv', float),\n                                                                          ('start', int), ('length', int)])\n                                new_data[:]['mean'] = old_data['mean']\n                                new_data[:]['stdv'] = np.sqrt(old_data['variance'])\n                                new_data[:]['start'] = old_data['start']\n                                new_data[:]['length'] = old_data['length']\n                                del read_group['Events']\n                                read_group.create_dataset('Events', data=new_data, compression='gzip')\n\n            # Update the version number.\n            handle.attrs['file_version'] = CURRENT_FAST5_VERSION"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread summary data suitable to encode as a json packet.", "response": "def read_summary_data(fname, component):\n        \"\"\" Read summary data suitable to encode as a json packet.\n\n        :param fname: The fast5 file to pull the summary data from.\n        :param component: The component name to pull summary data for.\n\n        :returns: A dictionary containing the summary data.\n        \"\"\"\n        summary = {}\n        with Fast5File(fname, mode='r') as fh:\n            summary['tracking_id'] = fh.get_tracking_id()\n            summary['channel_id'] = fh.get_channel_info()\n            read_info = fh.status.read_info\n            read_summary = []\n            for read in read_info:\n                read_summary.append({'read_number': read.read_number,\n                                     'read_id': read.read_id,\n                                     'start_time': read.start_time,\n                                     'duration': read.duration,\n                                     'start_mux': read.start_mux})\n            summary['reads'] = read_summary\n            analyses_list = fh.list_analyses(component)\n            _, group_names = zip(*analyses_list)\n            group_names = sorted(group_names)\n            group = group_names[-1]\n            summary['software'] = fh.get_analysis_attributes(group)\n            summary['software']['component'] = group[:-4]\n\n            summary['data'] = fh.get_summary_data(group)\n            summary['filename'] = os.path.basename(fname)\n        return summary"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding group to the cache.", "response": "def _add_group(self, group, attrs):\n        \"\"\"\n        :param group: group_name\n        :param attrs:\n        :return:\n        \"\"\"\n        self.handle.create_group(group)\n        if attrs is not None:\n            self._add_attributes(group, attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the attributes of the object at the given path.", "response": "def _get_attributes(self, path):\n        \"\"\"\n        :param path: filepath within fast5\n        :return: dictionary of attributes found at ``path``\n        :rtype dict\n        \"\"\"\n        path_grp = self.handle[path]\n        path_attr = path_grp.attrs\n        return dict(path_attr)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compare_hdf_files(file1, file2):\n    data1 = FileToDict()\n    data2 = FileToDict()\n    scanner1 = data1.scan\n    scanner2 = data2.scan\n    with h5py.File(file1, 'r') as fh1:\n        fh1.visititems(scanner1)\n    with h5py.File(file2, 'r') as fh2:\n        fh2.visititems(scanner2)\n    return data1.contents == data2.contents", "response": "Compare two hdf files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_conn_info(self):\n        return session.ConnectionInfo(self.request.remote_ip,\n                                      self.request.cookies,\n                                      self.request.arguments,\n                                      self.request.headers,\n                                      self.request.path)", "response": "Return ConnectionInfo object from current transport"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, session):\n        self._items[session.session_id] = session\n\n        if session.expiry is not None:\n            heappush(self._queue, session)", "response": "Add a session object to the container."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, session_id):\n        session = self._items.get(session_id, None)\n\n        if session is not None:\n            session.promoted = -1\n            session.on_delete(True)\n            del self._items[session_id]\n            return True\n\n        return False", "response": "Removes the container object with the given identifier from the container object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expire(self, current_time=None):\n        if not self._queue:\n            return\n\n        if current_time is None:\n            current_time = time()\n\n        while self._queue:\n            # Get top most item\n            top = self._queue[0]\n\n            # Early exit if item was not promoted and its expiration time\n            # is greater than now.\n            if top.promoted is None and top.expiry_date > current_time:\n                break\n\n            # Pop item from the stack\n            top = heappop(self._queue)\n\n            need_reschedule = (top.promoted is not None\n                               and top.promoted > current_time)\n\n            # Give chance to reschedule\n            if not need_reschedule:\n                top.promoted = None\n                top.on_delete(False)\n\n                need_reschedule = (top.promoted is not None\n                                   and top.promoted > current_time)\n\n            # If item is promoted and expiration time somewhere in future\n            # just reschedule it\n            if need_reschedule:\n                top.expiry_date = top.promoted\n                top.promoted = None\n                heappush(self._queue, top)\n            else:\n                del self._items[top.session_id]", "response": "Expire any old entries in the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_complete(self):\n        self.active = True\n\n        if self.should_finish():\n            self._detach()\n\n            # Avoid race condition when waiting for write callback and session getting closed in between\n            if not self._finished:\n                self.safe_finish()\n        else:\n            if self.session:\n                self.session.flush()", "response": "Verify if connection should be closed based on amount of data that was sent."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self, message, binary=False):\n        if not self.is_closed:\n            self.session.send_message(message, binary=binary)", "response": "Send a message to the client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart callbacks and start the thread.", "response": "def start(self, timeout=None):\n        \"\"\"Start callbacks\"\"\"\n        self._running = True\n\n        if timeout is None:\n            timeout = self.calculate_next_run()\n\n        self.io_loop.add_timeout(timeout, self._run)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump(self):\n        data = dict(\n            # Sessions\n            sessions_active=self.sess_active,\n\n            # Connections\n            connections_active=self.conn_active,\n            connections_ps=self.conn_ps.last_average,\n\n            # Packets\n            packets_sent_ps=self.pack_sent_ps.last_average,\n            packets_recv_ps=self.pack_recv_ps.last_average\n            )\n\n        for k, v in self.sess_transports.items():\n            data['transp_' + k] = v\n\n        return data", "response": "Return dictionary with current statistical information"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling client - side caching for the current request", "response": "def enable_cache(self):\n        \"\"\"Enable client-side caching for the current request\"\"\"\n        self.set_header('Cache-Control', 'max-age=%d, public' % CACHE_TIME)\n\n        d = datetime.datetime.now() + datetime.timedelta(seconds=CACHE_TIME)\n        self.set_header('Expires', d.strftime('%a, %d %b %Y %H:%M:%S'))\n\n        self.set_header('access-control-max-age', CACHE_TIME)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_session_cookie(self):\n        # If JSESSIONID support is disabled in the settings, ignore cookie logic\n        if not self.server.settings['jsessionid']:\n            return\n\n        cookie = self.cookies.get('JSESSIONID')\n\n        if not cookie:\n            cv = 'dummy'\n        else:\n            cv = cookie.value\n\n        self.set_cookie('JSESSIONID', cv)", "response": "Handle session cookie logic"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_session(self, session_id, register=True):\n        # TODO: Possible optimization here for settings.get\n        s = self._session_kls(self._connection,\n                            self,\n                            session_id,\n                            self.settings.get('disconnect_delay')\n                            )\n\n        if register:\n            self._sessions.add(s)\n\n        return s", "response": "Creates a new session object and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_argument(self, name):\n        val = self.arguments.get(name)\n        if val:\n            return val[0]\n        return None", "response": "Return single argument by name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_state(self):\n        if self.state == CONNECTING:\n            self.state = OPEN\n\n            self.conn.on_open(self.conn_info)", "response": "Verify if session was not yet opened. If it is open call connections on_open."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncloses the session or endpoint connection.", "response": "def close(self, code=3000, message='Go away!'):\n        \"\"\"Close session or endpoint connection.\n\n        `code`\n            Closing code\n        `message`\n            Close message\n        \"\"\"\n        if self.state != CLOSED:\n            try:\n                self.conn.on_close()\n            except:\n                LOG.debug(\"Failed to call on_close().\", exc_info=True)\n            finally:\n                self.state = CLOSED\n                self.close_reason = (code, message)\n                self.conn = None\n\n            # Bump stats\n            self.stats.on_sess_closed(self.transport_name)\n\n            # If we have active handler, notify that session was closed\n            if self.handler is not None:\n                self.handler.session_closed()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delayed_close(self):\n        self.state = CLOSING\n        self.server.io_loop.add_callback(self.close)", "response": "Delayed close - won t close immediately but on next ioloop tick."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the active Tornado handler for the session.", "response": "def set_handler(self, handler, start_heartbeat=True):\n        \"\"\"Set active handler for the session\n\n        `handler`\n            Associate active Tornado handler with the session\n        `start_heartbeat`\n            Should session start heartbeat immediately\n        \"\"\"\n        # Check if session already has associated handler\n        if self.handler is not None:\n            handler.send_pack(proto.disconnect(2010, \"Another connection still open\"))\n            return False\n\n        if self._verify_ip and self.conn_info is not None:\n            # If IP address doesn't match - refuse connection\n            if handler.request.remote_ip != self.conn_info.ip:\n                LOG.error('Attempted to attach to session %s (%s) from different IP (%s)' % (\n                              self.session_id,\n                              self.conn_info.ip,\n                              handler.request.remote_ip\n                              ))\n\n                handler.send_pack(proto.disconnect(2010, \"Attempted to connect to session from different IP\"))\n                return False\n\n        if (self.state == CLOSING or self.state == CLOSED) and not self.send_queue:\n            handler.send_pack(proto.disconnect(*self.get_close_reason()))\n            return False\n\n        # Associate handler and promote session\n        super(Session, self).set_handler(handler)\n\n        self.promote()\n\n        if start_heartbeat:\n            self.start_heartbeat()\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_handler(self, handler):\n        super(Session, self).remove_handler(handler)\n\n        self.promote()\n        self.stop_heartbeat()", "response": "Detach active handler from the session\n\n        handler to remove\n            Handler to remove\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_message(self, msg, stats=True, binary=False):\n        self.send_jsonified(proto.json_encode(bytes_to_str(msg)), stats)", "response": "Send or queue outgoing message to send\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_jsonified(self, msg, stats=True):\n        msg = bytes_to_str(msg)\n\n        if self._immediate_flush:\n            if self.handler and self.handler.active and not self.send_queue:\n                # Send message right away\n                self.handler.send_pack('a[%s]' % msg)\n            else:\n                if self.send_queue:\n                    self.send_queue += ','\n                self.send_queue += msg\n\n                self.flush()\n        else:\n            if self.send_queue:\n                self.send_queue += ','\n            self.send_queue += msg\n\n            if not self._pending_flush:\n                self.server.io_loop.add_callback(self.flush)\n                self._pending_flush = True\n\n        if stats:\n            self.stats.on_pack_sent(1)", "response": "Send a JSON - encoded message to the send_queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flush(self):\n        self._pending_flush = False\n\n        if self.handler is None or not self.handler.active or not self.send_queue:\n            return\n\n        self.handler.send_pack('a[%s]' % self.send_queue)\n        self.send_queue = ''", "response": "Flush the send queue if there s an active connection running"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_heartbeat(self):\n        self.stop_heartbeat()\n\n        self._heartbeat_timer = periodic.Callback(self._heartbeat,\n                                                  self._heartbeat_interval,\n                                                  self.server.io_loop)\n        self._heartbeat_timer.start()", "response": "Start the heartbeat timer"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_messages(self, msg_list):\n        self.stats.on_pack_recv(len(msg_list))\n\n        for msg in msg_list:\n            if self.state == OPEN:\n                self.conn.on_message(msg)", "response": "Handle incoming messages\n            Message list to process\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls by the socketjs server when the socket is opened.", "response": "def on_open(self, info):\n        \"\"\"sockjs-tornado on_open handler\"\"\"\n        # Store some properties\n        self.ip = info.ip\n\n        # Create fake request object\n        self.request = info\n\n        # Call open\n        self.open()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_prefix(self):\n        prefix = self.bucket\n        if self.prefix:\n            prefix += self.delimiter + self.prefix\n        return prefix", "response": "Full prefix of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a notebook model from a notebook file.", "response": "def _notebook_model_from_path(self, path, content=False, format=None):\n        \"\"\"\n        Build a notebook model from database record.\n        \"\"\"\n        model = base_model(path)\n        model[\"type\"] = \"notebook\"\n        if self.fs.isfile(path):\n            model[\"last_modified\"] = model[\"created\"] = self.fs.lstat(path)[\"ST_MTIME\"]\n        else:\n            model[\"last_modified\"] = model[\"created\"] = DUMMY_CREATED_DATE\n        if content:\n            if not self.fs.isfile(path):\n                self.no_such_entity(path)\n            file_content = self.fs.read(path)\n            nb_content = reads(file_content, as_version=NBFORMAT_VERSION)\n            self.mark_trusted_cells(nb_content, path)\n            model[\"format\"] = \"json\"\n            model[\"content\"] = nb_content\n            self.validate_notebook_model(model)\n        return model"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a file model from a file path.", "response": "def _file_model_from_path(self, path, content=False, format=None):\n        \"\"\"\n        Build a file model from database record.\n        \"\"\"\n        model = base_model(path)\n        model[\"type\"] = \"file\"\n        if self.fs.isfile(path):\n            model[\"last_modified\"] = model[\"created\"] = self.fs.lstat(path)[\"ST_MTIME\"]\n        else:\n            model[\"last_modified\"] = model[\"created\"] = DUMMY_CREATED_DATE\n        if content:\n            try:\n                content = self.fs.read(path)\n            except NoSuchFile as e:\n                self.no_such_entity(e.path)\n            except GenericFSError as e:\n                self.do_error(str(e), 500)\n            model[\"format\"] = format or \"text\"\n            model[\"content\"] = content\n            model[\"mimetype\"] = mimetypes.guess_type(path)[0] or \"text/plain\"\n            if format == \"base64\":\n                model[\"format\"] = format or \"base64\"\n                from base64 import b64decode\n                model[\"content\"] = b64decode(content)\n        return model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _convert_file_records(self, paths):\n        ret = []\n        for path in paths:\n            # path = self.fs.remove_prefix(path, self.prefix)  # Remove bucket prefix from paths\n            if os.path.basename(path) == self.fs.dir_keep_file:\n                continue\n            type_ = self.guess_type(path, allow_directory=True)\n            if type_ == \"notebook\":\n                ret.append(self._notebook_model_from_path(path, False))\n            elif type_ == \"file\":\n                ret.append(self._file_model_from_path(path, False, None))\n            elif type_ == \"directory\":\n                ret.append(self._directory_model_from_path(path, False))\n            else:\n                self.do_error(\"Unknown file type %s for file '%s'\" % (type_, path), 500)\n        return ret", "response": "Converts file records to a list of directories and Notebook records."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave a file or directory model to path.", "response": "def save(self, model, path):\n        \"\"\"Save a file or directory model to path.\n        \"\"\"\n        self.log.debug(\"S3contents.GenericManager: save %s: '%s'\", model, path)\n        if \"type\" not in model:\n            self.do_error(\"No model type provided\", 400)\n        if \"content\" not in model and model[\"type\"] != \"directory\":\n            self.do_error(\"No file content provided\", 400)\n\n        if model[\"type\"] not in (\"file\", \"directory\", \"notebook\"):\n            self.do_error(\"Unhandled contents type: %s\" % model[\"type\"], 400)\n\n        try:\n            if model[\"type\"] == \"notebook\":\n                validation_message = self._save_notebook(model, path)\n            elif model[\"type\"] == \"file\":\n                validation_message = self._save_file(model, path)\n            else:\n                validation_message = self._save_directory(path)\n        except Exception as e:\n            self.log.error(\"Error while saving file: %s %s\", path, e, exc_info=True)\n            self.do_error(\"Unexpected error while saving file: %s %s\" % (path, e), 500)\n\n        model = self.get(path, type=model[\"type\"], content=False)\n        if validation_message is not None:\n            model[\"message\"] = validation_message\n        return model"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrenaming a file or directory.", "response": "def rename_file(self, old_path, new_path):\n        \"\"\"Rename a file or directory.\n\n        NOTE: This method is unfortunately named on the base class.  It\n        actually moves a file or a directory.\n        \"\"\"\n        self.log.debug(\"S3contents.GenericManager: Init rename of '%s' to '%s'\", old_path, new_path)\n        if self.file_exists(new_path) or self.dir_exists(new_path):\n            self.already_exists(new_path)\n        elif self.file_exists(old_path) or self.dir_exists(old_path):\n            self.log.debug(\"S3contents.GenericManager: Actually renaming '%s' to '%s'\", old_path,\n                           new_path)\n            self.fs.mv(old_path, new_path)\n        else:\n            self.no_such_entity(old_path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete the file or directory at path.", "response": "def delete_file(self, path):\n        \"\"\"Delete the file or directory at path.\n        \"\"\"\n        self.log.debug(\"S3contents.GenericManager: delete_file '%s'\", path)\n        if self.file_exists(path) or self.dir_exists(path):\n            self.fs.rm(path)\n        else:\n            self.no_such_entity(path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_prefix(self):\n        prefix = self.bucket\n        if self.prefix:\n            prefix += self.separator + self.prefix\n        return prefix", "response": "Full prefix of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unprefix(self, path):\n        path = self.strip(path)\n        if isinstance(path, six.string_types):\n            path = path[len(self.prefix_):] if path.startswith(self.prefix_) else path\n            path = path[1:] if path.startswith(self.separator) else path\n            return path\n        if isinstance(path, (list, tuple)):\n            path = [p[len(self.prefix_):] if p.startswith(self.prefix_) else p for p in path]\n            path = [p[1:] if p.startswith(self.separator) else p for p in path]\n            return path", "response": "Removes the self. prefix_ from a path or list of paths"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninvoking an HTTP GET request on a url and return the response as a dictionary", "response": "def get(url, params={}):\n        \"\"\"Invoke an HTTP GET request on a url\n\n        Args:\n            url (string): URL endpoint to request\n            params (dict): Dictionary of url parameters\n        Returns:\n            dict: JSON response as a dictionary\n        \"\"\"\n        request_url = url\n\n        if len(params):\n            request_url = \"{}?{}\".format(url, urlencode(params))\n\n        try:\n            req = Request(request_url, headers={'User-Agent': 'Mozilla/5.0'})\n            response = json.loads(urlopen(req).read().decode(\"utf-8\"))\n\n            return response\n        except HTTPError as err:\n            raise MtgException(err.read())"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a resource by its id", "response": "def find(self, id):\n        \"\"\"Get a resource by its id\n\n        Args:\n            id (string): Resource id\n        Returns:\n            object: Instance of the resource type\n        \"\"\"\n        url = \"{}/{}/{}\".format(__endpoint__, self.type.RESOURCE, id)\n        response = RestClient.get(url)[self.type.RESOURCE[:-1]]\n        return self.type(response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_many(self, url, type, resource):\n        return [type(item) for item in RestClient.get(url)[resource]]", "response": "Get a list of resources\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter(self):\n\n        page = 1\n        fetch_all = True\n        url = \"{}/{}\".format(__endpoint__, self.type.RESOURCE)\n\n        if 'page' in self.params:\n            page = self.params['page']\n            fetch_all = False\n\n        response = RestClient.get(url, self.params)[self.type.RESOURCE]\n        while len(response):\n            for item in response:\n                yield self.type(item)\n\n            if not fetch_all:\n                break\n            else:\n                page += 1\n                self.where(page=page)\n            response = RestClient.get(url, self.params)[self.type.RESOURCE]", "response": "Gets all resources automating paging through data\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef array(self):\n        url = \"{}/{}\".format(__endpoint__, self.type.RESOURCE)\n        return RestClient.get(url, self.params)[self.type.RESOURCE]", "response": "Get all resources and return the result as an array"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(ply):\n    '''\n    Plot vertices and triangles from a PlyData instance. Assumptions:\n        `ply' has a 'vertex' element with 'x', 'y', and 'z'\n            properties;\n\n        `ply' has a 'face' element with an integral list property\n            'vertex_indices', all of whose elements have length 3.\n\n    '''\n    vertex = ply['vertex']\n\n    (x, y, z) = (vertex[t] for t in ('x', 'y', 'z'))\n\n    mlab.points3d(x, y, z, color=(1, 1, 1), mode='point')\n\n    if 'face' in ply:\n        tri_idx = ply['face']['vertex_indices']\n        idx_dtype = tri_idx[0].dtype\n\n        triangles = numpy.fromiter(tri_idx, [('data', idx_dtype, (3,))],\n                                   count=len(tri_idx))['data']\n\n        mlab.triangular_mesh(x, y, z, triangles,\n                             color=(1, 0, 0.4), opacity=0.5)", "response": "Plot vertices and triangles from a PlyData instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make2d(array, cols=None, dtype=None):\n    '''\n    Make a 2D array from an array of arrays.  The `cols' and `dtype'\n    arguments can be omitted if the array is not empty.\n\n    '''\n    if not len(array):\n        if cols is None or dtype is None:\n            raise RuntimeError(\n                \"cols and dtype must be specified for empty array\"\n            )\n        return _np.empty((0, cols), dtype=dtype)\n    return _np.vstack(array)", "response": "Make a 2D array from an array of arrays."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_header(stream):\n        '''\n        Parse a PLY header from a readable file-like stream.\n\n        '''\n        parser = _PlyHeaderParser()\n        while parser.consume(stream.readline()):\n            pass\n\n        return PlyData(\n            [PlyElement(*e) for e in parser.elements],\n            parser.format == 'ascii',\n            _byte_order_map[parser.format],\n            parser.comments,\n            parser.obj_info\n        )", "response": "Parse a PLY header from a readable file - like stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading PLY data from a readable file - like object or filename.", "response": "def read(stream):\n        '''\n        Read PLY data from a readable file-like object or filename.\n\n        '''\n        (must_close, stream) = _open_stream(stream, 'read')\n        try:\n            data = PlyData._parse_header(stream)\n            for elt in data:\n                elt._read(stream, data.text, data.byte_order)\n        finally:\n            if must_close:\n                stream.close()\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites PLY data to a writeable file - like object or filename.", "response": "def write(self, stream):\n        '''\n        Write PLY data to a writeable file-like object or filename.\n\n        '''\n        (must_close, stream) = _open_stream(stream, 'write')\n        try:\n            stream.write(self.header.encode('ascii'))\n            stream.write(b'\\n')\n            for elt in self:\n                elt._write(stream, self.text, self.byte_order)\n        finally:\n            if must_close:\n                stream.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the PLY - formatted metadata for the instance.", "response": "def header(self):\n        '''\n        Provide PLY-formatted metadata for the instance.\n\n        '''\n        lines = ['ply']\n\n        if self.text:\n            lines.append('format ascii 1.0')\n        else:\n            lines.append('format ' +\n                         _byte_order_reverse[self.byte_order] +\n                         ' 1.0')\n\n        # Some information is lost here, since all comments are placed\n        # between the 'format' line and the first element.\n        for c in self.comments:\n            lines.append('comment ' + c)\n\n        for c in self.obj_info:\n            lines.append('obj_info ' + c)\n\n        lines.extend(elt.header for elt in self.elements)\n        lines.append('end_header')\n        return '\\n'.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the numpy dtype of the in - memory representation of the object.", "response": "def dtype(self, byte_order='='):\n        '''\n        Return the numpy dtype of the in-memory representation of the\n        data.  (If there are no list properties, and the PLY format is\n        binary, then this also accurately describes the on-disk\n        representation of the element.)\n\n        '''\n        return _np.dtype([(prop.name, prop.dtype(byte_order))\n                          for prop in self.properties])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndescribes a PLY array s metadata.", "response": "def describe(data, name, len_types={}, val_types={},\n                 comments=[]):\n        '''\n        Construct a PlyElement from an array's metadata.\n\n        len_types and val_types can be given as mappings from list\n        property names to type strings (like 'u1', 'f4', etc., or\n        'int8', 'float32', etc.). These can be used to define the length\n        and value types of list properties.  List property lengths\n        always default to type 'u1' (8-bit unsigned integer), and value\n        types default to 'i4' (32-bit integer).\n\n        '''\n        if not isinstance(data, _np.ndarray):\n            raise TypeError(\"only numpy arrays are supported\")\n\n        if len(data.shape) != 1:\n            raise ValueError(\"only one-dimensional arrays are \"\n                             \"supported\")\n\n        count = len(data)\n\n        properties = []\n        descr = data.dtype.descr\n\n        for t in descr:\n            if not isinstance(t[1], str):\n                raise ValueError(\"nested records not supported\")\n\n            if not t[0]:\n                raise ValueError(\"field with empty name\")\n\n            if len(t) != 2 or t[1][1] == 'O':\n                # non-scalar field, which corresponds to a list\n                # property in PLY.\n\n                if t[1][1] == 'O':\n                    if len(t) != 2:\n                        raise ValueError(\"non-scalar object fields not \"\n                                         \"supported\")\n\n                len_str = _data_type_reverse[len_types.get(t[0], 'u1')]\n                if t[1][1] == 'O':\n                    val_type = val_types.get(t[0], 'i4')\n                    val_str = _lookup_type(val_type)\n                else:\n                    val_str = _lookup_type(t[1][1:])\n\n                prop = PlyListProperty(t[0], len_str, val_str)\n            else:\n                val_str = _lookup_type(t[1][1:])\n                prop = PlyProperty(t[0], val_str)\n\n            properties.append(prop)\n\n        elt = PlyElement(name, properties, count, comments)\n        elt.data = data\n\n        return elt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the actual data from a PLY file.", "response": "def _read(self, stream, text, byte_order):\n        '''\n        Read the actual data from a PLY file.\n\n        '''\n        dtype = self.dtype(byte_order)\n        if text:\n            self._read_txt(stream)\n        elif _can_mmap(stream) and not self._have_list:\n            # Loading the data is straightforward.  We will memory map\n            # the file in copy-on-write mode.\n            num_bytes = self.count * dtype.itemsize\n            offset = stream.tell()\n            stream.seek(0, 2)\n            max_bytes = stream.tell() - offset\n            if max_bytes < num_bytes:\n                raise PlyElementParseError(\"early end-of-file\", self,\n                                           max_bytes // dtype.itemsize)\n            self._data = _np.memmap(stream, dtype,\n                                    'c', offset, self.count)\n            # Fix stream position\n            stream.seek(offset + self.count * dtype.itemsize)\n        else:\n            # A simple load is impossible.\n            self._read_bin(stream, byte_order)\n\n        self._check_sanity()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _write(self, stream, text, byte_order):\n        '''\n        Write the data to a PLY file.\n\n        '''\n        if text:\n            self._write_txt(stream)\n        else:\n            if self._have_list:\n                # There are list properties, so serialization is\n                # slightly complicated.\n                self._write_bin(stream, byte_order)\n            else:\n                # no list properties, so serialization is\n                # straightforward.\n                stream.write(self.data.astype(self.dtype(byte_order),\n                                              copy=False).data)", "response": "Write the data to a PLY file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_txt(self, stream):\n        '''\n        Load a PLY element from an ASCII-format PLY file.  The element\n        may contain list properties.\n\n        '''\n        self._data = _np.empty(self.count, dtype=self.dtype())\n\n        k = 0\n        for line in _islice(iter(stream.readline, b''), self.count):\n            fields = iter(line.strip().split())\n            for prop in self.properties:\n                try:\n                    self._data[prop.name][k] = prop._from_fields(fields)\n                except StopIteration:\n                    raise PlyElementParseError(\"early end-of-line\",\n                                               self, k, prop)\n                except ValueError:\n                    raise PlyElementParseError(\"malformed input\",\n                                               self, k, prop)\n            try:\n                next(fields)\n            except StopIteration:\n                pass\n            else:\n                raise PlyElementParseError(\"expected end-of-line\",\n                                           self, k)\n            k += 1\n\n        if k < self.count:\n            del self._data\n            raise PlyElementParseError(\"early end-of-file\", self, k)", "response": "Load a PLY element from an ASCII - format PLY file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _write_txt(self, stream):\n        '''\n        Save a PLY element to an ASCII-format PLY file.  The element may\n        contain list properties.\n\n        '''\n        for rec in self.data:\n            fields = []\n            for prop in self.properties:\n                fields.extend(prop._to_fields(rec[prop.name]))\n\n            _np.savetxt(stream, [fields], '%.18g', newline='\\n')", "response": "Save a PLY element to an ASCII - format PLY file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_bin(self, stream, byte_order):\n        '''\n        Load a PLY element from a binary PLY file.  The element may\n        contain list properties.\n\n        '''\n        self._data = _np.empty(self.count, dtype=self.dtype(byte_order))\n\n        for k in _range(self.count):\n            for prop in self.properties:\n                try:\n                    self._data[prop.name][k] = \\\n                        prop._read_bin(stream, byte_order)\n                except StopIteration:\n                    raise PlyElementParseError(\"early end-of-file\",\n                                               self, k, prop)", "response": "Load a PLY element from a binary PLY file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving a PLY element to a binary PLY file.", "response": "def _write_bin(self, stream, byte_order):\n        '''\n        Save a PLY element to a binary PLY file.  The element may\n        contain list properties.\n\n        '''\n        for rec in self.data:\n            for prop in self.properties:\n                prop._write_bin(rec[prop.name], stream, byte_order)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef header(self):\n        '''\n        Format this element's metadata as it would appear in a PLY\n        header.\n\n        '''\n        lines = ['element %s %d' % (self.name, self.count)]\n\n        # Some information is lost here, since all comments are placed\n        # between the 'element' line and the first property definition.\n        for c in self.comments:\n            lines.append('comment ' + c)\n\n        lines.extend(list(map(str, self.properties)))\n\n        return '\\n'.join(lines)", "response": "Return the PLY\n        header of this element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _from_fields(self, fields):\n        '''\n        Parse from generator.  Raise StopIteration if the property could\n        not be read.\n\n        '''\n        return _np.dtype(self.dtype()).type(next(fields))", "response": "Parse from generator. Raise StopIteration if the property could not be read."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading data from a binary stream. Raise StopIteration if the data could not be read.", "response": "def _read_bin(self, stream, byte_order):\n        '''\n        Read data from a binary stream.  Raise StopIteration if the\n        property could not be read.\n\n        '''\n        try:\n            return _read_array(stream, self.dtype(byte_order), 1)[0]\n        except IndexError:\n            raise StopIteration"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _write_bin(self, data, stream, byte_order):\n        '''\n        Write data to a binary stream.\n\n        '''\n        _write_array(stream, _np.dtype(self.dtype(byte_order)).type(data))", "response": "Write data to a binary stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _to_fields(self, data):\n        '''\n        Return generator over the (numerical) PLY representation of the\n        list data (length followed by actual data).\n\n        '''\n        (len_t, val_t) = self.list_dtype()\n\n        data = _np.asarray(data, dtype=val_t).ravel()\n\n        yield _np.dtype(len_t).type(data.size)\n        for x in data:\n            yield x", "response": "Return generator over the numerical PLY representation of the\n        list data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _write_bin(self, data, stream, byte_order):\n        '''\n        Write data to a binary stream.\n\n        '''\n        (len_t, val_t) = self.list_dtype(byte_order)\n\n        data = _np.asarray(data, dtype=val_t).ravel()\n\n        _write_array(stream, _np.array(data.size, dtype=len_t))\n        _write_array(stream, data)", "response": "Write data to a binary stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread filenames from the input stream and detect their palette.", "response": "def color_stream_st(istream=sys.stdin, save_palette=False, **kwargs):\n    \"\"\"\n    Read filenames from the input stream and detect their palette.\n    \"\"\"\n    for line in istream:\n        filename = line.strip()\n        try:\n            palette = extract_colors(filename, **kwargs)\n\n        except Exception as e:\n            print(filename, e, file=sys.stderr)\n            continue\n\n        print_colors(filename, palette)\n        if save_palette:\n            save_palette_as_image(filename, palette)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef color_stream_mt(istream=sys.stdin, n=config.N_PROCESSES, **kwargs):\n    queue = multiprocessing.Queue(1000)\n    lock = multiprocessing.Lock()\n\n    pool = [multiprocessing.Process(target=color_process, args=(queue, lock),\n            kwargs=kwargs) for i in range(n)]\n    for p in pool:\n        p.start()\n\n    block = []\n    for line in istream:\n        block.append(line.strip())\n        if len(block) == config.BLOCK_SIZE:\n            queue.put(block)\n            block = []\n    if block:\n        queue.put(block)\n\n    for i in range(n):\n        queue.put(config.SENTINEL)\n\n    for p in pool:\n        p.join()", "response": "Read filenames from the input stream and detect their palette using multiple processes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreceives filenames and get the colors from their images.", "response": "def color_process(queue, lock):\n    \"Receive filenames and get the colors from their images.\"\n    while True:\n        block = queue.get()\n        if block == config.SENTINEL:\n            break\n\n        for filename in block:\n            try:\n                palette = extract_colors(filename)\n            except:  # TODO: it's too broad exception.\n                continue\n            lock.acquire()\n            try:\n                print_colors(filename, palette)\n            finally:\n                lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting major colors from a given image.", "response": "def extract_colors(\n        filename_or_img, min_saturation=config.MIN_SATURATION,\n        min_distance=config.MIN_DISTANCE, max_colors=config.MAX_COLORS,\n        min_prominence=config.MIN_PROMINENCE, n_quantized=config.N_QUANTIZED):\n    \"\"\"\n    Determine what the major colors are in the given image.\n    \"\"\"\n    if Image.isImageType(filename_or_img):\n        im = filename_or_img\n    else:\n        im = Image.open(filename_or_img)\n\n    # get point color count\n    if im.mode != 'RGB':\n        im = im.convert('RGB')\n    im = autocrop(im, config.WHITE)  # assume white box\n    im = im.convert(\n        'P', palette=Image.ADAPTIVE, colors=n_quantized).convert('RGB')\n    dist = Counter({color: count for count, color\n                    in im.getcolors(n_quantized)})\n    n_pixels = mul(*im.size)\n\n    # aggregate colors\n    to_canonical = {config.WHITE: config.WHITE, config.BLACK: config.BLACK}\n    aggregated = Counter({config.WHITE: 0, config.BLACK: 0})\n    sorted_cols = sorted(dist.items(), key=itemgetter(1), reverse=True)\n    for c, n in sorted_cols:\n        if c in aggregated:\n            # exact match!\n            aggregated[c] += n\n        else:\n            d, nearest = min((distance(c, alt), alt) for alt in aggregated)\n            if d < min_distance:\n                # nearby match\n                aggregated[nearest] += n\n                to_canonical[c] = nearest\n            else:\n                # no nearby match\n                aggregated[c] = n\n                to_canonical[c] = c\n\n    # order by prominence\n    colors = sorted(\n        [Color(c, n / float(n_pixels)) for c, n in aggregated.items()],\n        key=attrgetter('prominence'), reverse=True)\n\n    colors, bg_color = detect_background(im, colors, to_canonical)\n\n    # keep any color which meets the minimum saturation\n    sat_colors = [c for c in colors if meets_min_saturation(c, min_saturation)]\n    if bg_color and not meets_min_saturation(bg_color, min_saturation):\n        bg_color = None\n    if sat_colors:\n        colors = sat_colors\n    else:\n        # keep at least one color\n        colors = colors[:1]\n\n    # keep any color within 10% of the majority color\n    color_list = []\n    color_count = 0\n\n    for color in colors:\n        if color.prominence >= colors[0].prominence * min_prominence:\n            color_list.append(color)\n            color_count += 1\n\n        if color_count >= max_colors:\n            break\n\n    return Palette(color_list, bg_color)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_palette_as_image(filename, palette):\n    \"Save palette as a PNG with labeled, colored blocks\"\n    output_filename = '%s_palette.png' % filename[:filename.rfind('.')]\n    size = (80 * len(palette.colors), 80)\n    im = Image.new('RGB', size)\n    draw = ImageDraw.Draw(im)\n    for i, c in enumerate(palette.colors):\n        v = colorsys.rgb_to_hsv(*norm_color(c.value))[2]\n        (x1, y1) = (i * 80, 0)\n        (x2, y2) = ((i + 1) * 80 - 1, 79)\n        draw.rectangle([(x1, y1), (x2, y2)], fill=c.value)\n        if v < 0.6:\n            # white with shadow\n            draw.text((x1 + 4, y1 + 4), rgb_to_hex(c.value), (90, 90, 90))\n            draw.text((x1 + 3, y1 + 3), rgb_to_hex(c.value))\n        else:\n            # dark with bright \"shadow\"\n            draw.text((x1 + 4, y1 + 4), rgb_to_hex(c.value), (230, 230, 230))\n            draw.text((x1 + 3, y1 + 3), rgb_to_hex(c.value), (0, 0, 0))\n\n    im.save(output_filename, \"PNG\")", "response": "Save palette as a PNG with labeled colored blocks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef autocrop(im, bgcolor):\n    \"Crop away a border of the given background color.\"\n    if im.mode != \"RGB\":\n        im = im.convert(\"RGB\")\n    bg = Image.new(\"RGB\", im.size, bgcolor)\n    diff = ImageChops.difference(im, bg)\n    bbox = diff.getbbox()\n    if bbox:\n        return im.crop(bbox)\n\n    return im", "response": "Crop away a border of the given background color."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting data into InfluxDB and return it as a dictionary.", "response": "def convert_to_influx(mac, payload):\n    '''\n    Convert data into RuuviCollector naming schme and scale\n\n    returns:\n        Object to be written to InfluxDB\n    '''\n    dataFormat = payload[\"data_format\"] if ('data_format' in payload) else None\n    fields = {}\n    fields[\"temperature\"]               = payload[\"temperature\"] if ('temperature' in payload) else None\n    fields[\"humidity\"]                  = payload[\"humidity\"] if ('humidity' in payload) else None\n    fields[\"pressure\"]                  = payload[\"pressure\"] if ('pressure' in payload) else None\n    fields[\"accelerationX\"]             = payload[\"acceleration_x\"] if ('acceleration_x' in payload) else None\n    fields[\"accelerationY\"]             = payload[\"acceleration_y\"] if ('acceleration_y' in payload) else None\n    fields[\"accelerationZ\"]             = payload[\"acceleration_z\"] if ('acceleration_z' in payload) else None\n    fields[\"batteryVoltage\"]            = payload[\"battery\"]/1000.0 if ('battery' in payload) else None\n    fields[\"txPower\"]                   = payload[\"tx_power\"] if ('tx_power' in payload) else None\n    fields[\"movementCounter\"]           = payload[\"movement_counter\"] if ('movement_counter' in payload) else None\n    fields[\"measurementSequenceNumber\"] = payload[\"measurement_sequence_number\"] if ('measurement_sequence_number' in payload) else None\n    fields[\"tagID\"]                     = payload[\"tagID\"] if ('tagID' in payload) else None\n    fields[\"rssi\"]                      = payload[\"rssi\"] if ('rssi' in payload) else None\n    return {\n        \"measurement\": \"ruuvi_measurements\",\n        \"tags\": {\n            \"mac\": mac,\n            \"dataFormat\": dataFormat\n        },\n        \"fields\": fields\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbackground process function for RuuviTag Sensors", "response": "def _run_get_data_background(macs, queue, shared_data, bt_device):\n    \"\"\"\n    Background process function for RuuviTag Sensors\n    \"\"\"\n\n    run_flag = RunFlag()\n\n    def add_data(data):\n        if not shared_data['run_flag']:\n            run_flag.running = False\n\n        data[1]['time'] = datetime.utcnow().isoformat()\n        queue.put(data)\n\n    RuuviTagSensor.get_datas(add_data, macs, run_flag, bt_device)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _data_update(subjects, queue, run_flag):\n        while run_flag.running:\n            while not queue.empty():\n                data = queue.get()\n                for subject in [s for s in subjects if not s.is_disposed]:\n                    subject.on_next(data)\n            time.sleep(0.1)", "response": "Update the data of all the observers with the new data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new Subject object and adds it to the internal list of subjects.", "response": "def get_subject(self):\n        \"\"\"\n        Returns:\n            subject : Reactive Extension Subject\n        \"\"\"\n\n        if not self._run_flag.running:\n            raise Exception('RuuviTagReactive stopped')\n\n        subject = Subject()\n        self._subjects.append(subject)\n        return subject"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops the set of subject and all its data.", "response": "def stop(self):\n        \"\"\"\n        Stop get_datas\n        \"\"\"\n\n        self._run_flag.running = False\n        self._shared_data['run_flag'] = False\n\n        for s in self._subjects:\n            s.dispose()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_get_data_background(macs, queue):\n\n    def callback(data):\n        data[1]['time'] = str(datetime.now())\n        queue.put(data)\n\n    RuuviTagSensor.get_datas(callback, macs)", "response": "Background process from RuuviTag Sensors\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def data_update(queue):\n    global allData\n    while True:\n        while not queue.empty():\n            data = queue.get()\n            allData[data[0]] = data[1]\n        for key, value in tags.items():\n            if key in allData:\n                allData[key]['name'] = value\n        await asyncio.sleep(0.5)", "response": "Update allData variable to include all data sent by the background process"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_data(raw):\n        # TODO: Check from raw data correct data format\n        # Now this returns 2 also for Data Format 4\n        data = RuuviTagSensor._get_data_format_2and4(raw)\n\n        if data is not None:\n            return (2, data)\n\n        data = RuuviTagSensor._get_data_format_3(raw)\n\n        if data is not None:\n            return (3, data)\n\n        data = RuuviTagSensor._get_data_format_5(raw)\n\n        if data is not None:\n            return (5, data)\n\n        return (None, None)", "response": "Convert raw data from RuuviTag to a tuple of data format type and Sensor data format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_ruuvitags(bt_device=''):\n\n        log.info('Finding RuuviTags. Stop with Ctrl+C.')\n\n        datas = dict()\n        for new_data in RuuviTagSensor._get_ruuvitag_datas(bt_device=bt_device):\n            if new_data[0] in datas:\n                continue\n            datas[new_data[0]] = new_data[1]\n            log.info(new_data[0])\n            log.info(new_data[1])\n\n        return datas", "response": "Function will find all RuuviTags."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_data_for_sensors(macs=[], search_duratio_sec=5, bt_device=''):\n\n        log.info('Get latest data for sensors. Stop with Ctrl+C.')\n        log.info('Stops automatically in %ss', search_duratio_sec)\n        log.info('MACs: %s', macs)\n\n        datas = dict()\n\n        for new_data in RuuviTagSensor._get_ruuvitag_datas(macs, search_duratio_sec, bt_device=bt_device):\n            datas[new_data[0]] = new_data[1]\n\n        return datas", "response": "Get lates data for sensors in the MAC list."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets data for all ruuvitag sensors or sensors in the list.", "response": "def get_datas(callback, macs=[], run_flag=RunFlag(), bt_device=''):\n        \"\"\"\n        Get data for all ruuvitag sensors or sensors in the MAC's list.\n\n        Args:\n            callback (func): callback funcion to be called when new data is received\n            macs (list): MAC addresses\n            run_flag (object): RunFlag object. Function executes while run_flag.running\n            bt_device (string): Bluetooth device id\n        \"\"\"\n\n        log.info('Get latest data for sensors. Stop with Ctrl+C.')\n        log.info('MACs: %s', macs)\n\n        for new_data in RuuviTagSensor._get_ruuvitag_datas(macs, None, run_flag, bt_device):\n            callback(new_data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting data from BluetoothCommunication and handle data encoding.", "response": "def _get_ruuvitag_datas(macs=[], search_duratio_sec=None, run_flag=RunFlag(), bt_device=''):\n        \"\"\"\n        Get data from BluetoothCommunication and handle data encoding.\n\n        Args:\n            macs (list): MAC addresses. Default empty list\n            search_duratio_sec (int): Search duration in seconds. Default None\n            run_flag (object): RunFlag object. Function executes while run_flag.running. Default new RunFlag\n            bt_device (string): Bluetooth device id\n        Yields:\n            tuple: MAC and State of RuuviTag sensor data\n        \"\"\"\n\n        mac_blacklist = []\n        start_time = time.time()\n        data_iter = ble.get_datas(mac_blacklist, bt_device)\n\n        for ble_data in data_iter:\n            # Check duration\n            if search_duratio_sec and time.time() - start_time > search_duratio_sec:\n                data_iter.send(StopIteration)\n                break\n            # Check running flag\n            if not run_flag.running:\n                data_iter.send(StopIteration)\n                break\n            # Check MAC whitelist\n            if macs and not ble_data[0] in macs:\n                continue\n            (data_format, data) = RuuviTagSensor.convert_data(ble_data[1])\n            # Check that encoded data is valid RuuviTag data and it is sensor data\n            # If data is not valid RuuviTag data add MAC to blacklist\n            if data is not None:\n                state = get_decoder(data_format).decode_data(data)\n                if state is not None:\n                    yield (ble_data[0], state)\n            else:\n                mac_blacklist.append(ble_data[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts hexadcimal data to string.", "response": "def _get_data_format_2and4(raw):\n        \"\"\"\n        Validate that data is from RuuviTag and is Data Format 2 or 4. Convert hexadcimal data to string.\n        Encoded data part is after ruu.vi/# or r/\n\n        Returns:\n            string: Encoded sensor data\n        \"\"\"\n        try:\n            # TODO: Fix conversion so convered data will show https://ruu.vi/# and htts://r/\n            # Now it has e.g. [Non_ASCII characters]ruu.vi/#AjwYAMFc\n            base16_split = [raw[i:i + 2] for i in range(0, len(raw), 2)]\n            selected_hexs = filter(lambda x: int(x, 16) < 128, base16_split)\n            characters = [chr(int(c, 16)) for c in selected_hexs]\n            data = ''.join(characters)\n\n            # take only part after ruu.vi/# or r/\n            index = data.find('ruu.vi/#')\n            if index > -1:\n                return data[(index + 8):]\n            else:\n                index = data.find('r/')\n                if index > -1:\n                    return data[(index + 2):]\n                return None\n        except:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self):\n\n        (data_format, data) = RuuviTagSensor.get_data(self._mac, self._bt_device)\n\n        if data == self._data:\n            return self._state\n\n        self._data = data\n\n        if self._data is None:\n            self._state = {}\n        else:\n            self._state = get_decoder(data_format).decode_data(self._data)\n\n        return self._state", "response": "Update state of the current state of the current state."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts data into RuuviCollecor naming schme and scale.", "response": "def write_to_influxdb(received_data):\n    '''\n    Convert data into RuuviCollecor naming schme and scale\n    '''\n    dataFormat = received_data[1][\"data_format\"] if ('data_format' in received_data[1]) else None\n    fields = {}\n    fields[\"temperature\"]               = received_data[1][\"temperature\"] if ('temperature' in received_data[1]) else None\n    fields[\"humidity\"]                  = received_data[1][\"humidity\"] if ('humidity' in received_data[1]) else None\n    fields[\"pressure\"]                  = received_data[1][\"pressure\"] if ('pressure' in received_data[1]) else None\n    fields[\"accelerationX\"]             = received_data[1][\"acceleration_x\"] if ('acceleration_x' in received_data[1]) else None\n    fields[\"accelerationY\"]             = received_data[1][\"acceleration_y\"] if ('acceleration_y' in received_data[1]) else None\n    fields[\"accelerationZ\"]             = received_data[1][\"acceleration_z\"] if ('acceleration_z' in received_data[1]) else None\n    fields[\"batteryVoltage\"]            = received_data[1][\"battery\"]/1000.0 if ('battery' in received_data[1]) else None\n    fields[\"txPower\"]                   = received_data[1][\"tx_power\"] if ('tx_power' in received_data[1]) else None\n    fields[\"movementCounter\"]           = received_data[1][\"movement_counter\"] if ('movement_counter' in received_data[1]) else None\n    fields[\"measurementSequenceNumber\"] = received_data[1][\"measurement_sequence_number\"] if ('measurement_sequence_number' in received_data[1]) else None\n    fields[\"tagID\"]                     = received_data[1][\"tagID\"] if ('tagID' in received_data[1]) else None\n    fields[\"rssi\"]                      = received_data[1][\"rssi\"] if ('rssi' in received_data[1]) else None\n    json_body = [\n        {\n            \"measurement\": \"ruuvi_measurements\",\n            \"tags\": {\n                \"mac\": received_data[0],\n                \"dataFormat\": dataFormat\n            },\n            \"fields\": fields\n        }\n    ]\n    client.write_points(json_body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_get_data_background(macs, queue):\n    while True:\n        datas = RuuviTagSensor.get_data_for_sensors(macs, timeout_in_sec)\n        queue.put(datas)", "response": "Background process from RuuviTag Sensors\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_data():\n    global allData\n    while not q.empty():\n        allData = q.get()\n    for key, value in tags.items():\n        if key in allData:\n            allData[key]['name'] = value", "response": "Update all data sent by background process to global allData\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns temperature in celsius", "response": "def _get_temperature(self, decoded):\n        '''Return temperature in celsius'''\n        temp = (decoded[2] & 127) + decoded[3] / 100\n        sign = (decoded[2] >> 7) & 1\n        if sign == 0:\n            return round(temp, 2)\n        return round(-1 * temp, 2)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_data(self, encoded):\n        '''\n        Decode sensor data.\n\n        Returns:\n            dict: Sensor values\n        '''\n        try:\n            identifier = None\n            data_format = 2\n            if len(encoded) > 8:\n                data_format = 4\n                identifier = encoded[8:]\n                encoded = encoded[:8]\n            decoded = bytearray(base64.b64decode(encoded, '-_'))\n            return {\n                'data_format': data_format,\n                'temperature': self._get_temperature(decoded),\n                'humidity': self._get_humidity(decoded),\n                'pressure': self._get_pressure(decoded),\n                'identifier': identifier\n            }\n        except:\n            log.exception('Encoded value: %s not valid', encoded)\n            return None", "response": "Decode sensor data.\n\n        Returns:\n            dict: Sensor values"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_temperature(self, data):\n        '''Return temperature in celsius'''\n        temp = (data[2] & ~(1 << 7)) + (data[3] / 100)\n        sign = (data[2] >> 7) & 1\n        if sign == 0:\n            return round(temp, 2)\n        return round(-1 * temp, 2)", "response": "Return temperature in celsius"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode_data(self, data):\n        '''\n        Decode sensor data.\n\n        Returns:\n            dict: Sensor values\n        '''\n        try:\n            byte_data = bytearray.fromhex(data)\n            acc_x, acc_y, acc_z = self._get_acceleration(byte_data)\n            return {\n                'data_format': 3,\n                'humidity': self._get_humidity(byte_data),\n                'temperature': self._get_temperature(byte_data),\n                'pressure': self._get_pressure(byte_data),\n                'acceleration': math.sqrt(acc_x * acc_x + acc_y * acc_y + acc_z * acc_z),\n                'acceleration_x': acc_x,\n                'acceleration_y': acc_y,\n                'acceleration_z': acc_z,\n                'battery': self._get_battery(byte_data)\n            }\n        except Exception:\n            log.exception('Value: %s not valid', data)\n            return None", "response": "Decode sensor data.\n\n        Returns:\n            dict: Sensor values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_temperature(self, data):\n        '''Return temperature in celsius'''\n        if data[1:2] == 0x7FFF:\n            return None\n\n        temperature = twos_complement((data[1] << 8) + data[2], 16) / 200\n        return round(temperature, 2)", "response": "Return temperature in celsius"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_acceleration(self, data):\n        '''Return acceleration mG'''\n        if (data[7:8] == 0x7FFF or\n                data[9:10] == 0x7FFF or\n                data[11:12] == 0x7FFF):\n            return (None, None, None)\n\n        acc_x = twos_complement((data[7] << 8) + data[8], 16)\n        acc_y = twos_complement((data[9] << 8) + data[10], 16)\n        acc_z = twos_complement((data[11] << 8) + data[12], 16)\n        return (acc_x, acc_y, acc_z)", "response": "Return acceleration mG from the data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn battery voltage and tx power from the power info field of the log entry.", "response": "def _get_powerinfo(self, data):\n        '''Return battery voltage and tx power '''\n        power_info = (data[13] & 0xFF) << 8 | (data[14] & 0xFF)\n        battery_voltage = rshift(power_info, 5) + 1600\n        tx_power = (power_info & 0b11111) * 2 - 40\n\n        if rshift(power_info, 5) == 0b11111111111:\n            battery_voltage = None\n        if (power_info & 0b11111) == 0b11111:\n            tx_power = None\n\n        return (round(battery_voltage, 3), tx_power)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start(bt_device=''):\n        '''\n        Attributes:\n           device (string): BLE device (default hci0)\n        '''\n        # import ptyprocess here so as long as all implementations are in the same file, all will work\n        import ptyprocess\n\n        if not bt_device:\n            bt_device = 'hci0'\n\n        log.info('Start receiving broadcasts (device %s)', bt_device)\n        DEVNULL = subprocess.DEVNULL if sys.version_info >= (3, 3) else open(os.devnull, 'wb')\n\n        subprocess.call('sudo hciconfig %s reset' % bt_device, shell=True, stdout=DEVNULL)\n        hcitool = subprocess.Popen(['sudo', '-n', 'hcitool', 'lescan', '--duplicates'], stdout=DEVNULL)\n        hcidump = ptyprocess.PtyProcess.spawn(['sudo', '-n', 'hcidump', '--raw'])\n        return (hcitool, hcidump)", "response": "Start receiving broadcasts from a BLE device."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting a key into several sub - keys.", "response": "def split_key(key, max_keys=0):\n    \"\"\"Splits a key but allows dots in the key name if they're scaped properly.\n\n    Splitting this complex key:\n\n    complex_key = \".dont\\.splitme.d\\.o\\. origen.splitme\\.dontsplit.splitme.\"\n    split_key(complex_key)\n\n    results in:\n\n    ['', 'dont\\.splitme', 'd\\.o\\. origen', 'splitme\\.dontsplit', 'splitme', '']\n\n\n    Args:\n        key (basestring): The key to be splitted.\n        max_keys (int): The maximum number of keys to be extracted. 0 means no\n            limits.\n\n    Returns:\n        A list of keys\n    \"\"\"\n    parts = [x for x in re.split(SPLIT_REGEX, key) if x != \".\"]\n    result = []\n    while len(parts) > 0:\n        if max_keys > 0 and len(result) == max_keys:\n            break\n        result.append(parts.pop(0))\n\n    if len(parts) > 0:\n        result.append(\".\".join(parts))\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_python(self):\n        result = list(self)\n\n        for index, value in enumerate(result):\n            if isinstance(value, DottedCollection):\n                result[index] = value.to_python()\n\n        return result", "response": "Returns a plain python list and converts to plain python objects all\n        this object s descendants."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_python(self):\n        result = dict(self)\n\n        for key, value in iteritems(result):\n            if isinstance(value, DottedCollection):\n                result[key] = value.to_python()\n\n        return result", "response": "Returns a plain python dict and converts to plain python objects all\n        this object s descendants."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles one read or write cycle.", "response": "def _handle_one(self):\n        \"\"\"\n        Handle one read/write cycle.\n        \"\"\"\n        ready_to_read, ready_to_write, in_error = select.select(\n            [self.request], [self.request], [self.request], 0.1)\n\n        if in_error:\n            raise self.Disconnect()\n\n        # Write any commands to the client\n        while self.send_queue and ready_to_write:\n            msg = self.send_queue.pop(0)\n            self._send(msg)\n\n        # See if the client has any commands for us.\n        if ready_to_read:\n            self._handle_incoming()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling the initial setting of the user s nickname and nick changes.", "response": "def handle_nick(self, params):\n        \"\"\"\n        Handle the initial setting of the user's nickname and nick changes.\n        \"\"\"\n        nick = params\n\n        # Valid nickname?\n        if re.search(r'[^a-zA-Z0-9\\-\\[\\]\\'`^{}_]', nick):\n            raise IRCError.from_name('erroneusnickname', ':%s' % nick)\n\n        if self.server.clients.get(nick, None) == self:\n            # Already registered to user\n            return\n\n        if nick in self.server.clients:\n            # Someone else is using the nick\n            raise IRCError.from_name('nicknameinuse', 'NICK :%s' % (nick))\n\n        if not self.nick:\n            # New connection and nick is available; register and send welcome\n            # and MOTD.\n            self.nick = nick\n            self.server.clients[nick] = self\n            response = ':%s %s %s :%s' % (\n                self.server.servername,\n                events.codes['welcome'], self.nick, SRV_WELCOME)\n            self.send_queue.append(response)\n            response = ':%s 376 %s :End of MOTD command.' % (\n                self.server.servername, self.nick)\n            self.send_queue.append(response)\n            return\n\n        # Nick is available. Change the nick.\n        message = ':%s NICK :%s' % (self.client_ident(), nick)\n\n        self.server.clients.pop(self.nick)\n        self.nick = nick\n        self.server.clients[self.nick] = self\n\n        # Send a notification of the nick change to all the clients in the\n        # channels the client is in.\n        for channel in self.channels.values():\n            self._send_to_others(message, channel)\n\n        # Send a notification of the nick change to the client itself\n        return message"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_user(self, params):\n        params = params.split(' ', 3)\n\n        if len(params) != 4:\n            raise IRCError.from_name(\n                'needmoreparams',\n                'USER :Not enough parameters')\n\n        user, mode, unused, realname = params\n        self.user = user\n        self.mode = mode\n        self.realname = realname\n        return ''", "response": "Handle the USER command which identifies the user to the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle the JOIN command.", "response": "def handle_join(self, params):\n        \"\"\"\n        Handle the JOINing of a user to a channel. Valid channel names start\n        with a # and consist of a-z, A-Z, 0-9 and/or '_'.\n        \"\"\"\n        channel_names = params.split(' ', 1)[0]  # Ignore keys\n        for channel_name in channel_names.split(','):\n            r_channel_name = channel_name.strip()\n\n            # Valid channel name?\n            if not re.match('^#([a-zA-Z0-9_])+$', r_channel_name):\n                raise IRCError.from_name(\n                    'nosuchchannel',\n                    '%s :No such channel' % r_channel_name)\n\n            # Add user to the channel (create new channel if not exists)\n            channel = self.server.channels.setdefault(\n                r_channel_name,\n                IRCChannel(r_channel_name))\n            channel.clients.add(self)\n\n            # Add channel to user's channel list\n            self.channels[channel.name] = channel\n\n            # Send the topic\n            response_join = ':%s TOPIC %s :%s' % (\n                channel.topic_by,\n                channel.name, channel.topic)\n            self.send_queue.append(response_join)\n\n            # Send join message to everybody in the channel, including yourself\n            # and send user list of the channel back to the user.\n            response_join = ':%s JOIN :%s' % (\n                self.client_ident(),\n                r_channel_name)\n            for client in channel.clients:\n                client.send_queue.append(response_join)\n\n            nicks = [client.nick for client in channel.clients]\n            _vals = (\n                self.server.servername, self.nick, channel.name,\n                ' '.join(nicks))\n            response_userlist = ':%s 353 %s = %s :%s' % _vals\n            self.send_queue.append(response_userlist)\n\n            _vals = self.server.servername, self.nick, channel.name\n            response = ':%s 366 %s %s :End of /NAMES list' % _vals\n            self.send_queue.append(response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle a private message from a user or channel.", "response": "def handle_privmsg(self, params):\n        \"\"\"\n        Handle sending a private message to a user or channel.\n        \"\"\"\n        target, sep, msg = params.partition(' ')\n        if not msg:\n            raise IRCError.from_name(\n                'needmoreparams',\n                'PRIVMSG :Not enough parameters')\n\n        message = ':%s PRIVMSG %s %s' % (self.client_ident(), target, msg)\n        if target.startswith('#') or target.startswith('$'):\n            # Message to channel. Check if the channel exists.\n            channel = self.server.channels.get(target)\n            if not channel:\n                raise IRCError.from_name('nosuchnick', 'PRIVMSG :%s' % target)\n\n            if channel.name not in self.channels:\n                # The user isn't in the channel.\n                raise IRCError.from_name(\n                    'cannotsendtochan',\n                    '%s :Cannot send to channel' % channel.name)\n\n            self._send_to_others(message, channel)\n        else:\n            # Message to user\n            client = self.server.clients.get(target, None)\n            if not client:\n                raise IRCError.from_name('nosuchnick', 'PRIVMSG :%s' % target)\n\n            client.send_queue.append(message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends the message to all other clients in the specified channel except for self.", "response": "def _send_to_others(self, message, channel):\n        \"\"\"\n        Send the message to all clients in the specified channel except for\n        self.\n        \"\"\"\n        other_clients = [\n            client for client in channel.clients\n            if not client == self]\n        for client in other_clients:\n            client.send_queue.append(message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle a topic command.", "response": "def handle_topic(self, params):\n        \"\"\"\n        Handle a topic command.\n        \"\"\"\n        channel_name, sep, topic = params.partition(' ')\n\n        channel = self.server.channels.get(channel_name)\n        if not channel:\n            raise IRCError.from_name(\n                'nosuchnick', 'PRIVMSG :%s' % channel_name)\n        if channel.name not in self.channels:\n            # The user isn't in the channel.\n            raise IRCError.from_name(\n                'cannotsendtochan',\n                '%s :Cannot send to channel' % channel.name)\n\n        if topic:\n            channel.topic = topic.lstrip(':')\n            channel.topic_by = self.nick\n        message = ':%s TOPIC %s :%s' % (\n            self.client_ident(), channel_name,\n            channel.topic)\n        return message"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_part(self, params):\n        for pchannel in params.split(','):\n            if pchannel.strip() in self.server.channels:\n                # Send message to all clients in all channels user is in, and\n                # remove the user from the channels.\n                channel = self.server.channels.get(pchannel.strip())\n                response = ':%s PART :%s' % (self.client_ident(), pchannel)\n                if channel:\n                    for client in channel.clients:\n                        client.send_queue.append(response)\n                channel.clients.remove(self)\n                self.channels.pop(pchannel)\n            else:\n                _vars = self.server.servername, pchannel, pchannel\n                response = ':%s 403 %s :%s' % _vars\n                self.send_queue.append(response)", "response": "Handle a client parting from a channel."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_quit(self, params):\n        response = ':%s QUIT :%s' % (self.client_ident(), params.lstrip(':'))\n        # Send quit message to all clients in all channels user is in, and\n        # remove the user from the channels.\n        for channel in self.channels.values():\n            for client in channel.clients:\n                client.send_queue.append(response)\n            channel.clients.remove(self)", "response": "Handle the quit command."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_dump(self, params):\n        print(\"Clients:\", self.server.clients)\n        for client in self.server.clients.values():\n            print(\" \", client)\n            for channel in client.channels.values():\n                print(\"     \", channel.name)\n        print(\"Channels:\", self.server.channels)\n        for channel in self.server.channels.values():\n            print(\" \", channel.name, channel)\n            for client in channel.clients:\n                print(\"     \", client.nick, client)", "response": "Dump internal server information for debugging purposes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef client_ident(self):\n        return irc.client.NickMask.from_params(\n            self.nick, self.user,\n            self.server.servername)", "response": "Return the client identifier as included in many command replies."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinishes the connection with the client.", "response": "def finish(self):\n        \"\"\"\n        The client conection is finished. Do some cleanup to ensure that the\n        client doesn't linger around in any channel or the client list, in case\n        the client didn't properly close the connection with PART and QUIT.\n        \"\"\"\n        log.info('Client disconnected: %s', self.client_ident())\n        response = ':%s QUIT :EOF from client' % self.client_ident()\n        for channel in self.channels.values():\n            if self in channel.clients:\n                # Client is gone without properly QUITing or PARTing this\n                # channel.\n                for client in channel.clients:\n                    client.send_queue.append(response)\n                channel.clients.remove(self)\n        if self.nick:\n            self.server.clients.pop(self.nick)\n        log.info('Connection finished: %s', self.client_ident())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def connect(\n        self, server, port, nickname, password=None, username=None,\n        ircname=None, connect_factory=connection.AioFactory()\n    ):\n        \"\"\"Connect/reconnect to a server.\n\n        Arguments:\n\n        * server - Server name\n        * port - Port number\n        * nickname - The nickname\n        * password - Password (if any)\n        * username - The username\n        * ircname - The IRC name (\"realname\")\n\n        * connect_factory - An async callable that takes the event loop and the\n          server address, and returns a connection (with a socket interface)\n\n        This function can be called to reconnect a closed connection.\n\n        Returns the AioProtocol instance (used for handling incoming\n        IRC data) and the transport instance (used for handling\n        outgoing data).\n        \"\"\"\n        if self.connected:\n            self.disconnect(\"Changing servers\")\n\n        self.buffer = self.buffer_class()\n        self.handlers = {}\n        self.real_server_name = \"\"\n        self.real_nickname = nickname\n        self.server = server\n        self.port = port\n        self.server_address = (server, port)\n        self.nickname = nickname\n        self.username = username or nickname\n        self.ircname = ircname or nickname\n        self.password = password\n        self.connect_factory = connect_factory\n\n        protocol_instance = self.protocol_class(self, self.reactor.loop)\n        connection = self.connect_factory(\n            protocol_instance, self.server_address)\n        transport, protocol = await connection\n\n        self.transport = transport\n        self.protocol = protocol\n\n        self.connected = True\n        self.reactor._on_connect(self.protocol, self.transport)\n\n        # Log on...\n        if self.password:\n            self.pass_(self.password)\n        self.nick(self.nickname)\n        self.user(self.username, self.ircname)\n        return self", "response": "Connect to a server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_data(self, new_data):\n        self.buffer.feed(new_data)\n\n        # process each non-empty line after logging all lines\n        for line in self.buffer:\n            log.debug(\"FROM SERVER: %s\", line)\n            if not line:\n                continue\n            self._process_line(line)", "response": "Process incoming data from the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_raw(self, string):\n        log.debug('RAW: {}'.format(string))\n        if self.transport is None:\n            raise ServerNotConnectedError(\"Not connected.\")\n\n        self.transport.write(self._prep_message(string))", "response": "Send a string to the server via the asyncio transport."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disconnect(self, message=\"\"):\n        try:\n            del self.connected\n        except AttributeError:\n            return\n\n        self.quit(message)\n\n        self.transport.close()\n\n        self._handle_event(Event(\"disconnect\", self.server, \"\", [message]))", "response": "Hang up the connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the mode_string and return a list of triples.", "response": "def _parse_modes(mode_string, unary_modes=\"\"):\n    \"\"\"\n    Parse the mode_string and return a list of triples.\n\n    If no string is supplied return an empty list.\n\n    >>> _parse_modes('')\n    []\n\n    If no sign is supplied, return an empty list.\n\n    >>> _parse_modes('ab')\n    []\n\n    Discard unused args.\n\n    >>> _parse_modes('+a foo bar baz')\n    [['+', 'a', None]]\n\n    Return none for unary args when not provided\n\n    >>> _parse_modes('+abc foo', unary_modes='abc')\n    [['+', 'a', 'foo'], ['+', 'b', None], ['+', 'c', None]]\n\n    This function never throws an error:\n\n    >>> import random\n    >>> def random_text(min_len = 3, max_len = 80):\n    ...     len = random.randint(min_len, max_len)\n    ...     chars_to_choose = [chr(x) for x in range(0,1024)]\n    ...     chars = (random.choice(chars_to_choose) for x in range(len))\n    ...     return ''.join(chars)\n    >>> def random_texts(min_len = 3, max_len = 80):\n    ...     while True:\n    ...         yield random_text(min_len, max_len)\n    >>> import itertools\n    >>> texts = itertools.islice(random_texts(), 1000)\n    >>> set(type(_parse_modes(text)) for text in texts) == {list}\n    True\n    \"\"\"\n\n    # mode_string must be non-empty and begin with a sign\n    if not mode_string or not mode_string[0] in '+-':\n        return []\n\n    modes = []\n\n    parts = mode_string.split()\n\n    mode_part, args = parts[0], parts[1:]\n\n    for ch in mode_part:\n        if ch in \"+-\":\n            sign = ch\n            continue\n        arg = args.pop(0) if ch in unary_modes and args else None\n        modes.append([sign, ch, arg])\n    return modes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(item):\n        key, sep, value = item.partition('=')\n        value = value.replace('\\\\:', ';')\n        value = value.replace('\\\\s', ' ')\n        value = value.replace('\\\\n', '\\n')\n        value = value.replace('\\\\r', '\\r')\n        value = value.replace('\\\\\\\\', '\\\\')\n        value = value or None\n        return {\n            'key': key,\n            'value': value,\n        }", "response": "r Parse a tag into a dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_group(cls, group):\n        if not group:\n            return\n        tag_items = group.split(\";\")\n        return list(map(cls.parse, tag_items))", "response": "Construct a list of tags from a regex group"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_group(group):\n        if not group:\n            return []\n\n        main, sep, ext = group.partition(\" :\")\n        arguments = main.split()\n        if sep:\n            arguments.append(ext)\n\n        return arguments", "response": "Construct the list of arguments from a regex group"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets a feature value", "response": "def set(self, name, value=True):\n        \"set a feature value\"\n        setattr(self, name.lower(), value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the values from the a ServerConnection arguments", "response": "def load(self, arguments):\n        \"Load the values from the a ServerConnection arguments\"\n        features = arguments[1:-1]\n        list(map(self.load_feature, features))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing CHANLIMIT value into a dictionary.", "response": "def _parse_CHANLIMIT(value):\n        \"\"\"\n        >>> res = FeatureSet._parse_CHANLIMIT('ibe:250,xyz:100')\n        >>> len(res)\n        6\n        >>> res['x']\n        100\n        >>> res['i'] == res['b'] == res['e'] == 250\n        True\n        \"\"\"\n        pairs = map(string_int_pair, value.split(','))\n        return dict(\n            (target, number)\n            for target_keys, number in pairs\n            for target in target_keys\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _connect(self):\n        server = self.servers.peek()\n        try:\n            self.connect(\n                server.host, server.port, self._nickname,\n                server.password, ircname=self._realname,\n                **self.__connect_params)\n        except irc.client.ServerConnectionError:\n            pass", "response": "Establish a connection to the server at the front of the server_list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _on_namreply(self, connection, event):\n\n        ch_type, channel, nick_list = event.arguments\n\n        if channel == '*':\n            # User is not in any visible channel\n            # http://tools.ietf.org/html/rfc2812#section-3.2.5\n            return\n\n        for nick in nick_list.split():\n            nick_modes = []\n\n            if nick[0] in self.connection.features.prefix:\n                nick_modes.append(self.connection.features.prefix[nick[0]])\n                nick = nick[1:]\n\n            for mode in nick_modes:\n                self.channels[channel].set_mode(mode, nick)\n\n            self.channels[channel].add_user(nick)", "response": "Called when a NAMREPLY is received from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting to a new server possibly disconnecting from the current server.", "response": "def jump_server(self, msg=\"Changing servers\"):\n        \"\"\"Connect to a new server, possibly disconnecting from the current.\n\n        The bot will skip to next server in the server_list each time\n        jump_server is called.\n        \"\"\"\n        if self.connection.is_connected():\n            self.connection.disconnect(msg)\n\n        next(self.servers)\n        self._connect()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndefaults handler for ctcp events.", "response": "def on_ctcp(self, connection, event):\n        \"\"\"Default handler for ctcp events.\n\n        Replies to VERSION and PING requests and relays DCC requests\n        to the on_dccchat method.\n        \"\"\"\n        nick = event.source.nick\n        if event.arguments[0] == \"VERSION\":\n            connection.ctcp_reply(nick, \"VERSION \" + self.get_version())\n        elif event.arguments[0] == \"PING\":\n            if len(event.arguments) > 1:\n                connection.ctcp_reply(nick, \"PING \" + event.arguments[1])\n        elif (\n            event.arguments[0] == \"DCC\"\n                and event.arguments[1].split(\" \", 1)[0] == \"CHAT\"):\n            self.on_dccchat(connection, event)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_mode(self, mode, value=None):\n        if mode in self.user_modes:\n            self.mode_users[mode][value] = 1\n        else:\n            self.modes[mode] = value", "response": "Set the mode of the channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclearing the mode of the channel.", "response": "def clear_mode(self, mode, value=None):\n        \"\"\"Clear mode on the channel.\n\n        Arguments:\n\n            mode -- The mode (a single-character string).\n\n            value -- Value\n        \"\"\"\n        try:\n            if mode in self.user_modes:\n                del self.mode_users[mode][value]\n            else:\n                del self.modes[mode]\n        except KeyError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ip_numstr_to_quad(num):\n    packed = struct.pack('>L', int(num))\n    bytes = struct.unpack('BBBB', packed)\n    return \".\".join(map(str, bytes))", "response": "Convert an IP number given in ASCII\n    representation to an IP address string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ip_quad_to_numstr(quad):\n    bytes = map(int, quad.split(\".\"))\n    packed = struct.pack('BBBB', *bytes)\n    return str(struct.unpack('>L', packed)[0])", "response": "Convert an IP address string to an IP\n    number as a base - 10 integer given in ASCII representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect(\n            self, server, port, nickname, password=None, username=None,\n            ircname=None, connect_factory=connection.Factory()):\n        \"\"\"Connect/reconnect to a server.\n\n        Arguments:\n\n        * server - Server name\n        * port - Port number\n        * nickname - The nickname\n        * password - Password (if any)\n        * username - The username\n        * ircname - The IRC name (\"realname\")\n        * server_address - The remote host/port of the server\n        * connect_factory - A callable that takes the server address and\n          returns a connection (with a socket interface)\n\n        This function can be called to reconnect a closed connection.\n\n        Returns the ServerConnection object.\n        \"\"\"\n        log.debug(\n            \"connect(server=%r, port=%r, nickname=%r, ...)\", server,\n            port, nickname)\n\n        if self.connected:\n            self.disconnect(\"Changing servers\")\n\n        self.buffer = self.buffer_class()\n        self.handlers = {}\n        self.real_server_name = \"\"\n        self.real_nickname = nickname\n        self.server = server\n        self.port = port\n        self.server_address = (server, port)\n        self.nickname = nickname\n        self.username = username or nickname\n        self.ircname = ircname or nickname\n        self.password = password\n        self.connect_factory = connect_factory\n        try:\n            self.socket = self.connect_factory(self.server_address)\n        except socket.error as ex:\n            raise ServerConnectionError(\"Couldn't connect to socket: %s\" % ex)\n        self.connected = True\n        self.reactor._on_connect(self.socket)\n\n        # Log on...\n        if self.password:\n            self.pass_(self.password)\n        self.nick(self.nickname)\n        self.user(self.username, self.ircname)\n        return self", "response": "Connect to a server and return a new ServerConnection object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reconnect(self):\n        self.connect(*self._saved_connect.args, **self._saved_connect.kwargs)", "response": "Reconnect with the last arguments passed to self. connect"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close(self):\n        # Without this thread lock, there is a window during which\n        # select() can find a closed socket, leading to an EBADF error.\n        with self.reactor.mutex:\n            self.disconnect(\"Closing object\")\n            self.reactor._remove_connection(self)", "response": "Close the connection.\n\n        This method closes the connection permanently; after it has\n        been called, the object is unusable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_nick(self, name):\n        orig = self.get_nickname()\n        self.nick(name)\n        try:\n            yield orig\n        finally:\n            self.nick(orig)", "response": "Set the nick for the duration of the context."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads and process input from self. socket", "response": "def process_data(self):\n        \"read and process input from self.socket\"\n\n        try:\n            reader = getattr(self.socket, 'read', self.socket.recv)\n            new_data = reader(2 ** 14)\n        except socket.error:\n            # The server hung up.\n            self.disconnect(\"Connection reset by peer\")\n            return\n        if not new_data:\n            # Read nothing: connection must be down.\n            self.disconnect(\"Connection reset by peer\")\n            return\n\n        self.buffer.feed(new_data)\n\n        # process each non-empty line after logging all lines\n        for line in self.buffer:\n            log.debug(\"FROM SERVER: %s\", line)\n            if not line:\n                continue\n            self._process_line(line)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cap(self, subcommand, *args):\n        cap_subcommands = set('LS LIST REQ ACK NAK CLEAR END'.split())\n        client_subcommands = set(cap_subcommands) - {'NAK'}\n        assert subcommand in client_subcommands, \"invalid subcommand\"\n\n        def _multi_parameter(args):\n            \"\"\"\n            According to the spec::\n\n                If more than one capability is named, the RFC1459 designated\n                sentinel (:) for a multi-parameter argument must be present.\n\n            It's not obvious where the sentinel should be present or if it\n            must be omitted for a single parameter, so follow convention and\n            only include the sentinel prefixed to the first parameter if more\n            than one parameter is present.\n            \"\"\"\n            if len(args) > 1:\n                return (':' + args[0],) + args[1:]\n            return args\n\n        self.send_items('CAP', subcommand, *_multi_parameter(args))", "response": "Send a CAP command according to the spec."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a CTCP command.", "response": "def ctcp(self, ctcptype, target, parameter=\"\"):\n        \"\"\"Send a CTCP command.\"\"\"\n        ctcptype = ctcptype.upper()\n        tmpl = (\n            \"\\001{ctcptype} {parameter}\\001\" if parameter else\n            \"\\001{ctcptype}\\001\"\n        )\n        self.privmsg(target, tmpl.format(**vars()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disconnect(self, message=\"\"):\n        try:\n            del self.connected\n        except AttributeError:\n            return\n\n        self.quit(message)\n\n        try:\n            self.socket.shutdown(socket.SHUT_WR)\n            self.socket.close()\n        except socket.error:\n            pass\n        del self.socket\n        self._handle_event(Event(\"disconnect\", self.server, \"\", [message]))", "response": "Hang up the connection."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef kick(self, channel, nick, comment=\"\"):\n        self.send_items('KICK', channel, nick, comment and ':' + comment)", "response": "Send a KICK command."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a LIST command.", "response": "def list(self, channels=None, server=\"\"):\n        \"\"\"Send a LIST command.\"\"\"\n        self.send_items('LIST', ','.join(always_iterable(channels)), server)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef part(self, channels, message=\"\"):\n        self.send_items('PART', ','.join(always_iterable(channels)), message)", "response": "Send a PART command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef privmsg_many(self, targets, text):\n        target = ','.join(targets)\n        return self.privmsg(target, text)", "response": "Send a PRIVMSG command to multiple targets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a raw string to the server.", "response": "def send_raw(self, string):\n        \"\"\"Send raw string to the server.\n\n        The string will be padded with appropriate CR LF.\n        \"\"\"\n        if self.socket is None:\n            raise ServerNotConnectedError(\"Not connected.\")\n        sender = getattr(self.socket, 'write', self.socket.send)\n        try:\n            sender(self._prep_message(string))\n            log.debug(\"TO SERVER: %s\", string)\n        except socket.error:\n            # Ouch!\n            self.disconnect(\"Connection reset by peer.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef user(self, username, realname):\n        cmd = 'USER {username} 0 * :{realname}'.format(**locals())\n        self.send_raw(cmd)", "response": "Send a USER command."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a WHOWAS command.", "response": "def whowas(self, nick, max=\"\", server=\"\"):\n        \"\"\"Send a WHOWAS command.\"\"\"\n        self.send_items('WHOWAS', nick, max, server)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_keepalive(self, interval):\n        pinger = functools.partial(self.ping, 'keep-alive')\n        self.reactor.scheduler.execute_every(period=interval, func=pinger)", "response": "Set a keepalive to occur every interval on this connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef server(self):\n\n        conn = self.connection_class(self)\n        with self.mutex:\n            self.connections.append(conn)\n        return conn", "response": "Creates and returns a ServerConnection object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when there is more data to read on a socket.", "response": "def process_data(self, sockets):\n        \"\"\"Called when there is more data to read on connection sockets.\n\n        Arguments:\n\n            sockets -- A list of socket objects.\n\n        See documentation for Reactor.__init__.\n        \"\"\"\n        with self.mutex:\n            log.log(logging.DEBUG - 2, \"process_data()\")\n            for sock, conn in itertools.product(sockets, self.connections):\n                if sock == conn.socket:\n                    conn.process_data()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses data from connections once.", "response": "def process_once(self, timeout=0):\n        \"\"\"Process data from connections once.\n\n        Arguments:\n\n            timeout -- How long the select() call should wait if no\n                       data is available.\n\n        This method should be called periodically to check and process\n        incoming data, if there are any.  If that seems boring, look\n        at the process_forever method.\n        \"\"\"\n        log.log(logging.DEBUG - 2, \"process_once()\")\n        sockets = self.sockets\n        if sockets:\n            in_, out, err = select.select(sockets, [], [], timeout)\n            self.process_data(in_)\n        else:\n            time.sleep(timeout)\n        self.process_timeout()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns an infinite loop, processing data from connections. This method repeatedly calls process_once. Arguments: timeout -- Parameter to pass to process_once.", "response": "def process_forever(self, timeout=0.2):\n        \"\"\"Run an infinite loop, processing data from connections.\n\n        This method repeatedly calls process_once.\n\n        Arguments:\n\n            timeout -- Parameter to pass to process_once.\n        \"\"\"\n        # This loop should specifically *not* be mutex-locked.\n        # Otherwise no other thread would ever be able to change\n        # the shared state of a Reactor object running this function.\n        log.debug(\"process_forever(timeout=%s)\", timeout)\n        one = functools.partial(self.process_once, timeout=timeout)\n        consume(infinite_call(one))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disconnect_all(self, message=\"\"):\n        with self.mutex:\n            for conn in self.connections:\n                conn.disconnect(message)", "response": "Disconnects all the active connections."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_global_handler(self, event, handler, priority=0):\n        handler = PrioritizedHandler(priority, handler)\n        with self.mutex:\n            event_handlers = self.handlers.setdefault(event, [])\n            bisect.insort(event_handlers, handler)", "response": "Adds a global handler function for a specific event type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_global_handler(self, event, handler):\n        with self.mutex:\n            if event not in self.handlers:\n                return 0\n            for h in self.handlers[event]:\n                if handler == h.callback:\n                    self.handlers[event].remove(h)\n        return 1", "response": "Removes a global handler function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates and returns a DCCConnection object.", "response": "def dcc(self, dcctype=\"chat\"):\n        \"\"\"Creates and returns a DCCConnection object.\n\n        Arguments:\n\n            dcctype -- \"chat\" for DCC CHAT connections or \"raw\" for\n                       DCC SEND (or other DCC types). If \"chat\",\n                       incoming data will be split in newline-separated\n                       chunks. If \"raw\", incoming data is not touched.\n        \"\"\"\n        with self.mutex:\n            conn = DCCConnection(self, dcctype)\n            self.connections.append(conn)\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles an event incoming on ServerConnection connection.", "response": "def _handle_event(self, connection, event):\n        \"\"\"\n        Handle an Event event incoming on ServerConnection connection.\n        \"\"\"\n        with self.mutex:\n            matching_handlers = sorted(\n                self.handlers.get(\"all_events\", [])\n                + self.handlers.get(event.type, [])\n            )\n            for handler in matching_handlers:\n                result = handler.callback(connection, event)\n                if result == \"NO MORE\":\n                    return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove a connection from the list of connections.", "response": "def _remove_connection(self, connection):\n        \"\"\"[Internal]\"\"\"\n        with self.mutex:\n            self.connections.remove(connection)\n            self._on_disconnect(connection.socket)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self, address, port):\n        self.peeraddress = socket.gethostbyname(address)\n        self.peerport = port\n        self.buffer = buffer.LineBuffer()\n        self.handlers = {}\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            self.socket.connect((self.peeraddress, self.peerport))\n        except socket.error as x:\n            raise DCCConnectionError(\"Couldn't connect to socket: %s\" % x)\n        self.connected = True\n        self.reactor._on_connect(self.socket)\n        return self", "response": "Connect to a DCC peer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef listen(self, addr=None):\n        self.buffer = buffer.LineBuffer()\n        self.handlers = {}\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.passive = True\n        default_addr = socket.gethostbyname(socket.gethostname()), 0\n        try:\n            self.socket.bind(addr or default_addr)\n            self.localaddress, self.localport = self.socket.getsockname()\n            self.socket.listen(10)\n        except socket.error as x:\n            raise DCCConnectionError(\"Couldn't bind socket: %s\" % x)\n        return self", "response": "Wait for a connection from a DCC peer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disconnect(self, message=\"\"):\n        try:\n            del self.connected\n        except AttributeError:\n            return\n\n        try:\n            self.socket.shutdown(socket.SHUT_WR)\n            self.socket.close()\n        except socket.error:\n            pass\n        del self.socket\n        self.reactor._handle_event(\n            self,\n            Event(\"dcc_disconnect\", self.peeraddress, \"\", [message]))\n        self.reactor._remove_connection(self)", "response": "Disconnects from the DCC."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_data(self):\n\n        if self.passive and not self.connected:\n            conn, (self.peeraddress, self.peerport) = self.socket.accept()\n            self.socket.close()\n            self.socket = conn\n            self.connected = True\n            log.debug(\n                \"DCC connection from %s:%d\", self.peeraddress,\n                self.peerport)\n            self.reactor._handle_event(\n                self,\n                Event(\"dcc_connect\", self.peeraddress, None, None))\n            return\n\n        try:\n            new_data = self.socket.recv(2 ** 14)\n        except socket.error:\n            # The server hung up.\n            self.disconnect(\"Connection reset by peer\")\n            return\n        if not new_data:\n            # Read nothing: connection must be down.\n            self.disconnect(\"Connection reset by peer\")\n            return\n\n        if self.dcctype == \"chat\":\n            self.buffer.feed(new_data)\n\n            chunks = list(self.buffer)\n\n            if len(self.buffer) > 2 ** 14:\n                # Bad peer! Naughty peer!\n                log.info(\n                    \"Received >16k from a peer without a newline; \"\n                    \"disconnecting.\")\n                self.disconnect()\n                return\n        else:\n            chunks = [new_data]\n\n        command = \"dccmsg\"\n        prefix = self.peeraddress\n        target = None\n        for chunk in chunks:\n            log.debug(\"FROM PEER: %s\", chunk)\n            arguments = [chunk]\n            log.debug(\n                \"command: %s, source: %s, target: %s, arguments: %s\",\n                command, prefix, target, arguments)\n            event = Event(command, prefix, target, arguments)\n            self.reactor._handle_event(self, event)", "response": "Process the next DCC message from the peer."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends text to DCC peer.", "response": "def privmsg(self, text):\n        \"\"\"\n        Send text to DCC peer.\n\n        The text will be padded with a newline if it's a DCC CHAT session.\n        \"\"\"\n        if self.dcctype == 'chat':\n            text += '\\n'\n        return self.send_bytes(self.encode(text))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend data to DCC peer.", "response": "def send_bytes(self, bytes):\n        \"\"\"\n        Send data to DCC peer.\n        \"\"\"\n        try:\n            self.socket.send(bytes)\n            log.debug(\"TO PEER: %r\\n\", bytes)\n        except socket.error:\n            self.disconnect(\"Connection reset by peer.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndispatches events to on_<event. type > method.", "response": "def _dispatcher(self, connection, event):\n        \"\"\"\n        Dispatch events to on_<event.type> method, if present.\n        \"\"\"\n        log.debug(\"_dispatcher: %s\", event.type)\n\n        def do_nothing(connection, event):\n            return None\n        method = getattr(self, \"on_\" + event.type, do_nothing)\n        method(connection, event)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate and associate a new DCCConnection object with the current thread.", "response": "def dcc(self, *args, **kwargs):\n        \"\"\"Create and associate a new DCCConnection object.\n\n        Use the returned object to listen for or connect to\n        a DCC peer.\n        \"\"\"\n        dcc = self.reactor.dcc(*args, **kwargs)\n        self.dcc_connections.append(dcc)\n        return dcc"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dcc_connect(self, address, port, dcctype=\"chat\"):\n        warnings.warn(\"Use self.dcc(type).connect()\", DeprecationWarning)\n        return self.dcc(dcctype).connect(address, port)", "response": "Connect to a DCC peer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dcc_listen(self, dcctype=\"chat\"):\n        warnings.warn(\"Use self.dcc(type).listen()\", DeprecationWarning)\n        return self.dcc(dcctype).listen()", "response": "Listen for connections from a DCC peer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting to a VNCServer and return a Client instance that is usable", "response": "def connect(server, password=None,\n        factory_class=VNCDoToolFactory, proxy=ThreadedVNCClientProxy, timeout=None):\n    \"\"\" Connect to a VNCServer and return a Client instance that is usable\n    in the main thread of non-Twisted Python Applications, EXPERIMENTAL.\n\n    >>> from vncdotool import api\n    >>> with api.connect('host') as client\n    >>>     client.keyPress('c')\n\n    You may then call any regular VNCDoToolClient method on client from your\n    application code.\n\n    If you are using a GUI toolkit or other major async library please read\n    http://twistedmatrix.com/documents/13.0.0/core/howto/choosing-reactor.html\n    for a better method of intergrating vncdotool.\n    \"\"\"\n    if not reactor.running:\n        global _THREAD\n        _THREAD = threading.Thread(target=reactor.run, name='Twisted',\n                         kwargs={'installSignalHandlers': False})\n        _THREAD.daemon = True\n        _THREAD.start()\n\n        observer = PythonLoggingObserver()\n        observer.start()\n\n    factory = factory_class()\n\n    if password is not None:\n        factory.password = password\n\n    family, host, port = command.parse_server(server)\n    client = proxy(factory, timeout)\n    client.connect(host, port=port, family=family)\n\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nturning the string data into a list of bits ( 1 0 ) s", "response": "def __String_to_BitList(self, data):\n\t\t\"\"\"Turn the string data, into a list of bits (1, 0)'s\"\"\"\n\t\tif isinstance(data[0], str):\n\t\t\t# Turn the strings into integers. Python 3 uses a bytes\n\t\t\t# class, which already has this behaviour.\n\t\t\tdata = [ord(c) for c in data]\n\t\tl = len(data) * 8\n\t\tresult = [0] * l\n\t\tpos = 0\n\t\tfor ch in data:\n\t\t\ti = 7\n\t\t\twhile i >= 0:\n\t\t\t\tif ch & (1 << i) != 0:\n\t\t\t\t\tresult[pos] = 1\n\t\t\t\telse:\n\t\t\t\t\tresult[pos] = 0\n\t\t\t\tpos += 1\n\t\t\t\ti -= 1\n\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a key press to the server", "response": "def keyPress(self, key):\n        \"\"\" Send a key press to the server\n\n            key: string: either [a-z] or a from KEYMAP\n        \"\"\"\n        log.debug('keyPress %s', key)\n        self.keyDown(key)\n        self.keyUp(key)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a mouse click at the last set position", "response": "def mousePress(self, button):\n        \"\"\" Send a mouse click at the last set position\n\n            button: int: [1-n]\n\n        \"\"\"\n        log.debug('mousePress %s', button)\n        buttons = self.buttons | (1 << (button - 1))\n        self.mouseDown(button)\n        self.mouseUp(button)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a mouse button down event at the last set position", "response": "def mouseDown(self, button):\n        \"\"\" Send a mouse button down at the last set position\n\n            button: int: [1-n]\n\n        \"\"\"\n        log.debug('mouseDown %s', button)\n        self.buttons |= 1 << (button - 1)\n        self.pointerEvent(self.x, self.y, buttonmask=self.buttons)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncapturing a region of the current display.", "response": "def captureRegion(self, filename, x, y, w, h):\n        \"\"\" Save a region of the current display to filename\n        \"\"\"\n        log.debug('captureRegion %s', filename)\n        return self._capture(filename, x, y, x+w, y+h)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwait until the display matches a target image.", "response": "def expectScreen(self, filename, maxrms=0):\n        \"\"\" Wait until the display matches a target image\n\n            filename: an image file to read and compare against\n            maxrms: the maximum root mean square between histograms of the\n                    screen and target image\n        \"\"\"\n        log.debug('expectScreen %s', filename)\n        return self._expectFramebuffer(filename, 0, 0, maxrms)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwait until a portion of the screen matches the target image.", "response": "def expectRegion(self, filename, x, y, maxrms=0):\n        \"\"\" Wait until a portion of the screen matches the target image\n\n            The region compared is defined by the box\n            (x, y), (x + image.width, y + image.height)\n        \"\"\"\n        log.debug('expectRegion %s (%s, %s)', filename, x, y)\n        return self._expectFramebuffer(filename, x, y, maxrms)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmoves the mouse pointer to the specified position.", "response": "def mouseMove(self, x, y):\n        \"\"\" Move the mouse pointer to position (x, y)\n        \"\"\"\n        log.debug('mouseMove %d,%d', x, y)\n        self.x, self.y = x, y\n        self.pointerEvent(x, y, self.buttons)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmoves the mouse point to position x y in increments of step.", "response": "def mouseDrag(self, x, y, step=1):\n        \"\"\" Move the mouse point to position (x, y) in increments of step\n        \"\"\"\n        log.debug('mouseDrag %d,%d', x, y)\n        if x < self.x:\n            xsteps = [self.x - i for i in range(step, self.x - x + 1, step)]\n        else:\n            xsteps = range(self.x, x, step)\n\n        if y < self.y:\n            ysteps = [self.y - i for i in range(step, self.y - y + 1, step)]\n        else:\n            ysteps = range(self.y, y, step)\n\n        for ypos in ysteps:\n            time.sleep(.2)\n            self.mouseMove(self.x, ypos)\n\n        for xpos in xsteps:\n            time.sleep(.2)\n            self.mouseMove(xpos, self.y)\n\n        self.mouseMove(x, y)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting color ordering and 24 vs. 32 bpp info out of the pixel format information and sets the image mode.", "response": "def setImageMode(self):\n        \"\"\" Extracts color ordering and 24 vs. 32 bpp info out of the pixel format information\n        \"\"\"\n        if self._version_server == 3.889:\n            self.setPixelFormat(\n                    bpp = 16, depth = 16, bigendian = 0, truecolor = 1,\n                    redmax = 31, greenmax = 63, bluemax = 31,\n                    redshift = 11, greenshift = 5, blueshift = 0\n                    )\n            self.image_mode = \"BGR;16\"\n        elif (self.truecolor and (not self.bigendian) and self.depth == 24\n                and self.redmax == 255 and self.greenmax == 255 and self.bluemax == 255):\n\n            pixel = [\"X\"] * self.bypp\n            offsets = [offset // 8 for offset in (self.redshift, self.greenshift, self.blueshift)]\n            for offset, color in zip(offsets, \"RGB\"):\n                pixel[offset] = color\n            self.image_mode = \"\".join(pixel)\n        else:\n            self.setPixelFormat()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sendPassword(self, password):\n        pw = (password + '\\0' * 8)[:8]        #make sure its 8 chars long, zero padded\n        des = RFBDes(pw)\n        response = des.encrypt(self._challenge)\n        self.transport.write(response)", "response": "send password to the server"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handleDecodeHextileSubrectsColoured(self, block, bg, color, subrects, x, y, width, height, tx, ty, tw, th):\n        sz = self.bypp + 2\n        pos = 0\n        end = len(block)\n        while pos < end:\n            pos2 = pos + self.bypp\n            color = block[pos:pos2]\n            xy = ord(block[pos2])\n            wh = ord(block[pos2+1])\n            sx = xy >> 4\n            sy = xy & 0xf\n            sw = (wh >> 4) + 1\n            sh = (wh & 0xf) + 1\n            self.fillRectangle(tx + sx, ty + sy, sw, sh, color)\n            pos += sz\n        self._doNextHextileSubrect(bg, color, x, y, width, height, tx, ty)", "response": "Decode a hextile subrects with their own color"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles a key event.", "response": "def keyEvent(self, key, down=1):\n        \"\"\"For most ordinary keys, the \"keysym\" is the same as the corresponding ASCII value.\n        Other common keys are shown in the KEY_ constants.\"\"\"\n        self.transport.write(pack(\"!BBxxI\", 4, down, key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pointerEvent(self, x, y, buttonmask=0):\n        self.transport.write(pack(\"!BBHH\", 5, buttonmask, x, y))", "response": "Indicates either a pointer button press or release."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vncRequestPassword(self):\n        if self.factory.password is None:\n            log.msg(\"need a password\")\n            self.transport.loseConnection()\n            return\n        self.sendPassword(self.factory.password)", "response": "send a password to the vnc server"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfills the area with the color", "response": "def fillRectangle(self, x, y, width, height, color):\n        \"\"\"fill the area with the color. the color is a string in\n           the pixel format set up earlier\"\"\"\n        #fallback variant, use update recatngle\n        #override with specialized function for better performance\n        self.updateRectangle(x, y, width, height, color*width*height)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setKey(self, key):\n        newkey = []\n        for ki in range(len(key)):\n            bsrc = ord(key[ki])\n            btgt = 0\n            for i in range(8):\n                if bsrc & (1 << i):\n                    btgt = btgt | (1 << 7-i)\n            newkey.append(chr(btgt))\n        super(RFBDes, self).setKey(newkey)", "response": "This method is used to set the key for the DES protocol for authentication."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef not_played(self) -> int:\n        return len(self._data[MiniSeriesData].progress) - len(self.progress)", "response": "The number of games in the player that have not played yet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing and start a Picker.", "response": "def pick(options, title=None, indicator='*', default_index=0, multi_select=False, min_selection_count=0, options_map_func=None):\n    \"\"\"Construct and start a :class:`Picker <Picker>`.\n\n    Usage::\n\n      >>> from pick import pick\n      >>> title = 'Please choose an option: '\n      >>> options = ['option1', 'option2', 'option3']\n      >>> option, index = pick(options, title)\n    \"\"\"\n    picker = Picker(options, title, indicator, default_index, multi_select, min_selection_count, options_map_func)\n    return picker.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_selected(self):\n        if self.multi_select:\n            return_tuples = []\n            for selected in self.all_selected:\n                return_tuples.append((self.options[selected], selected))\n            return return_tuples\n        else:\n            return self.options[self.index], self.index", "response": "return the current selected option as a tuple or as a list of tuples"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndrawing the curses ui on the screen handle scroll if needed", "response": "def draw(self):\n        \"\"\"draw the curses ui on the screen, handle scroll if needed\"\"\"\n        self.screen.clear()\n\n        x, y = 1, 1  # start point\n        max_y, max_x = self.screen.getmaxyx()\n        max_rows = max_y - y  # the max rows we can draw\n\n        lines, current_line = self.get_lines()\n\n        # calculate how many lines we should scroll, relative to the top\n        scroll_top = getattr(self, 'scroll_top', 0)\n        if current_line <= scroll_top:\n            scroll_top = 0\n        elif current_line - scroll_top > max_rows:\n            scroll_top = current_line - max_rows\n        self.scroll_top = scroll_top\n\n        lines_to_draw = lines[scroll_top:scroll_top+max_rows]\n\n        for line in lines_to_draw:\n            if type(line) is tuple:\n                self.screen.addnstr(y, x, line[0], max_x-2, line[1])\n            else:\n                self.screen.addnstr(y, x, line, max_x-2)\n            y += 1\n\n        self.screen.refresh()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_file(file_id: str, download):\n    data = current_app.config[\"data\"]  # type: DataStorage\n    dao = data.get_files_dao()\n    file_fp, filename, upload_date = dao.get(file_id)\n\n    if download:\n        mime = mimetypes.guess_type(filename)[0]\n        if mime is None:\n            # unknown type\n            mime = \"binary/octet-stream\"\n\n        basename = os.path.basename(filename)\n        return send_file(file_fp, mimetype=mime, attachment_filename=basename, as_attachment=True)\n\n    else:\n        rawdata = file_fp.read()\n        try:\n            text = rawdata.decode('utf-8')\n        except UnicodeDecodeError:\n            # not decodable as utf-8\n            text = _get_binary_info(rawdata)\n        html = render_template(\"api/file_view.html\", content=text)\n        file_fp.close()\n        return Response(html)", "response": "Get a specific file from GridFS."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_files_zip(run_id: int, filetype: _FileType):\n    data = current_app.config[\"data\"]\n    dao_runs = data.get_run_dao()\n    dao_files = data.get_files_dao()\n    run = dao_runs.get(run_id)\n\n    if filetype == _FileType.ARTIFACT:\n        target_files = run['artifacts']\n    elif filetype == _FileType.SOURCE:\n        target_files = run['experiment']['sources']\n    else:\n        raise Exception(\"Unknown file type: %s\" % filetype)\n\n    memory_file = io.BytesIO()\n    with zipfile.ZipFile(memory_file, 'w') as zf:\n        for f in target_files:\n            # source and artifact files use a different data structure\n            file_id = f['file_id'] if 'file_id' in f else f[1]\n            file, filename, upload_date = dao_files.get(file_id)\n            data = zipfile.ZipInfo(filename, date_time=upload_date.timetuple())\n            data.compress_type = zipfile.ZIP_DEFLATED\n            zf.writestr(data, file.read())\n    memory_file.seek(0)\n\n    fn_suffix = _filetype_suffices[filetype]\n    return send_file(memory_file, attachment_filename='run{}_{}.zip'.format(run_id, fn_suffix), as_attachment=True)", "response": "Send all artifacts or sources of a run as ZIP."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the file identified by a file_id string.", "response": "def get(self, file_id: Union[str, bson.ObjectId]) -> [typing.BinaryIO, str, datetime.datetime]:\n        \"\"\"\n        Return the file identified by a file_id string.\n\n        The return value is a file-like object which also has the following attributes:\n        filename: str\n        upload_date: datetime\n        \"\"\"\n        if isinstance(file_id, str):\n            file_id = bson.ObjectId(file_id)\n        file = self._fs.get(file_id)\n        return file, file.filename, file.upload_date"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_record(self, collection_name, query):\n        cursor = self._get_collection(collection_name).find(query)\n        for record in cursor:\n            # Return the first record found.\n            return record\n        # Return None if nothing found.\n        return None", "response": "Find a single record in a MongoDB collection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_records(self, collection_name, query={}, sort_by=None,\n                     sort_direction=None, start=0, limit=None):\n        \"\"\"\n        Return a cursor of records from the given MongoDB collection.\n\n        :param collection_name: Name of the MongoDB collection to query.\n        :param query: Standard MongoDB query. By default no restriction.\n        :param sort_by: Name of a single field to sort by.\n        :param sort_direction: The direction to sort, \"asc\" or \"desc\".\n        :param start: Skip first n results.\n        :param limit: The maximum number of results to return.\n\n        :return: Cursor -- An iterable with results.\n        :raise DataSourceError\n        \"\"\"\n        cursor = self._get_collection(collection_name).find(query)\n        if sort_by is not None:\n            cursor = self._apply_sort(cursor, sort_by, sort_direction)\n        cursor = cursor.skip(start)\n        if limit is not None:\n            cursor = cursor.limit(limit)\n        return MongoDbCursor(cursor)", "response": "Return a cursor of records from the given MongoDB collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting PyMongo client pointing to the current database.", "response": "def _get_database(self, database_name):\n        \"\"\"\n        Get PyMongo client pointing to the current database.\n\n        :return: MongoDB client of the current database.\n        :raise DataSourceError\n        \"\"\"\n        try:\n            return self._client[database_name]\n        except InvalidName as ex:\n            raise DataSourceError(\"Cannot connect to database %s!\"\n                                  % self._database) from ex"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_collection(self, collection_name):\n        try:\n            return self._database[collection_name]\n        except InvalidName as ex:\n            raise DataSourceError(\"Cannot access MongoDB collection %s!\"\n                                  % collection_name) from ex\n        except Exception as ex:\n            raise DataSourceError(\"Unexpected error when accessing MongoDB\"\n                                  \"collection %s!\"\n                                  % collection_name) from ex", "response": "Get PyMongo client pointing to the current DB and the given collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of all runs in the named collection.", "response": "def get_runs(self, sort_by=None, sort_direction=None,\n                 start=0, limit=None, query={\"type\": \"and\", \"filters\": []}):\n        \"\"\"\n        Return multiple runs (default all).\n\n        The query format (optional):\n\n        {\"type\": \"and\", \"filters\": [\n         {\"field\": \"host.hostname\", \"operator\": \"==\", \"value\": \"ntbacer\"},\n         {\"type\": \"or\", \"filters\": [\n            {\"field\": \"result\", \"operator\": \"==\", \"value\": 2403.52},\n            {\"field\": \"host.python_version\", \"operator\": \"==\", \"value\":\"3.5.2\"}\n            ]}]}\n\n        The parameter is built from clauses.\n        Each clause is either conjunctive (``\"and\"``), disjunctive (``\"or\"``),\n        or a *terminal clause*. Each of the the earlier two types must specify\n        the ``\"filters`` array of other clauses to be joined together\n        by that logical connective (and/or).\n\n        A terminal clause does not specifies its type, instead,\n        it has a different set of fields:\n        the ``field`` to be queried on (based on the MongoDB schema,\n        using dot notation to access nested documents), the ``operator``\n        (one of ``\"==\"``, ``\"!=\"``, ``\"<\"``, ``\"<=\"``, ``\">``, ``\">=\"``,\n        and ``\"regex\"`` for regular expressions).\n        The ``value`` field contains the value to be compared with\n        (either a string or a number). Notice that for the ``status`` field,\n        the ``RUNNING`` and ``DEAD`` runs are compared by\n        :func:`~PyMongoDataAccess.RUNNING_DEAD_RUN_CLAUSE` and\n        :func:`~PyMongoDataAccess.RUNNING_NOT_DEAD_CLAUSE`\n        \"\"\"\n        mongo_query = self._to_mongo_query(query)\n        r = self.generic_dao.find_records(self.collection_name, mongo_query,\n                                          sort_by, sort_direction, start,\n                                          limit)\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a single run from the database.", "response": "def get(self, run_id):\n        \"\"\"\n        Get a single run from the database.\n\n        :param run_id: The ID of the run.\n        :return: The whole object from the database.\n        :raise NotFoundError when not found\n        \"\"\"\n        id = self._parse_id(run_id)\n\n        run = self.generic_dao.find_record(self.collection_name,\n                                           {\"_id\": id})\n        if run is None:\n            raise NotFoundError(\"Run %s not found.\" % run_id)\n        return run"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying a sort to a cursor.", "response": "def _apply_sort(cursor, sort_by, sort_direction):\n        \"\"\"\n        Apply sort to a cursor.\n\n        :param cursor: The cursor to apply sort on.\n        :param sort_by: The field name to sort by.\n        :param sort_direction: The direction to sort, \"asc\" or \"desc\".\n        :return:\n\n        \"\"\"\n        if sort_direction is not None and sort_direction.lower() == \"desc\":\n            sort = pymongo.DESCENDING\n        else:\n            sort = pymongo.ASCENDING\n        return cursor.sort(sort_by, sort)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the query received by the Sacred Web API to a MongoDB query.", "response": "def _to_mongo_query(query):\n        \"\"\"\n        Convert the query received by the Sacred Web API to a MongoDB query.\n\n        Takes a query in format\n        {\"type\": \"and\", \"filters\": [\n         {\"field\": \"host.hostname\", \"operator\": \"==\", \"value\": \"ntbacer\"},\n         {\"type\": \"or\", \"filters\": [\n            {\"field\": \"result\", \"operator\": \"==\", \"value\": 2403.52},\n            {\"field\": \"host.python_version\", \"operator\": \"==\", \"value\":\"3.5.2\"}\n            ]}]}\n        and returns an appropriate MongoDB Query.\n        :param query: A query in the Sacred Web API format.\n        :return: Mongo Query.\n        \"\"\"\n        mongo_query = []\n        for clause in query[\"filters\"]:\n            if clause.get(\"type\") is None:\n                mongo_clause = MongoRunDAO. \\\n                    _simple_clause_to_query(clause)\n            else:\n                # It's a subclause\n                mongo_clause = MongoRunDAO._to_mongo_query(clause)\n            mongo_query.append(mongo_clause)\n\n        if len(mongo_query) == 0:\n            return {}\n        if query[\"type\"] == \"and\":\n            return {\"$and\": mongo_query}\n        elif query[\"type\"] == \"or\":\n            return {\"$or\": mongo_query}\n        else:\n            raise ValueError(\"Unexpected query type %s\" % query.get(\"type\"))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _simple_clause_to_query(clause):\n        # It's a regular clause\n        mongo_clause = {}\n        value = clause[\"value\"]\n        if clause[\"field\"] == \"status\" and clause[\"value\"] in [\"DEAD\",\n                                                               \"RUNNING\"]:\n            return MongoRunDAO. \\\n                _status_filter_to_query(clause)\n        if clause[\"operator\"] == \"==\":\n            mongo_clause[clause[\"field\"]] = value\n        elif clause[\"operator\"] == \">\":\n            mongo_clause[clause[\"field\"]] = {\"$gt\": value}\n        elif clause[\"operator\"] == \">=\":\n            mongo_clause[clause[\"field\"]] = {\"$gte\": value}\n        elif clause[\"operator\"] == \"<\":\n            mongo_clause[clause[\"field\"]] = {\"$lt\": value}\n        elif clause[\"operator\"] == \"<=\":\n            mongo_clause[clause[\"field\"]] = {\"$lte\": value}\n        elif clause[\"operator\"] == \"!=\":\n            mongo_clause[clause[\"field\"]] = {\"$ne\": value}\n        elif clause[\"operator\"] == \"regex\":\n            mongo_clause[clause[\"field\"]] = {\"$regex\": value}\n        return mongo_clause", "response": "Convert a simple clause from the Sacred Web API format to the MongoDB format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _status_filter_to_query(clause):\n        if clause[\"value\"] == \"RUNNING\":\n            mongo_clause = MongoRunDAO.RUNNING_NOT_DEAD_CLAUSE\n        elif clause[\"value\"] == \"DEAD\":\n            mongo_clause = MongoRunDAO.RUNNING_DEAD_RUN_CLAUSE\n        if clause[\"operator\"] == \"!=\":\n            mongo_clause = {\"$not\": mongo_clause}\n        return mongo_clause", "response": "Convert a clause querying for an experiment state RUNNING or DEAD to a MongoDB clause."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a run with the given id from the backend.", "response": "def delete(self, run_id):\n        \"\"\"\n        Delete run with the given id from the backend.\n\n        :param run_id: Id of the run to delete.\n        :raise NotImplementedError If not supported by the backend.\n        :raise DataSourceError General data source error.\n        :raise NotFoundError The run was not found. (Some backends may succeed\n        even if the run does not exist.\n        \"\"\"\n        return self.generic_dao.delete_record(self.collection_name,\n                                              self._parse_id(run_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(debug, no_browser, m, mu, mc, f, port, sub_url):\n    if m or mu != (None, None):\n        add_mongo_config(app, m, mu, mc)\n        app.config[\"data\"].connect()\n    elif f:\n        app.config[\"data\"] = FileStorage(f)\n    else:\n        print(\"Must specify either a mongodb instance or \" +\n              \"a path to a file storage.\\nRun sacredboard --help \"\n              \"for more information.\", file=sys.stderr)\n        sys.exit(1)\n\n    app.config['DEBUG'] = debug\n    app.debug = debug\n\n    version = Sacredboard.get_version()\n    print(version)\n\n    app_config = {\n        \"http.serve_on_endpoint\": sub_url,\n        \"http.port\": port,\n        \"debug\": debug\n    }\n    _initialize_modules(app_config)\n    print(\"Starting sacredboard on port %d\" % server_runner.started_on_port)\n    if not no_browser:\n        click.launch(\"http://127.0.0.1:%d\" % server_runner.started_on_port)\n\n    server_runner.run_server()", "response": "This function starts the main function of the main function of the main function of the main function of the main function of the main function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_mongo_config(app, simple_connection_string,\n                     mongo_uri, collection_name):\n    \"\"\"\n    Configure the application to use MongoDB.\n\n    :param app: Flask application\n    :param simple_connection_string:\n                Expects host:port:database_name or database_name\n                Mutally_exclusive with mongo_uri\n    :param mongo_uri: Expects mongodb://... as defined\n                in https://docs.mongodb.com/manual/reference/connection-string/\n                Mutually exclusive with simple_connection_string (must be None)\n    :param collection_name: The collection containing Sacred's runs\n    :return:\n    \"\"\"\n    if mongo_uri != (None, None):\n        add_mongo_config_with_uri(app, mongo_uri[0], mongo_uri[1],\n                                  collection_name)\n        if simple_connection_string is not None:\n            print(\"Ignoring the -m option. Overridden by \"\n                  \"a more specific option (-mu).\", file=sys.stderr)\n    else:\n        # Use the default value 'sacred' when not specified\n        if simple_connection_string is None:\n            simple_connection_string = \"sacred\"\n        add_mongo_config_simple(app, simple_connection_string, collection_name)", "response": "Configure the application to use MongoDB."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconfigure the app to use MongoDB.", "response": "def add_mongo_config_simple(app, connection_string, collection_name):\n    \"\"\"\n    Configure the app to use MongoDB.\n\n    :param app: Flask Application\n    :type app: Flask\n    :param connection_string: in format host:port:database or database\n            (default: sacred)\n    :type connection_string: str\n\n    :param collection_name: Name of the collection\n    :type collection_name: str\n    \"\"\"\n    split_string = connection_string.split(\":\")\n    config = {\"host\": \"localhost\", \"port\": 27017, \"db\": \"sacred\"}\n    if len(split_string) > 0 and len(split_string[-1]) > 0:\n        config[\"db\"] = split_string[-1]\n    if len(split_string) > 1:\n        config[\"port\"] = int(split_string[-2])\n    if len(split_string) > 2:\n        config[\"host\"] = split_string[-3]\n    app.config[\"data\"] = PyMongoDataAccess.build_data_access(\n        config[\"host\"], config[\"port\"], config[\"db\"], collection_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconfiguring PyMongo with a MongoDB connection string.", "response": "def add_mongo_config_with_uri(app, connection_string_uri,\n                              database_name, collection_name):\n    \"\"\"\n    Configure PyMongo with a MongoDB connection string.\n\n    :param app: Flask application\n    :param connection_string_uri: MongoDB connection string\n    :param database_name: Sacred database name\n    :param collection_name: Sacred's collection with runs\n    :return:\n    \"\"\"\n    app.config[\"data\"] = PyMongoDataAccess.build_data_access_with_uri(\n        connection_string_uri, database_name, collection_name\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop_all_tensorboards():\n    for process in Process.instances:\n        print(\"Process '%s', running %d\" % (process.command[0],\n                                            process.is_running()))\n        if process.is_running() and process.command[0] == \"tensorboard\":\n            process.terminate()", "response": "Terminate all TensorBoard instances."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlaunching a new TensorBoard instance and wait for a message.", "response": "def run_tensorboard(logdir, listen_on=\"0.0.0.0\", port=0, tensorboard_args=None, timeout=10):\n    \"\"\"\n    Launch a new TensorBoard instance.\n\n    :param logdir: Path to a TensorFlow summary directory\n    :param listen_on: The IP address TensorBoard should listen on.\n    :param port: Port number to listen on. 0 for a random port.\n    :param tensorboard_args: Additional TensorBoard arguments.\n    :param timeout: Timeout after which the Timeout\n    :type timeout: float\n    :return: Returns the port TensorBoard is listening on.\n    :raise UnexpectedOutputError\n    :raise TensorboardNotFoundError\n    :raise TimeoutError\n    \"\"\"\n    if tensorboard_args is None:\n        tensorboard_args = []\n    tensorboard_instance = Process.create_process(\n        TENSORBOARD_BINARY.split(\" \") +\n        [\"--logdir\", logdir, \"--host\", listen_on, \"--port\", str(port)] + tensorboard_args)\n    try:\n        tensorboard_instance.run()\n    except FileNotFoundError as ex:\n        raise TensorboardNotFoundError(ex)\n\n    # Wait for a message that signaliezes start of Tensorboard\n    start = time.time()\n    data = \"\"\n    while time.time() - start < timeout:\n        line = tensorboard_instance.read_line_stderr(time_limit=timeout)\n        data += line\n        if \"at http://\" in line:\n            port = parse_port_from_tensorboard_output(line)\n            # Good case\n            return port\n        elif \"TensorBoard attempted to bind to port\" in line:\n            break\n\n    tensorboard_instance.terminate()\n    raise UnexpectedOutputError(\n        data,\n        expected=\"Confirmation that Tensorboard has started\"\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a TensorBoard port from its outputted message of TensorBoard 1. 8. 0 at http://martin - VirtualBox : 36869 at http://martin - VirtualBox. 36869 at http://martin - VirtualBox : 36869 at http://martin - VirtualBox : 36869 at http://martin - VirtualBox : 36869 at http://martin - VirtualBox : 36869 at http", "response": "def parse_port_from_tensorboard_output(tensorboard_output: str) -> int:\n    \"\"\"\n    Parse tensorboard port from its outputted message.\n\n    :param tensorboard_output: Output message of Tensorboard\n    in format TensorBoard 1.8.0 at http://martin-VirtualBox:36869\n    :return: Returns the port TensorBoard is listening on.\n    :raise UnexpectedOutputError\n    \"\"\"\n    search = re.search(\"at http://[^:]+:([0-9]+)\", tensorboard_output)\n    if search is not None:\n        port = search.group(1)\n        return int(port)\n    else:\n        raise UnexpectedOutputError(tensorboard_output, \"Address and port where Tensorboard has started,\"\n                                                        \" e.g. TensorBoard 1.8.0 at http://martin-VirtualBox:36869\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the database connection.", "response": "def connect(self):\n        \"\"\"Initialize the database connection.\"\"\"\n        self._client = self._create_client()\n        self._db = getattr(self._client, self._db_name)\n        self._generic_dao = GenericDAO(self._client, self._db_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a data access gateway.", "response": "def build_data_access(host, port, database_name, collection_name):\n        \"\"\"\n        Create data access gateway.\n\n        :param host: The database server to connect to.\n        :type host: str\n        :param port: Database port.\n        :type port: int\n        :param database_name: Database name.\n        :type database_name: str\n        :param collection_name: Name of the collection with Sacred runs.\n        :type collection_name: str\n        \"\"\"\n        return PyMongoDataAccess(\"mongodb://%s:%d\" % (host, port),\n                                 database_name, collection_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initialize(app: Flask, app_config):\n    sub_url = app_config[\"http.serve_on_endpoint\"]\n    if sub_url != \"/\":\n        app.wsgi_app = ReverseProxied(app.wsgi_app, script_name=sub_url)", "response": "Initialize the module.\n\n    :param app: The Flask application.\n    :param app_config: Application configuration dictionary. If `http.serve_on_endpoint` attribute\n     is specified (e.g. /sacredboard/), the application will be hosted on that endpoint\n    (e.g. http://localhost:5000/sacredboard/)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_tensorboard(run_id, tflog_id):\n    data = current_app.config[\"data\"]\n    # optimisticaly suppose the run exists...\n    run = data.get_run_dao().get(run_id)\n    base_dir = Path(run[\"experiment\"][\"base_dir\"])\n    log_dir = Path(run[\"info\"][\"tensorflow\"][\"logdirs\"][tflog_id])\n    # TODO ugly!!!\n    if log_dir.is_absolute():\n        path_to_log_dir = log_dir\n    else:\n        path_to_log_dir = base_dir.joinpath(log_dir)\n\n    port = int(tensorboard.run_tensorboard(str(path_to_log_dir)))\n    url_root = request.url_root\n    url_parts = re.search(\"://([^:/]+)\", url_root)\n    redirect_to_address = url_parts.group(1)\n    return redirect(\"http://%s:%d\" % (redirect_to_address, port))", "response": "Launch TensorBoard for a given run ID and log ID of that run."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, run_id, metric_id):\n        run_id = self._parse_run_id(run_id)\n        query = self._build_query(run_id, metric_id)\n        row = self._read_metric_from_db(metric_id, run_id, query)\n        metric = self._to_intermediary_object(row)\n        return metric", "response": "Read a metric from the given id and run."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self, run_id):\n        self.generic_dao.delete_record(\n            self.metrics_collection_name,\n            {\"run_id\": self._parse_run_id(run_id)})", "response": "Delete all metrics belonging to the given run."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the run with the given id.", "response": "def delete_run(self, run_id):\n        \"\"\"\n        Delete run of the given run_id.\n\n        :raise NotImplementedError If not supported by the backend.\n        :raise DataSourceError General data source error.\n        :raise NotFoundError The run was not found. (Some backends may succeed even if the run does not exist.\n        \"\"\"\n        ds = self.datastorage\n        ds.get_metrics_dao().delete(run_id)\n        # TODO: implement\n        # ds.get_artifact_dao().delete(run_id)\n        # ds.get_resource_dao().delete(run_id)\n        ds.get_run_dao().delete(run_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all runs in the file store.", "response": "def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={\"type\": \"and\", \"filters\": []}):\n        \"\"\"\n        Return all runs in the file store.\n\n        If a run is corrupt, e.g. missing files, it is skipped.\n\n        :param sort_by: NotImplemented\n        :param sort_direction:  NotImplemented\n        :param start: NotImplemented\n        :param limit: NotImplemented\n        :param query: NotImplemented\n        :return: FileStoreCursor\n        \"\"\"\n        all_run_ids = os.listdir(self.directory)\n\n        def run_iterator():\n            blacklist = set([\"_sources\"])\n            for id in all_run_ids:\n                if id in blacklist:\n                    continue\n                try:\n                    yield self.get(id)\n                except FileNotFoundError:\n                    # An incomplete experiment is a corrupt experiment.\n                    # Skip it for now.\n                    # TODO\n                    pass\n\n        count = len(all_run_ids)\n        return FileStoreCursor(count, run_iterator())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the run associated with a particular run_id.", "response": "def get(self, run_id):\n        \"\"\"\n        Return the run associated with a particular `run_id`.\n\n        :param run_id:\n        :return: dict\n        :raises FileNotFoundError\n        \"\"\"\n        config = _read_json(_path_to_config(self.directory, run_id))\n        run = _read_json(_path_to_run(self.directory, run_id))\n        try:\n            info = _read_json(_path_to_info(self.directory, run_id))\n        except IOError:\n            info = {}\n\n        return _create_run(run_id, run, config, info)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a specific Sacred metric from the database.", "response": "def get_metric(run_id, metric_id):\n    \"\"\"\n    Get a specific Sacred metric from the database.\n\n    Returns a JSON response or HTTP 404 if not found.\n    Issue: https://github.com/chovanecm/sacredboard/issues/58\n    \"\"\"\n    data = current_app.config[\"data\"]  # type: DataStorage\n    dao = data.get_metrics_dao()\n    metric = dao.get(run_id, metric_id)\n\n    return Response(render_template(\n        \"api/metric.js\",\n        run_id=metric[\"run_id\"],\n        metric_id=metric[\"metric_id\"],\n        name=metric[\"name\"],\n        steps=metric[\"steps\"],\n        timestamps=metric[\"timestamps\"],\n        values=metric[\"values\"]),\n        mimetype=\"application/json\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the server to run and determine the port.", "response": "def initialize(self, app: Flask, app_config):\n        \"\"\"\n        Prepare the server to run and determine the port.\n\n        :param app: The Flask Application.\n        :param app_config: Configuration dictionary. This module uses the `debug`\n        (`True`/`False`) and `http.port` attributes.\n        \"\"\"\n        debug = app_config[\"debug\"]\n        port = app_config[\"http.port\"]\n        if debug:\n            self.started_on_port = port\n            app.run(host=\"0.0.0.0\", debug=True, port=port)\n        else:\n            for port in range(port, port + 50):\n                self.http_server = WSGIServer(('0.0.0.0', port), app)\n                try:\n                    self.http_server.start()\n                except OSError:\n                    # try next port\n                    continue\n                self.started_on_port = port\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete the given run and corresponding entities.", "response": "def api_run_delete(run_id):\n    \"\"\"Delete the given run and corresponding entities.\"\"\"\n    data = current_app.config[\"data\"]  # type: DataStorage\n    RunFacade(data).delete_run(run_id)\n    return \"DELETED run %s\" % run_id"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef api_run_get(run_id):\n    data = current_app.config[\"data\"]\n    run = data.get_run_dao().get(run_id)\n    records_total = 1 if run is not None else 0\n    if records_total == 0:\n        return Response(\n            render_template(\n                \"api/error.js\",\n                error_code=404,\n                error_message=\"Run %s not found.\" % run_id),\n            status=404,\n            mimetype=\"application/json\")\n    records_filtered = records_total\n    return Response(render_template(\"api/runs.js\", runs=[run], draw=1,\n                                    recordsTotal=records_total,\n                                    recordsFiltered=records_filtered,\n                                    full_object=True),\n                    mimetype=\"application/json\")", "response": "Return a single run as a JSON object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a given URL parameter as int or return the default value.", "response": "def parse_int_arg(name, default):\n    \"\"\"Return a given URL parameter as int or return the default value.\"\"\"\n    return default if request.args.get(name) is None \\\n        else int(request.args.get(name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the Run query filter from the URL as a dictionary.", "response": "def parse_query_filter():\n    \"\"\"Parse the Run query filter from the URL as a dictionary.\"\"\"\n    query_string = request.args.get(\"queryFilter\")\n    if query_string is None:\n        return {\"type\": \"and\", \"filters\": []}\n    query = json.loads(query_string)\n    assert type(query) == dict\n    assert type(query.get(\"type\")) == str\n    return query"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all runs sort it and return a response.", "response": "def get_runs():\n    \"\"\"Get all runs, sort it and return a response.\"\"\"\n    data = current_app.config[\"data\"]\n    draw = parse_int_arg(\"draw\", 1)\n    start = parse_int_arg(\"start\", 0)\n    length = parse_int_arg(\"length\", -1)\n    length = length if length >= 0 else None\n    order_column = request.args.get(\"order[0][column]\")\n    order_dir = request.args.get(\"order[0][dir]\")\n\n    query = parse_query_filter()\n\n    if order_column is not None:\n        order_column = \\\n            request.args.get(\"columns[%d][name]\" % int(order_column))\n        if order_column == \"hostname\":\n            order_column = \"host.hostname\"\n\n    runs = data.get_run_dao().get_runs(\n        start=start, limit=length,\n        sort_by=order_column, sort_direction=order_dir, query=query)\n    # records_total should be the total size of the records in the database,\n    # not what was returned\n    records_total = runs.count()\n    records_filtered = runs.count()\n    return Response(render_template(\n        \"api/runs.js\", runs=runs,\n        draw=draw, recordsTotal=records_total,\n        recordsFiltered=records_filtered),\n        mimetype=\"application/json\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, file_id: str) -> [typing.BinaryIO, str, datetime.datetime]:\n        raise NotImplementedError(\"Downloading files for downloading files in FileStore has not been implemented yet.\")", "response": "Return the file identified by a file_id string its file name and upload date."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef timediff(time):\n    now = datetime.datetime.utcnow()\n    diff = now - time\n    diff_sec = diff.total_seconds()\n    return diff_sec", "response": "Return the difference in seconds between now and the given time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef last_line(text):\n    last_line_of_text = \"\"\n    while last_line_of_text == \"\" and len(text) > 0:\n        last_line_start = text.rfind(\"\\n\")\n        # Handle one-line strings (without \\n)\n        last_line_start = max(0, last_line_start)\n        last_line_of_text = text[last_line_start:].strip(\"\\r\\n \")\n        text = text[:last_line_start]\n    return last_line_of_text", "response": "Get the last meaningful line of the text that is the last non - empty line."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndumps Python object as JSON string.", "response": "def dump_json(obj):\n    \"\"\"Dump Python object as JSON string.\"\"\"\n    return simplejson.dumps(obj, ignore_nan=True, default=json_util.default)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nterminating all processes. :param wait: Wait for each to terminate :type wait: bool :return: :rtype:", "response": "def terminate_all(wait=False):\n        \"\"\"\n        Terminate all processes.\n\n        :param wait: Wait for each to terminate\n        :type wait: bool\n        :return:\n        :rtype:\n        \"\"\"\n        for instance in Process.instances:\n            if instance.is_running():\n                instance.terminate(wait)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a line from the process stdout.", "response": "def read_line(self, time_limit=None):\n        \"\"\"\n        Read a line from the process.\n\n        On Windows, this the time_limit has no effect, it always blocks.\n        \"\"\"\n        if self.proc is not None:\n            return self.proc.stdout.readline().decode()\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_line_stderr(self, time_limit=None):\n        if self.proc is not None:\n            return self.proc.stderr.readline().decode()\n        else:\n            return None", "response": "Read a line from the process s stderr."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef guess_import_format(filename, **kwargs):\n    '''\n    guess_import_format(filename) attempts to guess the file format for the given filename; it does\n      this guessing by looking at the file extension and using registered sniff-tests from\n      importers. It will not attempt to load the file, so if the extension of the filename is\n      missing, it is unlikely that this function will deduce the file-type (though load will often\n      succeeed at extracting the data by trying all types exhaustively). If guess_import_format\n      cannot deduce the format, it yields None.\n\n    Note that if the filename has an extension that is recognized by neuropythy but the file itself\n    is of another format, this function will never look beyond the extention in the filename;\n    neither this function nor load perform that level of deduction.\n    \n    Keyword arguments that are passed to load should also be passed to guess_import_format.\n    '''\n    # first try file extension\n    (_,filename) = os.path.split(filename)\n    if '.' in filename:\n        fnm = filename.lower()\n        fmt = next((k for (k,(_,es,_)) in six.iteritems(importers)\n                    if any(fnm.endswith('.' + e) for e in es)),\n                   None)\n        if fmt: return fmt\n    # that didn't work; let's check the sniffers\n    for (k,(_,_,sniff)) in six.iteritems(importers):\n        try:\n            if sniff(filename, **kwargs): return k\n        except Exception: pass\n    return None", "response": "This function tries to guess the file format for the given filename. It will try to guess the file format for the given filename. If it fails it will return None."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a neuropythy object from a file.", "response": "def load(filename, format=None, **kwargs):\n    '''\n    load(filename) yields the data contained in the file referenced by the given filename in a\n      neuropythy or neuropythy-friendly format.\n    load(filename, format) specifies that the given format should be used; this should be the name\n      of the importer (though a file extension that is recognized also will work).\n\n    Additionally, functions located in load.<format> may be used; so, for example, the following\n    are equivalent calls:\n      load(filename, 'nifti')\n      load.nifti(filename)\n    In fact, the load.nifti function is just the nifti importer, so help(load.nifti) will also\n    yield documentation for the nifti importer.\n\n    Keyword options may be passed to load; these must match those accepted by the given import\n    function.\n    '''\n    from neuropythy.util import ObjectWithMetaData\n    filename = os.path.expanduser(filename)\n    if format is None:\n        format = guess_import_format(filename, **kwargs)\n        if format is None:\n            # try formats and see if one works!\n            for (k,(f,_,_)) in six.iteritems(importers):\n                try:              return f(filename, **kwargs)\n                except Exception: pass\n            raise ValueError('Could not deduce format of file %s' % filename)\n    format = format.lower()\n    if format not in importers:\n        raise ValueError('Format \\'%s\\' not recognized by neuropythy' % format)\n    (f,_,_) = importers[format]\n    obj = f(filename, **kwargs)\n    if isinstance(obj, ObjectWithMetaData): return obj.with_meta(source_filename=filename)\n    else: return obj"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nforget about an importer of type name.", "response": "def forget_importer(name):\n    '''\n    forget_importer(name) yields True if an importer of type name was successfully forgotten from\n      the neuropythy importers list and false otherwise. This function must be called before an\n      importer can be replaced.\n    '''\n    global importers\n    name = name.lower()\n    if name in importers:\n        importers = importers.discard(name)\n        delattr(load, name)\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(filename, data, format=None, **kwargs):\n    '''\n    save(filename, data) writes the given data to the given filename then yieds that filename.\n    save(filename, data, format) specifies that the given format should be used; this should be the\n      name of the exporter (though a file extension that is recognized also will work).\n\n    Additionally, functions located in save.<format> may be used; so, for example, the following\n    are equivalent calls:\n      save(filename, image, 'nifti')\n      save.nifti(filename, image)\n    In fact, the save.nifti function is just the nifti exporter, so help(save.nifti) will also\n    yield documentation for the nifti exporter.\n\n    Keyword options may be passed to save; these must match those accepted by the given export\n    function.\n    '''\n    filename = os.path.expanduser(os.path.expandvars(filename))\n    if format is None:\n        format = guess_export_format(filename, data, **kwargs)\n        if format is None:\n            raise ValueError('Could not deduce export format for file %s' % filename)\n    else:\n        format = format.lower()\n        if format not in exporters:\n            # it might be an extension\n            fmt = next((k for (k,(_,es,_)) in six.iteritems(exporters) if format in es), None)\n            if fmt is None:\n                # okay, no idea what it is\n                raise ValueError('Format \\'%s\\' not recognized by neuropythy' % format)\n            format = fmt\n    (f,_,_) = exporters[format]\n    return f(filename, data, **kwargs)", "response": "Save the given data to a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a function that can be used to save a neuropythy file of the given type.", "response": "def exporter(name, extensions=None, sniff=None):\n    '''\n    @exporter(name) is a decorator that declares that the following function is an file saveing\n      function that should be registered with the neuropythy save function. See also the\n      forget_exporter function.\n\n    Any exporter function must take, as its first argument, a filename and, as its second argument,\n    the object to be exported; after that it may take any number of keyword arguments, but no other\n    non-keyword arguments. These keyword arguments can be passed to the neuropythy save function.\n    \n    The following options are accepted:\n      * extensions (default: None) may be a string or a collection of strings that indicate possible\n        file extensions for files of this type.\n      * sniff (default: None) may optionally be a function f(s, d) that yields True when the given\n        string s is a filename for a file of this type and/or the given object d is an object that\n        can be exported as this type. If no sniff is given, this type can still be detected by\n        running all exporters exhaustively and catching any raised exceptions; though this may\n        result in partial files written to disk, so is not used by save.\n    '''\n    name = name.lower()\n    if name in exporters:\n        raise ValueError('An exporter for type %s already exists; use forget_exporter' % name)\n    extensions = (extensions,) if pimms.is_str(extensions) else \\\n                 ()            if extensions is None       else \\\n                 tuple(extensions)\n    def _exporter(f):\n        global exporters\n        exporters = exporters.set(name, (f, extensions, sniff))\n        setattr(save, name, f)\n        return f\n    return _exporter"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nforgets about an exporter of type name from the neuropythy exporters list and return True if it was successfully forgetted from the neuropythy exporters list and False otherwise.", "response": "def forget_exporter(name):\n    '''\n    forget_exporter(name) yields True if an exporter of type name was successfully forgotten from\n      the neuropythy exporters list and false otherwise. This function must be called before an\n      exporter can be replaced.\n    '''\n    global exporters\n    name = name.lower()\n    if name in exporters:\n        exporters = exporters.discard(name)\n        delattr(save, name)\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a single object from a json file or stream object filename.", "response": "def load_json(filename, to='auto'):\n    '''\n    load_json(filename) yields the object represented by the json file or stream object filename.\n    \n    The optional argument to may be set to None to indicate that the JSON data should be returned\n    verbatim rather than parsed by neuropythy's denormalize system.\n    '''\n    from neuropythy.util import denormalize as denorm\n    if pimms.is_str(filename):\n        try:\n            with gzip.open(filename, 'rt') as fl: dat = json.load(fl)\n        except Exception:\n            with open(filename, 'rt') as fl: dat = json.load(fl)\n    else:\n        dat = json.load(filename)\n        filename = '<stream>'\n    if to is None: return dat\n    elif to == 'auto': return denorm(dat)\n    else: raise ValueError('unrecognized to option: %s' % to)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_json(filename, obj, normalize=True):\n    '''\n    save_json(filename, obj) writes the given object to the given filename (or stream) in a\n      normalized JSON format.\n\n    The optional argument normalize (default True) may be set to False to prevent the object from\n    being run through neuropythy's normalize system.\n    '''\n    from neuropythy.util import normalize as norm\n    dat = norm(obj) if normalize else obj\n    if pimms.is_str(filename):\n        if any(filename.endswith(s) for s in ('.gz', '.bz2', '.lzma')):\n            with gzip.open(filename, 'wt') as fl: json.dump(dat, fl)\n        else:\n            with open(filename, 'wt') as fl: json.dump(dat, fl)\n    else: json.dump(dat, filename)\n    return filename", "response": "Save the given object to a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_csv(filename, **kw):\n    '''\n    load_csv(filename) yields a pandas dataframe of the contents of the CSV file. If pandas cannot\n      be loaded, then an error is raised.\n\n    All optional arguments are passed along to the pandas.read_csv function.\n    '''\n    import pandas\n    if any(filename.endswith(s) for s in ('.gz', '.bz2', '.lzma')):\n        with gzip.open(filename, 'rt') as fl: data = pandas.read_csv(fl, **kw)\n    else:\n        with open(filename, 'rt') as fl: data = pandas.read_csv(fl, **kw)\n    return data", "response": "Load a pandas dataframe of the contents of the CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_csv(filename, dat, index=False, **kw):\n    '''\n    save_csv(filename, d) writes a pandas dataframe d to a CSV file with the given name. If pandas\n      cannot be loaded, then an error is raised. If d is not a dataframe, to_dataframe() is called\n      on it.\n\n    All optional arguments are passed along to the pandas.DataFrame.to_csv function.\n    '''\n    import pandas\n    from neuropythy.util import to_dataframe\n    d = to_dataframe(dat)\n    if any(filename.endswith(s) for s in ('.gz', '.bz2', '.lzma')):\n        with gzip.open(filename, 'wt', newlines='') as fl: d.to_csv(fl, index=index, **kw)\n    else:\n        with open(filename, 'wt') as fl: d.to_csv(fl, index=index, **kw)\n    return dat", "response": "Save a dataframe d to a CSV file with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_tsv(filename, sep='\\t', **kw):\n    '''\n    load_tsv(filename) yields a pandas dataframe of the contents of the TSV file. If pandas cannot\n      be loaded, then an error is raised.\n\n    All optional arguments are passed along to the pandas.read_csv function. Note that this function\n    is identical to the load_csv() function except that it has a default sep value of '\\t' instead\n    of ','.\n    '''\n    import pandas\n    if any(filename.endswith(s) for s in ('.gz', '.bz2', '.lzma')):\n        with gzip.open(filename, 'rt') as fl: data = pandas.read_csv(fl, sep=sep, **kw)\n    else:\n        with open(filename, 'rt') as fl: data = pandas.read_csv(fl, sep=sep, **kw)\n    return data", "response": "Load a pandas dataframe of the contents of the TSV file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_nifti(filename, to='auto'):\n    '''\n    load_nifti(filename) yields the Nifti1Image or Nifti2Image referened by the given filename by\n      using the nibabel load function.\n    \n    The optional argument to may be used to coerce the resulting data to a particular format; the\n    following arguments are understood:\n      * 'header' will yield just the image header\n      * 'data' will yield the image's data-array\n      * 'field' will yield a squeezed version of the image's data-array and will raise an error if\n        the data object has more than 2 non-unitary dimensions (appropriate for loading surface\n        properties stored in image files)\n      * 'affine' will yield the image's affine transformation\n      * 'image' will yield the raw image object\n      * 'auto' is equivalent to 'image' unless the image has no more than 2 non-unitary dimensions,\n        in which case it is assumed to be a surface-field and the return value is equivalent to\n        the 'field' value.\n    '''\n    img = nib.load(filename)\n    to = to.lower()\n    if to == 'image':    return img\n    elif to == 'data':   return img.get_data()\n    elif to == 'affine': return img.affine\n    elif to == 'header': return img.header\n    elif to == 'field':\n        dat = np.squeeze(np.asarray(img.get_data()))\n        if len(dat.shape) > 2:\n            raise ValueError('image requested as field has more than 2 non-unitary dimensions')\n        return dat\n    elif to in ['auto', 'automatic']:\n        dims = set(np.shape(img.get_data()))\n        if 1 < len(dims) < 4 and 1 in dims:\n            return np.squeeze(np.asarray(img.get_data()))\n        else:\n            return img\n    else:\n        raise ValueError('unrecognized \\'to\\' argument \\'%s\\'' % to)", "response": "Loads a Nifti file into a new Nifti1Image or Nifti2Image object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert an object into a Nifti2Image object.", "response": "def to_nifti(obj, like=None, header=None, affine=None, extensions=Ellipsis, version=1):\n    '''\n    to_nifti(obj) yields a Nifti2Image object that is as equivalent as possible to the given object\n      obj. If obj is a Nifti2Image already, then it is returned unmolested; other deduction rules\n      are described below.\n\n    The following options are accepted:\n      * like (default: None) may be provided to give a guide for the various header- and meta-data\n        that is included in the image. If this is a nifti image object, its meta-data are used; if\n        this is a subject, then the meta-data are deduced from the subject's voxel and native\n        orientation matrices. All other specific options below override anything deduced from the\n        like argument.\n      * header (default: None) may be a Nifti1 or Niti2 image header to be used as the nifti header\n        or to replace the header in a new image.\n      * affine (default: None) may specify the affine transform to be given to the image object.\n      * extensions (default: Ellipsis) may specify a nifti extensions object that should be included\n        in the header. The default value, Ellipsis, indicates that the extensions should not be\n        changed, and that None should be used if extensions are not implied in obj (if, for example,\n        obj is a data array rather than an image object with a header already.\n      * version (default: 2) may be specified as 1 or 2 for a Nifti1Image or Nifti2Image object,\n        respectively.\n    '''\n    from neuropythy.mri import Subject\n    obj0 = obj\n    # First go from like to explicit versions of affine and header:\n    if like is not None:\n        if isinstance(like, nib.analyze.AnalyzeHeader) or \\\n           isinstance(like, nib.freesurfer.mghformat.MGHHeader):\n            if header is None: header = like\n        elif isinstance(like, nib.analyze.SpatialImage):\n            if header is None: header = like.header\n            if affine is None: affine = like.affine\n        elif isinstance(like, Subject):\n            if affine is None: affine = like.voxel_to_native_matrix\n        else:\n            raise ValueError('Could not interpret like argument with type %s' % type(like))\n    # check to make sure that we have to change something:\n    elif ((version == 1 and isinstance(obj, nib.nifti1.Nifti1Image)) or\n          (version == 2 and isinstance(obj, nib.nifti2.Nifti2Image))):\n        if ((header is None or obj.header is header) and\n            (extensions is Ellipsis or extensions is obj.header.extensions or\n             (extensions is None and len(obj.header.extensions) == 0))):\n            return obj\n    # okay, now look at the header and affine etc.\n    if header is None:\n        if isinstance(obj, nib.analyze.SpatialImage):\n            header = obj.header\n        else:\n            header = nib.nifti1.Nifti1Header() if version == 1 else nib.nifti2.Nifti2Header()\n    if affine is None:\n        if isinstance(obj, nib.analyze.SpatialImage):\n            affine = obj.affine\n        else:\n            affine = np.eye(4)\n    if extensions is None:\n        extensions = nib.nifti1.Nifti1Extensions()\n    # Figure out what the data is\n    if isinstance(obj, nib.analyze.SpatialImage):\n        obj = obj.dataobj\n    elif not pimms.is_nparray(obj):\n        obj = np.asarray(obj)\n    if len(obj.shape) < 3: obj = np.asarray([[obj]])\n    # Okay, make a new object now...\n    if version == 1:\n        obj = nib.nifti1.Nifti1Image(obj, affine, header)\n    elif version == 2:\n        obj = nib.nifti2.Nifti2Image(obj, affine, header)\n    else:\n        raise ValueError('invalid version given (should be 1 or 2): %s' % version)\n    # add the extensions if they're needed\n    if extensions is not Ellipsis and (len(extensions) > 0 or len(obj.header.extensions) > 0):\n        obj.header.extensions = extensions\n    # Okay, that's it!\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave a nifti object to a file.", "response": "def save_nifti(filename, obj, like=None, header=None, affine=None, extensions=Ellipsis, version=1):\n    '''\n    save_nifti(filename, obj) saves the given object to the given filename in the nifti format and\n      returns the filename.\n\n    All options that can be given to the to_nifti function can also be passed to this function; they\n    are used to modify the object prior to exporting it.   \n    '''\n    obj = to_nifti(obj, like=like, header=header, affine=affine, version=version,\n                   extensions=extensions)\n    obj.to_filename(filename)\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef url_download(url, topath=None, create_dirs=True):\n    '''\n    url_download(url) yields the contents of the given url as a byte-string.\n    url_download(url, topath) downloads the given url to the given path, topath and yields that path\n      on success.\n\n    The option create_dirs (default: True) may be set to False to prevent the topath directory from\n    being created.\n    '''\n    # ensure directory exists\n    if topath: topath = os.path.expanduser(os.path.expandvars(topath))\n    if create_dirs and topath:\n        dnm = os.path.dirname(topath)\n        if not os.path.isdir(dnm): os.makedirs(os.path.abspath(dnm), 0o755)\n    if six.PY2:\n        response = urllib.request.urlopen(url)\n        if topath is None: topath = response.read()\n        else:\n            with open(topath, 'wb') as fl:\n                shutil.copyfileobj(response, fl)\n    else:\n        with urllib.request.urlopen(url) as response:\n            if topath is None: topath = response.read()\n            else:\n                with open(topath, 'wb') as fl:\n                    shutil.copyfileobj(response, fl)\n    return topath", "response": "Download the given url to the given path topath."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_tarball_path(path):\n    '''\n    split_tarball_path(path) yields a tuple (tarball, p) in which tarball is the path to the tarball\n      referenced by path and p is the internal path that followed that tarball. If no tarball is\n      included in the path, then (None, path) is returned. If no internal path is found following\n      the tarball, then (path, '') is returned.\n    '''\n    lpath = path.lower()\n    for e in tarball_endings:\n        if lpath.endswith(e): return (path, '')\n        ee = e + ':'\n        if ee not in lpath: continue\n        spl = path.split(ee)\n        tarball = spl[0] + e\n        p = ee.join(spl[1:])\n        return (tarball, p)\n    return (None, path)", "response": "Split a tarball path into a tuple of tarball and internal path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pseudo_dir(source_path, cache_path=None, delete=Ellipsis, credentials=None, meta_data=None):\n    '''\n    pseudo_dir(source_path) yields a pseudo-directory object that represents files in the given\n      source path.\n\n    Pseudo-dir objects act as an interface for loading data from abstract sources. The given source\n    path may be either a directory, a (possibly zipped) tarball, or a URL. In all cases but the\n    local directory, the pseudo-dir object will quietly extract/download the requested files to a\n    cache directory as their paths are requested. This is managed through two methods:\n      * find(args...) joins the argument list as in os.path.join, then, if the resulting file is\n        found in the source_path, this (relative) path-name is returned; otherwise None is returned.\n      * local_path(args...) joins the argument list as in os.path.join, then, if the resulting file\n        is found in the source_path, it is extracted/downloaded to the local cache directory if\n        necessary, and this path (or the original path when no cache directory is used) is returned.\n\n    The following optional arguments may be given:\n      * cache_path (default: None) specifies the cache directory in which to put any extracted or\n        downloaded contents. If None, then a temporary directory is created and used. If the source\n        path is a local directory, then the cache path is not needed and is instead ignored. Note\n        that if the cache path is not deleted, it can be reused across sessions--the pseudo-dir will\n        always check for files in the cache path before extracting or downloading them.\n      * delete (default: Ellipsis) may be set to True or False to declare that the cache directory\n        should be deleted at system exit (assuming a normal Python system exit). If Ellipsis, then\n        the cache_path is deleted only if it is created by the pseudo-dir object--given cache paths\n        are never deleted.\n      * credentials (default: None) may be set to a valid set of Amazon S3 credentials for use if\n        the source path is an S3 path. The contents are passed through the to_credentials function.\n      * meta_data (default: None) specifies an optional map of meta-data for the pseudo-dir.\n    '''\n    return PseudoDir(source_path, cache_path=cache_path, delete=delete, credentials=credentials,\n                     meta_data=meta_data)", "response": "Returns a pseudo - directory object that represents the files in the given source path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef file_map(path, instructions, **kw):\n    '''\n    file_map(path, instructions) yields a file-map object for the given path and instruction-set.\n    file_map(None, instructions) yields a lambda of exactly one argument that is equivalent to the\n      following:  lambda p: file_map(p, instructions)\n    \n    File-map objects are pimms immutable objects that combine a format-spec for a directory \n    (instructions) with a directory to yield a lazily-loaded data object. The format-spec is not\n    currently documented, but interested users should see the variable\n    neuropythy.hcp.files.hcp_filemap_instructions.\n    \n    The following options can be given:\n     * path_parameters (default: None) may be set to a map of parameters that are used to format the\n       filenames in the instructions.\n     * data_hierarchy (default: None) may specify how the data should be nested; see the variable\n       neuropythy.hcp.files.hcp_filemap_data_hierarchy.\n     * load_function (default: None) may specify the function that is used to load filenames; if\n       None then neuropythy.io.load is used.\n     * meta_data (default: None) may be passed on to the FileMap object.\n\n    Any additional keyword arguments given to the file_map function will be used as supplemental\n    paths.\n    '''\n    if path: return FileMap(path, instructions, **kw)\n    else:    return lambda path:file_map(path, instructions, **kw)", "response": "A function that returns a FileMap object for the given path and instruction - set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_subject(argv, worklog):\n    '''\n    calc_subject converts a subject_id into a subject object.\n\n    Afferent parameters:\n      @ argv\n        The FreeSurfer subject name(s), HCP subject ID(s), or path(s) of the subject(s) to which the\n        atlas should be applied.\n    '''\n    if len(argv) == 0: raise ValueError('No subject-id given')\n    elif len(argv) > 1: worklog.warn('WARNING: Unused subject arguments: %s' % (argv[1:],))\n    subject_id = argv[0]\n    try:\n        sub = freesurfer_subject(subject_id)\n        if sub is not None:\n            worklog('Using FreeSurfer subject: %s' % sub.path)\n            return sub\n    except Exception: pass\n    try:\n        sub = hcp_subject(subject_id)\n        if sub is not None:\n            worklog('Using HCP subject: %s' % sub.path)\n            return sub\n    except Exception: pass\n    raise ValueError('Could not load subject %s' % subject_id)", "response": "calc_subject converts a subject_id into a subject object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate all available atlases in the given atlas - subject - id and returns a list of all possible atlases in the given atlas - subject - id pairs.", "response": "def calc_atlases(worklog, atlas_subject_id='fsaverage'):\n    '''\n    cacl_atlases finds all available atlases in the possible subject directories of the given atlas\n    subject.\n\n    In order to be a template, it must either be a collection of files (either mgh/mgz or FreeSurfer\n    curv/morph-data files) named as '<hemi>.<template>_<quantity><ending>' such as the files\n    'lh.wang2015_mplbl.mgz' and 'rh.wang2015_mplbl.mgz'. They may additionally have a version\n    prior to the ending, as in 'lh.benson14_angle.v2_5.mgz'. Files without versions are considered \n    to be of a higher version than all versioned files. All files must be found in the atlas\n    subject's surf/ directory; however, all subjects in all FreeSurfer subjects paths with the same\n    subject id are searched if the atlas is not found in the atlas subejct's directory.\n\n    Afferent parameters:\n      @ atlas_subject_id \n        The FreeSurfer subject name subject path of the subject that is to be used as the atlas\n        subject from which the atlas is interpolated. HCP subjects are not currently supported.\n\n    Efferent values:\n      @ atlas_map \n        A persistent map whose keys are atlas names, the values of which are themselves persistent\n        maps whose keys are the versions of the given atlas (None potentially being included). The\n        values of these maps are again maps of hemisphere names then finally of the of the quantity\n        names (such as 'eccen' or 'maxprob') to the property vectors imported from the appropriate\n        files.\n    '''\n    try:              sub = freesurfer_subject(atlas_subject_id)\n    except Exception: sub = None\n    if sub is None:\n        try: sub = hcp_subject(atlas_subject_id)\n        except Exception: sub = None\n    if sub is None: raise ValueError('Could not load atlas subject %s' % atlas_subject_id)\n    worklog('Using Atlas subject: %s' % sub.path)\n    # Now find the requested atlases\n    atlases = AutoDict()\n    atlas_patt = r'^([lr]h)\\.([^_]+)_([^.]+)(\\.(v(\\d+(_\\d+)*)))?((\\.mg[hz])|\\.nii(\\.gz)?)?$'\n    atlas_hemi_ii = 1\n    atlas_atls_ii = 2\n    atlas_meas_ii = 3\n    atlas_vrsn_ii = 6\n    libdir = os.path.join(library_path(), 'data')\n    for pth in [libdir] + config['freesurfer_subject_paths'] + [sub.path]:\n        # see if appropriate files are in this directory\n        pth = os.path.join(pth, sub.name, 'surf')\n        if not os.path.isdir(pth): continue\n        for fl in os.listdir(pth):\n            m = re.match(atlas_patt, fl)\n            if m is None: continue\n            fl = os.path.join(pth, fl)\n            (h, atls, meas, vrsn) = [\n                m.group(ii) for ii in (atlas_hemi_ii, atlas_atls_ii, atlas_meas_ii, atlas_vrsn_ii)]\n            if vrsn is not None: vrsn = tuple([int(s) for s in vrsn.split('_')])\n            atlases[atls][vrsn][h][meas] = curry(nyio.load, fl)\n    # convert the possible atlas maps into persistent/lazy maps\n    atlas_map = pyr.pmap({a:pyr.pmap({v:pyr.pmap({h:pimms.lazy_map(hv)\n                                                  for (h,hv) in six.iteritems(vv)})\n                                      for (v,vv) in six.iteritems(av)})\n                          for (a,av) in six.iteritems(atlases)})\n    return {'atlas_map':atlas_map, 'atlas_subject':sub}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the cortices for the given subject and atlas.", "response": "def calc_cortices(subject, atlas_subject, worklog, hemis=None):\n    '''\n    calc_cortices extracts the hemisphere objects (of the subject) to which the atlas is being\n    applied. By default these are 'lh' and 'rh', but for HCP subjects other hemispheres may be\n    desired.\n\n    Afferent parameters:\n      @ hemis \n        The hemispheres onto which to put the atlas; this may take a number of forms: 'lh' or 'rh' \n        (or --hemis=lh / --hemis=rh) applies the atlas to the given hemisphere only; otherwise a\n        list of hemispheres may be specified as a python object (e.g.\n        --hemis='(\"lh_LR32k\",\"rh_LR32k\")') or as a comma or whitespace separated string (such as\n        'lh_LR32k,lh_LR59k' or 'rh rh_LR164k'). 'lr', 'both', and 'all' are equivalent to 'lh rh';\n        this is the default behavior if hemis is not provided explicitly.\n      @ subject \n        The neuropythy subject object onto which the atlas is being projected.\n      @ atlas_subject \n        Theneuropythy subject object from which the atlas is being projected.\n    '''\n    if hemis is None or hemis is Ellipsis: hemis = 'lr'\n    if pimms.is_str(hemis):\n        if hemis.lower() in ['lr', 'both', 'all']: hemis = ('lh','rh')\n        else: hemis = re.split(r'([,;:]|\\s)+', hemis)[::2]\n    if atlas_subject.name == 'fsaverage_sym':\n        hemis = ['rhx' if h == 'rh' else h for h in hemis]\n    sctcs = {}\n    actcs = {}\n    worklog('Preparing Hemispheres...')\n    for h in hemis:\n        if h not in subject.hemis:\n            raise ValueError('Subject %s does not have requested hemi %s' % (subject.name, h))\n        sctcs[h] = curry(lambda sub,h: sub.hemis[h], subject,       h)\n        h = 'lh' if h == 'rhx' else h\n        if h not in atlas_subject.hemis:\n            raise ValueError('Atlas subject %s does not have requested hemi %s' % (\n                atlas_subject.name, h))\n        actcs[h] = curry(lambda sub,h: sub.hemis[h], atlas_subject, h)\n    return {'subject_cortices': pimms.lazy_map(sctcs),\n            'atlas_cortices':   pimms.lazy_map(actcs)}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the lazy map of atlas projections for a set of subject - level atlas - cortices and atlas - properties.", "response": "def calc_atlas_projections(subject_cortices, atlas_cortices, atlas_map, worklog, atlases=Ellipsis):\n    '''\n    calc_atlas_projections calculates the lazy map of atlas projections.\n\n    Afferent parameters:\n      @ atlases \n        The atlases that should be applied to the subject. This can be specified as a list/tuple of\n        atlas names or as a string where the atlas names are separated by whitespace, commas, or\n        semicolons. For example, to specify the 'benson14' atlas as well as the 'wang15' atlas, then\n        ('benson14', 'wang15'), 'benson14 wang15' or 'benson14,wang15' would all be acceptable. To\n        specify an atlas version, separate the atlas-name and the version with a colon (:), such as\n        'benson14:2.5'. If no version is provided, then the highest version found is used. If\n        atlases is set to None or Ellipsis (the default), this is equivalent to 'benson14,wang15'.\n\n    Efferent values:\n      @ atlas_properties \n        The atlas properties is a nested pimms lazy map whose key-path are like those of the\n        atlas_map afferent parameter but which contains only those atlases requested via the atlases\n        afferent parameter and whose deepest values are interpolated property vectors for the \n        target subject.\n      @ atlas_version_tags \n        Each atlas can be specified as <atlas> or <atlas>:<version>; if the version is specified,\n        then the version tag string (e.g., '.v1_5') is included in this dictionary; if only <atlas>\n        was specified then this string is ''. If <atlas>: is specified, then the version string for\n        whichever atlas was used is included.\n    '''\n    # Parse the atlases argument first:\n    if atlases is Ellipsis:   atlases = ('benson14', 'wang15')\n    if pimms.is_str(atlases): atlases = tuple(re.split(r'([,;]|\\s)+', atlases)[::2])\n    def _atlas_to_atlver(atl):\n        atl0 = atl\n        if not pimms.is_vector(atl):\n            if ':' in atl:\n                atl = atl.split(':')\n                if len(atl) != 2: raise ValueError('Cannot parse atlas spec: %s' % atl0)\n            else: atl = [atl, None]\n        if len(atl) != 2: raise ValueError('Improperly specified atlas: %s' % atl0)\n        if pimms.is_str(atl[1]):\n            if len(atl[1]) == 0: atl = (atl[0], None)\n            else:\n                if atl[1][0] == 'v': atl[1] = atl[1][1:]\n                try: atl = (atl[0], tuple([int(x) for x in re.split(r'[-_.]+', atl[1])]))\n                except Exception:\n                    raise ValueError('Could not parse atlas version string: %s' % atl[1])\n        elif pimms.is_int(atl[1]):  atl = (atl[0], (atl[1],))\n        elif pimms.is_real(atl[1]): atl = (atl[0], (int(atl[1]), int(10*(atl[1] - int(atl[1]))),))\n        elif pimms.is_vector(atl[1], int): atl = (atl[0], tuple(atl[1]))\n        elif atl[1] is not None:\n            raise ValueError('atlas version must be a string (like \"v1_5_1\") or a list of ints')\n        else: atl = tuple(atl)\n        return atl + (atl0,)\n    # Okay, let's find these versions of the atlases in the atlas_map...\n    worklog('Preparing Atlases...')\n    wl = worklog.indent()\n    atl_props = AutoDict()\n    avt = AutoDict()\n    # keyfn is for sorting versions (newest version last)\n    keyfn = lambda k:((np.inf,)     if k is None                 else\n                      k + (np.inf,) if len(k) == 0 or k[-1] != 0 else \n                      k)\n    for (atl,version,atl0) in [_atlas_to_atlver(atl) for atl in atlases]:\n        if atl not in atlas_map: raise ValueError('Could not find an atlas named %s' % atl)\n        atldat = atlas_map[atl]\n        # if the version is None, we pick the highest of the available versions\n        if version is None: v = sorted(atldat.keys(), key=keyfn)[-1]\n        elif version in atldat: v = version\n        else: raise ValueError('Could not find specific version %s of atlas %s' % (version,atl))\n        # update the atlas-version-tag data\n        wl('Atlas: %s, Version: %s' % (atl, v))\n        avt[atl][v] = '' if v is None or ':' not in atl0 else ('.v' + '_'.join(map(str, v)))\n        lmaps = atlas_map[atl][v]\n        # convert these maps into interpolated properties...\n        for (h,hmap) in six.iteritems(lmaps):\n            hmap = pimms.lazy_map(\n                {m:curry(\n                    lambda hmap,h,m: atlas_cortices[h].interpolate(\n                        subject_cortices[h],\n                        hmap[m]),\n                    hmap, h, m)\n                 for m in six.iterkeys(hmap)})\n            lmaps = lmaps.set(h, hmap)\n        # add the lmaps (the persistent/lazy maps for this atlas version) in the atlprops\n        atl_props[atl][v] = lmaps\n    # That's all; we can return atl_props once we persist it\n    return {'atlas_properties':  pimms.persist(atl_props),\n            'atlas_version_tags': pimms.persist(avt)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(*argv):\n    '''\n    neuropythy.commands.atlas.main() runs the main function of the atlas command in the neuropythy\n      library. See the `python -m neuropythy atlas --help` or atlas.info for more information.\n    '''\n    argv = [aa for arg in argv for aa in (arg if pimms.is_vector(arg) else [arg])]\n    imap = pimms.argv_parse(\n        atlas_plan, argv,\n        arg_abbrevs=atlas_cmdline_abbrevs)\n    argv = imap['argv']\n    if len(argv) == 0 or '--help' in argv or '-h' in argv:\n        print(info)\n        return 1\n    try: imap['export_all_fn']()\n    except Exception as e:\n        sys.stderr.write('\\nERROR:\\n' + str(e) + '\\n')\n        sys.stderr.flush()\n        sys.exit(2)\n    return 0", "response": "main function of the atlas command in the neuropythy library."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting an argument c to a Cortex object.", "response": "def to_cortex(c):\n    '''\n    to_cortex(c) yields a Cortex object if the argument c can be coerced to one and otherwise raises\n      an error.\n\n    An object can be coerced to a Cortex object if:\n      * it is a cortex object\n      * it is a tuple (subject, h) where subject is a subject object and h is a subject hemisphere.\n    '''\n    if is_cortex(c): return c\n    elif pimms.is_vector(c) and len(c) == 2:\n        (s,h) = c\n        if is_subject(s) and pimms.is_str(h):\n            if h in s.hemis: return s.hemis[h]\n            else: raise ValueError('to_cortex: hemi %s not found in given subject' % h)\n    raise ValueError('Could not coerce argument to Cortex object')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_image_header(img):\n    '''\n    to_image_header(img) yields img.header if img is a nibabel image object.\n    to_image_header(hdr) yields hdr if hdr is a nibabel header object.\n    to_image_header(obj) raises an error for other input types.\n    '''\n    if not img.__module__.startswith('nibabel.'):\n        raise ValueError('to_image_header: only nibabel obejcts can be coerced to headers')\n    if type(img).__name__.endswith('Header'): return img\n    # if not a header given, must be an image given:\n    try: return img.header\n    except Exception:\n        raise ValueError('to_image_header: can only convert nibabel image or header objects')", "response": "Convert an image object to a header object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_image_type(image_type):\n    '''\n    to_image_type(image_type) yields an image-type class equivalent to the given image_type\n      argument, which may be a type name or alias or an image or header object or class.    \n    '''\n    if image_type is None: return None\n    if isinstance(image_type, type) and issubclass(image_type, ImageType): return image_type\n    if pimms.is_str(image_type):\n        image_type = image_type.lower()\n        if image_type in image_types_by_name: return image_types_by_name[image_type]\n        for it in image_types:\n            if image_type in it.aliases(): return it\n        raise ValueError('\"%s\" is not a valid image-type name or alias' % image_type)\n    for x in (image_type, type(image_type)):\n        try:    return image_types_by_image_type[x]\n        except Exception: pass\n        try:    return image_types_by_header_type[x]\n        except Exception: pass\n    raise ValueError('Unsupported image type: %s' % image_type)", "response": "Converts an image - type name or alias to an image - type class equivalent to the given image_type argument."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert an image object to a dictionary of meta - data.", "response": "def to_image_meta_data(img):\n    '''\n    to_image_meta_data(img) yields a dictionary of meta-data for the given nibabel image object img.\n    to_image_meta_data(hdr) yields the equivalent meta-data for the given nibabel image header.\n\n    Note that obj may also be a mapping object, in which case it is returned verbatim.\n    '''\n    if pimms.is_map(img): return img\n    try: hdr = img.header\n    except Exception: hdr = img\n    intype = to_image_type(hdr)\n    return intype.meta_data(hdr)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert an image to a Nifti1Image object.", "response": "def to_image(img, image_type=None, meta_data=None, **kwargs):\n    '''\n    to_image(array) yields a Nifti1Image of the given array with default meta-data.\n    to_image(array, image_type) yields an image object of the given type; image_type may either be\n      an image class or a class name (see supported types below).\n    to_image((array, meta_data)) uses the given mapping of meta-data to fill in the image's\n      meta-data; note that meta_data may simply be an affine transformation matrix.\n    to_image((array, affine, meta_data)) uses the given affine specifically (the given affine\n      overrides any affine included in the meta_data).\n\n    Note that the array may optionally be an image itself, in which case its meta-data is used as a\n    starting point for the new meta-data. Any meta-data passed as a tuple overwrites this meta-data,\n    and any meta-data passed as an optional argument overwrites this meta-data in turn.\n\n    The first optional argument, specifying image_type is as an image type if possible, but if a\n    meta-data mapping is passed as the first argument it is used as such; otherwise, the optional\n    third argument is named meta_data, and any additional keyword arguments passed to to_image are\n    merged into this meta_data object left-to-right (i.e., keyword arguments overwrite the meta_data\n    keys).\n    '''\n    # quick cleanup of args:\n    meta_data = pimms.merge({} if meta_data is None else meta_data, kwargs)\n    if image_type is None: image_type = 'nifti1'\n    # deduce image type\n    image_type = to_image_type(image_type)\n    # okay, next, parse the image argument itself:\n    if is_tuple(img):\n        if   len(img) == 1: (img,aff,mdat) = (img[0], None, None)\n        elif len(img) == 2: (img,aff,mdat) = (img[0], None, img[1])\n        elif len(img) == 3: (img,aff,mdat) = img\n        else: raise ValueError('cannot parse more than 3 elements from image tuple')\n    else: (aff,mdat) = (None,None)\n    # see if the img argument is an image object\n    try: (img,aff0,mdat0) = (img.dataobj, img.affine, to_image_meta_data(img))\n    except Exception: (aff0,mdat0) = (None, {})\n    # check that the affine wasn't given as the meta-data (e.g. (img,aff) instead of (img,mdat))\n    if aff is None and mdat is not None:\n        try:    (aff, mdat) = (to_affine(mdat, 3), {})\n        except Exception: pass\n    # parse the meta-data that has been given\n    mdat = dict(pimms.merge(mdat0, {} if mdat is None else mdat, meta_data))\n    # if there is an explicit affine, we put it into mdat now\n    if aff is not None: mdat['affine'] = to_affine(aff, 3)\n    if aff0 is not None and 'affine' not in mdat: mdat['affine'] = to_affine(aff0, 3)\n    # okay, we create the image now:\n    return image_type.create(img, meta_data=mdat)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_type(self, hdat, dataobj=None):\n        '''\n        Parses the dtype out of the header data or the array, depending on which is given; if both,\n          then the header-data overrides the array; if neither, then np.float32.\n        '''\n        try:    dataobj = dataobj.dataobj\n        except Exception: pass\n        dtype = np.asarray(dataobj).dtype if dataobj else self.default_type()\n        if   hdat and 'type'  in hdat: dtype = np.dtype(hdat['type'])\n        elif hdat and 'dtype' in hdat: dtype = np.dtype(hdat['dtype'])\n        return dtype", "response": "Parses the dtype out of the header data or the array depending on which is given ; if neither is given the dtype is np. float32."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the affine out of the given header data and yields it.", "response": "def parse_affine(self, hdat, dataobj=None):\n        '''\n        Parses the affine out of the given header data and yields it.\n        '''\n        if 'affine' in hdat: return to_affine(hdat['affine'])\n        else:                return to_affine(self.default_affine())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self, arr, meta_data={}, **kwargs):\n        '''\n        itype.create(dataobj) yields an image of the given image type itype that represents the\n          given data object dataobj.\n        itype.create(dataobj, meta_data) uses the given meta/header data to create the image.\n\n        Any number of keyword arguments may also be appended to the call; these are merged into the\n        meta_data argument.\n        '''\n        return self.to_image(arr, hdat=pimms.merge(meta_data, kwargs))", "response": "Returns an image of the given image type"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_dataset(dset, fn=None):\n    '''\n    add_dataset(dset) adds the given dataset to the neuropythy.data map.\n    add_dataset(name, fn) adds a dataset with the given name; fn must be a function of zero\n      arguments that yields the dataset.\n\n    add_dataset always yeilds None or raises an error.\n    '''\n    global data\n    if fn is None:\n        if not isinstance(dset, Dataset):\n            raise ValueError('Cannot add non-Dataset object to neuropythy datasets')\n        nm = dset.name\n        data = data.set(nm, dset)\n    else:\n        nm = Dataset.to_name(dset)\n        def _load_dset():\n            x = fn()\n            if not isinstance(x, Dataset):\n                raise ValueError('Loader for dataset %s failed to return a dataset' % nm)\n            return x\n        data = data.set(nm, _load_dset)\n    return None", "response": "Adds a dataset to the neuropythy. data map."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a flat model from a file.", "response": "def load_fmm_model(filename, radius=np.pi/3.0, sphere_radius=100.0):\n    '''\n    load_fmm_model(filename) yields the fmm model indicated by the given file name. Fmm models are\n    triangle meshes that define a field value at every vertex as well as the parameters of a\n    projection to the 2D cortical surface via a registartion name and projection parameters.\n    The following options may be given:\n      * radius (default: pi/3) specifies the radius that should be assumed by the model; fmm models\n        do not specify radius values in their projection parameters, but pi/3 is usually large for\n        localized models.\n      * sphere_radius (default: 100) specifies the radius of the sphere that should be assumed by\n        the model. Note that in Freesurfer, spheres have a radius of 100.\n    '''\n    if not os.path.exists(filename):\n        models_path = os.path.join(library_path(), 'models')\n        # we look for it in a number of ways:\n        fname = next((fnm\n                      for beg in ['', models_path] for end in ['', '.fmm', '.fmm.gz']\n                      for fnm0 in [filename + end]\n                      for fnm in [fnm0 if beg is None else os.path.join(beg, fnm0)]\n                      if os.path.exists(fnm)),\n                     None)\n        if fname is None:\n            raise ValueError('Given model/file name does not exist: %s' % filename)\n        filename = fname\n    if not os.path.isfile(filename):\n        raise ValueError('Given filename (%s) is not a file!' % filename)\n    gz = True if len(filename) > 3 and filename[-3:] == '.gz' else False\n    lines = None\n    with (gzip.open(filename, 'rt') if gz else open(filename, 'rt')) as f:\n        lines = f.read().split('\\n')\n    if len(lines) < 3 or lines[0] != 'Flat Mesh Model Version: 1.0':\n        raise ValueError('Given file does not contain to a valid flat mesh model!')\n    n = int(lines[1].split(':')[1].strip())\n    m = int(lines[2].split(':')[1].strip())\n    reg = lines[3].split(':')[1].strip()\n    hemi = lines[4].split(':')[1].strip().upper()\n    center = list(map(float, lines[5].split(':')[1].strip().split(',')))\n    onxaxis = list(map(float, lines[6].split(':')[1].strip().split(',')))\n    method = lines[7].split(':')[1].strip().lower()\n    tx = np.asarray(\n        [list(map(float, row.split(',')))\n         for row in lines[8].split(':')[1].strip(' \\t[]').split(';')])\n    if lines[9].startswith('AreaNames: ['):\n        # we load the area names\n        s = lines[9][12:-1]\n        area_names = tuple(s.split(' '))\n        l0 = 10        \n    else:\n        area_names = None\n        l0 = 9\n    crds = []\n    for row in lines[l0:(n+l0)]:\n        (left,right) = row.split(' :: ')\n        crds.append(list(map(float, left.split(','))))\n    crds = np.asarray([list(map(float, left.split(',')))\n                       for row in lines[l0:(n+l0)]\n                       for (left,right) in [row.split(' :: ')]])\n    vals = np.asarray([list(map(float, right.split(',')))\n                       for row in lines[l0:(n+l0)]\n                       for (left,right) in [row.split(' :: ')]])\n    tris = -1 + np.asarray(\n        [list(map(int, row.split(',')))\n         for row in lines[(n+l0):(n+m+l0)]])\n    return RegisteredRetinotopyModel(\n        RetinotopyMeshModel(tris, crds,\n                            90-180/np.pi*vals[:,0], vals[:,1], np.asarray(vals[:,2], dtype=np.int),\n                            transform=tx,\n                            area_name_to_id=area_names),\n        geo.MapProjection(registration=reg,\n                          center=center,\n                          center_right=onxaxis,\n                          method=method,\n                          radius=radius,\n                          sphere_radius=sphere_radius,\n                          chirality=hemi))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the field arguments.", "response": "def _parse_field_arguments(arg, faces, edges, coords):\n    '''See mesh_register.'''\n    if not hasattr(arg, '__iter__'):\n        raise RuntimeError('field argument must be a list-like collection of instructions')\n    pot = [_parse_field_argument(instruct, faces, edges, coords) for instruct in arg]\n    # make a new Potential sum unless the length is 1\n    if len(pot) <= 1:\n        return pot[0]\n    else:\n        sp = java_link().jvm.nben.mesh.registration.Fields.newSum()\n        for field in pot: sp.addField(field)\n        return sp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef java_potential_term(mesh, instructions):\n    '''\n    java_potential_term(mesh, instructions) yields a Java object that implements the potential field\n      described in the given list of instructions. Generally, this should not be invoked directly\n      and should only be called by mesh_register. Note: this expects a single term's description,\n      not a series of descriptions.\n    '''\n    faces  = to_java_ints(mesh.indexed_faces)\n    edges  = to_java_ints(mesh.indexed_edges)\n    coords = to_java_doubles(mesh.coordinates)\n    return _parse_field_arguments([instructions], faces, edges, coords)", "response": "Returns a Java object that implements the potential field\n      described in the given list of instructions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mesh_register(mesh, field, max_steps=2000, max_step_size=0.05, max_pe_change=1,\n                  method='random', return_report=False, initial_coordinates=None):\n    '''\n    mesh_register(mesh, field) yields the mesh that results from registering the given mesh by\n    minimizing the given potential field description over the position of the vertices in the\n    mesh. The mesh argument must be a Mesh object (see neuropythy.geometry) such as can be read\n    from FreeSurfer using the neuropythy.freesurfer_subject function. The field argument must be\n    a list of field names and arguments; with the exception of 'mesh' (or 'standard'), the \n    arguments must be a list, the first element of which is the field type name, the second\n    element of which is the field shape name, and the final element of which is a dictionary of\n    arguments accepted by the field shape.\n\n    The following are valid field type names:\n      * 'mesh' : the standard mesh potential, which includes an edge potential, an angle\n        potential, and a perimeter potential. Accepts no arguments, and must be passed as a\n        single string instead of a list.\n      * 'edge': an edge potential field in which the potential is a function of the change in the\n        edge length, summed over each edge in the mesh.\n      * 'angle': an angle potential field in which the potential is a function of the change in\n        the angle measure, summed over all angles in the mesh.\n      * 'perimeter': a potential that depends on the vertices on the perimeter of a 2D mesh\n        remaining in place; the potential changes as a function of the distance of each perimeter\n        vertex from its reference position.\n      * 'anchor': a potential that depends on the distance of a set of vertices from fixed points\n        in space. After the shape name second argument, an anchor must be followed by a list of\n        vertex ids then a list of fixed points to which the vertex ids are anchored:\n        ['anchor', shape_name, vertex_ids, fixed_points, args...].\n\n    The following are valid shape names:\n      * 'harmonic': a harmonic function with the form (c/q) * abs(x - x0)^q.\n        Parameters: \n          * 'scale', the scale parameter c; default: 1.\n          * 'order', the order parameter q; default: 2.\n      * 'Lennard-Jones': a Lennard-Jones function with the form c (1 + (r0/r)^q - 2(r0/r)^(q/2));\n        Parameters:\n          * 'scale': the scale parameter c; default: 1. \n          * 'order': the order parameter q; default: 2.\n      * 'Gaussian': A Gaussian function with the form c (1 - exp(-0.5 abs((x - x0)/s)^q))\n        Parameters:\n          * 'scale': the scale parameter c; default: 1.\n          * 'order': the order parameter q; default: 2.\n          * 'sigma': the standard deviation parameter s; default: 1.\n      * 'infinite-well': an infinite well function with the form \n        c ( (((x0 - m)/(x - m))^q - 1)^2 + (((M - x0)/(M - x))^q - 1)^2 )\n        Parameters:\n          * 'scale': the scale parameter c; default: 1.\n          * 'order': the order parameter q; default: 0.5.\n          * 'min': the minimum value m; default: 0.\n          * 'max': the maximum value M; default: pi.\n\n    Options: The following optional arguments are accepted.\n      * max_steps (default: 2000) the maximum number of steps to minimize for.\n      * max_step_size (default: 0.1) the maximum distance to allow a vertex to move in a single\n        minimization step.\n      * max_pe_change: the maximum fraction of the initial potential value that the minimizer\n        should minimize away before returning; i.e., 0 indicates that no minimization should be\n        allowed while 0.9 would indicate that the minimizer should minimize until the potential\n        is 10% or less of the initial potential.\n      * return_report (default: False) indicates that instead of returning the registered data,\n        mesh_register should instead return the Java Minimizer.Report object (for debugging).\n      * method (default: 'random') specifies the search algorithm used; available options are \n        'random', 'nimble', and 'pure'. Generally all options will converge on a similar solution,\n        but usually 'random' is fastest. The 'pure' option uses the nben library's step function,\n        which performs straight-forward gradient descent. The 'nimble' option performs a gradient\n        descent in which subsets of vertices in the mesh that have the highest gradients during the\n        registration are updated more often than those vertices with small gradients; this can\n        sometimes but not always increase the speed of the minimization. Note that instead of\n        'nimble', one may alternately provide ('nimble', k) where k is the number of partitions that\n        the vertices should be sorted into (by partition). 'nimble' by itself is equivalent to \n        ('nimble', 4). Note also that a single step of nimble minimization is equivalent to 2**k\n        steps of 'pure' minimization. Finally, the 'random' option uses the nben library's\n        randomStep function, which is a gradient descent algorithm that moves each vertex in the\n        direction of its negative gradient during each step but which randomizes the length of the\n        gradient at each individual vertex by drawing from an exponential distribution centered at\n        the vertex's actual gradient length. In effect, this can prevent vertices with very large\n        gradients from dominating the minimization and often results in the best results.\n      * initial_coordinates (default: None) specifies the start coordinates of the registration;\n        if None, uses those in the given mesh, which is generally desired.\n\n    Examples:\n      registered_mesh = mesh_register(\n         mesh,\n         [['edge', 'harmonic', 'scale', 0.5], # slightly weak edge potential\n          ['angle', 'infinite-well'], # default arguments for an infinite-well angle potential\n          ['anchor', 'Gaussian', [1, 10, 50], [[0.0, 0.0], [1.1, 1.1], [2.2, 2.2]]]],\n         max_step_size=0.05,\n         max_steps=10000)\n    '''\n    # Sanity checking:\n    # First, make sure that the arguments are all okay:\n    if not isinstance(mesh, geo.Mesh):\n        raise RuntimeError('mesh argument must be an instance of neuropythy.geometry.Mesh')\n    if not pimms.is_vector(max_steps): max_steps = [max_steps]\n    for ms in max_steps:\n        if not pimms.is_int(ms) or ms < 0:\n            raise RuntimeError('max_steps argument must be a positive integer')\n    if not pimms.is_vector(max_step_size): max_step_size = [max_step_size]\n    for mss in max_step_size:\n        if not pimms.is_number(mss) or mss <= 0:\n            raise RuntimeError('max_step_size must be a positive number')\n    if not pimms.is_number(max_pe_change) or max_pe_change <= 0 or max_pe_change > 1:\n        raise RuntimeError('max_pe_change must be a number x such that 0 < x <= 1')\n    if pimms.is_vector(method):\n        if method[0].lower() == 'nimble' and len(method) > 1 and not pimms.is_str(method[1]):\n            method = [method]\n    else: method = [method]\n    if initial_coordinates is None:\n        init_coords = mesh.coordinates\n    else:\n        init_coords = np.asarray(initial_coordinates)\n        if init_coords.shape[0] != mesh.coordinates.shape[0]:\n            init_coords = init_coords.T\n    # If steps is 0, we can skip most of this...\n    if np.sum(max_steps) == 0:\n        if return_report: return None\n        else: return init_coords\n    # Otherwise, we run at least some minimization\n    max_pe_change = float(max_pe_change)\n    nrounds = len(max_steps)\n    if nrounds > 1:\n        if len(max_step_size) == 1: max_step_size = [max_step_size[0] for _ in max_steps]\n        if len(method) == 1:        method        = [method[0]        for _ in max_steps]\n    # Parse the field argument.\n    faces  = to_java_ints(mesh.tess.indexed_faces)\n    edges  = to_java_ints(mesh.tess.indexed_edges)\n    coords = to_java_doubles(mesh.coordinates)\n    init_coords = coords if init_coords is mesh.coordinates else to_java_doubles(init_coords)\n    potential = _parse_field_arguments(field, faces, edges, coords)\n    # Okay, that's basically all we need to do the minimization...\n    rep = []\n    for (method,max_step_size,max_steps) in zip(method, max_step_size, max_steps):\n        minimizer = java_link().jvm.nben.mesh.registration.Minimizer(potential, init_coords)\n        max_step_size = float(max_step_size)\n        max_steps = int(max_steps)\n        if pimms.is_str(method):\n            method = method.lower()\n            if method == 'nimble': k = 4\n            else:                  k = 0\n        else:\n            k = method[1]\n            method = method[0].lower()\n        if method == 'pure':\n            r = minimizer.step(max_pe_change, max_steps, max_step_size)\n        elif method == 'random':\n            # if k is -1, we do the inverse version where we draw from the 1/mean distribution\n            r = minimizer.randomStep(max_pe_change, max_steps, max_step_size, k == -1)\n        elif method == 'nimble':\n            r = minimizer.nimbleStep(max_pe_change, max_steps, max_step_size, int(k))\n        else:\n            raise ValueError('Unrecognized method: %s' % method)\n        rep.append(r)\n        init_coords = minimizer.getX()\n    # Return the report if requested\n    if return_report:\n        return rep\n    else:\n        result = init_coords\n        return np.asarray([[x for x in row] for row in result])", "response": "This function registers a given mesh with the given field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mag_data(hemi, retinotopy='any', surface='midgray', mask=None,\n             weights=Ellipsis, weight_min=0, weight_transform=Ellipsis,\n             visual_area=None, visual_area_mask=Ellipsis,\n             eccentricity_range=None, polar_angle_range=None):\n    '''\n    mag_data(hemi) yields a map of visual/cortical magnification data for the given hemisphere.\n    mag_data(mesh) uses the given mesh.\n    mag_data([arg1, arg2...]) maps over the given hemisphere or mesh arguments.\n    mag_data(subject) is equivalent to mag_data([subject.lh, subject.rh]).\n    mag_data(mdata) for a valid magnification data map mdata (i.e., is_mag_data(mdata) is True or\n      mdata is a lazy map with integer keys) always yields mdata without considering any additional\n      arguments.\n\n    The data structure returned by magdata is a lazy map containing the keys:\n      * 'surface_coordinates': a (2 x N) or (3 x N) matrix of the mesh coordinates in the mask\n        (usually in mm).\n      * 'visual_coordinates': a (2 x N) matrix of the (x,y) visual field coordinates (in degrees).\n      * 'surface_areas': a length N vector of the surface areas of the faces in the mesh.\n      * 'visual_areas': a length N vector of the areas of the faces in the visual field.\n      * 'mesh': the full mesh from which the surface coordinates are obtained.\n      * 'submesh': the submesh of mesh of just the vertices in the mask (may be identical to mesh).\n      * 'mask': the mask used.\n      * 'retinotopy_data': the full set of retinotopy_data from the hemi/mesh; note that this will\n        include the key 'weights' of the weights actually used and 'visual_area' of the found or\n        specified visual area.\n      * 'masked_data': the subsampled retinotopy data from the hemi/mesh.\n    Note that if a visual_area property is found or provided (see options below), instead of\n    yielding a map of the above, a lazy map whose keys are the visual areas and whose values are the\n    maps described above is yielded instead.\n\n    The following named options are accepted (in order):\n      * retinotopy ('any') specifies the value passed to the retinotopy_data function to obtain the\n        retinotopic mapping data; this may be a map of such data.\n      * surface ('midgray') specifies the surface to use.\n      * mask (None) specifies the mask to use.\n      * weights, weight_min, weight_transform (Ellipsis, 0, Ellipsis) are used as in the\n        to_property() function  in neuropythy.geometry except weights, which, if equal to Ellipsis,\n        attempts to use the weights found by retinotopy_data() if any.\n      * visual_area (Ellipsis) specifies the property to use for the visual area label; Ellipsis is\n        equivalent to whatever visual area label is found by the retinotopy_data() function if any.\n      * visual_area_mask (Ellipsis) specifies which visual areas to include in the returned maps,\n        assuming a visual_area property is found; Ellipsis is equivalent to everything but 0; None\n        is equivalent to everything.\n      * eccentricity_range (None) specifies the eccentricity range to include.\n      * polar_angle_range (None) specifies the polar_angle_range to include.\n    '''\n    if is_mag_data(hemi): return hemi\n    elif pimms.is_lazy_map(hemi) and pimms.is_vector(hemi.keys(), 'int'): return hemi\n    if mri.is_subject(hemi): hemi = (hemi.lh. hemi.rh)\n    if pimms.is_vector(hemi):\n        return tuple([mag_data(h, retinotopy=retinotopy, surface=surface, mask=mask,\n                               weights=weights, weight_min=weight_min,\n                               weight_transform=weight_transform, visual_area=visual_area,\n                               visual_area_mask=visual_area_mask,\n                               eccentricity_range=eccentricity_range,\n                               polar_angle_range=polar_angle_range)\n                      for h in hemi])\n    # get the mesh\n    mesh = geo.to_mesh((hemi, surface))\n    # First, find the retino data\n    retino = retinotopy_data(hemi, retinotopy)\n    # we can process the rest the mask now, including weights and ranges\n    if weights is Ellipsis: weights = retino.get('variance_explained', None)\n    mask = hemi.mask(mask, indices=True)\n    (arng,erng) = (polar_angle_range, eccentricity_range)\n    (ang,ecc) = (retino['polar_angle'], retino['eccentricity'])\n    if pimms.is_str(arng):\n        tmp = to_hemi_str(arng)\n        arng = (-180,0) if tmp == 'rh' else (0,180) if tmp == 'lh' else (-180,180)\n    elif arng is None:\n        tmp = ang[mask]\n        tmp = tmp[np.isfinite(tmp)]\n        arng = (np.min(tmp), np.max(tmp))\n    if erng is None:\n        tmp = ecc[mask]\n        tmp = tmp[np.isfinite(tmp)]\n        erng = (0, np.max(tmp))\n    elif pimms.is_scalar(erng): erng = (0, erng)\n    (ang,wgt) = hemi.property(retino['polar_angle'], weights=weights, weight_min=weight_min,\n                              weight_transform=weight_transform, yield_weight=True)\n    ecc = hemi.property(retino['eccentricity'], weights=weights, weight_min=weight_min,\n                        weight_transform=weight_transform, data_range=erng)\n    # apply angle range if given\n    ((mn,mx),mid) = (arng, np.mean(arng))\n    oks = mask[np.isfinite(ang[mask])]\n    u = ang[oks]\n    u = np.mod(u + 180 - mid, 360) - 180 + mid\n    ang[oks[np.where((mn <= u) & (u < mx))[0]]] = np.inf\n    # mark/unify the out-of-range ones\n    bad = np.where(np.isinf(ang) | np.isinf(ecc))[0]\n    ang[bad] = np.inf\n    ecc[bad] = np.inf\n    wgt[bad] = 0\n    wgt *= zinv(np.sum(wgt[mask]))\n    # get visual and surface coords\n    vcoords = np.asarray(as_retinotopy(retino, 'geographical'))\n    scoords = mesh.coordinates\n    # now figure out the visual area so we can call down if we need to\n    if visual_area is Ellipsis: visual_area = retino.get('visual_area', None)\n    if visual_area is not None: retino['visual_area'] = visual_area\n    if wgt is not None: retino['weights'] = wgt\n    rdata = pimms.persist(retino)\n    # calculate the range area\n    (tmn,tmx) = [np.pi/180.0 * u for u in arng]\n    if tmx - tmn >= 2*np.pi: (tmn,tmx) = (-np.pi,np.pi)\n    (emn,emx) = erng\n    rarea = 0.5 * (emx*emx - emn*emn) * (tmx - tmn)\n    # okay, we have the data organized; we can do the calculation based on this, but we may have a\n    # visual area mask to apply as well; here's how we do it regardless of mask\n    def finish_mag_data(mask):\n        if len(mask) == 0: return None\n        # now that we have the mask, we can subsample\n        submesh = mesh.submesh(mask)\n        mdata = pyr.pmap({k:(v[mask]   if pimms.is_vector(v) else\n                             v[:,mask] if pimms.is_matrix(v) else\n                             None)\n                          for (k,v) in six.iteritems(rdata)})\n        fs = submesh.tess.indexed_faces\n        (vx, sx)  = [x[:,mask]                        for x in (vcoords, scoords)]\n        (vfx,sfx) = [np.asarray([x[:,f] for f in fs]) for x in (vx,      sx)]\n        (va, sa)  = [geo.triangle_area(*x)            for x in (vfx, sfx)]\n        return pyr.m(surface_coordinates=sx, visual_coordinates=vx,\n                     surface_areas=sa,       visual_areas=va,\n                     mesh=mesh,              submesh=submesh,\n                     retinotopy_data=rdata,  masked_data=mdata,\n                     mask=mask,              area_of_range=rarea)\n    # if there's no visal area, we just use the mask as is\n    if visual_area is None: return finish_mag_data(mask)\n    # otherwise, we return a lazy map of the visual area mask values\n    visual_area = hemi.property(visual_area, mask=mask, null=0, dtype=np.int)\n    vam = (np.unique(visual_area)                    if visual_area_mask is None     else\n           np.setdiff1d(np.unique(visual_area), [0]) if visual_area_mask is Ellipsis else\n           np.unique(list(visual_area_mask)))\n    return pimms.lazy_map({va: curry(finish_mag_data, mask[visual_area[mask] == va])\n                           for va in vam})", "response": "Return a map of visual and cortical magnification data for a given hemisphere."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_mag_data(mdat):\n    '''\n    is_mag_data(dat) yields True if the given data is a valid set of magnification data and False\n      otherwise.\n\n    Note that this does not return True for all valid return values of the mag_data() function:\n    specifically, if the mag_data() function yields a list of mag-data maps or a lazy-map of the\n    mag-data maps split out by visual area, then this will return False. This function only returns\n    True for a map of mag data itself.\n    '''\n    if not pimms.is_map(mdat): return False\n    for k in ['surface_coordinates', 'visual_coordinates', 'mesh', 'submesh', 'mask',\n              'retinotopy_data', 'masked_data', 'surface_areas', 'visual_areas']:\n        if k not in mdat: return False\n    return True", "response": "Returns True if the given data is a valid set of magnification data and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef disk_vmag(hemi, retinotopy='any', to=None, **kw):\n    '''\n    disk_vmag(mesh) yields the visual magnification based on the projection of disks on the cortical\n      surface into the visual field.\n\n    All options accepted by mag_data() are accepted by disk_vmag().\n    '''\n    mdat = mag_data(hemi, retinotopy=retinotopy, **kw)\n    if pimms.is_vector(mdat): return tuple([face_vmag(m, to=to) for m in mdat])\n    elif pimms.is_vector(mdat.keys(), 'int'):\n        return pimms.lazy_map({k: curry(lambda k: face_vmag(mdat[k], to=to), k)\n                               for k in six.iterkeys(mdat)})\n    #TODO: implement the disk_vmag calculation using mdat\n    # convert to the appropriate type according to the to param\n    raise NotImplementedError()", "response": "disk_vmag returns the visual magnification based on the projection of disks on the cortical\n      surface into the visual field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef field_of_view(mesh, retinotopy='any', mask=None, search_scale=3.0, bins=6):\n    '''\n    field_of_view(obj) yields a field-of-view function for the given vertex-set object or mapping of\n      retinotopy data obj.\n\n    The field-of-view function is a measurement of how much total pRF weight there is at each point\n    in the visual field; essentially it is the sum of all visual-field Gaussian pRFs, where the\n    Gaussian's are normalized both by the pRF size (i.e., each pRF is a 2D normal distribution whose\n    center comes from its polar angle and eccentricity and whose sigma parameter is the pRF radius)\n    and the weight (if any weight such as the variance explained is included in the retinotopy data,\n    then the normal distributions are multiplied by this weight, otherwise weights are considered to\n    be uniform). Note that the weights are normalized by their sum, so the resulting field-of-view\n    function should be a valid probability distribution over the visual field.\n\n    The returned object fov = field_of_view(obj) is a special field-of-view object that can be\n    called as fov(x, y) or fov(coord) where x and y may be lists or values and coord may be a 2D\n    vector or an (n x 2) matrix.\n\n    The following options may be provided (also in argument order):\n      * retinotopy (default: 'any') specifies the retinotopy data to be used; if this is a string,\n        then runs retinotopy_data(obj, retinotopy) and uses the result, otherwise runs\n        retinotopy_data(retinotopy) and uses the result; note that this must have a radius parameter\n        as well as polar angle and eccentricity or equivalent parameters that specify the pRF center\n      * mask (default: None) specifies the mask that should be used; this is interpreted by a call\n        to mesh.mask(mask)\n      * search_scale (default: 3.0) specifies how many Gaussian standard deviations away from a pRF\n        center should be included when calculating the sum of Gaussians at a point\n      * bins (default: 6) specifies the number of bins to divide the pRFs into based on the radius\n        value; this is used to prune the search of pRFs that overlap a point to just those\n        reasonanly close to the point in question\n    '''\n    # First, find the retino data\n    if pimms.is_str(retinotopy):\n        retino = retinotopy_data(mesh, retinotopy)\n    else:\n        retino = retinotopy_data(retinotopy)\n    # Convert from polar angle/eccen to longitude/latitude\n    (ang,ecc) = as_retinotopy(retino, 'visual')\n    if 'radius' not in retino:\n        raise ValueError('Retinotopy data must contain a radius, sigma, or size value')\n    sig = retino['radius']\n    wgt = next((retino[q] for q in ('variance_explained', 'weight') if q in retino), None)\n    # Get the indices we care about\n    ii = geo.to_mask(mesh, mask, indices=True)\n    return FieldOfView(ang[ii], ecc[ii], sig[ii], weight=wgt[ii],\n                       search_scale=search_scale, bins=bins)", "response": "Returns a field - of - view function for the given vertex - set or mapping of vertex - set objects or mapping of vertex - set objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the neighborhood - based cortical magnification for the given mesh.", "response": "def cmag(mesh, retinotopy='any', surface=None, to='vertices'):\n    '''\n    cmag(mesh) yields the neighborhood-based cortical magnification for the given mesh.\n    cmag(mesh, retinotopy) uses the given retinotopy argument; this must be interpretable by\n      the as_retinotopy function, or should be the name of a source (such as 'empirical' or\n      'any').\n\n    The neighborhood-based cortical magnification data is yielded as a map whose keys are 'radial',\n    'tangential', 'areal', and 'field_sign'; the units of 'radial' and 'tangential' magnifications \n    are cortical-distance/degree and the units on the 'areal' magnification is\n    (cortical-distance/degree)^2; the field sign has no unit.\n\n    Note that if the retinotopy source is not given, this function will by default search for any\n    source using the retinotopy_data function.\n\n    The option surface (default None) can be provided to indicate that while the retinotopy and\n    results should be formatted for the given mesh (i.e., the result should have a value for each\n    vertex in mesh), the surface coordinates used to calculate areas on the cortical surface should\n    come from the given surface. The surface option may be a super-mesh of mesh.\n\n    The option to='faces' or to='vertices' (the default) specifies whether the return-values should\n    be for the vertices or the faces of the given mesh. Vertex data are calculated from the face\n    data by summing and averaging.\n    '''\n    # First, find the retino data\n    if pimms.is_str(retinotopy):\n        retino = retinotopy_data(mesh, retinotopy)\n    else:\n        retino = retinotopy\n    # If this is a topology, we want to change to white surface\n    if isinstance(mesh, geo.Topology): mesh = mesh.white_surface\n    # Convert from polar angle/eccen to longitude/latitude\n    vcoords = np.asarray(as_retinotopy(retino, 'geographical'))\n    # note the surface coordinates\n    if surface is None: scoords = mesh.coordinates\n    else:\n        scoords = surface.coordinates\n        if scoords.shape[1] > mesh.vertex_count:\n            scoords = scoords[:, surface.index(mesh.labels)]\n    faces = mesh.tess.indexed_faces\n    sx = mesh.face_coordinates\n    # to understand this calculation, see this stack exchange question:\n    # https://math.stackexchange.com/questions/2431913/gradient-of-angle-between-scalar-fields\n    # each face has a directional magnification; we need to start with the face side lengths\n    (s0,s1,s2) = np.sqrt(np.sum((np.roll(sx, -1, axis=0) - sx)**2, axis=1))\n    # we want a couple other handy values:\n    (s0_2,s1_2,s2_2) = (s0**2, s1**2, s2**2)\n    s0_inv = zinv(s0)\n    b = 0.5 * (s0_2 - s1_2 + s2_2) * s0_inv\n    h = 0.5 * np.sqrt(2*s0_2*(s1_2 + s2_2) - s0_2**2 - (s1_2 - s2_2)**2) * s0_inv\n    h_inv = zinv(h)\n    # get the visual coordinates at each face also\n    vx = np.asarray([vcoords[:,f] for f in faces])\n    # we already have enough data to calculate areal magnification\n    s_areas = geo.triangle_area(*sx)\n    v_areas = geo.triangle_area(*vx)\n    arl_mag = s_areas * zinv(v_areas)\n    # calculate the gradient at each triangle; this array is dimension 2 x 2 x m where m is the\n    # number of triangles; the first dimension is (vx,vy) and the second dimension is (fx,fy); fx\n    # and fy are the coordinates in an arbitrary coordinate system built for each face.\n    # So, to reiterate, grad is ((d(vx0)/d(fx0), d(vx0)/d(fx1)) (d(vx1)/d(fx0), d(vx1)/d(fx1)))\n    dvx0_dfx = (vx[2] - vx[1]) * s0_inv\n    dvx1_dfx = (vx[0] - (vx[1] + b*dvx0_dfx)) * h_inv\n    grad = np.asarray([dvx0_dfx, dvx1_dfx])\n    # Okay, we want to know the field signs; this is just whether the cross product of the two grad\n    # vectors (dvx0/dfx and dvx1/dfx) has a positive z\n    fsgn = np.sign(grad[0,0]*grad[1,1] - grad[0,1]*grad[1,0])\n    # We can calculate the angle too, which is just the arccos of the normalized dot-product\n    grad_norms_2 = np.sum(grad**2, axis=1)\n    grad_norms = np.sqrt(grad_norms_2)\n    (dvx_norms_inv, dvy_norms_inv) = zinv(grad_norms)\n    ngrad = grad * ((dvx_norms_inv, dvx_norms_inv), (dvy_norms_inv, dvy_norms_inv))\n    dp = np.clip(np.sum(ngrad[0] * ngrad[1], axis=0), -1, 1)\n    fang = fsgn * np.arccos(dp)\n    # Great; now we can calculate the drad and dtan; we have dx and dy, so we just need to\n    # calculate the jacobian of ((drad/dvx, drad/dvy), (dtan/dvx, dtan/dvy))\n    vx_ctr = np.mean(vx, axis=0)\n    (x0, y0) = vx_ctr\n    den_inv = zinv(np.sqrt(x0**2 + y0**2))\n    drad_dvx = np.asarray([x0,  y0]) * den_inv\n    dtan_dvx = np.asarray([-y0, x0]) * den_inv\n    # get dtan and drad\n    drad_dfx = np.asarray([np.sum(drad_dvx[i]*grad[:,i], axis=0) for i in [0,1]])\n    dtan_dfx = np.asarray([np.sum(dtan_dvx[i]*grad[:,i], axis=0) for i in [0,1]])\n    # we can now turn these into the magnitudes plus the field sign\n    rad_mag = zinv(np.sqrt(np.sum(drad_dfx**2, axis=0)))\n    tan_mag = zinv(np.sqrt(np.sum(dtan_dfx**2, axis=0)))\n    # this is the entire result if we are doing faces only\n    if to == 'faces':\n        return {'radial': rad_mag, 'tangential': tan_mag, 'areal': arl_mag, 'field_sign': fsgn}\n    # okay, we need to do some averaging!\n    mtx = simplex_summation_matrix(mesh.tess.indexed_faces)\n    cols = np.asarray(mtx.sum(axis=1), dtype=np.float)[:,0]\n    cols_inv = zinv(cols)\n    # for areal magnification, we want to do summation over the s and v areas then divide\n    s_areas = mtx.dot(s_areas)\n    v_areas = mtx.dot(v_areas)\n    arl_mag = s_areas * zinv(v_areas)\n    # for the others, we just average\n    (rad_mag, tan_mag, fsgn) = [cols_inv * mtx.dot(x) for x in (rad_mag, tan_mag, fsgn)]\n    return {'radial': rad_mag, 'tangential': tan_mag, 'areal': arl_mag, 'field_sign': fsgn}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of neighborhood - based cortical magnification values for the vertices in the given mesh.", "response": "def neighborhood_cortical_magnification(mesh, coordinates):\n    '''\n    neighborhood_cortical_magnification(mesh, visual_coordinates) yields a list of neighborhood-\n    based cortical magnification values for the vertices in the given mesh if their visual field\n    coordinates are given by the visual_coordinates matrix (must be like [x_values, y_values]). If\n    either x-value or y-value of a coordinate is either None or numpy.nan, then that cortical\n    magnification value is numpy.nan.\n    '''\n    idcs = _cmag_coord_idcs(coordinates)\n    neis = mesh.tess.indexed_neighborhoods\n    coords_vis = np.asarray(coordinates if len(coordinates) == 2 else coordinates.T)\n    coords_srf = mesh.coordinates\n    res = np.full((mesh.vertex_count, 3), np.nan, dtype=np.float)\n    res = np.array([row for row in [(np.nan,np.nan,np.nan)] for _ in range(mesh.vertex_count)],\n                   dtype=np.float)\n    for idx in idcs:\n        nei = neis[idx]\n        pts_vis = coords_vis[:,nei]\n        pts_srf = coords_srf[:,nei]\n        x0_vis = coords_vis[:,idx]\n        x0_srf = coords_srf[:,idx]\n        if any(u is None or np.isnan(u) for pt in pts_vis for u in pt): continue\n        # find tangential, radial, and areal magnifications\n        x0col_vis = np.asarray([x0_vis]).T\n        x0col_srf = np.asarray([x0_srf]).T\n        # areal is easy\n        voronoi_vis = (pts_vis - x0col_vis) * 0.5 + x0col_vis\n        voronoi_srf = (pts_srf - x0col_srf) * 0.5 + x0col_srf\n        area_vis = np.sum([geo.triangle_area(x0_vis, a, b)\n                           for (a,b) in zip(voronoi_vis.T, np.roll(voronoi_vis, 1, axis=1).T)])\n        area_srf = np.sum([geo.triangle_area(x0_srf, a, b)\n                           for (a,b) in zip(voronoi_srf.T, np.roll(voronoi_srf, 1, axis=1).T)])\n        res[idx,2] = np.inf if np.isclose(area_vis, 0) else area_srf/area_vis\n        # radial and tangentual we do together because they are very similar:\n        # find the intersection lines then add up their distances along the cortex\n        pts_vis = voronoi_vis\n        pts_srf = voronoi_srf\n        segs_srf = (pts_srf, np.roll(pts_srf, -1, axis=1))\n        segs_vis = (pts_vis, np.roll(pts_vis, -1, axis=1))\n        segs_vis_t = np.transpose(segs_vis, (2,0,1))\n        segs_srf_t = np.transpose(segs_srf, (2,0,1))\n        x0norm_vis = npla.norm(x0_vis)\n        if not np.isclose(x0norm_vis, 0):\n            dirvecs = x0_vis / x0norm_vis\n            dirvecs = np.asarray([dirvecs, [-dirvecs[1], dirvecs[0]]])\n            for dirno in [0,1]:\n                dirvec = dirvecs[dirno]\n                line = (x0_vis, x0_vis + dirvec)\n                try:\n                    isects_vis = np.asarray(geo.line_segment_intersection_2D(line, segs_vis))\n                    # okay, these will all be nan but two of them; they are the points we care about\n                    isect_idcs = np.unique(np.where(np.logical_not(np.isnan(isects_vis)))[1])\n                except Exception:\n                    isect_idcs = []\n                if len(isect_idcs) != 2:\n                    res[idx,dirno] = np.nan\n                    continue\n                isects_vis = isects_vis[:,isect_idcs].T\n                # we need the distance in visual space\n                len_vis = npla.norm(isects_vis[0] - isects_vis[1])\n                if np.isclose(len_vis, 0): res[idx,dirno] = np.inf\n                else:\n                    # we also need the distances on the surface: find the points by projection\n                    fsegs_srf = segs_srf_t[isect_idcs]\n                    fsegs_vis = segs_vis_t[isect_idcs]\n                    s02lens_vis = npla.norm(fsegs_vis[:,0] - fsegs_vis[:,1], axis=1)\n                    s01lens_vis = npla.norm(fsegs_vis[:,0] - isects_vis, axis=1)\n                    vecs_srf = fsegs_srf[:,1] - fsegs_srf[:,0]\n                    s02lens_srf = npla.norm(vecs_srf, axis=1)\n                    isects_srf = np.transpose([(s01lens_vis/s02lens_vis)]) * vecs_srf \\\n                                 + fsegs_srf[:,0]\n                    len_srf = np.sum(npla.norm(isects_srf - x0_srf, axis=1))\n                    res[idx,dirno] = len_srf / len_vis\n    # That's it!\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a list of paths to a list of subjects.", "response": "def to_subject_paths(paths):\n    '''\n    to_subject_paths(paths) accepts either a string that is a :-separated list of directories or a\n     list of directories and yields a list of all the existing directories.\n    '''\n    if paths is None: return []\n    if pimms.is_str(paths): paths = paths.split(':')\n    paths = [os.path.expanduser(p) for p in paths]\n    return [p for p in paths if os.path.isdir(p)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_freesurfer_subject_path(path):\n    '''\n    is_freesurfer_subject_path(path) yields True if the given path appears to be a valid freesurfer\n      subject path and False otherwise.\n    A path is considered to be freesurfer-subject-like if it contains the directories mri/, surf/,\n    and label/.\n    '''\n    if not os.path.isdir(path): return False\n    else: return all(os.path.isdir(os.path.join(path, d)) for d in ['mri', 'surf', 'label'])", "response": "Returns True if the given path is a valid freesurfer - subject - like subject path and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_subject_path(sub, check_path=True):\n    '''\n    find_subject_path(sub) yields the full path of a Freesurfer subject with the name given by the\n      string sub, if such a subject can be found in the Freesurfer search paths. See also\n      add_subject_path.\n\n    If no subject is found, then None is returned.\n\n    The optional argument check_path (default: True) may be set to False to indicate that sanity\n    checks regarding the state of the file-system should be skipped. If check_path is set to True,\n    then a directory will only be returned if it is a directory on the file-system and it contains\n    at least the subdirectories mri/, label/, and surf/. If check_path is set to False, then\n    find_subject_path will first search for any directory that satisfies the above requirements,\n    and, failing that, will search for any directory matching sub that at least exists. If this also\n    fails, then sub itself is returned (i.e., sub is presumed to be a valid path).\n    '''\n    sdirs = config['freesurfer_subject_paths']\n    # if it's a full/relative path already, use it:\n    if is_freesurfer_subject_path(sub): return sub\n    path = next((p for sd in sdirs for p in [os.path.join(sd, sub)]\n                 if is_freesurfer_subject_path(p)),\n                None)\n    if path is not None: return path\n    if check_path: return None\n    if os.path.isdir(sub): return sub\n    return next((p for sd in sdirs for p in [os.path.join(sd, sub)]\n                 if os.path.isdir(p)),\n                sub)", "response": "Returns the full path of a Freesurfer subject with the given name given by the\n      string sub."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a FreeSurfer Subject object for the given subject name.", "response": "def subject(name, meta_data=None, check_path=True):\n    '''\n    subject(name) yields a freesurfer Subject object for the subject with the given name. Subjects\n      are cached and not reloaded, so multiple calls to subject(name) will yield the same immutable\n      subject object..\n\n    Note that subects returned by freesurfer_subject() are always persistent Immutable objects; this\n    means that you must create a transient version of the subject to modify it via the member\n    function sub.transient(). Better, you can make copies of the objects with desired modifications\n    using the copy method--see the pimms library documentation regarding immutable classes and\n    objects.\n\n    The following options are accepted:\n      * meta_data (default: None) may optionally be a map that contains meta-data to be passed along\n        to the subject object (note that this meta-data will not be cached).\n      * check_path (default: True) may optionally be set to False to ignore the requirement that a\n        directory contain at least the mri/, label/, and surf/ directories to be considered a valid\n        FreeSurfer subject directory. Subject objects returned using this argument are not cached.\n        Additionally, check_path may be set to None instead of False, indicating that no checks or\n        search should be performed; the string name should be trusted to be an exact relative or\n        absolute path to a valid FreeSurfer subejct.\n    '''\n    name = os.path.expanduser(os.path.expandvars(name))\n    if check_path is None:\n        sub = Subject(name, check_path=False)\n        if isinstance(sub, Subject): sub.persist()\n    else:\n        subpath = find_subject_path(name, check_path=check_path)\n        if subpath is None and name == 'fsaverage':\n            # we can use the benson and winawer 2018 dataset\n            import neuropythy as ny\n            try: return ny.data['benson_winawer_2018'].subjects['fsaverage']\n            except Exception: pass # error message below is more accurate...\n        if subpath is None:\n            raise ValueError('Could not locate subject with name \\'%s\\'' % name)\n        elif check_path:\n            fpath = '/' + os.path.relpath(subpath, '/')\n            if fpath in subject._cache:\n                sub = subject._cache[fpath]\n            else:\n                sub = Subject(subpath)\n                if isinstance(sub, Subject): subject._cache[fpath] = sub.persist()\n        else:\n            sub = Subject(subpath, check_path=False)\n            if isinstance(sub, Subject): sub.persist()\n    return (None                     if sub is None           else\n            sub.with_meta(meta_data) if meta_data is not None else\n            sub)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nforget about cached data for a subject with the given sid.", "response": "def forget_subject(sid):\n    '''\n    forget_subject(sid) causes neuropythy's freesurfer module to forget about cached data for the\n      subject with subject id sid. The sid may be any sid that can be passed to the subject()\n      function.\n\n    This function is useful for batch-processing of subjects in a memory-limited environment; e.g.,\n    if you run out of memory while processing FreeSurfer subjects it is possibly because neuropythy\n    is caching all of their data instead of freeing it.\n    '''\n    sub = subject(sid)\n    if sub.path in subject._cache:\n        del subject._cache[sub.path]\n    else:\n        for (k,v) in six.iteritems(subject._cache):\n            if v is sub:\n                del subject._cache[k]\n                break\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a FreeSurfer tkr VOX2RAS matrix for the given nibabel image object img and optional voxel dimensions zooms.", "response": "def tkr_vox2ras(img, zooms=None):\n    '''\n    tkr_vox2ras(img) yields the FreeSurfer tkr VOX2RAS matrix for the given nibabel image object\n      img. The img must have a get_shape() method and header member with a get_zooms() method.\n    tkr_vox2ras(hdr) operates on a nibabel image header object.\n    tkr_vox2ras(shape, zooms) operates on the shape (e.g., for FreeSurfer subjects (256,256,256))\n      and the zooms or voxel dimensions (e.g., for FreeSurfer subjects, (1.0, 1.0, 1.0)).\n    '''\n    if zooms is not None:\n        # let's assume that they passed shape, zooms\n        shape = img\n    else:\n        try:              img = img.header\n        except Exception: pass\n        try:              (shape, zooms) = (img.get_data_shape(), img.get_zooms())\n        except Exception: raise ValueError('single argument must be nibabel image or header')\n    # Okay, we have shape and zooms...\n    zooms = zooms[0:3]\n    shape = shape[0:3]\n    (dC, dR, dS) = zooms\n    (nC, nR, nS) = 0.5 * (np.asarray(shape) * zooms)\n    return np.asarray([[-dC,   0,   0,  nC],\n                       [  0,   0,  dS, -nS],\n                       [  0, -dR,   0,  nR],\n                       [  0,   0,   0,   1]])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the MGH file and return the image as a sequence of MGHImage objects.", "response": "def load_mgh(filename, to='auto'):\n    '''\n    load_mgh(filename) yields the MGHImage referened by the given filename by using the\n      nibabel.freesurfer.mghformat.load function.\n    \n    The optional argument 'to' may be used to coerce the resulting data to a particular format; the\n    following arguments are understood:\n      * 'header' will yield just the image header\n      * 'data' will yield the image's data-array\n      * 'field' will yield a squeezed version of the image's data-array and will raise an error if\n        the data object has more than 2 non-unitary dimensions (appropriate for loading surface\n        properties stored in image files)\n      * 'affine' will yield the image's affine transformation\n      * 'image' will yield the raw image object\n      * 'auto' is equivalent to 'image' unless the image has no more than 2 non-unitary dimensions,\n        in which case it is assumed to be a surface-field and the return value is equivalent to\n        the 'field' value.\n    '''\n    img = fsmgh.load(filename)\n    to = to.lower()\n    if to == 'image':    return img\n    elif to == 'data':   return img.get_data()\n    elif to == 'affine': return img.affine\n    elif to == 'header': return img.header\n    elif to == 'field':\n        dat = np.squeeze(img.get_data())\n        if len(dat.shape) > 2:\n            raise ValueError('image requested as field has more than 2 non-unitary dimensions')\n        return dat\n    elif to in ['auto', 'automatic']:\n        dims = set(img.dataobj.shape)\n        if 1 < len(dims) < 4 and 1 in dims:\n            return np.squeeze(img.get_data())\n        else:\n            return img\n    else:\n        raise ValueError('unrecognized \\'to\\' argument \\'%s\\'' % to)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an object to an MGHmage object.", "response": "def to_mgh(obj, like=None, header=None, affine=None, extra=Ellipsis):\n    '''\n    to_mgh(obj) yields an MGHmage object that is as equivalent as possible to the given object obj.\n      If obj is an MGHImage already and no changes are requested, then it is returned unmolested;\n      otherwise, the optional arguments can be used to edit the header, affine, and exta.\n\n    The following options are accepted:\n      * like (default: None) may be provided to give a guide for the various header- and meta-data\n        that is included in the image. If this is a nifti image object, its meta-data are used; if\n        this is a subject, then the meta-data are deduced from the subject's voxel and native\n        orientation matrices. All other specific options below override anything deduced from the\n        like argument.\n      * header (default: None) may be a Nifti1 or Niti2 image header to be used as the nifti header\n        or to replace the header in a new image.\n      * affine (default: None) may specify the affine transform to be given to the image object.\n      * extra (default: Ellipsis) may specify an extra mapping for storage with the image data; the\n        default value Ellipsis indicates that such a mapping should be left as is if it already\n        exists in the obj or in the like option and should otherwise be empty.\n    '''\n    obj0 = obj\n    # First go from like to explicit versions of affine and header:\n    if like is not None:\n        if isinstance(like, nib.analyze.AnalyzeHeader) or isinstance(obj, fsmgh.MGHHeader):\n            if header is None: header = like\n        elif isinstance(like, nib.analyze.SpatialImage):\n            if header is None: header = like.header\n            if affine is None: affine = like.affine\n            if extra is Ellipsis: extra = like.extra\n        elif isinstance(like, mri.Subject):\n            if affine is None: affine = like.voxel_to_native_matrix\n        else:\n            raise ValueError('Could not interpret like argument with type %s' % type(like))\n    # check to make sure that we have to change something:\n    elif (isinstance(obj, fsmgh.MGHImage)):\n        if ((header is None or obj.header is header) and\n            (extra is Ellipsis or extra == obj.extra or (extra is None and len(obj.extra) == 0))):\n            return obj\n    # okay, now look at the header and affine etc.\n    if header is None:\n        if isinstance(obj, nib.analyze.SpatialImage):\n            header = obj.header\n        else:\n            header = None\n    if affine is None:\n        if isinstance(obj, nib.analyze.SpatialImage):\n            affine = obj.affine\n        else:\n            affine = np.eye(4)\n    if extra is None: extra = {}\n    # Figure out what the data is\n    if isinstance(obj, nib.analyze.SpatialImage):\n        obj = obj.dataobj\n    else:\n        obj = np.asarray(obj)\n    if len(obj.shape) < 3: obj = np.asarray([[obj]])\n    # make sure the dtype isn't a high-bit integer or float\n    if np.issubdtype(obj.dtype, np.integer) and obj.dtype.itemsize > 4:\n        obj = np.asarray(obj, dtype=np.int32)\n    elif np.issubdtype(obj.dtype, np.floating) and obj.dtype.itemsize > 4:\n        obj = np.asarray(obj, dtype=np.float32)\n    # Okay, make a new object now...\n    obj = fsmgh.MGHImage(obj, affine, header=header, extra=extra)\n    # Okay, that's it!\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the given object to the given filename in the mgh format and returns the filename.", "response": "def save_mgh(filename, obj, like=None, header=None, affine=None, extra=Ellipsis):\n    '''\n    save_mgh(filename, obj) saves the given object to the given filename in the mgh format and\n      returns the filename.\n\n    All options that can be given to the to_mgh function can also be passed to this function; they\n    are used to modify the object prior to exporting it.\n    '''\n    obj = to_mgh(obj, like=like, header=header, affine=affine, extra=extra)\n    obj.to_filename(filename)\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a freesurfer geometry file into a new object.", "response": "def load_freesurfer_geometry(filename, to='mesh', warn=False):\n    '''\n    load_freesurfer_geometry(filename) yields the data stored at the freesurfer geometry file given\n      by filename. The optional argument 'to' may be used to change the kind of data that is\n      returned.\n\n    The following are valid settings for the 'to' keyword argument:\n      * 'mesh' (the default) yields a mesh object\n      * 'tess' yields a tess object (discarding coordinates)\n      * 'raw' yields a tuple of numpy arrays, identical to the read_geometry return value.\n    '''\n    if not warn:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore',\n                                    category=UserWarning,\n                                    module='nibabel')\n            (xs, fs, info) = fsio.read_geometry(filename, read_metadata=True)\n    else:\n        (xs, fs, info) = fsio.read_geometry(filename, read_metadata=True)\n    # see if there's chirality data here...\n    filename = os.path.split(filename)[1]\n    filename = filename.lower()\n    if   filename.startswith('lh'): info['chirality'] = 'lh.'\n    elif filename.startswith('rh'): info['chirality'] = 'rh.'\n    # parse it into something\n    to = to.lower()\n    if to in ['mesh', 'auto', 'automatic']:\n        return geo.Mesh(fs, xs, meta_data=info)\n    elif to in ['tess', 'tesselation']:\n        return geo.Tesselation(fs, meta_data=info)\n    elif to in ['coords', 'coordinates']:\n        return xs\n    elif to in ['triangles', 'faces']:\n        return fs\n    elif to in ['meta', 'meta_data']:\n        return info\n    elif to =='raw':\n        return (xs, fs)\n    else:\n        raise ValueError('Could not understand \\'to\\' argument: %s' % to)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving a freesurfer geometry object to a file.", "response": "def save_freesurfer_geometry(filename, obj, volume_info=None, create_stamp=None):\n    '''\n    save_mgh(filename, obj) saves the given object to the given filename in the mgh format and\n      returns the filename.\n\n    All options that can be given to the to_mgh function can also be passed to this function; they\n    are used to modify the object prior to exporting it.\n    '''\n    obj = geo.to_mesh(obj)\n    fsio.write_geometry(filename, obj.coordinates.T, obj.tess.faces.T,\n                        volume_info=volume_info, create_stamp=create_stamp)\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_freesurfer_morph(filename, obj, face_count=0):\n    '''\n    save_freesurfer_morph(filename, obj) saves the given object using nibabel.freesurfer.io's\n      write_morph_data function, and returns the given filename.\n    '''\n    fsio.write_morph_data(filename, obj, fnum=face_count)\n    return filename", "response": "Save the given object to a file using the Freesurfer morphology."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a string to a heli - style string.", "response": "def to_hemi_str(s):\n    '''\n    to_hemi_str(s) yields either 'lh', 'rh', or 'lr' depending on the input s.\n\n    The match rules for s are as follows:\n      * if s is None or Ellipsis, returns 'lr'\n      * if s is not a string, error; otherwise s becomes s.lower()\n      * if s is in ('lh','rh','lr'), returns s\n      * if s is in ('left', 'l', 'sh'), returns 'lh'\n      * if s is in ('right', 'r', 'dh'), returns 'rh'\n      * if s in in ('both', 'all', 'xh'), returns 'lr'\n      * otherwise, raises an error\n    '''\n    if s is None or s is Ellipsis: return 'lr'\n    if not pimms.is_str(s): raise ValueError('to_hemi_str(%s): not a string or ... or None' % s)\n    s = s.lower()\n    if   s in ('lh',    'rh',  'lr'): return s\n    elif s in ('left',  'l',   'sh'): return 'lh'\n    elif s in ('right', 'r',   'dh'): return 'rh'\n    elif s in ('both',  'all', 'xh'): return 'lr'\n    else: raise ValueError('Could not understand to_hemi_str argument: %s' % s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnormalizing the given object to a JSON - friendly description of the given object.", "response": "def normalize(data):\n    '''\n    normalize(obj) yields a JSON-friendly normalized description of the given object. If the data\n      cannot be normalized an error is raised.\n\n    Any object that implements a normalize() function can be normalized, so long as the mapping \n    object returned by normalize() itself can be normalized. Note that the normalize() function\n    must return a mapping object.\n\n    Objects that can be represented as themselves are returned as themselves. Any other object will\n    be represented as a map that includes the reserved key '__type__' which will map to a\n    2-element list [module_name, class_name]; upon denomrlization, the module and class k are looked\n    up and k.denomalize(data) is called.\n    '''\n    if data is None: return None\n    elif pimms.is_array(data, 'complex') and not pimms.is_array(data, 'real'):\n        # any complex number must be handled specially:\n        return {normalize_type_key: [None, 'complex'], 're':np.real(data), 'im': np.imag(data)}\n    elif is_set(data):\n        # sets also have a special type:\n        return {normalize_type_key: [None, 'set'], 'elements': normalize(list(data))}\n    elif pimms.is_scalar(data, ('number', 'string', 'unicode', 'bool')):\n        # scalars are already normalized\n        return data\n    elif sps.issparse(data):\n        # sparse matrices always get encoded as if they were csr_matrices (for now)\n        (i,j,v) = sps.find(data)\n        return {normalize_type_key: [None, 'sparse_matrix'],\n                'rows':i.tolist(), 'cols':j.tolist(), 'vals': v.tolist(),\n                'shape':data.shape}\n    elif pimms.is_map(data):\n        newdict = {}\n        for (k,v) in six.iteritems(data):\n            if not pimms.is_str(k):\n                raise ValueError('Only maps with strings for keys can be normalized')\n            newdict[k] = normalize(v)\n        return newdict\n    elif pimms.is_array(data, ('number', 'string', 'unicode', 'bool')):\n        # numpy arrays just get turned into lists\n        return data.tolist() if pimms.is_nparray(data) else data\n    elif data is Ellipsis:\n        return {normalize_type_key: [None, 'ellipsis']}\n    elif pimms.is_scalar(data):\n        # we have an object of some type we don't really recognize\n        try:    m = data.normalize()\n        except Exception: raise ValueError('Failed to run obj.normalize() on unrecognized obj: %s' % data)\n        if not pimms.is_map(m): raise ValueError('obj.normalize() returned non-map; obj: %s' % data)\n        m = dict(m)\n        tt = type(data)\n        m[normalize_type_key] = [tt.__module__, tt.__name__]\n        return m\n    else:\n        # we have an array/list of some kind that isn't a number, string, or boolean\n        return [normalize(x) for x in data]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a denormalized version of the given JSON - friendly normalized data.", "response": "def denormalize(data):\n    '''\n    denormalize(data) yield a denormalized version of the given JSON-friendly normalized data. This\n      is the inverse of the normalize(obj) function.\n\n    The normalize and denormalize functions use the reserved keyword '__type__' along with the\n    <obj>.normalize() and <class>.denormalize(data) functions to manage types of objects that are\n    not JSON-compatible. Please see help(normalize) for more details.\n    '''\n    if   data is None: return None\n    elif pimms.is_scalar(data, ('number', 'bool', 'string', 'unicode')): return data\n    elif pimms.is_map(data):\n        # see if it's a non-native map\n        if normalize_type_key in data:\n            (mdl,cls) = data[normalize_type_key]\n            if mdl is None:\n                if   cls == 'ellipsis': return Ellipsis\n                elif cls == 'complex':  return np.array(data['re']) + 1j*np.array(data['im'])\n                elif cls == 'set':      return set(denormalize(data['elements']))\n                elif cls == 'sparse_matrix':\n                    return sps.csr_matrix((data['vals'], (data['rows'],data['cols'])),\n                                          shape=data['shape'])\n                else: raise ValueError('unrecognized builtin denormalize class: %s' % cls)\n            else:\n                cls = getattr(importlib.import_module(mdl), cls)\n                d = {k:denormalize(v) for (k,v) in six.iteritems(data) if k != normalize_type_key}\n                return cls.denormalize(d)\n        else: return {k:denormalize(v) for (k,v) in six.iteritems(data)} # native map\n    else:\n        # must be a list of some type\n        if not hasattr(data, '__iter__'):\n            msg = 'denormalize does not recognized object %s with type %s' % (data, type(data))\n            raise ValueError(msg)\n        # lists of primitives need not be changed\n        if pimms.is_array(data, ('number', 'bool', 'string', 'unicode')): return data\n        return [denormalize(x) for x in data]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_affine(aff, dims=None):\n    '''\n    to_affine(None) yields None.\n    to_affine(data) yields an affine transformation matrix equivalent to that given in data. Such a\n      matrix may be specified either as (matrix, offset_vector), as an (n+1)x(n+1) matrix, or, as an\n      n x (n+1) matrix.\n    to_affine(data, dims) additionally requires that the dimensionality of the data be dims; meaning\n      that the returned matrix will be of size (dims+1) x (dims+1).\n    '''\n    if aff is None: return None\n    if isinstance(aff, _tuple_type):\n        # allowed to be (mtx, offset)\n        if (len(aff) != 2                       or\n            not pimms.is_matrix(aff[0], 'real') or\n            not pimms.is_vector(aff[1], 'real')):\n            raise ValueError('affine transforms must be matrices or (mtx,offset) tuples')\n        mtx = np.asarray(aff[0])\n        off = np.asarray(aff[1])\n        if dims is not None:\n            if mtx.shape[0] != dims or mtx.shape[1] != dims:\n                raise ValueError('%dD affine matrix must be %d x %d' % (dims,dims,dims))\n            if off.shape[0] != dims:\n                raise ValueError('%dD affine offset must have length %d' % (dims,dims))\n        else:\n            dims = off.shape[0]\n            if mtx.shape[0] != dims or mtx.shape[1] != dims:\n                raise ValueError('with offset size=%d, matrix must be %d x %d' % (dims,dims,dims))\n        aff = np.zeros((dims+1,dims+1), dtype=np.float)\n        aff[dims,dims] = 1\n        aff[0:dims,0:dims] = mtx\n        aff[0:dims,dims] = off\n        return pimms.imm_array(aff)\n    if not pimms.is_matrix(aff, 'real'):\n        raise ValueError('affine transforms must be matrices or (mtx, offset) tuples')\n    aff = np.asarray(aff)\n    if dims is None:\n        dims = aff.shape[1] - 1\n    if aff.shape[0] == dims:\n        lastrow = np.zeros((1,dims+1))\n        lastrow[0,-1] = 1\n        aff = np.concatenate((aff, lastrow))\n    if aff.shape[1] != dims+1 or aff.shape[0] != dims+1:\n        arg = (dims, dims,dims+1, dims+1,dims+1)\n        raise ValueError('%dD affine matrix must be %dx%d or %dx%d' % arg)\n    return aff", "response": "Converts an affine transformation matrix to an affine transformation matrix equivalent to that given in data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncoerces the object d to a pandas DataFrame object.", "response": "def to_dataframe(d, **kw):\n    '''\n    to_dataframe(d) attempts to coerce the object d to a pandas DataFrame object. If d is a\n      tuple of 2 items whose second argument is a dictionary, then the dictionary will be taken\n      as arguments for the dataframe constructor. These arguments may alternately be given as\n      standard keyword arguments.\n    '''\n    import pandas\n    if pimms.is_itable(d): d = d.dataframe\n    if is_dataframe(d): return d if len(kw) == 0 else pandas.DataFrame(d, **kw)\n    if is_tuple(d) and len(d) == 2 and pimms.is_map(d[1]):\n        try: return to_dataframe(d[0], **pimms.merge(d[1], kw))\n        except Exception: pass\n    # try various options:\n    try: return pandas.DataFrame(d, **kw)\n    except Exception: pass\n    try: return pandas.DataFrame.from_records(d, **kw)\n    except Exception: pass\n    try: return pandas.DataFrame.from_dict(d, **kw)\n    except Exception: pass\n    raise ValueError('Coersion to dataframe failed for object %s' % d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dataframe_select(df, *cols, **filters):\n    '''\n    dataframe_select(df, k1=v1, k2=v2...) yields df after selecting all the columns in which the\n      given keys (k1, k2, etc.) have been selected such that the associated columns in the dataframe\n      contain only the rows whose cells match the given values.\n    dataframe_select(df, col1, col2...) selects the given columns.\n    dataframe_select(df, col1, col2..., k1=v1, k2=v2...) selects both.\n    \n    If a value is a tuple/list of 2 elements, then it is considered a range where cells must fall\n    between the values. If value is a tuple/list of more than 2 elements or is a set of any length\n    then it is a list of values, any one of which can match the cell.\n    '''\n    ii = np.ones(len(df), dtype='bool')\n    for (k,v) in six.iteritems(filters):\n        vals = df[k].values\n        if   pimms.is_set(v):                    jj = np.isin(vals, list(v))\n        elif pimms.is_vector(v) and len(v) == 2: jj = (v[0] <= vals) & (vals < v[1])\n        elif pimms.is_vector(v):                 jj = np.isin(vals, list(v))\n        else:                                    jj = (vals == v)\n        ii = np.logical_and(ii, jj)\n    if len(ii) != np.sum(ii): df = df.loc[ii]\n    if len(cols) > 0: df = df[list(cols)]\n    return df", "response": "Returns a dataframe where the given columns are selected."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an auto - dict that vivifies the given value of ival on miss.", "response": "def auto_dict(ival=None, miss=None):\n    '''\n    auto_dict() yields an auto-dict that vivifies value of {} on miss.\n    auto_dict(ival) uses the given dict ival as an initializer.\n    auto_dict(ival, miss) uses the given miss function.\n    auto_dict(None, miss) is equivalent to auto_dict() with the given miss function.\n\n    If the miss argument (also a named parameter, miss) is an empty list, an empty dict, or an\n    empty set, then the miss is taken to be an anonymous lambda function that returns an empty\n    item of the same type.\n    '''\n    if ival is None: d = AutoDict()\n    else: d = AutoDict(ival)\n    if miss == {} or miss is None: return d\n    elif miss == []: d.on_miss = lambda:[]\n    elif miss == set([]): d.on_miss = lambda:set([])\n    else: d.on_miss = miss\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef address_data(data, dims=None, surface=0.5, strict=True):\n    '''\n    address_data(addr) yields the tuple (faces, coords) of the address data where both faces and\n      coords are guaranteed to be numpy arrays with sizes (3 x n) and (d x n); this will coerce\n      the data found in addr if necessary to do this. If the data is not valid, then an error is\n      raised. If the address is empty, this yields (None, None).\n\n    The following options may be given:\n       * dims (default None) specifies the dimensions requested for the coordinates. If 2, then\n         the final dimension is dropped from 3D coordinates; if 3 then will add the optional\n         surface argument as the final dimension of 2D coordinates.\n       * surface (default: 0.5) specifies the surface to use for 2D addresses when a 3D address;\n         is requested. If None, then an error will be raised when this condition is encountered.\n         This should be either 'white', 'pial', 'midgray', or a real number in the range [0,1]\n         where 0 is the white surface and 1 is the pial surface.\n       * strict (default: True) specifies whether an error should be raised when there are\n         non-finite values found in the faces or the coordinates matrices. These values are usually\n         indicative of an attempt to address a point that was not inside the mesh/cortex.\n    '''\n    if data is None: return (None, None)\n    if not is_address(data): raise ValueError('argument is not an address')\n    faces = np.asarray(data['faces'])\n    coords = np.asarray(data['coordinates'])\n    if len(faces.shape) > 2 or len(coords.shape) > 2:\n        raise ValueError('address data contained high-dimensional arrays')\n    elif len(faces.shape) != len(coords.shape):\n        raise ValueError('address data faces and coordinates are different shapes')\n    elif len(faces) == 0: return (None, None)\n    if len(faces.shape) == 2 and faces.shape[0] != 3: faces = faces.T\n    if faces.shape[0] != 3: raise ValueError('address contained bad face matrix')\n    if len(coords.shape) == 2 and coords.shape[0] not in (2,3): coords = coords.T\n    if coords.shape[0] not in (2,3): raise ValueError('address coords are neither 2D nor 3D')\n    if dims is None: dims = coords.shape[0]\n    elif coords.shape[0] != dims:\n        if dims == 2: coords = coords[:2]\n        else:\n            if surface is None: raise ValueError('address data must be 3D')\n            elif pimms.is_str(surface):\n                surface = surface.lower()\n                if surface == 'pial': surface = 1\n                elif surface == 'white': surface = 0\n                elif surface in ('midgray', 'mid', 'middle'): surface = 0.5\n                else: raise ValueError('unrecognized surface name: %s' % surface)\n            if not pimms.is_real(surface) or surface < 0 or surface > 1:\n                raise ValueError('surface must be a real number in [0,1]')\n            coords = np.vstack((coords, np.full((1, coords.shape[1]), surface)))\n    if strict:\n        if np.sum(np.logical_not(np.isfinite(coords))) > 0:\n            w = np.where(np.logical_not(np.isfinite(coords)))\n            if len(w[0]) > 10:\n                raise ValueError('address contains %d non-finite coords' % len(w[0]))\n            else:\n                raise ValueError('address contains %d non-finite coords (%s)' % (len(w),w))\n        if np.sum(np.logical_not(np.isfinite(faces))) > 0:\n            w = np.where(np.logical_not(np.isfinite(faces)))\n            if len(w[0]) > 10:\n                raise ValueError('address contains %d non-finite faces' % len(w[0]))\n            else:\n                raise ValueError('address contains %d non-finite faces (%s)' % (len(w[0]),w))\n    return (faces, coords)", "response": "Convert a dictionary of data into a tuple of faces and coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the sum of all the values in a numpy array object.", "response": "def cplus(*args):\n    '''\n    cplus(a, b...) returns the sum of all the values as a numpy array object. Like numpy's add\n      function or a+b syntax, plus will thread over the latest dimension possible.\n\n    Additionally, cplus works correctly with sparse arrays.\n    '''\n    n = len(args)\n    if   n == 0: return np.asarray(0)\n    elif n == 1: return np.asarray(args[0])\n    elif n >  2: return reduce(plus, args)\n    (a,b) = args\n    if sps.issparse(a):\n        if not sps.issparse(b):\n            b = np.asarray(b)\n            if len(b.shape) == 0: b = np.reshape(b, (1,1))\n    elif sps.issparse(b):\n        a = np.asarray(a)\n        if len(a.shape) == 0: a = np.reshape(a, (1,1))\n    else:\n        a = np.asarray(a)\n        b = np.asarray(b)\n    return a + b"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plus(*args):\n    '''\n    plus(a, b...) returns the sum of all the values as a numpy array object. Unlike numpy's add\n      function or a+b syntax, plus will thread over the earliest dimension possible; thus if a.shape\n      a.shape is (4,2) and b.shape is 4, plus(a,b) is a equivalent to\n      [ai+bi for (ai,bi) in zip(a,b)].\n    '''\n    n = len(args)\n    if   n == 0: return np.asarray(0)\n    elif n == 1: return np.asarray(args[0])\n    elif n >  2: return reduce(plus, args)\n    (a,b) = unbroadcast(*args)\n    return a + b", "response": "A function that adds all the values in the given list as a numpy array object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cminus(a, b):\n    '''\n    cminus(a, b) returns the difference a - b as a numpy array object. Like numpy's subtract\n      function or a - b syntax, minus will thread over the latest dimension possible.\n    '''\n    # adding/subtracting a constant to/from a sparse array is an error...\n    spa = sps.issparse(a)\n    spb = sps.issparse(b)\n    if not spa: a = np.asarray(a)\n    if not spb: b = np.asarray(b)\n    if   spa: b = np.reshape(b, (1,1)) if len(np.shape(b)) == 0 else b\n    elif spb: a = np.reshape(a, (1,1)) if len(np.shape(a)) == 0 else a\n    return a - b", "response": "Returns the difference a - b as a numpy array object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef minus(a, b):\n    '''\n    minus(a, b) returns the difference a - b as a numpy array object. Unlike numpy's subtract\n      function or a - b syntax, minus will thread over the earliest dimension possible; thus if\n      a.shape is (4,2) and b.shape is 4, a - b is a equivalent to [ai-bi for (ai,bi) in zip(a,b)].\n    '''\n    (a,b) = unbroadcast(a,b)\n    return a - b", "response": "Returns the difference a - b as a numpy array object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the product of all the values in args as a numpy array object.", "response": "def ctimes(*args):\n    '''\n    ctimes(a, b...) returns the product of all the values as a numpy array object. Like numpy's\n      multiply function or a*b syntax, times will thread over the latest dimension possible; thus\n      if a.shape is (4,2) and b.shape is 2, times(a,b) is a equivalent to a * b.\n\n    Unlike numpy's multiply function, ctimes works with sparse matrices and will reify them.\n    '''\n    n = len(args)\n    if   n == 0: return np.asarray(0)\n    elif n == 1: return np.asarray(args[0])\n    elif n >  2: return reduce(plus, args)\n    (a,b) = args\n    if   sps.issparse(a): return a.multiply(b)\n    elif sps.issparse(b): return b.multiply(a)\n    else:                 return np.asarray(a) * b"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninverse of a sequence of elements.", "response": "def inv(x):\n    '''\n    inv(x) yields the inverse of x, 1/x.\n\n    Note that inv supports sparse matrices, but it is forced to reify them. Additionally, because\n    inv raises an error on divide-by-zero, they are unlikely to work. For better sparse-matrix\n    support, see zinv.\n    '''\n    if sps.issparse(x): return 1.0 / x.toarray()        \n    else:               return 1.0 / np.asarray(x)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the inverse of the array x.", "response": "def zinv(x, null=0):\n    '''\n    zinv(x) yields 1/x if x is not close to 0 and 0 otherwise. Automatically threads over arrays and\n      supports sparse-arrays.\n\n    The optional argument null (default: 0) may be given to specify that zeros in the arary x should\n    instead be replaced with the given value. Note that if this value is not equal to 0, then any\n    sparse array passed to zinv must be reified.\n\n    The zinv function never raises an error due to divide-by-zero; if you desire this behavior, use\n    the inv function instead.\n    '''\n    if sps.issparse(x):\n        if null != 0: return zinv(x.toarray(), null=null)\n        x = x.copy()\n        x.data = zinv(x.data)\n        try: x.eliminate_zeros()\n        except Exception: pass\n        return x\n    else:\n        x = np.asarray(x)\n        z = np.isclose(x, 0)\n        r = np.logical_not(z) / (x + z)\n        if null == 0: return r\n        r[z] = null\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cdivide(a, b):\n    '''\n    cdivide(a, b) returns the quotient a / b as a numpy array object. Like numpy's divide function\n      or a/b syntax, divide will thread over the latest dimension possible. Unlike numpy's divide,\n      cdivide works with sparse matrices.\n\n    Note that warnings/errors are raised by this function when divide-by-zero occurs, so it is\n    usually not useful to use cdivide() with sparse matrices--see czdivide instead.\n    '''\n    if   sps.issparse(a): return a.multiply(inv(b))\n    elif sps.issparse(b): return np.asarray(a) / b.toarray()\n    else:                 return np.asarray(a) / np.asarray(b)", "response": "This function returns the quotient a and b as a numpy array object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndivide two numpy arrays.", "response": "def divide(a, b):\n    '''\n    divide(a, b) returns the quotient a / b as a numpy array object. Unlike numpy's divide function\n      or a/b syntax, divide will thread over the earliest dimension possible; thus if a.shape is\n      (4,2) and b.shape is 4, divide(a,b) is a equivalent to [ai/bi for (ai,bi) in zip(a,b)].\n\n    Note that divide(a,b) supports sparse array arguments, but if b is a sparse matrix, then it will\n    be reified. Additionally, errors are raised by this function when divide-by-zero occurs, so it\n    is usually not useful to use divide() with sparse matrices--see zdivide instead.\n    '''\n    (a,b) = unbroadcast(a,b)\n    return cdivide(a,b)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the quotient a and b as a numpy array object.", "response": "def czdivide(a, b, null=0):\n    '''\n    czdivide(a, b) returns the quotient a / b as a numpy array object. Like numpy's divide function\n      or a/b syntax, czdivide will thread over the latest dimension possible. Unlike numpy's divide,\n      czdivide works with sparse matrices. Additionally, czdivide multiplies a by the zinv of b, so\n      divide-by-zero entries are replaced with 0 in the result.\n\n    The optional argument null (default: 0) may be given to specify that zeros in the arary b should\n    instead be replaced with the given value in the result. Note that if this value is not equal to\n    0, then any sparse array passed as argument b must be reified.\n\n    The czdivide function never raises an error due to divide-by-zero; if you desire this behavior,\n    use the cdivide function instead.\n    '''\n    if null == 0:         return a.multiply(zinv(b)) if sps.issparse(a) else a * zinv(b)\n    elif sps.issparse(b): b = b.toarray()\n    else:                 b = np.asarray(b)\n    z = np.isclose(b, 0)\n    q = np.logical_not(z)\n    zi = q / (b + z)\n    if sps.issparse(a):\n        r = a.multiply(zi).tocsr()\n    else:\n        r = np.asarray(a) * zi\n    r[np.ones(a.shape, dtype=np.bool)*z] = null\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cpower(a,b):\n    '''\n    cpower(a,b) is equivalent to a**b except that it also operates over sparse arrays; though it\n    must reify them to do so.\n    '''\n    if sps.issparse(a): a = a.toarray()\n    if sps.issparse(b): b = b.toarray()\n    return a ** b", "response": "cpower is equivalent to a ** b except that it operates over sparse arrays ; it also operates over sparse arrays ; it also operates over sparse arrays ; it also operates over sparse arrays ; it also operates over sparse arrays ; it also operates over sparse arrays ; it must reify them to do so."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the power of a and b.", "response": "def power(a,b):\n    '''\n    power(a,b) is equivalent to a**b except that, like the neuropythy.util.times function, it\n      threads over the earliest dimension possible rather than the latest, as numpy's power function\n      and ** syntax do. The power() function also works with sparse arrays; though it must reify\n      them during the process.\n    '''\n    (a,b) = unbroadcast(a,b)\n    return cpower(a,b)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inner(a,b):\n    '''\n    inner(a,b) yields the dot product of a and b, doing so in a fashion that respects sparse\n      matrices when encountered. This does not error check for bad dimensionality.\n\n    If a or b are constants, then the result is just the a*b; if a and b are both vectors or both\n    matrices, then the inner product is dot(a,b); if a is a vector and b is a matrix, this is\n    equivalent to as if a were a matrix with 1 row; and if a is a matrix and b a vector, this is\n    equivalent to as if b were a matrix with 1 column.\n    '''\n    if   sps.issparse(a): return a.dot(b)\n    else: a = np.asarray(a)\n    if len(a.shape) == 0: return a*b\n    if sps.issparse(b):\n        if len(a.shape) == 1: return b.T.dot(a)\n        else:                 return b.T.dot(a.T).T\n    else: b = np.asarray(b)\n    if len(b.shape) == 0: return a*b\n    if len(a.shape) == 1 and len(b.shape) == 2: return np.dot(b.T, a)\n    else: return np.dot(a,b)", "response": "inner function for sparse matrices"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cosine(x):\n    '''\n    cosine(x) is equivalent to cos(x) except that it also works on sparse arrays.\n    '''\n    # cos(0) = 1 so no point in keeping these sparse\n    if sps.issparse(x): x = x.toarray(x)\n    return np.cos(x)", "response": "cosine is equivalent to cos ( x ) except that it works on sparse arrays."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tangent(x, null=(-np.inf, np.inf), rtol=default_rtol, atol=default_atol):\n    '''\n    tangent(x) is equivalent to tan(x) except that it also works on sparse arrays.\n\n    The optional argument null (default, (-numpy.inf, numpy.inf)) may be specified to indicate what\n    value(s) should be assigned when x == -pi/2 or -pi/2. If only one number is given, then it is\n    used for both values; otherwise the first value corresponds to -pi/2 and the second to pi/2.\n    A value of x is considered to be equal to one of these valids based on numpy.isclose. The\n    optional arguments rtol and atol are passed along to isclose. If null is None, then no\n    replacement is performed.\n    '''\n    if sps.issparse(x):\n        x = x.copy()\n        x.data = tangent(x.data, null=null, rtol=rtol, atol=atol)\n        return x\n    else: x = np.asarray(x)\n    if rtol is None: rtol = default_rtol\n    if atol is None: atol = default_atol\n    try:    (nln,nlp) = null\n    except Exception: (nln,nlp) = (null,null)\n    x = np.mod(x + pi, tau) - pi\n    ii = None if nln is None else np.where(np.isclose(x, neghpi, rtol=rtol, atol=atol))\n    jj = None if nlp is None else np.where(np.isclose(x, hpi,    rtol=rtol, atol=atol))\n    x = np.tan(x)\n    if ii: x[ii] = nln\n    if jj: x[jj] = nlp\n    return x", "response": "tangent function for the is\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cotangent(x, null=(-np.inf, np.inf), rtol=default_rtol, atol=default_atol):\n    '''\n    cotangent(x) is equivalent to cot(x) except that it also works on sparse arrays.\n\n    The optional argument null (default, (-numpy.inf, numpy.inf)) may be specified to indicate what\n    value(s) should be assigned when x == 0 or pi. If only one number is given, then it is used for\n    both values; otherwise the first value corresponds to 0 and the second to pi.  A value of x is\n    considered to be equal to one of these valids based on numpy.isclose. The optional arguments\n    rtol and atol are passed along to isclose. If null is None, then no replacement is performed.\n    '''\n    if sps.issparse(x): x = x.toarray()\n    else:               x = np.asarray(x)\n    if rtol is None: rtol = default_rtol\n    if atol is None: atol = default_atol\n    try:    (nln,nlp) = null\n    except Exception: (nln,nlp) = (null,null)\n    x = np.mod(x + hpi, tau) - hpi\n    ii = None if nln is None else np.where(np.isclose(x, 0,  rtol=rtol, atol=atol))\n    jj = None if nlp is None else np.where(np.isclose(x, pi, rtol=rtol, atol=atol))\n    x = np.tan(x)\n    if ii: x[ii] = 1\n    if jj: x[jj] = 1\n    x = 1.0 / x\n    if ii: x[ii] = nln\n    if jj: x[jj] = nlp\n    return x", "response": "cotangent function for the non - sparse array x"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nflatters yields a numpy array equivalent to x but whose first dimension has been flattened. flatter(x, k) yields a numpy array whose first k dimensions have been flattened; if k is negative, the last k dimensions are flattened. If np.inf or -np.inf is passed, then this is equivalent to flattest(x). Note that flatter(x) is equivalent to flatter(x,1). flatter(x, 0) yields x.", "response": "def flatter(x, k=1):\n    '''\n    flatter(x) yields a numpy array equivalent to x but whose first dimension has been flattened.\n    flatter(x, k) yields a numpy array whose first k dimensions have been flattened; if k is\n      negative, the last k dimensions are flattened. If np.inf or -np.inf is passed, then this is\n      equivalent to flattest(x). Note that flatter(x) is equivalent to flatter(x,1).\n    flatter(x, 0) yields x.\n    '''\n    if k == 0: return x\n    x = x.toarray() if sps.issparse(x) else np.asarray(x)\n    if len(x.shape) - abs(k) < 2: return x.flatten()\n    k += np.sign(k)\n    if k > 0: return np.reshape(x, (-1,) + x.shape[k:])\n    else:     return np.reshape(x, x.shape[:k] + (-1,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparts returns a copy of x with the given indices.", "response": "def part(x, *args):\n    '''\n    part(x, ii, jj...) is equivalent to x[ii, jj...] if x is a sparse matrix or numpy array and is\n      equivalent to np.asarray(x)[ii][:, jj][...] if x is not. If only one argument is passed and\n      it is a tuple, then it is passed like x[ii] alone.\n\n    The part function is comparible with slices (though the must be entered using the slice(...)\n    rather than the : syntax) and Ellipsis.\n    '''\n    n = len(args)\n    sl = slice(None)\n    if sps.issparse(x):\n        if n == 1: return x[args[0]]\n        elif n > 2: raise ValueError('Too many indices for sparse matrix')\n        (ii,jj) = args\n        if   ii is Ellipsis: ii = sl\n        elif jj is Ellipsis: jj = sl\n        ni = pimms.is_number(ii)\n        nj = pimms.is_number(jj)\n        if   ni and nj: return x[ii,jj]\n        elif ni:        return x[ii,jj].toarray()[0]\n        elif nj:        return x[ii,jj].toarray()[:,0]\n        else:           return x[ii][:,jj]\n    else:\n        x = np.asarray(x)\n        if n == 1: return x[args[0]]\n        i0 = []\n        for (k,arg) in enumerate(args):\n            if arg is Ellipsis:\n                # special case...\n                #if Ellipsis in args[ii+1:]: raise ValueError('only one ellipsis allowed per part')\n                left = n - k - 1\n                i0 = [sl for _ in range(len(x.shape) - left)]\n            else:\n                x = x[tuple(i0 + [arg])]\n                if not pimms.is_number(arg): i0.append(sl)\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhacking a sequence of arrays.", "response": "def hstack(tup):\n    '''\n    hstack(x) is equivalent to numpy.hstack(x) or scipy.sparse.hstack(x) except that it works\n      correctly with both sparse and dense arrays (if any inputs are dense, it converts all inputs\n      to dense arrays).\n    '''\n    if all([sps.issparse(u) for u in tup]): return sps.hstack(tup, format=tup[0].format)\n    else: return np.hstack([u.toarray() if sps.issparse(u) else u for u in tup])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef repmat(x, r, c):\n    '''\n    repmat(x, r, c) is equivalent to numpy.matlib.repmat(x, r, c) except that it works correctly for\n      sparse matrices.\n    '''\n    if sps.issparse(x):\n        row = sps.hstack([x for _ in range(c)])\n        return sps.vstack([row for _ in range(r)], format=x.format)\n    else: return np.matlib.repmat(x, r, c)", "response": "repeatedly repmat a matrix of size r with size c"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef replace_close(x, xhat, rtol=default_rtol, atol=default_atol, copy=True):\n    '''\n    replace_close(x, xhat) yields x if x is not close to xhat and xhat otherwise. Closeness is\n      determined by numpy's isclose(), and the atol and rtol options are passed along.\n\n    The x and xhat arguments may be lists/arrays.\n\n    The optional argument copy may also be set to False to chop x in-place.\n    '''\n    if rtol is None: rtol = default_rtol\n    if atol is None: atol = default_atol\n    x = np.array(x) if copy else np.asarray(x)\n    w = np.isclose(x, xhat, rtol=rtol, atol=atol)\n    x[w] = np.asarray(xhat)[w]\n    return x", "response": "Replace close values in x with xhat."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chop(x, rtol=default_rtol, atol=default_atol, copy=True):\n    '''\n    chop(x) yields x if x is not close to round(x) and round(x) otherwise. Closeness is determined\n      by numpy's isclose(), and the atol and rtol options are passed along.\n\n    The x and xhat arguments may be lists/arrays.\n\n    The optional argument copy may also be set to False to chop x in-place.\n    '''\n    return replace_close(x, np.round(x), rtol=rtol, atol=atol, copy=copy)", "response": "Chops a sequence of numbers into a single number."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef library_path():\n    '''\n    library_path() yields the path of the neuropythy library.\n    '''\n    return os.path.abspath(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'lib'))", "response": "Returns the absolute path of the neuropythy library."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef curve_spline(x, y=None, weights=None, order=1, even_out=True,\n                 smoothing=None, periodic=False, meta_data=None):\n    '''\n    curve_spline(coords) yields a bicubic spline function through\n      the points in the given coordinate matrix.\n    curve_spline(x, y) uses the coordinate matrix [x,y].\n\n    The function returned by curve_spline() is f(t), defined on the\n    interval from 0 to n-1 where n is the number of points in the\n    coordinate matrix provided.\n    \n    The following options are accepted:\n      * weights (None) the weights to use in smoothing.\n      * smoothing (None) the amount to smooth the points.\n      * order (3) the order of the polynomial used in the splines.\n      * periodic (False) whether the points are periodic or not.\n      * even_out (True) whether to even out the distances along\n        the curve.\n      * meta_data (None) an optional map of meta-data to give the\n        spline representation.\n    '''\n    curv = CurveSpline(x,y, \n                       weights=weights, order=order,\n                       smoothing=smoothing, periodic=periodic,\n                       meta_data=meta_data)\n    if even_out: curv = curv.even_out()\n    return curv", "response": "Generates a bicubic spline function through the given coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_curve_spline(obj):\n    '''\n    to_curve_spline(obj) obj if obj is a curve spline and otherwise attempts to coerce obj into a\n      curve spline, raising an error if it cannot.\n    '''\n    if   is_curve_spline(obj):            return obj\n    elif is_tuple(obj) and len(obj) == 2: (crds,opts) = obj\n    else:                                 (crds,opts) = (obj,{})\n    if pimms.is_matrix(crds) or is_curve_spline(crds): crds = [crds]\n    spls = [c for c in crds if is_curve_spline(c)]\n    opts = dict(opts)\n    if 'weights' not in opts and len(spls) == len(crds):\n        if all(c.weights is not None for c in crds):\n            opts['weights'] = np.concatenate([c.weights for c in crds])\n    if 'order' not in opts and len(spls) > 0:\n        opts['order'] = np.min([c.order for c in spls])\n    if 'smoothing' not in opts and len(spls) > 0:\n        sm = set([c.smoothing for c in spls])\n        if len(sm) == 1: opts['smoothing'] = list(sm)[0]\n        else: opts['smoothing'] = None\n    crds = [x.crds if is_curve_spline(crds) else np.asarray(x) for x in crds]\n    crds = [x if x.shape[0] == 2 else x.T for x in crds]\n    crds = np.hstack(crds)\n    return curve_spline(crds, **opts)", "response": "Converts a single object into a curve spline."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef curve_intersection(c1, c2, grid=16):\n    '''\n    curve_intersect(c1, c2) yields the parametric distances (t1, t2) such that c1(t1) == c2(t2).\n      \n    The optional parameter grid may specify the number of grid-points\n    to use in the initial search for a start-point (default: 16).\n    '''\n    from scipy.optimize import minimize\n    from neuropythy.geometry import segment_intersection_2D\n    if c1.coordinates.shape[1] > c2.coordinates.shape[1]:\n        (t1,t2) = curve_intersection(c2, c1, grid=grid)\n        return (t2,t1)\n    # before doing a search, see if there are literal exact intersections of the segments\n    x1s  = c1.coordinates.T\n    x2s  = c2.coordinates\n    for (ts,te,xs,xe) in zip(c1.t[:-1], c1.t[1:], x1s[:-1], x1s[1:]):\n        pts = segment_intersection_2D((xs,xe), (x2s[:,:-1], x2s[:,1:]))\n        ii = np.where(np.isfinite(pts[0]))[0]\n        if len(ii) > 0:\n            ii = ii[0]\n            def f(t): return np.sum((c1(t[0]) - c2(t[1]))**2)\n            t01 = 0.5*(ts + te)\n            t02 = 0.5*(c2.t[ii] + c2.t[ii+1])\n            (t1,t2) = minimize(f, (t01, t02)).x\n            return (t1,t2)\n    if pimms.is_vector(grid): (ts1,ts2) = [c.t[0] + (c.t[-1] - c.t[0])*grid for c in (c1,c2)]\n    else:                     (ts1,ts2) = [np.linspace(c.t[0], c.t[-1], grid) for c in (c1,c2)]\n    (pts1,pts2) = [c(ts) for (c,ts) in zip([c1,c2],[ts1,ts2])]\n    ds = np.sqrt([np.sum((pts2.T - pp)**2, axis=1) for pp in pts1.T])\n    (ii,jj) = np.unravel_index(np.argmin(ds), ds.shape)\n    (t01,t02) = (ts1[ii], ts2[jj])\n    ttt = []\n    def f(t): return np.sum((c1(t[0]) - c2(t[1]))**2)\n    (t1,t2) = minimize(f, (t01, t02)).x\n    return (t1,t2)", "response": "Find the intersection of two curves."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a single curve that merges all of the given list of curves together.", "response": "def close_curves(*crvs, **kw):\n    '''\n    close_curves(crv1, crv2...) yields a single curve that merges all of the given list of curves\n      together. The curves must be given in order, such that the i'th curve should be connected to\n      to the (i+1)'th curve circularly to form a perimeter.\n\n    The following optional parameters may be given:\n      * grid may specify the number of grid-points to use in the initial search for a start-point\n        (default: 16).\n      * order may specify the order of the resulting curve; by default (None) uses the lowest order\n        of all curves.\n      * smoothing (None) the amount to smooth the points.\n      * even_out (True) whether to even out the distances along the curve.\n      * meta_data (None) an optional map of meta-data to give the spline representation.\n    '''\n    for k in six.iterkeys(kw):\n        if k not in close_curves.default_options: raise ValueError('Unrecognized option: %s' % k)\n    kw = {k:(kw[k] if k in kw else v) for (k,v) in six.iteritems(close_curves.default_options)}\n    (grid, order) = (kw['grid'], kw['order'])\n    crvs = [(crv if is_curve_spline(crv) else to_curve_spline(crv)).even_out() for crv in crvs]\n    # find all intersections:\n    isects = [curve_intersection(u,v, grid=grid)\n              for (u,v) in zip(crvs, np.roll(crvs,-1))]\n    # subsample curves\n    crds = np.hstack([crv.subcurve(s1[1], s0[0]).coordinates[:,:-1]\n                      for (crv,s0,s1) in zip(crvs, isects, np.roll(isects,1,0))])\n    kw['order'] = np.min([crv.order for crv in crvs]) if order is None else order\n    kw = {k:v for (k,v) in six.iteritems(kw)\n          if v is not None and k in ('order','smoothing','even_out','meta_data')}\n    return curve_spline(crds, periodic=True, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new DataStruct object that can be used to create a new mapping.", "response": "def data_struct(*args, **kw):\n    '''\n    data_struct(args...) collapses all arguments (which must be maps) and keyword arguments\n      right-to-left into a single mapping and uses this mapping to create a DataStruct object.\n    '''\n    m = pimms.merge(*args, **kw)\n    return DataStruct(**m)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a temporary directory and yields its path.", "response": "def tmpdir(prefix='npythy_tempdir_', delete=True):\n    '''\n    tmpdir() creates a temporary directory and yields its path. At python exit, the directory and\n      all of its contents are recursively deleted (so long as the the normal python exit process is\n      allowed to call the atexit handlers).\n    tmpdir(prefix) uses the given prefix in the tempfile.mkdtemp() call.\n    \n    The option delete may be set to False to specify that the tempdir should not be deleted on exit.\n    '''\n    path = tempfile.mkdtemp(prefix=prefix)\n    if not os.path.isdir(path): raise ValueError('Could not find or create temp directory')\n    if delete: atexit.register(shutil.rmtree, path)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of directories contained in the given path specification.", "response": "def dirpath_to_list(p):\n    '''\n    dirpath_to_list(path) yields a list of directories contained in the given path specification.\n\n    A path may be either a single directory name (==> [path]), a :-separated list of directories\n    (==> path.split(':')), a list of directory names (==> path), or None (==> []). Note that the\n    return value filters out parts of the path that are not directories.\n    '''\n    if   p is None: p = []\n    elif pimms.is_str(p): p = p.split(':')\n    if len(p) > 0 and not pimms.is_vector(p, str):\n        raise ValueError('Path is not equivalent to a list of dirs')\n    return [pp for pp in p if os.path.isdir(pp)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef finto(x, ii, n, null=0):\n    '''\n    finto(x,ii,n) yields a vector u of length n such that u[ii] = x.\n\n    Notes:\n      * The ii index may be a tuple (as can be passed to numpy arrays' getitem method) in order to\n        specify that the specific elements of a multidimensional output be set. In this case, the\n        argument n should be a tuple of sizes (a single integer is taken to be a square/cube/etc).\n      * x may be a sparse-array, but in it will be reified by this function.\n\n    The following optional arguments are allowed:\n      * null (defaut: 0) specifies the value that should appear in the elements of u that are not\n        set.\n    '''\n    x  = x.toarray() if sps.issparse(x) else np.asarray(x)\n    shx = x.shape\n    if isinstance(ii, tuple):\n        if not pimms.is_vector(n): n = tuple([n for _ in ii])\n        if len(n) != len(ii): raise ValueError('%d-dim index but %d-dim output' % (len(ii),len(n)))\n        sh = n + shx[1:]\n    elif pimms.is_int(ii): sh = (n,) + shx\n    else:                  sh = (n,) + shx[1:]\n    u = np.zeros(sh, dtype=x.dtype) if null == 0 else np.full(sh, null, dtype=x.dtype)\n    u[ii] = x\n    return u", "response": "f into x ii n yields a vector u of length n such that u [ ii ] = x."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef const_potential(f):\n    '''\n    const_potential(f) yields f if f is a constant potential function; if f is a constant, yields\n      a potential function that always yields f; otherwise raises an error.\n    '''\n    if is_const_potential(f): return f\n    elif pimms.is_array(f, 'number'): return PotentialConstant(f)\n    else: raise ValueError('Could not convert given value to potential constant: %s' % f)", "response": "Returns a potential constant for the given value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef const(f):\n    '''\n    const(f) yields f.c if f is a constant potential function; if f is a constant, it yields f or\n      the equivalent numpy array object; if f is a potential function that is not const, or is not\n      a valid potential function constant, yields None.\n    '''\n    if is_const_potential(f): return f.c\n    elif pimms.is_array(f, 'number'):\n        if sps.issparse(f): return f\n        else: return np.asarray(f)\n    else: return None", "response": "Returns a constant potential function or None if the potential function is not constant."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a single object to a potential function.", "response": "def to_potential(f):\n    '''\n    to_potential(f) yields f if f is a potential function; if f is not, but f can be converted to\n      a potential function, that conversion is performed then the result is yielded.\n    to_potential(Ellipsis) yields a potential function whose output is simply its input (i.e., the\n      identity function).\n    to_potential(None) is equivalent to to_potential(0).\n\n    The following can be converted into potential functions:\n      * Anything for which pimms.is_array(x, 'number') yields True (i.e., arrays of constants).\n      * Any tuple (g, h) where g(x) yields a potential value and h(x) yields a jacobian matrix for\n        the parameter vector x.\n    '''\n    if   is_potential(f): return f\n    elif f is Ellipsis:   return identity\n    elif pimms.is_array(f, 'number'): return const_potential(f)\n    elif isinstance(f, tuple) and len(f) == 2: return PotentialLambda(f[0], f[1])\n    else: raise ValueError('Could not convert object to potential function')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a potential function f that is the result of composing together all the passed arguments g h etc. after calling to_potential on each.", "response": "def compose(*args):\n    '''\n    compose(g, h...) yields a potential function f that is the result of composing together all the\n      arguments g, h, etc. after calling to_potential() on each. The result is defined such that\n      f(x) is equivalent to g(h(...(x))).\n    '''\n    return reduce(lambda h,g: PotentialComposition(g,h), reversed(list(map(to_potential, args))))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a potential part of a constant or constant potential u.", "response": "def part(f, ii):\n    '''\n    part(u, ii) for constant or constant potential u yields a constant-potential form of u[ii].\n    part(f, ii) for potential function f yields a potential function g(x) that is equivalent to\n      f(x[ii]).\n    '''\n    f = to_potential(f)\n    if is_const_potential(f): return PotentialConstant(f.c[ii])\n    else:                     return compose(PotentialPart(ii), to_potential(f))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sum(x, weights=None):\n    '''\n    sum(x) yields either a potential-sum object if x is a potential function or the sum of x if x\n      is not. If x is not a potential-field then it must be a vector.\n    sum(x, weights=w) uses the given weights to produce a weighted sum.\n    '''\n    x = to_potential(x)\n    if is_const_potential(x): return PotentialConstant(np.sum(x.c))\n    else:                     return PotentialSum(x, weights=weights)", "response": "Returns a potential - sum object if x is a potential function or a vector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dot product of two arrays.", "response": "def dot(a, b, ashape=None, bshape=None):\n    '''\n    dot(a,b) yields a potential function that represents the dot product of a and b.\n\n    Currently only vector and scalars are allowed.\n    '''\n    a = to_potential(a)\n    b = to_potential(b)\n    if is_const_potential(a) and is_const_potential(b): return PotentialConstant(np.dot(a.c, b.c))\n    else: return DotPotential(a, b, g_shape=ashape, h_shape=bshape)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a potential function h that calculates the vector norms of the rows of .", "response": "def row_norms(ii, f=Ellipsis, squared=False):\n    '''\n    row_norms(ii) yields a potential function h(x) that calculates the vector norms of the rows of\n      the matrix formed by [x[i] for i in ii] (ii is a matrix of parameter indices).\n    row_norms(ii, f) yield a potential function h(x) equivalent to compose(row_norms(ii), f).\n    '''\n    try:\n        (n,m) = ii\n        # matrix shape given\n        ii = np.reshape(np.arange(n*m), (n,m))\n    except Exception: ii = np.asarray(ii)\n    f = to_potential(f)\n    if is_const_potential(f):\n        q = flattest(f.c)\n        q = np.sum([q[i]**2 for i in ii.T], axis=0)\n        return PotentialConstant(q if squared else np.sqrt(q))\n    F = reduce(lambda a,b: a + b, [part(Ellipsis, col)**2 for col in ii.T])\n    F = compose(F, f)\n    if not squared: F = sqrt(F)\n    return F"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a potential function that computes the distance between two nodes.", "response": "def distances(a, b, shape, squared=False, axis=1):\n    '''\n    distances(a, b, (n,d)) yields a potential function whose output is equivalent to the row-norms\n      of reshape(a(x), (n,d)) - reshape(b(x), (n,d)).\n    \n    The shape argument (n,m) may alternately be a matrix of parameter indices, as can be passed to\n    row_norms and col_norms.\n\n    The following optional arguments are accepted:\n      * squared (default: False) specifies whether the output should be the square distance or the\n        distance.\n      * axis (default: 1) specifies whether the rows (axis = 1) or columns (axis = 0) are treated\n        as the vectors between which the distances should be calculated.\n    '''\n    a = to_potential(a)\n    b = to_potential(b)\n    if axis == 1: return row_norms(shape, a - b, squared=squared)\n    else:         return col_norms(shape, a - b, squared=squared)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a cosine well of the given frequency.", "response": "def cos_well(f=Ellipsis, width=np.pi/2, offset=0, scale=1):\n    '''\n    cos_well() yields a potential function g(x) that calculates 0.5*(1 - cos(x)) for -pi/2 <= x\n      <= pi/2 and is 1 outside of that range.\n    \n    The full formulat of the cosine well is, including optional arguments:\n      scale / 2 * (1 - cos((x - offset) / (width/pi)))\n\n    The following optional arguments may be given:\n      * width (default: pi) specifies that the frequency of the cos-curve should be pi/width; the\n        width is the distance between the points on the cos-curve with the value of 1.\n      * offset (default: 0) specifies the offset of the minimum value of the coine curve on the\n        x-axis.\n      * scale (default: 1) specifies the height of the cosine well.\n    '''\n    f = to_potential(f)\n    freq = np.pi/width*2\n    (xmn,xmx) = (offset - width/2, offset + width/2)\n    F = piecewise(scale, ((xmn,xmx), scale/2 * (1 - cos(freq * (identity - offset)))))\n    if   is_const_potential(f):    return const_potential(F.value(f.c))\n    elif is_identity_potential(f): return F\n    else:                          return compose(F, f)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a potential function f that calculates a Gaussian function over x.", "response": "def gaussian(f=Ellipsis, mu=0, sigma=1, scale=1, invert=False, normalize=False):\n    '''\n    gaussian() yields a potential function f(x) that calculates a Gaussian function over x; the\n      formula used is given below.\n    gaussian(g) yields a function h(x) such that, if f(x) is yielded by gaussian(), h(x) = f(g(x)).\n\n    The formula employed by the Gaussian function is as follows, with mu, sigma, and scale all being\n    parameters that one can provide via optional arguments:\n      scale * exp(0.5 * ((x - mu) / sigma)**2)\n    \n    The following optional arguments may be given:\n      * mu (default: 0) specifies the mean of the Gaussian.\n      * sigma (default: 1) specifies the standard deviation (sigma) parameger of the Gaussian.\n      * scale (default: 1) specifies the scale to use.\n      * invert (default: False) specifies whether the Gaussian should be inverted. If inverted, then\n        the formula, scale * exp(...), is replaced with scale * (1 - exp(...)).\n      * normalize (default: False) specifies whether the result should be multiplied by the inverse\n        of the area under the uninverted and unscaled curve; i.e., if normalize is True, the entire\n        result is multiplied by 1/sqrt(2*pi*sigma**2).\n    '''\n    f = to_potential(f)\n    F = exp(-0.5 * ((f - mu) / sigma)**2)\n    if invert: F = 1 - F\n    F = F * scale\n    if normalize: F = F / (np.sqrt(2.0*np.pi) * sigma)\n    return F"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a potential function that calculates the error function over the input x.", "response": "def erf(f=Ellipsis):\n    '''\n    erf(x) yields a potential function that calculates the error function over the input x. If x is\n      a constant, yields a constant potential function.\n    erf() is equivalent to erf(...), which is just the error function, calculated over its inputs.\n    '''\n    f = to_potential(f)\n    if is_const_potential(f): return const_potential(np.erf(f.c))\n    elif is_identity_potential(f): return ErfPotential()\n    else: return compose(ErfPotential(), f)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a sigmoid potential function that is equivalent to the integral of gaussian function i. e. the error function with scaled scale and normalize.", "response": "def sigmoid(f=Ellipsis, mu=0, sigma=1, scale=1, invert=False, normalize=False):\n    '''\n    sigmoid() yields a potential function that is equivalent to the integral of gaussian(), i.e.,\n      the error function, but scaled to match gaussian().\n    sigmoid(f) is equivalent to compose(sigmoid(), f).\n\n    All options that are accepted by the gaussian() function are accepted by sigmoid() with the same\n    default values and are handled in an equivalent manner with the exception of the invert option;\n    when a sigmoid is inverted, the function approaches its maximum value at -inf and approaches 0\n    at inf.\n\n    Note that because sigmoid() explicitly matches gaussian(), the base formula used is as follows:\n      f(x) = scale * sigma * sqrt(pi/2) * erf((x - mu) / (sqrt(2) * sigma))\n      k*sig*Sqrt[Pi/2] Erf[(x - mu)/sig/Sqrt[2]]\n    '''\n    f = to_potential(f)\n    F = erf((f - mu) / (sigma * np.sqrt(2.0)))\n    if invert: F = 1 - F\n    F = np.sqrt(np.pi / 2) * scale * F\n    if normalize: F = F / (np.sqrt(2.0*np.pi) * sigma)\n    return F"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a potential function equivalent to the absolute value of the input.", "response": "def abs(f=Ellipsis):\n    '''\n    abs() yields a potential function equivalent to the absolute value of the input.\n    abs(f) yields the absolute value of the potential function f.\n\n    Note that abs has a derivative of 0 at 0; this is not mathematically correct, but it is useful\n    for the purposes of numerical methods. If you want traditional behavior, it is suggested that\n    one instead employ sqrt(f**2).\n    '''\n    f = to_potential(f)\n    if is_const_potential(f): return const_potential(np.abs(f.c))\n    elif is_identity_potential(f): return AbsPotential()\n    else: return compose(AbsPotential(), f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a potential function equivalent to the sign of the input.", "response": "def sign(f=Ellipsis):\n    '''\n    sign() yields a potential function equivalent to the sign of the input.\n    sign(f) yields the sign of the potential function f.\n\n    Note that sign has a derivative of 0 at all points; this is not mathematically correct, but it is\n    useful for the purposes of numerical methods. If you want traditional behavior, it is suggested\n    that one instead employ f/sqrt(f**2).\n    '''\n    f = to_potential(f)\n    if is_const_potential(f): return const_potential(np.sign(f.c))\n    elif is_identity_potential(f): return SignPotential()\n    else: return compose(SignPotential(), f)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a potential function f that calculates the signed area of each face represented by the simplices matrix faces.", "response": "def signed_face_areas(faces, axis=1):\n    '''\n    signed_face_areas(faces) yields a potential function f(x) that calculates the signed area of\n      each face represented by the simplices matrix faces.\n\n    If faces is None, then the parameters must arrive in the form of a flattened (n x 3 x 2) matrix\n    where n is the number of triangles. Otherwise, the faces matrix must be either (n x 3) or (n x 3\n    x s); if the former, each row must list the vertex indices for the faces where the vertex matrix\n    is presumed to be shaped (V x 2). Alternately, faces may be a full (n x 3 x 2) simplex array of\n    the indices into the parameters.\n\n    The optional argument axis (default: 1) may be set to 0 if the faces argument is a matrix but\n    the coordinate matrix will be (2 x V) instead of (V x 2).\n    '''\n    faces = np.asarray(faces)\n    if len(faces.shape) == 2:\n        if faces.shape[1] != 3: faces = faces.T\n        n = 2 * (np.max(faces) + 1)\n        if axis == 0: tmp = np.reshape(np.arange(n), (2,-1)).T\n        else:         tmp = np.reshape(np.arange(n), (-1,2))\n        faces = np.reshape(tmp[faces.flat], (-1,3,2))\n    faces = faces.flatten()\n    return compose(TriangleSignedArea2DPotential(), part(Ellipsis, faces))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a list of colors to rgba values.", "response": "def to_rgba(val):\n    '''\n    to_rgba(val) is identical to matplotlib.colors.to_rgba(val) except that it operates over lists\n      as well as individual elements to yield matrices of rgba values. In addition, it always yields\n      numpy vectors or matrices.\n    '''\n    if pimms.is_npmatrix(val) and val.shape[1] == 4: return val\n    try: return np.asarray(matplotlib.colors.to_rgba(val))\n    except Exception: return np.asarray([matplotlib.colors.to_rgba(u) for u in val])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef color_overlap(color1, *args):\n    '''\n    color_overlap(color1, color2...) yields the rgba value associated with overlaying color2 on top\n      of color1 followed by any additional colors (overlaid left to right). This respects alpha\n      values when calculating the results.\n    Note that colors may be lists of colors, in which case a matrix of RGBA values is yielded.\n    '''\n    args = list(args)\n    args.insert(0, color1)\n    rgba = np.asarray([0.5,0.5,0.5,0])\n    for c in args:\n        c = to_rgba(c)\n        a = c[...,3]\n        a0 = rgba[...,3]\n        if   np.isclose(a0, 0).all(): rgba = np.ones(rgba.shape) * c\n        elif np.isclose(a,  0).all(): continue\n        else:                         rgba = times(a, c) + times(1-a, rgba)\n    return rgba", "response": "color_overlap yields the rgba value associated with overlaying color1 on top\n      of color2 followed by any additional colors overlaid left to right."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsees eccen_colors angle_colors sigma_colors and varea_colors.", "response": "def retino_colors(vcolorfn, *args, **kwargs):\n    'See eccen_colors, angle_colors, sigma_colors, and varea_colors.'\n    if len(args) == 0:\n        def _retino_color_pass(*args, **new_kwargs):\n            return retino_colors(vcolorfn, *args,\n                                 **{k:(new_kwargs[k] if k in new_kwargs else kwargs[k])\n                                    for k in set(kwargs.keys() + new_kwargs.keys())})\n        return _retino_color_pass\n    elif len(args) > 1:\n        raise ValueError('retinotopy color functions accepts at most one argument')\n    m = args[0]\n    # we need to handle the arguments\n    if isinstance(m, (geo.VertexSet, pimms.ITable)):\n        tbl = m.properties if isinstance(m, geo.VertexSet) else m\n        n = tbl.row_count\n        # if the weight or property arguments are lists, we need to thread these along\n        if 'property' in kwargs:\n            props = kwargs['property']\n            del kwargs['property']\n            if not (pimms.is_vector(props) or pimms.is_matrix(props)):\n                props = [props for _ in range(n)]\n        else: props = None\n        if 'weight' in kwargs:\n            ws = kwargs['weight']\n            del kwargs['weight']\n            if not pimms.is_vector(ws) and not pimms.is_matrix(ws): ws = [ws for _ in range(n)]\n        else: ws = None\n        vcolorfn0 = vcolorfn(Ellipsis, **kwargs) if len(kwargs) > 0 else vcolorfn\n        if props is None and ws is None: vcfn = lambda m,k:vcolorfn0(m)\n        elif props is None:              vcfn = lambda m,k:vcolorfn0(m, weight=ws[k])\n        elif ws is None:                 vcfn = lambda m,k:vcolorfn0(m, property=props[k])\n        else: vcfn = lambda m,k:vcolorfn0(m, property=props[k], weight=ws[k])\n        return np.asarray([vcfn(r,kk) for (kk,r) in enumerate(tbl.rows)])\n    else:\n        return vcolorfn(m, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef guess_cortex_cmap(pname):\n    '''\n    guess_cortex_cmap(proptery_name) yields a tuple (cmap, (vmin, vmax)) of a cortical color map\n      appropriate to the given property name and the suggested value scaling for the cmap. If the\n      given property is not a string or is not recognized then the log_eccentricity axis is used\n      and the suggested vmin and vmax are None.\n    '''\n    if not pimms.is_str(pname): return (log_eccentricity, None, None)\n    if pname in colormaps: return colormaps[pname]\n    # check each manually\n    for (k,v) in six.iteritems(colormaps):\n        if pname.endswith(k): return v\n    for (k,v) in six.iteritems(colormaps):\n        if pname.startswith(k): return v\n    return (log_eccentricity, None, None)", "response": "Guess the cortical color map corresponding to the given property name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_cmap(zs, cmap, vmin=None, vmax=None):\n    '''\n    apply_cmap(z, cmap) applies the given cmap to the values in z; if vmin and/or vmad are passed,\n      they are used to scale z.\n    '''\n    if vmin is None: vmin = np.min(zs)\n    if vmax is None: vmax = np.max(zs)\n    if pimms.is_str(cmap): cmap = matplotlib.cm.get_cmap(cmap)\n    return cmap((zs - vmin) / (vmax - vmin))", "response": "Apply a colormap to the values in z."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trace_roi(hemi, map_proj, closed=True, event_handlers=None, alternate_plots=None, **kw):\n    '''\n    trace_roi(hemi, map_proj) yields an ROIDrawer object that controls the tracing of lines around\n      an ROI in a 2D matplotlib plot. After the ROI has been drawn in the plot, the resulting path\n      trace can be obtained via the roi_drawer.trace object.\n\n    ROI tracing is very simple: any point in the plot is appended to the path as it is clicked; in\n    order to eliminate the previous point, hold shift while clicking. To end the path, hold control\n    while clicking. To abort the path, hold both shift and control while clicking. (Double-clicking\n    should be equivalent to control-clicking, but this does not work in all setups.) In order to use\n    the ROI tracing, `%matplotlib notebook` is recommended.\n\n    The trace_roi() function accepts all options that can be passed to cortex_plot() as well as the\n    following options:\n      * closed (default: True) specifies whether the path-trace that is constructed should be closed\n        (True) or open (False).\n      * event_handlers (default: None) specifies additional event handlers (named by key) for the\n        ROIDrawer().\n      * alternate_plots (default: None) specifies a list of alternate plotting functions that must\n        accept the axes as an argument (these are cycled with tab-click). Alternately, these may be\n        matplotlib TriMesh objects that are simply made visible.\n    '''\n    if event_handlers is None: event_handlers = {}\n    if alternate_plots is None: alternate_plots = []\n    # if if there are alternate plots, go ahead and set them up:\n    if alternate_plots:\n        pass", "response": "A function that creates a trace object for a given ROI."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a path to the list of subject directories in which to search for a new subject.", "response": "def add_subject_path(path, index=None):\n    '''\n    add_subject_path(path) will add the given path to the list of subject directories in which to\n      search for HCP subjects. The optional argument index may be given to specify the precedence of\n      this path when searching for a new subject; the default, 0, always inserts the path at the\n      front of the list; a value of k indicates that the new list should have the new path at index\n      k.\n    The path may contain :'s, in which case the individual directories are separated and added.  If\n    the given path is not a directory or the path could not be inserted, yields False; otherwise,\n    yields True. If the string contains a : and multiple paths, then True is yielded only if all\n    paths were successfully inserted.  See also subject_paths.\n    '''\n    paths = [p for p in path.split(':') if len(p) > 0]\n    if len(paths) > 1:\n        tests = [add_subject_path(p, index=index) for p in reversed(paths)]\n        return all(t for t in tests)\n    else:\n        spaths = config['hcp_subject_paths']\n        path = os.path.expanduser(path)\n        if not os.path.isdir(path): return False\n        if path in spaths: return True\n        try:\n            if index is None or index is Ellipsis:\n                sd = spaths + [path]\n            else:\n                sd = spaths + []\n                sd.insert(index, path)\n            config['hcp_subject_paths'] = sd\n            return True\n        except Exception:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_subject_path(sid):\n    '''\n    find_subject_path(sub) yields the full path of a HCP subject with the name given by the string\n      sub, if such a subject can be found in the HCP search paths. See also add_subject_path.\n\n    If no subject is found, then None is returned.\n    '''\n    # if it's a full/relative path already, use it:\n    sub = str(sid)\n    sdirs = config['hcp_subject_paths']\n    if _auto_download_options is not None and 'subjects_path' in _auto_download_options:\n        sdirs = list(sdirs) + [_auto_download_options['subjects_path']]\n    if os.path.isdir(sub): return os.path.abspath(sub)\n    pth = next((os.path.abspath(p) for sd in sdirs\n                for p in [os.path.join(sd, sub)]\n                if os.path.isdir(p)),\n               None)\n    if pth is not None: return pth\n    # see if we can create them\n    if _auto_download_options is None or not _auto_download_options['structure'] or \\\n       not _auto_downloadable(sid):\n        return None\n    # first see if the subject existst there\n    pth = os.path.join(_auto_download_options['subjects_path'], sub)\n    if os.path.isdir(pth): return pth\n    try: os.makedirs(os.path.abspath(pth), 0o755)\n    except Exception: return None\n    return pth", "response": "Find the full path of a HCP subject with the given sid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_subject_id(s):\n    '''\n    to_subject_id(s) coerces the given string or number into an integer subject id. If s is not a\n      valid subejct id, raises an exception.\n    '''\n    if not pimms.is_number(s) and not pimms.is_str(s):\n        raise ValueError('invalid type for subject id: %s' % str(type(s)))\n    if pimms.is_str(s):\n        try: s = os.path.expanduser(s)\n        except Exception: pass\n        if os.path.isdir(s): s = s.split(os.sep)[-1]\n    s = int(s)\n    if s > 999999 or s < 100000:\n        raise ValueError('subject ids must be 6-digit integers whose first digit is > 0')\n    return s", "response": "Coerces the given string or number into an integer subject id."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gifti_to_array(gii):\n    '''\n    gifti_to_array(gii) yields the squeezed array of data contained in the given gifti object, gii,\n      Note that if gii does not contain simple data in its darray object, then this will produce\n      undefined results. This operation is effectively equivalent to:\n      np.squeeze([x.data for x in gii.darrays]).\n    gifti_to_array(gii_filename) is equivalent to gifti_to_array(neyropythy.load(gii_filename)).\n    '''\n    if pimms.is_str(gii): return gifti_to_array(ny.load(gii, 'gifti'))\n    elif pimms.is_nparray(gii): return gii #already done\n    elif isinstance(gii, nib.gifti.gifti.GiftiImage):\n        return np.squeeze(np.asarray([x.data for x in gii.darrays]))\n    else: raise ValueError('Could not understand argument to gifti_to_array')", "response": "Converts a gifti object into a squeezed array of data contained in it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_fsLR_atlasroi(filename, data):\n    '''\n    Loads the appropriate atlas for the given data; data may point to a cifti file whose atlas is\n    needed or to an atlas file.\n    '''\n    (fdir, fnm) = os.path.split(filename)\n    fparts = fnm.split('.')\n    atl = fparts[-3]\n    if atl in _load_fsLR_atlasroi.atlases: return _load_fsLR_atlasroi.atlases[atl]\n    sid = data['id']\n    fnm = [os.path.join(fdir, '%d.%s.atlasroi.%s.shape.gii' % (sid, h, atl))  for h in ('L', 'R')]\n    if data['cifti']:\n        dat = [{'id':data['id'], 'type':'property', 'name':'atlas', 'hemi':h} for h in data['hemi']]\n    else:\n        dat = [{'id':data['id'], 'type':'property', 'name':'atlas', 'hemi':(h + data['hemi'][2:])}\n               for h in ('lh','rh')]\n    # loading an atlas file; this is easier\n    rois = tuple([_load(f, d).astype('bool') for (f,d) in zip(fnm, dat)])\n    # add these to the cache\n    if atl != 'native': _load_fsLR_atlasroi.atlases[atl] = rois\n    return rois", "response": "Loads the appropriate atlas for the given data ; data may point to a cifti file whose atlas is\n    needed or to an atlas file whose atlas is\n    needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_fsLR_atlasroi_for_size(size, sid=100610):\n    '''\n    Loads the appropriate atlas for the given size of data; size should be the number of stored\n    vertices and sub-corticel voxels stored in the cifti file.\n    '''\n    from .core import subject\n    # it doesn't matter what subject we request, so just use any one\n    fls = _load_fsLR_atlasroi_for_size.sizes\n    if size not in fls: raise ValueError('unknown fs_LR atlas size: %s' % size)\n    (n,fls) = _load_fsLR_atlasroi_for_size.sizes[size]\n    fl = os.path.join(subject(sid).path, 'MNINonLinear', *fls)\n    dat = {'id':sid, 'cifti':True, 'hemi':('lh_LR%dk_MSMAll' % n ,'rh_LR%dk_MSMAll' % n)}\n    return _load_fsLR_atlasroi(fl, dat)", "response": "Loads the appropriate atlas for the given size of data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading the data for a given subject id.", "response": "def download(sid, credentials=None, subjects_path=None, overwrite=False, release='HCP_1200',\n             database='hcp-openaccess', file_list=None):\n    '''\n    download(sid) downloads the data for subject with the given subject id. By default, the subject\n      will be placed in the first HCP subject directory in the subjects directories list.\n\n    Note: In order for downloading to work, you must have s3fs installed. This is not a requirement\n    for the neuropythy library and does not install automatically when installing via pip. The\n    github repository for this library can be found at https://github.com/dask/s3fs. Installation\n    instructions can be found here: http://s3fs.readthedocs.io/en/latest/install.html\n\n    Accepted options include:\n      * credentials (default: None) may be used to specify the Amazon AWS Bucket credentials, which\n        can be generated from the HCP db (https://db.humanconnectome.org/). If this argument can be\n        coerced to a credentials tuple via the to_credentials function, that result will be used. If\n        None, then the function will try to use the hcp_credentials configuration item in\n        neuropythy.config; otherwise an error is raised.\n      * subjects_path (default: None) specifies where the subject should be placed. If None, then\n        the first directory in the subjects paths list is used. If there is not one of these then\n        an error is raised.\n      * overwrite (default: False) specifies whether or not to overwrite files that already exist.\n        In addition to True (do overwrite) and False (don't overwrite), the value 'error' indicates\n        that an error should be raised if a file already exists.\n    '''\n    if s3fs is None:\n        raise RuntimeError('s3fs was not successfully loaded, so downloads may not occur; check '\n                           'your Python configuration to make sure that s3fs is installed. See '\n                           'http://s3fs.readthedocs.io/en/latest/install.html for details.')\n    if credentials is None: credentials = config['hcp_credentials']\n    if credentials is None: raise ValueError('No hcp_credentials specified or found')\n    (s3fs_key, s3fs_secret) = to_credentials(credentials)\n    if subjects_path is None:\n        sdirs = config['hcp_subject_paths']\n        subjects_path = next((sd for sd in sdirs if os.path.isdir(sd)), None)\n        if subjects_path is None: raise ValueError('No subjects path given or found')\n    else: subjects_path = os.path.expanduser(subjects_path)\n    # Make sure we can connect to the bucket first...\n    fs = s3fs.S3FileSystem(key=s3fs_key, secret=s3fs_secret)\n    # Okay, make sure the release is found\n    if not fs.exists('/'.join([database, release])):\n        raise ValueError('database/release (%s/%s) not found' % (database, release))\n    # Check on the subject id to\n    sid = to_subject_id(sid)\n    hcp_sdir = '/'.join([database, release, str(sid)])\n    if not fs.exists(hcp_sdir): raise ValueError('Subject %d not found in release' % sid)\n    # Okay, lets download this subject!\n    loc_sdir = os.path.join(subjects_path, str(sid))\n    # walk through the subject structures\n    pulled = []\n    for flnm in six.iterkeys(subject_structure['filemap']):\n        flnm = flnm.format({'id':sid})\n        loc_flnm = os.path.join(loc_sdir, flnm)\n        hcp_flnm = '/'.join([hcp_sdir, flnm])\n        if not overwrite and os.path.isfile(loc_flnm): continue\n        # gotta download it!\n        basedir = os.path.split(loc_flnm)[0]\n        if not os.path.isdir(basedir): os.makedirs(os.path.abspath(basedir), 0o755)\n        fs.get(hcp_flnm, loc_flnm)\n        pulled.append(loc_flnm)\n    return pulled"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_retinotopy_cache(sdir, sid, alignment='MSMAll'):\n    '''\n    Returns the subject's retinotopy cache as a lazy map, or None if no cache exists. The hemi keys\n    in the returned map will spell out the cortex name (e.g., lh_native_MSMAll or rh_LR32k_MSMAll).\n    '''\n    fs = {h:{(k,kk):os.path.join(sdir, 'retinotopy', v % (h, alignment))\n             for (kk,vv) in six.iteritems(_retinotopy_cache_tr)\n             for (k,v)   in six.iteritems(vv)}\n          for h in ('lh','rh')}\n    files = {}\n    for h in ['lh','rh']:\n        for ((k,kk),v) in six.iteritems(fs[h]):\n            if not os.path.isfile(v): continue\n            hh = _retinotopy_cache_surf_tr[v.split('.')[-2]] % (h, alignment)\n            if hh not in files: files[hh] = {}\n            files[hh][k] = v\n    if len(files) == 0: return files\n    def _loader(fls, h, k): return (lambda:nyio.load(fls[h][k]))\n    return {h:pimms.lazy_map({k:_loader(files, h, k) for k in six.iterkeys(v)})\n            for (h,v) in six.iteritems(files)}", "response": "Load the subject s retinotopy cache as a lazy map."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_retinotopy_cache(sdir, sid, hemi, props, alignment='MSMAll', overwrite=False):\n    '''\n    Saves the subject's retinotopy cache from the given properties. The first argument is the\n    subject's directory (not the subjects' directory).\n    '''\n    h = hemi[:2]\n    htype = hemi.split('_')[1]\n    if _auto_download_options is None \\\n       or 'retinotopy_cache' not in _auto_download_options \\\n       or not _auto_download_options['retinotopy_cache']:\n        return\n    files = {k:os.path.join(sdir, 'retinotopy', v % (h, alignment))\n             for (k,v) in six.iteritems(_retinotopy_cache_tr[htype])}\n    for (p,fl) in six.iteritems(files):\n        if p not in props or (not overwrite and os.path.exists(fl)): continue\n        p = np.asarray(props[p])\n        if np.issubdtype(p.dtype, np.floating): p = np.asarray(p, np.float32)\n        dr = os.path.split(os.path.abspath(fl))[0]\n        if not os.path.isdir(dr): os.makedirs(os.path.abspath(dr), 0o755)\n        nyio.save(fl, p)", "response": "Saves the subject s retinotopy cache from the given properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subject_filemap(sid, subject_path=None):\n    '''\n    subject_filemap(sid) yields a persistent lazy map structure that loads the relevant files as\n      requested for the given subject. The sid may be a subject id or the path of a subject\n      directory. If a subject id is given, then the subject is searched for in the known subject\n      paths.\n\n    The optional argument subject_path may be set to a specific path to ensure that the subject\n    id is only searched for in the given path.\n    '''\n    # see if sid is a subject id\n    if pimms.is_int(sid):\n        if subject_path is None: sdir = find_subject_path(sid)\n        else: sdir = os.path.expanduser(os.path.join(subject_path, str(sid)))\n    elif pimms.is_str(sid):\n        try: sid = os.path.expanduser(sid)\n        except Exception: pass\n        sdir = sid if os.path.isdir(sid) else find_subject_path(sid)\n        sid  = int(sdir.split(os.sep)[-1])\n    else: raise ValueError('Cannot understand HCP subject ID %s' % sid)\n    if sdir is None:\n        if _auto_download_options is not None and _auto_download_options['structure'] \\\n           and _auto_downloadable(sid):\n            # we didn't find it, but we have a place to put it\n            sdir = _auto_download_options['subjects_path']\n            sdir = os.path.join(sdir, str(sid))\n            if not os.path.isdir(sdir): os.makedirs(os.path.abspath(sdir), 0o755)\n        else:\n            raise ValueError('Could not find HCP subject %s' % sid)\n    ff = {'id':sid}\n    def _make_lambda(flnm, dat): return lambda:_load(flnm, dat)\n    # walk through the subject structure's filemap to make a lazy map that loads things\n    dats = {}\n    fls = {}\n    for (k,v) in six.iteritems(subject_structure['filemap']):\n        flnm = os.path.join(sdir, k.format(ff))\n        dat  = pimms.merge(v, ff)\n        fls[flnm]  = _make_lambda(flnm, dat)\n        dats[flnm] = dat\n    fls = pimms.lazy_map(fls)\n    # and the the hemispheres to make hemispheres...\n    def _lookup_fl(flnm):\n        def _f():\n            obj = fls[flnm]\n            dat = dats[flnm]\n            if 'cifti' in dat and dat['cifti']:\n                # we need to handle the cifti files by splitting them up according to atlases\n                (ldat,rdat) = _cifti_to_hemis(np.asarray(obj), sid)\n                obj = ldat if dat['hemi'][0:2] == 'lh' else rdat\n                obj.setflags(write=False)\n            return obj\n        return _f\n    hems = pyr.pmap(\n        {h: pyr.pmap(\n            {k: pimms.lazy_map({nm: _lookup_fl(fnm)\n                                for (nm,fl) in six.iteritems(v)\n                                for fnm in [os.path.join(sdir, fl.format(ff))]})\n             for (k,v) in six.iteritems(entries)})\n         for (h,entries) in six.iteritems(subject_structure['hemis'])})\n    # and the images\n    imgs = pimms.lazy_map({k: _lookup_fl(os.path.join(sdir, v.format(ff)))\n                           for (k,v) in six.iteritems(subject_structure['images'])})\n    # and retinotopy if appropriate\n    for size in [32, 59]:\n        retsubs = _retinotopy_submap(size=size)\n        if retsubs is not None and sid in retsubs:\n            def _make_loader(sz):\n                rp = retinotopy_prefix if sz == 59 else lowres_retinotopy_prefix\n                ldr = pimms.lazy_map({0:lambda:_retinotopy_data('full', sid, size=sz)})\n                return pyr.pmap(\n                    {('lh_LR%dk_MSMAll' % sz):pimms.lazy_map(\n                        {(rp+'_polar_angle'):lambda:ldr[0]['prf_polar_angle'][0],\n                         (rp+'_eccentricity'):lambda:ldr[0]['prf_eccentricity'][0],\n                         (rp+'_radius'):lambda:ldr[0]['prf_radius'][0],\n                         (rp+'_variance_explained'):lambda:ldr[0]['prf_variance_explained'][0]}),\n                    ('rh_LR%dk_MSMAll' % sz):pimms.lazy_map(\n                        {(rp+'_polar_angle'):lambda:ldr[0]['prf_polar_angle'][1],\n                         (rp+'_eccentricity'):lambda:ldr[0]['prf_eccentricity'][1],\n                         (rp+'_radius'):lambda:ldr[0]['prf_radius'][1],\n                         (rp+'_variance_explained'):lambda:ldr[0]['prf_variance_explained'][1]})})\n            ret = _make_loader(size)\n            # merge into the hemis\n            for h in six.iterkeys(ret):\n                hdat = hems[h]\n                prop = pimms.merge(hdat['properties'], ret[h])\n                hems = hems.set(h, hdat.set('properties', prop))\n    return pyr.pmap({'images': imgs, 'hemis': hems})", "response": "Returns a persistent lazy map structure that loads the relevant files as\n      requested for the given subject."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calc_arguments(args):\n    '''\n    calc_arguments is a calculator that parses the command-line arguments for the registration\n    command and produces the subject, the model, the log function, and the additional options.\n    '''\n    (args, opts) = _retinotopy_parser(args)\n    # We do some of the options right here...\n    if opts['help']:\n        print(info, file=sys.stdout)\n        sys.exit(1)\n    # and if we are verbose, lets setup a note function\n    verbose = opts['verbose']\n    def note(s):\n        if verbose:\n            print(s, file=sys.stdout)\n            sys.stdout.flush()\n        return verbose\n    def error(s):\n        print(s, file=sys.stderr)\n        sys.stderr.flush()\n        sys.exit(1)\n    if len(args) < 1: error('subject argument is required')\n    # Add the subjects directory, if there is one\n    if 'subjects_dir' in opts and opts['subjects_dir'] is not None:\n        add_subject_path(opts['subjects_dir'])\n    # Get the subject now\n    try: sub = subject(args[0])\n    except Exception: error('Failed to load subject %s' % args[0])\n    # and the model\n    if len(args) > 1:       mdl_name = args[1]\n    elif opts['model_sym']: mdl_name = 'schira'\n    else:                   mdl_name = 'benson17'\n    try:\n        if opts['model_sym']:\n            model = {h:retinotopy_model(mdl_name).persist() for h in ['lh', 'rh']}\n        else:\n            model = {h:retinotopy_model(mdl_name, hemi=h).persist() for h in ['lh', 'rh']}\n    except Exception: error('Could not load retinotopy model %s' % mdl_name)\n\n    # Now, we want to run a few filters on the options\n    # Parse the simple numbers\n    for o in ['weight_min', 'scale', 'max_step_size', 'max_out_eccen',\n              'max_in_eccen', 'min_in_eccen', 'field_sign_weight', 'radius_weight']:\n        opts[o] = float(opts[o])\n    opts['max_steps'] = int(opts['max_steps'])\n    # Make a note:\n    note('Processing subject: %s' % sub.name)\n    del opts['help']\n    del opts['verbose']\n    del opts['subjects_dir']\n    # That's all we need!\n    return pimms.merge(opts,\n                       {'subject': sub.persist(),\n                        'model':   pyr.pmap(model),\n                        'options': pyr.pmap(opts),\n                        'note':    note,\n                        'error':   error})", "response": "This function is used to parse the command - line arguments for the registration\n    command and produces the subject model and log function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_surface_files(note, error, registrations, subject,\n                       no_surf_export, no_reg_export, surface_format, surface_path,\n                       angle_tag, eccen_tag, label_tag, radius_tag, registration_name):\n    '''\n    save_surface_files is the calculator that saves the registration data out as surface files,\n    which are put back in the registration as the value 'surface_files'.\n    '''\n    if no_surf_export: return {'surface_files': ()}\n    surface_format = surface_format.lower()\n    # make an exporter for properties:\n    if surface_format in ['curv', 'morph', 'auto', 'automatic']:\n        def export(flnm, p):\n            fsio.write_morph_data(flnm, p)\n            return flnm\n    elif surface_format in ['mgh', 'mgz']:\n        def export(flnm, p):\n            flnm = flnm + '.' + surface_format\n            dt = np.int32 if np.issubdtype(p.dtype, np.dtype(int).type) else np.float32\n            img = fsmgh.MGHImage(np.asarray([[p]], dtype=dt), np.eye(4))\n            img.to_filename(flnm)\n            return flnm\n    elif surface_format in ['nifti', 'nii', 'niigz', 'nii.gz']:\n        surface_format = 'nii' if surface_format == 'nii' else 'nii.gz'\n        def export(flnm, p):\n            flnm = flnm + '.' + surface_format\n            dt = np.int32 if np.issubdtype(p.dtype, np.dtype(int).type) else np.float32\n            img = nib.Nifti1Image(np.asarray([[p]], dtype=dt), np.eye(4))\n            img.to_filename(flnm)\n            return flnm\n    else:\n        error('Could not understand surface file-format %s' % surface_format)\n    path = surface_path if surface_path else os.path.join(subject.path, 'surf')\n    files = []\n    note('Exporting files...')\n    for h in six.iterkeys(registrations):\n        hemi = subject.hemis[h]\n        reg = registrations[h]\n        note('Extracting %s predicted mesh...' % h.upper())\n        pmesh = reg['predicted_mesh']\n        for (pname,tag) in zip(['polar_angle', 'eccentricity', 'visual_area', 'radius'],\n                               [angle_tag, eccen_tag, label_tag, radius_tag]):\n            flnm = export(os.path.join(path, h + '.' + tag), pmesh.prop(pname))\n            files.append(flnm)\n        # last do the registration itself\n        if registration_name and not no_reg_export:\n            flnm = os.path.join(path, h + '.' + registration_name + '.sphere.reg')\n            fsio.write_geometry(flnm, pmesh.coordinates.T, pmesh.tess.faces.T)\n    return {'surface_files': tuple(files)}", "response": "This function saves the data of the specified resource in the specified surface format to the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the registration data to the specified volume files.", "response": "def save_volume_files(note, error, registrations, subject,\n                      no_vol_export, volume_format, volume_path,\n                      angle_tag, eccen_tag, label_tag, radius_tag):\n    '''\n    save_volume_files is the calculator that saves the registration data out as volume files,\n    which are put back in the registration as the value 'volume_files'.\n    '''\n    if no_vol_export: return {'volume_files': ()}\n    volume_format = volume_format.lower()\n    # make an exporter for properties:\n    if volume_format in ['mgh', 'mgz', 'auto', 'automatic', 'default']:\n        volume_format = 'mgh' if volume_format == 'mgh' else 'mgz'\n        def export(flnm, d):\n            flnm = flnm + '.' + volume_format\n            dt = np.int32 if np.issubdtype(d.dtype, np.dtype(int).type) else np.float32\n            img = fsmgh.MGHImage(np.asarray(d, dtype=dt), subject.voxel_to_native_matrix)\n            img.to_filename(flnm)\n            return flnm\n    elif volume_format in ['nifti', 'nii', 'niigz', 'nii.gz']:\n        volume_format = 'nii' if volume_format == 'nii' else 'nii.gz'\n        def export(flnm, p):\n            flnm = flnm + '.' + volume_format\n            dt = np.int32 if np.issubdtype(p.dtype, np.dtype(int).type) else np.float32\n            img = nib.Nifti1Image(np.asarray(p, dtype=dt), subject.voxel_to_native_matrix)\n            img.to_filename(flnm)\n            return flnm\n    else:\n        error('Could not understand volume file-format %s' % volume_format)\n    path = volume_path if volume_path else os.path.join(subject.path, 'mri')\n    files = []\n    note('Extracting predicted meshes for volume export...')\n    hemis = [registrations[h]['predicted_mesh'] if h in registrations else None\n             for h in ['lh', 'rh']]\n    for (pname,tag) in zip(['polar_angle', 'eccentricity', 'visual_area', 'radius'],\n                           [angle_tag, eccen_tag, label_tag, radius_tag]):\n        # we have to make the volume first...\n        dat = tuple([None if h is None else h.prop(pname) for h in hemis])\n        (mtd,dt) = ('nearest',np.int32) if pname == 'visual_area' else ('linear',np.float32)\n        note('Constructing %s image...' % pname)\n        img = subject.cortex_to_image(dat, method=mtd, dtype=dt)\n        flnm = export(os.path.join(path, tag), img)\n        files.append(flnm)\n    return {'volume_files': tuple(files)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(args):\n    '''\n    register_retinotopy.main(args) can be given a list of arguments, such as sys.argv[1:]; these\n    arguments may include any options and must include at least one subject id. All subjects whose\n    ids are given are registered to a retinotopy model, and the resulting registration, as well as\n    the predictions made by the model in the registration, are exported.\n    '''\n    m = register_retinotopy_plan(args=args)\n    # force completion\n    files = m['files']\n    if len(files) > 0:\n        return 0\n    else:\n        print('Error: No files exported.', file=sys.stderr)\n        return 1", "response": "This function is called by the register_retinotopy module. It is intended to be used by the register_retinotopy module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loadrc(filename):\n    '''\n    loadrc(filename) yields the dict object encoded in JSON in the given filename; if the filename\n      does not exist or does not contain a valid JSON dict, then an error is raised.\n    '''\n    filename = os.path.expanduser(os.path.expandvars(filename))\n    if not os.path.isfile(filename): raise ValueError('Filename %s does not exist' % filename)\n    with open(filename, 'r') as fl:\n        dat = json.load(fl)\n    try: dat = dict(dat)\n    except Exception: raise ValueError('Given file %s does not contain a dictionary' % filename)\n    return dat", "response": "Load a single file into a dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef saverc(filename, dat, overwrite=False):\n    '''\n    saverc(filename, d) saves the given configuration dictionary d to the given filename in JSON\n      format. If d is not a dictionary or if filename already exists or cannot be created, an error\n      is raised. This funciton does not create directories.\n\n    The optional argument overwrite (default: False) may be passed as True to overwrite files that\n    already exist.\n    '''\n    filename = os.path.expanduser(os.path.expandvars(filename))\n    if not overwrite and os.path.isfile(filename):\n        raise ValueError('Given filename %s already exists' % filename)\n    if not pimms.is_map(dat):\n        try: dat = dict(dat)\n        except Exception: raise ValueError('Given config data must be a dictionary')\n    with open(filename, 'w') as fl:\n        json.dump(dat, fl, sort_keys=True)\n    return filename", "response": "Save the given configuration dictionary dat to the given filename in JSON\n      format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a string to a set of credentials.", "response": "def str_to_credentials(s):\n    '''\n    str_to_credentials(s) yields (key, secret) if the given string is a valid representation of a\n      set of credentials. Valid representations include '<key>:<secret>' and '<key>\\n<secret>'. All\n      initial and trailing whitespace is always stripped from both key and scret. If a newline\n      appears in the string, then this character always takes precedense as the separator over a\n      colon character. The given string may also be a json object, in which case it is parsed and\n      considered valid if it is either a \n    '''\n    if not pimms.is_str(s): raise ValueError('str_to_credentials requires a string argument')\n    s = s.strip()\n    # First try a json object:\n    try:\n        js = json.loads(s)\n        return to_credentials(s)\n    except Exception: pass\n    # must be '<key>\\n<secret>' or '<key>:<secret>'\n    dat = s.split('\\n')\n    if len(dat) == 1: dat = s.split(':')\n    if len(dat) != 2: raise ValueError('String \"%s\" does not appear to be a credentials file' % s)\n    return tuple([q.strip() for q in dat])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading credentials from a file.", "response": "def load_credentials(flnm):\n    '''\n    load_credentials(filename) yields the credentials stored in the given file as a tuple\n      (key, secret). The file must contain <key>:<secret> on a single line. If the file does not\n      contain valid credentials, then an exception is raised. Yields (key, secret).\n\n    Optionally, if the file contains exactly two lines, then the key is taken as the first line and\n    the secret is taken as the second line. This can be used if ':' is a valid character in either\n    the key or secret for a particular service.\n\n    Note that if the key/secret are more than 8kb in size, this function's behaviour is undefined.\n    '''\n    flnm = os.path.expanduser(os.path.expandvars(flnm))\n    with open(flnm, 'r') as fl: dat = fl.read(1024 * 8)\n    # see if its a 2-line file:\n    try:              return str_to_credentials(dat)\n    except Exception: raise ValueError('File %s does not contain a valid credential string' % flnm)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_credentials(arg):\n    '''\n    to_credentials(arg) converts arg into a pair (key, secret) if arg can be coerced into such a\n      pair and otherwise raises an error.\n    \n    Possible inputs include:\n      * A tuple (key, secret)\n      * A mapping with the keys 'key' and 'secret'\n      * The name of a file that can load credentials via the load_credentials() function\n      * A string that separates the key and secret by ':', e.g., 'mykey:mysecret'\n      * A string that separates the key and secret by a \"\\n\", e.g., \"mykey\\nmysecret\"\n    '''\n    if pimms.is_str(arg):\n        try: return load_credentials(arg)\n        except Exception: pass\n        try: return str_to_credentials(arg)\n        except Exception:\n            raise ValueError('String \"%s\" is neither a file containing credentials nor a valid'\n                             ' credentials string itself.' % arg)\n    elif pimms.is_map(arg) and 'key' in arg and 'secret' in arg: return (arg['key'], arg['secret'])\n    elif pimms.is_vector(arg, str) and len(arg) == 2: return tuple(arg)\n    else: raise ValueError('given argument cannot be coerced to credentials: %s' % arg)", "response": "Converts arg into a pair of key and secret."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef detect_credentials(config_name, extra_environ=None, filenames=None,\n                       aws_profile_name=None, default_value=Ellipsis):\n    '''\n    detect_credentials(config_name) attempts to locate Amazon S3 Bucket credentials from the given\n      configuration item config_name.\n\n    The following optional arguments are accepted:\n      * extra_environ (default: None) may specify a string or a tuple (key_name, secret_name) or a\n        list of strings or tuples; strings are treated as an additional environment variable that\n        should be checked for credentials while tuples are treated as paired varialbes: if both are\n        defined, then they are checked as separate holders of a key/secret pair. Note that a list\n        of strings is considered a pair of solo environment varialbes while a tuple of strings is\n        considered a single (key_name, secret_name) pair.\n      * filenames (default: None) may specify a list of filenames that are checked in order for\n        credentials.\n      * aws_profile_name (default: None) may specify a profile name that appears in the\n        ~/.aws/credentials file that will be checked for aws_access_key_id and aws_secret_access_key\n        values. The files ~/.amazon/credentials and ~/.credentials are also checked. Note that this\n        may be a list of profiles to check.\n      * default_value (default: Ellipsis) may specify a value to return when no credentials are\n        found; if this value is None, then it is always returned; otherwise, the value is passed\n        through to_credentials() and any errors are allowed to propagate out of\n        detect_credentials(). If default_value is Ellipsis then an error is simply raised stating\n        that no credentials could be found.\n\n    The detect_credentials() function looks at the following locations in the following order,\n    assuming that it has been provided with the relevant information:\n      * first, if the Neuropythy configuration variable config_name is set via either the npythyrc\n        file or the associated environment variable, then it is coerced into credentials;\n      * next, if the environment contains both the variables key_name and secret_name (from the \n        optional argument key_secret_environ), then these values are used;\n      * next, if the filenames argument is given, then all files it refers to are checked for\n        credentials; these files are expanded with both os.expanduser and os.expandvars.\n      * finally, if no credentials were detected, an error is raised.\n    '''\n    # Check the config first:\n    if config_name is not None and config[config_name] is not None: return config[config_name]\n    # Okay, not found there; check the key/secret environment variables\n    if   extra_environ is None: extra_environ = []\n    elif pimms.is_str(extra_environ): extra_environ = [extra_environ]\n    elif pimms.is_vector(extra_environ):\n        if pimms.is_vector(extra_environ, str):\n            if len(extra_environ) == 2 and isinstance(extra_environ, _tuple_type):\n                extra_environ = [extra_environ]\n    elif not pimms.is_matrix(extra_environ, str):\n        raise ValueError('extra_environ must be a string, tuple of strings, or list of these')\n    for ee in extra_environ:\n        if pimms.is_str(ee):\n            if ee in os.environ:\n                try:              return to_credentials(q)\n                except Exception: pass\n        elif pimms.is_vector(ee, str) and len(ee) == 2:\n            if ee[0] in os.environ and ee[1] in os.environ:\n                (k,s) = [os.environ[q] for q in ee]\n                if len(k) > 0 and len(s) > 0: continue\n                return (k,s)\n        else: raise ValueError('cannot interpret extra_environ argument: %s' % ee)\n    # Okay, next we check the filenames\n    if filenames is None: filenames = []\n    elif pimms.is_str(filenames): filenames = [filenames]\n    for flnm in filenames:\n        flnm = os.expanduser(os.expandvars(flnm))\n        if os.path.isfile(flnm):\n            try:              return to_credentials(flnm)\n            except Exception: pass\n    # okay... let's check the AWS credentials file, if it exists\n    if pimms.is_str(aws_profile_name): aws_profile_name = [aws_profile_name]\n    elif aws_profile_name is None or len(aws_profile_name) == 0: aws_profile_name = None\n    elif not pimms.is_vector(aws_profile_name, str):\n        raise ValueError('Invalid aws_profile_name value: %s' % aws_profile_name)\n    if aws_profile_name is not None:\n        try:\n            cc = confparse.ConfigParser()\n            cc.read([os.expanduser(os.path.join('~', '.aws', 'credentials')),\n                     os.expanduser(os.path.join('~', '.amazon', 'credentials')),\n                     os.expanduser(os.path.join('~', '.credentials'))])\n            for awsprof in aws_profile_names:\n                try:\n                    aws_access_key_id     = cc.get(awsprof, 'aws_access_key_id')\n                    aws_secret_access_key = cc.get(awsprof, 'aws_secret_access_key')\n                    return (aws_access_key_id, aws_secret_access_key)\n                except Exception: pass\n        except Exception: pass\n    # no match!\n    if default_value is None:\n        return None\n    elif default_value is Ellipsis:\n        if config_name is None: raise ValueError('No valid credentials were detected')\n        else: raise ValueError('No valid credentials (%s) were detected' % config_name)\n    else: return to_credentials(default_value)", "response": "Detects Amazon S3 Bucket credentials from the given configuration item config_name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize a vetor with the same direction as u but unit length yields a vetor with the same direction as u but unit length yields u.", "response": "def normalize(u):\n    '''\n    normalize(u) yields a vetor with the same direction as u but unit length, or, if u has zero\n    length, yields u.\n    '''\n    u = np.asarray(u)\n    unorm = np.sqrt(np.sum(u**2, axis=0))\n    z = np.isclose(unorm, 0)\n    c = np.logical_not(z) / (unorm + z)\n    return u * c"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vector_angle_cos(u, v):\n    '''\n    vector_angle_cos(u, v) yields the cosine of the angle between the two vectors u and v. If u\n    or v (or both) is a (d x n) matrix of n vectors, the result will be a length n vector of the\n    cosines.\n    '''\n    u = np.asarray(u)\n    v = np.asarray(v)\n    return (u * v).sum(0) / np.sqrt((u ** 2).sum(0) * (v ** 2).sum(0))", "response": "vector_angle_cos yields the cosine of the angle between the two vectors u and v."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vector_angle(u, v, direction=None):\n    '''\n    vector_angle(u, v) yields the angle between the two vectors u and v. The optional argument \n    direction is by default None, which specifies that the smallest possible angle between the\n    vectors be reported; if the vectors u and v are 2D vectors and direction parameters True and\n    False specify the clockwise or counter-clockwise directions, respectively; if the vectors are\n    3D vectors, then direction may be a 3D point that is not in the plane containing u, v, and the\n    origin, and it specifies around which direction (u x v or v x u) the the counter-clockwise angle\n    from u to v should be reported (the cross product vector that has a positive dot product with\n    the direction argument is used as the rotation axis).\n    '''\n    if direction is None:\n        return np.arccos(vector_angle_cos(u, v))\n    elif direction is True:\n        return np.arctan2(v[1], v[0]) - np.arctan2(u[1], u[0])\n    elif direction is False:\n        return np.arctan2(u[1], u[0]) - np.arctan2(v[1], v[0])\n    else:\n        axis1 = normalize(u)\n        axis2 = normalize(np.cross(u, v))\n        if np.dot(axis2, direction) < 0:\n            axis2 = -axis2\n        return np.arctan2(np.dot(axis2, v), np.dot(axis1, v))", "response": "returns the angle between two vectors u and v"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef spherical_distance(pt0, pt1):\n    '''\n    spherical_distance(a, b) yields the angular distance between points a and b, both of which\n      should be expressed in spherical coordinates as (longitude, latitude).\n    If a and/or b are (2 x n) matrices, then the calculation is performed over all columns.\n    The spherical_distance function uses the Haversine formula; accordingly it may suffer from\n    rounding errors in the case of nearly antipodal points.\n    '''\n    dtheta = pt1[0] - pt0[0]\n    dphi   = pt1[1] - pt0[1]\n    a = np.sin(dphi/2)**2 + np.cos(pt0[1]) * np.cos(pt1[1]) * np.sin(dtheta/2)**2\n    return 2 * np.arcsin(np.sqrt(a))", "response": "Returns the angular distance between two points pt0 and pt1."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rotation_matrix_3D(u, th):\n    # normalize the axis:\n    u = normalize(u)\n    # We use the Euler-Rodrigues formula;\n    # see https://en.wikipedia.org/wiki/Euler-Rodrigues_formula\n    a = math.cos(0.5 * th)\n    s = math.sin(0.5 * th)\n    (b, c, d) = -s * u\n    (a2, b2, c2, d2) = (a*a, b*b, c*c, d*d)\n    (bc, ad, ac, ab, bd, cd) = (b*c, a*d, a*c, a*b, b*d, c*d)\n    return np.array([[a2 + b2 - c2 - d2, 2*(bc + ad),         2*(bd - ac)],\n                     [2*(bc - ad),       a2 + c2 - b2 - d2,   2*(cd + ab)],\n                     [2*(bd + ac),       2*(cd - ab),         a2 + d2 - b2 - c2]])", "response": "returns a 3D numpy matrix that rotates any vector about the axis u radians counter - clockwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rotation_matrix_2D(th):\n    '''\n    rotation_matrix_2D(th) yields a 2D numpy rotation matrix th radians counter-clockwise about the\n    origin.\n    '''\n    s = np.sin(th)\n    c = np.cos(th)\n    return np.array([[c, -s], [s, c]])", "response": "returns a 2D numpy rotation matrix th radians counter - clockwise about the\n    origin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef alignment_matrix_3D(u, v):\n    '''\n    alignment_matrix_3D(u, v) yields a 3x3 numpy array that rotates the vector u to the vector v\n    around the origin.\n    '''\n    # normalize both vectors:\n    u = normalize(u)\n    v = normalize(v)\n    # get the cross product of u cross v\n    w = np.cross(u, v)\n    # the angle we need to rotate\n    th = vector_angle(u, v)\n    # we rotate around this vector by the angle between them\n    return rotation_matrix_3D(w, th)", "response": "returns a 3x3 numpy array that rotates the vector u to the vector v around the origin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef point_in_segment(ac, b):\n    '''\n    point_in_segment((a,b), c) yields True if point x is in segment (a,b) and False otherwise. Note\n    that this differs from point_on_segment in that a point that if c is equal to a or b it is\n    considered 'on' but not 'in' the segment.\n    '''\n    (a,c) = ac\n    abc = [np.asarray(u) for u in (a,b,c)]\n    if any(len(u.shape) > 1 for u in abc): (a,b,c) = [np.reshape(u,(len(u),-1)) for u in abc]\n    else:                                  (a,b,c) = abc\n    vab = b - a\n    vbc = c - b\n    vac = c - a\n    dab = np.sqrt(np.sum(vab**2, axis=0))\n    dbc = np.sqrt(np.sum(vbc**2, axis=0))\n    dac = np.sqrt(np.sum(vac**2, axis=0))\n    return np.isclose(dab + dbc, dac) & ~np.isclose(dac,dab) & ~np.isclose(dac,dbc)", "response": "Point x is in segment a b c yields True if point x is in segment b and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef segments_colinear(ab, cd):\n    '''\n    segments_colinear_2D((a, b), (c, d)) yields True if either a or b is on the line segment (c,d) or\n    if c or d is on the line segment (a,b) and the lines are colinear; otherwise yields False. All\n    of a, b, c, and d must be (x,y) coordinates or 2xN (x,y) coordinate matrices, or (x,y,z) or 3xN\n    matrices.\n    '''\n    (a,b) = ab\n    (c,d) = cd\n    ss = [point_on_segment(ab, c), point_on_segment(ab, d),\n          point_on_segment(cd, a), point_on_segment(cd, b)]\n    return np.sum(ss, axis=0) > 1", "response": "Returns True if the lines are colinear."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if points a and b are close to each other and False otherwise.", "response": "def points_close(a,b):\n    '''\n    points_close(a,b) yields True if points a and b are close to each other and False otherwise.\n    '''\n    (a,b) = [np.asarray(u) for u in (a,b)]\n    if len(a.shape) == 2 or len(b.shape) == 2: (a,b) = [np.reshape(u,(len(u),-1)) for u in (a,b)]\n    return np.isclose(np.sqrt(np.sum((a - b)**2, axis=0)), 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef segments_overlapping(ab, cd):\n    '''\n    segments_overlapping((a, b), (c, d)) yields True if the line segments (a,b) and (c,d) are both\n    colinear and have a non-finite overlap. If (a,b) and (c,d) touch at a single point, they are not\n    considered overlapping.\n    '''\n    (a,b) = ab\n    (c,d) = cd\n    ss = [point_in_segment(ab, c), point_in_segment(ab, d),\n          point_in_segment(cd, a), point_in_segment(cd, b)]\n    return (~(points_close(a,b) | points_close(c,d)) & \n            ((np.sum(ss, axis=0) > 1) |\n             (points_close(a,c) & points_close(b,d)) |\n             (points_close(a,d) & points_close(b,c))))", "response": "Returns a boolean array indicating if two line segments overlap."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef line_intersection_2D(abarg, cdarg):\n    '''\n    line_intersection((a, b), (c, d)) yields the intersection point between the lines that pass\n    through the given pairs of points. If any lines are parallel, (numpy.nan, numpy.nan) is\n    returned; note that a, b, c, and d can all be 2 x n matrices of x and y coordinate row-vectors.\n    '''\n    ((x1,y1),(x2,y2)) = abarg\n    ((x3,y3),(x4,y4)) = cdarg\n    dx12 = (x1 - x2)\n    dx34 = (x3 - x4)\n    dy12 = (y1 - y2)\n    dy34 = (y3 - y4)\n    denom = dx12*dy34 - dy12*dx34\n    unit = np.isclose(denom, 0)\n    if unit is True: return (np.nan, np.nan)\n    denom = unit + denom\n    q12 = (x1*y2 - y1*x2) / denom\n    q34 = (x3*y4 - y3*x4) / denom\n    xi = q12*dx34 - q34*dx12\n    yi = q12*dy34 - q34*dy12\n    if   unit is False: return (xi, yi)\n    elif unit is True:  return (np.nan, np.nan)\n    else:\n        xi = np.asarray(xi)\n        yi = np.asarray(yi)\n        xi[unit] = np.nan\n        yi[unit] = np.nan\n        return (xi, yi)", "response": "line_intersection yields the intersection point between the lines that pass\n    through the given pairs of points."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn true if the lines touch the 2D polygon.", "response": "def lines_touch_2D(ab, cd):\n    '''\n    lines_touch_2D((a,b), (c,d)) is equivalent to lines_colinear((a,b), (c,d)) |\n    numpy.isfinite(line_intersection_2D((a,b), (c,d))[0])\n    '''\n    return lines_colinear(ab, cd) | np.isfinite(line_intersection_2D(ab, cd)[0])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef segments_touch_2D(ab, cd):\n    '''\n    segmentss_touch_2D((a,b), (c,d)) is equivalent to segments_colinear((a,b), (c,d)) |\n    numpy.isfinite(segment_intersection_2D((a,b), (c,d))[0])\n    '''\n    return segments_colinear(ab, cd) | np.isfinite(segment_intersection_2D(ab, cd)[0])", "response": "Return true if two segments touch the 2D segment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef triangle_area(a,b,c):\n    '''\n    triangle_area(a, b, c) yields the area of the triangle whose vertices are given by the points a,\n    b, and c.\n    '''\n    (a,b,c) = [np.asarray(x) for x in (a,b,c)]\n    sides = np.sqrt(np.sum([(p1 - p2)**2 for (p1,p2) in zip([b,c,a],[c,a,b])], axis=1))\n    s = 0.5 * np.sum(sides, axis=0)\n    s = np.clip(s * np.prod(s - sides, axis=0), 0.0, None)\n    return np.sqrt(s)", "response": "Calculates the area of a triangle in the graph."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef triangle_normal(a,b,c):\n    '''\n    triangle_normal(a, b, c) yields the normal vector of the triangle whose vertices are given by\n      the points a, b, and c. If the points are 2D points, then 3D normal vectors are still yielded,\n      that are always (0,0,1) or (0,0,-1). This function auto-threads over matrices, in which case\n      they must be in equivalent orientations, and the result is returned in whatever orientation\n      they are given in. In some cases, the intended orientation of the matrices is ambiguous (e.g.,\n      if a, b, and c are 2 x 3 matrices), in which case the matrix is always assumed to be given in\n      (dims x vertices) orientation.\n    '''\n    (a,b,c) = [np.asarray(x) for x in (a,b,c)]\n    if len(a.shape) == 1 and len(b.shape) == 1 and len(c.shape) == 1:\n        return triangle_normal(*[np.transpose([x]) for x in (a,b,c)])[:,0]\n    (a,b,c) = [np.transpose([x]) if len(x.shape) == 1 else x for x in (a,b,c)]\n    # find a required number of dimensions, if possible\n    if a.shape[0] in (2,3):\n        dims = a.shape[0]\n        tx = True\n    else:\n        dims = a.shape[1]\n        (a,b,c) = [x.T for x in (a,b,c)]\n        tx = False\n    n = (a.shape[1] if a.shape[1] != 1 else b.shape[1] if b.shape[1] != 1 else\n         c.shape[1] if c.shape[1] != 1 else 1)\n    if dims == 2:\n        (a,b,c) = [np.vstack((x, np.zeros((1,n)))) for x in (a,b,c)]\n    ab = normalize(b - a)\n    ac = normalize(c - a)\n    res = np.cross(ab, ac, axisa=0, axisb=0)\n    return res.T if tx else res", "response": "This function returns the normal vector of the triangle whose vertices are given by a b and c."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cartesian_to_barycentric_2D(tri, xy):\n    '''\n    cartesian_to_barycentric_2D(tri, xy) yields a (2 x n) barycentric coordinate matrix \n    (or just a tuple if xy is a single (x, y) coordinate) of the first two barycentric coordinates\n    for the triangle coordinate array tri. The array tri should be 3 (vertices) x 2 (coordinates) x\n    n (triangles) unless xy is a tuple, in which case it should be a (3 x 2) matrix.\n    '''\n    xy = np.asarray(xy)\n    tri = np.asarray(tri)\n    if len(xy.shape) == 1:\n        return cartesian_to_barycentric_2D(np.transpose(np.asarray([tri]), (1,2,0)),\n                                           np.asarray([xy]).T)[:,0]\n    xy = xy if xy.shape[0] == 2 else xy.T\n    if tri.shape[0] == 3:\n        tri = tri if tri.shape[1] == 2 else np.transpose(tri, (0,2,1))\n    elif tri.shape[1] == 3:\n        tri = tri.T if tri.shape[0] == 2 else np.transpose(tri, (1,2,0))\n    elif tri.shape[2] == 3:\n        tri = np.transpose(tri, (2,1,0) if tri.shape[1] == 2 else (2,0,1))\n    if tri.shape[0] != 3 or tri.shape[1] != 2:\n        raise ValueError('Triangle array did not have dimensions of sizes 3 and 2')\n    if xy.shape[0] != 2:\n        raise ValueError('coordinate matrix did not have a dimension of size 2')\n    if tri.shape[2] != xy.shape[1]:\n        raise ValueError('number of triangles and coordinates must match')\n    # Okay, everything's the right shape...\n    (x,y) = xy\n    ((x1,y1), (x2,y2), (x3,y3)) = tri\n    x_x3  = x  - x3\n    x1_x3 = x1 - x3\n    x3_x2 = x3 - x2\n    y_y3  = y  - y3\n    y1_y3 = y1 - y3\n    y2_y3 = y2 - y3\n    num1 = (y2_y3*x_x3  + x3_x2*y_y3)\n    num2 = (-y1_y3*x_x3 + x1_x3*y_y3)\n    den  = (y2_y3*x1_x3 + x3_x2*y1_y3)\n    zero = np.isclose(den, 0)\n    den += zero\n    unit = 1 - zero\n    l1 = unit * num1 / den\n    l2 = unit * num2 / den\n    return np.asarray((l1, l2))", "response": "Returns a 2D matrix that contains the barycentric coordinates of a triangle and a 2D coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef barycentric_to_cartesian(tri, bc):\n    '''\n    barycentric_to_cartesian(tri, bc) yields the d x n coordinate matrix of the given barycentric\n    coordinate matrix (also d x n) bc interpolated in the n triangles given in the array tri. See\n    also cartesian_to_barycentric. If tri and bc represent one triangle and coordinate, then just\n    the coordinate and not a matrix is returned. The value d, dimensions, must be 2 or 3.\n    '''\n    bc = np.asarray(bc)\n    tri = np.asarray(tri)\n    if len(bc.shape) == 1:\n        return barycentric_to_cartesian(np.transpose(np.asarray([tri]), (1,2,0)),\n                                        np.asarray([bc]).T)[:,0]\n    bc = bc if bc.shape[0] == 2 else bc.T\n    if bc.shape[0] != 2: raise ValueError('barycentric matrix did not have a dimension of size 2')\n    n = bc.shape[1]\n    # we know how many bc's there are now; lets reorient tri to match with the last dimension as n\n    if len(tri.shape) == 2:\n        tri = np.transpose([tri for _ in range(n)], (1,2,0))\n    # the possible orientations of tri:\n    if tri.shape[0] == 3:\n        if tri.shape[1] in [2,3] and tri.shape[2] == n:\n            pass # default orientation\n        elif tri.shape[1] == n and tri.shape[2] in [2,3]:\n            tri = np.transpose(tri, (0,2,1))\n        else: raise ValueError('could not deduce triangle dimensions')\n    elif tri.shape[1] == 3:\n        if tri.shape[0] in [2,3] and tri.shape[2] == n:\n            tri = np.transpose(tri, (1,0,2))\n        elif tri.shape[0] == n and tri.shape[2] in [2,3]:\n            tri = np.transpose(tri, (1,2,0))\n        else: raise ValueError('could not deduce triangle dimensions')\n    elif tri.shape[2] == 3:\n        if tri.shape[0] in [2,3] and tri.shape[1] == n:\n            tri = np.transpose(tri, (2,0,1))\n        elif tri.shape[0] == n and tri.shape[1] in [2,3]:\n            tri = np.transpose(tri, (2,1,0))\n        else: raise ValueError('could not deduce triangle dimensions')\n    else: raise ValueError('At least one dimension of triangles must be 3')\n    if tri.shape[0] != 3 or (tri.shape[1] not in [2,3]):\n        raise ValueError('Triangle array did not have dimensions of sizes 3 and (2 or 3)')\n    if tri.shape[2] != n:\n        raise ValueError('number of triangles and coordinates must match')\n    (l1,l2) = bc\n    (p1, p2, p3) = tri\n    l3 = (1 - l1 - l2)\n    return np.asarray([x1*l1 + x2*l2 + x3*l3 for (x1,x2,x3) in zip(p1, p2, p3)])", "response": "Given a barycentric array tri returns the d x n coordinate matrix of the given barycentric array bc interpolated in the given n triangles and the value d x n coordinate matrix of the given barycentric array tri."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef triangle_address(fx, pt):\n    '''\n    triangle_address(FX, P) yields an address coordinate (t,r) for the point P in the triangle\n    defined by the (3 x d)-sized coordinate matrix FX, in which each row of the matrix is the\n    d-dimensional vector representing the respective triangle vertx for triangle [A,B,C]. The\n    resulting coordinates (t,r) (0 <= t <= 1, 0 <= r <= 1) address the point P such that, if t gives\n    the fraction of the angle from vector AB to vector AC that is made by the angle between vectors\n    AB and AP, and r gives the fraction ||AP||/||AR|| where R is the point of intersection between\n    lines AP and BC. If P is a (d x n)-sized matrix of points, then a (2 x n) matrix of addresses\n    is returned.\n    '''\n    fx = np.asarray(fx)\n    pt = np.asarray(pt)\n    # The triangle vectors...\n    ab = fx[1] - fx[0]\n    ac = fx[2] - fx[0]\n    bc = fx[2] - fx[1]\n    ap = np.asarray([pt_i - a_i for (pt_i, a_i) in zip(pt, fx[0])])\n    # get the unnormalized distance...\n    r = np.sqrt((ap ** 2).sum(0))\n    # now we can find the angle...\n    unit = 1 - r.astype(bool)\n    t0 = vector_angle(ab, ac)\n    t = vector_angle(ap + [ab_i * unit for ab_i in ab], ab)\n    sint = np.sin(t)\n    sindt = np.sin(t0 - t)\n    # finding r0 is tricker--we use this fancy formula based on the law of sines\n    q0 = np.sqrt((bc ** 2).sum(0))          # B->C distance\n    beta = vector_angle(-ab, bc)            # Angle at B\n    sinGamma = np.sin(math.pi - beta - t0)\n    sinBeta  = np.sin(beta)\n    r0 = q0 * sinBeta * sinGamma / (sinBeta * sindt + sinGamma * sint)\n    return np.asarray([t/t0, r/r0])", "response": "This function returns the address of a triangle at a given point."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef triangle_unaddress(fx, tr):\n    '''\n    triangle_unaddress(FX, tr) yields the point P, inside the reference triangle given by the \n    (3 x d)-sized coordinate matrix FX, that is addressed by the address coordinate tr, which may\n    either be a 2-d vector or a (2 x n)-sized matrix.\n    '''\n    fx = np.asarray(fx)\n    tr = np.asarray(tr)\n    # the triangle vectors...\n    ab = fx[1] - fx[0]\n    ac = fx[2] - fx[0]\n    bc = fx[2] - fx[1]\n    return np.asarray([ax + tr[1]*(abx + tr[0]*bcx) for (ax, bcx, abx) in zip(fx[0], bc, ab)])", "response": "This function unaddresses a triangle given a 3x3 coordinate matrix FX and a 3x3 coordinate matrix tr."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the determinate of the given matrix array which may have more than 2 dimensions and added point - wise.", "response": "def det4D(m):\n    '''\n    det4D(array) yields the determinate of the given matrix array, which may have more than 2\n      dimensions, in which case the later dimensions are multiplied and added point-wise.\n    '''\n    # I just solved this in Mathematica, copy-pasted, and replaced the string '] m' with ']*m':\n    # Mathematica code: Det@Table[m[i][j], {i, 0, 3}, {j, 0, 3}]\n    return (m[0][3]*m[1][2]*m[2][1]*m[3][0] - m[0][2]*m[1][3]*m[2][1]*m[3][0] -\n            m[0][3]*m[1][1]*m[2][2]*m[3][0] + m[0][1]*m[1][3]*m[2][2]*m[3][0] +\n            m[0][2]*m[1][1]*m[2][3]*m[3][0] - m[0][1]*m[1][2]*m[2][3]*m[3][0] -\n            m[0][3]*m[1][2]*m[2][0]*m[3][1] + m[0][2]*m[1][3]*m[2][0]*m[3][1] +\n            m[0][3]*m[1][0]*m[2][2]*m[3][1] - m[0][0]*m[1][3]*m[2][2]*m[3][1] -\n            m[0][2]*m[1][0]*m[2][3]*m[3][1] + m[0][0]*m[1][2]*m[2][3]*m[3][1] +\n            m[0][3]*m[1][1]*m[2][0]*m[3][2] - m[0][1]*m[1][3]*m[2][0]*m[3][2] -\n            m[0][3]*m[1][0]*m[2][1]*m[3][2] + m[0][0]*m[1][3]*m[2][1]*m[3][2] +\n            m[0][1]*m[1][0]*m[2][3]*m[3][2] - m[0][0]*m[1][1]*m[2][3]*m[3][2] -\n            m[0][2]*m[1][1]*m[2][0]*m[3][3] + m[0][1]*m[1][2]*m[2][0]*m[3][3] +\n            m[0][2]*m[1][0]*m[2][1]*m[3][3] - m[0][0]*m[1][2]*m[2][1]*m[3][3] -\n            m[0][1]*m[1][0]*m[2][2]*m[3][3] + m[0][0]*m[1][1]*m[2][2]*m[3][3])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef det_4x3(a,b,c,d):\n    '''\n    det_4x3(a,b,c,d) yields the determinate of the matrix formed the given rows, which may have\n      more than 1 dimension, in which case the later dimensions are multiplied and added point-wise.\n      The point's must be 3D points; the matrix is given a fourth column of 1s and the resulting\n      determinant is of this matrix. \n    '''\n    # I just solved this in Mathematica, copy-pasted, and replaced the string '] m' with ']*m':\n    # Mathematica code: Det@Table[If[j == 3, 1, i[j]], {i, {a, b, c, d}}, {j, 0, 3}]\n    return (a[1]*b[2]*c[0] + a[2]*b[0]*c[1] - a[2]*b[1]*c[0] - a[0]*b[2]*c[1] -\n            a[1]*b[0]*c[2] + a[0]*b[1]*c[2] + a[2]*b[1]*d[0] - a[1]*b[2]*d[0] -\n            a[2]*c[1]*d[0] + b[2]*c[1]*d[0] + a[1]*c[2]*d[0] - b[1]*c[2]*d[0] -\n            a[2]*b[0]*d[1] + a[0]*b[2]*d[1] + a[2]*c[0]*d[1] - b[2]*c[0]*d[1] -\n            a[0]*c[2]*d[1] + b[0]*c[2]*d[1] + a[1]*b[0]*d[2] - a[0]*b[1]*d[2] -\n            a[1]*c[0]*d[2] + b[1]*c[0]*d[2] + a[0]*c[1]*d[2] - b[0]*c[1]*d[2])", "response": "This function is used to compute the determinate of the matrix formed by a 4x3 transformation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tetrahedral_barycentric_coordinates(tetra, pt):\n    '''\n    tetrahedral_barycentric_coordinates(tetrahedron, point) yields a list of weights for each vertex\n      in the given tetrahedron in the same order as the vertices given. If all weights are 0, then\n      the point is not inside the tetrahedron.\n    '''\n    # I found a description of this algorithm here (Nov. 2017):\n    # http://steve.hollasch.net/cgindex/geometry/ptintet.html\n    tetra = np.asarray(tetra)\n    if tetra.shape[0] != 4:\n        if tetra.shape[1] == 4:\n            if tetra.shape[0] == 3:\n                tetra = np.transpose(tetra, (1,0) if len(tetra.shape) == 2 else (1,0,2))\n            else:\n                tetra = np.transpose(tetra, (1,2,0))\n        elif tetra.shape[1] == 3:\n            tetra = np.transpose(tetra, (2,1,0))\n        else:\n            tetra = np.transpose(tetra, (2,0,1))\n    elif tetra.shape[1] != 3:\n        tetra = np.transpose(tetra, (0,2,1))\n    if pt.shape[0] != 3: pt = pt.T\n    # Okay, calculate the determinants...\n    d_ = det_4x3(tetra[0], tetra[1], tetra[2], tetra[3])\n    d0 = det_4x3(pt,       tetra[1], tetra[2], tetra[3])\n    d1 = det_4x3(tetra[0], pt,       tetra[2], tetra[3])\n    d2 = det_4x3(tetra[0], tetra[1], pt,       tetra[3])\n    d3 = det_4x3(tetra[0], tetra[1], tetra[2], pt)\n    s_ = np.sign(d_)\n    z_ = np.logical_or(np.any([s_ * si == -1 for si in np.sign([d0,d1,d2,d3])], axis=0),\n                       np.isclose(d_,0))\n    x_ = np.logical_not(z_)\n    d_inv = x_ / (x_ * d_ + z_)\n    return np.asarray([d_inv * dq for dq in (d0,d1,d2,d3)])", "response": "This function calculates the tetrahedral barycentric coordinates for a given tetrahedron and point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef point_in_tetrahedron(tetra, pt):\n    '''\n    point_in_tetrahedron(tetrahedron, point) yields True if the given point is in the given \n      tetrahedron. If either tetrahedron or point (or both) are lists of shapes/points, then this\n      calculation is automatically threaded over all the given arguments.\n    '''\n    bcs = tetrahedral_barycentric_coordinates(tetra, pt)\n    return np.logical_not(np.all(np.isclose(bcs, 0), axis=0))", "response": "Returns True if the given point is in the given tetrahedron."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prism_barycentric_coordinates(tri1, tri2, pt):\n    '''\n    prism_barycentric_coordinates(tri1, tri2, point) yields a list of weights for each vertex\n      in the given tetrahedron in the same order as the vertices given. If all weights are 0, then\n      the point is not inside the tetrahedron. The returned weights are (a,b,d) in a numpy array;\n      the values a, b, and c are the barycentric coordinates corresponding to the three points of\n      the triangles (where c = (1 - a - b) and the value d is the fractional distance (in the range\n      [0,1]) of the point between tri1 (d=0) and tri2 (d=1).\n    '''\n    pt = np.asarray(pt)\n    tri1 = np.asarray(tri1)\n    tri2 = np.asarray(tri2)\n    (tri1,tri2) = [\n        (np.transpose(tri, (1,0) if len(tri.shape) == 2 else (2,0,1)) if tri.shape[0] != 3 else\n         np.transpose(tri, (0,2,1))                                   if tri.shape[1] != 3 else\n         tri)\n        for tri in (tri1,tri2)]\n    pt = pt.T if pt.shape[0] != 3 else pt\n    # get the individual tetrahedron bc coordinates\n    bcs1 = tetrahedral_barycentric_coordinates([tri1[0], tri1[1], tri1[2], tri2[0]], pt)\n    bcs2 = tetrahedral_barycentric_coordinates([tri1[1], tri1[2], tri2[0], tri2[1]], pt)\n    bcs3 = tetrahedral_barycentric_coordinates([tri1[2], tri2[0], tri2[1], tri2[2]], pt)\n    bcs4 = tetrahedral_barycentric_coordinates([tri1[0], tri1[1], tri2[0], tri2[1]], pt)\n    bcs5 = tetrahedral_barycentric_coordinates([tri1[0], tri1[2], tri2[0], tri2[2]], pt)\n    bcs6 = tetrahedral_barycentric_coordinates([tri1[1], tri1[2], tri2[1], tri2[2]], pt)\n    bcs = ((bcs1[0] + bcs4[0] + bcs5[0],\n            bcs1[1] + bcs2[0] + bcs4[1] + bcs6[0],\n            bcs1[2] + bcs2[1] + bcs3[0] + bcs5[1] + bcs6[1]),\n           (bcs1[3] + bcs2[2] + bcs3[1] + bcs4[2] + bcs5[2],\n            bcs2[3] + bcs3[2] + bcs4[3] + bcs6[2],\n            bcs3[3] + bcs5[3] + bcs6[3]))\n    # convert into (a,b,c,d) coordinates\n    abc = np.sum(bcs, axis=0)\n    d = np.sum(bcs[1], axis=0)\n    return np.asarray((abc[0], abc[1], d))", "response": "Returns a list of prism barycentric coordinates corresponding to the given point."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the given point is inside the prism that stretches between two triangles.", "response": "def point_in_prism(tri1, tri2, pt):\n    '''\n    point_in_prism(tri1, tri2, pt) yields True if the given point is inside the prism that stretches\n      between triangle 1 and triangle 2. Will automatically thread over extended dimensions. If\n      multiple triangles are given, then the vertices must be an earlier dimension than the\n      coordinates; e.g., a 3 x 3 x n array will be assumed to organized such that element [0,1,k] is\n      the y coordinate of the first vertex of the k'th triangle.\n    '''\n    bcs = prism_barycentric_coordinates(tri1, tri2, pt)\n    return np.logical_not(np.isclose(np.sum(bcs, axis=0), 0))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(args):\n    '''\n    surface_to_rubbon.main(args) can be given a list of arguments, such as sys.argv[1:]; these\n    arguments may include any options and must include exactly one subject id and one output\n    filename. Additionally one or two surface input filenames must be given. The surface files are\n    projected into the ribbon and written to the output filename. For more information see the\n    string stored in surface_to_image.info.\n    '''\n    # Parse the arguments\n    (args, opts) = _surface_to_ribbon_parser(args)\n    # First, help?\n    if opts['help']:\n        print(info, file=sys.stdout)\n        return 1\n    # and if we are verbose, lets setup a note function\n    verbose = opts['verbose']\n    def note(s):\n        if verbose: print(s, file=sys.stdout)\n        return verbose\n    # Add the subjects directory, if there is one\n    if 'subjects_dir' in opts and opts['subjects_dir'] is not None:\n        add_subject_path(opts['subjects_dir'])\n    # figure out our arguments:\n    (lhfl, rhfl) = (opts['lh_file'], opts['rh_file'])\n    if len(args) == 0:\n      raise ValueError('Not enough arguments provided!')\n    elif len(args) == 1:\n      # must be that the subject is in the env?\n      sub = find_subject_path(os.getenv('SUBJECT'))\n      outfl = args[0]\n    elif len(args) == 2:\n      sbpth = find_subject_path(args[0])\n      if sbpth is not None:\n        sub = sbpth\n      else:\n        sub = find_subject_path(os.getenv('SUBJECT'))\n        if lhfl is not None: rhfl = args[0]\n        elif rhfl is not None: lhfl = args[0]\n        else: raise ValueError('Given arg is not a subject: %s' % args[0])\n      outfl = args[1]\n    elif len(args) == 3:\n      sbpth0 = find_subject_path(args[0])\n      sbpth1 = find_subject_path(args[1])\n      if sbpth0 is not None:\n        sub = sbpth0\n        if lhfl is not None: rhfl = args[1]\n        elif rhfl is not None: lhfl = args[1]\n        else: raise ValueError('Too many arguments given: %s' % args[1])\n      elif sbpth1 is not None:\n        sub = sbpth1\n        if lhfl is not None: rhfl = args[0]\n        elif rhfl is not None: lhfl = args[0]\n        else: raise ValueError('Too many arguments given: %s' % args[0])\n      else:\n        sub = find_subject_path(os.getenv('SUBJECT'))\n        if lhfl is not None or rhfl is not None:\n          raise ValueError('Too many arguments and no subject given')\n        (lhfl, rhfl) = args\n      outfl = args[2]\n    elif len(args) == 4:\n      if lhfl is not None or rhfl is not None:\n          raise ValueError('Too many arguments and no subject given')\n      subidx = next((i for (i,a) in enumerate(args) if find_subject_path(a) is not None), None)\n      if subidx is None: raise ValueError('No subject given')\n      sub = find_subject_path(args[subidx])\n      del args[subidx]\n      (lhfl, rhfl, outfl) = args\n    else:\n      raise ValueError('Too many arguments provided!')\n    if sub is None: raise ValueError('No subject specified or found in $SUBJECT')\n    if lhfl is None and rhfl is None: raise ValueError('No surfaces provided')\n    # check the method\n    method = opts['method'].lower()\n    if method not in ['linear', 'lines', 'nearest', 'auto']:\n        raise ValueError('Unsupported method: %s' % method)\n    # and the datatype\n    if opts['dtype'] is None: dtyp = None\n    elif opts['dtype'].lower() == 'float': dtyp = np.float32\n    elif opts['dtype'].lower() == 'int': dtyp = np.int32\n    else: raise ValueError('Type argument must be float or int')\n    if method == 'auto':\n      if dtyp is np.float32: method = 'linear'\n      elif dtyp is np.int32: method = 'nearest'\n      else: method = 'linear'\n    # Now, load the data:\n    note('Reading surfaces...')\n    (lhdat, rhdat) = (None, None)\n    if lhfl is not None:\n        note('   - Reading LH file: %s' % lhfl)\n        lhdat = read_surf_file(lhfl)\n    if rhfl is not None:\n        note('   - Reading RH file: %s' % rhfl)\n        rhdat = read_surf_file(rhfl)\n    (dat, hemi) = (rhdat, 'rh') if lhdat is None else \\\n                  (lhdat, 'lh') if rhdat is None else \\\n                  ((lhdat, rhdat), None)\n    sub = subject(sub)\n    # okay, make the volume...\n    note('Generating volume...')\n    vol = sub.cortex_to_image(dat, hemi=hemi, method=method, fill=opts['fill'], dtype=dtyp)\n    # and write out the file\n    note('Exporting volume file: %s' % outfl)\n    save(outfl, vol, affine=sub.voxel_to_native_matrix)\n    note('surface_to_image complete!')\n    return 0", "response": "This is the main function for the ribbon surface_to_rubbon program."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreload all of the modules of neuropythy and returns the reloaded module.", "response": "def reload_neuropythy():\n    '''\n    reload_neuropythy() reloads all of the modules of neuropythy and returns the reloaded\n    neuropythy module. This is similar to reload(neuropythy) except that it reloads all the\n    neuropythy submodules prior to reloading neuropythy.\n\n    Example:\n      import neuropythy as ny\n      # ... some nonsense that breaks the library ...\n      ny = ny.reload_neuropythy()\n    '''\n    import sys, six\n    if not six.PY2:\n        try:              from importlib import reload\n        except Exception: from imp import reload\n    for mdl in submodules:\n        if mdl in sys.modules:\n            sys.modules[mdl] = reload(sys.modules[mdl])\n    return reload(sys.modules['neuropythy'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize_numpy(m, t):\n    '''\n    serialize_numpy(m, type) converts the numpy array m into a byte stream that can be read by the\n    nben.util.Py4j Java class. The function assumes that the type of the array needn't be encoded\n    in the bytearray itself. The bytearray will begin with an integer, the number of dimensions,\n    followed by that number of integers (the dimension sizes themselves) then the bytes of the\n    array, flattened.\n    The argument type gives the type of the array to be transferred and must be 'i' for integer or\n    'd' for double (or any other string accepted by array.array()).\n    '''\n    # Start with the header: <number of dimensions> <dim1-size> <dim2-size> ...\n    header = array('i', [len(m.shape)] + list(m.shape))\n    # Now, we can do the array itself, just flattened\n    body = array(t, m.flatten().tolist())\n    # Wrap bytes if necessary...\n    if sys.byteorder != 'big':\n        header.byteswap()\n        body.byteswap()\n    # And return the result:\n    return bytearray(header.tostring() + body.tostring())", "response": "Serializes a numpy array into a byte stream that can be read by the\n    nben. util. Py4j Java class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_java_doubles(m):\n    '''\n    to_java_doubles(m) yields a java array object for the vector or matrix m.\n    '''\n    global _java\n    if _java is None: _init_registration()\n    m = np.asarray(m)\n    dims = len(m.shape)\n    if dims > 2: raise ValueError('1D and 2D arrays supported only')\n    bindat = serialize_numpy(m, 'd')\n    return (_java.jvm.nben.util.Numpy.double2FromBytes(bindat) if dims == 2\n            else _java.jvm.nben.util.Numpy.double1FromBytes(bindat))", "response": "Returns a java array object for the vector or matrix m."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_java_ints(m):\n    '''\n    to_java_ints(m) yields a java array object for the vector or matrix m.\n    '''\n    global _java\n    if _java is None: _init_registration()\n    m = np.asarray(m)\n    dims = len(m.shape)\n    if dims > 2: raise ValueError('1D and 2D arrays supported only')\n    bindat = serialize_numpy(m, 'i')\n    return (_java.jvm.nben.util.Numpy.int2FromBytes(bindat) if dims == 2\n            else _java.jvm.nben.util.Numpy.int1FromBytes(bindat))", "response": "returns a java array object for the vector or matrix m."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert an array into a Java array.", "response": "def to_java_array(m):\n    '''\n    to_java_array(m) yields to_java_ints(m) if m is an array of integers and to_java_doubles(m) if\n    m is anything else. The numpy array m is tested via numpy.issubdtype(m.dtype, numpy.int64).\n    '''\n    if not hasattr(m, '__iter__'): return m\n    m = np.asarray(m)\n    if np.issubdtype(m.dtype, np.dtype(int).type) or all(isinstance(x, num.Integral) for x in m):\n        return to_java_ints(m)\n    else:\n        return to_java_doubles(m)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the set of indices from the given vertex - set or itable object obj that correspond to the given mask m.", "response": "def to_mask(obj, m=None, indices=None):\n    '''\n    to_mask(obj, m) yields the set of indices from the given vertex-set or itable object obj that\n      correspond to the given mask m.\n    to_mask((obj, m)) is equivalent to to_mask(obj, m).\n    \n    The mask m may take any of the following forms:\n       * a list of vertex indices\n       * a boolean array (one value per vertex)\n       * a property name, which can be cast to a boolean array\n       * a tuple (property, value) where property is a list of values, one per vertex, and value\n         is the value that must match in order for a vertex to be included (this is basically\n         equivalent to the mask (property == value); note that property may also be a property\n         name\n       * a tuple (property, min, max), which specifies that the property must be between min and\n         max for a vertex to be included (min < p <= max)\n       * a tuple (property, (val1, val2...)), which specifies that the property must be any of\n         the values in (val1, val2...) for a vertex to be included\n       * None, indicating that all labels should be returned\n       * a dict/mapping with one item whose key is either 'and', or 'or' and whose value is a list,\n         each of whose elements matches one of the above.\n    \n    Note that the optional argument indices (default: False) may be set to true to yield the\n    vertex indices instead of the vertex labels. If obj is not a VertexSet object, then this\n    option is ignored.\n    '''\n    if not pimms.is_map(obj) and pimms.is_vector(obj) and len(obj) < 4 and m is None:\n        if   len(obj) == 1: obj = obj[0]\n        elif len(obj) == 2: (obj, m) = obj\n        else:\n            (obj, m, q) = obj\n            if indices is None:\n                if pimms.is_map(q): indices = q.get('indices', False)\n                else: indices = q\n    if indices is None: indices = False\n    if is_vset(obj):\n        lbls = obj.labels\n        idcs = obj.indices\n        obj = obj.properties\n    else:\n        obj = pimms.itable(obj)\n        lbls = np.arange(0, obj.row_count, 1, dtype=np.int)\n        idcs = lbls\n    if m is None: return idcs if indices else lbls\n    if is_tuple(m):\n        if len(m) == 0: return np.asarray([], dtype=np.int)\n        p = to_property(obj, m[0])\n        if len(m) == 2 and hasattr(m[1], '__iter__'):\n            m = reduce(lambda q,u: np.logical_or(q, p == u), m[1], np.zeros(len(p), dtype=np.bool))\n        elif len(m) == 2:\n            m = (p == m[1])\n        elif len(m) == 3:\n            m = np.logical_and(m[1] < p, p <= m[2])\n    elif pimms.is_str(m):\n        m = np.asarray(obj[m], dtype=np.bool)\n    elif pimms.is_map(m):\n        if len(m) != 1: raise ValueError('Dicts used as masks must contain 1 item')\n        (k,v) = next(six.iteritems(m))\n        if not hasattr(v, '__iter__'): raise ValueError('Value of dict-mask must be an iterator')\n        if not pimms.is_str(k): raise ValueError('Key of dict-mask must be \"or\", or \"and\"')\n        v = [to_mask(obj, u, indices=indices) for u in v]\n        if   k in ('and', 'intersect', 'intersection', '^', '&', '&&'):\n            return reduce(np.intersect1d, v)\n        elif k in ('or',  'union', 'v', '|' '||'):\n            return reduce(np.union1d, v)\n    # at this point, m should be a boolean array or a list of indices\n    return idcs[m] if indices else lbls[m]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a property array that is a generator that yields the given property array from the given object.", "response": "def to_property(obj, prop=None,\n                dtype=Ellipsis,\n                outliers=None,  data_range=None,    clipped=np.inf,\n                weights=None,   weight_min=0,       weight_transform=Ellipsis,\n                mask=None,      valid_range=None,   null=np.nan,\n                transform=None, yield_weight=False):\n    '''\n    to_property(obj, prop) yields the given property from obj after performing a set of filters on\n      the property, as specified by the options. In the property array that is returned, the values\n      that are considered outliers (data out of some range) are indicated by numpy.inf, and values\n      that are not in the optionally-specified mask are given the value numpy.nan; these may be\n      changed with the clipped and null options, respectively.\n    to_property((obj, prop)) is equivalent to to_property(obj, prop).\n\n    The property argument prop may be either specified as a string (a property name in the object)\n    or as a property vector. The weights option may also be specified this way. Additionally, the\n    prop arg may be a list such as ['polar_angle', 'eccentricity'] where each element is either a\n    string or a vector, in which case the result is a matrix of properties. Finally, prop may be\n    a set of property names, in which case the return value is an itable whose keys are the property\n    names.\n\n    The obj argument may be either a VertexSet object (such as a Mesh or Tesselation) or a mapping\n    object such as a pimms ITable. If no strings are used to specify properties, it may additionally\n    be omitted or set to None.\n\n    The following options are accepted:\n      * outliers (default:None) specifies the vertices that should be considered outliers; this\n        may be either None (no outliers explicitly specified), a list of indices, or a boolean\n        mask.\n      * data_range (default:None) specifies the acceptable data range for values in the\n        property; if None then this paramter is ignored. If specified as a pair of numbers\n        (min, max), then data that is less than the min or greater than the max is marked as an\n        outlier (in addition to other explicitly specified outliers). The values np.inf or \n        -np.inf can be specified to indicate a one-sided range.\n      * clipped (default:np.inf) specifies the value to be used to mark an out-of-range value in\n        the returned array.\n      * mask (default:None) specifies the vertices that should be included in the property \n        array; values are specified in the mask similarly to the outliers option, except that\n        mask values are included rather than excluded. The mask takes precedence over the \n        outliers, in that a null (out-of-mask) value is always marked as null rather than\n        clipped.\n      * valid_range (default: None) specifies the range of values that are considered valid; \n        i.e., values outside of the range are marked as null. Specified the same way as\n        data_range.\n      * null (default: np.nan) specifies the value marked in the array as out-of-mask.\n      * transform (default:None) may optionally provide a function to be passed the array prior\n        to being returned (after null and clipped values are marked).\n      * dtype (defaut:Ellipsis) specifies the type of the array that should be returned.\n        Ellipsis indicates that the type of the given property should be used. If None, then a\n        normal Python array is returned. Otherwise, should be a numpy type such as numpy.real64\n        or numpy.complex128.\n      * weights (default:Ellipsis) specifies the property or property array that should be\n        examined as the weights. The default, Ellipsis, simply chops values that are close to or\n        less than 0 such that they are equal to 0. None specifies that no transformation should\n        be applied.\n      * weight_min (default:0) specifies the value at-or-below which the weight is considered \n        insignificant and the value is marked as clipped.\n      * weight_transform (default:None) specifies a function that should be applied to the\n        weight array before being used in the function.\n      * yield_weight (default:False) specifies, if True, that instead of yielding prop, yield\n        the tuple (prop, weights).\n    \n    '''\n    # was an arg given, or is obj a tuple?\n    if pimms.is_vector(obj) and len(obj) < 4 and prop is None:\n        kw0 = dict(dtype=dtype,           null=null,\n                   outliers=outliers,     data_range=data_range,\n                   clipped=clipped,       weights=weights,\n                   weight_min=weight_min, weight_transform=weight_transform,\n                   mask=mask,             valid_range=valid_range,\n                   transform=transform,   yield_weight=yield_weight)\n        if   len(obj) == 2: return to_property(obj[0], obj[1], **kw0)\n        elif len(obj) == 3: return to_property(obj[0], obj[1], **pimms.merge(kw0, obj[2]))\n        else: raise ValueError('Bad input vector given to to_property()')\n    # we could have been given a property alone or a map/vertex-set and a property\n    if prop is None: raise ValueError('No property given to to_property()')\n    # if it's a vertex-set, we want to note that and get the map\n    if isinstance(obj, VertexSet): (vset, obj) = (obj,  obj.properties)\n    elif pimms.is_map(obj):        (vset, obj) = (None, obj)\n    elif obj is None:              (vset, obj) = (None, None)\n    else: ValueError('Data object given to to_properties() is neither a vertex-set nor a mapping')\n    # Now, get the property array, as an array\n    if pimms.is_str(prop):\n        if obj is None: raise ValueError('a property name but no data object given to to_property')\n        else: prop = obj[prop]\n    if is_set(prop):\n        def _lazy_prop(kk):\n            return lambda:to_property(obj, kk,\n                                      dtype=dtype,           null=null,\n                                      outliers=outliers,     data_range=data_range,\n                                      clipped=clipped,       weights=weights,\n                                      weight_min=weight_min, weight_transform=weight_transform,\n                                      mask=mask,             valid_range=valid_range,\n                                      transform=transform,   yield_weight=yield_weight)\n        return pimms.itable({k:_lazy_prop(k) for k in prop})\n    elif (pimms.is_matrix(prop) or\n          (pimms.is_vector(prop) and all(pimms.is_str(p) or pimms.is_vector(p) for p in prop))):\n        return np.asarray([to_property(obj, k,\n                                       dtype=dtype,           null=null,\n                                       outliers=outliers,     data_range=data_range,\n                                       clipped=clipped,       weights=weights,\n                                       weight_min=weight_min, weight_transform=weight_transform,\n                                       mask=mask,             valid_range=valid_range,\n                                       transform=transform,   yield_weight=yield_weight)\n                           for k in prop])\n    elif not pimms.is_vector(prop):\n        raise ValueError('prop must be a property name or a vector or a combination of these')\n    else: prop = np.asarray(prop)\n    if dtype is Ellipsis:  dtype = prop.dtype\n    # Go ahead and process the weights\n    if pimms.is_str(weights):\n        if obj is None: raise ValueError('a weight name but no data object given to to_property')\n        else: weights = obj[weights]\n    weights_orig = weights\n    if weights is None or weight_min is None: low_weight = np.asarray([], dtype=np.int)\n    else:\n        if weight_transform is Ellipsis:\n            weights = np.array(weights, dtype=np.float)\n            weights[weights < 0] = 0\n        elif weight_transform is not None:\n            weight = weight_transform(np.asarray(weights))\n        if not pimms.is_vector(weights, 'real'):\n            raise ValueError('weights must be a real-valued vector or property name for such')\n        low_weight = (np.asarray([], dtype=np.int) if weight_min is None else\n                      np.where(weights < weight_min)[0])\n    # we can also process the outliers\n    outliers = np.asarray([], dtype=np.int) if outliers is None else np.arange(len(prop))[outliers]\n    outliers = np.union1d(outliers, low_weight) # low-weight vertices are treated as outliers\n    # make sure we interpret mask correctly...\n    mask = to_mask(obj, mask, indices=True)\n    # Now process the property depending on whether the type is numeric or not\n    if pimms.is_array(prop, 'number'):\n        if pimms.is_array(prop, 'int'): prop = np.array(prop, dtype=np.float)\n        else: prop = np.array(prop) # complex or reals can support nan\n        if not np.isnan(null): prop[prop == null] = np.nan\n        mask_nan = np.isnan(prop)\n        mask_inf = np.isinf(prop)\n        where_nan = np.where(mask_nan)[0]\n        where_inf = np.where(mask_inf)[0]\n        where_ok  = np.where(np.logical_not(mask_nan | mask_inf))[0]\n        # look at the valid_range...\n        if valid_range is None: where_inv = np.asarray([], dtype=np.int)\n        else: where_inv = where_ok[(prop[where_ok] < valid_range[0]) |\n                                   (prop[where_ok] > valid_range[1])]\n        where_nan = np.union1d(where_nan, where_inv)\n        mask = np.setdiff1d(mask, where_nan)\n        # Find the outliers: values specified as outliers or inf values; will build this as we go\n        outliers = np.intersect1d(outliers, mask) # outliers not in the mask don't matter anyway\n        # If there's a data range argument, deal with how it affects outliers\n        if data_range is not None:\n            if not pimms.is_vector(data_range): data_range = (0, data_range)\n            mii = mask[(prop[mask] < data_range[0]) | (prop[mask] > data_range[1])]\n            outliers = np.union1d(outliers, mii)\n        # no matter what, trim out the infinite values (even if inf was in the data range)\n        outliers = np.union1d(outliers, mask[np.isinf(prop[mask])])\n        # Okay, mark everything in the prop:\n        unmask = np.setdiff1d(np.arange(len(prop), dtype=np.int), mask)\n        if len(outliers) > 0:  prop[outliers]  = clipped\n        if len(unmask) > 0: prop[unmask] = null\n        prop = prop.astype(dtype)\n    elif len(mask) < len(prop) or len(outliers) > 0:\n        # not a number array; we cannot do fancy trimming of values\n        tmp = np.full(len(prop), null, dtype=dtype)\n        tmp[mask] = prop[mask]\n        if len(outliers) > 0: tmp[outliers] = clipped\n    if yield_weight:\n        if weights is None or not pimms.is_vector(weights): weights = np.ones(len(prop))\n        else: weights = np.array(weights, dtype=np.float)\n        weights[where_nan] = 0\n        weights[outliers] = 0\n    # transform?\n    if transform: prop = transform(prop)\n    # That's it, just return\n    return (prop, weights) if yield_weight else prop"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tess(faces, properties=None, meta_data=None):\n    '''\n    tess(faces) yields a Tesselation object from the given face matrix.\n    '''\n    return Tesselation(faces, properties=properties, meta_data=meta_data)", "response": "Returns a Tesselation object from the given face matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mesh(faces, coordinates, meta_data=None, properties=None):\n    '''\n    mesh(faces, coordinates) yields a mesh with the given face and coordinate matrices.\n    '''\n    return Mesh(faces, coordinates, meta_data=meta_data, properties=properties)", "response": "Returns a mesh with the given face and coordinate matrices."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deduce_chirality(obj):\n    '''\n    deduce_chirality(x) attempts to deduce the chirality of x ('lh', 'rh', or 'lr') and yeilds the\n      deduced string. If no chirality can be deduced, yields None. Note that a if x is either None\n      or Ellipsis, this is converted into 'lr'.\n    '''\n    # few simple tests:\n    try: return obj.chirality\n    except Exception: pass\n    try: return obj.meta_data['chirality']\n    except Exception: pass\n    try: return obj.meta_data['hemi']\n    except Exception: pass\n    try: return obj.meta_data['hemisphere']\n    except Exception: pass\n    if obj is None or obj is Ellipsis: return 'lr'\n    try: return to_hemi_str(obj)\n    except Exception: pass\n    return None", "response": "deduce chiral of the object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_projections_from_path(p):\n    '''\n    load_projections_from_path(p) yields a lazy-map of all the map projection files found in the\n      given path specification p. The path specification may be either a directory name, a\n      :-separated list of directory names, or a python list of directory names.\n\n    In order to be included in the returned mapping of projections, a projection must have a\n    filename that matches the following format:\n      <hemi>.<name>[.mp | .map | .projection | <nothing>].json[.gz]\n    For example, the following match:\n      lh.occipital_pole.mp.json    => map_projections['lh']['occipital_pole']\n      rh.frontal.json.gz           => map_projections['rh']['frontal']\n      lr.motor.projection.json.gz  => map_projections['lr']['motor']\n    '''\n    p = dirpath_to_list(p)\n    return pyr.pmap(\n        {h:pimms.lazy_map(\n            {parts[1]: curry(lambda flnm,h: load_map_projection(flnm, chirality=h),\n                             os.path.join(pp, fl), h)\n             for pp    in p\n             for fl    in os.listdir(pp)  if fl.endswith('.json') or fl.endswith('.json.gz')\n             for parts in [fl.split('.')] if len(parts) > 2 and parts[0] == h})\n         for h in ('lh','rh','lr')})", "response": "Load all the map projections found in the given path specification p."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the given path is valid and updates the neuropythy. map_projections global variable.", "response": "def check_projections_path(path):\n    '''\n    check_projections_path(path) yields the given path after checking that it is valid and updating\n      neuropythy.map_projections to include any projections found on the given path.\n\n    This function is called whenever neuropythy.config['projections_path'] is edited; it should not\n    generally be called directly.\n    '''\n    path = dirpath_to_list(path)\n    tmp = load_projections_from_path(path)\n    # okay, seems like it passed; go ahead and update\n    global map_projections\n    map_projections = pyr.pmap({h: pimms.merge(npythy_map_projections[h], tmp[h])\n                                for h in six.iterkeys(npythy_map_projections)})\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map_projection(name=None, chirality=Ellipsis,\n                   center=Ellipsis, center_right=Ellipsis, radius=Ellipsis,\n                   method=Ellipsis, registration=Ellipsis, sphere_radius=Ellipsis,\n                   pre_affine=Ellipsis, post_affine=Ellipsis, meta_data=Ellipsis, remember=False):\n    '''\n    map_projection(name, hemi) yields the map projection with the given name if it exists; hemi must\n      be either 'lh', 'rh', or 'lr'/None.\n    map_projection(name, topo) yields a map projection using the given topology object topo to\n      determine the hemisphere and assigning to the resulting projection's 'mesh' parameter the\n      appropriate registration from the given topology.\n    map_projection(name, mesh) uses the given mesh; the mesh's meta-data must specify the hemisphere\n      for this to work--otherwise 'lr' is always used as the hemisphere.\n    map_projection(affine, hemi) creates a map projection from the given affine matrix, which must\n      align a set of spherical coordinates to a new set of 3D coordinates that are used as input to\n      the method argument (default method 'orthographic' uses the first two of these coordinates as\n      the x and y values of the map).\n    map_projection() creates a new map projection using the optional arguments, if provided.\n\n    All options that can be passed to load_map_projection and MapProjection can be passed to\n    map_projection:\n      * name is the first optional parameter appearing as name and affine above.\n      * chirality is the second optional parameter, and, if set, will ensure that the resulting\n        map projection's chirality is equivalent to the given chirality.\n      * center specifies the 3D vector that points toward the center of the map.\n      * center_right specifies the 3D vector that points toward any point on the positive x-axis\n        of the resulting map.\n      * radius specifies the radius that should be assumed by the model in radians of the\n        cortical sphere; if the default value (Ellipsis) is given, then pi/3.5 is used.\n      * method specifies the projection method used (default: 'equirectangular').\n      * registration specifies the registration to which the map is aligned (default: 'native').\n      * chirality specifies whether the projection applies to left or right hemispheres.\n      * sphere_radius specifies the radius of the sphere that should be assumed by the model.\n        Note that in Freesurfer, spheres have a radius of 100.\n      * pre_affine specifies the pre-projection affine transform to use on the cortical sphere. Note\n        that if the first (name) argument is provided as an affine transform, then that transform is\n        applied after the pre_affine but before alignment of the center and center_right points.\n      * post_affine specifies the post-projection affine transform to use on the 2D map.\n      * meta_data specifies any additional meta-data to attach to the projection.\n      * remember may be set to True to indicate that, after the map projection is constructed, the\n        map_projection cache of named projections should be updated with the provided name. This\n        can only be used when the provided name or first argument is a string.\n    '''\n    global map_projections # save flag lets us modify this\n    # make a dict of the map parameters:\n    kw = dict(center=center, center_right=center_right, radius=radius,\n              method=method, registration=registration, sphere_radius=sphere_radius,\n              pre_affine=pre_affine, post_affine=post_affine, meta_data=meta_data)\n    kw = {k:v for (k,v) in six.iteritems(kw) if v is not Ellipsis}\n    # interpret the hemi argument first\n    hemi = chirality\n    if hemi is None or hemi is Ellipsis:\n        hemi = 'lr'\n        topo = None\n        mesh = None\n    if pimms.is_str(hemi):\n        hemi = to_hemi_str(hemi)\n        topo = None\n        mesh = None\n    elif is_topo(hemi):\n        topo = hemi\n        hemi = hemi.chirality\n        mesh = None\n    elif is_mesh(hemi):\n        mesh = hemi\n        hemi = deduce_chirality(hemi)\n        topo = None\n    else: raise ValueError('Could not understand map_projection hemi argument: %s' % hemi)\n    hemi = to_hemi_str(hemi)\n    # name might be an affine matrix\n    try:              aff = to_affine(name)\n    except Exception: aff = None\n    if pimms.is_matrix(aff):\n        # see if this is an affine matrix\n        aff = np.asarray(aff)\n        (n,m) = aff.shape\n        mtx = None\n        # might be a transformation into 2-space or into 3-space:\n        if n == 2:\n            if   m == 3: mtx = np.vstack([np.hstack([mtx, [[0,0,0],[0,0,0]]]), [[0],[0],[0],[1]]])\n            elif m == 4: mtx = np.vstack([mtx, [[0,0,0,0], [0,0,0,1]]])\n        elif n == 3:\n            if   m == 3: mtx = np.vstack([np.hstack([mtx, [[0,0,0]]]), [[0],[0],[0],[1]]])\n            elif m == 4: mtx = np.vstack([mtx, [[0,0,0,1]]])\n        elif n == 4:\n            if   m == 4: mtx = aff\n        if mtx is None: raise ValueError('Invalid affine matrix shape; must be {2,3}x{3,4} or 4x4')\n        # Okay, since center and center-right ignore the pre-affine matrix, we can just use this as\n        # the pre-affine and set the center to whatever comes out\n        kw['pre_affine'] = mtx if kw.get('pre_affine') is None else mtx.dot(to_affine(pre_affine))\n        name = None\n    # load name if it's a string\n    if pimms.is_str(name):\n        if   name         in map_projections[hemi]: mp = map_projections[hemi][name]\n        elif name.lower() in map_projections[hemi]: mp = map_projections[hemi][name.lower()]\n        else:\n            try: mp = load_map_projection(name, chirality=hemi)\n            except Exception:\n                raise ValueError('could neither find nor load projection %s (%s)' % (name,hemi))\n        # update parameters if need-be:\n        if len(kw) > 0: mp = mp.copy(**kw)\n    elif name is None:\n        # make a new map_projection\n        if chirality is not Ellipsis: kw['chirality'] = hemi\n        mp = MapProjection(**kw)\n    elif is_map_projection(name):\n        # just updating an existing projection\n        if chirality is not Ellipsis: kw['chirality'] = hemi\n        mp = name\n        if len(kw) > 0: mp = mp.copy(**kw)\n    else: raise ValueError('first argument must be affine, string, or None')\n    # if we have a topology/mesh, we should add it:\n    if topo is not None: mesh = mp.extract_mesh(topo)\n    if mesh is not None: mp   = mp.copy(mesh=mesh)\n    # if save was requested, save it\n    mp = mp.persist()\n    if remember is True:\n        if pimms.is_str(name):\n            name = name.lower()\n            hval = map_projections.get(hemi, pyr.m())\n            map_projections = map_projections.set(hemi, hval.set(name, mp))\n        else: warnings.warn('Cannot save map-projection with non-string name')\n    # okay, return the projection\n    return mp", "response": "Returns a new map projection with the given name chirality center right radius and method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a given argument to a map projection.", "response": "def to_map_projection(arg, hemi=Ellipsis, chirality=Ellipsis,\n                      center=Ellipsis, center_right=Ellipsis, radius=Ellipsis,\n                      method=Ellipsis, registration=Ellipsis, sphere_radius=Ellipsis,\n                      pre_affine=Ellipsis, post_affine=Ellipsis, meta_data=Ellipsis):\n    '''\n    to_map_projection(mp) yields mp if mp is a map projection object.\n    to_map_projection((name, hemi)) is equivalent to map_projection(name, chirality=hemi).\n    to_map_projection((name, opts)) uses the given options dictionary as options to map_projection;\n      (name, hemi, opts) is also allowed as input.\n    to_map_projection(filename) yields the map projection loaded from the given filename.\n    to_map_projection('<name>:<hemi>') is equivalent to to_map_projection(('<name>', '<hemi>')).\n    to_map_projection('<name>') is equivalent to to_map_projection(('<name>', 'lr')).\n    to_map_projection((affine, hemi)) converts the given affine transformation, which must be a\n      transformation from spherical coordinates to 2D map coordinates (once the transformed z-value\n      is dropped), to a map projection. The hemi argument may alternately be an options mapping.\n\n    The to_map_projection() function may also be called with the the elements of the above tuples\n    passed directly; i.e. to_map_projection(name, hemi) is equivalent to\n    to_map_projection((name,hemi)).\n\n    Additionaly, all optional arguments to the map_projection function may be given and will be\n    copied into the map_projection that is returned. Note that the named chirality argument is used\n    to set the chirality of the returned map projection but never to specify the chirality of a\n    map projection that is being looked up or loaded; for that use the second argument, second tuple\n    entry, or hemi keyword.\n    '''\n    kw = dict(center=center, center_right=center_right, radius=radius, chirality=chirality,\n              method=method, registration=registration, sphere_radius=sphere_radius,\n              pre_affine=pre_affine, post_affine=post_affine, meta_data=meta_data)\n    kw = {k:v for (k,v) in six.iteritems(kw) if v is not Ellipsis}\n    if pimms.is_vector(arg):\n        if   len(arg) == 1: arg = arg[0]\n        elif len(arg) == 2:\n            (arg, tmp) = arg\n            if pimms.is_map(tmp):\n                kw = {k:v for (k,v) in six.iteritems(pimms.merge(tmp, kw)) if v is not Ellipsis}\n            elif hemi is Ellipsis: hemi = arg\n        elif len(arg) == 3:\n            (arg, h, opts) = arg\n            kw = {k:v for (k,v) in six.iteritems(pimms.merge(opts, kw)) if v is not Ellipsis}\n            if hemi is Ellipsis: hemi = h\n        else: raise ValueError('Invalid vector argument given to to_map_projection()')\n    hemi = deduce_chirality(hemi)\n    mp = None\n    if   is_map_projection(arg): mp = arg\n    elif pimms.is_str(arg):\n        # first see if there's a hemi appended\n        if ':' in arg:\n            spl = arg.split(':')\n            (a,h) = (':'.join(spl[:-1]), spl[-1])\n            try:\n                (hemtmp, arg) = (to_hemi_str(h), a)\n                if hemi is None: hemi = hemtmp\n            except Exception: pass\n        # otherwise, strings alone might be map projection names or filenames\n        mp = map_projection(arg, hemi)\n    else: raise ValueError('Cannot interpret argument to to_map_projection')\n    if len(kw) == 0: return mp\n    else: return mp.copy(**kw)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_flatmap(name, hemi=Ellipsis, \n               center=Ellipsis, center_right=Ellipsis, radius=Ellipsis, chirality=Ellipsis,\n               method=Ellipsis, registration=Ellipsis, sphere_radius=Ellipsis,\n               pre_affine=Ellipsis, post_affine=Ellipsis, meta_data=Ellipsis):\n    '''\n    to_flatmap(name, topo) yields a flatmap of the given topology topo using the map projection\n      obtained via to_map_projection(name).\n    to_flatmap(name, mesh) yields a flatmap of the given mesh. If no hemisphere is specified in the\n      name argument nor in the mesh meta-data, then 'lr' is assumed.\n    to_flatmap(name, subj) uses the given hemisphere from the given subject.\n    to_flatmap((name, obj)) is equivalent to to_flatmap(name, obj).\n    to_flatmap((name, obj, opts_map)) is equivalent to to_flatmap(name, obj, **opts_map).\n\n    All optional arguments that can be passed to map_projection() may also be passed to the\n    to_flatmap() function and are copied into the map projection object prior to its use in creating\n    the resulting 2D mesh. Note that the named chirality argument is used to set the chirality of\n    the returned map projection but never to specify the chirality of a map projection that is being\n    looked up or loaded; for that use the second argument, second tuple entry, or hemi keyword.\n    '''\n    from neuropythy.mri import is_subject\n    kw = dict(center=center, center_right=center_right, radius=radius, chirality=chirality,\n              method=method, registration=registration, sphere_radius=sphere_radius,\n              pre_affine=pre_affine, post_affine=post_affine, meta_data=meta_data)\n    kw = {k:v for (k,v) in six.iteritems(kw) if v is not Ellipsis}\n    if pimms.is_vector(name):\n        if   len(name) == 1: name = name[0]\n        elif len(name) == 2:\n            (name, tmp) = name\n            if pimms.is_map(tmp):\n                kw = {k:v for (k,v) in six.iteritems(pimms.merge(tmp, kw)) if v is not Ellipsis}\n            else: hemi = tmp\n        elif len(name) == 3:\n            (name, hemi, opts) = name\n            kw = {k:v for (k,v) in six.iteritems(pimms.merge(opts, kw)) if v is not Ellipsis}\n    if 'hemi' in kw:\n        if hemi is Ellipsis: hemi = kw['hemi']\n        del kw['hemi']\n    # okay, we can get the map projection now...\n    mp = to_map_projection(name, hemi, **kw)\n    return mp(hemi)", "response": "Converts a neuropythy object to a flatmap object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef topo(tess, registrations, properties=None, meta_data=None, chirality=None):\n    '''\n    topo(tess, regs) yields a Topology object with the given tesselation object tess and the given\n      registration map regs.\n    '''\n    return Topology(tess, registrations, properties=properties, meta_data=meta_data,\n                    chirarality=chirality)", "response": "Returns a Topology object with the given tesselation object tess and the given registrations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a PathTrace object that represents the given path of points on the given map projection.", "response": "def path_trace(map_projection, pts, closed=False, meta_data=None):\n    '''\n    path_trace(proj, points) yields a path-trace object that represents the given path of points on\n      the given map projection proj.\n    \n    The following options may be given:\n      * closed (default: False) specifies whether the points form a closed loop. If they do form\n        such a loop, the points should be given in the same ordering (counter-clockwise or\n        clockwise) that mesh vertices are given in; usually counter-clockwise.\n      * meta_data (default: None) specifies an optional additional meta-data map to append to the\n        object.\n    '''\n    return PathTrace(map_projection, pts, closed=closed, meta_data=meta_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a path - trace formed by joining the list of path traces pt1 and pt2 at their intersection points.", "response": "def close_path_traces(*args):\n    '''\n    close_path_traces(pt1, pt2...) yields the path-trace formed by joining the list of path traces\n      at their intersection points in the order given. Note that the direction in which each\n      individual path trace's coordinates are specified is ultimately ignored by this function--the\n      only ordering that matters is the order in which the list of paths is given.\n\n    Each path argument may alternately be a curve-spline object or coordinate matrix, so long as all\n    paths and curves track the same 2D space.\n    '''\n    pts = [x for x in args if is_path_trace(x)]\n    if len(pts) == 0:\n        if len(args) == 1 and hasattr(args[0], '__iter__'): return close_path_traces(*args[0])\n        raise ValueError('at least one argument to close_path_traces must be a path trace')\n    mp0 = pts[0].map_projection\n    if not all(mp is None or mp.normalize() == mp0.normalize()\n               for x in pts[1:] for mp in [x.map_projection]):\n        warnings.warn('path traces do not share a map projection')\n    crvs = [x.curve if is_path_trace(x) else to_curve_spline(x) for x in args]\n    loop = close_curves(*crvs)\n    return path_trace(mp0, loop.coordinates, closed=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a lazy map of isoline lines of the given property and value.", "response": "def isolines(obj, prop, val,\n             outliers=None,  data_range=None,  clipped=np.inf,\n             weights=None,   weight_min=0,     weight_transform=Ellipsis,\n             mask=None,      valid_range=None, transform=None,\n             smooth=False,   yield_addresses=False):\n    '''\n    isolines(mesh, prop, val) yields a 2 x D x N array of points that represent the lines of\n      equality of the given property to the given value, according to linear interpolation\n      of the property values over the mesh; D represents the dimensions of the mesh (2D or\n      3D) and N is the number of faces whose prop values surround val.\n    isolines(cortex, prop, val) yields the lines as addresses instead of coordinates.\n    isolines(subject, prop, val) yields a lazy map whose keys are the keys of subject.hemis\n      and whose values are equivalent to isolines(subject.hemis[key], prop, val).\n    \n    The following optional arguments may be given:\n      * yield_addresses (default: False) may be set to True to instruct isolines to return the\n        addresses instead of the coordinates, even for meshes; if the first argument is not a\n        mesh then this argument is ignored.\n      * smooth (default: None) may optionally be True or a value n or a tuple (n,m) that are\n        used as arguments to the smooth_lines() function.\n      * Almost all options that can be passed to the to_property() function can be passed to\n        this function and are in turn passed along to to_property():\n          * outliers       * data_range          * clipped        * weights\n          * weight_min     * weight_transform    * mask           * valid_range\n          * transform\n        Weights, if specified, are not used in the isoline calculation; they are only used for\n        thresholding.\n    '''\n    import scipy.sparse as sps\n    from neuropythy import (is_subject, is_list, is_tuple, is_topo, is_mesh, is_tess)\n    from neuropythy.util import flattest\n    if is_subject(obj):\n        kw = dict(outliers=outliers,     data_range=data_range,\n                  clipped=clipped,       weights=weights,\n                  weight_min=weight_min, weight_transform=weight_transform,\n                  mask=mask,             valid_range=valid_range,\n                  transform=transform,   smooth=smooth,\n                  yield_addresses=yield_addresses)\n        return pimms.lazy_map({h:curry(lambda h: isolines(mesh.hemis[h],prop,val,**kw),h)\n                               for h in six.iterkeys(mesh.hemis)})\n    elif not (is_topo(obj) or is_mesh(obj)):\n        raise ValueError('argument must be a mesh, topology, or subject')\n    if   smooth is True:          smooth = ()\n    elif smooth in [False,None]:  smooth = None\n    elif pimms.is_vector(smooth): smooth = tuple(smooth)\n    else: raise ValueError('unrecognized smooth argument')\n    # find the addresses over the faces:\n    fs = obj.tess.indexed_faces if not is_tess(obj) else obj.indexed_faces\n    N  = obj.vertex_count\n    p  = obj.property(prop,\n                      outliers=outliers,     data_range=data_range,\n                      clipped=clipped,       weights=weights,\n                      weight_min=weight_min, weight_transform=weight_transform,\n                      mask=mask,             valid_range=valid_range,\n                      transform=transform)\n    ii  = np.isfinite(p)\n    fii = np.where(np.sum([ii[f] for f in fs], axis=0) == 3)[0]\n    fs  = fs[:,fii]\n    fp  = np.asarray([p[f] for f in fs])\n    lt  = (fp <= val)\n    slt = np.sum(lt, 0)\n    (ii1,ii2) = [(slt == k) for k in (1,2)]\n    wmtx = sps.lil_matrix((N, N), dtype='float')\n    emtx = sps.dok_matrix((N*N, N*N), dtype='bool')\n    for (ii,fl) in zip([ii1,ii2], [True,False]):\n        tmp = np.array(lt.T)\n        tmp[~ii,:] = False\n        if fl:\n            w = fs.T[tmp]\n            tmp[ii] = ~tmp[ii]\n            (u,v) = np.reshape(fs.T[tmp], (-1,2)).T\n        else:\n            (u,v) = np.reshape(fs.T[tmp], (-1,2)).T\n            tmp[ii] = ~tmp[ii]\n            w = fs.T[tmp]\n        (pu,pv,pw) = [p[q] for q in (u,v,w)]\n        # the line is from somewhere on (u,w) to somewhere on (v,w)\n        (wu, wv) = [(val - pw) / (px - pw) for px in (pu,pv)]\n        # put these in the weight matrix\n        wmtx[u,w] = wu\n        wmtx[v,w] = wv\n        wmtx[w,u] = (1 - wu)\n        wmtx[w,v] = (1 - wv)\n        # and in the edge matrix\n        e1 = np.sort([u,w],axis=0).T.dot([N,1])\n        e2 = np.sort([v,w],axis=0).T.dot([N,1])\n        emtx[e1, e2] = True\n        emtx[e2, e1] = True\n    # okay, now convert these into sequential lines...\n    wmtx = wmtx.tocsr()\n    (rs,cs,xs) = sps.find(emtx)\n    # we need to compress emtx\n    ((rs,ridx),(cs,cidx)) = [np.unique(x, return_inverse=True) for x in (rs,cs)]\n    assert(len(ridx) == len(cidx) and len(cidx) == len(xs) and np.array_equal(rs,cs))\n    em = sps.csr_matrix((xs, (ridx,cidx)), dtype='bool')\n    ecounts = flattest(em.sum(axis=1))\n    ccs = sps.csgraph.connected_components(em, directed=False)[1]\n    lines = {}\n    loops = {}\n    for (lbl,st) in zip(*np.unique(ccs, return_index=True)):\n        if lbl < 0: continue\n        o0 = sps.csgraph.depth_first_order(em, st, directed=False)[0]\n        o = rs[o0]\n        (u,v) = (np.floor(o/N).astype('int'), np.mod(o, N).astype('int'))\n        w = flattest(wmtx[u,v])\n        if ecounts[o0[-1]] == 2:\n            loops[lbl] = (u,v,w)\n        else:\n            if ecounts[st] != 1:\n                o0 = sps.csgraph.depth_first_order(em, o0[-1], directed=False)[0]\n                o = rs[o0]\n                (u,v) = (np.floor(o/N).astype('int'), np.mod(o, N).astype('int'))\n                w = flattest(wmtx[u,v])\n            lines[lbl] = (u,v,w)\n    # okay, we add all of these to the addrs that we return; it's potentially\n    # more useful to have each face's full crossing in its own BC coordinates\n    # at times, and the duplicates can be eliminated with just a slice\n    addrs = []\n    for (lbl,(us,vs,ws)) in six.iteritems(lines):\n        (u0s,v0s,u1s,v1s) = (us[:-1],vs[:-1],us[1:],vs[1:])\n        qs = [np.setdiff1d([u1,v1], [u0,v0])[0] for (u0,v0,u1,v1) in zip(u0s,v0s,u1s,v1s)]\n        fs = np.asarray([u0s, qs, v0s], dtype=np.int)\n        fend = (u1s[-1], np.setdiff1d(fs[:,-1], (u1s[-1],v1s[-1])), v1s[-1])\n        fs = np.hstack([fs, np.reshape(fend, (3,1))])\n        # convert faces back to labels\n        fs = np.asarray([obj.labels[f] for f in fs])\n        # make the weights\n        ws = np.asarray([ws, 0*ws])\n        addrs.append({'faces': fs, 'coordinates': ws})\n    addrs = list(sorted(addrs, key=lambda a:a['faces'].shape[0]))\n    # if obj is a topology or addresses were requested, return them now\n    if yield_addresses or not is_mesh(obj): return addrs\n    # otherwise, we now convert these into coordinates\n    xs = [obj.unaddress(addr) for addr in addrs]\n    if smooth is not None: xs = smooth_lines(xs, *smooth)\n    return xs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_tess(obj):\n    '''\n    to_tess(obj) yields a Tesselation object that is equivalent to obj; if obj is a tesselation\n      object already and no changes are requested (see options) then obj is returned unmolested.\n\n    The following objects can be converted into tesselations:\n      * a tesselation object\n      * a mesh or topology object (yields their tess objects)\n      * a 3 x n or n x 3 matrix of integers (the faces)\n      * a tuple of coordinates and faces that can be passed to to_mesh\n    '''\n    if   is_tess(obj): return obj\n    elif is_mesh(obj): return obj.tess\n    elif is_topo(obj): return obj.tess\n    else:\n        # couple things to try: (1) might specify a tess face matrix, (2) might be a mesh-like obj\n        try:    return tess(obj)\n        except Exception: pass\n        try:    return to_mesh(obj).tess\n        except Exception: pass\n    raise ValueError('Could not convert argument to tesselation object')", "response": "Converts an object into a Tesselation object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts obj into a mesh object.", "response": "def to_mesh(obj):\n    '''\n    to_mesh(obj) yields a Mesh object that is equivalent to obj or identical to obj if obj is itself\n      a mesh object.\n\n    The following objects can be converted into meshes:\n      * a mesh object\n      * a tuple (coords, faces) where coords is a coordinate matrix and faces is a matrix of\n        coordinate indices that make-up the triangles\n      * a tuple (faces, coords) where faces is a triangle matrix and coords is a coordinate matrix;\n        note that if neither matrix is of integer type, then the latter ordering (which is the same\n        as that accepted by the mesh() function) is assumed.\n      * a tuple (topo, regname) specifying the registration name to use (note that regname may\n        optionally start with 'reg:' which is ignored).\n      * a tuple (cortex, surfname) specifying the surface name to use. Note that surfname may\n        optionally start with 'surf:' or 'reg:', both of which are used only to determine whether\n        to lookup a registration or a surface. If no 'surf:' or 'reg:' is given as a prefix, then\n        a surface is tried first followed by a registration. The surface name 'sphere' is\n        automatically translated to 'reg:native' and any surface name of the form '<name>_sphere' is\n        automatically translated to 'reg:<name>'.\n      * a tuple (topo/cortex, mesh) results in the mesh being returned.\n      * a tuple (mesh, string) or (mesh, None) results in mesh with the second argument ignored.\n      * a tuple (mesh1, mesh2) results in mesh2 with mesh1 ignored.\n\n    Note that some of the behavior described above is desirable because of a common use case of the\n    to_mesh function. When another function f accepts as arguments both a hemi/topology object as\n    well as an optional surface argument, the purpose is often to obtain a specific mesh from the\n    topology but to allow the user to specify which or to pass their own mesh.\n    '''\n    if   is_mesh(obj): return obj\n    elif pimms.is_vector(obj) and len(obj) == 2:\n        (a,b) = obj\n        if   pimms.is_matrix(a, 'int') and pimms.is_matrix(b, 'real'): return mesh(a, b)\n        elif pimms.is_matrix(b, 'int') and pimms.is_matrix(a, 'real'): return mesh(b, a)\n        elif is_mesh(a) and (b is None or pimms.is_str(b)): return a\n        elif is_mesh(a) and is_mesh(b): return b\n        elif is_topo(a):\n            from neuropythy import is_cortex\n            if   is_mesh(b):          return b\n            elif not pimms.is_str(b): raise ValueError('to_mesh: non-str surf/reg name: %s' % (b,))\n            (b0, lb) = (b, b.lower())\n            # check for translations of the name first:\n            s = b[4:] if lb.startswith('reg:') else b[5:] if lb.startswith('surf:') else b\n            ls = s.lower()\n            if ls.endswith('_sphere'): b = ('reg:' + s[:-7])\n            elif ls == 'sphere': b = 'reg:native'\n            lb = b.lower()\n            # we try surfaces first (if a is a cortex and has surfaces)\n            if is_cortex(a) and not lb.startswith('reg:'):\n                (s,ls) = (b[5:],lb[5:]) if lb.startswith('surf:') else (b,lb)\n                if   s  in a.surfaces: return a.surfaces[s]\n                elif ls in a.surfaces: return a.surfaces[ls]\n            # then check registrations\n            if not lb.startswith('surf:'):\n                (s,ls) = (b[4:],lb[4:]) if lb.startswith('reg:') else (b,lb)\n                if   s  in a.registrations: return a.registrations[s]\n                elif ls in a.registrations: return a.registrations[ls]\n            # nothing found\n            raise ValueError('to_mesh: mesh named \"%s\" not found in topology %s' % (b0, a))\n        else: raise ValueError('to_mesh: could not deduce meaning of row: %s' % (obj,))\n    else: raise ValueError('Could not deduce how object can be convertex into a mesh')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a gifti file into a new object.", "response": "def load_gifti(filename, to='auto'):\n    '''\n    load_gifti(filename) yields the nibabel gifti data structure loaded by nibabel from the given\n      filename. Currently, this load method is not particlarly sophisticated and simply returns this\n      data.\n    \n    The optional argument to may be used to coerce the resulting data to a particular format; the\n    following arguments are understood:\n      * 'auto' currently returns the nibabel data structure.\n      * 'mesh' returns the data as a mesh, assuming that there are two darray elements stored in the\n        gifti file, the first of which must be a coordinate matrix and a triangle topology.\n      * 'coordinates' returns the data as a coordinate matrix.\n      * 'tesselation' returns the data as a tesselation object.\n      * 'raw' returns the entire gifti image object (None will also yield this result).\n    '''\n    dat = nib.load(filename)\n    to = 'raw' if to is None else to.lower()\n    if to in ['raw', 'image', 'gifti', 'all', 'full']:\n        return dat\n    if to in ['auto', 'automatic']:\n        # is this a mesh gifti?\n        pset = dat.get_arrays_from_intent('pointset')\n        tris = dat.get_arrays_from_intent('triangle')\n        if len(pset) == 1 and len(tris) == 1:\n            (cor, tri) = (pset[0].data, tris[0].data)\n            # okay, try making it:\n            try: return Mesh(tri, cor)\n            except Exception: pass\n        elif len(pset) == 1 and len(tris) == 0:\n            # just a pointset\n            return pset[0].data\n        elif len(tris) == 1 and len(pset) == 0:\n            # Just a topology...\n            return Tesselation(tris[0].data)\n        # Maybe it's a stat? If so, we want to return the data array...\n        # see the nifti1 header for these numbers, but stats are intent 2-24\n        stats = [v for k in range(2,25) for v in dat.get_arrays_from_intent(k)]\n        if len(stats) == 1: return np.squeeze(stats[0].data)\n        # most other possibilities are also basic arrays, so if there's only one of them, we can\n        # just yield that array\n        if len(dat.darrays) == 1: return np.squeeze(dat.darrays[0].data)\n        # We don't know what it is; return the whole thing:\n        return dat\n    elif to in ['coords', 'coordinates', 'xyz']:\n        cor = dat.darrays[0].data\n        if pimms.is_matrix(cor, np.inexact): return cor\n        else: raise ValueError('give gifti file did not contain coordinates')\n    elif to in ['tess', 'tesselation', 'triangles', 'tri', 'triangulation']:\n        cor = dat.darrays[0].data\n        if pimms.is_matrix(cor, 'int'): return Tesselation(cor)\n        else: raise ValueError('give gifti file did not contain tesselation')\n    elif to in ['mesh']:\n        if len(dat.darrays) == 2: (cor,    tri) = dat.darrays\n        else:                     (cor, _, tri) = dat.darrays\n        cor = cor.data\n        tri = tri.data\n        # possible that these were given in the wrong order:\n        if pimms.is_matrix(tri, np.inexact) and pimms.is_matrix(cor, 'int'):\n            (cor,tri) = (tri,cor)\n        # okay, try making it:\n        return Mesh(tri, cor)\n    else: raise ValueError('option \"to\" given to load_gift could not be understood')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef empirical_retinotopy_data(hemi, retino_type):\n    '''\n    empirical_retinotopy_data(hemi, t) yields a numpy array of data for the given cortex object hemi\n    and retinotopy type t; it does this by looking at the properties in hemi and picking out any\n    combination that is commonly used to denote empirical retinotopy data. These common names are\n    stored in _empirical_retintopy_names, in order of preference, which may be modified.\n    The argument t should be one of 'polar_angle', 'eccentricity', 'weight'.\n    '''\n    dat = _empirical_retinotopy_names[retino_type.lower()]\n    hdat = {s.lower(): s for s in six.iterkeys(hemi.properties)}\n    return next((hemi.prop(hdat[s.lower()]) for s in dat if s.lower() in hdat), None)", "response": "This function returns a numpy array of data for the given cortex object hemi and retinotopy type t."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef basic_retinotopy_data(hemi, retino_type):\n    '''\n    basic_retinotopy_data(hemi, t) yields a numpy array of data for the given cortex object hemi\n    and retinotopy type t; it does this by looking at the properties in hemi and picking out any\n    combination that is commonly used to denote empirical retinotopy data. These common names are\n    stored in _predicted_retintopy_names, in order of preference, which may be modified.\n    The argument t should be one of 'polar_angle', 'eccentricity', 'visual_area', or 'weight'.\n    Unlike the related functions empirical_retinotopy_data and predicted_retinotopy_data, this\n    function calls both of these (predicted first then empirical) in the case that it does not\n    find a valid property.\n    '''\n    dat = _retinotopy_names[retino_type.lower()]\n    val = next((hemi.prop(s) for s in six.iterkeys(hemi.properties) if s.lower() in dat), None)\n    if val is None and retino_type.lower() != 'weight':\n        val = predicted_retinotopy_data(hemi, retino_type)\n    if val is None and retino_type.lower() != 'visual_area':\n        val = empirical_retinotopy_data(hemi, retino_type)\n    return val", "response": "basic_retinotopy_data yields a numpy array of data for the given cortex object hemi\n    and retinotopy type t."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_retinotopy_argument(obj, retino_type, arg, default='any'):\n    '''\n    extract_retinotopy_argument(o, retino_type, argument) yields retinotopy data of the given\n    retinotopy type (e.g., 'polar_angle', 'eccentricity', 'variance_explained', 'visual_area',\n    'weight') from the given hemisphere or cortical mesh object o, according to the given\n    argument. If the argument is a string, then it is considered a property name and that is\n    returned regardless of its value. If the argument is an iterable, then it is returned. If\n    the argument is None, then retinotopy will automatically be extracted, if found, by calling\n    the retinotopy_data function.\n    The option default (which, by default, is 'any') specifies which function should be used to\n    extract retinotopy in the case that the argument is None. The value 'any' indicates that the\n    function retinotopy_data should be used, while the values 'empirical' and 'predicted' specify\n    that the empirical_retinotopy_data and predicted_retinotopy_data functions should be used,\n    respectively.\n    '''\n    if   pimms.is_str(arg):        values = obj.prop(arg)\n    elif hasattr(arg, '__iter__'): values = arg\n    elif arg is not None:          raise ValueError('cannot interpret retinotopy arg: %s' % arg)\n    elif default == 'predicted':   values = predicted_retinotopy_data(obj, retino_type)\n    elif default == 'empirical':   values = empirical_retinotopy_data(obj, retino_type)\n    elif default == 'any':         values = basic_retinotopy_data(obj, retino_type)\n    else:                          raise ValueError('bad default retinotopy: %s' % default)\n    if values is None:\n        raise RuntimeError('No %s retinotopy data found given argument: %s' % (retino_type, arg))\n    n = obj.vertex_count\n    values = np.asarray(values)\n    if len(values) != n:\n        found = False\n        # could be that we were given a mesh data-field for a map\n        try:              values = values[obj.labels]\n        except Exception: values = None\n        if values is None:\n            raise RuntimeError('%s data: length %s should be %s' % (retino_type, len(values), n))\n    return values", "response": "Extracts the retinotopy data of the given retinotopy type from the given hemisphere or cortical mesh object o according to the given argument."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_retinotopy(data, output_style='visual', units=Ellipsis, prefix=None, suffix=None):\n    '''\n    as_retinotopy(data) converts the given data, if possible, into a 2-tuple, (polar_angle, eccen),\n      both in degrees, with 0 degrees of polar angle corresponding to the upper vertical meridian\n      and negative values corresponding to the left visual hemifield.\n    as_retinotopy(data, output_style) yields the given retinotopy data in the given output_style;\n      as_retinotopy(data) is equivalent to as_retinotopy(data, 'visual').\n\n    This function is intended as a general conversion routine between various sources of retinotopy\n    data. All lookups are done in a case insensitive manner. Data may be specified in any of the\n    following ways:\n      * A cortical mesh containing recognized properties (such as 'polar_angle' and 'eccentricity'\n        or 'latitude' and 'longitude'.\n      * A dict with recognized fields.\n      * A tuple of (polar_angle, eccentricity) (assumed to be in 'visual' style).\n      * A numpy vector of complex numbers (assumed in 'complex' style).\n      * An n x 2 or 2 x n matrix whose rows/columns are (polar_angle, eccentricity) values (assumed\n        in 'visual' style).\n\n    The following output_styles are accepted:\n      * 'visual':       polar-axis:         upper vertical meridian\n                        positive-direction: clockwise\n                        fields:             ['polar_angle' (degrees), 'eccentricity' (degrees)]\n      * 'spherical':    polar-axis:         right horizontal meridian\n                        positive-direction: counter-clockwise\n                        fields:             ['theta' (radians), 'rho' (radians)]\n      * 'standard':     polar-axis:         right horizontal meridian\n                        positive-direction: counter-clockwise\n                        fields:             ['angle' (radians), 'eccentricity' (degrees)]\n      * 'cartesian':    axes:               x/y correspond to RHM/UVM\n                        positive-direction: left/up\n                        fields:             ('x' (degrees), 'y' (degrees))\n      * 'geographical': axes:               x/y correspond to RHM/UVM\n                        positive-direction: left/up\n                        fields:             ('longitude' (degrees), 'latitude' (degrees))\n      * 'complex':      axes:               x/y correspond to RHM/UVM\n                        positive-direction: left/up\n                        fields:             longitude (degrees) + I*latitude (degrees)\n      * 'complex-rad':  axes:               x/y correspond to RHM/UVM\n                        positive-direction: left/up\n                        fields:             longitude (radians) + I*latitude (radians)\n      * 'visual-rad':   polar-axis:         upper vertical meridian\n                        positive-direction: clockwise\n                        fields:             ['angle' (radians), 'eccentricity' (radians)]\n\n    The following options may be given:\n      * units (Ellipsis) specifies the unit that should be assumed (degrees or radians);\n        if Ellipsis is given, then auto-detect the unit if possible. This may be a map whose keys\n        are 'polar_angle' and 'eccentricity' (or the equivalent titles in data) and whose keys are\n        the individual units.\n      * prefix (None) specifies a prefix that is required for any keys or property names.\n      * suffix (None) specifies a suffix that is required for any keys or property names.\n    '''\n    # simple sanity check:\n    output_style = output_style.lower()\n    if output_style not in _retinotopy_style_fns:\n        raise ValueError('Unrecognized output style: %s' % output_style)\n    # First step: get the retinotopy into a format we can deal with easily\n    if is_tuple(data) and len(data) == 2:\n        data = {'polar_angle': data[0], 'eccentricity': data[1]}\n    if isinstance(data, list):\n        data = np.asarray(data)\n    if pimms.is_nparray(data):\n        if pimms.is_vector(data, np.complexfloating):\n            data = {'complex': data}\n        else:\n            if data.shape[0] != 2: data = data.T\n            data = {'polar_angle': data[0], 'eccentricity': data[1]}\n    # We now assume that data is a dict type; or is a mesh;\n    # figure out the data we have and make it into theta/rho\n    if isinstance(data, geo.VertexSet):\n        pnames = {k.lower():k for k in six.iterkeys(data.properties)}\n        mem_dat = lambda k: k in pnames\n        get_dat = lambda k: data.prop(pnames[k])\n    else:\n        def _make_lambda(data,k): return lambda:data[k]\n        data = pimms.lazy_map({k.lower():_make_lambda(data,k) for k in six.iterkeys(data)})\n        mem_dat = lambda k: k in data\n        get_dat = lambda k: data[k]\n    # Check in a particular order:\n    suffix = '' if suffix is None else suffix.lower()\n    prefix = '' if prefix is None else prefix.lower()\n    (angle_key, eccen_key, x_key, y_key, z_key) = [\n        next((k for k in aliases if mem_dat(prefix + k + suffix)), None)\n        for aliases in [['polar_angle', 'polar angle', 'angle', 'ang', 'polang', 'theta'],\n                        ['eccentricity', 'eccen', 'ecc', 'rho'],\n                        ['x', 'longitude', 'lon'], ['y', 'latitude', 'lat'],\n                        ['z', 'complex', 'complex-rad', 'coordinate']]]\n    rad2deg = 180.0 / np.pi\n    deg2rad = np.pi / 180.0\n    (hpi, dpi) = (np.pi / 2.0, np.pi * 2.0)\n    if angle_key and eccen_key:\n        akey = prefix + angle_key + suffix\n        ekey = prefix + eccen_key + suffix\n        theta = np.asarray(get_dat(akey))\n        rho   = np.asarray(get_dat(ekey))\n        theta = theta * (deg2rad if _default_polar_angle_units[angle_key]  == 'deg' else 1)\n        rho   = rho   * (rad2deg if _default_eccentricity_units[eccen_key] == 'rad' else 1)\n        if _default_polar_angle_axis[angle_key] == 'UVM': theta = theta - hpi\n        if _default_polar_angle_dir[angle_key] == 'cw':   theta = -theta\n        ok = np.where(np.isfinite(theta))[0]\n        theta[ok[theta[ok] < -np.pi]] += dpi\n        theta[ok[theta[ok] >  np.pi]] -= dpi\n    elif x_key and y_key:\n        (x,y) = [np.asarray(get_dat(prefix + k + suffix)) for k in [x_key, y_key]]\n        if _default_x_units[x_key] == 'rad': x *= rad2deg\n        if _default_y_units[y_key] == 'rad': y *= rad2deg\n        theta = np.arctan2(y, x)\n        rho   = np.sqrt(x*x + y*y)\n    elif z_key:\n        z = get_dat(prefix + z_key + suffix)\n        theta = np.angle(z)\n        rho   = np.abs(z)\n        if _default_z_units[z_key] == 'rad': rho *= rad2deg\n    else:\n        raise ValueError('could not identify a valid retinotopic representation in data')\n    # Now, we just have to convert to the requested output style\n    f = _retinotopy_style_fns[output_style]\n    return f(theta, rho)", "response": "This function converts the given data into a cortical mesh and returns the resulting data in the given output_style."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary containing the data for the given source.", "response": "def retinotopy_data(m, source='any'):\n    '''\n    retinotopy_data(m) yields a dict containing a retinotopy dataset with the keys 'polar_angle',\n      'eccentricity', and any other related fields for the given retinotopy type; for example,\n      'pRF_size' and 'variance_explained' may be included for measured retinotopy datasets and\n      'visual_area' may be included for atlas or model datasets. The coordinates are always in the\n      'visual' retinotopy style, but can be reinterpreted with as_retinotopy.\n    retinotopy_data(m, source) may be used to specify a particular source for the data; this may be\n      either 'empirical', 'model', or 'any'; or it may be a prefix/suffix beginning/ending with\n      an _ character.\n    '''\n    if pimms.is_map(source):\n        if all(k in source for k in ['polar_angle', 'eccentricity']): return source\n    if geo.is_vset(m): return retinotopy_data(m.properties, source=source)\n    source = source.lower()\n    model_rets = ['predicted', 'model', 'template', 'atlas', 'inferred']\n    empir_rets = ['empirical', 'measured', 'prf', 'data']\n    wild = False\n    extra_fields = {'empirical':('radius','variance_explained'), 'model':('radius','visual_area')}\n    check_fields = []\n    if source in empir_rets:\n        fixes = empir_rets\n        check_fields = extra_fields['empirical']\n    elif source in model_rets:\n        fixes = model_rets\n        check_fields = extra_fields['model']\n    elif source in ['any', '*', 'all']:\n        fixes = model_rets + empir_rets\n        check_fields = extra_fields['model'] + extra_fields['empirical']\n        wild = True\n    elif source in ['none', 'basic']:\n        fixes = []\n        check_fields = extra_fields['model'] + extra_fields['empirical']\n        wild = True\n    else: fixes = []\n    # first, try all the fixes as prefixes then suffixes\n    (z, prefix, suffix) = (None, None, None)\n    if wild:\n        try: z = as_retinotopy(m, 'visual')\n        except Exception: pass\n    for fix in fixes:\n        if z: break\n        try:\n            z = as_retinotopy(m, 'visual', prefix=(fix + '_'))\n            prefix = fix + '_'\n        except Exception: pass\n    for fix in fixes:\n        if z: break\n        try:\n            z = as_retinotopy(m, 'visual', suffix=('_' + fix))\n            suffix = fix + '_'\n        except Exception: pass\n    # if none of those worked, try with no prefix/suffix\n    if not z:\n        try:\n            pref = source if source.endswith('_') else (source + '_')\n            z = as_retinotopy(m, 'visual', prefix=pref)\n            prefix = pref\n            check_fields = extra_fields['model'] + extra_fields['empirical']\n        except Exception:\n            raise\n            try:\n                suff = source if source.startswith('_') else ('_' + source)\n                z = as_retinotopy(m, 'visual', suffix=suff)\n                suffix = suff\n                check_fields = extra_fields['model'] + extra_fields['empirical']\n            except Exception: pass\n    # if still not z... we couldn't figure it out\n    if not z: raise ValueError('Could not find an interpretation for source %s' % source)\n    # okay, we found it; make it into a dict\n    res = {'polar_angle': z[0], 'eccentricity': z[1]}\n    # check for extra fields if relevant\n    pnames = {k.lower():k for k in m} if check_fields else {}\n    for fname in set(check_fields):\n        for (aliases, trfn) in retinotopic_property_aliases.get(fname, []):\n            if trfn is None: trfn = lambda x:x\n            for f in aliases:\n                if prefix: f = prefix + f\n                if suffix: f = f + suffix\n                f = f.lower()\n                if f in pnames:\n                    res[fname] = trfn(m[pnames[f]])\n                    trfn = None\n                    break\n            if trfn is None: break\n    # That's it\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npredicting the pRF size for a patch of cortex at the given eccentricity and area.", "response": "def predict_pRF_radius(eccentricity, visual_area='V1', source='Wandell2015'):\n    '''\n    predict_pRF_radius(eccentricity) yields an estimate of the pRF size for a patch of cortex at the\n      given eccentricity in V1.\n    predict_pRF_radius(eccentricity, area) yields an estimate in the given visual area (may be given\n      by the keyword visual_area).\n    predict_pRF_radius(eccentricity, area, source) uses the given source to estimate the pRF size\n      (may be given by the keyword source).\n\n    The following visual areas can be specified:\n      * 'V1' (default), 'V2', 'V3'\n      * 'hV4'\n      * 'V3a', 'V3b'\n      * 'VO1', 'VO2'\n      * 'LO1', 'LO2'\n      * 'TO1', 'TO2'\n\n    The following sources may be given:\n      * 'Wandell2015': Wandell BA, Winawer J (2015) Computational neuroimaging and population\n                       receptive fields. Trends Cogn Sci. 19(6):349-57.\n                       doi:10.1016/j.tics.2015.03.009.\n      * 'Kay2013: Kay KN, Winawer J, Mezer A, Wandell BA (2013) Compressive spatial summation in\n                  human visual cortex. J Neurophysiol. 110(2):481-94.\n    The default source is 'Wandell2015'.\n    '''\n    visual_area = visual_area.lower()\n    if pimms.is_str(source):\n        source = source.lower()\n        if source not in pRF_data:\n            raise ValueError('Given source (%s) not found in pRF-size database' % source)\n        dat = pRF_data[source]\n        dat = dat[visual_area]\n    else:\n        dat = {'m':source[0], 'b':source[1]}\n    return dat['m']*eccentricity + dat['b']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfits a pRF radius to the pRF radius and yields the pRF radius.", "response": "def fit_pRF_radius(ctx, retinotopy=Ellipsis, mask=None, weight=Ellipsis, slope_only=False):\n    '''\n    fit_pRF_radius(ctx) fits a line, m*eccen + b, to the pRF radius and yields the tuple (m, b).\n\n    The following options may be given:\n      * retinotopy (default: Ellipsis) specifies the prefix for the retinotopy (passed to\n        retinotopy_data() to find the retinotopic dataset).\n      * mask (default: None) specifies the mask over which to perform the calculation. This is\n        passed to the to_mask() function. In the case that mask is a set or frozenset, then it is\n        treated as a conjunction (intersection) of masks.\n      * weight (default: None) specifies that a weight should be used; if this is True or Ellipsis,\n        will use the variance_explained if it is part of the retinotopy dataset; if this is False or\n        None, uses no weight; otherwise, this must be a weight property or property name.\n      * slope_only (default: False) may be set to True to instead fit radius = m*eccen and return\n        only m.\n    '''\n    rdat = retinotopy_data(ctx, retinotopy)\n    if 'radius' not in rdat: raise ValueError('No pRF radius found in dataset %s' % retinotopy)\n    rad = rdat['radius']\n    (ang,ecc) = as_retinotopy(rdat, 'visual')\n    if isinstance(mask, (set, frozenset)):\n        mask = reduce(np.intersect1d, [ctx.mask(m, indices=True) for m in mask])\n    else: mask = ctx.mask(mask, indices=True)\n    # get a weight if provided:\n    if weight in [False, None]: wgt = np.ones(rad.shape)\n    elif weight in [True, Ellipsis]:\n        if 'variance_explained' in rdat: wgt = rdat['variance_explained']\n        else: wgt = np.ones(rad.shape)\n    else: wgt = ctx.property(weight)\n    # get the relevant eccen and radius values\n    (ecc,rad,wgt) = [x[mask] for x in (ecc,rad,wgt)]\n    # fit a line...\n    if slope_only:\n        ecc = np.reshape(ecc * wgt, (len(ecc), 1))\n        rad = np.reshape(rad * wgt, (len(rad), 1))\n        return np.linalg.lstsq(ecc, rad)[0]\n    else:\n        return tuple(np.polyfit(ecc, rad, 1, w=wgt))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retinotopic_field_sign(m, element='vertices', retinotopy=Ellipsis, invert_field=False):\n    '''\n    retinotopic_field_sign(mesh) yields a property array of the field sign of every vertex in the \n    mesh m; this value may not be exactly 1 (same as VF) or -1 (mirror-image) but some value\n    in-between; this is because the field sign is calculated exactly (1, 0, or -1) for each triangle\n    in the mesh then is average onto the vertices. To get only the triangle field signs, use\n    retinotopic_field_sign(m, 'triangles').\n\n    The following options are accepted:\n      * element ('vertices') may be 'vertices' to specify that the vertex signs should be returned\n        or 'triangles' (or 'faces') to specify that the triangle field signs should be returned.\n      * retinotopy (Ellipsis) specifies the retinotopic dataset to be used. If se to 'empirical' or\n        'predicted', the retinotopy data is auto-detected from the given categories; if set to\n        Ellipsis, a property pair like 'polar_angle' and 'eccentricity' or 'lat' and 'lon' are\n        searched for using the as_retinotopy function; otherwise, this may be a retinotopy dataset\n        recognizable by as_retinotopy.\n      * invert_field (False) specifies that the inverse of the field sign should be returned.\n    '''\n    tsign = _retinotopic_field_sign_triangles(m, retinotopy)\n    t = m.tess if isinstance(m, geo.Mesh) or isinstance(m, geo.Topology) else m\n    if invert_field: tsign = -tsign\n    element = element.lower()\n    if element == 'triangles' or element == 'faces': return tsign\n    vfs = t.vertex_faces\n    vfs = np.asarray([np.mean(tsign[list(ii)]) if len(ii) > 0 else 0 for ii in vfs])\n    return vfs", "response": "Return the field sign of every vertex in the given mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retinotopy_model(name='benson17', hemi=None,\n                     radius=np.pi/2.5, sphere_radius=100.0,\n                     search_paths=None, update=False):\n    '''\n    retinotopy_model() yields a standard retinotopy model of V1, V2, and V3 as well as other areas\n    (depending on the options). The model itself is represented as a RegisteredRetinotopyModel\n    object, which may internally store a set of meshes with values at the vertices that define the\n    polar angle and eccentricity, or as another object (such as with the SchiraModel). The mesh\n    models are loaded from files in the neuropythy lib directory. Because the model's class is\n    RegisteredRetinotopyModel, so the details of the model's 2D projection onto the cortical surface\n    are included in the model.\n    \n    The following options may be given:\n      * name (default: 'benson17') indicates the name of the model to load; the Benson17 model is\n        included with the neuropythy library along with various others. If name is a filename, this\n        file is loaded (must be a valid fmm or fmm.gz file). Currently, models that are included\n        with neuropythy are: Benson17, Benson17-uncorrected, Schira10, and Benson14 (which is\n        identical to Schira10, as Schira10 was used by Benson14).\n      * hemi (default: None) specifies that the model should go with a particular hemisphere, either\n        'lh' or 'rh'. Generally, model files are names lh.<model>.fmm.gz or rh.<model>.fmm.gz, but\n        models intended for the fsaverage_sym don't necessarily get a prefix. Note that you can\n        leave this as none and just specify that the model name is 'lh.model' instead.\n      * radius, sphere_radius (defaults: pi/2.5 and 100.0, respectively) specify the radius of the\n        projection (on the surface of the sphere) and the radius of the sphere (100 is the radius\n        for Freesurfer spheres). See neuropythy.registration.load_fmm_model for mode details.\n      * search_paths (default: None) specifies directories in which to look for fmm model files. No\n        matter what is included in these files, the neuropythy library's folders are searched last.\n    '''\n    origname = name\n    tup = (name,hemi,radius,sphere_radius)\n    if tup in retinotopy_model.cache:\n        return retinotopy_model.cache[tup]\n    if os.path.isfile(name):\n        fname = name\n        name = None\n    elif name.lower() in ['schira', 'schira10', 'schira2010', 'benson14', 'benson2014']:\n        tmp = get_default_schira_model()\n        retinotopy_model.cache[tup] = tmp\n        return tmp\n    else:\n        name = name if hemi is None else ('%s.%s' % (hemi.lower(), name))\n        if len(name) > 4 and name[-4:] == '.fmm':\n            fname = name\n            name = name[:-4]\n        elif len(name) > 7 and name[-7:] == '.fmm.gz':\n            fname = name\n            name = name[:-7]\n        else:\n            fname = name + '.fmm'\n            # Find it in the search paths...\n            spaths = ([] if search_paths is None else search_paths) + _retinotopy_model_paths\n            fname = next(\n                (os.path.join(path, nm0)\n                 for path in spaths\n                 for nm0 in os.listdir(path)\n                 for nm in [nm0[:-4] if len(nm0) > 4 and nm0[-4:] == '.fmm'    else \\\n                            nm0[:-7] if len(nm0) > 7 and nm0[-7:] == '.fmm.gz' else \\\n                            None]\n                 if nm is not None and nm == name),\n                None)\n    if fname is None: raise ValueError('Cannot find an FFM file with the name %s' % origname)\n    # Okay, load the model...\n    mdl = load_fmm_model(fname).persist()\n    retinotopy_model.cache[tup] = mdl\n    return mdl", "response": "Returns a standard retinotopy model of the current system of the available model types."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef occipital_flatmap(cortex, radius=None):\n    '''\n    occipital_flatmap(cortex) yields a flattened mesh of the occipital cortex of the given cortex\n      object.\n      \n    Note that if the cortex is not registrered to fsaverage, this will fail.\n\n    The option radius may be given to specify the fraction of the cortical sphere (in radians) to\n    include in the map.\n    '''\n    mdl = retinotopy_model('benson17', hemi=cortex.chirality)\n    mp = mdl.map_projection\n    if radius is not None: mp = mp.copy(radius=radius)\n    return mp(cortex)", "response": "Returns a flattened mesh of the occipital cortex of the given cortex\n      object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of objects that can be used with a given mesh.", "response": "def retinotopy_mesh_field(mesh, mdl,\n                          polar_angle=None, eccentricity=None, weight=None,\n                          weight_min=0, scale=1, sigma=None, shape=2, suffix=None,\n                          max_eccentricity=Ellipsis,\n                          max_polar_angle=180,\n                          angle_type='both',\n                          exclusion_threshold=None):\n    '''\n    retinotopy_mesh_field(mesh, model) yields a list that can be used with mesh_register as a\n      potential term. This should generally be used in a similar fashion to retinotopy_anchors.\n\n    Options:\n      * polar_angle (default None) specifies that the given data should be used in place of the\n        'polar_angle' or 'PRF_polar_angle'  property values. The given argument must be numeric and\n        the same length as the the number of vertices in the mesh. If None is given, then the\n        property value of the mesh is used; if a list is given and any element is None, then the\n        weight for that vertex is treated as a zero. If the option is a string, then the property\n        value with the same name isused as the polar_angle data.\n      * eccentricity (default None) specifies that the given data should be used in places of the\n        'eccentricity' or 'PRF_eccentricity' property values. The eccentricity option is handled \n        virtually identically to the polar_angle option.\n      * weight (default None) specifies that the weight or scale of the data; this is handled\n        generally like the polar_angle and eccentricity options, but may also be 1, indicating that\n        all vertices with polar_angle and eccentricity values defined will be given a weight of 1.\n        If weight is left as None, then the function will check for 'weight',\n        'variance_explained', 'PRF_variance_explained', and 'retinotopy_weight' values and will use\n        the first found (in that order). If none of these is found, then a value of 1 is assumed.\n      * weight_min (default 0) specifies that the weight must be higher than the given value inn\n        order to be included in the fit; vertices with weights below this value have their weights\n        truncated to 0.\n      * scale (default 1) specifies a constant by which to multiply all weights for all anchors; the\n        value None is interpreted as 1.\n      * shape (default 2.0) specifies the exponent in the harmonic function.\n      * sigma (default None) specifies, if given as a number, the sigma parameter of the Gaussian\n        potential function; if sigma is None, however, the potential function is harmonic.\n      * suffix (default None) specifies any additional arguments that should be appended to the \n        potential function description list that is produced by this function; i.e., the \n        retinotopy_anchors function produces a list, and the contents of suffix, if given and not\n        None, are appended to that list (see mesh_register).\n      * max_eccentricity (default Ellipsis) specifies how the eccentricity portion of the potential\n        field should be normalized. Specifically, in order to ensure that polar angle and\n        eccentricity contribute roughly equally to the potential, this should be approximately the\n        max eccentricity appearing in the data on the mesh. If the argument is the default then the\n        actual max eccentricity will be used.\n      * max_polar_angle (default: 180) is used the same way as the max_eccentricity function, but if\n        Ellipsis is given, the value 180 is always assumed regardless of measured data.\n      * exclusion_threshold (default None) specifies that if the initial norm of a vertex's gradient\n        is greater than exclusion_threshold * std + median (where std and median are calculated over\n        the vertices with non-zero gradients) then its weight is set to 0 and it is not kept as part\n        of the potential field.\n      * angle_type (default: None) specifies that only one type of angle should be included in the\n        mesh; this may be one of 'polar', 'eccen', 'eccentricity', 'angle', or 'polar_angle'. If\n        None, then both polar angle and eccentricity are included.\n\n    Example:\n     # The retinotopy_anchors function is intended for use with mesh_register, as follows:\n     # Define our Schira Model:\n     model = neuropythy.retinotopy_model()\n     # Make sure our mesh has polar angle, eccentricity, and weight data:\n     mesh.prop('polar_angle',  polar_angle_vertex_data);\n     mesh.prop('eccentricity', eccentricity_vertex_data);\n     mesh.prop('weight',       variance_explained_vertex_data);\n     # register the mesh using the retinotopy and model:\n     registered_mesh = neuropythy.registration.mesh_register(\n        mesh,\n        ['mesh', retinotopy_mesh_field(mesh, model)],\n        max_step_size=0.05,\n        max_steps=2000)\n    '''\n    #TODO: given a 3D mesh and a registered model, we should be able to return a 3D version of the\n    # anchors by unprojecting them\n    if pimms.is_str(mdl):\n        mdl = retinotopy_model(mdl)\n    if not isinstance(mdl, RetinotopyMeshModel):\n        if isinstance(mdl, RegisteredRetinotopyModel):\n            mdl = mdl.model\n        if not isinstance(mdl, RetinotopyMeshModel):\n            raise RuntimeError('given model is not a RetinotopyMeshModel instance!')\n    if not hasattr(mdl, 'data') or 'polar_angle' not in mdl.data or 'eccentricity' not in mdl.data:\n        raise ValueError('Retinotopy model does not have polar angle and eccentricity data')\n    if not isinstance(mesh, CorticalMesh):\n        raise RuntimeError('given mesh is not a CorticalMesh object!')\n    n = mesh.vertex_count\n    X = mesh.coordinates.T\n    if weight_min is None: weight_min = 0\n    # make sure we have our polar angle/eccen/weight values:\n    # (weight is odd because it might be a single number, so handle that first)\n    (polar_angle, eccentricity, weight) = [\n        extract_retinotopy_argument(mesh, name, arg, default='empirical')\n        for (name, arg) in [\n                ('polar_angle', polar_angle),\n                ('eccentricity', eccentricity),\n                ('weight', [weight for i in range(n)] \\\n                           if isinstance(weight, Number) or np.issubdtype(type(weight), np.float) \\\n                           else weight)]]\n    # Make sure they contain no None/invalid values\n    (polar_angle, eccentricity, weight) = _retinotopy_vectors_to_float(\n        polar_angle, eccentricity, weight,\n        weight_min=weight_min)\n    if np.sum(weight > 0) == 0:\n        raise ValueError('No positive weights found')\n    idcs = [i for (i,w) in enumerate(weight) if w > 0]\n    # Okay, let's get the model data ready\n    mdl_1s = np.ones(mdl.forward.coordinates.shape[0])\n    mdl_coords = np.dot(mdl.transform, np.vstack((mdl.forward.coordinates.T, mdl_1s)))[:2].T\n    mdl_faces  = mdl.forward.triangles\n    mdl_data   = np.asarray([mdl.data['polar_angle'], mdl.data['eccentricity']])\n    # Get just the relevant weights and the scale\n    wgts = weight[idcs] * (1 if scale is None else scale)\n    # and the relevant polar angle/eccen data\n    msh_data = np.asarray([polar_angle, eccentricity])[:,idcs]\n    # format shape correctly\n    shape = np.full((len(idcs)), float(shape), dtype=np.float32)\n    # Last thing before constructing the field description: normalize both polar angle and eccen to\n    # cover a range of 0-1:\n    if max_eccentricity is Ellipsis: max_eccentricity = np.max(msh_data[1])\n    if max_polar_angle  is Ellipsis: max_polar_angle  = 180\n    if max_polar_angle is not None:\n        msh_data[0] /= max_polar_angle\n        mdl_data[0] /= max_polar_angle\n    if max_eccentricity is not None:\n        msh_data[1] /= max_eccentricity\n        mdl_data[1] /= max_eccentricity\n    # Check if we are making an eccentricity-only or a polar-angle-only field:\n    if angle_type is not None:\n        angle_type = angle_type.lower()\n        if angle_type != 'both' and angle_type != 'all':\n            convert = {'eccen':'eccen', 'eccentricity':'eccen', 'radius':'eccen',\n                       'angle':'angle', 'polar_angle': 'angle', 'polar': 'angle'}\n            angle_type = convert[angle_type]\n            mdl_data = [mdl_data[0 if angle_type == 'angle' else 1]]\n            msh_data = [msh_data[0 if angle_type == 'angle' else 1]]\n    # okay, we've partially parsed the data that was given; now we can construct the final list of\n    # instructions:\n    if sigma is None:\n        field_desc = ['mesh-field', 'harmonic', mdl_coords, mdl_faces, mdl_data, idcs, msh_data,\n                      'scale', wgts, 'order', shape]\n    else:\n        if not hasattr(sigma, '__iter__'): sigma = [sigma for _ in wgts]\n        field_desc = ['mesh-field', 'gaussian', mdl_coords, mdl_faces, mdl_data, idcs, msh_data,\n                      'scale', wgts, 'order', shape, 'sigma', sigma]\n    if suffix is not None: field_desc += suffix\n    # now, if we want to exclude outliers, we do so here:\n    if exclusion_threshold is not None:\n        jpe = java_potential_term(mesh, field_desc)\n        jcrds = to_java_doubles(mesh.coordinates)\n        jgrad = to_java_doubles(np.zeros(mesh.coordinates.shape))\n        jpe.calculate(jcrds,jgrad)\n        gnorms = np.sum((np.asarray([[x for x in row] for row in jgrad])[:, idcs])**2, axis=0)\n        gnorms_pos = gnorms[gnorms > 0]\n        mdn = np.median(gnorms_pos)\n        std = np.std(gnorms_pos)\n        gn_idcs = np.where(gnorms > mdn + std*3.5)[0]\n        for i in gn_idcs: wgts[i] = 0;\n    return field_desc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retinotopy_anchors(mesh, mdl,\n                       polar_angle=None, eccentricity=None,\n                       weight=None, weight_min=0.1,\n                       field_sign_weight=0, field_sign=None, invert_field_sign=False,\n                       radius_weight=0, radius_weight_source='Wandell2015', radius=None,\n                       model_field_sign=None,\n                       model_hemi=Ellipsis,\n                       scale=1,\n                       shape='Gaussian', suffix=None,\n                       sigma=[0.1, 2.0, 8.0],\n                       select='close'):\n    '''\n    retinotopy_anchors(mesh, model) is intended for use with the mesh_register function and the\n    retinotopy_model() function and/or the RetinotopyModel class; it yields a description of the\n    anchor points that tie relevant vertices the given mesh to points predicted by the given model\n    object. Any instance of the RetinotopyModel class should work as a model argument; this includes\n    SchiraModel objects as well as RetinotopyMeshModel objects such as those returned by the\n    retinotopy_model() function. If the model given is a string, then it is passed to the\n    retinotopy_model() function first.\n\n    Options:\n      * polar_angle (default None) specifies that the given data should be used in place of the\n        'polar_angle' or 'PRF_polar_angle'  property values. The given argument must be numeric and\n        the same length as the the number of vertices in the mesh. If None is given, then the\n        property value of the mesh is used; if a list is given and any element is None, then the\n        weight for that vertex is treated as a zero. If the option is a string, then the property\n        value with the same name isused as the polar_angle data.\n      * eccentricity (default None) specifies that the given data should be used in places of the\n        'eccentricity' or 'PRF_eccentricity' property values. The eccentricity option is handled \n        virtually identically to the polar_angle option.\n      * weight (default None) specifies that the weight or scale of the data; this is handled\n        generally like the polar_angle and eccentricity options, but may also be 1, indicating that\n        all vertices with polar_angle and eccentricity values defined will be given a weight of 1.\n        If weight is left as None, then the function will check for 'weight',\n        'variance_explained', 'PRF_variance_explained', and 'retinotopy_weight' values and will use\n        the first found (in that order). If none of these is found, then a value of 1 is assumed.\n      * weight_min (default 0) specifies that the weight must be higher than the given value inn\n        order to be included in the fit; vertices with weights below this value have their weights\n        truncated to 0.\n      * scale (default 1) specifies a constant by which to multiply all weights for all anchors; the\n        value None is interpreted as 1.\n      * shape (default 'Gaussian') specifies the shape of the potential function (see mesh_register)\n      * model_hemi (default: None) specifies the hemisphere of the model to load; if None, then\n        looks for a non-specific model.\n      * suffix (default None) specifies any additional arguments that should be appended to the \n        potential function description list that is produced by this function; i.e., the \n        retinotopy_anchors function produces a list, and the contents of suffix, if given and not\n        None, are appended to that list (see mesh_register).\n      * select (default 'close') specifies a function that will be called with two arguments for\n        every vertex given an anchor; the arguments are the vertex label and the matrix of anchors.\n        The function should return a list of anchors to use for the label (None is equivalent to\n        lambda id,anc: anc). The parameter may alternately be specified using the string 'close':\n        select=['close', [k]] indicates that any anchor more than k times the average edge-length in\n        the mesh should be excluded; a value of just ['close', k] on the other hand indicates that\n        any anchor more than k distance from the vertex should be exlcuded. The default value,\n        'close', is equivalent to ['close', [40]].\n      * sigma (default [0.1, 2.0, 4.0]) specifies how the sigma parameter should be handled; if\n        None, then no sigma value is specified; if a single number, then all sigma values are\n        assigned that value; if a list of three numbers, then the first is the minimum sigma value,\n        the second is the fraction of the minimum distance between paired anchor points, and the \n        last is the maximum sigma --- the idea with this form of the argument is that the ideal\n        sigma value in many cases is approximately 0.25 to 0.5 times the distance between anchors\n        to which a single vertex is attracted; for any anchor a to which a vertex u is attracted,\n        the sigma of a is the middle sigma-argument value times the minimum distance from a to all\n        other anchors to which u is attracted (clipped by the min and max sigma).\n      * field_sign_weight (default: 0) specifies the amount of weight that should be put on the\n        retinotopic field of the model as a method of attenuating the weights on those anchors whose\n        empirical retinotopic values and predicted model locations do not match. The weight that\n        results is calculated from the difference in empirical field-sign for each vertex and the\n        visual area field sign based on the labels in the model. The higher the field-sign weight,\n        (approaching 1) the more the resulting value is a geometric mean of the field-sign-based\n        weight and the original weights. As this value approaches 0, the resulting weights are more\n        like the original weights.\n      * radius_weight (default: 0) specifies the amount of weight that should be put on the\n        receptive field radius of the model as a method of attenuating the weights on those anchors\n        whose empirical retinotopic values and predicted model locations do not match. The weight\n        that results is calculated from the difference in empirical RF radius for each vertex and\n        the predicted RF radius based on the labels in the model. The higher the radius weight,\n        (approaching 1) the more the resulting value is a geometric mean of the field-sign-based\n        weight and the original weights. As this value approaches 0, the resulting weights are more\n        like the original weights.\n      * radius_weight_source (default: 'Wandell2015') specifies the source for predicting RF radius;\n        based on eccentricity and visual area label.\n\n    Example:\n     # The retinotopy_anchors function is intended for use with mesh_register, as follows:\n     # Define our Schira Model:\n     model = neuropythy.registration.SchiraModel()\n     # Make sure our mesh has polar angle, eccentricity, and weight data:\n     mesh.prop('polar_angle',  polar_angle_vertex_data);\n     mesh.prop('eccentricity', eccentricity_vertex_data);\n     mesh.prop('weight',       variance_explained_vertex_data);\n     # register the mesh using the retinotopy and model:\n     registered_mesh = neuropythy.registration.mesh_register(\n        mesh,\n        ['mesh', retinotopy_anchors(mesh, model)],\n        max_step_size=0.05,\n        max_steps=2000)\n    '''\n    if pimms.is_str(mdl):\n        hemi = None\n        if pimms.is_str(model_hemi):\n            model_hemi = model_hemi.upper()\n            hemnames = {k:h\n                        for (h,als) in [('LH', ['LH','L','LEFT','RHX','RX']),\n                                        ('RH', ['RH','R','RIGHT','LHX','LX'])]\n                        for k in als}\n            if model_hemi in hemnames: hemi = hemnames[model_hemi]\n            else: raise ValueError('Unrecognized hemisphere name: %s' % model_hemi)\n        elif model_hemi is not None:\n            raise ValueError('model_hemi must be a string, Ellipsis, or None')\n        mdl = retinotopy_model(mdl, hemi=hemi)\n    if not isinstance(mdl, RetinotopyModel):\n        raise RuntimeError('given model is not a RetinotopyModel instance!')\n    if not isinstance(mesh, geo.Mesh):\n        raise RuntimeError('given mesh is not a Mesh object!')\n    n = mesh.vertex_count\n    X = mesh.coordinates.T\n    if weight_min is None: weight_min = 0\n    # make sure we have our polar angle/eccen/weight values:\n    # (weight is odd because it might be a single number, so handle that first)\n    (polar_angle, eccentricity, weight) = [\n        extract_retinotopy_argument(mesh, name, arg, default='empirical')\n        for (name, arg) in [\n                ('polar_angle', polar_angle),\n                ('eccentricity', eccentricity),\n                ('weight', np.full(n, weight) if pimms.is_number(weight) else weight)]]\n    # Make sure they contain no None/invalid values\n    (polar_angle, eccentricity, weight) = _retinotopy_vectors_to_float(\n        polar_angle, eccentricity, weight,\n        weight_min=weight_min)\n    if np.sum(weight > 0) == 0:\n        raise ValueError('No positive weights found')\n    idcs = np.where(weight > 0)[0]\n    # Interpret the select arg if necessary (but don't apply it yet)\n    select = ['close', [40]] if select == 'close'   else \\\n             ['close', [40]] if select == ['close'] else \\\n             select\n    if select is None:\n        select = lambda a,b: b\n    elif ((pimms.is_vector(select) or is_list(select) or is_tuple(select))\n          and len(select) == 2 and select[0] == 'close'):\n        if pimms.is_vector(select[1]): d = np.mean(mesh.edge_lengths) * select[1][0]\n        else:                          d = select[1]\n        select = lambda idx,ancs: [a for a in ancs if a[0] is not None if npla.norm(X[idx] - a) < d]\n    # Okay, apply the model:\n    res = mdl.angle_to_cortex(polar_angle[idcs], eccentricity[idcs])\n    oks = np.isfinite(np.sum(np.reshape(res, (res.shape[0], -1)), axis=1))\n    # Organize the data; trim out those not selected\n    data = [[[i for _ in r], r, [ksidx[tuple(a)] for a in r]]\n            for (i,r0,ok) in zip(idcs, res, oks) if ok\n            for ksidx in [{tuple(a):(k+1) for (k,a) in enumerate(r0)}]\n            for r in [select(i, r0)]\n            if len(r) > 0]\n    # Flatten out the data into arguments for Java\n    idcs = [int(i) for d in data for i in d[0]]\n    ancs = np.asarray([pt for d in data for pt in d[1]]).T\n    labs = np.asarray([ii for d in data for ii in d[2]]).T\n    # Get just the relevant weights and the scale\n    wgts = np.asarray(weight[idcs] * (1 if scale is None else scale))\n    # add in the field-sign weights and radius weights if requested here;\n    if not np.isclose(field_sign_weight, 0) and mdl.area_name_to_id is not None:\n        id2n = mdl.area_id_to_name\n        if field_sign is True or field_sign is Ellipsis or field_sign is None:\n            from .cmag import cmag\n            r = {'polar_angle': polar_angle,  'eccentricity': eccentricity}\n            #field_sign = retinotopic_field_sign(mesh, retinotopy=r)\n            field_sign = cmag(mesh, r)['field_sign']\n        elif pimms.is_str(field_sign): field_sign = mesh.prop(field_sign)\n        field_sign = np.asarray(field_sign)\n        if invert_field_sign: field_sign = -field_sign\n        fswgts = 1.0 - 0.25 * np.asarray(\n            [(fs - visual_area_field_signs[id2n[l]]) if l in id2n else 0\n             for (l,fs) in zip(labs,field_sign[idcs])])**2\n        # average the weights at some fraction with the original weights\n        fswgts = field_sign_weight*fswgts + (1 - field_sign_weight)*wgts\n    else: fswgts = None\n    # add in radius weights if requested as well\n    if not np.isclose(radius_weight, 0) and mdl.area_name_to_id is not None:\n        id2n = mdl.area_id_to_name\n        emprad = extract_retinotopy_argument(mesh, 'radius', radius, default='empirical')\n        emprad = emprad[idcs]\n        emprad = np.argsort(np.argsort(emprad)) * (1.0 / len(emprad)) - 0.5\n        eccs = eccentricity[idcs]\n        prerad = np.asarray([predict_pRF_radius(ecc, id2n[lbl], source=radius_weight_source)\n                             for (ecc,lbl) in zip(eccs,labs)])\n        prerad = np.argsort(np.argsort(prerad)) * (1.0 / len(prerad)) - 0.5\n        rdwgts = 1.0 - (emprad - prerad)**2\n        # average the weights at some fraction with the original weights\n        rdwgts = radius_weight*rdwgts + (1-radius_weight)*wgts\n    else: rdwgts = None\n    # apply the weights\n    if fswgts is not None:\n        if rdwgts is not None: wgts = np.power(fswgts*rdwgts*wgts, 1.0/3.0)\n        else:                  wgts = np.sqrt(fswgts*wgts)\n    elif rdwgts is not None:   wgts = np.sqrt(rdwgts*wgts)\n    # Figure out the sigma parameter:\n    if sigma is None: sigs = None\n    elif pimms.is_number(sigma): sigs = sigma\n    elif pimms.is_vector(sigma) and len(sigma) == 3:\n        [minsig, mult, maxsig] = sigma\n        sigs = np.clip(\n            [mult*min([npla.norm(a0 - a) for a in anchs if a is not a0]) if len(iii) > 1 else maxsig\n             for (iii,anchs,_) in data\n             for a0 in anchs],\n            minsig, maxsig)\n    else:\n        raise ValueError('sigma must be a number or a list of 3 numbers')\n    # okay, we've partially parsed the data that was given; now we can construct the final list of\n    # instructions:\n    tmp =  (['anchor', shape,\n             np.asarray(idcs, dtype=np.int),\n             np.asarray(ancs, dtype=np.float64),\n             'scale', np.asarray(wgts, dtype=np.float64)]\n            + ([] if sigs is None else ['sigma', sigs])\n            + ([] if suffix is None else suffix))\n    return tmp", "response": "This function returns a description of the anchor points that tie relevant vertices to predicted vertices."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_empirical_retinotopy(cortex,\n                              polar_angle=None, eccentricity=None, pRF_radius=None, weight=None,\n                              eccentricity_range=None, weight_min=0,\n                              invert_rh_angle=False,\n                              partial_voluming_correction=False):\n    '''\n    calc_empirical_retinotopy computes the value empirical_retinotopy, which is an itable object\n      storing the retinotopy data for the registration.\n\n    Required afferent parameters:\n      @ cortex Must be the cortex object that is to be registered to the model of retinotopy.\n \n    Optional afferent parameters:\n      @ polar_angle May be an array of polar angle values or a polar angle property name; if None\n        (the default), attempts to auto-detect an empirical polar angle property.\n      @ eccentricity May be an array of eccentricity values or an eccentricity property name; if\n        None (the default), attempts to auto-detect an empirical eccentricity property.\n      @ pRF_radius May be an array of receptive field radius values or the property name for such an\n        array; if None (the default), attempts to auto-detect an empirical radius property.\n      @ weight May be an array of weight values or a weight property name; if None (the default),\n        attempts to auto-detect an empirical weight property, such as variance_explained.\n      @ eccentricity_range May be a maximum eccentricity value or a (min, max) eccentricity range\n        to be used in the registration; if None, then no clipping is done.\n      @ weight_min May be given to indicate that weight values below this value should not be\n        included in the registration; the default is 0.\n      @ partial_voluming_correction May be set to True (default is False) to indicate that partial\n        voluming correction should be used to adjust the weights.\n      @ invert_rh_angle May be set to True (default is False) to indicate that the right hemisphere\n        has its polar angle stored with opposite sign to the model polar angle.\n\n    Efferent values:\n      @ empirical_retinotopy Will be a pimms itable of the empirical retinotopy data to be used in\n        the registration; the table's keys will be 'polar_angle', 'eccentricity', and 'weight';\n        values that should be excluded for any reason will have 0 weight and undefined angles.\n    '''\n    data = {}  # the map we build up in this function\n    n = cortex.vertex_count\n    (emin,emax) = (-np.inf,np.inf)       if eccentricity_range is None          else \\\n                  (0,eccentricity_range) if pimms.is_number(eccentricity_range) else \\\n                  eccentricity_range\n    # Step 1: get our properties straight ##########################################################\n    (ang, ecc, rad, wgt) = [\n        np.array(extract_retinotopy_argument(cortex, name, arg, default='empirical'))\n        for (name, arg) in [\n                ('polar_angle', polar_angle),\n                ('eccentricity', eccentricity),\n                ('radius', pRF_radius),\n                ('weight', np.full(n, weight) if pimms.is_number(weight) else weight)]]\n    if wgt is None: wgt = np.ones(len(ecc))\n    bad = np.logical_not(np.isfinite(np.prod([ang, ecc, wgt], axis=0)))\n    ecc[bad] = 0\n    wgt[bad] = 0\n    if rad is not None: rad[bad] = 0\n    # do partial voluming correction if requested\n    if partial_voluming_correction: wgt = wgt * (1 - cortex.partial_voluming_factor)\n    # now trim and finalize\n    bad = bad | (wgt <= weight_min) | (ecc < emin) | (ecc > emax)\n    wgt[bad] = 0\n    ang[bad] = 0\n    ecc[bad] = 0\n    for x in [ang, ecc, wgt, rad]:\n        if x is not None:\n            x.setflags(write=False)\n    # that's it!\n    dat = dict(polar_angle=ang, eccentricity=ecc, weight=wgt)\n    if rad is not None: dat['radius'] = rad\n    return (pimms.itable(dat),)", "response": "This function calculates the value empirical_retinotopy for the given cortex object and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calc_model(cortex, model_argument, model_hemi=Ellipsis, radius=np.pi/3):\n    '''\n    calc_model loads the appropriate model object given the model argument, which may given the name\n    of the model or a model object itself.\n\n    Required afferent parameters:\n      @ model_argument Must be either a RegisteredRetinotopyModel object or the name of a model that\n        can be loaded.\n\n    Optional afferent parameters:\n      @ model_hemi May be used to specify the hemisphere of the model; this is usually only used\n        when the fsaverage_sym hemisphere is desired, in which case this should be set to None; if\n        left at the default value (Ellipsis), then it will use the hemisphere of the cortex param.\n\n    Provided efferent values:\n      @ model Will be the RegisteredRetinotopyModel object to which the mesh should be registered.\n    '''\n    if pimms.is_str(model_argument):\n        h = cortex.chirality if model_hemi is Ellipsis else \\\n            None             if model_hemi is None     else \\\n            model_hemi\n        model = retinotopy_model(model_argument, hemi=h, radius=radius)\n    else:\n        model = model_argument\n    if not isinstance(model, RegisteredRetinotopyModel):\n        raise ValueError('model must be a RegisteredRetinotopyModel')\n    return model", "response": "calc_model loads the appropriate model object given the model argument."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the prediction for a given registration map.", "response": "def calc_prediction(registered_map, preregistration_mesh, native_mesh, model):\n    '''\n    calc_registration_prediction is a pimms calculator that creates the both the prediction and the\n    registration_prediction, both of which are pimms itables including the fields 'polar_angle',\n    'eccentricity', and 'visual_area'. The registration_prediction data describe the vertices for\n    the registered_map, not necessarily of the native_mesh, while the prediction describes the\n    native mesh.\n\n    Provided efferent values:\n      @ registered_mesh Will be a mesh object that is equivalent to the preregistration_mesh but\n        with the coordinates and predicted fields (from the registration) filled in. Note that this\n        mesh is still in the resampled configuration is resampling was performed.\n      @ registration_prediction Will be a pimms ITable object with columns 'polar_angle', \n        'eccentricity', and 'visual_area'. For values outside of the model region, visual_area will\n        be 0 and other values will be undefined (but are typically 0). The registration_prediction\n        describes the values on the registrered_mesh.\n      @ prediction will be a pimms ITable object with columns 'polar_angle', 'eccentricity', and\n        'visual_area'. For values outside of the model region, visual_area will be 0 and other\n        values will be undefined (but are typically 0). The prediction describes the values on the\n        native_mesh and the predicted_mesh.\n    '''\n    # invert the map projection to make the registration map into a mesh\n    coords3d = np.array(preregistration_mesh.coordinates)\n    idcs = registered_map.labels\n    coords3d[:,idcs] = registered_map.meta('projection').inverse(registered_map.coordinates)\n    rmesh = preregistration_mesh.copy(coordinates=coords3d)\n    # go ahead and get the model predictions...\n    d = model.cortex_to_angle(registered_map.coordinates)\n    id2n = model.area_id_to_name\n    (ang, ecc) = d[0:2]\n    lbl = np.asarray(d[2], dtype=np.int)\n    rad = np.asarray([predict_pRF_radius(e, id2n[l]) if l > 0 else 0 for (e,l) in zip(ecc,lbl)])\n    d = {'polar_angle':ang, 'eccentricity':ecc, 'visual_area':lbl, 'radius':rad}\n    # okay, put these on the mesh\n    rpred = {}\n    for (k,v) in six.iteritems(d):\n        v.setflags(write=False)\n        tmp = np.zeros(rmesh.vertex_count, dtype=v.dtype)\n        tmp[registered_map.labels] = v\n        tmp.setflags(write=False)\n        rpred[k] = tmp\n    rpred = pyr.pmap(rpred)\n    rmesh = rmesh.with_prop(rpred)\n    # next, do all of this for the native mesh..\n    if native_mesh is preregistration_mesh:\n        pred = rpred\n        pmesh = rmesh\n    else:\n        # we need to address the native coordinates in the prereg coordinates then unaddress them\n        # in the registered coordinates; this will let us make a native-registered-map and repeat\n        # the exercise above\n        addr = preregistration_mesh.address(native_mesh.coordinates)\n        natreg_mesh = native_mesh.copy(coordinates=rmesh.unaddress(addr))\n        d = model.cortex_to_angle(natreg_mesh)\n        (ang,ecc) = d[0:2]\n        lbl = np.asarray(d[2], dtype=np.int)\n        rad = np.asarray([predict_pRF_radius(e, id2n[l]) if l > 0 else 0 for (e,l) in zip(ecc,lbl)])\n        pred = pyr.m(polar_angle=ang, eccentricity=ecc, radius=rad, visual_area=lbl)\n        pmesh = natreg_mesh.with_prop(pred)\n    return {'registered_mesh'        : rmesh,\n            'registration_prediction': rpred,\n            'prediction'             : pred,\n            'predicted_mesh'         : pmesh}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_retinotopy(hemi,\n                        model='benson17', model_hemi=Ellipsis,\n                        polar_angle=None, eccentricity=None, weight=None, pRF_radius=None,\n                        weight_min=0.1,\n                        eccentricity_range=None,\n                        partial_voluming_correction=False,\n                        radius_weight=1, field_sign_weight=1, invert_rh_field_sign=False,\n                        scale=20.0,\n                        sigma=Ellipsis,\n                        select='close',\n                        prior=None,\n                        resample=Ellipsis,\n                        radius=np.pi/3,\n                        max_steps=2000, max_step_size=0.05, method='random',\n                        yield_imap=False):\n    '''\n    register_retinotopy(hemi) registers the given hemisphere object, hemi, to a model of V1, V2,\n      and V3 retinotopy, and yields a copy of hemi that is identical but additionally contains\n      the registration 'retinotopy', whose coordinates are aligned with the model.\n\n    Registration attempts to align the vertex positions of the hemisphere's spherical surface with a\n    model of polar angle and eccentricity. This alignment proceeds through several steps and can\n    be modified by several options. A description of these steps and options are provided here. For\n    most cases, the default options should work relatively well.\n\n    Method:\n      (1) Prepare for registration by several intitialization substeps:\n            a. Extract the polar angle, eccentricity and weight data from the hemisphere. These\n               data are usually properties on the mesh and can be modifies by the options\n               polar_angle, eccentricity, and weight, which can be either property names or list\n               of property values. By default (None), a property is chosen using the functions\n               neuropythy.vision.extract_retinotopy_argument with the default option set to\n               'empirical'.\n            b. If partial voluming correction is enabled (via the option\n               partial_voluming_correction), multiply the weight by (1 - p) where p is \n               hemi.partial_volume_factor.\n            c. If there is a prior that is specified as a belief about the retinotopy, then a\n               Registration is created for the hemisphere such that its vertices are arranged\n               according to that prior (see also the prior option). Note that because hemi's\n               coordinates must always be projected into the registration specified by the model,\n               the prior must be the name of a registration to which the model's specified subject\n               is also registered. This is clear in the case of an example. The default value for\n               this is 'retinotopy'; assuming that our model is specified on the fsaverage_sym, \n               surface, the initial positions of the coordinates for the registration process would\n               be the result of starting with hemi's fsaverage_sym-aligned coordinates then warping\n               these coordinates in a way that is equivalent to the warping from fsaverage_sym's \n               native spherical coordinates to fsaverage_sym's retinotopy registration coordinates.\n               Note that the retinotopy registration would usually be specified in a file in the\n               fsaverage_sym subject's surf directory: surf/lh.retinotopy.sphere.reg.\n               If no prior is specified (option value None), then the vertices that are used are\n               those aligned with the registration of the model, which will usually be 'fsaverage'\n               or 'fsaverage_sym'.\n            d. If the option resample is not None, then the vertex coordinates are resampled onto\n               either the fsaverage or fsaverage_sym's native sphere surface. (The value of resample\n               should be either 'fsaverage' or 'fsaverage_sym'.) Resampling can prevent vertices\n               that have been rearranged by alignment with the model's specified registration or by\n               application of a prior from beginning the alignment with very high initial gradients\n               and is recommended for subject alignments.\n               If resample is None then no changes are made.\n            e. A 2D projection of the (possibly aligned, prior-warped, and resampled) cortical\n               surface is made according to the projection parameters of the model. This map is the\n               mesh that is warped to eventually fit the model.\n      (2) Perform the registration by running neuropythy.registration.mesh_register. This step\n          consists of two major components.\n            a. Create the potential function, which we will minimize. The potential function is a\n               complex function whose inputs are the coordinates of all of the vertices and whose\n               output is a potential value that increases both as the mesh is warped and as the\n               vertices with retinotopy predictions get farther away from the positions in the model\n               that their retinotopy values would predict they should lie. The balance of these\n               two forces is best controlled by the option functional_scale. The potential function\n               fundamentally consists of four terms; the first three describe mesh deformations and\n               the last describes the model fit.\n                - The edge deformation term is described for any vertices u and v that are connected\n                  by an edge in the mesh; it's value is c/p (r(u,v) - r0(u,v))^2 where c is the\n                  edge_scale, p is the number of edges in the mesh, r(a,b) is the distance between\n                  vertices a and b, and r0(a,b) is the distance between a and b in the initial mesh.\n                - The angle deformation term is described for any three vertices (u,v,w) that form\n                  an angle in the mesh; its value is c/m h(t(u,v,w), t0(u,v,w)) where c is the\n                  angle_scale argument, m is the number of angles in the mesh, t is the value of the\n                  angle (u,v,w), t0 is the value of the angle in the initial mesh, and h(t,t0) is an\n                  infinite-well function that asymptotes to positive infinity as t approaches both 0\n                  and pi and is minimal when t = t0 (see the nben's \n                  nben.mesh.registration.InfiniteWell documentation for more details).\n                - The perimeter term prevents the perimeter vertices form moving significantly;\n                  this primarily prevents the mesh from wrapping in on itself during registration.\n                  The form of this term is, for any vertex u on the mesh perimeter, \n                  (x(u) - x0(u))^2 where x and x0 are the position and initial position of the\n                  vertex.\n                - Finally, the functional term is minimized when the vertices best align with the\n                  retinotopy model.\n            b. Register the mesh vertices to the potential function using the nben Java library. The\n               particular parameters of the registration are method, max_steps, and max_step_size.\n\n    Options:\n      * model specifies the instance of the retinotopy model to use; this must be an\n        instance of the RegisteredRetinotopyModel class or a string that can be passed to the\n        retinotopy_model() function (default: 'standard').\n      * model_hemi specifies the hemisphere of the model; generally you shouldn't have to set this\n        unless you are using an fsaverage_sym model, in which case it should be set to None; in all\n        other cases, the default value (Ellipsis) instructs the function to auto-detect the\n        hemisphere.\n      * polar_angle, eccentricity, pRF_radius, and weight specify the property names for the\n        respective quantities; these may alternately be lists or numpy arrays of values. If weight\n        is not given or found, then unity weight for all vertices is assumed. By default, each will\n        check the  hemisphere's properties for properties with compatible names; it will prefer the\n        properties PRF_polar_angle, PRF_ecentricity, and PRF_variance_explained if possible.\n      * weight_min (default: 0.1) specifies the minimum value a vertex must have in the weight\n        property in order to be considered as retinotopically relevant.\n      * eccentricity_range (default: None) specifies that any vertex whose eccentricity is too low\n        or too high should be given a weight of 0 in the registration.\n      * partial_voluming_correction (default: True), if True, specifies that the value\n        (1 - hemi.partial_volume_factor) should be applied to all weight values (i.e., weights\n        should be down-weighted when likely to be affected by a partial voluming error).\n      * field_sign_weight (default: 1) indicates the relative weight (between 0 and 1) that should\n        be given to the field-sign as a method of determining which anchors have the strongest\n        springs. A value of 1 indicates that the effective weights of anchors should be the \n        geometric mean of the empirical retinotopic weight and field-sign-based weight; a value of 0\n        indicates that no attention should be paid to the field sign weight.\n      * radius_weight (default: 1) indicates the relative weight (between 0 and 1) that should\n        be given to the pRF radius as a method of determining which anchors have the strongest\n        springs. A value of 1 indicates that the effective weights of anchors should be the \n        geometric mean of the empirical retinotopic weight and pRF-radius-based weight; a value of 0\n        indicates that no attention should be paid to the radius-based weight.\n      * sigma specifies the standard deviation of the Gaussian shape for the Schira model anchors;\n        see retinotopy_anchors for more information.\n      * scale (default: 1.0) specifies the strength of the functional constraints (i.e. the anchors:\n        the part of the minimization responsible for ensuring that retinotopic coordinates are\n        aligned); the anatomical constraints (i.e. the edges and angles: the part of the\n        minimization responsible for ensuring that the mesh is not overly deformed) are always held\n        at a strength of 1.0.\n      * select specifies the select option that should be passed to retinotopy_anchors.\n      * max_steps (default 2,000) specifies the maximum number of registration steps to run. This\n        may be a tuple (max_steps, stride) in which case the registered map that is returned will\n        contain a piece of meta-data, 'trajectory' containing the vertex coordinates every stride\n        steps of the registration.\n      * max_step_size (default 0.05) specifies the maxmim distance a single vertex is allowed to\n        move in a single step of the minimization.\n      * method (default 'random') is the method argument passed to mesh_register. This should be\n        'random', 'pure', or 'nimble'. Generally, 'random' is recommended.\n      * yield_imap (default: False) specifies whether the return value should be the new\n        Mesh object or a pimms imap (i.e., a persistent mapping of the result of a pimms\n        calculation) containing the meta-data that was used during the registration\n        calculations. If this is True, then register_retinotopy will return immediately, and\n        calculations will only be performed as the relevant data are requested from the returned\n        imap. The item 'predicted_mesh' gives the return value when yield_imap is set to False.\n      * radius (default: pi/3) specifies the radius, in radians, of the included portion of the map\n        projection (projected about the occipital pole).\n      * sigma (default Ellipsis) specifies the sigma argument to be passed onto the \n        retinotopy_anchors function (see help(retinotopy_anchors)); the default value, Ellipsis,\n        is interpreted as the default value of the retinotopy_anchors function's sigma option.\n      * prior (default: None) specifies the prior that should be used, if found, in the \n        topology registrations for the subject associated with the retinotopy_model's registration.\n      * resample (default: Ellipsis) specifies that the data should be resampled to one of\n        the uniform meshes, 'fsaverage' or 'fsaverage_sym', prior to registration; if None then no\n        resampling is performed; if Ellipsis, then auto-detect either fsaverage or fsaverage_sym\n        based on the model_hemi option (if it is None, fsaverage_sym, else fsaverage).\n    '''\n    # create the imap\n    m = retinotopy_registration(\n        cortex=hemi, model_argument=model, model_hemi=model_hemi,\n        polar_angle=polar_angle, eccentricity=eccentricity, weight=weight, pRF_radius=pRF_radius,\n        weight_min=weight_min,  eccentricity_range=eccentricity_range,\n        partial_voluming_correction=partial_voluming_correction,\n        radius_weight=radius_weight, field_sign_weight=field_sign_weight,\n        invert_rh_field_sign=invert_rh_field_sign,\n        scale=scale, sigma=sigma, select=select, prior=prior, resample=resample, radius=radius,\n        max_steps=max_steps, max_step_size=max_step_size, method=method)\n    return m if yield_imap else m['predicted_mesh']", "response": "This function registers a given hemisphere object with a model of V1 V2 and V3 retinotopy."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef predict_retinotopy(sub, template='benson14', registration='fsaverage'):\n    '''\n    predict_retinotopy(subject) yields a pair of dictionaries each with four keys: angle, eccen,\n      sigma, and varea. Each of these keys maps to a numpy array with one entry per vertex.  The\n      first element of the yielded pair is the left hemisphere map and the second is the right\n      hemisphere map. The values are obtained by resampling the Benson et al. 2014 anatomically\n      defined template of retinotopy to the given subject.\n\n    The following optional arguments may be given:\n      * template (default: 'benson14') specifies the template to use.\n      * registration (default: 'fsaverage') specifies the subject registration to use; generally can\n        only be 'fsaverage' or 'fsaverage_sym'.\n    '''\n    template = template.lower()\n    retino_tmpls = predict_retinotopy.retinotopy_templates[registration]\n    hemis = ['lh','rh'] if registration == 'fsaverage' else ['sym']\n    if template not in retino_tmpls:\n        libdir = os.path.join(library_path(), 'data')\n        search_paths = [libdir]\n        # just hard-baked-in for now.\n        suff = 'v4_0' if registration == 'fsaverage' else 'v3_0'\n        filenames = {(hname, fnm): ('%s.%s_%s.%s.mgz' % (hname,template,fnm,suff))\n                     for fnm in ['angle','eccen','varea','sigma']\n                     for hname in hemis}\n        # find an appropriate directory\n        tmpl_path = next((os.path.join(path0, registration)\n                          for path0 in search_paths\n                          if all(os.path.isfile(os.path.join(path0, registration, 'surf', s))\n                                 for s in six.itervalues(filenames))),\n                         None)\n        if tmpl_path is None:\n            raise ValueError('No subject found with appropriate surf/*.%s_* files!' % template)\n        tmpl_sub = nyfs.subject(registration)\n        spath = os.path.join(tmpl_path, 'surf')\n        retino_tmpls[template] = pimms.persist(\n            {h:{k: pimms.imm_array(dat)\n                for k in ['angle', 'eccen', 'varea', 'sigma']\n                for dat in [nyio.load(os.path.join(tmpl_path, 'surf', filenames[(h,k)]))]}\n             for h in hemis})\n\n    # Okay, we just need to interpolate over to this subject\n    tmpl = retino_tmpls[template]\n    if not all(s in tmpl for s in hemis):\n        raise ValueError('could not find matching template')\n    if registration == 'fsaverage_sym':\n        sym = nyfs.subject('fsaverage_sym')\n        if isinstance(sub, mri.Subject):\n            subj_hems = (sub.lh, sub.hemis['rhx'])\n            tmpl_hems = (sym.lh, sym.lh)\n            chrs_hems = ('lh','rh')\n        else:\n            subj_hems = (sub,)\n            tmpl_hems = (sym.lh,)\n            chrs_hems = (sub.chirality,)\n    else:\n        fsa = nyfs.subject('fsaverage')\n        if isinstance(sub, mri.Subject):\n            subj_hems = (sub.lh, sub.rh)\n            tmpl_hems = (fsa.lh, fsa.rh)\n            chrs_hems = ('lh','rh')\n        else:\n            subj_hems = (sub,)\n            tmpl_hems = ((fsa.lh if sub.chirality == 'lh' else fsa.rh),)\n            chrs_hems = (sub.chirality,)\n    tpl = tuple([th.interpolate(sh, tmpl[h if registration == 'fsaverage' else 'sym'])\n                for (sh,th,h) in zip(subj_hems, tmpl_hems, chrs_hems)])\n    return tpl[0] if len(tpl) == 1 else tpl", "response": "predict_retinotopy returns a pair of dictionaries each with four keys angle eccen sigma and varea and varea. Each key is a tuple of two elements each containing the angle eccen sigma and varea. The second element is a numpy array with one entry per vertex. The third element is the left hemisphere map and the third is the right hemisphere map."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a pimms itable comparing two datasets.", "response": "def retinotopy_comparison(arg1, arg2, arg3=None,\n                          eccentricity_range=None, polar_angle_range=None, visual_area_mask=None,\n                          weight=Ellipsis, weight_min=None, visual_area=Ellipsis,\n                          method='rmse', distance='scaled', gold=None):\n    '''\n    retinotopy_comparison(dataset1, dataset2) yields a pimms itable comparing the two retinotopy\n      datasets.\n    retinotopy_error(obj, dataset1, dataset2) is equivalent to retinotopy_comparison(x, y) where x\n      and y are retinotopy(obj, dataset1) and retinotopy_data(obj, dataset2).\n    \n    The datasets may be specified in a number of ways, some of which may be incompatible with\n    certain options. The simplest way to specify a dataset is as a vector of complex numbers, which\n    are taken to represent positions in the visual field with (a + bi) corresponding to the\n    coordinate (a deg, b deg) in the visual field. Alternately, an n x 2 or 2 x n matrix will be\n    interpreted as (polar angle, eccentricity) coordinates, in terms of visual degrees (see the\n    as_retinotopy function: as_retinotopy(arg, 'visual') yields this input format). Alternately,\n    the datasets may be mappings such as those retuend by the retinotopy_data function; in this case\n    as_retinotopy is used to extract the visual coordinates (so they need not be specified in visual\n    coordinates specifically in this case). In this last case, additional properties such as the\n    variance explained and pRF size can be returned, making it valuable for more sophisticated error\n    methods or distance metrics.\n\n    The returned dataset will always have a row for each row in the two datasets (which must have\n    the same number of rows). However, many rows may have a weight of 0 even if no weights were \n    specified in the options; this is because other limitations may have been specified (such as\n    in the eccentricity_range or visual_areas). The returned dataset will always contain the\n    following columns:\n      * 'weight' gives the weight assigned to this particular vertex; the weights will always sum to\n        1 unless all vertices have 0 weight.\n      * 'polar_angle_1' and 'polar_angle_2', 'eccentricity_1', 'eccenticity_2', 'x_1', 'x_2', 'y_1',\n        'y_2', 'z_1', and 'z_2' all give the visual field coordinates in degrees; the z values give\n        complex numbers equivalent to the x/y values.\n      * 'radius_1' and 'radius_2' give the radii (sigma parameters) of the pRF gaussians.\n      * 'polar_angle_error', 'eccentricity_error', and 'center_error' all give the difference\n        between the visual field points in the two datasets; note that polar_angle_error in\n        particular is an error measure of rotations around the visual field and not of visual field\n        position. The 'center_error' is the distance between the centers of the visual field, in\n        degrees. The 'radius_error' value is also given.\n      * 'visual_area_1' and 'visual_area_2' specify the visual areas of the individual datasets; if\n        either of the datasets did not have a visual area, it will be omitted. Additionally, the\n        property 'visual_area' specifies the visual area suggested for use in analyses; this is\n        chosen based on the following: (1) if there is a gold standard dataset specified that has\n        a visual area, use it; (2) if only one of the datasets has a visual area, use it; (3) if\n        both have a visual area, then use the (varea1 == varea2) * varea1 (the areas that agree are\n        kept and all others are set to 0); (4) if neither has a visual area, then this property is\n        omitted. In all cases where a 'visual_area' property is included, those vertices that do not\n        overlap with the given visual_area_option option will be set to 0 along with the\n        corresponding weights.\n      * A variety of other lazily-calculated error metrics are included.\n\n    The following options are accepted:\n      * eccentricity_range (default: None) specifies the range of eccentricity to include in the\n        calculation (in degrees). This may be specified as emax or (emin, emax).\n      * polar_angle_range (default: None) specifies the range of polar angles to include in the\n        calculation. Like eccentricity range it may be specified as (amin, amax) but amax alone is\n        not allowed. Additionally the strings 'lh' and 'rvf' are equivalent to (0,180) and the\n        strings 'rh' and 'lvf' are equivalent to (-180,0).\n      * weight (default: Ellipsis) specifies the weights to be used in the calculation. This may be\n        None to specify that no weights should be used, or a property name or an array of weight\n        values. Alternately, it may be a tuple (w1, w2) of the weights for datasets 1 and 2. If the\n        argument is Ellipsis, then it will use weights if they are found in the retinotopy dataset;\n        both datasets may contain weights in which the product is used.\n      * weight_min (default: None) specifies the minimum weight a vertex must have to be included in\n        the calculation.\n      * visual_area (default: Ellipsis) specifies the visual area labels to be used in the\n        calculation. This may be None to specify that no labels should be used, or a property name\n        or an array of labels. Alternately, it may be a tuple (l1, l2) of the labels for datasets 1 \n        and 2. If the argument is Ellipsis, then it will use labels if they are found in the\n        retinotopy dataset; both datasets may contain labels in which the gold standard's labels are\n        used if there is a gold standard and the overlapping labels are used otherwise.\n      * visual_area_mask (default: None) specifies a list of visual areas included in the\n        calculation; this is applied to all datasets with a visual_area key; see the 'visual_area'\n        columns above and the visual_area option. If None, then no visual areas are filtered;\n        otherwise, arguments should like (1,2,3), which would usually specify that areas V1, V2, and\n        V3, be included.\n      * gold (default: None) specifies which dataset should be considered the gold standard; this\n        should be either 1 or 2. If a gold-standard dataset is specified, then it is used in certain\n        calculations; for example, when scaling an error by eccentricity, the gold-standard's\n        eccentricity will be used unless there is no gold standard, in which case the mean of the\n        two values are used.\n    '''\n    if arg3 is not None: (obj, dsets) = (arg1, [retinotopy_data(arg1, aa) for aa in (arg2,arg3)])\n    else:                (obj, dsets) = (None,    [arg1, arg2])\n    (gi,gold) = (None,False) if not gold else (gold-1,True)\n    # we'll build up this result as we go...\n    result = {}\n    # they must have a retinotopy representation:\n    vis = [as_retinotopy(ds, 'visual') for ds in dsets]\n    ps = (vis[0][0], vis[1][0])\n    es = (vis[0][1], vis[1][1])\n    rs = [ds['radius'] if 'radius' in ds else None for ds in dsets]\n    for ii in (0,1):\n        s = '_%d' % (ii + 1)\n        (p,e) = (ps[ii],es[ii])\n        result['polar_angle'  + s] = p\n        result['eccentricity' + s] = e\n        if rs[ii] is not None: result['radius' + s] = rs[ii]\n        p = np.pi/180.0 * (90.0 - p)\n        (x,y) = (e*np.cos(p), e*np.sin(p))\n        result['x' + s] = x\n        result['y' + s] = y\n        result['z' + s] = x + 1j * y\n    n = len(ps[0])\n    # figure out the weight\n    if pimms.is_vector(weight) and len(weight) == 2:\n        ws = [(None  if w is None                   else\n               ds[w] if pimms.is_str(w) and w in ds else\n               geo.to_property(obj, w))\n              for (w,ds) in zip(weight, dsets)]\n        weight = Ellipsis\n    else:\n        ws = [next((ds[k] for k in ('weight','variance_explained') if k in ds), None)\n              for ds in dsets]\n    if pimms.is_vector(weight, 'real'):\n        wgt = weight\n    elif pimms.is_str(weight):\n        if obj is None: raise ValueError('weight property name but no vertex-set given')\n        wgt = geo.to_property(obj, weight)\n    elif weight is Ellipsis:\n        if gold: wgt = ws[gi]\n        elif ws[0] is None and ws[1] is None: wgt = None\n        elif ws[0] is None: wgt = ws[1]\n        elif ws[1] is None: wgt = ws[0]\n        else: wgt = ws[0] * ws[1]\n    else: raise ValueError('Could not parse weight argument')\n    if wgt is None: wgt = np.ones(n)\n    if ws[0] is not None: result['weight_1'] = ws[0]\n    if ws[1] is not None: result['weight_2'] = ws[1]\n    # figure out the visual areas\n    if is_tuple(visual_area) and len(visual_area) == 2:\n        ls = [(None  if l is None                   else\n               ds[l] if pimms.is_str(l) and l in ds else\n               geo.to_property(obj, l))\n              for (l,ds) in zip(visual_area, dsets)]\n        visual_area = Ellipsis\n    else:\n        ls = [next((ds[k] for k in ('visual_area','label') if k in ds), None)\n              for ds in dsets]\n    if pimms.is_vector(visual_area):\n        lbl = visual_area\n    elif pimms.is_str(visual_area):\n        if obj is None: raise ValueError('visual_area property name but no vertex-set given')\n        lbl = geo.to_property(obj, visual_area)\n    elif visual_area is None:\n        lbl = None\n    elif visual_area is Ellipsis:\n        if gold: lbl = ls[gi]\n        elif ls[0] is None and ls[1] is None: lbl = None\n        elif ls[0] is None: lbl = ls[1]\n        elif ls[1] is None: lbl = ls[0]\n        else: lbl = l[0] * (l[0] == l[1])\n    else: raise ValueError('Could not parse visual_area argument')\n    if ls[0] is not None: result['visual_area_1'] = ls[0]\n    if ls[1] is not None: result['visual_area_2'] = ls[1]\n    # Okay, now let's do some filtering; we clear weights as we go\n    wgt = np.array(wgt)\n    # Weight must be greater than the min\n    if weight_min is not None: wgt[wgt < weight_min] = 0\n    # Visual areas must be in the mask\n    lbl = None if lbl is None else np.array(lbl)\n    if lbl is not None and visual_area_mask is not None:\n        if pimms.is_int(visual_area_mask): visual_area_mask = [visual_area_mask]\n        oomask = (0 == np.sum([lbl == va for va in visual_area_mask], axis=0))\n        wgt[oomask] = 0\n        lbl[oomask] = 0\n    if lbl is not None: result['visual_area'] = lbl\n    # eccen must be in range\n    if eccentricity_range is not None:\n        er = eccentricity_range\n        if pimms.is_real(er): er = (0,er)\n        if gold: wgt[(es[gi] < er[0]) | (es[gi] > er[1])] = 0\n        else:    wgt[(es[0] < er[0]) | (es[0] > er[1]) | (es[1] < er[0]) | (es[1] > er[1])] = 0\n    # angle must be in range\n    if polar_angle_range is not None:\n        pr = polar_angle_range\n        if pimms.is_str(pr):\n            pr = pr.lower()\n            if   pr in ['lh', 'rvf']: pr = (   0, 180)\n            elif pr in ['rh', 'lvf']: pr = (-180,   0)\n            else: raise ValueError('unrecognized polar angle range argument: %s' % pr)\n        if gold: wgt[(ps[gi] < pr[0]) | (ps[gi] > pr[1])] = 0\n        else:    wgt[(ps[0] < pr[0]) | (ps[0] > pr[1]) | (ps[1] < pr[0]) | (ps[1] > pr[1])] = 0\n    # okay! Now we can add the weight into the result\n    result['weight'] = wgt * zinv(np.sum(wgt))\n    # now we add a bunch of calculations we can perform on the data!\n    # first: metrics of distance\n    gsecc = es[gi] if gold else np.mean(es, axis=0)\n    gsang = ps[gi] if gold else np.mean(ps, axis=0)\n    gsrad = rs[gi] if gold else rs[0] if rs[1] is None else rs[1] if rs[0] is None else \\\n            np.mean(rs, axis=0)\n    gsecc_inv = zinv(gsecc)\n    gsrad_inv = None if gsrad is None else zinv(gsrad)\n    for (tag,resprop) in [('z', 'center'), ('polar_angle', 'polar_angle'),\n                          ('eccentricity', 'eccentricity'), ('x', 'x'), ('y', 'y')]:\n        serr = result[tag + '_1'] - result[tag + '_2']\n        aerr = np.abs(serr)\n        result[resprop + '_error'] = serr\n        result[resprop + '_abs_error'] = aerr\n        result[resprop + '_scaled_error'] = aerr * gsecc_inv\n        if gsrad_inv is not None:\n            result[resprop + '_radii_error'] = aerr * gsrad_inv\n    return pimms.itable(result)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visual_isolines(hemi, retinotopy='any', visual_area=Ellipsis, mask=None, surface='midgray',\n                    weights=Ellipsis, min_weight=0.05,\n                    eccentricity_range=None, polar_angle_range=None,\n                    eccentricity_lines=8, polar_angle_lines=8,\n                    max_step_scale=1.0/16.0):\n    '''\n    visual_isolines(hemi) yields a dictionary of the iso-angle and iso-eccentricity lines for the\n      given hemisphere hemi. The returned structure depends on the optional arguments and is\n      documented below.\n    visual_isolines(hemi, retino) uses the retinotopy found by retinotopy_data(hemi, retino).\n    \n    The following optional arguments may be given:\n      * visual_area (default: Ellipsis) specifies the property that should be used as the visual\n        area labels; if Ellipsis, then uses any visual area property found in the retinotopy data\n        and otherwise uses none; if set to None, then the visual area is always ignored. Note that\n        visual area values of 0 are ignored as well.\n      * mask (default: None) specifies an additional mask that should be applied; when visual_area\n        is None or not found, then this is the only masking that is performed.\n      * weights (default: Ellipsis) specifies the property that should be used as the weights on\n        the polar angle and eccentricity data; if Ellipsis, indicates that the variance explained or\n        weight property found in the retinotopy data should be used if any.\n      * min_weight (default: 0.05) specifies the minimum weight that should be included.\n      * polar_angle_range (default: None) may speficy the (min,max) polar angle to include.\n      * eccentricity_range (default: None) may specify the max or (min,max) eccentricity to include.\n      * eccentricity_lines (default: 8) specifies the eccentricity values at which to draw\n        iso-eccentricity lines. If this is an integer, it specifies that these should be chosen\n        using even spacing along the inverse CDF of the eccentricity values above the weight\n        threshold.\n      * polar_angle_lines (default: 8) specifies the polar angle values at which to draw the\n        iso-angular lines. If this is an integer, it specifies that these should be chosen using\n        even spacing along the inverse CDF of the eccentricity values above the weight threshold.\n      * surface (default: 'midgray') specifies the surface to use for calculating surface\n        coordinates if hemi is a topology and not a mesh.\n\n    Return value:\n      The return value of this function is a nested dictionary structure. If a visual area property\n      is used, then the first level of keys are the visual areas (excluding 0) that are found.\n      After this level, the next level's keys are 'polar_angle' and 'eccentricity' with the\n      following level's keys indicating the polar angle or eccentricity value that was used. These\n      internal dicts of iso-angle keys map to values that are lists of lines (there may be many\n      contiguous lines at one angle); each line is given by a map of addresses and other meta-data\n      about the line.\n    '''\n    from neuropythy import (to_mask, retinotopy_data, isolines, is_cortex, to_mesh)\n    from neuropythy.util import curry\n    # first off, apply the mask if specified:\n    if mask is not None: mask = to_mask(hemi, mask)\n    else: mask = hemi.tess.indices\n    # now, get the retinotopy\n    retino = retinotopy_data(hemi, retinotopy)\n    if   visual_area is Ellipsis: visual_area = retino.get('visual_area', None)\n    elif visual_area is not None: visual_area = hemi.property(visual_area, mask=mask, null=0)\n    # if there is a visual area, we just recurse setting these values as a mask\n    if visual_area is not None:\n        vas = np.unique(visual_area)\n        kw = dict(weights=weights, min_weight=min_weight, surface=surface, visual_area=None,\n                  eccentricity_lines=eccentricity_lines, polar_angle_lines=polar_angle_lines,\n                  eccentricity_range=eccentricity_range, polar_angle_range=polar_angle_range,\n                  max_step_scale=max_step_scale)\n        def recurse(va):\n            vamask = to_mask(hemi, (visual_area, va))\n            return visual_isolines(hemi, retinotopy, mask=np.intersect1d(mask, vamask), **kw)\n        return pimms.lazy_map({va:curry(recurse, va) for va in vas if va != 0})\n    # If there are weights, figure them out and apply them to the mask\n    ve = 'variance_explained'\n    weights = (retino[ve] if weights is Ellipsis and ve in retino   else\n               None       if weights is Ellipsis or weights is None else\n               hemi.property(weights))\n    if weights is not None and min_weight is not None:\n        mask = np.intersect1d(mask, np.where(weights >= min_weight)[0])\n    # make sure polar angle and eccen are in retino:\n    (ang,ecc) = as_retinotopy(retino, 'visual')\n    retino['polar_angle'] = ang\n    retino['eccentricity'] = ecc\n    # if there's a surface to get...\n    mesh = to_mesh((hemi, surface))\n    # when we calculate the isolines we use this function which also adds in the polar angles and\n    # eccentricities of the addressed lines\n    def calc_isolines(hemi, dat, ln, mask=None):\n        addrs = isolines(hemi, dat, ln, mask=mask, yield_addresses=True)\n        (angs,eccs) = [[hemi.interpolate(addr, retino[nm]) for addr in addrs]\n                       for nm in ('polar_angle', 'eccentricity')]\n        vxys = [np.asarray([e*np.cos(t), e*np.sin(t)])\n                for (a,e) in zip(angs,eccs) for t in [np.pi/180.0*(90.0 - a)]]\n        sxys = [mesh.unaddress(addr) for addr in addrs]\n        # trim/cut where needed\n        if max_step_scale is not None: \n            iiss = []\n            for (vxy,ecc) in zip(vxys, eccs):\n                dists = np.sqrt(np.sum((vxy[:,:-1] - vxy[:,1:])**2, 0))\n                es = np.mean([ecc[:-1], ecc[1:]], 0)\n                mx = max_step_scale * es\n                bd = dists > mx\n                ok = np.where(~bd)[0]\n                bd = np.where(bd)[0]\n                (oi,bi) = (0,0)\n                iis = []\n                while True:\n                    if   oi >= len(ok): break\n                    elif bi >= len(bd):\n                        iis.append(ok[oi:])\n                        break\n                    elif ok[oi] < bd[bi]:\n                        n = bd[bi] - ok[oi]\n                        iis.append(ok[oi:(oi+n)])\n                        oi += n\n                    else: bi += ok[oi] - bd[bi]\n                iiss.append(iis)\n            # okay, fix all the lists...\n            (angs,eccs) = [[u[ii]   for (u,iis) in zip(q,iiss) for ii in iis] for q in (angs,eccs)]\n            (vxys,sxys) = [[u[:,ii] for (u,iis) in zip(q,iiss) for ii in iis] for q in (vxys,sxys)]\n            addrs = [{k:addr[k][:,ii] for k in ('faces','coordinates')}\n                     for (addr,iis) in zip(addrs,iiss) for ii in iis]\n        return pimms.persist({'addresses': addrs, 'polar_angles': angs, 'eccentricities':eccs,\n                              'visual_coordinates': vxys, 'surface_coordinates': sxys})\n    # okay, we are operating over just the mask we have plus polar angle/eccentricity ranges\n    r = {}\n    (ang,ecc) = as_retinotopy(retino, 'visual')\n    if eccentricity_range is not None:\n        rng     = eccentricity_range\n        (mn,mx) = rng if pimms.is_vector(rng) else (np.min(ecc), rng)\n        mask    = np.setdiff1d(mask, np.where((ecc < mn) | (ecc > mx))[0])\n    if polar_angle_range is not None:\n        rng     = polar_angle_range\n        (mn,mx) = rng if pimms.is_vector(rng) else (np.min(ang), rng)\n        # we need to center the angle on this range here\n        aa = np.mean(rng)\n        aa = np.mod(ang + aa, 360) - aa\n        mask = np.setdiff1d(mask, np.where((aa < mn) | (aa > mx))[0])\n    for (p,dat,lns,rng) in zip(['polar_angle','eccentricity'], [ang,ecc],\n                               [polar_angle_lines, eccentricity_lines],\n                               [polar_angle_range, eccentricity_range]):\n        # first, figure out the lines themselves\n        if pimms.is_int(lns): lns = np.percentile(dat[mask], np.linspace(0, 100, 2*lns + 1)[1::2])\n        # now grab them from the hemisphere...\n        r[p] = pimms.lazy_map({ln:curry(calc_isolines, hemi, dat, ln, mask=mask) for ln in lns})\n    return pyr.pmap(r)", "response": "Generates a dictionary of iso - angle and iso - eccentricity lines for the given hemisphere hemisphere."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_retinotopy_potential(hemi, retinotopy=Ellipsis, mask=Ellipsis, weight=Ellipsis,\n                               surface='midgray', min_weight=Ellipsis, min_eccentricity=0.75,\n                               visual_area=None, map_visual_areas=Ellipsis,\n                               visual_area_field_signs=Ellipsis,\n                               measurement_uncertainty=0.3, measurement_knob=1,\n                               magnification_knob=0, fieldsign_knob=6, edge_knob=0):\n    '''\n    clean_retinotopy_potential(hemi) yields a retinotopic potential function for the given\n      hemisphere that, when minimized, should yeild a cleaned/smoothed version of the retinotopic\n      maps.\n\n    The potential function f returned by clean_retinotopy_potential() is a PotentialFunction object,\n    as defined in neuropythy.optimize. The potential function consists of four terms that are summed\n    with weights derived from the four '*_knob' options (see below). The function f as well as the\n    three terms that it comprises require as input a matrix X of the pRF centers of mesh or the\n    masked part of the mesh (X0 is the initial measurement matrix). These four potential terms are:\n      * The measurement potential. The retinotopic map that is being minimized is referred to as the\n        measured map, and the measurement potential function, fm(X), increases as X becomes farther\n        from X0. Explicitly, fm(X) is the sum over all pRF centers (x,y) in X (with initial position\n        (x0,y0) in X0) of exp(-0.5 * ((x - x0)**2 + (y - y0)**2) / s**2). The parameter s is the\n        initial eccentricity (sqrt(x0**2 + y0**2)) times the measurement_uncertainty option.\n      * The magnification potential. The retinotopy cleaning is performed in part by attempting to\n        smooth the visual magnification (the inverse of cortical magnification: deg**2 / mm**2)\n        across the cortical surface; the magnification potential fg(X) specifies how the visual\n        magnification contributes to the overall potential: it decreases as the magnification\n        becomes smoother and increases as it becomes less smooth. Explicitly, fg(X) is equal to the\n        sum over all pairs of faces (s,t) sharing one edge e of\n        (vmag(s) - sgn(vmag(t))*vmag(e))**2 + (vmag(t) - sgn(vmag(s))*vmag(e))**2. Note that the\n        function vmag(s) yields the areal visual magnification (deg**2 / mm**2) of any face s and\n        vmag(e) is the square of the linear magnification of any edge e; additionally, the sign of\n        vmag(s) for a face s is always equal to the fieldsign of the face (while for edges vmag(e)\n        is always positive).\n      * The fieldsign potential. The next way in which the potential function attempts to clean the\n        retinotopy is via the use of fieldsign: adjacent faces should have the same fieldsign under\n        most circumstanced. This is modeled by the function fs(X), which is 0 for any pair of faces\n        that have the same fieldsign and non-zero for faces that have different fieldsigns. The\n        form of fs(X) is the sum over all pairs of adjacent triangles (s,t) of -vmag(s)*vmag(t) if\n        vmag(s) and vmag(t) have different fieldsigns, otherwise 0.\n      * The edge potential. Finally, the potential function attempts to force edges to be smooth by\n        penalizing edges whose endpoints are far apart in the visual field. The edge potential\n        function fe(X) is equal to the sum for all edges (u,v) of\n        (x[u] - x[v])**2 + (y[u] - y[v])**2 / mean(eccen(u), eccen(v)).\n\n    Note additionally that all four potential functions are normalized by a factor intended to keep\n    them on similar scales (this factor is not mentioned above or below, but it is automatically\n    applied to all potential terms). For the magnification, fieldsign, and edge potential terms, the\n    normalization factor is 1/m where m is the number of non-perimeter edges (or, alternately, the\n    number of adjacent face pairs) in the mesh. For the measurement potential term, the\n    normalization factor is 1/W where W is the sum of the weights on the measurement vertices (if\n    no weights are given, they are considered to be 1 for each vertex).\n\n    The following options may be given:\n      * retinotopy (default: Ellipsis) specifies the retinotopy data to use for the hemisphere;\n        the argument may be a map from retinotopy_data or a valid argument to it. The default\n        indicates that the result of calling retinotopy_data(hemi) is used.\n      * mask (default: Ellipsis) specifies that the specific mask should be used; by default, the\n        mask is made using the vertices kept in to_flatmap('occipital_pole', hemi, radius=pi/2.75).\n      * weight (default: Ellipsis) specifies the weight to use; the default indicates that the\n        weight found in the retinotopy data, if any, should be used. If None, then all values\n        in the mask are equally weighted.\n      * visual_area (default: Ellipsis) specifies the visual area labels to use; the default\n        indicates that the visual area property found in the retinotopy data should be used, if any.\n        If None then no visual area splitting is done. This property is only important if \n        map_visual_areas is not False or None; otherwise it is ignored.\n      * map_visual_areas (default: Ellipsis) specifies whether the return value should be a lazy map\n        whose keys are visual area labels and whose values are recursed calls to this function for\n        only the subset of the mesh with the associated label. May be False or None to specify that\n        a single potential should be yielded. May be a list of labels to specify that only those\n        visual areas should be mapped; the default value (Ellipsis) uses all labels in visual_areas\n        except for 0.\n      * min_weight (default: Ellipsis) specifies the minimum weight to include, after the\n        weights have been normalized such that sum(weights) == 1. If the value is a list or\n        tuple containing a single item [p] then p is taken to be a percentile below which\n        vertices should be excluded. The default, Ellipsis, is equivalent to [5].\n      * min_eccentricity (default: 0.75) specifies the eccentricity below which no measurement-based\n        potential is applied; i.e., by default, vertices with eccentricity below 0.75 degrees will\n        be considered as having 0 weight.\n      * surface (default: 'midgray') specifies which surface should be used to establish cortical\n        magnification; may be 'pial', 'midgray', or 'white'.\n      * measurement_uncertainty (default: 0.3) is used to determine the standard deviation of the \n        Gaussian potential well used to prevent individual vertices with valid retinotopic\n        measurements from straying too far from their initial measured positions. In other words, if\n        a vertex has a weight that is above threshold and a pRF center of (x0,y0), then the \n        measurement-potential for that vertex is exp(-0.5 * ((x - x0)**2 + (y - y0)**2)/s**2) where\n        (x,y) is the center of the pRF during minimization and s is equal to\n        measurement_uncertainty * sqrt(x0**2 + y0**2).\n      * measurement_knob, magnification_knob, fieldsign_knob, and edge_knob (defaults: 1, 0, 12, 0,\n        respectively) specify the relative weights of the terms of the potential function on a log2\n        scale. In other words, if the measurement, magnification, fieldsign, and edge potential\n        terms are fm, fg, fs, and fe while the knobs are km, kg, ks, and ke, then the overall\n        potential function f is equal to:\n        f(X) = (2**km * fm(X) + 2**kg * fg(X) + 2**ks * fs(X) + 2**ke * fe(X)) / q\n        where w = (2**km + 2**kg + 2**ks + 2**ke)\n        If any knob is set to None, then its value is 0 instead of 2**k.\n    '''\n    from neuropythy.util import curry\n    import neuropythy.optimize as op\n    # first, get the mesh and the retinotopy data\n    mesh = geo.to_mesh((hemi, surface))\n    rdat = (retinotopy_data(mesh) if retinotopy is Ellipsis   else\n            retinotopy            if pimms.is_map(retinotopy) else\n            retinotopy_data(mesh, retinotopy))\n    lbls = (rdat.get('visual_area') if visual_area is Ellipsis else\n            None                    if visual_area is None     else\n            hemi.property(visual_area))\n    wght = (rdat.get('variance_explained') if weight is Ellipsis                      else\n            weight                         if pimms.is_vector(weight)                 else\n            rdat.get(weight)               if pimms.is_str(weight) and weight in rdat else\n            hemi.property(weight)          if weight is not None                      else\n            None)\n    # figure out the mask\n    if mask is Ellipsis:\n        if mesh.coordinates.shape[0] == 2: mask = mesh.indices\n        else: mask = geo.to_flatmap('occipital_pole', hemi, radius=np.pi/2.75).labels\n    else: mask = hemi.mask(mask, indices=True)\n    global_field_sign = None\n    # if we are splitting on visual area, we should do that here:\n    if map_visual_areas and lbls is not None:\n        # get the visual areas\n        vas = (np.unique(lbls)                    if map_visual_areas == 'all'           else\n               np.setdiff1d(np.unique(lbls), [0]) if map_visual_areas in [True,Ellipsis] else\n               np.unique(map_visual_areas))\n        # we also want to have the field-sign map handy if provided\n        if visual_area_field_signs is None: visual_area_field_signs = {}\n        elif visual_area_field_signs is Ellipsis: visual_area_field_signs = {1:-1, 2:1, 3:-1, 4:1}\n        # special case when map_visual_areas is an integer/string (label)\n        if pimms.is_int(map_visual_areas) or pimms.is_str(map_visual_areas):\n            mask = np.intersect1d(mask, np.where(lbls == map_visual_areas)[0])\n            global_field_sign = visual_area_field_signs.get(map_visual_areas)\n        else: # otherwise we return a lazy map\n            kw = dict(retinotopy=rdat, mask=mask, weight=wght,\n                      surface=surface, min_weight=min_weight, min_eccentricity=min_eccentricity,\n                      visual_area=lbls, measurement_uncertainty=measurement_uncertainty,\n                      measurement_knob=measurement_knob,\n                      magnification_knob=magnification_knob, fieldsign_knob=fieldsign_knob,\n                      edge_knob=edge_knob, visual_area_field_signs=visual_area_field_signs)\n            return pimms.lazy_map(\n                {lbl: curry(clean_retinotopy_potential, hemi, map_visual_areas=lbl, **kw)\n                 for lbl in vas})\n    # fix rdat, weight, and mesh to match the mask\n    (supermesh, orig_mask) = (mesh, mask)\n    rdat = {k:(v[mask] if len(v) > len(mask) else v) for (k,v) in six.iteritems(rdat)}\n    mesh = supermesh.submesh(mask)\n    # possible that the mask got further downsampled:\n    mask = supermesh.tess.index(mesh.labels)\n    if len(mask) == len(orig_mask): smsk = np.arange(len(mask))\n    else:\n        tmp = set(mask)\n        smsk = np.asarray([k for (k,u) in enumerate(orig_mask) if u in tmp])\n    n = mesh.vertex_count # number vertices\n    N = 2*n # number parameters\n    if wght is None:                  wght = np.ones(n)\n    elif len(wght) == len(orig_mask): wght = np.array(wght)[smsk]\n    elif len(wght) > n:               wght = np.array(wght)[mask]\n    else:                             wght = np.array(wght)\n    wght[~np.isfinite(wght)] = 0\n    if min_eccentricity < 0 or np.isclose(min_eccentricity, 0):\n        raise ValueError('min_eccentricity should be small but must be > 0')\n    # we'll need a potential function...\n    # The input to the potential function will be a 2 x N matrix of (x,y) visual field coordinates:\n    xy      = op.identity\n    (x,y)   = (xy[np.arange(0,N,2)],xy[np.arange(1,N,2)])\n    # We have a few components to the potential function\n    # [1] Deviation from measurements:\n    # These are the initial measurements we will use\n    xy0     = np.array(as_retinotopy(rdat, 'geographical')).T\n    if len(xy0) == len(orig_mask): xy0 = xy0[smsk]\n    elif len(xy0) > n:             xy0 = xy0[mask]\n    (x0,y0) = xy0.T\n    xy0     = xy0.flatten()\n    ecc0    = np.sqrt(x0**2 + y0**2)\n    ii      = np.where(ecc0 > min_eccentricity)[0]\n    minw    = (0                             if min_weight is None          else\n               np.percentile(wght[ii], 5)    if min_weight is Ellipsis      else\n               min_weight                    if pimms.is_number(min_weight) else\n               0                             if np.std(wght[ii]) < 0.00001  else\n               np.percentile(wght[ii], min_weight[0]))\n    ii      = np.intersect1d(ii, np.where(wght > minw)[0])\n    wsum    = np.sum(wght[ii])\n    if wsum < 0 or np.isclose(wsum, 0): raise ValueError('all-zero weights given')\n    wght    = wght / wsum\n    s2_meas = (measurement_uncertainty * ecc0[ii])**2\n    d2_meas = (x[ii] - x0[ii])**2 + (y[ii] - y0[ii])**2\n    f_meas  = (1 - op.exp(-0.5*d2_meas/s2_meas)) * wght[ii]\n    f_meas  = op.sum(f_meas)\n    # [2] For adjacent triangles, how different are the cortical magnifications?\n    sarea   = mesh.face_areas\n    faces   = mesh.tess.indexed_faces.T\n    selen   = mesh.edge_lengths\n    # we want to ensure that vmag is locally smooth across triangles, but we need\n    # to make sure there aren't any edges or faces with 0 surface-areas (so that\n    # we don't divide by zero)\n    mnden   = 0.0001\n    (e,s,t) = np.transpose([(i,e[0],e[1]) for (i,e) in enumerate(mesh.tess.edge_faces)\n                            if len(e) == 2 and selen[i] > mnden\n                            if sarea[e[0]] > mnden and sarea[e[1]] > mnden])    \n    m       = len(e)\n    (fis,q) = np.unique(np.concatenate([s,t]), return_inverse=True)\n    (s,t)   = np.reshape(q, (2,-1))\n    faces   = faces[fis]\n    sarea   = sarea[fis]\n    selen   = selen[e]\n    (u,v)   = mesh.tess.indexed_edges[:,e]\n    # we will use visual mag instead of cortical mag: this way we aren't worried about\n    # varea going to 0 and creating a singularity, and it should have a linear \n    # relationship with eccentricity\n    velen2  = (x[u] - x[v])**2 + (y[u] - y[v])**2\n    vme     = velen2 / selen**2 # visual magnification: edges\n    varea   = op.signed_face_areas(faces)\n    vmf     = varea / sarea # visual magnification: faces\n    vms     = vmf[s]\n    vmt     = vmf[t]\n    vsgns   = op.sign(vmf)\n    f_magn  = (1.0 / m) * op.sum((vms - vsgns[t]*vme)**2 + (vmt - vsgns[s]*vme)**2)\n    # [3] we want a special function for faces whose vmags are different signs\n    if global_field_sign is None:\n        f_sign = op.compose(op.piecewise(0, ((-np.inf, 0), -op.identity)), vms*vmt)\n        f_sign = (1.0 / m) * op.sum(f_sign)\n    else:\n        vmfsgn = vmf * global_field_sign\n        f_sign = op.compose(op.piecewise(0, ((-np.inf, 0), -op.identity)), vmfsgn)\n        f_sign = (1.0 / m) * op.sum(f_sign)\n    # and the edge potential...\n    ex      = 0.5*(x[u] + x[v])\n    ey      = 0.5*(y[u] + y[v])\n    eecc2   = (ex**2 + ey**2)\n    f_edge  = (1.0 / m) * op.sum(((x[u] - x[v])**2 + (y[u] - y[v])**2) / (eecc2 + 0.05))\n    # This is the potential function:\n    (k_meas, k_magn, k_sign, k_edge) = [\n        0 if knob is None else (2**knob)\n        for knob in (measurement_knob, magnification_knob, fieldsign_knob, edge_knob)]\n    fs = (k_meas*f_meas, k_magn*f_magn, k_sign*f_sign, k_edge*f_edge)\n    f = (fs[0] + fs[1] + fs[2] + fs[3]) / (k_meas + k_magn + k_sign + k_edge)\n    xy0 = np.reshape(xy0, (-1,2))\n    object.__setattr__(f, 'meta_data',\n                       pyr.m(f_meas=f_meas, f_magn=f_magn, f_sign=f_sign, f_edge=f_edge,\n                             mesh=mesh, X0=xy0))\n    return f", "response": "This function returns a function that can be used to clean up the non - minimized or smoothed version of the retinotopy of the given hemisphere."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subject(sid, subjects_path=None, meta_data=None, default_alignment='MSMAll'):\n    '''\n    subject(sid) yields a HCP Subject object for the subject with the given subject id; sid may be a\n      path to a subject or a subject id, in which case the subject paths are searched for it.\n    subject(None, path) yields a non-standard HCP subject at the given path.\n    subject(sid, path) yields the specific HCP Subject at the given path.\n\n    Subjects are cached and not reloaded.  Note that subects returned by subject() are always\n    persistent Immutable objects; this means that you must create a transient version of the subject\n    to modify it via the member function sub.transient(). Better, you can make copies of the objects\n    with desired modifications using the copy method.\n\n    This function works with the neuropythy.hcp.auto_download() function; if you have enabled auto-\n    downloading, then subjects returned from this subject may be downloading themselves lazily.\n    '''\n    if subjects_path is None:\n        if os.path.isdir(str(sid)):\n            (fdir, fnm) = os.path.split(str(sid))\n            try: sid = to_subject_id(fnm)\n            except Exception: sid = None\n            pth = fdir\n        else:\n            sid = to_subject_id(sid)\n            fnm = str(sid)\n            fdir = find_subject_path(sid)\n            if fdir is None: raise ValueError('Could not locate subject with id \\'%s\\'' % sid)\n            pth = os.path.split(fdir)[0]\n    else:\n        if sid is None:\n            (pth, fnm) = os.path.split(subjects_path)\n        else:\n            sid = to_subject_id(sid)\n            fnm = str(sid)\n            fdir = subjects_path\n    fdir = os.path.abspath(os.path.join(pth, fnm))\n    if fdir in subject._cache: return subject._cache[fdir]\n    sub = Subject(sid, fdir, meta_data=meta_data, default_alignment=default_alignment).persist()\n    if isinstance(sub, Subject): subject._cache[fdir] = sub\n    return sub", "response": "Returns a HCP Subject object for the given subject id."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(*args):\n    '''\n    benson14_retinotopy.main(args...) runs the benson14_retinotopy command; see \n    benson14_retinotopy.info for more information.\n    '''\n    # Parse the arguments...\n    (args, opts) = _benson14_parser(*args)\n    # help?\n    if opts['help']:\n        print(info, file=sys.stdout)\n        return 1\n    # verbose?\n    if opts['verbose']:\n        def note(s):\n            print(s, file=sys.stdout)\n            return True\n    else:\n        def note(s): return False\n    # based on format, how do we export?\n    sfmt = opts['surf_format'].lower()\n    if sfmt in ['curv', 'auto', 'automatic', 'morph']:\n        sfmt = 'freesurfer_morph'\n        sext = ''\n    elif sfmt == 'nifti':\n        sext = '.nii.gz'\n    elif sfmt in ['mgh', 'mgz', 'nii', 'nii.gz']:\n        sext = '.' + sfmt\n    else:\n        raise ValueError('Unknown surface format: %s' % opts['surf_format'])\n    vfmt = opts['vol_format'].lower()\n    if vfmt == 'nifti':\n        vext = '.nii.gz'\n    elif vfmt in ['mgh', 'mgz', 'nii', 'nii.gz']:\n        vext = '.' + vfmt\n    else:\n        raise ValueError('Unknown volume format: %s' % opts['vol_format'])\n    # Add the subjects directory, if there is one\n    if 'subjects_dir' in opts and opts['subjects_dir'] is not None:\n        add_subject_path(opts['subjects_dir'])\n    ow = not opts['no_overwrite']\n    nse = opts['no_surf_export']\n    nve = opts['no_vol_export']\n    tr = {'angle': opts['angle_tag'],\n          'eccen': opts['eccen_tag'],\n          'varea': opts['label_tag'],\n          'sigma': opts['sigma_tag']}\n    # okay, now go through the subjects...\n    for subnm in args:\n        note('Processing subject %s:' % subnm)\n        sub = subject(subnm)\n        note('   - Interpolating template...')\n        (lhdat, rhdat) = predict_retinotopy(sub,\n                                            template=opts['template'],\n                                            registration=opts['registration'])\n        # Export surfaces\n        if nse:\n            note('   - Skipping surface export.')\n        else:\n            note('   - Exporting surfaces:')\n            for (t,dat) in six.iteritems(lhdat):\n                flnm = os.path.join(sub.path, 'surf', 'lh.' + tr[t] + sext)\n                if ow or not os.path.exists(flnm):\n                    note('    - Exporting LH prediction file: %s' % flnm)\n                    nyio.save(flnm, dat, format=sfmt)\n                else:\n                    note('    - Not overwriting existing file: %s' % flnm)\n            for (t,dat) in six.iteritems(rhdat):\n                flnm = os.path.join(sub.path, 'surf', 'rh.' + tr[t] + sext)\n                if ow or not os.path.exists(flnm):\n                    note('    - Exporting RH prediction file: %s' % flnm)\n                    nyio.save(flnm, dat, format=sfmt)\n                else:\n                    note('    - Not overwriting existing file: %s' % flnm)\n        # Export volumes\n        if nve:\n            note('   - Skipping volume export.')\n        else:\n            note('   - Exporting Volumes:')\n            for t in lhdat.keys():\n                flnm = os.path.join(sub.path, 'mri', tr[t] + vext)\n                if ow or not os.path.exists(flnm):\n                    note('    - Preparing volume file: %s' % flnm)\n                    dtyp = (np.int32 if t == 'varea' else np.float32)\n                    vol = sub.cortex_to_image(\n                        (lhdat[t], rhdat[t]),\n                        method=('nearest' if t == 'varea' else 'linear'),\n                        dtype=dtyp)\n                    note('    - Exporting volume file: %s' % flnm)\n                    nyio.save(flnm, vol, like=sub)\n                else:\n                    note('    - Not overwriting existing file: %s' % flnm)\n        note('   Subject %s finished!' % sub.name)\n    return 0", "response": "Main function for the benson14_retinotopy command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef notify(self, state):\n        try:\n            self.socket.sendall(_b(state))\n        except:\n            if self.debug:\n                raise", "response": "Send a notification to systemd."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef typename(obj):\n    if hasattr(obj, '__class__'):\n        return getattr(obj, '__class__').__name__\n    else:\n        return type(obj).__name__", "response": "Returns the type of obj as a string. More descriptive and specific than\n    type ( obj ) and safe for any object unlike __class__."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stats(self, **kwargs):\n\n\t\tparams = {}\n\t\tparams.update(kwargs)\n\t\tresponse = self.__request('global/', params)\n\t\treturn response", "response": "This endpoint displays the global data found at the top of coinmarketcap. com."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the underlying : class : ChoiceItem for a given value.", "response": "def get_choice(cls, value):\n        \"\"\"\n        Return the underlying :class:`ChoiceItem` for a given value.\n        \"\"\"\n        attribute_for_value = cls.attributes[value]\n        return cls._fields[attribute_for_value]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping a function to swallow errors in the resultset tables.", "response": "def _surpress_formatting_errors(fn):\n    \"\"\"\n    I know this is dangerous and the wrong way to solve the problem, but when\n    using both row and columns summaries it's easier to just swallow errors\n    so users can format their tables how they need.\n    \"\"\"\n    @wraps(fn)\n    def inner(*args, **kwargs):\n        try:\n            return fn(*args, **kwargs)\n        except ValueError:\n            return \"\"\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _format_numer(number_format, prefix='', suffix=''):\n    @_surpress_formatting_errors\n    def inner(v):\n        if isinstance(v, Number):\n            return (\"{{}}{{:{}}}{{}}\"\n                    .format(number_format)\n                    .format(prefix, v, suffix))\n        else:\n            raise TypeError(\"Numberic type required.\")\n    return inner", "response": "Format a number to a string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_percent(precision=2, **kwargs):\n    if not isinstance(precision, Integral):\n        raise TypeError(\"Precision must be an integer.\")\n\n    return _surpress_formatting_errors(\n        _format_numer(\".{}%\".format(precision))\n    )", "response": "Convert a number to a percentage string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting value to unit.", "response": "def as_unit(unit, precision=2, location='suffix'):\n    \"\"\"Convert value to unit.\n\n    Parameters:\n    -----------\n    :param v: numerical value\n    :param unit: string of unit\n    :param precision: int\n        decimal places to round to\n    :param location:\n        'prefix' or 'suffix' representing where the currency symbol falls\n        relative to the value\n    \"\"\"\n    if not isinstance(precision, Integral):\n        raise TypeError(\"Precision must be an integer.\")\n\n    if location == 'prefix':\n        formatter = partial(_format_numer, prefix=unit)\n    elif location == 'suffix':\n        formatter = partial(_format_numer, suffix=unit)\n    else:\n        raise ValueError(\"location must be either 'prefix' or 'suffix'.\")\n\n    return _surpress_formatting_errors(\n        formatter(\"0.{}f\".format(precision))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply(self, df):\n\n        if self.subset:\n            if _axis_is_rows(self.axis):\n                df = df[self.subset]\n            if _axis_is_cols(self.axis):\n                df = df.loc[self.subset]\n\n        result = df.agg(self.func, axis=self.axis, *self.args, **self.kwargs)\n        result.name = self.title\n        return result", "response": "Compute aggregate over DataFrame df"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply Summary over Pandas Styler", "response": "def apply(self, styler):\n        \"\"\"Apply Summary over Pandas Styler\"\"\"\n        return styler.format(self.formatter, *self.args, **self.kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _apply_summaries(self):\n\n        def as_frame(r):\n            if isinstance(r, pd.Series):\n                return r.to_frame()\n            else:\n                return r\n\n        df = self.data\n\n        if df.index.nlevels > 1:\n            raise ValueError(\n                \"You cannot currently have both summary rows and columns on a \"\n                \"MultiIndex.\"\n            )\n\n        _df = df\n        if self.summary_rows:\n            rows = pd.concat([agg.apply(_df)\n                              for agg in self._cleaned_summary_rows], axis=1).T\n            df = pd.concat([df, as_frame(rows)], axis=0)\n\n        if self.summary_cols:\n            cols = pd.concat([agg.apply(_df)\n                              for agg in self._cleaned_summary_cols], axis=1)\n            df = pd.concat([df, as_frame(cols)], axis=1)\n\n        return df", "response": "Add all summary rows and columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef style(self):\n        row_titles = [a.title for a in self._cleaned_summary_rows]\n        col_titles = [a.title for a in self._cleaned_summary_cols]\n        row_ix = pd.IndexSlice[row_titles, :]\n        col_ix = pd.IndexSlice[:, col_titles]\n\n        def handle_na(df):\n            df.loc[col_ix] = df.loc[col_ix].fillna('')\n            df.loc[row_ix] = df.loc[row_ix].fillna('')\n            return df\n\n        styler = (\n            self\n            .frame\n            .pipe(handle_na)\n            .style\n            .applymap(lambda r: 'font-weight: 900', subset=row_ix)\n            .applymap(lambda r: 'font-weight: 900', subset=col_ix)\n        )\n\n        for formatter in self.formatters:\n            styler = formatter.apply(styler)\n\n        return styler", "response": "Add summaries and convert to Pandas Styler"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef summary(self,\n                func=methodcaller('sum'),\n                title='Total',\n                axis=0,\n                subset=None,\n                *args,\n                **kwargs):\n        \"\"\"Add multiple summary rows or columns to the dataframe.\n\n        Parameters\n        ----------\n        :param func: function to be used for a summary.\n        :param titles: Title for this summary column.\n        :param axis:\n            Same as numpy and pandas axis argument. A value of None will cause\n            the summary to be applied to both rows and columns.\n        :param args: Positional arguments passed to all the functions.\n        :param kwargs: Keyword arguments passed to all the functions.\n\n        The results of summary can be chained together.\n        \"\"\"\n\n        if axis is None:\n            return (\n                self\n                .summary(\n                    func=func,\n                    title=title,\n                    axis=0,\n                    subset=subset,\n                    *args,\n                    **kwargs\n                )\n                .summary(\n                    func=func,\n                    title=title,\n                    axis=1,\n                    subset=subset,\n                    *args,\n                    **kwargs\n                )\n            )\n        else:\n            agg = Aggregate(title, func, subset=subset,\n                            axis=axis, *args, **kwargs)\n            return self._add_summary(agg)", "response": "Adds multiple summary rows or columns to the dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_percent(self, precision=2, *args, **kwargs):\n        f = Formatter(as_percent(precision), args, kwargs)\n        return self._add_formatter(f)", "response": "Format the subset as percentages."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_currency(self, currency='USD', locale=LOCALE_OBJ, *args, **kwargs):\n        f = Formatter(\n            as_currency(currency=currency, locale=locale),\n            args,\n            kwargs\n        )\n        return self._add_formatter(f)", "response": "Format subset as currency"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting subset as with units", "response": "def as_unit(self, unit, location='suffix', *args, **kwargs):\n        \"\"\"Format subset as with units\n\n        :param unit: string to use as unit\n        :param location: prefix or suffix\n        :param subset: Pandas subset\n        \"\"\"\n        f = Formatter(\n            as_unit(unit, location=location),\n            args,\n            kwargs\n        )\n        return self._add_formatter(f)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreports overall results of the assessment process", "response": "def result_report_overall(self):\n        \"\"\"Report overall results\n\n        Returns\n        -------\n        str\n            result report in string format\n\n        \"\"\"\n\n        results = self.results_overall_metrics()\n\n        output = self.ui.section_header('Overall metrics (micro-average)', indent=2) + '\\n'\n\n        if results['f_measure']:\n            output += self.ui.line('F-measure', indent=2) + '\\n'\n            output += self.ui.data(field='F-measure (F1)', value=float(results['f_measure']['f_measure']) * 100,\n                                   unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Precision', value=float(results['f_measure']['precision']) * 100,\n                                   unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Recall', value=float(results['f_measure']['recall']) * 100,\n                                   unit='%', indent=4) + '\\n'\n\n        if results['error_rate']:\n            output += self.ui.line('Error rate', indent=2) + '\\n'\n            output += self.ui.data(field='Error rate (ER)', value=float(results['error_rate']['error_rate']),\n                                   indent=4) + '\\n'\n            output += self.ui.data(field='Substitution rate', value=float(results['error_rate']['substitution_rate']),\n                                   indent=4) + '\\n'\n            output += self.ui.data(field='Deletion rate', value=float(results['error_rate']['deletion_rate']),\n                                   indent=4) + '\\n'\n            output += self.ui.data(field='Insertion rate', value=float(results['error_rate']['insertion_rate']),\n                                   indent=4) + '\\n'\n\n        if results['accuracy']:\n            output += self.ui.line('Accuracy', indent=2) + '\\n'\n            output += self.ui.data(field='Sensitivity', value=float(results['accuracy']['sensitivity']*100),\n                                   unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Specificity', value=float(results['accuracy']['specificity'] * 100),\n                                   unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Balanced accuracy', value=float(results['accuracy']['balanced_accuracy'] * 100),\n                                   unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Accuracy', value=float(results['accuracy']['accuracy'] * 100),\n                                   unit='%', indent=4) + '\\n'\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef result_report_class_wise_average(self):\n\n        results = self.results_class_wise_average_metrics()\n\n        output = self.ui.section_header('Class-wise average metrics (macro-average)', indent=2) + '\\n'\n\n        if results['f_measure']:\n            output += self.ui.line('F-measure', indent=2) + '\\n'\n            output += self.ui.data(field='F-measure (F1)', value=float(results['f_measure']['f_measure']) * 100,\n                                   unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Precision', value=float(results['f_measure']['precision']) * 100,\n                                   unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Recall', value=float(results['f_measure']['recall']) * 100,\n                                   unit='%', indent=4) + '\\n'\n\n        if results['error_rate']:\n            output += self.ui.line('Error rate', indent=2) + '\\n'\n            output += self.ui.data(field='Error rate (ER)', value=float(results['error_rate']['error_rate']),\n                                   indent=4) + '\\n'\n            output += self.ui.data(field='Deletion rate', value=float(results['error_rate']['deletion_rate']),\n                                   indent=4) + '\\n'\n            output += self.ui.data(field='Insertion rate', value=float(results['error_rate']['insertion_rate']),\n                                   indent=4) + '\\n'\n\n        if results['accuracy']:\n            output += self.ui.line('Accuracy', indent=2) + '\\n'\n            output += self.ui.data(field='Sensitivity', value=float(results['accuracy']['sensitivity']*100),\n                                   unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Specificity', value=float(results['accuracy']['specificity'] * 100),\n                                   unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Balanced accuracy', value=float(results['accuracy']['balanced_accuracy'] * 100),\n                                   unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Accuracy', value=float(results['accuracy']['accuracy'] * 100),\n                                   unit='%', indent=4) + '\\n'\n\n        output += \"  \\n\"\n\n        return output", "response": "Report class - wise average metrics for a single object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef result_report_class_wise(self):\n\n        results = self.results_class_wise_metrics()\n\n        accuracy_present = True\n        for event_label in self.event_label_list:\n            if 'accuracy' not in results[event_label]['accuracy']:\n                accuracy_present = False\n\n        output = self.ui.section_header('Class-wise metrics', indent=2) + '\\n'\n\n        headers = ['Event label', 'Nref', 'Nsys', 'F', 'Pre', 'Rec', 'ER', 'Del', 'Ins']\n        sep = ['-', '-', '-', '-', '-', '-', '-', '-', '-']\n        widths = [15, 8, 8, 9, 9, 9, 9, 9, 9]\n        separators = [True, False, True, False, False, True, False, False, True]\n        if accuracy_present:\n            headers += ['Sens', 'Spec', 'Bacc', 'Acc']\n            sep += ['-', '-', '-', '-']\n            widths += [9, 9, 9, 9]\n            separators += [False, False, False, False]\n\n        output += self.ui.row(*headers, widths=widths, indent=4, separators=separators) + '\\n'\n        output += self.ui.row(*sep) + '\\n'\n        for event_label in self.event_label_list:\n            data = [\n                event_label,\n                results[event_label]['count']['Nref'],\n                results[event_label]['count']['Nsys'],\n                results[event_label]['f_measure']['f_measure'] * 100,\n                results[event_label]['f_measure']['precision'] * 100,\n                results[event_label]['f_measure']['recall'] * 100,\n                results[event_label]['error_rate']['error_rate'],\n                results[event_label]['error_rate']['deletion_rate'],\n                results[event_label]['error_rate']['insertion_rate']\n            ]\n\n            types = [\n                'str15',\n                'int',\n                'int',\n                'float1_percentage',\n                'float1_percentage',\n                'float1_percentage',\n                'float2',\n                'float2',\n                'float2',\n            ]\n\n            if accuracy_present:\n                data += [\n                    results[event_label]['accuracy']['sensitivity'] * 100,\n                    results[event_label]['accuracy']['specificity'] * 100,\n                    results[event_label]['accuracy']['balanced_accuracy'] * 100,\n                    results[event_label]['accuracy']['accuracy'] * 100\n                ]\n\n                types += [\n                    'float1_percentage',\n                    'float1_percentage',\n                    'float1_percentage',\n                    'float1_percentage',\n                ]\n\n            output += self.ui.row(*data, types=types) + '\\n'\n\n        return output", "response": "Report class - wise results of the test."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef results_class_wise_metrics(self):\n\n        results = {}\n        for event_label in self.event_label_list:\n            if event_label not in results:\n                results[event_label] = {}\n\n            results[event_label]['f_measure'] = self.class_wise_f_measure(event_label)\n            results[event_label]['accuracy'] = self.class_wise_accuracy(event_label)\n            results[event_label]['error_rate'] = self.class_wise_error_rate(event_label)\n            results[event_label]['count'] = self.class_wise_count(event_label)\n\n        return results", "response": "Returns a dictionary of class - wise metrics for all class - wise infrastructure in the current language."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of class - wise averaged metrics for the current class.", "response": "def results_class_wise_average_metrics(self):\n        \"\"\"Class-wise averaged metrics\n\n        Returns\n        -------\n        dict\n            results in a dictionary format\n\n        \"\"\"\n\n        event_wise_results = self.results_class_wise_metrics()\n\n        event_wise_f_measure = []\n        event_wise_precision = []\n        event_wise_recall = []\n        \n        event_wise_error_rate = []\n        event_wise_deletion_rate = []\n        event_wise_insertion_rate = []\n        \n        event_wise_sensitivity = []\n        event_wise_specificity = []\n        event_wise_balanced_accuracy = []\n        event_wise_accuracy = []\n\n        for event_label in event_wise_results:\n            # F-measure\n            event_wise_f_measure.append(event_wise_results[event_label]['f_measure']['f_measure'])\n            event_wise_precision.append(event_wise_results[event_label]['f_measure']['precision'])\n            event_wise_recall.append(event_wise_results[event_label]['f_measure']['recall'])\n            \n            # Error rate\n            event_wise_error_rate.append(event_wise_results[event_label]['error_rate']['error_rate'])\n            event_wise_deletion_rate.append(event_wise_results[event_label]['error_rate']['deletion_rate'])\n            event_wise_insertion_rate.append(event_wise_results[event_label]['error_rate']['insertion_rate'])\n\n            # Accuracy\n            if 'sensitivity' in event_wise_results[event_label]['accuracy']:\n                event_wise_sensitivity.append(event_wise_results[event_label]['accuracy']['sensitivity'])\n\n            if 'specificity' in event_wise_results[event_label]['accuracy']:\n                event_wise_specificity.append(event_wise_results[event_label]['accuracy']['specificity'])\n\n            if 'balanced_accuracy' in event_wise_results[event_label]['accuracy']:\n                event_wise_balanced_accuracy.append(event_wise_results[event_label]['accuracy']['balanced_accuracy'])\n\n            if 'accuracy' in event_wise_results[event_label]['accuracy']:\n                event_wise_accuracy.append(event_wise_results[event_label]['accuracy']['accuracy'])\n\n        if event_wise_f_measure:\n            event_wise_f_measure_dict = {\n                'f_measure': float(numpy.nanmean(event_wise_f_measure)),\n                'precision': float(numpy.nanmean(event_wise_precision)),\n                'recall': float(numpy.nanmean(event_wise_recall))\n            }\n\n        else:\n            event_wise_f_measure_dict = {}\n\n        if event_wise_error_rate:\n            event_wise_error_rate_dict = {\n                'error_rate': float(numpy.nanmean(event_wise_error_rate)),\n                'deletion_rate': float(numpy.nanmean(event_wise_deletion_rate)),\n                'insertion_rate': float(numpy.nanmean(event_wise_insertion_rate))\n            }\n\n        else:\n            event_wise_error_rate_dict = {}\n\n        if event_wise_accuracy:\n            event_wise_accuracy_dict = {\n                'sensitivity': float(numpy.nanmean(event_wise_sensitivity)),\n                'specificity': float(numpy.nanmean(event_wise_specificity)),\n                'balanced_accuracy': float(numpy.nanmean(event_wise_balanced_accuracy)),\n                'accuracy': float(numpy.nanmean(event_wise_accuracy))\n            }\n\n        else:\n            event_wise_accuracy_dict = {}\n\n        return {\n            'f_measure': event_wise_f_measure_dict,\n            'error_rate': event_wise_error_rate_dict,\n            'accuracy': event_wise_accuracy_dict\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate file pair (reference and estimated) Parameters ---------- reference_event_list : list of dict or dcase_util.containers.MetaDataContainer Reference event list. estimated_event_list : list of dict or dcase_util.containers.MetaDataContainer Estimated event list. evaluated_length_seconds : float, optional Evaluated length. If none given, maximum offset is used. Default value None Returns ------- self", "response": "def evaluate(self, reference_event_list, estimated_event_list, evaluated_length_seconds=None):\n        \"\"\"Evaluate file pair (reference and estimated)\n\n        Parameters\n        ----------\n\n        reference_event_list : list of dict or dcase_util.containers.MetaDataContainer\n            Reference event list.\n\n        estimated_event_list : list of dict or dcase_util.containers.MetaDataContainer\n            Estimated event list.\n\n        evaluated_length_seconds : float, optional\n            Evaluated length. If none given, maximum offset is used.\n            Default value None\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n\n        # Make sure input is dcase_util.containers.MetaDataContainer\n        if not isinstance(reference_event_list, dcase_util.containers.MetaDataContainer):\n            reference_event_list = dcase_util.containers.MetaDataContainer(reference_event_list)\n\n        if not isinstance(estimated_event_list, dcase_util.containers.MetaDataContainer):\n            estimated_event_list = dcase_util.containers.MetaDataContainer(estimated_event_list)\n\n        # Check that input event list have event only from one file\n        reference_files = reference_event_list.unique_files\n        if len(reference_files) > 1:\n            raise ValueError(\n                \"reference_event_list contains events from multiple files. Evaluate only file by file.\"\n            )\n\n        estimated_files = estimated_event_list.unique_files\n        if len(estimated_files) > 1:\n            raise ValueError(\n                \"estimated_event_list contains events from multiple files. Evaluate only file by file.\"\n            )\n\n        # Evaluate only valid events\n        valid_reference_event_list = dcase_util.containers.MetaDataContainer()\n        for item in reference_event_list:\n            if 'event_onset' in item and 'event_offset' in item and 'event_label' in item:\n                valid_reference_event_list.append(item)\n\n            elif 'onset' in item and 'offset' in item and 'event_label' in item:\n                valid_reference_event_list.append(item)\n\n        reference_event_list = valid_reference_event_list\n\n        valid_estimated_event_list = dcase_util.containers.MetaDataContainer()\n        for item in estimated_event_list:\n            if 'event_onset' in item and 'event_offset' in item and 'event_label' in item:\n                valid_estimated_event_list.append(item)\n\n            elif 'onset' in item and 'offset' in item and 'event_label' in item:\n                valid_estimated_event_list.append(item)\n\n        estimated_event_list = valid_estimated_event_list\n\n        # Convert event list into frame-based representation\n        reference_event_roll = util.event_list_to_event_roll(\n            source_event_list=reference_event_list,\n            event_label_list=self.event_label_list,\n            time_resolution=self.time_resolution\n        )\n\n        estimated_event_roll = util.event_list_to_event_roll(\n            source_event_list=estimated_event_list,\n            event_label_list=self.event_label_list,\n            time_resolution=self.time_resolution\n        )\n\n        if evaluated_length_seconds is None:\n            evaluated_length_seconds = max(reference_event_list.max_offset, estimated_event_list.max_offset)\n            evaluated_length_segments = int(math.ceil(evaluated_length_seconds * 1 / float(self.time_resolution)))\n\n        else:\n            evaluated_length_segments = int(math.ceil(evaluated_length_seconds * 1 / float(self.time_resolution)))\n\n        self.evaluated_length_seconds += evaluated_length_seconds\n        self.evaluated_files += 1\n\n        reference_event_roll, estimated_event_roll = util.match_event_roll_lengths(\n            reference_event_roll,\n            estimated_event_roll,\n            evaluated_length_segments\n        )\n\n        # Compute segment-based overall metrics\n        for segment_id in range(0, reference_event_roll.shape[0]):\n            annotated_segment = reference_event_roll[segment_id, :]\n            system_segment = estimated_event_roll[segment_id, :]\n\n            Ntp = sum(system_segment + annotated_segment > 1)\n            Ntn = sum(system_segment + annotated_segment == 0)\n            Nfp = sum(system_segment - annotated_segment > 0)\n            Nfn = sum(annotated_segment - system_segment > 0)\n\n            Nref = sum(annotated_segment)\n            Nsys = sum(system_segment)\n\n            S = min(Nref, Nsys) - Ntp\n            D = max(0, Nref - Nsys)\n            I = max(0, Nsys - Nref)\n\n            self.overall['Ntp'] += Ntp\n            self.overall['Ntn'] += Ntn\n            self.overall['Nfp'] += Nfp\n            self.overall['Nfn'] += Nfn\n            self.overall['Nref'] += Nref\n            self.overall['Nsys'] += Nsys\n            self.overall['S'] += S\n            self.overall['D'] += D\n            self.overall['I'] += I\n\n        # Compute segment-based class-wise metrics\n        for class_id, class_label in enumerate(self.event_label_list):\n            annotated_segment = reference_event_roll[:, class_id]\n            system_segment = estimated_event_roll[:, class_id]\n\n            Ntp = sum(system_segment + annotated_segment > 1)\n            Ntn = sum(system_segment + annotated_segment == 0)\n            Nfp = sum(system_segment - annotated_segment > 0)\n            Nfn = sum(annotated_segment - system_segment > 0)\n\n            Nref = sum(annotated_segment)\n            Nsys = sum(system_segment)\n\n            self.class_wise[class_label]['Ntp'] += Ntp\n            self.class_wise[class_label]['Ntn'] += Ntn\n            self.class_wise[class_label]['Nfp'] += Nfp\n            self.class_wise[class_label]['Nfn'] += Nfn\n            self.class_wise[class_label]['Nref'] += Nref\n            self.class_wise[class_label]['Nsys'] += Nsys\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef overall_f_measure(self):\n        if self.overall['Nsys'] == 0 and self.empty_system_output_handling == 'zero_score':\n            precision = 0\n\n        else:\n            precision = metric.precision(\n                Ntp=self.overall['Ntp'],\n                Nsys=self.overall['Nsys']\n            )\n\n        recall = metric.recall(\n            Ntp=self.overall['Ntp'],\n            Nref=self.overall['Nref']\n        )\n\n        f_measure = metric.f_measure(\n            precision=precision,\n            recall=recall\n        )\n\n        return {\n            'f_measure': f_measure,\n            'precision': precision,\n            'recall': recall\n        }", "response": "Overall f - measure metrics. Returns a dictionary format of overall f - measure precision and recall."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of class - wise counts for the event label.", "response": "def class_wise_count(self, event_label):\n        \"\"\"Class-wise counts (Nref and Nsys)\n\n        Returns\n        -------\n        dict\n            results in a dictionary format\n\n        \"\"\"\n\n        return {\n            'Nref': float(self.class_wise[event_label]['Nref']),\n            'Nsys': float(self.class_wise[event_label]['Nsys'])\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclass - wise f - measure metrics. Returns a dictionary format of the class - wise f - measure metrics.", "response": "def class_wise_f_measure(self, event_label):\n        \"\"\"Class-wise f-measure metrics (f_measure, precision, and recall)\n\n        Returns\n        -------\n        dict\n            results in a dictionary format\n\n        \"\"\"\n        if self.class_wise[event_label]['Nsys'] == 0 and self.empty_system_output_handling == 'zero_score':\n            precision = 0\n\n        else:\n            precision = metric.precision(\n                Ntp=self.class_wise[event_label]['Ntp'],\n                Nsys=self.class_wise[event_label]['Nsys']\n            )\n\n        recall = metric.recall(\n            Ntp=self.class_wise[event_label]['Ntp'],\n            Nref=self.class_wise[event_label]['Nref']\n        )\n\n        f_measure = metric.f_measure(\n            precision=precision,\n            recall=recall\n        )\n\n        return {\n            'f_measure': f_measure,\n            'precision': precision,\n            'recall': recall\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef class_wise_error_rate(self, event_label):\n\n        deletion_rate = metric.deletion_rate(\n            Nref=self.class_wise[event_label]['Nref'],\n            Ndeletions=self.class_wise[event_label]['Nfn']\n        )\n\n        insertion_rate = metric.insertion_rate(\n            Nref=self.class_wise[event_label]['Nref'],\n            Ninsertions=self.class_wise[event_label]['Nfp']\n        )\n\n        error_rate = metric.error_rate(\n            deletion_rate_value=deletion_rate,\n            insertion_rate_value=insertion_rate\n        )\n\n        return {\n            'error_rate': error_rate,\n            'deletion_rate': deletion_rate,\n            'insertion_rate': insertion_rate\n        }", "response": "Returns a dictionary of class - wise error rate metrics for the class - wise event with the given label."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of class - wise accuracy metrics for the given event label.", "response": "def class_wise_accuracy(self, event_label, factor=0.5):\n        \"\"\"Class-wise accuracy metrics (sensitivity, specificity, accuracy, and balanced_accuracy)\n\n        Returns\n        -------\n        dict\n            results in a dictionary format\n\n        \"\"\"\n\n        sensitivity = metric.sensitivity(\n            Ntp=self.class_wise[event_label]['Ntp'],\n            Nfn=self.class_wise[event_label]['Nfn']\n        )\n\n        specificity = metric.specificity(\n            Ntn=self.class_wise[event_label]['Ntn'],\n            Nfp=self.class_wise[event_label]['Nfp']\n        )\n\n        balanced_accuracy = metric.balanced_accuracy(\n            sensitivity=sensitivity,\n            specificity=specificity,\n            factor=factor\n        )\n\n        accuracy = metric.accuracy(\n            Ntp=self.class_wise[event_label]['Ntp'],\n            Ntn=self.class_wise[event_label]['Ntn'],\n            Nfp=self.class_wise[event_label]['Nfp'],\n            Nfn=self.class_wise[event_label]['Nfn']\n        )\n\n        return {\n            'accuracy': accuracy,\n            'balanced_accuracy': balanced_accuracy,\n            'sensitivity': sensitivity,\n            'specificity': specificity\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreports metric parameters Returns ------- str result report in string format", "response": "def result_report_parameters(self):\n        \"\"\"Report metric parameters\n\n        Returns\n        -------\n        str\n            result report in string format\n\n        \"\"\"\n\n        output = self.ui.data(field='Evaluated length', value=self.evaluated_length_seconds, unit='sec') + '\\n'\n        output += self.ui.data(field='Evaluated files', value=self.evaluated_files) + '\\n'\n\n        if self.time_resolution < 1:\n            output += self.ui.data(field='Segment length', value=self.time_resolution * 1000, unit='ms') + '\\n'\n\n        else:\n            output += self.ui.data(field='Segment length', value=self.time_resolution, unit='sec') + '\\n'\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate(self, reference_event_list, estimated_event_list):\n\n        # Make sure input is dcase_util.containers.MetaDataContainer\n        if not isinstance(reference_event_list, dcase_util.containers.MetaDataContainer):\n            reference_event_list = dcase_util.containers.MetaDataContainer(reference_event_list)\n\n        if not isinstance(estimated_event_list, dcase_util.containers.MetaDataContainer):\n            estimated_event_list = dcase_util.containers.MetaDataContainer(estimated_event_list)\n\n        # Check that input event list have event only from one file\n        reference_files = reference_event_list.unique_files\n        if len(reference_files) > 1:\n            raise ValueError(\n                \"reference_event_list contains events from multiple files. Evaluate only file by file.\"\n            )\n\n        estimated_files = estimated_event_list.unique_files\n        if len(estimated_files) > 1:\n            raise ValueError(\n                \"estimated_event_list contains events from multiple files. Evaluate only file by file.\"\n            )\n\n        # Evaluate only valid events\n        valid_reference_event_list = dcase_util.containers.MetaDataContainer()\n        for item in reference_event_list:\n            if 'event_onset' in item and 'event_offset' in item and 'event_label' in item:\n                valid_reference_event_list.append(item)\n\n            elif 'onset' in item and 'offset' in item and 'event_label' in item:\n                valid_reference_event_list.append(item)\n\n        reference_event_list = valid_reference_event_list\n\n        valid_estimated_event_list = dcase_util.containers.MetaDataContainer()\n        for item in estimated_event_list:\n            if 'event_onset' in item and 'event_offset' in item and 'event_label' in item:\n                valid_estimated_event_list.append(item)\n\n            elif 'onset' in item and 'offset' in item and 'event_label' in item:\n                valid_estimated_event_list.append(item)\n\n        estimated_event_list = valid_estimated_event_list\n\n        self.evaluated_length += reference_event_list.max_offset\n        self.evaluated_files += 1\n\n        # Overall metrics\n\n        # Total number of detected and reference events\n        Nsys = len(estimated_event_list)\n        Nref = len(reference_event_list)\n\n        if self.event_matching_type == 'optimal':\n            label_hit_matrix = numpy.zeros((len(reference_event_list), len(estimated_event_list)), dtype=bool)\n            for j in range(0, len(reference_event_list)):\n                for i in range(0, len(estimated_event_list)):\n                    label_hit_matrix[j, i] = reference_event_list[j]['event_label'] == estimated_event_list[i]['event_label']\n\n            hit_matrix = label_hit_matrix\n            if self.evaluate_onset:\n                onset_hit_matrix = numpy.zeros((len(reference_event_list), len(estimated_event_list)), dtype=bool)\n                for j in range(0, len(reference_event_list)):\n                    for i in range(0, len(estimated_event_list)):\n                        onset_hit_matrix[j, i] = self.validate_onset(\n                            reference_event=reference_event_list[j],\n                            estimated_event=estimated_event_list[i],\n                            t_collar=self.t_collar\n                        )\n\n                hit_matrix *= onset_hit_matrix\n\n            if self.evaluate_offset:\n                offset_hit_matrix = numpy.zeros((len(reference_event_list), len(estimated_event_list)), dtype=bool)\n                for j in range(0, len(reference_event_list)):\n                    for i in range(0, len(estimated_event_list)):\n                        offset_hit_matrix[j, i] = self.validate_offset(\n                            reference_event=reference_event_list[j],\n                            estimated_event=estimated_event_list[i],\n                            t_collar=self.t_collar,\n                            percentage_of_length=self.percentage_of_length\n                        )\n\n                hit_matrix *= offset_hit_matrix\n\n            hits = numpy.where(hit_matrix)\n            G = {}\n            for ref_i, est_i in zip(*hits):\n                if est_i not in G:\n                    G[est_i] = []\n\n                G[est_i].append(ref_i)\n\n            matching = sorted(util.bipartite_match(G).items())\n\n            ref_correct = numpy.zeros(Nref, dtype=bool)\n            sys_correct = numpy.zeros(Nsys, dtype=bool)\n            for item in matching:\n                ref_correct[item[0]] = True\n                sys_correct[item[1]] = True\n\n            Ntp = len(matching)\n            # Substitutions\n            Nsubs = 0\n\n            ref_leftover = numpy.nonzero(numpy.logical_not(ref_correct))[0]\n            sys_leftover = numpy.nonzero(numpy.logical_not(sys_correct))[0]\n            sys_counted = numpy.zeros(Nsys, dtype=bool)\n            for j in ref_leftover:\n                for i in sys_leftover:\n                    if not sys_counted[i]:\n                        if self.evaluate_onset:\n                            onset_condition = self.validate_onset(\n                                reference_event=reference_event_list[j],\n                                estimated_event=estimated_event_list[i],\n                                t_collar=self.t_collar\n                            )\n\n                        else:\n                            onset_condition = True\n\n                        if self.evaluate_offset:\n                            offset_condition = self.validate_offset(\n                                reference_event=reference_event_list[j],\n                                estimated_event=estimated_event_list[i],\n                                t_collar=self.t_collar,\n                                percentage_of_length=self.percentage_of_length\n                            )\n\n                        else:\n                            offset_condition = True\n\n                        if onset_condition and offset_condition:\n                            sys_counted[i] = True\n                            Nsubs += 1\n                            break\n\n        elif self.event_matching_type == 'greedy':\n            sys_correct = numpy.zeros(Nsys, dtype=bool)\n            ref_correct = numpy.zeros(Nref, dtype=bool)\n\n            # Number of correctly detected events\n            for j in range(0, len(reference_event_list)):\n                for i in range(0, len(estimated_event_list)):\n                    if not sys_correct[i]:  # skip already matched events\n                        label_condition = reference_event_list[j]['event_label'] == estimated_event_list[i]['event_label']\n\n                        if self.evaluate_onset:\n                            onset_condition = self.validate_onset(\n                                reference_event=reference_event_list[j],\n                                estimated_event=estimated_event_list[i],\n                                t_collar=self.t_collar\n                            )\n\n                        else:\n                            onset_condition = True\n\n                        if self.evaluate_offset:\n                            offset_condition = self.validate_offset(\n                                reference_event=reference_event_list[j],\n                                estimated_event=estimated_event_list[i],\n                                t_collar=self.t_collar,\n                                percentage_of_length=self.percentage_of_length\n                            )\n\n                        else:\n                            offset_condition = True\n\n                        if label_condition and onset_condition and offset_condition:\n                            ref_correct[j] = True\n                            sys_correct[i] = True\n                            break\n\n            Ntp = numpy.sum(sys_correct)\n\n            ref_leftover = numpy.nonzero(numpy.logical_not(ref_correct))[0]\n            sys_leftover = numpy.nonzero(numpy.logical_not(sys_correct))[0]\n\n            # Substitutions\n            Nsubs = 0\n            sys_counted = numpy.zeros(Nsys, dtype=bool)\n            for j in ref_leftover:\n                for i in sys_leftover:\n                    if not sys_counted[i]:\n                        if self.evaluate_onset:\n                            onset_condition = self.validate_onset(\n                                reference_event=reference_event_list[j],\n                                estimated_event=estimated_event_list[i],\n                                t_collar=self.t_collar\n                            )\n\n                        else:\n                            onset_condition = True\n\n                        if self.evaluate_offset:\n                            offset_condition = self.validate_offset(\n                                reference_event=reference_event_list[j],\n                                estimated_event=estimated_event_list[i],\n                                t_collar=self.t_collar,\n                                percentage_of_length=self.percentage_of_length\n                            )\n\n                        else:\n                            offset_condition = True\n\n                        if onset_condition and offset_condition:\n                            sys_counted[i] = True\n                            Nsubs += 1\n                            break\n\n        Nfp = Nsys - Ntp - Nsubs\n        Nfn = Nref - Ntp - Nsubs\n\n        self.overall['Nref'] += Nref\n        self.overall['Nsys'] += Nsys\n        self.overall['Ntp'] += Ntp\n        self.overall['Nsubs'] += Nsubs\n        self.overall['Nfp'] += Nfp\n        self.overall['Nfn'] += Nfn\n\n        # Class-wise metrics\n        for class_id, class_label in enumerate(self.event_label_list):\n            Nref = 0.0\n            Nsys = 0.0\n            Ntp = 0.0\n\n            # Count event frequencies in the ground truth\n            for i in range(0, len(reference_event_list)):\n                if reference_event_list[i]['event_label'] == class_label:\n                    Nref += 1\n\n            # Count event frequencies in the system output\n            for i in range(0, len(estimated_event_list)):\n                if estimated_event_list[i]['event_label'] == class_label:\n                    Nsys += 1\n\n            if self.event_matching_type == 'optimal':\n                class_reference_event_list = reference_event_list.filter(event_label=class_label)\n                class_estimated_event_list = estimated_event_list.filter(event_label=class_label)\n\n                hit_matrix = numpy.ones((len(class_reference_event_list), len(class_estimated_event_list)), dtype=bool)\n                if self.evaluate_onset:\n                    onset_hit_matrix = numpy.zeros((len(class_reference_event_list), len(class_estimated_event_list)), dtype=bool)\n                    for j in range(0, len(class_reference_event_list)):\n                        for i in range(0, len(class_estimated_event_list)):\n                            onset_hit_matrix[j, i] = self.validate_onset(\n                                reference_event=class_reference_event_list[j],\n                                estimated_event=class_estimated_event_list[i],\n                                t_collar=self.t_collar\n                            )\n\n                    hit_matrix *= onset_hit_matrix\n\n                if self.evaluate_offset:\n                    offset_hit_matrix = numpy.zeros((len(class_reference_event_list), len(class_estimated_event_list)), dtype=bool)\n                    for j in range(0, len(class_reference_event_list)):\n                        for i in range(0, len(class_estimated_event_list)):\n                            offset_hit_matrix[j, i] = self.validate_offset(\n                                reference_event=class_reference_event_list[j],\n                                estimated_event=class_estimated_event_list[i],\n                                t_collar=self.t_collar,\n                                percentage_of_length=self.percentage_of_length\n                            )\n\n                    hit_matrix *= offset_hit_matrix\n\n                hits = numpy.where(hit_matrix)\n                G = {}\n                for ref_i, est_i in zip(*hits):\n                    if est_i not in G:\n                        G[est_i] = []\n\n                    G[est_i].append(ref_i)\n\n                matching = sorted(util.bipartite_match(G).items())\n\n                ref_correct = numpy.zeros(int(Nref), dtype=bool)\n                sys_correct = numpy.zeros(int(Nsys), dtype=bool)\n\n                for item in matching:\n                    ref_correct[item[0]] = True\n                    sys_correct[item[1]] = True\n\n                Ntp = len(matching)\n\n            elif self.event_matching_type == 'greedy':\n                sys_counted = numpy.zeros(len(estimated_event_list), dtype=bool)\n                for j in range(0, len(reference_event_list)):\n                    if reference_event_list[j]['event_label'] == class_label:\n                        for i in range(0, len(estimated_event_list)):\n                            if estimated_event_list[i]['event_label'] == class_label and not sys_counted[i]:\n                                if self.evaluate_onset:\n                                    onset_condition = self.validate_onset(\n                                        reference_event=reference_event_list[j],\n                                        estimated_event=estimated_event_list[i],\n                                        t_collar=self.t_collar\n                                    )\n\n                                else:\n                                    onset_condition = True\n\n                                if self.evaluate_offset:\n                                    offset_condition = self.validate_offset(\n                                        reference_event=reference_event_list[j],\n                                        estimated_event=estimated_event_list[i],\n                                        t_collar=self.t_collar,\n                                        percentage_of_length=self.percentage_of_length\n                                    )\n\n                                else:\n                                    offset_condition = True\n\n                                if onset_condition and offset_condition:\n                                    sys_counted[i] = True\n                                    Ntp += 1\n                                    break\n\n            Nfp = Nsys - Ntp\n            Nfn = Nref - Ntp\n\n            self.class_wise[class_label]['Nref'] += Nref\n            self.class_wise[class_label]['Nsys'] += Nsys\n\n            self.class_wise[class_label]['Ntp'] += Ntp\n            self.class_wise[class_label]['Nfp'] += Nfp\n            self.class_wise[class_label]['Nfn'] += Nfn\n\n        return self", "response": "Evaluate file pair (reference and estimated)\n\n        Parameters\n        ----------\n\n        reference_event_list : event list\n            Reference event list\n\n        estimated_event_list : event list\n            Estimated event list\n\n        Returns\n        -------\n        self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresetting internal state of the object to its initial state.", "response": "def reset(self):\n        \"\"\"Reset internal state\n        \"\"\"\n\n        self.overall = {\n            'Nref': 0.0,\n            'Nsys': 0.0,\n            'Nsubs': 0.0,\n            'Ntp': 0.0,\n            'Nfp': 0.0,\n            'Nfn': 0.0,\n        }\n        self.class_wise = {}\n\n        for class_label in self.event_label_list:\n            self.class_wise[class_label] = {\n                'Nref': 0.0,\n                'Nsys': 0.0,\n                'Ntp': 0.0,\n                'Ntn': 0.0,\n                'Nfp': 0.0,\n                'Nfn': 0.0,\n            }\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate the estimated event based on event onset.", "response": "def validate_onset(reference_event, estimated_event, t_collar=0.200):\n        \"\"\"Validate estimated event based on event onset\n\n        Parameters\n        ----------\n        reference_event : dict\n            Reference event.\n\n        estimated_event: dict\n            Estimated event.\n\n        t_collar : float > 0, seconds\n            Time collar with which the estimated onset has to be in order to be consider valid estimation.\n            Default value 0.2\n\n        Returns\n        -------\n        bool\n\n        \"\"\"\n\n        # Detect field naming style used and validate onset\n        if 'event_onset' in reference_event and 'event_onset' in estimated_event:\n            return math.fabs(reference_event['event_onset'] - estimated_event['event_onset']) <= t_collar\n\n        elif 'onset' in reference_event and 'onset' in estimated_event:\n            return math.fabs(reference_event['onset'] - estimated_event['onset']) <= t_collar"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_offset(reference_event, estimated_event, t_collar=0.200, percentage_of_length=0.5):\n\n        # Detect field naming style used and validate onset\n        if 'event_offset' in reference_event and 'event_offset' in estimated_event:\n            annotated_length = reference_event['event_offset'] - reference_event['event_onset']\n\n            return math.fabs(reference_event['event_offset'] - estimated_event['event_offset']) <= max(t_collar, percentage_of_length * annotated_length)\n\n        elif 'offset' in reference_event and 'offset' in estimated_event:\n            annotated_length = reference_event['offset'] - reference_event['onset']\n\n            return math.fabs(reference_event['offset'] - estimated_event['offset']) <= max(t_collar, percentage_of_length * annotated_length)", "response": "Validate the estimated event based on the event offset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreport metric parameters Returns ------- str result report in string format", "response": "def result_report_parameters(self):\n        \"\"\"Report metric parameters\n\n        Returns\n        -------\n        str\n            result report in string format\n\n        \"\"\"\n\n        output = self.ui.data(field='Evaluated length', value=self.evaluated_length, unit='sec') + '\\n'\n        output += self.ui.data(field='Evaluated files', value=self.evaluated_files) + '\\n'\n\n        output += self.ui.data(field='Evaluate onset', value=self.evaluate_onset) + '\\n'\n        output += self.ui.data(field='Evaluate offset', value=self.evaluate_offset) + '\\n'\n\n        if self.t_collar < 1:\n            output += self.ui.data(field='T collar', value=self.t_collar*1000, unit='ms') + '\\n'\n\n        else:\n            output += self.ui.data(field='T collar', value=self.t_collar, unit='sec') + '\\n'\n\n        output += self.ui.data(field='Offset (length)', value=self.percentage_of_length*100, unit='%') + '\\n'\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading event list from CSV formatted text - file.", "response": "def load_event_list(filename, **kwargs):\n    \"\"\"Load event list from csv formatted text-file\n\n    Supported formats (see more `dcase_util.containers.MetaDataContainer.load()` method):\n\n    - [event onset (float >= 0)][delimiter][event offset (float >= 0)]\n    - [event onset (float >= 0)][delimiter][event offset (float >= 0)][delimiter][label]\n    - [filename][delimiter][event onset (float >= 0)][delimiter][event offset (float >= 0)][delimiter][event label]\n    - [filename][delimiter][scene_label][delimiter][event onset (float >= 0)][delimiter][event offset (float >= 0)][delimiter][event label]\n    - [filename]\n\n    Supported delimiters: ``,``, ``;``, ``tab``\n\n    Example of event list file::\n\n        21.64715\t23.00552\talert\n        36.91184\t38.27021\talert\n        69.72575\t71.09029\talert\n        63.53990\t64.89827\talert\n        84.25553\t84.83920\talert\n        20.92974\t21.82661\tclearthroat\n        28.39992\t29.29679\tclearthroat\n        80.47837\t81.95937\tclearthroat\n        44.48363\t45.96463\tclearthroat\n        78.13073\t79.05953\tclearthroat\n        15.17031\t16.27235\tcough\n        20.54931\t21.65135\tcough\n        27.79964\t28.90168\tcough\n        75.45959\t76.32490\tcough\n        70.81708\t71.91912\tcough\n        21.23203\t22.55902\tdoorslam\n        7.546220\t9.014880\tdoorslam\n        34.11303\t35.04183\tdoorslam\n        45.86001\t47.32867\tdoorslam\n\n\n    Parameters\n    ----------\n    filename : str\n        Path to the csv-file\n\n    Returns\n    -------\n    list of dict\n        Event list\n\n    \"\"\"\n\n    return dcase_util.containers.MetaDataContainer().load(filename=filename, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_scene_list(filename, **kwargs):\n\n    return dcase_util.containers.MetaDataContainer().load(filename=filename, **kwargs)", "response": "Load the scene list from a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads file pair list csv formatted text - file", "response": "def load_file_pair_list(filename):\n    \"\"\"Load file pair list csv formatted text-file\n\n    Format is [reference_file][delimiter][estimated_file]\n\n    Supported delimiters: ``,``, ``;``, ``tab``\n\n    Example of file-list::\n\n        office_snr0_high_v2.txt\toffice_snr0_high_v2_detected.txt\n        office_snr0_med_v2.txt\toffice_snr0_med_v2_detected.txt\n\n    Parameters\n    ----------\n    filename : str\n        Path to the csv-file\n\n    Returns\n    -------\n    file_list: list\n        File pair dicts in a list\n\n    \"\"\"\n\n    data = []\n\n    input_file = open(filename, 'rt')\n\n    try:\n        dialect = csv.Sniffer().sniff(input_file.readline(), [',', ';', '\\t'])\n\n    except csv.Error:\n        raise ValueError('Unknown delimiter in file [{file}].'.format(file=filename))\n\n    input_file.seek(0)  \n\n    for row in csv.reader(input_file, dialect):\n        if len(row) == 2:\n            data.append(\n                {\n                    'reference_file': row[0],\n                    'estimated_file': row[1]\n                }\n            )\n\n        else:\n            raise ValueError('Unknown file pair list format in file [{file}].'.format(file=filename))\n\n    input_file.close()\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nevaluating file pair (reference and estimated) Parameters ---------- reference_scene_list : list of dict or dcase_util.containers.MetaDataContainer Reference scene list. Default value None estimated_scene_list : list of dict or dcase_util.containers.MetaDataContainer Estimated scene list. Default value None estimated_scene_probabilities : dcase_util.containers.ProbabilityContainer Estimated scene probabilities. Currently not used. Default value None Returns ------- self", "response": "def evaluate(self, reference_scene_list, estimated_scene_list=None, estimated_scene_probabilities=None):\n        \"\"\"Evaluate file pair (reference and estimated)\n\n        Parameters\n        ----------\n\n        reference_scene_list : list of dict or dcase_util.containers.MetaDataContainer\n            Reference scene list.\n            Default value None\n\n        estimated_scene_list : list of dict or dcase_util.containers.MetaDataContainer\n            Estimated scene list.\n            Default value None\n\n        estimated_scene_probabilities : dcase_util.containers.ProbabilityContainer\n            Estimated scene probabilities. Currently not used.\n            Default value None\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n\n        if estimated_scene_list is None and estimated_scene_probabilities is None:\n            raise ValueError(\"Nothing to evaluate, give at least estimated_scene_list or estimated_scene_probabilities\")\n\n        # Make sure reference_scene_list is dcase_util.containers.MetaDataContainer\n        if not isinstance(estimated_scene_list, dcase_util.containers.MetaDataContainer):\n            reference_scene_list = dcase_util.containers.MetaDataContainer(reference_scene_list)\n\n        # Make sure estimated_scene_list is dcase_util.containers.MetaDataContainer\n        if not isinstance(estimated_scene_list, dcase_util.containers.MetaDataContainer):\n            estimated_scene_list = dcase_util.containers.MetaDataContainer(estimated_scene_list)\n\n        # Make sure estimated_tag_probabilities is dcase_util.containers.ProbabilityContainer\n        if estimated_scene_probabilities is not None:\n            if not isinstance(estimated_scene_probabilities, dcase_util.containers.ProbabilityContainer):\n                estimated_scene_probabilities = dcase_util.containers.ProbabilityContainer(estimated_scene_probabilities)\n\n        # Translate \"file\" field to \"filename\"\n        for item in reference_scene_list:\n            if 'filename' not in item and 'file' in item:\n                item['filename'] = item['file']\n\n        for item in estimated_scene_list:\n            if 'filename' not in item and 'file' in item:\n                item['filename'] = item['file']\n\n        y_true = []\n        y_pred = []\n\n        for estimated_item in estimated_scene_list:\n            reference_item_matched = {}\n            for reference_item in reference_scene_list:\n                if estimated_item['filename'] == reference_item['filename']:\n                    reference_item_matched = reference_item\n                    break\n\n            if not reference_item_matched:\n                raise ValueError(\n                    \"Cannot find reference_item for estimated item [{item}]\".format(item=estimated_item['file'])\n                )\n\n            y_true.append(reference_item_matched['scene_label'])\n            y_pred.append(estimated_item['scene_label'])\n\n        y_true = numpy.array(y_true)\n        y_pred = numpy.array(y_pred)\n\n        Ncorr_overall = 0\n        for scene_id, scene_label in enumerate(self.scene_label_list):\n            true_id = numpy.where(y_true == scene_label)[0]\n            pred_id = numpy.where(y_pred == scene_label)[0]\n\n            Ncorr = 0\n            for id in true_id:\n                if id in pred_id:\n                    Ncorr += 1\n\n            Ncorr_overall += Ncorr\n            self.scene_wise[scene_label]['Ncorr'] += Ncorr\n            self.scene_wise[scene_label]['Nref'] += true_id.shape[0]\n            self.scene_wise[scene_label]['Nsys'] += pred_id.shape[0]\n\n        self.overall['Ncorr'] += Ncorr_overall\n        self.overall['Nref'] += y_true.shape[0]\n        self.overall['Nsys'] += y_pred.shape[0]\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets internal state of the object to 0. 0.", "response": "def reset(self):\n        \"\"\"Reset internal state\n        \"\"\"\n        self.overall = {\n            'Ncorr': 0.0,\n            'Nref': 0.0,\n            'Nsys': 0.0\n        }\n\n        self.scene_wise = {}\n        for label in self.scene_label_list:\n            self.scene_wise[label] = {\n                'Ncorr': 0.0,\n                'Nref': 0.0,\n                'Nsys': 0.0\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef result_report_class_wise(self):\n\n        results = self.results_class_wise_metrics()\n\n        output = self.ui.section_header('Class-wise metrics', indent=2) + '\\n'\n\n        output += self.ui.row(\n            'Scene label', 'Ncorr', 'Nref', 'Accuracy',\n            widths=[20, 12, 12, 12],\n            separators=[True, False, True, False],\n            indent=4\n        ) + '\\n'\n\n        output += self.ui.row('-', '-', '-', '-') + '\\n'\n\n        for scene_label in self.scene_label_list:\n            output += self.ui.row(\n                scene_label,\n                results[scene_label]['count']['Ncorr'],\n                results[scene_label]['count']['Nref'],\n                results[scene_label]['accuracy']['accuracy'] * 100,\n                types=['str', 'int', 'int', 'float1_percentage']\n            ) + '\\n'\n\n        return output", "response": "Report class - wise results of a single resource in string format"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef result_report_class_wise_average(self):\n\n        results = self.results_class_wise_average_metrics()\n\n        output = self.ui.section_header('Class-wise average metrics (macro-average)', indent=2) + '\\n'\n\n        output += self.ui.line('Accuracy', indent=2) + '\\n'\n        output += self.ui.data(field='Accuracy',\n                               value=float(results['accuracy']['accuracy']) * 100, unit='%', indent=4) + '\\n'\n\n        return output", "response": "Report class - wise average metrics Returns ------- str result report in string format"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary of class - wise metrics for the current class - wise state of the object.", "response": "def results_class_wise_metrics(self):\n        \"\"\"Class-wise metrics\n\n        Returns\n        -------\n        dict\n            results in a dictionary format\n\n        \"\"\"\n\n        results = {}\n        for scene_id, scene_label in enumerate(self.scene_label_list):\n            if scene_label not in results:\n                results[scene_label] = {}\n\n            results[scene_label]['count'] = {}\n            results[scene_label]['count']['Ncorr'] = self.scene_wise[scene_label]['Ncorr']\n            results[scene_label]['count']['Nref'] = self.scene_wise[scene_label]['Nref']\n            results[scene_label]['count']['Nsys'] = self.scene_wise[scene_label]['Nsys']\n            results[scene_label]['accuracy'] = {\n                'accuracy': metric.accuracy_corr(\n                    Ncorr=self.scene_wise[scene_label]['Ncorr'],\n                    N=self.scene_wise[scene_label]['Nref']\n                )\n            }\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of class - wise averaged metrics for the current locale.", "response": "def results_class_wise_average_metrics(self):\n        \"\"\"Class-wise averaged metrics\n\n        Returns\n        -------\n        dict\n            results in a dictionary format\n\n        \"\"\"\n\n        scene_wise_results = self.results_class_wise_metrics()\n\n        scene_wise_accuracy = []\n        for scene_label in scene_wise_results:\n            scene_wise_accuracy.append(scene_wise_results[scene_label]['accuracy']['accuracy'])\n\n        return {\n            'accuracy': {\n                'accuracy': float(numpy.mean(scene_wise_accuracy))\n            }\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclass - wise accuracy Returns ------- dict results in a dictionary format", "response": "def class_wise_accuracy(self, scene_label):\n        \"\"\"Class-wise accuracy\n\n        Returns\n        -------\n        dict\n            results in a dictionary format\n\n        \"\"\"\n\n        if len(self.accuracies_per_class.shape) == 2:\n            return {\n                'accuracy': float(numpy.mean(self.accuracies_per_class[:, self.scene_label_list.index(scene_label)]))\n            }\n\n        else:\n            return {\n                'accuracy': float(numpy.mean(self.accuracies_per_class[self.scene_label_list.index(scene_label)]))\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unique_scene_labels(scene_list):\n    if isinstance(scene_list, dcase_util.containers.MetaDataContainer):\n        return scene_list.unique_scene_labels\n\n    else:\n        labels = []\n        for item in scene_list:\n            if 'scene_label' in item and item['scene_label'] not in labels:\n                labels.append(item['scene_label'])\n\n        labels.sort()\n        return labels", "response": "Find the unique scene labels in alphabetical order"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recall(Ntp, Nref, eps=numpy.spacing(1)):\n\n    if Nref == 0:\n        return numpy.nan\n    else:\n        return float(Ntp / float(Nref))", "response": "Recall a base on the number of true positives and reference."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef substitution_rate(Nref, Nsubstitutions, eps=numpy.spacing(1)):\n\n    return float(Nsubstitutions / (Nref + eps))", "response": "Returns the substitution rate of a single resource in a single resource tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the deletion rate of the base language tree.", "response": "def deletion_rate(Nref, Ndeletions, eps=numpy.spacing(1)):\n    \"\"\"Deletion rate\n\n    Parameters\n    ----------\n    Nref : int >=0\n        Number of entries in the reference.\n\n    Ndeletions : int >=0\n        Number of deletions.\n\n    eps : float\n        eps.\n        Default value numpy.spacing(1)\n\n    Returns\n    -------\n    deletion_rate: float\n        Deletion rate\n        \n    \"\"\"\n\n    return float(Ndeletions / (Nref + eps))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the insertion rate of a single resource in a single resource tree.", "response": "def insertion_rate(Nref, Ninsertions, eps=numpy.spacing(1)):\n    \"\"\"Insertion rate\n\n    Parameters\n    ----------\n    Nref : int >=0\n        Number of entries in the reference.\n\n    Ninsertions : int >=0\n        Number of insertions.\n\n    eps : float\n        eps.\n        Default value numpy.spacing(1)\n\n    Returns\n    -------\n    insertion_rate: float\n        Insertion rate\n\n    \"\"\"\n\n    return float(Ninsertions / (Nref + eps))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef equal_error_rate(y_true, y_score, eps=numpy.spacing(1)):\n\n    from sklearn import metrics\n\n    if numpy.any(y_true):\n        false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n            y_true=y_true,\n            y_score=y_score,\n            drop_intermediate=True\n        )\n\n        points = [(0, 0)] + list(zip(false_positive_rate, true_positive_rate))\n        for i, point in enumerate(points):\n            if point[0] + eps >= 1 - point[1]:\n                break\n\n        point1 = points[i - 1]\n        point2 = points[i]\n\n        # Interpolate between point1 and point2\n        if abs(point2[0] - point1[0]) < eps:\n            eer = point1[0]\n\n        else:\n            m = (point2[1] - point1[1]) / (point2[0] - point1[0])\n            o = point1[1] - m * point1[0]\n            eer = (1 - o) / (1 + m)\n\n    else:\n        eer = numpy.nan\n\n    return eer", "response": "Calculates the EER of the true binary labels and true positive rate."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts event list into event roll binary activity matrix.", "response": "def event_list_to_event_roll(source_event_list, event_label_list=None, time_resolution=0.01):\n    \"\"\"Convert event list into event roll, binary activity matrix\n\n    Parameters\n    ----------\n    source_event_list : list, shape=(n,)\n        A list containing event dicts\n\n    event_label_list : list, shape=(k,) or None\n        A list of containing unique labels in alphabetical order\n        (Default value = None)\n    time_resolution : float > 0\n        Time resolution in seconds of the event roll\n        (Default value = 0.01)\n\n    Returns\n    -------\n\n    event_roll: np.ndarray, shape=(m,k)\n        Event roll\n\n    \"\"\"\n\n    if isinstance(source_event_list, dcase_util.containers.MetaDataContainer):\n        max_offset_value = source_event_list.max_offset\n\n        if event_label_list is None:\n            event_label_list = source_event_list.unique_event_labels\n\n    elif isinstance(source_event_list, list):\n        max_offset_value = event_list.max_event_offset(source_event_list)\n\n        if event_label_list is None:\n            event_label_list = event_list.unique_event_labels(source_event_list)\n\n    else:\n        raise ValueError('Unknown source_event_list type.')\n\n    # Initialize event roll\n    event_roll = numpy.zeros((int(math.ceil(max_offset_value * 1 / time_resolution)), len(event_label_list)))\n\n    # Fill-in event_roll\n    for event in source_event_list:\n        pos = event_label_list.index(event['event_label'])\n\n        if 'event_onset' in event and 'event_offset' in event:\n            event_onset = event['event_onset']\n            event_offset = event['event_offset']\n\n        elif 'onset' in event and 'offset' in event:\n            event_onset = event['onset']\n            event_offset = event['offset']\n\n        onset = int(math.floor(event_onset * 1 / float(time_resolution)))\n        offset = int(math.ceil(event_offset * 1 / float(time_resolution)))\n\n        event_roll[onset:offset, pos] = 1\n\n    return event_roll"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pad_event_roll(event_roll, length):\n\n    if length > event_roll.shape[0]:\n        padding = numpy.zeros((length-event_roll.shape[0], event_roll.shape[1]))\n        event_roll = numpy.vstack((event_roll, padding))\n\n    return event_roll", "response": "Pads the event roll to given length"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match_event_roll_lengths(event_roll_a, event_roll_b, length=None):\n\n    # Fix durations of both event_rolls to be equal\n    if length is None:\n        length = max(event_roll_b.shape[0], event_roll_a.shape[0])\n\n    else:\n        length = int(length)\n\n    if length < event_roll_a.shape[0]:\n        event_roll_a = event_roll_a[0:length, :]\n\n    else:\n        event_roll_a = pad_event_roll(\n            event_roll=event_roll_a,\n            length=length\n        )\n\n    if length < event_roll_b.shape[0]:\n        event_roll_b = event_roll_b[0:length, :]\n\n    else:\n        event_roll_b = pad_event_roll(\n            event_roll=event_roll_b,\n            length=length\n        )\n\n    return event_roll_a, event_roll_b", "response": "Given two event rolls and length of the event rolls return the event rolls that are equal."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfilters event list based on given fields", "response": "def filter_event_list(event_list, scene_label=None, event_label=None, filename=None):\n    \"\"\"Filter event list based on given fields\n\n    Parameters\n    ----------\n    event_list : list, shape=(n,)\n        A list containing event dicts\n\n    scene_label : str\n        Scene label\n\n    event_label : str\n        Event label\n\n    filename : str\n        Filename\n\n    Returns\n    -------\n    event_list: list, shape=(n,)\n        A list containing event dicts\n\n    \"\"\"\n\n    return dcase_util.containers.MetaDataContainer(event_list).filter(\n        filename=filename,\n        scene_label=scene_label,\n        event_label=event_label\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the unique files in alphabetical order", "response": "def unique_files(event_list):\n    \"\"\"Find the unique files\n\n    Parameters\n    ----------\n    event_list : list or dcase_util.containers.MetaDataContainer\n        A list containing event dicts\n\n    Returns\n    -------\n    list\n        Unique filenames in alphabetical order\n\n    \"\"\"\n\n    if isinstance(event_list, dcase_util.containers.MetaDataContainer):\n        return event_list.unique_files\n\n    else:\n        files = {}\n        for event in event_list:\n            if 'file' in event:\n                files[event['file']] = event['file']\n\n            elif 'filename' in event:\n                files[event['filename']] = event['filename']\n\n        files = list(files.keys())\n        files.sort()\n        return files"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the unique event labels in alphabetical order", "response": "def unique_event_labels(event_list):\n    \"\"\"Find the unique event labels\n\n    Parameters\n    ----------\n    event_list : list or dcase_util.containers.MetaDataContainer\n        A list containing event dicts\n\n    Returns\n    -------\n    list\n        Unique labels in alphabetical order\n\n    \"\"\"\n\n    if isinstance(event_list, dcase_util.containers.MetaDataContainer):\n        return event_list.unique_event_labels\n\n    else:\n        labels = []\n        for event in event_list:\n            if 'event_label' in event and event['event_label'] not in labels:\n                labels.append(event['event_label'])\n\n        labels.sort()\n        return labels"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the offset of the last event in the event_list", "response": "def max_event_offset(event_list):\n    \"\"\"Find the offset (end-time) of last event\n\n    Parameters\n    ----------\n    event_list : list or dcase_util.containers.MetaDataContainer\n        A list containing event dicts\n\n    Returns\n    -------\n    float > 0\n        maximum offset\n\n    \"\"\"\n\n    if isinstance(event_list, dcase_util.containers.MetaDataContainer):\n        return event_list.max_offset\n\n    else:\n        max_offset = 0\n        for event in event_list:\n            if 'event_offset' in event:\n                if event['event_offset'] > max_offset:\n                    max_offset = event['event_offset']\n\n            elif 'offset' in event:\n                if event['offset'] > max_offset:\n                    max_offset = event['offset']\n\n        return max_offset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating estimated against reference_tag_list", "response": "def evaluate(self, reference_tag_list, estimated_tag_list=None, estimated_tag_probabilities=None):\n        \"\"\"Evaluate estimated against reference\n\n        Parameters\n        ----------\n\n        reference_tag_list : list of dict or dcase_util.containers.MetaDataContainer\n            Reference tag list\n\n        estimated_tag_list : list of dict or dcase_util.containers.MetaDataContainer\n            Estimated tag list\n\n        estimated_tag_probabilities : list of dict or dcase_util.containers.ProbabilityContainer\n            Estimated tag probabilities\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n\n        if estimated_tag_list is None and estimated_tag_probabilities is None:\n            raise ValueError(\"Nothing to evaluate, give at least estimated_tag_list or estimated_tag_probabilities\")\n\n        # Make sure reference_tag_list is dcase_util.containers.MetaDataContainer\n        if not isinstance(reference_tag_list, dcase_util.containers.MetaDataContainer):\n            reference_tag_list = dcase_util.containers.MetaDataContainer(reference_tag_list)\n\n        # Make sure estimated_tag_list is dcase_util.containers.MetaDataContainer\n        if estimated_tag_list is not None:\n            if not isinstance(estimated_tag_list, dcase_util.containers.MetaDataContainer):\n                estimated_tag_list = dcase_util.containers.MetaDataContainer(estimated_tag_list)\n\n        # Make sure estimated_tag_probabilities is dcase_util.containers.ProbabilityContainer\n        if estimated_tag_probabilities is not None:\n            if not isinstance(estimated_tag_probabilities, dcase_util.containers.ProbabilityContainer):\n                estimated_tag_probabilities = dcase_util.containers.ProbabilityContainer(estimated_tag_probabilities)\n\n        y_true = []\n        y_pred = []\n\n        # Go though reference and estimated list label by label, and file by file\n        for label in self.tag_label_list:\n            for filename in reference_tag_list.unique_files:\n                reference_item = reference_tag_list.filter(filename=filename)[0]\n\n                # Populate y_true based on reference_item\n                if label in reference_item.tags:\n                    self.y_true[label].append(1)\n                    y_true.append(1)\n\n                else:\n                    self.y_true[label].append(0)\n                    y_true.append(0)\n\n                if estimated_tag_list is not None:\n                    # Evaluate based on estimated tags\n\n                    estimated_item = estimated_tag_list.filter(filename=filename)[0]\n\n                    if not estimated_item:\n                        raise ValueError(\n                            \"Not all reference files estimated, please check [{file}]\".format(\n                                file=filename\n                            )\n                        )\n                    # Store nref\n                    if label in reference_item.tags:\n                        self.tag_wise[label]['Nref'] += 1\n\n                    # Populate y_pred based estimated_item\n                    if label in estimated_item.tags:\n                        self.y_pred[label].append(1)\n                        y_pred.append(1)\n                        self.tag_wise[label]['Nsys'] += 1\n\n                    else:\n                        self.y_pred[label].append(0)\n                        y_pred.append(0)\n\n                    # Accumulate intermediate values\n                    # True positives (TP)\n                    if label in reference_item.tags and label in estimated_item.tags:\n                        self.tag_wise[label]['Ntp'] += 1\n\n                    # True negatives (TN)\n                    if label not in reference_item.tags and label not in estimated_item.tags:\n                        self.tag_wise[label]['Ntn'] += 1\n\n                    # False positives (FP)\n                    if label not in reference_item.tags and label in estimated_item.tags:\n                        self.tag_wise[label]['Nfp'] += 1\n\n                    # False negatives (FN)\n                    if label in reference_item.tags and label not in estimated_item.tags:\n                        self.tag_wise[label]['Nfn'] += 1\n\n                if estimated_tag_probabilities is not None:\n                    # Evaluate based on per tag probabilities\n\n                    estimated_item = estimated_tag_probabilities.filter(filename=filename, label=label)[0]\n                    self.y_pred_score[label].append(float(estimated_item['probability']))\n\n        if estimated_tag_list is not None:\n            # Evaluate based on estimated tags\n\n            self.overall['Nref'] += sum(y_true)\n            self.overall['Nsys'] += sum(y_pred)\n\n            y_true = numpy.array(y_true)\n            y_pred = numpy.array(y_pred)\n\n            self.overall['Ntp'] += sum(y_pred + y_true > 1)\n            self.overall['Ntn'] += sum(y_pred + y_true == 0)\n            self.overall['Nfp'] += sum(y_pred - y_true > 0)\n            self.overall['Nfn'] += sum(y_true - y_pred > 0)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset(self):\n        self.overall = {\n            'Nref': 0.0,\n            'Nsys': 0.0,\n            'Ntp': 0.0,\n            'Ntn': 0.0,\n            'Nfp': 0.0,\n            'Nfn': 0.0,\n        }\n\n        self.tag_wise = {}\n        for label in self.tag_label_list:\n            self.tag_wise[label] = {\n                'Nref': 0.0,\n                'Nsys': 0.0,\n                'Ntp': 0.0,\n                'Ntn': 0.0,\n                'Nfp': 0.0,\n                'Nfn': 0.0,\n            }\n\n        return self", "response": "Reset internal state of the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef results_overall_metrics(self):\n\n        # F-measure\n        if self.overall['Nref'] > 0:\n            precision = metric.precision(\n                Ntp=self.overall['Ntp'],\n                Nsys=self.overall['Nsys']\n            )\n\n            recall = metric.recall(\n                Ntp=self.overall['Ntp'],\n                Nref=self.overall['Nref']\n            )\n\n            f_measure = metric.f_measure(\n                precision=precision,\n                recall=recall\n            )\n\n        else:\n            precision = None\n            recall = None\n            f_measure = None\n\n        y_true = []\n        y_score = []\n        for tag_id, tag_label in enumerate(self.tag_label_list):\n            y_true += self.y_true[tag_label]\n            y_score += self.y_pred_score[tag_label]\n\n        if y_score:\n            eer = metric.equal_error_rate(\n                y_true=y_true,\n                y_score=y_score\n            )\n\n        else:\n            eer = None\n\n        return {\n                'count': {\n                    'Nref': self.overall['Nref'],\n                    'Nsys': self.overall['Nsys'],\n                },\n                'f_measure': {\n                    'f_measure': f_measure,\n                    'precision': precision,\n                    'recall': recall,\n                },\n                'eer': {\n                    'eer': eer\n                }\n\n        }", "response": "Overall metrics\n            returns a dictionary format of the overall metrics"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of class - wise metrics for the class - wise tag - wise results.", "response": "def results_class_wise_metrics(self):\n        \"\"\"Class-wise metrics\n\n        Returns\n        -------\n        dict\n            results in a dictionary format\n\n        \"\"\"\n\n        results = {}\n        for tag_id, tag_label in enumerate(self.tag_label_list):\n            if tag_label not in results:\n                results[tag_label] = {}\n\n            # Counts\n            results[tag_label]['count'] = {\n                'Nref': self.tag_wise[tag_label]['Nref'],\n                'Nsys': self.tag_wise[tag_label]['Nsys'],\n                'Ntp': self.tag_wise[tag_label]['Ntp'],\n                'Ntn': self.tag_wise[tag_label]['Ntn'],\n                'Nfp': self.tag_wise[tag_label]['Nfp'],\n                'Nfn': self.tag_wise[tag_label]['Nfn'],\n            }\n\n            # Equal error rate\n            if self.y_pred_score[tag_label]:\n                results[tag_label]['eer'] = {\n                    'eer': metric.equal_error_rate(\n                        y_true=self.y_true[tag_label],\n                        y_score=self.y_pred_score[tag_label]\n                    )\n                }\n\n            else:\n                results[tag_label]['eer'] = {\n                    'eer': None\n                }\n\n            # F-measure\n            if self.tag_wise[tag_label]['Nref'] > 0:\n\n                precision = metric.precision(\n                    Ntp=self.tag_wise[tag_label]['Ntp'],\n                    Nsys=self.tag_wise[tag_label]['Nsys']\n                )\n\n                recall = metric.recall(\n                    Ntp=self.tag_wise[tag_label]['Ntp'],\n                    Nref=self.tag_wise[tag_label]['Nref']\n                )\n\n                f_measure = metric.f_measure(precision=precision, recall=recall)\n\n            else:\n                precision = None\n                recall = None\n                f_measure = None\n\n            results[tag_label]['f_measure'] = {\n                'f_measure': f_measure,\n                'precision': precision,\n                'recall': recall,\n            }\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary of class - wise averaged metrics for the current locale.", "response": "def results_class_wise_average_metrics(self):\n        \"\"\"Class-wise averaged metrics\n\n        Returns\n        -------\n        dict\n            results in a dictionary format\n\n        \"\"\"\n\n        class_wise_results = self.results_class_wise_metrics()\n\n        class_wise_eer = []\n        class_wise_fmeasure = []\n        class_wise_precision = []\n        class_wise_recall = []\n\n        for class_label in class_wise_results:\n            if class_wise_results[class_label]['eer']['eer'] is not None:\n                class_wise_eer.append(class_wise_results[class_label]['eer']['eer'])\n\n            if class_wise_results[class_label]['f_measure']['f_measure'] is not None:\n                class_wise_fmeasure.append(class_wise_results[class_label]['f_measure']['f_measure'])\n                class_wise_precision.append(class_wise_results[class_label]['f_measure']['precision'])\n                class_wise_recall.append(class_wise_results[class_label]['f_measure']['recall'])\n\n        if class_wise_eer:\n            eer = float(numpy.nanmean(class_wise_eer))\n\n        else:\n            eer = None\n\n        if class_wise_fmeasure:\n            f_measure = float(numpy.nanmean(class_wise_fmeasure))\n\n        else:\n            f_measure = None\n\n        if class_wise_precision:\n            precision = float(numpy.nanmean(class_wise_precision))\n\n        else:\n            precision = None\n\n        if class_wise_recall:\n            recall = float(numpy.nanmean(class_wise_recall))\n\n        else:\n            recall = None\n\n        return {\n            'eer': {\n                'eer': eer\n            },\n            'f_measure': {\n                'f_measure': f_measure,\n                'precision': precision,\n                'recall': recall,\n            }\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef result_report_parameters(self):\n\n        output = self.ui.data(field='Tags', value=len(self.tag_label_list)) + '\\n'\n        output += self.ui.data(field='Evaluated units', value=int(self.overall['Nref'])) + '\\n'\n\n        return output", "response": "Report metric parameters\n\n        Returns\n        -------\n        str\n            result report in string format"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef result_report_overall(self):\n\n        results = self.results_overall_metrics()\n\n        output = self.ui.section_header('Overall metrics (micro-average)', indent=2) + '\\n'\n\n        if 'f_measure' in results and results['f_measure']:\n\n            if results['f_measure']['f_measure'] is not None:\n                f_measure = results['f_measure']['f_measure'] * 100\n\n            else:\n                f_measure = None\n\n            if results['f_measure']['precision'] is not None:\n                precision = results['f_measure']['precision'] * 100\n\n            else:\n                precision = None\n\n            if results['f_measure']['recall'] is not None:\n                recall = results['f_measure']['recall'] * 100\n\n            else:\n                recall = None\n\n            output += self.ui.line('F-measure', indent=2) + '\\n'\n            output += self.ui.data(field='F-measure (F1)', value=f_measure, unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Precision', value=precision, unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Recall', value=recall, unit='%', indent=4) + '\\n'\n\n        if 'eer' in results and results['eer']:\n            if results['eer']['eer'] is not None:\n                eer = results['eer']['eer'] * 100\n\n            else:\n                eer = None\n\n            output += self.ui.line('Equal error rate', indent=2) + '\\n'\n            output += self.ui.data(field='Equal error rate (EER)', value=eer, unit='%', indent=4) + '\\n'\n\n        return output", "response": "Report overall results of the assessment process"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef result_report_class_wise_average(self):\n\n        results = self.results_class_wise_average_metrics()\n\n        output = self.ui.section_header('Class-wise average metrics (macro-average)', indent=2) + '\\n'\n\n        if 'f_measure' in results and results['f_measure']:\n            if results['f_measure']['f_measure'] is not None:\n                f_measure = results['f_measure']['f_measure'] * 100\n\n            else:\n                f_measure = None\n\n            if results['f_measure']['precision'] is not None:\n                precision = results['f_measure']['precision'] * 100\n\n            else:\n                precision = None\n\n            if results['f_measure']['recall'] is not None:\n                recall = results['f_measure']['recall'] * 100\n\n            else:\n                recall = None\n\n            output += self.ui.line('F-measure', indent=2) + '\\n'\n            output += self.ui.data(field='F-measure (F1)', value=f_measure, unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Precision', value=precision, unit='%', indent=4) + '\\n'\n            output += self.ui.data(field='Recall', value=recall, unit='%', indent=4) + '\\n'\n\n        if 'eer' in results and results['eer']:\n            if results['eer']['eer'] is not None:\n                eer = results['eer']['eer'] * 100\n\n            else:\n                eer = None\n\n            output += self.ui.line('Equal error rate', indent=2) + '\\n'\n            output += self.ui.data(field='Equal error rate (EER)', value=eer, unit='%', indent=4) + '\\n'\n\n        return output", "response": "Report class - wise average metrics"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreporting class - wise results of the test.", "response": "def result_report_class_wise(self):\n        \"\"\"Report class-wise results\n\n        Returns\n        -------\n        str\n            result report in string format\n\n        \"\"\"\n\n        results = self.results_class_wise_metrics()\n\n        output = self.ui.section_header('Class-wise metrics', indent=2) + '\\n'\n        output += self.ui.row(\n            'Tag', 'Nref', 'Nsys', 'F-score', 'Pre', 'Rec', 'EER',\n            widths=[20, 12, 12, 12, 12, 12, 12],\n            separators=[True, False, True, False, False, True, False],\n            indent=4\n        ) + '\\n'\n        output += self.ui.row('-', '-', '-', '-', '-', '-', '-') + '\\n'\n\n        for tag_label in self.tag_label_list:\n            if results[tag_label]['f_measure']['f_measure'] is not None:\n                f_measure = results[tag_label]['f_measure']['f_measure'] * 100\n\n            else:\n                f_measure = None\n\n            if results[tag_label]['f_measure']['precision'] is not None:\n                precision = results[tag_label]['f_measure']['precision'] * 100\n\n            else:\n                precision = None\n\n            if results[tag_label]['f_measure']['recall'] is not None:\n                recall = results[tag_label]['f_measure']['recall'] * 100\n\n            else:\n                recall = None\n\n            if results[tag_label]['eer']['eer'] is not None:\n                eer = results[tag_label]['eer']['eer'] * 100\n\n            else:\n                eer = None\n\n            output += self.ui.row(\n                tag_label,\n                results[tag_label]['count']['Nref'],\n                results[tag_label]['count']['Nsys'],\n                f_measure,\n                precision,\n                recall,\n                eer,\n                types=['str', 'int', 'int', 'float1_percentage', 'float1', 'float1', 'float1_percentage']\n            ) + '\\n'\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nobtains channel id for channel name if present in self. search_params.", "response": "def __getChannelId(self):\n\n        \"\"\"\n        Obtain channel id for channel name, if present in ``self.search_params``.\n        \"\"\"\n\n        if not self.search_params.get(\"channelId\"):\n            return\n\n        api_fixed_url = \"https://www.googleapis.com/youtube/v3/channels?part=id&maxResults=1&fields=items%2Fid&\"\n        url = api_fixed_url + urlencode({\"key\": self.api_key, \"forUsername\": self.search_params[\"channelId\"]})\n        get = requests.get(url).json()\n\n        try:\n            self.search_params[\"channelId\"] = get['items'][0]['id']\n            return # got it\n\n        except IndexError:\n            pass # try searching now...\n\n        api_fixed_url = \"https://www.googleapis.com/youtube/v3/search?part=snippet&type=channel&fields=items%2Fid&\"\n        url = api_fixed_url + urlencode({\"key\": self.api_key, \"q\": self.search_params['channelId']})\n        get = requests.get(url).json()\n\n        try:\n            self.search_params[\"channelId\"] = get['items'][0]['id']['channelId']\n\n        except IndexError:\n            del self.search_params[\"channelId\"]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a search query for advanced search options and return a dictionary of parameters and options.", "response": "def __searchParser(self, query):\n\n        \"\"\"\n        Parse `query` for advanced search options.\n\n        Parameters\n        ----------\n        query : str\n            Search query to parse. Besides a search query, user can specify additional search parameters and YTFS\n            specific options. Syntax:\n            Additional search parameters: ``option:value``. if `value` contains spaces, then surround it with\n            parentheses; available parameters: `channel`, `max`, `before`, `after`, `order`.\n            YTFS options: specify options between ``[`` and ``]``; Available options: `a`, `v`, `f`, `P`, `s`, `m`.\n            If an option takes a parameter, then specify it beetween parentheses.\n\n            Examples: ``channel:foo search query``, ``my favourite music [a]``,\n            ``channel:(the famous funny cats channel) [vf(240)P] funny cats max:20``.\n\n            Invalid parameters/options are ignored.\n\n        Returns\n        -------\n        params : tuple\n            Tuple: 0 - dictionary of url GET parameters; 1 - dictionary of YTStor options.\n        \"\"\"\n\n        ret = dict()\n\n        parse_params = True\n\n        buf = \"\"\n        ptr = \"\"\n\n        p_avail = (\"channel\", \"max\", \"before\", \"after\", \"order\")\n\n        opts = dict()\n        par_open = False\n\n        translate = {\n            'a': 'audio',\n            'v': 'video',\n            'f': 'format',\n            's': 'stream',\n            'P': 'stream',\n            'm': 'metadata',\n            'max': 'maxResults',\n            'channel': 'channelId',\n            'before': 'publishedBefore',\n            'after': 'publishedAfter',\n            'order': 'order',\n            '': 'q'\n        }\n\n        for i in query+' ':\n\n            if parse_params:\n\n                if not par_open:\n\n                    if i == ' ': # flush buf\n\n                        try:\n                            if ret.get(translate[ptr]):\n                                ret[ translate[ptr] ] += ' '\n                            else:\n                                ret[ translate[ptr] ] = ''\n\n                            ret[ translate[ptr] ] += buf\n\n                        except KeyError:\n                            pass\n\n                        ptr = \"\"\n                        buf = \"\"\n\n                    elif i == ':' and buf in p_avail:\n\n                        ptr = buf\n                        buf = \"\"\n\n                    elif not buf and i == '[': # buf must be empty\n\n                        parse_params = False\n                        ptr = \"\"\n\n                    elif i != '(':\n                        buf += i\n\n                elif not (par_open == 1 and i == ')'):\n                    buf += i\n\n                if i == '(': par_open += 1\n                if par_open > 0 and i == ')': par_open -= 1\n\n\n            else:\n\n                if i == ']':\n\n                    parse_params = True\n                    par_open = False\n                    ptr = \"\"\n                    buf = \"\"\n\n                elif ptr and not par_open and i == '(':\n                    par_open = True\n\n                elif par_open:\n\n                    if i == ')':\n\n                        try:\n                            opts[ translate[ptr] ] = buf\n                        except KeyError:\n                            pass\n\n                        par_open = False\n                        buf = \"\"\n\n                    else:\n                        buf += i\n\n                elif i.isalpha():\n\n                    ptr = i\n\n                    try:\n                        opts[ translate[ptr] ] = not i.isupper()\n                    except KeyError:\n                        pass\n\n        return (ret, opts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __search(self, pt=\"\"):\n\n        \"\"\"\n        Method responsible for searching using YouTube API.\n\n        Parameters\n        ----------\n        pt : str\n            Token of search results page. If ``None``, then the first page is downloaded.\n\n        Returns\n        -------\n        results : dict\n            Parsed JSON returned by YouTube API.\n        \"\"\"\n\n        if not self.search_params.get('q') and not self.search_params.get('channelId'):\n            return {'items': []} # no valid query - no results.\n\n        api_fixed_url = \"https://www.googleapis.com/youtube/v3/search?part=snippet&type=video&fields=items(id%2Ckind%2Csnippet)%2CnextPageToken%2CprevPageToken&\"\n\n        d = {\"key\": self.api_key, \"pageToken\": pt}\n        d.update(self.search_params)\n        url = api_fixed_url + urlencode(d)\n\n        try:\n            get = requests.get(url)\n        except requests.exceptions.ConnectionError:\n            raise ConnectionError\n\n        if get.status_code != 200:\n            return {'items': []} # no valid query - no results.\n\n        return get.json()", "response": "Method responsible for searching using YouTube API."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreloading search results or load another page.", "response": "def updateResults(self, forward=None):\n\n        \"\"\"\n        Reload search results or load another \"page\".\n\n        Parameters\n        ----------\n        forward : bool or None, optional\n            Whether move forwards or backwards (``True`` or ``False``). If ``None``, then first page is loaded.\n        \"\"\"\n\n        # this choses data we need.\n        files = lambda x: {\n            i['snippet']['title'].replace('/', '\\\\'): YTStor(\n                {'yid': i['id']['videoId'], 'pub_date': i['snippet']['publishedAt']},\n                opts=self.yts_opts) for i in x['items']\n        }\n        descs = lambda x: {\n            (i['snippet']['title'].replace('/', '\\\\') + '.txt'): YTMetaStor(\n                {\n                    'title': i['snippet']['title'],\n                    'yid': i['id']['videoId'],\n                    'desc': i['snippet']['description'],\n                    'channel': i['snippet']['channelTitle'],\n                    'pub_date': i['snippet']['publishedAt']\n                },\n                opts=dict()\n            ) for i in x['items']\n        }\n        thumbs = lambda x: {\n            (i['snippet']['title'].replace('/', '\\\\') + '.jpg'): YTMetaStor(\n                {'url': i['snippet']['thumbnails']['high']['url'], 'pub_date': i['snippet']['publishedAt']}, opts=dict()\n            ) for i in x['items']\n        }\n\n        try:\n            if self.adj_tokens[forward] is None: # in case someone would somehow cross boundary.\n                forward = None\n        except KeyError:\n            pass\n\n        recv = None\n        try:\n            try:\n                data = self.avail_files[ self.adj_tokens[forward] ] # maybe data is already available locally.\n            except KeyError:\n                recv = self.__search( self.adj_tokens[forward] ) # nope, we have to search.\n        except KeyError: # wrong index in adj_tokens\n\n            if forward is None:\n                recv = self.__search()\n            else:\n                raise ValueError(\"Valid values for forward are True, False or None (default).\")\n\n        if recv is not None:\n\n            _d = files(recv)\n            if self.preferences['metadata']['desc']: _d.update(descs(recv))\n            if self.preferences['metadata']['thumb']: _d.update(thumbs(recv))\n\n            data = (None, _d) # little format unification.\n\n        if len(self.avail_files) > 4:\n\n            pop = self.avail_files.popitem(False) # get rid of the oldest data.\n            for s in pop[1][1].values(): s.clean()\n\n        adj_t = deepcopy(self.adj_tokens) # this will we write to avail_files, now we update self.adj_tokens.\n\n        if data[0] is None: # get tokens from obtained results.\n            try:\n                self.adj_tokens[False] = recv['prevPageToken']\n            except KeyError:\n                self.adj_tokens[False] = None\n\n            try:\n                self.adj_tokens[True] = recv['nextPageToken']\n            except KeyError:\n                self.adj_tokens[True] = None\n\n        else: # already in avail_files.\n            self.adj_tokens = data[0]\n\n        if forward is not None:\n            # backup last results in avail_files:\n            self.avail_files[ self.adj_tokens[not forward] ] = (adj_t, self.visible_files)\n\n        self.visible_files = data[1]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear the data. For each YTStor object present in this object is executed.", "response": "def clean(self):\n\n        \"\"\"Clear the data. For each ``YTStor`` object present in this object ``clean`` method is executed.\"\"\"\n\n        for s in self.visible_files.values():\n            s.clean()\n        for s in [sub[1][x] for sub in self.avail_files.values() for x in sub[1]]: # Double list comprehensions aren't very\n            s.clean()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading desired range of data and put it in yts object.", "response": "def fetch(yts, needed_range, fh):\n\n        \"\"\"\n        Download desired range of data and put it in `yts` object (e.g. ``YTStor``).\n\n        Parameters\n        ----------\n        yts : YTStor\n            Stor-like object to which we will write.\n        needed_range : tuple\n            Two element tuple that represents a data range - compliant with ``range_t`` subrange definition.\n        fh : int\n            Descriptor used by a process for filesystem operations.\n        \n        Returns\n        -------\n        None\n            Method does not return; data is written directly to `yts` object.\n        \"\"\"\n\n        if yts.preferences['audio'] and yts.preferences['video'] and isinstance(yts.url, tuple) and not yts.preferences['stream']:\n            #condition for merging.\n\n            with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as d, tempfile.NamedTemporaryFile(prefix='a') as a, tempfile.NamedTemporaryFile(prefix='v') as v:\n                # after with statement, files - save d - shall be removed\n\n                v.write(yts.r_session.get(yts.url[0]).content)\n                a.write(yts.r_session.get(yts.url[1]).content)\n\n                PP = youtube_dl.postprocessor.FFmpegMergerPP(yts.ytdl)\n                PP.run({'filepath': d.name, '__files_to_merge': (v.name, a.name)}) # merge\n\n                _d = d.name\n\n            with open(_d, mode=\"rb\") as d:\n\n                yts.data.write( d.read() )\n                yts.data.flush()\n\n                yts.filesize = os.path.getsize(_d)\n\n                yts.avail += (0, yts.filesize)\n\n            os.remove(_d)\n\n        else: # no merging\n\n            if yts.preferences['stream'] is False: # preload\n\n                yts.data.write(yts.r_session.get(yts.url).content)\n                yts.data.flush()\n\n                yts.avail += (0, yts.filesize)\n\n            else: # stream\n\n                hr = (needed_range[0], needed_range[1] - 1)\n\n                get = yts.r_session.get(yts.url, headers={'Range': 'bytes=' + '-'.join(str(i) for i in hr)})\n\n                yts.data.seek(hr[0])\n                yts.data.write(get.content)\n                yts.data.flush()\n\n                ret = list( int(s) for s in get.headers.get('content-range').split(' ')[1].split('/')[0].split('-') )\n                ret[1] += 1\n\n                yts.avail += tuple(ret)\n                yts.processing_range -= needed_range"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef registerHandler(self, fh): # Do I even need that? possible FIXME.\n\n        \"\"\"\n        Register new file descriptor.\n\n        Parameters\n        ----------\n        fh : int\n            File descriptor.\n        \"\"\"\n\n        self.fds.add(fh)\n        self.atime = int(time()) # update access time\n\n        self.lock.acquire()\n\n        try:\n            if (0, self.filesize) not in self.avail and self.preferences['stream'] is False:\n\n                Downloader.fetch(self, None, fh) # lock forces other threads to wait, so fetch will perform just once.\n        except requests.exceptions.ConnectionError:\n            raise ConnectionError\n\n        finally:\n            self.lock.release()", "response": "Register new file descriptor and download it if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading data. Method returns data instantly, if they're avaialable and in ``self.safe_range``. Otherwise data is downloaded and then returned. Parameters ---------- offset : int Read offset length : int Length of data to read. fh : int File descriptor.", "response": "def read(self, offset, length, fh):\n\n        \"\"\"\n        Read data. Method returns data instantly, if they're avaialable and in ``self.safe_range``. Otherwise data is\n        downloaded and then returned.\n\n        Parameters\n        ----------\n        offset : int\n            Read offset\n        length : int\n            Length of data to read.\n        fh : int\n            File descriptor.\n        \"\"\"\n\n        current = (offset, offset + length)\n\n        safe = [ current[0] - ( 8 * length ), current[1] + ( 16 * length ) ]\n        if safe[0] < 0: safe[0] = 0\n        if safe[1] > self.filesize: safe[1] = self.filesize\n        safe = tuple(safe)\n\n        self.lock.acquire()\n\n        try:\n            dl = range_t({safe}) - self.avail\n\n            for r in dl.toset():\n                try:\n                    Downloader.fetch(self, r, fh) # download is, let's say, atomic thanks to lock\n                except requests.exceptions.ConnectionError:\n                    raise ConnectionError\n\n            if self.disk + 1 < len(self.avail):\n\n                self.data.rollover()\n                self.disk += 1\n\n        finally:\n            self.lock.release()\n\n        self.data.seek(offset)\n        return self.data.read(length)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean(self):\n\n        \"\"\"\n        Clear data. Explicitly close ``self.data`` if object is unused.\n        \"\"\"\n\n        self.closing = True # schedule for closing.\n\n        if not self.fds:\n            self.data.close()", "response": "Clear data. Explicitly close self. data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the data from the file.", "response": "def read(self, offset, length, fh):\n\n        \"\"\"\n        Read data.\n\n        Parameters\n        ----------\n        offset : int\n            Read offset.\n        length : int\n            Length of data.\n        fh : int\n            File descriptor, ignored.\n        \"\"\"\n\n        self.data.seek(offset)\n        return self.data.read(length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new entry to the internal list of control files.", "response": "def push(self, yts):\n\n        \"\"\"\n        Search for, add and return new file descriptor.\n\n        Parameters\n        ----------\n        yts : YTStor-obj or None\n            ``YTStor`` object for which we want to allocate a descriptor or ``None``, if we allocate descriptor for a\n            control file.\n\n        Returns\n        -------\n        k : int\n            File descriptor.\n        \"\"\"\n\n        if not isinstance(yts, (YTStor, YTMetaStor, type(None))):\n            raise TypeError(\"Expected YTStor object, YTMetaStor object or None.\")\n\n        k = 0\n        while k in self.keys():\n            k += 1\n        self[k] = yts\n\n        return k"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a path to a tuple identifier.", "response": "def __pathToTuple(self, path):\n\n        \"\"\"\n        Convert directory or file path to its tuple identifier.\n\n        Parameters\n        ----------\n        path : str\n            Path to convert. It can look like /, /directory, /directory/ or /directory/filename.\n\n        Returns\n        -------\n        tup_id : tuple\n            Two element tuple identifier of directory/file of (`directory`, `filename`) format. If path leads to main\n            directory, then both fields of tuple will be ``None``. If path leads to a directory, then field `filename`\n            will be ``None``.\n\n        Raises\n        ------\n        YTFS.PathConvertError\n            When invalid path is given.\n        \"\"\"\n\n        if not path or path.count('/') > 2:\n            raise YTFS.PathConvertError(\"Bad path given\") # empty or too deep path\n\n        try:\n            split = path.split('/')\n        except (AttributeError, TypeError):\n            raise TypeError(\"Path has to be string\") #path is not a string\n\n        if split[0]:\n            raise YTFS.PathConvertError(\"Path needs to start with '/'\") # path doesn't start with '/'.\n        del split[0]\n\n        try:\n            if not split[-1]: split.pop() # given path ended with '/'.\n        except IndexError:\n            raise YTFS.PathConvertError(\"Bad path given\") # at least one element in split should exist at the moment\n\n        if len(split) > 2:\n            raise YTFS.PathConvertError(\"Path is too deep. Max allowed level is 2\") # should happen due to first check, but ...\n\n        try:\n            d = split[0]\n        except IndexError:\n            d = None\n        try:\n            f = split[1]\n        except IndexError:\n            f = None\n\n        if not d and f:\n            raise YTFS.PathConvertError(\"Bad path given\") # filename is present, but directory is not #sheeeeeeiiit\n\n        return (d, f)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __exists(self, p):\n\n        \"\"\"\n        Check if file of given path exists.\n\n        Parameters\n        ----------\n        p : str or tuple\n            Path or tuple identifier.\n\n        Returns\n        -------\n        exists : bool\n            ``True``, if file exists. Otherwise ``False``.\n        \"\"\"\n\n        try:\n            p = self.__pathToTuple(p)\n        except TypeError:\n            pass\n\n        return ((not p[0] and not p[1]) or (p[0] in self.searches and not p[1]) or (p[0] in self.searches and\n            p[1] in self.searches[p[0]]))", "response": "Check if file of given path exists."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the file attributes of a specific file.", "response": "def getattr(self, tid, fh=None):\n\n        \"\"\"\n        File attributes.\n\n        Parameters\n        ----------\n        tid : str\n            Path to file. Original `path` argument is converted to tuple identifier by ``_pathdec`` decorator.\n        fh : int\n            File descriptor. Unnecessary, therefore ignored.\n\n        Returns\n        -------\n        st : dict\n            Dictionary that contains file attributes. See: ``man 2 stat``.\n        \"\"\"\n\n        if not self.__exists(tid):\n            raise FuseOSError(errno.ENOENT)\n\n        pt = self.PathType.get(tid)\n\n        st = deepcopy(self.st)\n        st['st_atime'] = int(time())\n        st['st_mtime'] = st['st_atime']\n        st['st_ctime'] = st['st_atime']\n\n        if pt is self.PathType.file:\n            st['st_mode'] = stat.S_IFREG | 0o444\n            st['st_nlink'] = 1\n\n            st['st_size'] = self.searches[ tid[0] ][ tid[1] ].filesize\n\n            st['st_ctime'] = self.searches[ tid[0] ][ tid[1] ].ctime\n            st['st_mtime'] = st['st_ctime']\n            st['st_atime'] = self.searches[ tid[0] ][ tid[1] ].atime\n\n        elif pt is self.PathType.ctrl:\n\n            st['st_mode'] = stat.S_IFREG | 0o774\n            st['st_nlink'] = 1\n            st['st_size'] = len(self.__sh_script)\n\n        elif pt is self.PathType.main:\n            st['st_mode'] = stat.S_IFDIR | 0o774\n\n        st['st_blocks'] = math.ceil(st['st_size'] / st['st_blksize'])\n\n        return st"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the contents of a directory.", "response": "def readdir(self, tid, fh):\n\n        \"\"\"\n        Read directory contents. Lists visible elements of ``YTActions`` object.\n\n        Parameters\n        ----------\n        tid : str\n            Path to file. Original `path` argument is converted to tuple identifier by ``_pathdec`` decorator.\n        fh : int\n            File descriptor. Ommited in the function body.\n\n        Returns\n        -------\n        list\n            List of filenames, wich will be shown as directory content.\n        \"\"\"\n\n        ret = []\n        pt = self.PathType.get(tid)\n        try:\n            if pt is self.PathType.main:\n                ret = list(self.searches)\n\n            elif pt is self.PathType.subdir:\n                ret = list(self.searches[tid[0]])\n\n            elif pt is self.PathType.file:\n                raise FuseOSError(errno.ENOTDIR)\n\n            else:\n                raise FuseOSError(errno.ENOENT)\n\n        except KeyError:\n            raise FuseOSError(errno.ENOENT)\n\n        return ['.', '..'] + ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rename(self, old, new):\n\n        \"\"\"\n        Directory renaming support. Needed because many file managers create directories with default names, wich\n        makes it impossible to perform a search without CLI. Name changes for files are not allowed, only for\n        directories.\n\n        Parameters\n        ----------\n        old : str\n            Old name. Converted to tuple identifier by ``_pathdec`` decorator.\n        new : str\n            New name. Converted to tuple identifier in actual function body.\n        \"\"\"\n\n        new = self.__pathToTuple(new)\n\n        if not self.__exists(old):\n            raise FuseOSError(errno.ENOENT)\n\n        if self.PathType.get(old) is not self.PathType.subdir or self.PathType.get(new) is not self.PathType.subdir:\n            raise FuseOSError(errno.EPERM)\n\n        if self.__exists(new):\n            raise FuseOSError(errno.EEXIST)\n\n        try:\n            new_dir_ent = YTActions(new[0])\n            new_dir_ent.updateResults()\n        except ConnectionError:\n            raise FuseOSError(errno.ENETDOWN)\n\n        self.searches[new[0]] = new_dir_ent # as in mkdir\n\n        try:\n            del self.searches[old[0]]\n\n        except KeyError:\n            raise FuseOSError(errno.ENOENT)\n\n        return 0", "response": "Rename a key - value entry in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rmdir(self, tid):\n\n        \"\"\"\n        Directory removal. ``YTActions`` object under `tid` is told to clean all data, and then it is deleted.\n\n        Parameters\n        ----------\n        tid : str\n            Path to file. Original `path` argument is converted to tuple identifier by ``_pathdec`` decorator.\n        \"\"\"\n\n        pt = self.PathType.get(tid)\n\n        if pt is self.PathType.main:\n            raise FuseOSError(errno.EINVAL)\n        elif pt is not self.PathType.subdir:\n            raise FuseOSError(errno.ENOTDIR)\n\n        try:\n            self.searches[tid[0]].clean()\n            del self.searches[tid[0]]\n\n        except KeyError:\n            raise FuseOSError(errno.ENOENT)\n\n        return 0", "response": "Directory removal. ``YTActions`` object under `tid` is told to clean all data, and then it is deleted.\n\n        Parameters\n        ----------\n        tid : str\n            Path to file. Original `path` argument is converted to tuple identifier by ``_pathdec`` decorator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening a new YTStor object associated with this file.", "response": "def open(self, tid, flags):\n\n        \"\"\"\n        File open. ``YTStor`` object associated with this file is initialised and written to ``self.fds``.\n\n        Parameters\n        ----------\n        tid : str\n            Path to file. Original `path` argument is converted to tuple identifier by ``_pathdec`` decorator.\n        flags : int\n            File open mode. Read-only access is allowed.\n\n        Returns\n        -------\n        int\n            New file descriptor\n        \"\"\"\n\n        pt = self.PathType.get(tid)\n\n        if pt is not self.PathType.file and pt is not self.PathType.ctrl:\n            raise FuseOSError(errno.EINVAL)\n\n        if pt is not self.PathType.ctrl and (flags & os.O_WRONLY or flags & os.O_RDWR):\n            raise FuseOSError(errno.EPERM)\n\n        if not self.__exists(tid):\n            raise FuseOSError(errno.ENOENT)\n\n        try:\n            yts = self.searches[tid[0]][tid[1]]\n\n        except KeyError:\n            return self.fds.push(None) # for control file no association is needed.\n\n        try:\n            obI = yts.obtainInfo() # network may fail\n        except ConnectionError:\n            raise FuseOSError(errno.ENETDOWN)\n\n        if obI:\n            fh = self.fds.push(yts)\n\n            try:\n                yts.registerHandler(fh)\n            except ConnectionError:\n                raise FuseOSError(errno.ENETDOWN)\n\n            return fh\n        else:\n            raise FuseOSError(errno.EINVAL)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a movie from a file.", "response": "def read(self, tid, length, offset, fh):\n\n        \"\"\"\n        Read from a file. Data is obtained from ``YTStor`` object (which is kept under `fh` descriptor) using its\n        ``read`` method.\n\n        Parameters\n        ----------\n        tid : str\n            Path to file. Original `path` argument is converted to tuple identifier by ``_pathdec`` decorator.\n        length : int\n            Length of data to read.\n        offset : int\n            Posision from which reading will start.\n        fh : int\n            File descriptor.\n\n        Returns\n        -------\n        bytes\n            Movie data.\n        \"\"\"\n\n        try:\n            return self.fds[fh].read(offset, length, fh)\n\n        except AttributeError: # control file\n\n            if tid[1] not in (\" next\", \" prev\"):\n                raise FuseOSError(errno.EINVAL)\n\n            return self.__sh_script[offset:offset+length]\n\n        except KeyError: # descriptor does not exist.\n            raise FuseOSError(errno.EBADF)\n\n        except ConnectionError:\n            raise FuseOSError(errno.ESTALE)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, tid, data, offset, fh):\n\n        \"\"\"\n        Write operation. Applicable only for control files - updateResults is called.\n\n        Parameters\n        ----------\n        tid : str\n            Path to file. Original `path` argument is converted to tuple identifier by ``_pathdec`` decorator.\n        data : bytes\n            Ignored.\n        offset : int\n            Ignored.\n        fh : int\n            File descriptor.\n\n        Returns\n        -------\n        int\n            Length of data written.\n        \"\"\"\n\n        if tid[1] == \" next\":\n            d = True\n        elif tid[1] == \" prev\":\n            d = False\n        else:\n            raise FuseOSError(errno.EPERM)\n\n        try:\n            self.searches[tid[0]].updateResults(d)\n        except KeyError:\n            raise FuseOSError(errno.EINVAL) # sth went wrong...\n        except ConnectionError:\n            raise FuseOSError(errno.ENETDOWN)\n\n        return len(data)", "response": "Write data to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef release(self, tid, fh):\n\n        \"\"\"\n        Close file. Descriptor is removed from ``self.fds``.\n\n        Parameters\n        ----------\n        tid : str\n            Path to file. Ignored.\n        fh : int\n            File descriptor to release.\n        \"\"\"\n\n        try:\n            try:\n                self.fds[fh].unregisterHandler(fh)\n\n            except AttributeError:\n                pass\n\n            del self.fds[fh]\n\n        except KeyError:\n            raise FuseOSError(errno.EBADF)\n\n        return 0", "response": "Release a file descriptor from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __optimize(self):\n\n        \"\"\"\n        Merge overlapping or contacting subranges from ``self.__has`` attribute and update it. Called from all methods\n        that modify object contents.\n\n        Returns\n        -------\n        None\n            Method does not return. It does internal modifications on ``self.__has`` attribute.\n        \"\"\"\n\n        ret = []\n\n        for (begin, end) in sorted(self.__has):\n\n            if ret and begin <= ret[-1][1] < end: # when current range overlaps with the last one from ret\n                ret[-1] = (ret[-1][0], end)\n            elif not ret or begin > ret[-1][1]:\n                ret.append( (begin, end) )\n\n        self.__has = set(ret)", "response": "This method is used to optimize the number of items in the current set of items."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting input data to a range tuple.", "response": "def __val_convert(self, val): # maybe I should make a decorator from this.\n\n        \"\"\"\n        Convert input data to a range tuple (start, end).\n\n        Parameters\n        ----------\n        val : int or tuple or list or range\n            Two element indexed object, that represents a range, or integer.\n\n        Returns\n        -------\n        converted : tuple\n            Tuple that represents a range.\n        \"\"\"\n\n        converted = (0, 0) # just in case\n\n        # validate and change val to a tuple.\n\n        if isinstance(val, range) and 0 <= val.start < val.stop and val.step == 1:\n\n            converted = (val.start, val.stop)\n\n        elif (isinstance(val, tuple) or isinstance(val, list)) and 0 <= val[0] < val[1] and len(val) == 2:\n\n            converted = val\n\n        elif isinstance(val, int):\n            converted = (val, val+1)\n\n        else:\n            raise ValueError(\"Expected indexed positive value of lenght 2, integer or range of step 1\")\n\n        return converted"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef contains(self, val):\n\n        \"\"\"\n        Check if given value or range is present.\n\n        Parameters\n        ----------\n        val : int or tuple or list or range\n            Range or integer being checked.\n\n        Returns\n        -------\n        retlen : int\n            Length of overlapping with `val` subranges.\n        \"\"\"\n\n        (start, end) = self.__val_convert(val) # conversion\n\n        retlen = 0\n        for r in self.__has:\n            if start < r[1] and end > r[0]:\n                retlen += ((end < r[1] and end) or r[1]) - ((start > r[0] and start) or r[0])\n\n        return retlen", "response": "Check if given value or range is present."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match(self, val):\n\n        \"\"\"\n        Search for overlapping with `val` subranges. In fact, it is a visible wrapper of hidden ``__match_l``.\n\n        Parameters\n        ----------\n        val : int or tuple or list or range\n            Range or integer being checked.\n\n        Returns\n        -------\n        set\n            Set of overlapping subranges.\n        \"\"\"\n        \n        conv = self.__val_convert(val) # conversion\n\n        return self.__match_l(conv, self.__has)", "response": "Returns True if the set contains the key val."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the area under the ROC curve.", "response": "def auc(x, y, reorder=False): #from sklearn, http://scikit-learn.org, licensed under BSD License\n    \"\"\"Compute Area Under the Curve (AUC) using the trapezoidal rule\n\n    This is a general fuction, given points on a curve.  For computing the area\n    under the ROC-curve, see :func:`auc_score`.\n\n    Parameters\n    ----------\n    x : array, shape = [n]\n        x coordinates.\n\n    y : array, shape = [n]\n        y coordinates.\n\n    reorder : boolean, optional (default=False)\n        If True, assume that the curve is ascending in the case of ties, as for\n        an ROC curve. If the curve is non-ascending, the result will be wrong.\n\n    Returns\n    -------\n    auc : float\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import metrics\n    >>> y = np.array([1, 1, 2, 2])\n    >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n    >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n    >>> metrics.auc(fpr, tpr)\n    0.75\n\n    See also\n    --------\n    auc_score : Computes the area under the ROC curve\n\n    \"\"\"\n    if np is None:\n        raise ImportError(\"No numpy installed\")\n\n    # XXX: Consider using  ``scipy.integrate`` instead, or moving to\n    # ``utils.extmath``\n    if not isinstance(x, np.ndarray): x = np.array(x)\n    if not isinstance(x, np.ndarray): y = np.array(y)\n    if x.shape[0] < 2:\n        raise ValueError('At least 2 points are needed to compute'\n                         ' area under curve, but x.shape = %s' % x.shape)\n\n    if reorder:\n        # reorder the data points according to the x axis and using y to\n        # break ties\n        x, y = np.array(sorted(points for points in zip(x, y))).T\n        h = np.diff(x)\n    else:\n        h = np.diff(x)\n        if np.any(h < 0):\n            h *= -1\n            assert not np.any(h < 0), (\"Reordering is not turned on, and \"\n                                       \"The x array is not increasing: %s\" % x)\n\n    area = np.sum(h * (y[1:] + y[:-1])) / 2.0\n    return area"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting a training set, test set and optimally a development set from one file, or multiple *interdependent* files (such as a parallel corpus). It is assumed each line contains one instance (such as a word or sentence for example).", "response": "def filesampler(files, testsetsize = 0.1, devsetsize = 0, trainsetsize = 0, outputdir = '', encoding='utf-8'):\n        \"\"\"Extract a training set, test set and optimally a development set from one file, or multiple *interdependent* files (such as a parallel corpus). It is assumed each line contains one instance (such as a word or sentence for example).\"\"\"\n\n        if not isinstance(files, list):\n            files = list(files)\n\n        total = 0\n        for filename in files:\n            f = io.open(filename,'r', encoding=encoding)\n            count = 0\n            for line in f:\n                count += 1\n            f.close()\n            if total == 0:\n                total = count\n            elif total != count:\n                raise Exception(\"Size mismatch, when multiple files are specified they must contain the exact same amount of lines! (\" +str(count)  + \" vs \" + str(total) +\")\")\n\n        #support for relative values:\n        if testsetsize < 1:\n            testsetsize = int(total * testsetsize)\n        if devsetsize < 1 and devsetsize > 0:\n            devsetsize = int(total * devsetsize)\n\n\n        if testsetsize >= total or devsetsize >= total or testsetsize + devsetsize >= total:\n            raise Exception(\"Test set and/or development set too large! No samples left for training set!\")\n\n\n        trainset = {}\n        testset = {}\n        devset = {}\n        for i in range(1,total+1):\n            trainset[i] = True\n        for i in random.sample(trainset.keys(), int(testsetsize)):\n            testset[i] = True\n            del trainset[i]\n\n        if devsetsize > 0:\n            for i in random.sample(trainset.keys(), int(devsetsize)):\n                devset[i] = True\n                del trainset[i]\n\n        if trainsetsize > 0:\n            newtrainset = {}\n            for i in random.sample(trainset.keys(), int(trainsetsize)):\n                newtrainset[i] = True\n            trainset = newtrainset\n\n        for filename in files:\n            if not outputdir:\n                ftrain = io.open(filename + '.train','w',encoding=encoding)\n            else:\n                ftrain = io.open(outputdir + '/' +  os.path.basename(filename) + '.train','w',encoding=encoding)\n            if not outputdir:\n                ftest = io.open(filename + '.test','w',encoding=encoding)\n            else:\n                ftest = io.open(outputdir + '/' + os.path.basename(filename) + '.test','w',encoding=encoding)\n            if devsetsize > 0:\n                if not outputdir:\n                    fdev = io.open(filename + '.dev','w',encoding=encoding)\n                else:\n                    fdev = io.open(outputdir + '/' +  os.path.basename(filename) + '.dev','w',encoding=encoding)\n\n            f = io.open(filename,'r',encoding=encoding)\n            for linenum, line in enumerate(f):\n                if linenum+1 in trainset:\n                    ftrain.write(line)\n                elif linenum+1 in testset:\n                    ftest.write(line)\n                elif devsetsize > 0 and linenum+1 in devset:\n                    fdev.write(line)\n            f.close()\n\n            ftrain.close()\n            ftest.close()\n            if devsetsize > 0: fdev.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef done(self, warn=True):\n        if not self.process:\n            raise Exception(\"Not implemented yet or process not started yet, make sure to overload the done() method in your Experiment class\")\n        self.process.poll()\n        if self.process.returncode == None:\n            return False\n        elif self.process.returncode > 0:\n            raise ProcessFailed()\n        else:\n            self.endtime = datetime.datetime.now()\n            return True", "response": "Is the process done?"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn target range only if source index aligns to a single consecutive range of target tokens.", "response": "def getalignedtarget(self, index):\n        \"\"\"Returns target range only if source index aligns to a single consecutive range of target tokens.\"\"\"\n        targetindices = []\n        target = None\n        foundindex = -1\n        for sourceindex, targetindex in self.alignment:\n            if sourceindex == index:\n                targetindices.append(targetindex)\n        if len(targetindices) > 1:\n            for i in range(1,len(targetindices)):\n                if abs(targetindices[i] - targetindices[i-1]) != 1:\n                    break  # not consecutive\n            foundindex = (min(targetindices), max(targetindices))\n            target = ' '.join(self.target[min(targetindices):max(targetindices)+1])\n        elif targetindices:\n            foundindex = targetindices[0]\n            target = self.target[foundindex]\n\n        return target, foundindex"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the targetword for a specified index in the source words", "response": "def targetword(self, index, targetwords, alignment):\n        \"\"\"Return the aligned targetword for a specified index in the source words\"\"\"\n        if alignment[index]:\n            return targetwords[alignment[index]]\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the targetwords for a specified index in the source words", "response": "def targetwords(self, index, targetwords, alignment):\n        \"\"\"Return the aligned targetwords for a specified index in the source words\"\"\"\n        return [ targetwords[x] for x in alignment[index] ]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nproducing all lists of size positive integers in decreasing order that add up to n.", "response": "def sum_to_n(n, size, limit=None): #from http://stackoverflow.com/questions/2065553/python-get-all-numbers-that-add-up-to-a-number\n    \"\"\"Produce all lists of `size` positive integers in decreasing order\n    that add up to `n`.\"\"\"\n    if size == 1:\n        yield [n]\n        return\n    if limit is None:\n        limit = n\n    start = (n + size - 1) // size\n    stop = min(limit, n - size + 1) + 1\n    for i in range(start, stop):\n        for tail in sum_to_n(n - i, size - 1, i):\n            yield [i] + tail"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes all possible single consecutive gaps in any sequence of the specified length.", "response": "def consecutivegaps(n, leftmargin = 0, rightmargin = 0):\n    \"\"\"Compute all possible single consecutive gaps in any sequence of the specified length. Returns\n    (beginindex, length) tuples. Runs in  O(n(n+1) / 2) time. Argument is the length of the sequence rather than the sequence itself\"\"\"\n    begin = leftmargin\n    while begin < n:\n        length = (n - rightmargin) - begin\n        while length > 0:\n            yield (begin, length)\n            length -= 1\n        begin += 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef possiblesplits(n, minsplits=2, maxsplits=0):\n    if not maxsplits: maxsplits = n\n    for nrsplits in range(minsplits,maxsplits + 1):\n        for split in sum_to_n(n,nrsplits):\n            split_with_indices = []\n            begin = 0\n            for length in split:\n                split_with_indices.append( (begin, length) )\n                begin += length\n            yield split_with_indices", "response": "Returns a generator that yields all possible splits of a sequence of length n."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nfa(self, nextstate):\n        if self.interval:\n            mininterval, maxinterval = self.interval #pylint: disable=unpacking-non-sequence\n            nextstate2 = nextstate\n            for i in range(maxinterval):\n                state = State(transitions=[(self,self.match, nextstate2)])\n                if i+1> mininterval:\n                    if nextstate is not nextstate2: state.transitions.append((self,self.match, nextstate))\n                    if maxinterval == MAXINTERVAL:\n                        state.epsilon.append(state)\n                        break\n                nextstate2 = state\n            return state\n        else:\n            state = State(transitions=[(self,self.match, nextstate)])\n            return state", "response": "Returns an initial state for an NFA"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nfa(self):\n        finalstate = State(final=True)\n        nextstate = finalstate\n        for tokenexpr in reversed(self):\n            state = tokenexpr.nfa(nextstate)\n            nextstate = state\n        return NFA(state)", "response": "convert the expression into an NFA"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef product(seq):\n    if len(seq) == 0:\n        return 0\n    else:\n        product = 1\n        for x in seq:\n            product *= x\n        return product", "response": "Return the product of a sequence of numerical values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stddev(values, meanval=None):  #from AI: A Modern Appproach\n    if meanval == None: meanval = mean(values)\n    return math.sqrt( sum([(x - meanval)**2 for x in values]) / (len(values)-1) )", "response": "Returns the standard deviation of a set of values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmultiplies each number by a constant such that the sum is 1. 0.", "response": "def normalize(numbers, total=1.0):  #from AI: A Modern Appproach\n    \"\"\"Multiply each number by a constant such that the sum is 1.0 (or total).\n    >>> normalize([1,2,1])\n    [0.25, 0.5, 0.25]\n    \"\"\"\n    k = total / sum(numbers)\n    return [k * n for n in numbers]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a frequency list from a file.", "response": "def load(self, filename):\n        \"\"\"Load a frequency list from file (in the format produced by the save method)\"\"\"\n        f = io.open(filename,'r',encoding='utf-8')\n        for line in f:\n            data = line.strip().split(\"\\t\")\n            type, count = data[:2]\n            self.count(type,count)\n        f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving a frequency list to file", "response": "def save(self, filename, addnormalised=False):\n        \"\"\"Save a frequency list to file, can be loaded later using the load method\"\"\"\n        f = io.open(filename,'w',encoding='utf-8')\n        for line in self.output(\"\\t\", addnormalised):\n            f.write(line + '\\n')\n        f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef count(self, type, amount = 1):\n        if self.dovalidation: type = self._validate(type)\n        if self._ranked: self._ranked = None\n        if type in self._count:\n            self._count[type] += amount\n        else:\n            self._count[type] = amount\n        self.total += amount", "response": "Count a certain type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the probability of the token type", "response": "def p(self, type):\n        \"\"\"Returns the probability (relative frequency) of the token\"\"\"\n        if self.dovalidation: type = self._validate(type)\n        return self._count[type] / float(self.total)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef output(self,delimiter = '\\t', addnormalised=False):\n        for type, count in self:\n            if isinstance(type,tuple) or isinstance(type,list):\n                if addnormalised:\n                    yield \" \".join((u(x) for x in type)) + delimiter + str(count) + delimiter + str(count/self.total)\n                else:\n                    yield \" \".join((u(x) for x in type)) + delimiter + str(count)\n            elif isstring(type):\n                if addnormalised:\n                    yield type + delimiter + str(count) + delimiter + str(count/self.total)\n                else:\n                    yield type + delimiter + str(count)\n            else:\n                if addnormalised:\n                    yield str(type) + delimiter + str(count) + delimiter + str(count/self.total)\n                else:\n                    yield str(type) + delimiter + str(count)", "response": "Print a representation of the frequency list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef information(self, type):\n        if not self.base:\n            return -math.log(self._dist[type])\n        else:\n            return -math.log(self._dist[type], self.base)", "response": "Computes the information content of the specified type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the entropy of the distribution", "response": "def entropy(self, base = 2):\n        \"\"\"Compute the entropy of the distribution\"\"\"\n        entropy = 0\n        if not base and self.base: base = self.base\n        for type in self._dist:\n            if not base:\n                entropy += self._dist[type] * -math.log(self._dist[type])\n            else:\n                entropy += self._dist[type] * -math.log(self._dist[type], base)\n        return entropy"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maxentropy(self, base = 2):\n        if not base and self.base: base = self.base\n        if not base:\n            return math.log(len(self._dist))\n        else:\n            return math.log(len(self._dist), base)", "response": "Compute the maximum entropy of the distribution"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsee if a node communicates with another. Returns the probability of the longest path.", "response": "def communicates(self,fromstate, tostate, maxlength=999999):\n        \"\"\"See if a node communicates (directly or indirectly) with another. Returns the probability of the *shortest* path (probably, but not necessarily the highest probability)\"\"\"\n        if (not (fromstate in self.nodes)) or (not (tostate in self.nodes)):\n            return 0\n        assert (fromstate != tostate)\n\n\n        def _test(node,length,prob):\n            if length > maxlength:\n                return 0\n            if node == tostate:\n                prob *= self.edges_out[node][tostate]\n                return True\n\n            for child in self.edges_out[node].keys():\n                if not child in visited:\n                    visited.add(child)\n                    if child == tostate:\n                        return prob * self.edges_out[node][tostate]\n                    else:\n                        r = _test(child, length+1, prob * self.edges_out[node][tostate])\n                        if r:\n                            return r\n            return 0\n\n        visited = set(fromstate)\n        return _test(fromstate,1,1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the probability of the given sequence or subsequence ( if subsequence = True default ).", "response": "def p(self, sequence, subsequence=True):\n        \"\"\"Returns the probability of the given sequence or subsequence (if subsequence=True, default).\"\"\"\n        if sequence[0] != self.startstate:\n            if isinstance(sequence, tuple):\n                sequence = (self.startstate,) + sequence\n            else:\n                sequence = (self.startstate,) + tuple(sequence)\n        if self.endstate:\n            if sequence[-1] != self.endstate:\n                if isinstance(sequence, tuple):\n                    sequence = sequence + (self.endstate,)\n                else:\n                    sequence = tuple(sequence) + (self.endstate,)\n\n        prevnode = None\n        prob = 1\n        for node in sequence:\n            if prevnode:\n                try:\n                    prob *= self.edges_out[prevnode][node]\n                except:\n                    return 0\n        return prob"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process(self, sourcewords, debug=False):\n\n        if isinstance( sourcewords, list ) or isinstance( sourcewords, tuple ):\n            sourcewords_s = \" \".join(sourcewords)            \n        else:\n            sourcewords_s = sourcewords\n            sourcewords = sourcewords.split(' ')\n        \n        self.socket.sendall(sourcewords_s.encode(self.encoding) +'\\n\\0')\n        if debug: print(\"Sent:\",sourcewords_s.encode(self.encoding),file=sys.stderr)\n        \n        results = []\n        done = False\n        while not done:    \n            data = b\"\"\n            while not data:\n                buffer = self.socket.recv(self.BUFSIZE)\n                if debug: print(\"Buffer: [\"+repr(buffer)+\"]\",file=sys.stderr)\n                if buffer[-1] == '\\0':\n                    data += buffer[:-1]\n                    done = True\n                    break\n                else:\n                    data += buffer\n\n            \n            data = u(data,self.encoding)\n            if debug: print(\"Received:\",data,file=sys.stderr) \n\n            for i, line in enumerate(data.strip(' \\t\\0\\r\\n').split('\\n')):\n                if not line.strip():\n                    done = True\n                    break\n                else:\n                    cols = line.split(\" \")\n                    subwords = cols[0].lower().split(\"_\")\n                    if len(cols) > 2: #this seems a bit odd?\n                        for word in subwords: #split multiword expressions\n                            results.append( (word, cols[1], cols[2], i, len(subwords) > 1 ) ) #word, lemma, pos, index, multiword?\n\n        sourcewords = [ w.lower() for w in sourcewords ]          \n\n        alignment = []\n        for i, sourceword in enumerate(sourcewords):\n            found = False\n            best = 0  \n            distance = 999999          \n            for j, (targetword, lemma, pos, index, multiword) in enumerate(results):\n                if sourceword == targetword and abs(i-j) < distance:\n                    found = True\n                    best = j\n                    distance = abs(i-j)\n\n            if found:\n                alignment.append(results[best])\n            else:                \n                alignment.append((None,None,None,None,False)) #no alignment found\n        return alignment", "response": "Process a list of words passing it to the server and realigning the output with the original words"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes PoS features like N ( soort ev basis onz st ) into a PosAnnotation data structure based on Matje van de Camp", "response": "def parse_cgn_postag(rawtag, raisefeatureexceptions = False):\n    global subsets, constraints\n    \"\"\"decodes PoS features like \"N(soort,ev,basis,onz,stan)\" into a PosAnnotation data structure \n    based on CGN tag overview compiled by Matje van de Camp\"\"\"\n    \n    \n    begin = rawtag.find('(')\n    if rawtag[-1] == ')' and begin > 0:\n        tag = folia.PosAnnotation(None, cls=rawtag,set='http://ilk.uvt.nl/folia/sets/cgn')\n\n        \n        head = rawtag[0:begin]\n        tag.append( folia.Feature, subset='head',cls=head)\n\n        rawfeatures = rawtag[begin+1:-1].split(',')\n        for rawfeature in rawfeatures:            \n            if rawfeature:\n                found = False\n                for subset, classes in subsets.items():\n                    if rawfeature in classes:\n                        if subset in constraints:\n                            if not head in constraints[subset]:\n                                continue #constraint not met!\n                        found = True\n                        tag.append( folia.Feature, subset=subset,cls=rawfeature)\n                        break\n                if not found:\n                    print(\"\\t\\tUnknown feature value: \" + rawfeature + \" in \" + rawtag, file=stderr)\n                    if raisefeatureexceptions:\n                        raise InvalidFeatureException(\"Unknown feature value: \" + rawfeature + \" in \" + rawtag)\n                    else:    \n                        continue\n        return tag\n    else:\n        raise InvalidTagException(\"Not a valid CGN tag\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking the FoLiA version returns 1 if the document is newer than the library 0 if it is older", "response": "def checkversion(version, REFVERSION=FOLIAVERSION):\n    \"\"\"Checks FoLiA version, returns 1 if the document is newer than the library, -1 if it is older, 0 if it is equal\"\"\"\n    try:\n        for refversion, docversion in zip([int(x) for x in REFVERSION.split('.')], [int(x) for x in version.split('.')]):\n            if docversion > refversion:\n                return 1 #doc is newer than library\n            elif docversion < refversion:\n                return -1 #doc is older than library\n        return 0 #versions are equal\n    except ValueError:\n        raise ValueError(\"Unable to parse document FoLiA version, invalid syntax\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parsetime(s):\n    try:\n        fields = s.split('.')\n        subfields = fields[0].split(':')\n        H = int(subfields[0])\n        M = int(subfields[1])\n        S = int(subfields[2])\n        if len(subfields) > 3:\n            m = int(subfields[3])\n        else:\n            m = 0\n        if len(fields) > 1:\n            m = int(fields[1])\n        return (H,M,S,m)\n    except:\n        raise ValueError(\"Invalid timestamp, must be in HH:MM:SS.mmm format: \" + s)", "response": "Internal function to parse the time parses time in HH. MM. mmm format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a datetime string into a datetime object.", "response": "def parse_datetime(s): #source: http://stackoverflow.com/questions/2211362/how-to-parse-xsddatetime-format\n    \"\"\"Returns (datetime, tz offset in minutes) or (None, None).\"\"\"\n    m = re.match(r\"\"\" ^\n        (?P<year>-?[0-9]{4}) - (?P<month>[0-9]{2}) - (?P<day>[0-9]{2})\n        T (?P<hour>[0-9]{2}) : (?P<minute>[0-9]{2}) : (?P<second>[0-9]{2})\n        (?P<microsecond>\\.[0-9]{1,6})?\n        (?P<tz>\n        Z | (?P<tz_hr>[-+][0-9]{2}) : (?P<tz_min>[0-9]{2})\n        )?\n        $ \"\"\", s, re.X)\n    if m is not None:\n        values = m.groupdict()\n        #if values[\"tz\"] in (\"Z\", None):\n        #    tz = 0\n        #else:\n        #    tz = int(values[\"tz_hr\"]) * 60 + int(values[\"tz_min\"])\n        if values[\"microsecond\"] is None:\n            values[\"microsecond\"] = 0\n        else:\n            values[\"microsecond\"] = values[\"microsecond\"][1:]\n            values[\"microsecond\"] += \"0\" * (6 - len(values[\"microsecond\"]))\n        values = dict((k, int(v)) for k, v in values.items() if not k.startswith(\"tz\"))\n        try:\n            return datetime(**values) # , tz\n        except ValueError:\n            pass\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef commonancestors(Class, *args):\n\n    commonancestors = None #pylint: disable=redefined-outer-name\n    for sibling in args:\n        ancestors = list( sibling.ancestors(Class) )\n        if commonancestors is None:\n            commonancestors = copy(ancestors)\n        else:\n            removeancestors = []\n            for a in commonancestors: #pylint: disable=not-an-iterable\n                if not a in ancestors:\n                    removeancestors.append(a)\n            for a in removeancestors:\n                commonancestors.remove(a)\n    if commonancestors:\n        for commonancestor in commonancestors:\n            yield commonancestor", "response": "Generator function to find common ancestors of a particular type of elements."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a RelaxNG Schema for FoLiA. Optionally saves it to file.", "response": "def relaxng(filename=None):\n    \"\"\"Generates a RelaxNG Schema for FoLiA. Optionally saves it to file.\n\n    Args:\n        filename (str): Save the schema to the following filename\n\n    Returns:\n        lxml.ElementTree: The schema\n    \"\"\"\n    E = ElementMaker(namespace=\"http://relaxng.org/ns/structure/1.0\",nsmap={None:'http://relaxng.org/ns/structure/1.0' , 'folia': NSFOLIA, 'xml' : \"http://www.w3.org/XML/1998/namespace\"})\n    grammar = E.grammar( E.start( E.element( #FoLiA\n                E.attribute(name='id',ns=\"http://www.w3.org/XML/1998/namespace\"),\n                E.optional( E.attribute(name='version') ),\n                E.optional( E.attribute(name='generator') ),\n                E.element( #metadata\n                    E.optional(E.attribute(name='type')),\n                    E.optional(E.attribute(name='src')),\n                    E.element( E.zeroOrMore( E.choice( *relaxng_declarations() ) ) ,name='annotations'),\n                    E.zeroOrMore(\n                        E.element(E.attribute(name='id'), E.text(), name='meta'),\n                    ),\n                    E.zeroOrMore(\n                        E.ref(name=\"foreign-data\"),\n                    ),\n                    E.zeroOrMore(\n                        E.element( #submetadata\n                            E.attribute(name='id',ns=\"http://www.w3.org/XML/1998/namespace\"),\n                            E.optional(E.attribute(name='type')),\n                            E.optional(E.attribute(name='src')),\n                            E.zeroOrMore(\n                                E.element(E.attribute(name='id'), E.text(), name='meta'),\n                            ),\n                            E.zeroOrMore(\n                                E.ref(name=\"foreign-data\"),\n                            ),\n                            name=\"submetadata\"\n                        )\n                    ),\n                    #E.optional(\n                    #    E.ref(name='METATRANSCRIPT')\n                    #),\n                    name='metadata',\n                    #ns=NSFOLIA,\n                ),\n                E.interleave(\n                    E.zeroOrMore(\n                        E.ref(name='text'),\n                    ),\n                    E.zeroOrMore(\n                        E.ref(name='speech'),\n                    ),\n                ),\n                name='FoLiA',\n                ns = NSFOLIA\n            ) ),\n            #definitions needed for ForeignData (allow any content) - see http://www.microhowto.info/howto/match_arbitrary_content_using_relax_ng.html\n            E.define( E.interleave(E.zeroOrMore(E.ref(name=\"any_element\")),E.text()), name=\"any_content\"),\n            E.define( E.element(E.anyName(), E.zeroOrMore(E.ref(name=\"any_attribute\")), E.zeroOrMore(E.ref(name=\"any_content\"))), name=\"any_element\"),\n            E.define( E.attribute(E.anyName()), name=\"any_attribute\"),\n            #Definition for allowing alien-namespace attributes on any element\n            E.define( E.zeroOrMore(E.attribute(E.anyName(getattr(E,'except')(E.nsName(),E.nsName(ns=\"\"),E.nsName(ns=\"http://www.w3.org/XML/1998/namespace\"),E.nsName(ns=\"http://www.w3.org/1999/xlink\"))))), name=\"allow_foreign_attributes\"),\n            datatypeLibrary=\"http://www.w3.org/2001/XMLSchema-datatypes\",\n            )\n\n    done = {}\n    for c in globals().values():\n        if 'relaxng' in dir(c):\n            if c.relaxng and c.XMLTAG and not c.XMLTAG in done:\n                done[c.XMLTAG] = True\n                definition = c.relaxng()\n                grammar.append( definition )\n                if c.XMLTAG == 'item': #nasty backward-compatibility hack to allow deprecated listitem element (actually called item)\n                    definition_alias = c.relaxng()\n                    definition_alias.set('name','listitem')\n                    definition_alias[0].set('name','listitem')\n                    grammar.append( definition_alias )\n\n    #for e in relaxng_imdi():\n    #    grammar.append(e)\n    if filename:\n        if sys.version < '3':\n            f = io.open(filename,'w',encoding='utf-8')\n        else:\n            f = io.open(filename,'wb')\n        if LXE:\n            if sys.version < '3':\n                f.write( ElementTree.tostring(relaxng(),pretty_print=True).replace(\"</define>\",\"</define>\\n\\n\") )\n            else:\n                f.write( ElementTree.tostring(relaxng(),pretty_print=True).replace(b\"</define>\",b\"</define>\\n\\n\") )\n        else:\n            f.write( ElementTree.tostring(relaxng()).replace(\"</define>\",\"</define>\\n\\n\") )\n        f.close()\n\n    return grammar"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nobtain the description associated with the element.", "response": "def description(self):\n        \"\"\"Obtain the description associated with the element.\n\n        Raises:\n            :class:`NoSuchAnnotation` if there is no associated description.\"\"\"\n        for e in self:\n            if isinstance(e, Description):\n                return e.value\n        raise NoSuchAnnotation"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the phonetic content associated with this element.", "response": "def textcontent(self, cls='current', correctionhandling=CorrectionHandling.CURRENT):\n        \"\"\"Get the text content explicitly associated with this element (of the specified class).\n\n        Unlike :meth:`text`, this method does not recurse into child elements (with the sole exception of the Correction/New element), and it returns the :class:`TextContent` instance rather than the actual text!\n\n        Parameters:\n            cls (str): The class of the text content to obtain, defaults to ``current``.\n            correctionhandling: Specifies what content to retrieve when corrections are encountered. The default is ``CorrectionHandling.CURRENT``, which will retrieve the corrected/current content. You can set this to ``CorrectionHandling.ORIGINAL`` if you want the content prior to correction, and ``CorrectionHandling.EITHER`` if you don't care.\n\n        Returns:\n            The phonetic content (:class:`TextContent`)\n\n        Raises:\n            :class:`NoSuchText` if there is no text content for the element\n\n        See also:\n            :meth:`text`\n            :meth:`phoncontent`\n            :meth:`phon`\n        \"\"\"\n        if not self.PRINTABLE: #only printable elements can hold text\n            raise NoSuchText\n\n\n        #Find explicit text content (same class)\n        for e in self:\n            if isinstance(e, TextContent):\n                if cls is None or e.cls == cls:\n                    return e\n            elif isinstance(e, Correction):\n                try:\n                    return e.textcontent(cls, correctionhandling)\n                except NoSuchText:\n                    pass\n        raise NoSuchText"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef findcorrectionhandling(self, cls):\n        if cls == \"current\":\n            return CorrectionHandling.CURRENT\n        elif cls == \"original\":\n            return CorrectionHandling.ORIGINAL #backward compatibility\n        else:\n            correctionhandling = None\n            #but any other class may be anything\n            #Do we have corrections at all? otherwise no need to bother\n            for correction in self.select(Correction):\n                #yes, in which branch is the text class found?\n                found = False\n                hastext = False\n                if correction.hasnew():\n                    found = True\n                    doublecorrection = correction.new().count(Correction) > 0\n                    if doublecorrection: return None #skipping text validation, correction is too complex (nested) to handle for now\n                    for t in  correction.new().select(TextContent):\n                        hastext = True\n                        if t.cls == cls:\n                            if correctionhandling is not None and correctionhandling != CorrectionHandling.CURRENT:\n                                return None #inconsistent\n                            else:\n                                correctionhandling = CorrectionHandling.CURRENT\n                            break\n                elif correction.hascurrent():\n                    found = True\n                    doublecorrection = correction.current().count(Correction) > 0\n                    if doublecorrection: return None #skipping text validation, correction is too complex (nested) to handle for now\n                    for t in  correction.current().select(TextContent):\n                        hastext = True\n                        if t.cls == cls:\n                            if correctionhandling is not None and correctionhandling != CorrectionHandling.CURRENT:\n                                return None #inconsistent\n                            else:\n                                correctionhandling = CorrectionHandling.CURRENT\n                            break\n                if correction.hasoriginal():\n                    found = True\n                    doublecorrection = correction.original().count(Correction) > 0\n                    if doublecorrection: return None #skipping text validation, correction is too complex (nested) to handle for now\n                    for t in  correction.original().select(TextContent):\n                        hastext = True\n                        if t.cls == cls:\n                            if correctionhandling is not None and correctionhandling != CorrectionHandling.ORIGINAL:\n                                return None #inconsistent\n                            else:\n                                correctionhandling = CorrectionHandling.ORIGINAL\n                            break\n            if correctionhandling is None:\n                #well, we couldn't find our textclass in any correction, just fall back to current and let text validation fail if needed\n                return CorrectionHandling.CURRENT", "response": "Find the proper correctionhandling given a textclass by looking in the underlying corrections where it is reused"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning text validation on this element.", "response": "def textvalidation(self, warnonly=None):\n        \"\"\"Run text validation on this element. Checks whether any text redundancy is consistent and whether offsets are valid.\n\n        Parameters:\n            warnonly (bool): Warn only (True) or raise exceptions (False). If set to None then this value will be determined based on the document's FoLiA version (Warn only before FoLiA v1.5)\n\n        Returns:\n            bool\n        \"\"\"\n\n        if warnonly is None and self.doc and self.doc.version:\n            warnonly = (checkversion(self.doc.version, '1.5.0') < 0) #warn only for documents older than FoLiA v1.5\n        valid = True\n        for cls in self.doc.textclasses:\n            if self.hastext(cls, strict=True) and not isinstance(self, (Linebreak, Whitespace)):\n                if self.doc and self.doc.debug: print(\"[PyNLPl FoLiA DEBUG] Text validation on \" + repr(self),file=stderr)\n                correctionhandling = self.findcorrectionhandling(cls)\n                if correctionhandling is None:\n                    #skipping text validation, correction is too complex (nested) to handle for now; just assume valid (benefit of the doubt)\n                    if self.doc and self.doc.debug: print(\"[PyNLPl FoLiA DEBUG] SKIPPING Text validation on \" + repr(self) + \", too complex to handle (nested corrections or inconsistent use)\",file=stderr)\n                    return True #just assume it's valid then\n\n                strictnormtext = self.text(cls,retaintokenisation=False,strict=True, normalize_spaces=True)\n                deepnormtext = self.text(cls,retaintokenisation=False,strict=False, normalize_spaces=True)\n                if strictnormtext != deepnormtext:\n                    valid = False\n                    deviation = 0\n                    for i, (c1,c2) in enumerate(zip(strictnormtext,deepnormtext)):\n                        if c1 != c2:\n                            deviation = i\n                            break\n                    msg = \"Text for \" + self.__class__.__name__ + \", ID \" + str(self.id) + \", class \" + cls  + \", is inconsistent: EXPECTED (after normalization) *****>\\n\" + deepnormtext + \"\\n****> BUT FOUND (after normalization) ****>\\n\" + strictnormtext + \"\\n******* DEVIATION POINT: \" + strictnormtext[max(0,deviation-10):deviation] + \"<*HERE*>\" + strictnormtext[deviation:deviation+10]\n                    if warnonly:\n                        print(\"TEXT VALIDATION ERROR: \" + msg,file=sys.stderr)\n                    else:\n                        raise InconsistentText(msg)\n\n                #validate offsets\n                tc = self.textcontent(cls)\n                if tc.offset is not None:\n                    #we can't validate the reference of this element yet since it may point to higher level elements still being created!! we store it in a buffer that will\n                    #be processed by pendingvalidation() after parsing and prior to serialisation\n                    if self.doc and self.doc.debug: print(\"[PyNLPl FoLiA DEBUG] Queing element for later offset validation: \" + repr(self),file=stderr)\n                    self.doc.offsetvalidationbuffer.append( (self, cls) )\n        return valid"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the text associated with this element.", "response": "def text(self, cls='current', retaintokenisation=False, previousdelimiter=\"\",strict=False, correctionhandling=CorrectionHandling.CURRENT, normalize_spaces=False):\n        \"\"\"Get the text associated with this element (of the specified class)\n\n        The text will be constructed from child-elements whereever possible, as they are more specific.\n        If no text can be obtained from the children and the element has itself text associated with\n        it, then that will be used.\n\n        Parameters:\n            cls (str): The class of the text content to obtain, defaults to ``current``.\n            retaintokenisation (bool): If set, the space attribute on words will be ignored, otherwise it will be adhered to and text will be detokenised as much as possible. Defaults to ``False``.\n            previousdelimiter (str): Can be set to a delimiter that was last outputed, useful when chaining calls to :meth:`text`. Defaults to an empty string.\n            strict (bool):  Set this iif you are strictly interested in the text explicitly associated with the element, without recursing into children. Defaults to ``False``.\n            correctionhandling: Specifies what text to retrieve when corrections are encountered. The default is ``CorrectionHandling.CURRENT``, which will retrieve the corrected/current text. You can set this to ``CorrectionHandling.ORIGINAL`` if you want the text prior to correction, and ``CorrectionHandling.EITHER`` if you don't care.\n            normalize_spaces (bool): Return the text with multiple spaces, linebreaks, tabs normalized to single spaces\n\n        Example::\n\n            word.text()\n\n        Returns:\n            The text of the element (``unicode`` instance in Python 2, ``str`` in Python 3)\n\n        Raises:\n            :class:`NoSuchText`: if no text is found at all.\n        \"\"\"\n\n        if strict:\n            return self.textcontent(cls, correctionhandling).text(normalize_spaces=normalize_spaces)\n\n        if self.TEXTCONTAINER:\n            s = \"\"\n            for e in self:\n                if isstring(e):\n                    s += e\n                elif e.PRINTABLE:\n                    if s: s += e.TEXTDELIMITER #for AbstractMarkup, will usually be \"\"\n                    s += e.text()\n            if normalize_spaces:\n                return norm_spaces(s)\n            else:\n                return s\n        elif not self.PRINTABLE: #only printable elements can hold text\n            raise NoSuchText\n        else:\n            #Get text from children first\n            delimiter = \"\"\n            s = \"\"\n            for e in self:\n                #was: e.PRINTABLE and not isinstance(e, TextContent) and not isinstance(e, String):\n                if isinstance(e, (AbstractStructureElement, Correction, AbstractSpanAnnotation)):   #AbstractSpanAnnotation is needed when requesting text() on nested span annotations\n                    try:\n                        s += e.text(cls,retaintokenisation, delimiter,False,correctionhandling)\n\n                        #delimiter will be buffered and only printed upon next iteration, this prevents the delimiter being outputted at the end of a sequence and to be compounded with other delimiters\n                        delimiter = e.gettextdelimiter(retaintokenisation)\n                    except NoSuchText:\n                        #No text, that's okay, just continue\n                        continue\n\n            if not s and self.hastext(cls, correctionhandling):\n                s = self.textcontent(cls, correctionhandling).text()\n\n            if s and previousdelimiter:\n                s = previousdelimiter + s\n            if s:\n                if normalize_spaces:\n                    return norm_spaces(s)\n                else:\n                    return s\n            else:\n                #No text found at all :`(\n                raise NoSuchText"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the phonetic content explicitly associated with this element.", "response": "def phoncontent(self, cls='current', correctionhandling=CorrectionHandling.CURRENT):\n        \"\"\"Get the phonetic content explicitly associated with this element (of the specified class).\n\n        Unlike :meth:`phon`, this method does not recurse into child elements (with the sole exception of the Correction/New element), and it returns the PhonContent instance rather than the actual text!\n\n        Parameters:\n            cls (str): The class of the phonetic content to obtain, defaults to ``current``.\n            correctionhandling: Specifies what content to retrieve when corrections are encountered. The default is ``CorrectionHandling.CURRENT``, which will retrieve the corrected/current content. You can set this to ``CorrectionHandling.ORIGINAL`` if you want the content prior to correction, and ``CorrectionHandling.EITHER`` if you don't care.\n\n        Returns:\n            The phonetic content (:class:`PhonContent`)\n\n        Raises:\n            :class:`NoSuchPhon` if there is no phonetic content for the element\n\n        See also:\n            :meth:`phon`\n            :meth:`textcontent`\n            :meth:`text`\n        \"\"\"\n        if not self.SPEAKABLE: #only printable elements can hold text\n            raise NoSuchPhon\n\n\n        #Find explicit text content (same class)\n        for e in self:\n            if isinstance(e, PhonContent):\n                if cls is None or e.cls == cls:\n                    return e\n            elif isinstance(e, Correction):\n                try:\n                    return e.phoncontent(cls, correctionhandling)\n                except NoSuchPhon:\n                    pass\n        raise NoSuchPhon"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the URL of the audio or video file associated with the element.", "response": "def speech_src(self):\n        \"\"\"Retrieves the URL/filename of the audio or video file associated with the element.\n\n        The source is inherited from ancestor elements if none is specified. For this reason, always use this method rather than access the ``src`` attribute directly.\n\n        Returns:\n            str or None if not found\n        \"\"\"\n        if self.src:\n            return self.src\n        elif self.parent:\n            return self.parent.speech_src()\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef speech_speaker(self):\n        if self.speaker:\n            return self.speaker\n        elif self.parent:\n            return self.parent.speech_speaker()\n        else:\n            return None", "response": "Retrieves the speaker of the audio or video file associated with the element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef phon(self, cls='current', previousdelimiter=\"\", strict=False,correctionhandling=CorrectionHandling.CURRENT):\n\n        if strict:\n            return self.phoncontent(cls,correctionhandling).phon()\n\n        if self.PHONCONTAINER:\n            s = \"\"\n            for e in self:\n                if isstring(e):\n                    s += e\n                else:\n                    try:\n                        if s: s += e.TEXTDELIMITER #We use TEXTDELIMITER for phon too\n                    except AttributeError:\n                        pass\n                    s += e.phon()\n            return s\n        elif not self.SPEAKABLE: #only readable elements can hold phonetic content\n            raise NoSuchPhon\n        else:\n            #Get text from children first\n            delimiter = \"\"\n            s = \"\"\n            for e in self:\n                if e.SPEAKABLE and not isinstance(e, PhonContent) and not isinstance(e,String):\n                    try:\n                        s += e.phon(cls, delimiter,False,correctionhandling)\n\n                        #delimiter will be buffered and only printed upon next iteration, this prevents the delimiter being outputted at the end of a sequence and to be compounded with other delimiters\n                        delimiter = e.gettextdelimiter() #We use TEXTDELIMITER for phon too\n                    except NoSuchPhon:\n                        #No text, that's okay, just continue\n                        continue\n\n            if not s and self.hasphon(cls):\n                s = self.phoncontent(cls,correctionhandling).phon()\n\n            if s and previousdelimiter:\n                return previousdelimiter + s\n            elif s:\n                return s\n            else:\n                #No text found at all :`(\n                raise NoSuchPhon", "response": "Returns the phonetic representation of this element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nobtaining the feature class value of the specific subset.", "response": "def feat(self,subset):\n        \"\"\"Obtain the feature class value of the specific subset.\n\n        If a feature occurs multiple times, the values will be returned in a list.\n\n        Example::\n\n            sense = word.annotation(folia.Sense)\n            synset = sense.feat('synset')\n\n        Returns:\n            str or list\n        \"\"\"\n        r = None\n        for f in self:\n            if isinstance(f, Feature) and f.subset == subset:\n                if r: #support for multiclass features\n                    if isinstance(r,list):\n                        r.append(f.cls)\n                    else:\n                        r = [r, f.cls]\n                else:\n                    r = f.cls\n        if r is None:\n            raise NoSuchAnnotation\n        else:\n            return r"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a deep copy of this element and all its children.", "response": "def copy(self, newdoc=None, idsuffix=\"\"):\n        \"\"\"Make a deep copy of this element and all its children.\n\n        Parameters:\n            newdoc (:class:`Document`): The document the copy should be associated with.\n            idsuffix (str or bool): If set to a string, the ID of the copy will be append with this (prevents duplicate IDs when making copies for the same document). If set to ``True``, a random suffix will be generated.\n\n        Returns:\n            a copy of the element\n        \"\"\"\n        if idsuffix is True: idsuffix = \".copy.\" + \"%08x\" % random.getrandbits(32) #random 32-bit hash for each copy, same one will be reused for all children\n        c = deepcopy(self)\n        if idsuffix:\n            c.addidsuffix(idsuffix)\n        c.setparents()\n        c.setdoc(newdoc)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copychildren(self, newdoc=None, idsuffix=\"\"):\n        if idsuffix is True: idsuffix = \".copy.\" + \"%08x\" % random.getrandbits(32) #random 32-bit hash for each copy, same one will be reused for all children\n        for c in self:\n            if isinstance(c, AbstractElement):\n                yield c.copy(newdoc,idsuffix)", "response": "Generator creating a deep copy of the children of this element."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef addidsuffix(self, idsuffix, recursive = True):\n        if self.id: self.id += idsuffix\n        if recursive:\n            for e in self:\n                try:\n                    e.addidsuffix(idsuffix, recursive)\n                except Exception:\n                    pass", "response": "Appends a suffix to this element s ID and optionally to all child IDs as well."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setparents(self):\n        for c in self:\n            if isinstance(c, AbstractElement):\n                c.parent = self\n                c.setparents()", "response": "Correct all parent relations for elements within the scop."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets a different document. Usually no need to call this directly by calling copy.", "response": "def setdoc(self,newdoc):\n        \"\"\"Set a different document. Usually no need to call this directly, invoked implicitly by :meth:`copy`\"\"\"\n        self.doc = newdoc\n        if self.doc and self.id:\n            self.doc.index[self.id] = self\n        for c in self:\n            if isinstance(c, AbstractElement):\n                c.setdoc(newdoc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if this element has text of the specified class.", "response": "def hastext(self,cls='current',strict=True, correctionhandling=CorrectionHandling.CURRENT): #pylint: disable=too-many-return-statements\n        \"\"\"Does this element have text (of the specified class)\n\n        By default, and unlike :meth:`text`, this checks strictly, i.e. the element itself must have the text and it is not inherited from its children.\n\n        Parameters:\n            cls (str): The class of the text content to obtain, defaults to ``current``.\n            strict (bool):  Set this if you are strictly interested in the text explicitly associated with the element, without recursing into children. Defaults to ``True``.\n            correctionhandling: Specifies what text to check for when corrections are encountered. The default is ``CorrectionHandling.CURRENT``, which will retrieve the corrected/current text. You can set this to ``CorrectionHandling.ORIGINAL`` if you want the text prior to correction, and ``CorrectionHandling.EITHER`` if you don't care.\n\n        Returns:\n            bool\n        \"\"\"\n        if not self.PRINTABLE: #only printable elements can hold text\n            return False\n        elif self.TEXTCONTAINER:\n            return True\n        else:\n            try:\n                if strict:\n                    self.textcontent(cls, correctionhandling) #will raise NoSuchTextException when not found\n                    return True\n                else:\n                    #Check children\n                    for e in self:\n                        if e.PRINTABLE and not isinstance(e, TextContent):\n                            if e.hastext(cls, strict, correctionhandling):\n                                return True\n\n                    self.textcontent(cls, correctionhandling)  #will raise NoSuchTextException when not found\n                    return True\n            except NoSuchText:\n                return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if this element has phonetic content of the specified class.", "response": "def hasphon(self,cls='current',strict=True,correctionhandling=CorrectionHandling.CURRENT): #pylint: disable=too-many-return-statements\n        \"\"\"Does this element have phonetic content (of the specified class)\n\n        By default, and unlike :meth:`phon`, this checks strictly, i.e. the element itself must have the phonetic content and it is not inherited from its children.\n\n        Parameters:\n            cls (str): The class of the phonetic content to obtain, defaults to ``current``.\n            strict (bool):  Set this if you are strictly interested in the phonetic content explicitly associated with the element, without recursing into children. Defaults to ``True``.\n            correctionhandling: Specifies what phonetic content to check for when corrections are encountered. The default is ``CorrectionHandling.CURRENT``, which will retrieve the corrected/current phonetic content. You can set this to ``CorrectionHandling.ORIGINAL`` if you want the phonetic content prior to correction, and ``CorrectionHandling.EITHER`` if you don't care.\n\n        Returns:\n            bool\n        \"\"\"\n        if not self.SPEAKABLE: #only printable elements can hold text\n            return False\n        elif self.PHONCONTAINER:\n            return True\n        else:\n            try:\n                if strict:\n                    self.phoncontent(cls, correctionhandling)\n                    return True\n                else:\n                    #Check children\n                    for e in self:\n                        if e.SPEAKABLE and not isinstance(e, PhonContent):\n                            if e.hasphon(cls, strict, correctionhandling):\n                                return True\n\n                    self.phoncontent(cls)  #will raise NoSuchTextException when not found\n                    return True\n            except NoSuchPhon:\n                return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the text for this element.", "response": "def settext(self, text, cls='current'):\n        \"\"\"Set the text for this element.\n\n        Arguments:\n            text (str): The text\n            cls (str): The class of the text, defaults to ``current`` (leave this unless you know what you are doing). There may be only one text content element of each class associated with the element.\n        \"\"\"\n        self.replace(TextContent, value=text, cls=cls)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nassociates a document with this element.", "response": "def setdocument(self, doc):\n        \"\"\"Associate a document with this element.\n\n        Arguments:\n            doc (:class:`Document`): A document\n\n        Each element must be associated with a FoLiA document.\n        \"\"\"\n        assert isinstance(doc, Document)\n\n        if not self.doc:\n            self.doc = doc\n            if self.id:\n                if self.id in doc:\n                    raise DuplicateIDError(self.id)\n                else:\n                    self.doc.index[id] = self\n\n        for e in self: #recursive for all children\n            if isinstance(e,AbstractElement): e.setdocument(doc)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest whether a new element of this class can be added to the parent.", "response": "def addable(Class, parent, set=None, raiseexceptions=True):\n        \"\"\"Tests whether a new element of this class can be added to the parent.\n\n        This method is mostly for internal use.\n        This will use the ``OCCURRENCES`` property, but may be overidden by subclasses for more customised behaviour.\n\n        Parameters:\n            parent (:class:`AbstractElement`): The element that is being added to\n            set (str or None): The set\n            raiseexceptions (bool): Raise an exception if the element can't be added?\n\n        Returns:\n            bool\n\n        Raises:\n            ValueError\n         \"\"\"\n\n\n        if not parent.__class__.accepts(Class, raiseexceptions, parent):\n            return False\n\n        if Class.OCCURRENCES > 0:\n            #check if the parent doesn't have too many already\n            count = parent.count(Class,None,True,[True, AbstractStructureElement]) #never descend into embedded structure annotatioton\n            if count >= Class.OCCURRENCES:\n                if raiseexceptions:\n                    if parent.id:\n                        extra = ' (id=' + parent.id + ')'\n                    else:\n                        extra = ''\n                    raise DuplicateAnnotationError(\"Unable to add another object of type \" + Class.__name__ + \" to \" + parent.__class__.__name__ + \" \" + extra + \". There are already \" + str(count) + \" instances of this class, which is the maximum.\")\n                else:\n                    return False\n\n        if Class.OCCURRENCES_PER_SET > 0 and set and Class.REQUIRED_ATTRIBS and Attrib.CLASS in Class.REQUIRED_ATTRIBS:\n            count = parent.count(Class,set,True, [True, AbstractStructureElement])\n            if count >= Class.OCCURRENCES_PER_SET:\n                if raiseexceptions:\n                    if parent.id:\n                        extra = ' (id=' + parent.id + ')'\n                    else:\n                        extra = ''\n                    raise DuplicateAnnotationError(\"Unable to add another object of set \" + set + \" and type \" + Class.__name__ + \" to \" + parent.__class__.__name__ + \" \" + extra + \". There are already \" + str(count) + \" instances of this class, which is the maximum for the set.\")\n                else:\n                    return False\n\n\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deepvalidation(self):\n        if self.doc and self.doc.deepvalidation and self.set and self.set[0] != '_':\n            try:\n                self.doc.setdefinitions[self.set].testclass(self.cls)\n            except KeyError:\n                if self.cls and not self.doc.allowadhocsets:\n                    raise DeepValidationError(\"Set definition \" + self.set + \" for \" + self.XMLTAG + \" not loaded!\")\n            except DeepValidationError as e:\n                errormsg =  str(e) + \" (in set \" + self.set+\" for \" + self.XMLTAG\n                if self.id:\n                    errormsg += \" with ID \" + self.id\n                errormsg += \")\"\n                raise DeepValidationError(errormsg)", "response": "Perform deep validation of this element."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nappends a child element like append but replaces any existing child element of the same type and set.", "response": "def replace(self, child, *args, **kwargs):\n        \"\"\"Appends a child element like ``append()``, but replaces any existing child element of the same type and set. If no such child element exists, this will act the same as append()\n\n        Keyword arguments:\n            alternative (bool): If set to True, the *replaced* element will be made into an alternative. Simply use :meth:`AbstractElement.append` if you want the added element\n            to be an alternative.\n\n        See :meth:`AbstractElement.append` for more information and all parameters.\n        \"\"\"\n\n        if 'set' in kwargs:\n            set = kwargs['set']\n            del kwargs['set']\n        else:\n            try:\n                set = child.set\n            except AttributeError:\n                set = None\n\n        if inspect.isclass(child):\n            Class = child\n            replace = Class.findreplaceables(self, set, **kwargs)\n        elif (self.TEXTCONTAINER or self.PHONCONTAINER) and isstring(child):\n            #replace will replace ALL text content, removing text markup along the way!\n            self.data = []\n            return self.append(child, *args,**kwargs)\n        else:\n            Class = child.__class__\n            kwargs['instance'] = child\n            replace = Class.findreplaceables(self,set,**kwargs)\n            del kwargs['instance']\n\n        kwargs['set'] = set #was deleted temporarily for findreplaceables\n\n        if len(replace) == 0:\n            #nothing to replace, simply call append\n            if 'alternative' in kwargs:\n                del kwargs['alternative'] #has other meaning in append()\n            return self.append(child, *args, **kwargs)\n        elif len(replace) > 1:\n            raise Exception(\"Unable to replace. Multiple candidates found, unable to choose.\")\n        elif len(replace) == 1:\n            if 'alternative' in kwargs and kwargs['alternative']:\n                #old version becomes alternative\n                if replace[0] in self.data:\n                    self.data.remove(replace[0])\n                alt = self.append(Alternative)\n                alt.append(replace[0])\n                del kwargs['alternative'] #has other meaning in append()\n            else:\n                #remove old version competely\n                self.remove(replace[0])\n            e = self.append(child, *args, **kwargs)\n            self.updatetext()\n            return e"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ancestor(self, *Classes):\n        for e in self.ancestors(tuple(Classes)):\n            return e\n        raise NoSuchAnnotation", "response": "Find the most immediate ancestor of the specified type. Multiple classes may be specified."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xml(self, attribs = None,elements = None, skipchildren = False):\n        E = ElementMaker(namespace=NSFOLIA,nsmap={None: NSFOLIA, 'xml' : \"http://www.w3.org/XML/1998/namespace\"})\n\n        if not attribs: attribs = {}\n        if not elements: elements = []\n\n        if self.id:\n            attribs['{http://www.w3.org/XML/1998/namespace}id'] = self.id\n\n        #Some attributes only need to be added if they are not the same as what's already set in the declaration\n        if not isinstance(self, AbstractAnnotationLayer):\n            if '{' + NSFOLIA + '}set' not in attribs: #do not override if overloaded function already set it\n                try:\n                    if self.set:\n                        if not self.ANNOTATIONTYPE in self.doc.annotationdefaults or len(self.doc.annotationdefaults[self.ANNOTATIONTYPE]) != 1 or list(self.doc.annotationdefaults[self.ANNOTATIONTYPE].keys())[0] != self.set:\n                            if self.set != None:\n                                if self.ANNOTATIONTYPE in self.doc.set_alias and self.set in self.doc.set_alias[self.ANNOTATIONTYPE]:\n                                    attribs['{' + NSFOLIA + '}set'] = self.doc.set_alias[self.ANNOTATIONTYPE][self.set] #use alias instead\n                                else:\n                                    attribs['{' + NSFOLIA + '}set'] = self.set\n                except AttributeError:\n                    pass\n\n        if '{' + NSFOLIA + '}class' not in attribs: #do not override if caller already set it\n            try:\n                if self.cls:\n                    attribs['{' + NSFOLIA + '}class'] = self.cls\n            except AttributeError:\n                pass\n\n        if '{' + NSFOLIA + '}annotator' not in attribs: #do not override if caller already set it\n            try:\n                if self.annotator and ((not (self.ANNOTATIONTYPE in self.doc.annotationdefaults)) or (not ( 'annotator' in self.doc.annotationdefaults[self.ANNOTATIONTYPE][self.set])) or (self.annotator != self.doc.annotationdefaults[self.ANNOTATIONTYPE][self.set]['annotator'])):\n                    attribs['{' + NSFOLIA + '}annotator'] = self.annotator\n                if self.annotatortype and ((not (self.ANNOTATIONTYPE in self.doc.annotationdefaults)) or (not ('annotatortype' in self.doc.annotationdefaults[self.ANNOTATIONTYPE][self.set])) or (self.annotatortype != self.doc.annotationdefaults[self.ANNOTATIONTYPE][self.set]['annotatortype'])):\n                    if self.annotatortype == AnnotatorType.AUTO:\n                        attribs['{' + NSFOLIA + '}annotatortype'] = 'auto'\n                    elif self.annotatortype == AnnotatorType.MANUAL:\n                        attribs['{' + NSFOLIA + '}annotatortype'] = 'manual'\n            except AttributeError:\n                pass\n\n        if '{' + NSFOLIA + '}confidence' not in attribs: #do not override if caller already set it\n            if self.confidence:\n                attribs['{' + NSFOLIA + '}confidence'] = str(self.confidence)\n\n        if '{' + NSFOLIA + '}n' not in attribs: #do not override if caller already set it\n            if self.n:\n                attribs['{' + NSFOLIA + '}n'] = str(self.n)\n\n        if '{' + NSFOLIA + '}auth' not in attribs: #do not override if caller already set it\n            try:\n                if not self.AUTH or not self.auth: #(former is static, latter isn't)\n                    attribs['{' + NSFOLIA + '}auth'] = 'no'\n            except AttributeError:\n                pass\n\n        if '{' + NSFOLIA + '}datetime' not in attribs: #do not override if caller already set it\n            if self.datetime and ((not (self.ANNOTATIONTYPE in self.doc.annotationdefaults)) or (not ( 'datetime' in self.doc.annotationdefaults[self.ANNOTATIONTYPE][self.set])) or (self.datetime != self.doc.annotationdefaults[self.ANNOTATIONTYPE][self.set]['datetime'])):\n                attribs['{' + NSFOLIA + '}datetime'] = self.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n        if '{' + NSFOLIA + '}src' not in attribs: #do not override if caller already set it\n            if self.src:\n                attribs['{' + NSFOLIA + '}src'] = self.src\n\n        if '{' + NSFOLIA + '}speaker' not in attribs: #do not override if caller already set it\n            if self.speaker:\n                attribs['{' + NSFOLIA + '}speaker'] = self.speaker\n\n        if '{' + NSFOLIA + '}begintime' not in attribs: #do not override if caller already set it\n            if self.begintime:\n                attribs['{' + NSFOLIA + '}begintime'] = \"%02d:%02d:%02d.%03d\" % self.begintime\n\n        if '{' + NSFOLIA + '}endtime' not in attribs: #do not override if caller already set it\n            if self.endtime:\n                attribs['{' + NSFOLIA + '}endtime'] = \"%02d:%02d:%02d.%03d\" % self.endtime\n\n        if '{' + NSFOLIA + '}textclass' not in attribs: #do not override if caller already set it\n            if self.textclass and self.textclass != \"current\":\n                attribs['{' + NSFOLIA + '}textclass'] = self.textclass\n\n        if '{' + NSFOLIA + '}metadata' not in attribs: #do not override if caller already set it\n            if self.metadata:\n                attribs['{' + NSFOLIA + '}metadata'] = self.metadata\n\n        if self.XLINK:\n            if self.href:\n                attribs['{http://www.w3.org/1999/xlink}href'] = self.href\n                if not self.xlinktype:\n                    attribs['{http://www.w3.org/1999/xlink}type'] = \"simple\"\n            if self.xlinktype:\n                attribs['{http://www.w3.org/1999/xlink}type'] = self.xlinktype\n            if self.xlinklabel:\n                attribs['{http://www.w3.org/1999/xlink}label'] = self.xlinklabel\n            if self.xlinkrole:\n                attribs['{http://www.w3.org/1999/xlink}role'] = self.xlinkrole\n            if self.xlinkshow:\n                attribs['{http://www.w3.org/1999/xlink}show'] = self.xlinkshow\n            if self.xlinktitle:\n                attribs['{http://www.w3.org/1999/xlink}title'] = self.xlinktitle\n\n        omitchildren =  []\n\n        #Are there predetermined Features in ACCEPTED_DATA?\n        for c in self.ACCEPTED_DATA:\n            if issubclass(c, Feature) and c.SUBSET:\n                #Do we have any of those?\n                for c2 in self.data:\n                    if c2.__class__ is c and c.SUBSET == c2.SUBSET and c2.cls:\n                        #Yes, serialize them as attributes\n                        attribs[c2.SUBSET] = c2.cls\n                        omitchildren.append(c2) #and skip them as elements\n                        break #only one\n\n        e  = makeelement(E, '{' + NSFOLIA + '}' + self.XMLTAG, **attribs)\n\n\n\n        if not skipchildren and self.data:\n            #append children,\n            # we want make sure that text elements are in the right order, 'current' class first\n            # so we first put them in  a list\n            textelements = []\n            otherelements = []\n            for child in self:\n                if isinstance(child, TextContent):\n                    if child.cls == 'current':\n                        textelements.insert(0, child)\n                    else:\n                        textelements.append(child)\n                elif not child in omitchildren:\n                    otherelements.append(child)\n            for child in textelements+otherelements:\n                if (self.TEXTCONTAINER or self.PHONCONTAINER) and isstring(child):\n                    if len(e) == 0:\n                        if e.text:\n                            e.text += child\n                        else:\n                            e.text = child\n                    else:\n                        #add to tail of last child\n                        if e[-1].tail:\n                            e[-1].tail += child\n                        else:\n                            e[-1].tail = child\n\n                else:\n                    xml = child.xml() #may return None in rare occassions, meaning we wan to skip\n                    if not xml is None:\n                        e.append(xml)\n\n        if elements: #extra elements\n            for e2 in elements:\n                if isinstance(e2, str) or (sys.version < '3' and isinstance(e2, unicode)):\n                    if e.text is None:\n                        e.text = e2\n                    else:\n                        e.text += e2\n                else:\n                    e.append(e2)\n        return e", "response": "Serialises the FoLiA element and all its contents to XML."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserialising the FoLiA element and all its contents to a Python dictionary suitable for serialisation to JSON.", "response": "def json(self, attribs=None, recurse=True, ignorelist=False):\n        \"\"\"Serialises the FoLiA element and all its contents to a Python dictionary suitable for serialisation to JSON.\n\n        Example::\n\n            import json\n            json.dumps(word.json())\n\n        Returns:\n            dict\n        \"\"\"\n        jsonnode = {}\n\n        jsonnode['type'] = self.XMLTAG\n        if self.id:\n            jsonnode['id'] = self.id\n        if self.set:\n            jsonnode['set'] = self.set\n        if self.cls:\n            jsonnode['class'] = self.cls\n        if self.annotator:\n            jsonnode['annotator'] = self.annotator\n        if self.annotatortype:\n            if self.annotatortype == AnnotatorType.AUTO:\n                jsonnode['annotatortype'] = \"auto\"\n            elif self.annotatortype == AnnotatorType.MANUAL:\n                jsonnode['annotatortype'] = \"manual\"\n        if self.confidence is not None:\n            jsonnode['confidence'] = self.confidence\n        if self.n:\n            jsonnode['n'] = self.n\n        if self.auth:\n            jsonnode['auth'] = self.auth\n        if self.datetime:\n            jsonnode['datetime'] = self.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n        if recurse: #pylint: disable=too-many-nested-blocks\n            jsonnode['children'] = []\n            if self.TEXTCONTAINER:\n                jsonnode['text'] = self.text()\n            if self.PHONCONTAINER:\n                jsonnode['phon'] = self.phon()\n            for child in self:\n                if self.TEXTCONTAINER and isstring(child):\n                    jsonnode['children'].append(child)\n                elif not self.PHONCONTAINER:\n                    #check ignore list\n                    ignore = False\n                    if ignorelist:\n                        for e in ignorelist:\n                            if isinstance(child,e):\n                                ignore = True\n                                break\n                    if not ignore:\n                        jsonnode['children'].append(child.json(attribs,recurse,ignorelist))\n\n        if attribs:\n            for attrib in attribs:\n                jsonnode[attrib] = attribs\n\n        return jsonnode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef xmlstring(self, pretty_print=False):\n        s = ElementTree.tostring(self.xml(), xml_declaration=False, pretty_print=pretty_print, encoding='utf-8')\n        if sys.version < '3':\n            if isinstance(s, str):\n                s = unicode(s,'utf-8') #pylint: disable=undefined-variable\n        else:\n            if isinstance(s,bytes):\n                s = str(s,'utf-8')\n\n        s = s.replace('ns0:','') #ugly patch to get rid of namespace prefix\n        s = s.replace(':ns0','')\n        return s", "response": "Serialises this FoLiA element and all its contents to XML."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nselecting child elements of the specified class.", "response": "def select(self, Class, set=None, recursive=True,  ignore=True, node=None): #pylint: disable=bad-classmethod-argument,redefined-builtin\n        \"\"\"Select child elements of the specified class.\n\n        A further restriction can be made based on set.\n\n        Arguments:\n            Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\n            Set (str): The set to match against, only elements pertaining to this set will be returned. If set to None (default), all elements regardless of set will be returned.\n            recursive (bool): Select recursively? Descending into child elements? Defaults to ``True``.\n            ignore: A list of Classes to ignore, if set to ``True`` instead of a list, all non-authoritative elements will be skipped (this is the default behaviour and corresponds to the following elements: :class:`Alternative`, :class:`AlternativeLayer`, :class:`Suggestion`, and :class:`folia.Original`. These elements and those contained within are never *authorative*. You may also include the boolean True as a member of a list, if you want to skip additional tags along the predefined non-authoritative ones.\n            * ``node``: Reserved for internal usage, used in recursion.\n\n        Yields:\n            Elements (instances derived from :class:`AbstractElement`)\n\n        Example::\n\n            for sense in text.select(folia.Sense, 'cornetto', True, [folia.Original, folia.Suggestion, folia.Alternative] ):\n                ..\n\n        \"\"\"\n\n        #if ignorelist is True:\n        #    ignorelist = default_ignore\n\n        if not node:\n            node = self\n        for e in self.data: #pylint: disable=too-many-nested-blocks\n            if (not self.TEXTCONTAINER and not self.PHONCONTAINER) or isinstance(e, AbstractElement):\n                if ignore is True:\n                    try:\n                        if not e.auth:\n                            continue\n                    except AttributeError:\n                        #not all elements have auth attribute..\n                        pass\n                elif ignore: #list\n                    doignore = False\n                    for c in ignore:\n                        if c is True:\n                            try:\n                                if not e.auth:\n                                    doignore =True\n                                    break\n                            except AttributeError:\n                                #not all elements have auth attribute..\n                                pass\n                        elif c == e.__class__ or issubclass(e.__class__,c):\n                            doignore = True\n                            break\n                    if doignore:\n                        continue\n\n                if isinstance(e, Class):\n                    if not set is None:\n                        try:\n                            if e.set != set:\n                                continue\n                        except AttributeError:\n                            continue\n                    yield e\n                if recursive:\n                    for e2 in e.select(Class, set, recursive, ignore, e):\n                        if not set is None:\n                            try:\n                                if e2.set != set:\n                                    continue\n                            except AttributeError:\n                                continue\n                        yield e2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef count(self, Class, set=None, recursive=True,  ignore=True, node=None):\n        return sum(1 for i in self.select(Class,set,recursive,ignore,node) )", "response": "Like : meth : AbstractElement. select but merely counts the elements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a depth - first flat list of all items below this element", "response": "def items(self, founditems=[]): #pylint: disable=dangerous-default-value\n        \"\"\"Returns a depth-first flat list of *all* items below this element (not limited to AbstractElement)\"\"\"\n        l = []\n        for e in self.data:\n            if  e not in founditems: #prevent going in recursive loops\n                l.append(e)\n                if isinstance(e, AbstractElement):\n                    l += e.items(l)\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getmetadata(self, key=None):\n        if self.metadata:\n            d =  self.doc.submetadata[self.metadata]\n        elif self.parent:\n            d =  self.parent.getmetadata()\n        elif self.doc:\n            d =  self.doc.metadata\n        else:\n            return None\n        if key:\n            return d[key]\n        else:\n            return d", "response": "Get the metadata that applies to this element automatically inherited from parent elements"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getindex(self, child, recursive=True, ignore=True):\n\n        #breadth first search\n        for i, c in enumerate(self.data):\n            if c is child:\n                return i\n        if recursive:  #pylint: disable=too-many-nested-blocks\n            for i, c in enumerate(self.data):\n                if ignore is True:\n                    try:\n                        if not c.auth:\n                            continue\n                    except AttributeError:\n                        #not all elements have auth attribute..\n                        pass\n                elif ignore: #list\n                    doignore = False\n                    for e in ignore:\n                        if e is True:\n                            try:\n                                if not c.auth:\n                                    doignore =True\n                                    break\n                            except AttributeError:\n                                #not all elements have auth attribute..\n                                pass\n                        elif e == c.__class__ or issubclass(c.__class__,e):\n                            doignore = True\n                            break\n                    if doignore:\n                        continue\n                if isinstance(c, AbstractElement):\n                    j = c.getindex(child, recursive)\n                    if j != -1:\n                        return i #yes, i ... not j!\n        return -1", "response": "Get the index at which an element occurs recursive by default!"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a boolean indicating whether this element precedes the other element", "response": "def precedes(self, other):\n        \"\"\"Returns a boolean indicating whether this element precedes the other element\"\"\"\n        try:\n            ancestor = next(commonancestors(AbstractElement, self, other))\n        except StopIteration:\n            raise Exception(\"Elements share no common ancestor\")\n        #now we just do a depth first search and see who comes first\n        def callback(e):\n            if e is self:\n                return True\n            elif e is other:\n                return False\n            return None\n        result = ancestor.depthfirstsearch(callback)\n        if result is None:\n            raise Exception(\"Unable to find relation between elements! (shouldn't happen)\")\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the next element in the specified scope. Returns None if no next element is found.", "response": "def next(self, Class=True, scope=True, reverse=False):\n        \"\"\"Returns the next element, if it is of the specified type and if it does not cross the boundary of the defined scope. Returns None if no next element is found. Non-authoritative elements are never returned.\n\n        Arguments:\n            * ``Class``: The class to select; any python class subclassed off `'AbstractElement``, may also be a tuple of multiple classes. Set to ``True`` to constrain to the same class as that of the current instance, set to ``None`` to not constrain at all\n            * ``scope``: A list of classes which are never crossed looking for a next element. Set to ``True`` to constrain to a default list of structure elements (Sentence,Paragraph,Division,Event, ListItem,Caption), set to ``None`` to not constrain at all.\n\n        \"\"\"\n        if Class is True: Class = self.__class__\n        if scope is True: scope = STRUCTURESCOPE\n\n        structural = Class is not None and issubclass(Class,AbstractStructureElement)\n\n        if reverse:\n            order = reversed\n            descendindex = -1\n        else:\n            order = lambda x: x #pylint: disable=redefined-variable-type\n            descendindex = 0\n\n        child = self\n        parent = self.parent\n        while parent: #pylint: disable=too-many-nested-blocks\n            if len(parent) > 1:\n                returnnext = False\n                for e in order(parent):\n                    if e is child:\n                        #we found the current item, next item will be the one to return\n                        returnnext = True\n                    elif returnnext and e.auth and not isinstance(e,AbstractAnnotationLayer) and (not structural or (structural and (not isinstance(e,(AbstractTokenAnnotation,TextContent)) ) )):\n                        if structural and isinstance(e,Correction):\n                            if not list(e.select(AbstractStructureElement)): #skip-over non-structural correction\n                                continue\n\n                        if Class is None or (isinstance(Class,tuple) and (any(isinstance(e,C) for C in Class))) or isinstance(e,Class):\n                            return e\n                        else:\n                            #this is not yet the element of the type we are looking for, we are going to descend again in the very leftmost (rightmost if reversed) branch only\n                            while e.data:\n                                e = e.data[descendindex]\n                                if not isinstance(e, AbstractElement):\n                                    return None #we've gone too far\n                                if e.auth and not isinstance(e,AbstractAnnotationLayer):\n                                    if Class is None or (isinstance(Class,tuple) and (any(isinstance(e,C) for C in Class))) or isinstance(e,Class):\n                                        return e\n                                    else:\n                                        #descend deeper\n                                        continue\n                        return None\n\n            #generational iteration\n            child = parent\n            if scope is not None and child.__class__ in scope:\n                #you shall not pass!\n                break\n            parent = parent.parent\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the previous element of the specified type and scope.", "response": "def previous(self, Class=True, scope=True):\n        \"\"\"Returns the previous element, if it is of the specified type and if it does not cross the boundary of the defined scope. Returns None if no next element is found. Non-authoritative elements are never returned.\n\n        Arguments:\n            * ``Class``: The class to select; any python class subclassed off `'AbstractElement``. Set to ``True`` to constrain to the same class as that of the current instance, set to ``None`` to not constrain at all\n            * ``scope``: A list of classes which are never crossed looking for a next element. Set to ``True`` to constrain to a default list of structure elements (Sentence,Paragraph,Division,Event, ListItem,Caption), set to ``None`` to not constrain at all.\n\n        \"\"\"\n        return self.next(Class,scope, True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the left context for an element as a list.", "response": "def leftcontext(self, size, placeholder=None, scope=None):\n        \"\"\"Returns the left context for an element, as a list. This method crosses sentence/paragraph boundaries by default, which can be restricted by setting scope\"\"\"\n\n        if size == 0: return [] #for efficiency\n\n        context = []\n        e = self\n        while len(context) < size:\n            e = e.previous(True,scope)\n            if not e: break\n            context.append(e)\n\n        if placeholder:\n            while len(context) < size:\n                context.append(placeholder)\n\n        context.reverse()\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rightcontext(self, size, placeholder=None, scope=None):\n\n        if size == 0: return [] #for efficiency\n\n        context = []\n        e = self\n        while len(context) < size:\n            e = e.next(True,scope)\n            if not e: break\n            context.append(e)\n\n        if placeholder:\n            while len(context) < size:\n                context.append(placeholder)\n\n        return context", "response": "Returns the right context for an element as a list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef context(self, size, placeholder=None, scope=None):\n        return self.leftcontext(size, placeholder,scope) + [self] + self.rightcontext(size, placeholder,scope)", "response": "Returns this word in context the current word and the next word in context"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef relaxng(cls, includechildren=True,extraattribs = None, extraelements=None, origclass = None):\n\n        E = ElementMaker(namespace=\"http://relaxng.org/ns/structure/1.0\",nsmap={None:'http://relaxng.org/ns/structure/1.0' , 'folia': \"http://ilk.uvt.nl/folia\", 'xml' : \"http://www.w3.org/XML/1998/namespace\",'a':\"http://relaxng.org/ns/annotation/0.9\" })\n\n        if origclass: cls = origclass\n\n        preamble = []\n        try:\n            if cls.__doc__:\n                E2 = ElementMaker(namespace=\"http://relaxng.org/ns/annotation/0.9\", nsmap={'a':'http://relaxng.org/ns/annotation/0.9'} )\n                preamble.append(E2.documentation(cls.__doc__))\n        except AttributeError:\n            pass\n\n        if cls.REQUIRED_ATTRIBS is None: cls.REQUIRED_ATTRIBS = () #bit hacky\n        if cls.OPTIONAL_ATTRIBS is None: cls.OPTIONAL_ATTRIBS = () #bit hacky\n\n\n        attribs = [ ]\n        if cls.REQUIRED_ATTRIBS and Attrib.ID in cls.REQUIRED_ATTRIBS:\n            attribs.append( E.attribute(E.data(type='ID',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'),name='id', ns=\"http://www.w3.org/XML/1998/namespace\") )\n        elif Attrib.ID in cls.OPTIONAL_ATTRIBS:\n            attribs.append( E.optional( E.attribute(E.data(type='ID',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'), name='id', ns=\"http://www.w3.org/XML/1998/namespace\") ) )\n        if Attrib.CLASS in cls.REQUIRED_ATTRIBS:\n            #Set is a tough one, we can't require it as it may be defined in the declaration: we make it optional and need schematron to resolve this later\n            attribs.append( E.attribute(E.data(type='string',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'),name='class') )\n            attribs.append( E.optional( E.attribute( E.data(type='string',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'),name='set' ) ) )\n        elif Attrib.CLASS in cls.OPTIONAL_ATTRIBS:\n            attribs.append( E.optional( E.attribute(E.data(type='string',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'),name='class') ) )\n            attribs.append( E.optional( E.attribute(E.data(type='string',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'), name='set' ) ) )\n        if Attrib.ANNOTATOR in cls.REQUIRED_ATTRIBS or Attrib.ANNOTATOR in cls.OPTIONAL_ATTRIBS:\n            #Similarly tough\n            attribs.append( E.optional( E.attribute(E.data(type='string',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'), name='annotator') ) )\n            attribs.append( E.optional( E.attribute(name='annotatortype') ) )\n        if Attrib.CONFIDENCE in cls.REQUIRED_ATTRIBS:\n            attribs.append(  E.attribute(E.data(type='double',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'), name='confidence') )\n        elif Attrib.CONFIDENCE in cls.OPTIONAL_ATTRIBS:\n            attribs.append(  E.optional( E.attribute(E.data(type='double',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'), name='confidence') ) )\n        if Attrib.N in cls.REQUIRED_ATTRIBS:\n            attribs.append( E.attribute( E.data(type='string',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'),name='n') )\n        elif Attrib.N in cls.OPTIONAL_ATTRIBS:\n            attribs.append( E.optional( E.attribute( E.data(type='string',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'),name='n') ) )\n        if Attrib.DATETIME in cls.REQUIRED_ATTRIBS:\n            attribs.append( E.attribute(E.data(type='dateTime',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'), name='datetime') )\n        elif Attrib.DATETIME in cls.OPTIONAL_ATTRIBS:\n            attribs.append( E.optional( E.attribute( E.data(type='dateTime',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'),  name='datetime') ) )\n        if Attrib.BEGINTIME in cls.REQUIRED_ATTRIBS:\n            attribs.append(E.attribute(name='begintime') )\n        elif Attrib.BEGINTIME in cls.OPTIONAL_ATTRIBS:\n            attribs.append( E.optional( E.attribute(name='begintime') ) )\n        if Attrib.ENDTIME in cls.REQUIRED_ATTRIBS:\n            attribs.append(E.attribute(name='endtime') )\n        elif Attrib.ENDTIME in cls.OPTIONAL_ATTRIBS:\n            attribs.append( E.optional( E.attribute(name='endtime') ) )\n        if Attrib.SRC in cls.REQUIRED_ATTRIBS:\n            attribs.append(E.attribute(E.data(type='anyURI',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'),name='src') )\n        elif Attrib.SRC in cls.OPTIONAL_ATTRIBS:\n            attribs.append( E.optional( E.attribute(E.data(type='anyURI',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'),name='src') ) )\n        if Attrib.SPEAKER in cls.REQUIRED_ATTRIBS:\n            attribs.append(E.attribute(E.data(type='string',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'), name='speaker') )\n        elif Attrib.SPEAKER in cls.OPTIONAL_ATTRIBS:\n            attribs.append( E.optional( E.attribute(E.data(type='string',datatypeLibrary='http://www.w3.org/2001/XMLSchema-datatypes'),name='speaker') ) )\n        if Attrib.TEXTCLASS in cls.REQUIRED_ATTRIBS:\n            attribs.append(E.attribute(name='textclass') )\n        elif Attrib.TEXTCLASS in cls.OPTIONAL_ATTRIBS:\n            attribs.append( E.optional( E.attribute(name='textclass') ) )\n        if Attrib.METADATA in cls.REQUIRED_ATTRIBS:\n            attribs.append(E.attribute(name='metadata') )\n        elif Attrib.METADATA in cls.OPTIONAL_ATTRIBS:\n            attribs.append( E.optional( E.attribute(name='metadata') ) )\n        if cls.XLINK:\n            attribs += [ #loose interpretation of specs, not checking whether xlink combinations are valid\n                    E.optional(E.attribute(name='href',ns=\"http://www.w3.org/1999/xlink\"),E.attribute(name='type',ns=\"http://www.w3.org/1999/xlink\") ),\n                    E.optional(E.attribute(name='role',ns=\"http://www.w3.org/1999/xlink\")),\n                    E.optional(E.attribute(name='title',ns=\"http://www.w3.org/1999/xlink\")),\n                    E.optional(E.attribute(name='label',ns=\"http://www.w3.org/1999/xlink\")),\n                    E.optional(E.attribute(name='show',ns=\"http://www.w3.org/1999/xlink\")),\n            ]\n\n        attribs.append( E.optional( E.attribute( name='auth' ) ) )\n\n\n\n        if extraattribs:\n            for e in extraattribs:\n                attribs.append(e) #s\n\n        attribs.append( E.ref(name=\"allow_foreign_attributes\") )\n\n\n        elements = [] #(including attributes)\n        if cls.TEXTCONTAINER or cls.PHONCONTAINER:\n            elements.append( E.text())\n            #We actually want to require non-empty text (E.text() is not sufficient)\n            #but this is not solved yet, see https://github.com/proycon/folia/issues/19\n            #elements.append( E.data(E.param(r\".+\",name=\"pattern\"),type='string'))\n            #elements.append( E.data(E.param(r\"(.|\\n|\\r)*\\S+(.|\\n|\\r)*\",name=\"pattern\"),type='string'))\n        done = {}\n        if includechildren and cls.ACCEPTED_DATA: #pylint: disable=too-many-nested-blocks\n            for c in cls.ACCEPTED_DATA:\n                if c.__name__[:8] == 'Abstract' and inspect.isclass(c):\n                    for c2 in globals().values():\n                        try:\n                            if inspect.isclass(c2) and issubclass(c2, c):\n                                try:\n                                    if c2.XMLTAG and c2.XMLTAG not in done:\n                                        if c2.OCCURRENCES == 1:\n                                            elements.append( E.optional( E.ref(name=c2.XMLTAG) ) )\n                                        else:\n                                            elements.append( E.zeroOrMore( E.ref(name=c2.XMLTAG) ) )\n                                            if c2.XMLTAG == 'item': #nasty hack for backward compatibility with deprecated listitem element\n                                                elements.append( E.zeroOrMore( E.ref(name='listitem') ) )\n                                        done[c2.XMLTAG] = True\n                                except AttributeError:\n                                    continue\n                        except TypeError:\n                            pass\n                elif issubclass(c, Feature) and c.SUBSET:\n                    attribs.append( E.optional( E.attribute(name=c.SUBSET)))  #features as attributes\n                else:\n                    try:\n                        if c.XMLTAG and c.XMLTAG not in done:\n                            if cls.REQUIRED_DATA and c in cls.REQUIRED_DATA:\n                                if c.OCCURRENCES == 1:\n                                    elements.append( E.ref(name=c.XMLTAG) )\n                                else:\n                                    elements.append( E.oneOrMore( E.ref(name=c.XMLTAG) ) )\n                            elif c.OCCURRENCES == 1:\n                                elements.append( E.optional( E.ref(name=c.XMLTAG) ) )\n                            else:\n                                elements.append( E.zeroOrMore( E.ref(name=c.XMLTAG) ) )\n                                if c.XMLTAG == 'item':\n                                    #nasty hack for backward compatibility with deprecated listitem element\n                                    elements.append( E.zeroOrMore( E.ref(name='listitem') )  )\n                            done[c.XMLTAG] = True\n                    except AttributeError:\n                        continue\n\n        if extraelements:\n            for e in extraelements:\n                elements.append( e )\n\n        if elements:\n            if len(elements) > 1:\n                attribs.append( E.interleave(*elements) )\n            else:\n                attribs.append( *elements )\n\n        if not attribs:\n            attribs.append( E.empty() )\n\n        if cls.XMLTAG in ('desc','comment'):\n            return E.define( E.element(E.text(), *(preamble + attribs), **{'name': cls.XMLTAG}), name=cls.XMLTAG, ns=NSFOLIA)\n        else:\n            return E.define( E.element(*(preamble + attribs), **{'name': cls.XMLTAG}), name=cls.XMLTAG, ns=NSFOLIA)", "response": "Returns a RelaxNG definition for this element."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parsexml(Class, node, doc, **kwargs): #pylint: disable=bad-classmethod-argument\n\n        assert issubclass(Class, AbstractElement)\n\n        if doc.preparsexmlcallback:\n            result = doc.preparsexmlcallback(node)\n            if not result:\n                return None\n            if isinstance(result, AbstractElement):\n                return result\n\n\n\n        dcoi = node.tag.startswith('{' + NSDCOI + '}')\n        args = []\n        if not kwargs: kwargs = {}\n        text = None #for dcoi support\n        if (Class.TEXTCONTAINER or Class.PHONCONTAINER) and node.text:\n            args.append(node.text)\n\n\n        for subnode in node: #pylint: disable=too-many-nested-blocks\n            #don't trip over comments\n            if isinstance(subnode, ElementTree._Comment): #pylint: disable=protected-access\n                if (Class.TEXTCONTAINER or Class.PHONCONTAINER) and subnode.tail:\n                    args.append(subnode.tail)\n            else:\n                if subnode.tag.startswith('{' + NSFOLIA + '}'):\n                    if doc.debug >= 1: print(\"[PyNLPl FoLiA DEBUG] Processing subnode \" + subnode.tag[nslen:],file=stderr)\n                    try:\n                        e = doc.parsexml(subnode, Class)\n                    except ParseError as e:\n                        raise #just re-raise deepest parseError\n                    except Exception as e:\n                        #Python 3 will preserve full original traceback, Python 2 does not, original cause is explicitly passed to ParseError anyway:\n                        raise ParseError(\"FoLiA exception in handling of <\" + subnode.tag[len(NSFOLIA)+2:] + \"> @ line \" + str(subnode.sourceline) + \": [\" + e.__class__.__name__ + \"] \" + str(e), cause=e)\n                    if e is not None:\n                        args.append(e)\n                    if (Class.TEXTCONTAINER or Class.PHONCONTAINER) and subnode.tail:\n                        args.append(subnode.tail)\n                elif subnode.tag.startswith('{' + NSDCOI + '}'):\n                    #Dcoi support\n                    if Class is Text and subnode.tag[nslendcoi:] == 'body':\n                        for subsubnode in subnode:\n                            if doc.debug >= 1: print(\"[PyNLPl FoLiA DEBUG] Processing DCOI subnode \" + subnode.tag[nslendcoi:],file=stderr)\n                            e = doc.parsexml(subsubnode, Class)\n                            if e is not None:\n                                args.append(e)\n                    else:\n                        if doc.debug >= 1: print( \"[PyNLPl FoLiA DEBUG] Processing DCOI subnode \" + subnode.tag[nslendcoi:],file=stderr)\n                        e = doc.parsexml(subnode, Class)\n                        if e is not None:\n                            args.append(e)\n                elif doc.debug >= 1:\n                    print(\"[PyNLPl FoLiA DEBUG] Ignoring subnode outside of FoLiA namespace: \" + subnode.tag,file=stderr)\n\n\n\n        if dcoi:\n            dcoipos = dcoilemma = dcoicorrection = dcoicorrectionoriginal = None\n        for key, value in node.attrib.items():\n            if key[0] == '{' or key =='XMLid':\n                if key == '{http://www.w3.org/XML/1998/namespace}id' or key == 'XMLid':\n                    key = 'id'\n                elif key.startswith( '{' + NSFOLIA + '}'):\n                    key = key[nslen:]\n                    if key == 'id':\n                        #ID in FoLiA namespace is always a reference, passed in kwargs as follows:\n                        key = 'idref'\n                elif Class.XLINK and key.startswith('{http://www.w3.org/1999/xlink}'):\n                    key = key[30:]\n                    if key != 'href':\n                        key = 'xlink' + key #xlinktype, xlinkrole, xlinklabel, xlinkshow, etc..\n                elif key.startswith('{' + NSDCOI + '}'):\n                    key = key[nslendcoi:]\n\n            #D-Coi support:\n            if dcoi:\n                if Class is Word and key == 'pos':\n                    dcoipos = value\n                    continue\n                elif Class is Word and  key == 'lemma':\n                    dcoilemma = value\n                    continue\n                elif Class is Word and  key == 'correction':\n                    dcoicorrection = value #class\n                    continue\n                elif Class is Word and  key == 'original':\n                    dcoicorrectionoriginal = value\n                    continue\n                elif Class is Gap and  key == 'reason':\n                    key = 'class'\n                elif Class is Gap and  key == 'hand':\n                    key = 'annotator'\n                elif Class is Division and  key == 'type':\n                    key = 'cls'\n\n            kwargs[key] = value\n\n        #D-Coi support:\n        if dcoi and TextContent in Class.ACCEPTED_DATA and node.text:\n            text = node.text.strip()\n\n            kwargs['text'] = text\n            if not AnnotationType.TOKEN in doc.annotationdefaults:\n                doc.declare(AnnotationType.TOKEN, set='http://ilk.uvt.nl/folia/sets/ilktok.foliaset')\n\n        if doc.debug >= 1: print(\"[PyNLPl FoLiA DEBUG] Found \" + node.tag[nslen:],file=stderr)\n        instance = Class(doc, *args, **kwargs)\n        #if id:\n        #    if doc.debug >= 1: print >>stderr, \"[PyNLPl FoLiA DEBUG] Adding to index: \" + id\n        #    doc.index[id] = instance\n        if dcoi:\n            if dcoipos:\n                if not AnnotationType.POS in doc.annotationdefaults:\n                    doc.declare(AnnotationType.POS, set='http://ilk.uvt.nl/folia/sets/cgn-legacy.foliaset')\n                instance.append( PosAnnotation(doc, cls=dcoipos) )\n            if dcoilemma:\n                if not AnnotationType.LEMMA in doc.annotationdefaults:\n                    doc.declare(AnnotationType.LEMMA, set='http://ilk.uvt.nl/folia/sets/mblem-nl.foliaset')\n                instance.append( LemmaAnnotation(doc, cls=dcoilemma) )\n            if dcoicorrection and dcoicorrectionoriginal and text:\n                if not AnnotationType.CORRECTION in doc.annotationdefaults:\n                    doc.declare(AnnotationType.CORRECTION, set='http://ilk.uvt.nl/folia/sets/dcoi-corrections.foliaset')\n                instance.correct(generate_id_in=instance, cls=dcoicorrection, original=dcoicorrectionoriginal, new=text)\n\n        if doc.parsexmlcallback:\n            result = doc.parsexmlcallback(instance)\n            if not result:\n                return None\n            if isinstance(result, AbstractElement):\n                return result\n\n        return instance", "response": "Internal method used for turning an XML element into an instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, child):\n        if not isinstance(child, AbstractElement):\n            raise ValueError(\"Expected AbstractElement, got \" + str(type(child)))\n        if child.parent == self:\n            child.parent = None\n        self.data.remove(child)\n        #delete from index\n        if child.id and self.doc and child.id in self.doc.index:\n            del self.doc.index[child.id]", "response": "Removes the child element from the data and index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef incorrection(self):\n        e = self.parent\n\n        while e:\n            if isinstance(e, Correction):\n                return e\n            if isinstance(e, AbstractStructureElement):\n                break\n            e = e.parent\n        return None", "response": "Returns the Correction element if it is part of a correction."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef correct(self, **kwargs):\n\n        if 'insertindex_offset' in kwargs:\n            del kwargs['insertindex_offset'] #dealt with in an earlier stage\n\n        if 'confidence' in kwargs and kwargs['confidence'] is None:\n            del kwargs['confidence']\n\n        if 'reuse' in kwargs:\n            #reuse an existing correction instead of making a new one\n            if isinstance(kwargs['reuse'], Correction):\n                c = kwargs['reuse']\n            else: #assume it's an index\n                try:\n                    c = self.doc.index[kwargs['reuse']]\n                    assert isinstance(c, Correction)\n                except:\n                    raise ValueError(\"reuse= must point to an existing correction (id or instance)! Got \" + str(kwargs['reuse']))\n\n            suggestionsonly = (not c.hasnew(True) and not c.hasoriginal(True) and c.hassuggestions(True))\n\n            if 'new' in kwargs and c.hascurrent():\n                #can't add new if there's current, so first set original to current, and then delete current\n\n                if 'current' in kwargs:\n                    raise Exception(\"Can't set both new= and current= !\")\n                if 'original' not in kwargs:\n                    kwargs['original'] = c.current()\n\n                c.remove(c.current())\n        else:\n            if 'id' not in kwargs and 'generate_id_in' not in kwargs:\n                kwargs['generate_id_in'] = self\n            kwargs2 = copy(kwargs)\n            for x in ['new','original','suggestion', 'suggestions','current', 'insertindex','nooriginal']:\n                if x in kwargs2:\n                    del kwargs2[x]\n            c = Correction(self.doc, **kwargs2)\n\n        addnew = False\n        if 'insertindex' in kwargs:\n            insertindex = int(kwargs['insertindex'])\n            del kwargs['insertindex']\n        else:\n            insertindex = -1 #append\n\n        if 'nooriginal' in kwargs and kwargs['nooriginal']:\n            nooriginal = True\n            del kwargs['nooriginal']\n        else:\n            nooriginal = False\n\n        if 'current' in kwargs:\n            if 'original' in kwargs or 'new' in kwargs: raise Exception(\"When setting current=, original= and new= can not be set!\")\n            if not isinstance(kwargs['current'], list) and not isinstance(kwargs['current'], tuple): kwargs['current'] = [kwargs['current']] #support both lists (for multiple elements at once), as well as single element\n            c.replace(Current(self.doc, *kwargs['current']))\n            for o in kwargs['current']: #delete current from current element\n                if o in self and isinstance(o, AbstractElement): #pylint: disable=unsupported-membership-test\n                    if insertindex == -1: insertindex = self.data.index(o)\n                    self.remove(o)\n            del kwargs['current']\n        if 'new' in kwargs:\n            if not isinstance(kwargs['new'], list) and not isinstance(kwargs['new'], tuple): kwargs['new'] = [kwargs['new']] #support both lists (for multiple elements at once), as well as single element\n            addnew = New(self.doc, *kwargs['new']) #pylint: disable=redefined-variable-type\n            c.replace(addnew)\n            for current in c.select(Current): #delete current if present\n                c.remove(current)\n            del kwargs['new']\n        if 'original' in kwargs and kwargs['original']:\n            if not isinstance(kwargs['original'], list) and not isinstance(kwargs['original'], tuple): kwargs['original'] = [kwargs['original']] #support both lists (for multiple elements at once), as well as single element\n            c.replace(Original(self.doc, *kwargs['original']))\n            for o in kwargs['original']: #delete original from current element\n                if o in self and isinstance(o, AbstractElement): #pylint: disable=unsupported-membership-test\n                    if insertindex == -1: insertindex = self.data.index(o)\n                    self.remove(o)\n            for o in kwargs['original']: #make sure IDs are still properly set after removal\n                o.addtoindex()\n            for current in c.select(Current):  #delete current if present\n                c.remove(current)\n            del kwargs['original']\n        elif addnew and not nooriginal:\n            #original not specified, find automagically:\n            original = []\n            for new in addnew:\n                kwargs2 = {}\n                if isinstance(new, TextContent):\n                    kwargs2['cls'] = new.cls\n                try:\n                    set = new.set\n                except AttributeError:\n                    set = None\n                #print(\"DEBUG: Finding replaceables within \" + str(repr(self)) + \" for \", str(repr(new)), \" set \" ,set , \" args \" ,repr(kwargs2),file=sys.stderr)\n                replaceables = new.__class__.findreplaceables(self, set, **kwargs2)\n                #print(\"DEBUG: \" , len(replaceables) , \" found\",file=sys.stderr)\n                original += replaceables\n            if not original:\n                #print(\"DEBUG: \", self.xmlstring(),file=sys.stderr)\n                raise Exception(\"No original= specified and unable to automatically infer on \" + str(repr(self)) + \" for \" + str(repr(new)) + \" with set \" + set)\n            else:\n                c.replace( Original(self.doc, *original))\n                for current in c.select(Current):  #delete current if present\n                    c.remove(current)\n\n        if addnew and not nooriginal:\n            for original in c.original():\n                if original in self: #pylint: disable=unsupported-membership-test\n                    self.remove(original)\n\n        if 'suggestion' in kwargs:\n            kwargs['suggestions'] = [kwargs['suggestion']]\n            del kwargs['suggestion']\n        if 'suggestions' in kwargs:\n            for suggestion in kwargs['suggestions']:\n                if isinstance(suggestion, Suggestion):\n                    c.append(suggestion)\n                elif isinstance(suggestion, list) or isinstance(suggestion, tuple):\n                    c.append(Suggestion(self.doc, *suggestion))\n                else:\n                    c.append(Suggestion(self.doc, suggestion))\n            del kwargs['suggestions']\n\n\n\n\n        if 'reuse' in kwargs:\n            if addnew and suggestionsonly:\n                #What was previously only a suggestion, now becomes a real correction\n                #If annotator, annotatortypes\n                #are associated with the correction as a whole, move it to the suggestions\n                #correction-wide annotator, annotatortypes might be overwritten\n                for suggestion in c.suggestions():\n                    if c.annotator and not suggestion.annotator:\n                        suggestion.annotator = c.annotator\n                    if c.annotatortype and not suggestion.annotatortype:\n                        suggestion.annotatortype = c.annotatortype\n\n            if 'annotator' in kwargs:\n                c.annotator = kwargs['annotator'] #pylint: disable=attribute-defined-outside-init\n            if 'annotatortype' in kwargs:\n                c.annotatortype = kwargs['annotatortype'] #pylint: disable=attribute-defined-outside-init\n            if 'confidence' in kwargs:\n                c.confidence = float(kwargs['confidence']) #pylint: disable=attribute-defined-outside-init\n            c.addtoindex()\n            del kwargs['reuse']\n        else:\n            c.addtoindex()\n            if insertindex == -1:\n                self.append(c)\n            else:\n                self.insert(insertindex, c)\n        return c", "response": "Apply a correction to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nobtain child elements of the specified class and optionally set.", "response": "def annotations(self,Class,set=None):\n        \"\"\"Obtain child elements (annotations) of the specified class.\n\n        A further restriction can be made based on set.\n\n        Arguments:\n            Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\n            Set (str): The set to match against, only elements pertaining to this set will be returned. If set to None (default), all elements regardless of set will be returned.\n\n        Yields:\n            Elements (instances derived from :class:`AbstractElement`)\n\n        Example::\n\n            for sense in text.annotations(folia.Sense, 'http://some/path/cornetto'):\n                ..\n\n        See also:\n            :meth:`AbstractElement.select`\n\n        Raises:\n            :meth:`AllowTokenAnnotation.annotations`\n            :class:`NoSuchAnnotation` if no such annotation exists\n        \"\"\"\n        found = False\n        for e in self.select(Class,set,True,default_ignore_annotations):\n            found = True\n            yield e\n        if not found:\n            raise NoSuchAnnotation()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an integer indicating whether such as annotation exists and how many.", "response": "def hasannotation(self,Class,set=None):\n        \"\"\"Returns an integer indicating whether such as annotation exists, and if so, how many.\n\n        See :meth:`AllowTokenAnnotation.annotations`` for a description of the parameters.\"\"\"\n        return sum( 1 for _ in self.select(Class,set,True,default_ignore_annotations))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef annotation(self, type, set=None):\n        \"\"\"Will return a **single** annotation (even if there are multiple). Raises a ``NoSuchAnnotation`` exception if none was found\"\"\"\n        for e in self.select(type,set,True,default_ignore_annotations):\n            return e\n        raise NoSuchAnnotation()", "response": "Obtain a single annotation element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nappend a child to the end of the structure element.", "response": "def append(self, child, *args, **kwargs):\n        \"\"\"See ``AbstractElement.append()``\"\"\"\n        e = super(AbstractStructureElement,self).append(child, *args, **kwargs)\n        self._setmaxid(e)\n        return e"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a generator of Word elements found under this element.", "response": "def words(self, index = None):\n        \"\"\"Returns a generator of Word elements found (recursively) under this element.\n\n        Arguments:\n            * ``index``: If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning the list of all\n        \"\"\"\n        if index is None:\n            return self.select(Word,None,True,default_ignore_structure)\n        else:\n            if index < 0:\n                index = self.count(Word,None,True,default_ignore_structure) + index\n            for i, e in enumerate(self.select(Word,None,True,default_ignore_structure)):\n                if i == index:\n                    return e\n            raise IndexError"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a generator of Paragraph elements found under this element.", "response": "def paragraphs(self, index = None):\n        \"\"\"Returns a generator of Paragraph elements found (recursively) under this element.\n\n        Arguments:\n            index (int or None): If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning the generator of all\n        \"\"\"\n        if index is None:\n            return self.select(Paragraph,None,True,default_ignore_structure)\n        else:\n            if index < 0:\n                index = self.count(Paragraph,None,True,default_ignore_structure) + index\n            for i,e in enumerate(self.select(Paragraph,None,True,default_ignore_structure)):\n                if i == index:\n                    return e\n            raise IndexError"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a generator of Sentence elements found under this element.", "response": "def sentences(self, index = None):\n        \"\"\"Returns a generator of Sentence elements found (recursively) under this element\n\n        Arguments:\n            index (int or None): If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning a generator of all\n        \"\"\"\n        if index is None:\n            return self.select(Sentence,None,True,default_ignore_structure)\n        else:\n            if index < 0:\n                index = self.count(Sentence,None,True,default_ignore_structure) + index\n            for i,e in enumerate(self.select(Sentence,None,True,default_ignore_structure)):\n                if i == index:\n                    return e\n            raise IndexError"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef layers(self, annotationtype=None,set=None):\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\n        return [ x for x in self.select(AbstractAnnotationLayer,set,False,True) if annotationtype is None or x.ANNOTATIONTYPE == annotationtype ]", "response": "Returns a list of annotation layers found directly under this element does not include alternative layers"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndoes the specified annotation layer exist?", "response": "def hasannotationlayer(self, annotationtype=None,set=None):\n        \"\"\"Does the specified annotation layer exist?\"\"\"\n        l = self.layers(annotationtype, set)\n        return (len(l) > 0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the XML representation of this element.", "response": "def xml(self, attribs = None,elements = None, skipchildren = False):\n        \"\"\"See :meth:`AbstractElement.xml`\"\"\"\n        if not attribs: attribs = {}\n        if self.idref:\n            attribs['id'] = self.idref\n        return super(AbstractTextMarkup,self).xml(attribs,elements, skipchildren)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef json(self,attribs =None, recurse=True, ignorelist=False):\n        if not attribs: attribs = {}\n        if self.idref:\n            attribs['id'] = self.idref\n        return super(AbstractTextMarkup,self).json(attribs,recurse, ignorelist)", "response": "See : meth : AbstractElement. json"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nobtaining the text of the content.", "response": "def text(self, normalize_spaces=False):\n        \"\"\"Obtain the text (unicode instance)\"\"\"\n        return super(TextContent,self).text(normalize_spaces=normalize_spaces)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getreference(self, validate=True):\n\n        if self.offset is None: return None #nothing to test\n        if self.ref:\n            ref = self.doc[self.ref]\n        else:\n            ref = self.finddefaultreference()\n\n        if not ref:\n            raise UnresolvableTextContent(\"Default reference for textcontent not found!\")\n        elif not ref.hastext(self.cls):\n            raise UnresolvableTextContent(\"Reference (ID \" + str(ref.id) + \") has no such text (class=\" + self.cls+\")\")\n        elif validate and self.text() != ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])]:\n            raise UnresolvableTextContent(\"Reference (ID \" + str(ref.id) + \", class=\" + self.cls+\") found but no text match at specified offset (\"+str(self.offset)+\")! Expected '\" + self.text() + \"', got '\" + ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])] +\"'\")\n        else:\n            #finally, we made it!\n            return ref", "response": "Returns and validates the Text Content s reference. Raises UnresolvableTextContent when invalid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef xml(self, attribs = None,elements = None, skipchildren = False):\n        attribs = {}\n        if not self.offset is None:\n            attribs['{' + NSFOLIA + '}offset'] = str(self.offset)\n        if self.parent and self.ref:\n            attribs['{' + NSFOLIA + '}ref'] = self.ref\n\n        #if self.cls != 'current' and not (self.cls == 'original' and any( isinstance(x, Original) for x in self.ancestors() )  ):\n        #    attribs['{' + NSFOLIA + '}class'] = self.cls\n        #else:\n        #    if '{' + NSFOLIA + '}class' in attribs:\n        #        del attribs['{' + NSFOLIA + '}class']\n        #return E.t(self.value, **attribs)\n\n        e = super(TextContent,self).xml(attribs,elements,skipchildren)\n        if '{' + NSFOLIA + '}class' in e.attrib and e.attrib['{' + NSFOLIA + '}class'] == \"current\":\n            #delete 'class=current'\n            del e.attrib['{' + NSFOLIA + '}class']\n\n        return e", "response": "See : meth : AbstractElement. xml"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn and validate the Phonetic Content s reference. Raises UnresolvableTextContent when invalid", "response": "def getreference(self, validate=True):\n        \"\"\"Return and validate the Phonetic Content's reference. Raises UnresolvableTextContent when invalid\"\"\"\n\n        if self.offset is None: return None #nothing to test\n        if self.ref:\n            ref = self.doc[self.ref]\n        else:\n            ref = self.finddefaultreference()\n\n        if not ref:\n            raise UnresolvableTextContent(\"Default reference for phonetic content not found!\")\n        elif not ref.hasphon(self.cls):\n            raise UnresolvableTextContent(\"Reference has no such phonetic content (class=\" + self.cls+\")\")\n        elif validate and self.phon() != ref.textcontent(self.cls).phon()[self.offset:self.offset+len(self.data[0])]:\n            raise UnresolvableTextContent(\"Reference (class=\" + self.cls+\") found but no phonetic match at specified offset (\"+str(self.offset)+\")! Expected '\" + self.text() + \"', got '\" + ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])] +\"'\")\n        else:\n            #finally, we made it!\n            return ref"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef finddefaultreference(self):\n\n        depth = 0\n        e = self\n        while True:\n            if e.parent:\n                e = e.parent #pylint: disable=redefined-variable-type\n            else:\n                #no parent, breaking\n                return False\n\n            if isinstance(e,AbstractStructureElement) or isinstance(e,AbstractSubtokenAnnotation):\n                depth += 1\n                if depth == 2:\n                    return e\n\n\n        return False", "response": "Find the default reference for text offsets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef findreplaceables(Class, parent, set, **kwargs):#pylint: disable=bad-classmethod-argument\n        #some extra behaviour for text content elements, replace also based on the 'corrected' attribute:\n        if 'cls' not in kwargs:\n            kwargs['cls'] = 'current'\n        replace = super(PhonContent, Class).findreplaceables(parent, set, **kwargs)\n        replace = [ x for x in replace if x.cls == kwargs['cls']]\n        del kwargs['cls'] #always delete what we processed\n        return replace", "response": "Find all replaceables of a given class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the PhonContent XML node.", "response": "def parsexml(Class, node, doc, **kwargs):#pylint: disable=bad-classmethod-argument\n        \"\"\"(Method for internal usage, see AbstractElement)\"\"\"\n        if not kwargs: kwargs = {}\n        if 'offset' in node.attrib:\n            kwargs['offset'] = int(node.attrib['offset'])\n        if 'ref' in node.attrib:\n            kwargs['ref'] = node.attrib['ref']\n        return super(PhonContent,Class).parsexml(node,doc, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef phonemes(self,set=None):\n        for layer in self.select(PhonologyLayer):\n            for p in layer.select(Phoneme, set):\n                yield p", "response": "Generator yielding all phonemes in a particular set."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef morpheme(self,index, set=None):\n        for layer in self.select(MorphologyLayer):\n            for i, m in enumerate(layer.select(Morpheme, set)):\n                if index == i:\n                    return m\n        raise NoSuchAnnotation", "response": "Returns a specific morpheme then'th morpheme."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a specific phoneme then'th morpheme given the particular set.", "response": "def phoneme(self,index, set=None):\n        \"\"\"Returns a specific phoneme, the n'th morpheme (given the particular set if specified).\"\"\"\n        for layer in self.select(PhonologyLayer):\n            for i, p in enumerate(layer.select(Phoneme, set)):\n                if index == i:\n                    return p\n        raise NoSuchAnnotation"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef findspans(self, type,set=None):\n\n        if issubclass(type, AbstractAnnotationLayer):\n            layerclass = type\n        else:\n            layerclass = ANNOTATIONTYPE2LAYERCLASS[type.ANNOTATIONTYPE]\n        e = self\n        while True:\n            if not e.parent: break\n            e = e.parent\n            for layer in e.select(layerclass,set,False):\n                if type is layerclass:\n                    for e2 in layer.select(AbstractSpanAnnotation,set,True, (True, Word, Morpheme)):\n                        if not isinstance(e2, AbstractSpanRole) and self in e2.wrefs():\n                            yield e2\n                else:\n                    for e2 in layer.select(type,set,True, (True, Word, Morpheme)):\n                        if not isinstance(e2, AbstractSpanRole) and self in e2.wrefs():\n                            yield e2", "response": "Yields all span annotation elements of the specified type that include this word."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform deep validation of this element.", "response": "def deepvalidation(self):\n        \"\"\"Perform deep validation of this element.\n\n        Raises:\n            :class:`DeepValidationError`\n        \"\"\"\n        if self.doc and self.doc.deepvalidation and self.parent.set and self.parent.set[0] != '_':\n            try:\n                self.doc.setdefinitions[self.parent.set].testsubclass(self.parent.cls, self.subset, self.cls)\n            except KeyError as e:\n                if self.parent.cls and not self.doc.allowadhocsets:\n                    raise DeepValidationError(\"Set definition \" + self.parent.set + \" for \" + self.parent.XMLTAG + \" not loaded (feature validation failed)!\")\n            except DeepValidationError as e:\n                errormsg =  str(e) + \" (in set \" + self.parent.set+\" for \" + self.parent.XMLTAG\n                if self.parent.id:\n                    errormsg += \" with ID \" + self.parent.id\n                errormsg +=  \")\"\n                raise DeepValidationError(errormsg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsees : meth : AbstractElement. xml", "response": "def xml(self, attribs = None,elements = None, skipchildren = False):\n        \"\"\"See :meth:`AbstractElement.xml`\"\"\"\n        if not attribs: attribs = {}\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={None: \"http://ilk.uvt.nl/folia\", 'xml' : \"http://www.w3.org/XML/1998/namespace\"})\n        e = super(AbstractSpanAnnotation,self).xml(attribs, elements, True)\n        for child in self:\n            if isinstance(child, (Word, Morpheme, Phoneme)):\n                #Include REFERENCES to word items instead of word items themselves\n                attribs['{' + NSFOLIA + '}id'] = child.id\n                if child.PRINTABLE and child.hastext(self.textclass):\n                    attribs['{' + NSFOLIA + '}t'] = child.text(self.textclass)\n                e.append( E.wref(**attribs) )\n            elif not (isinstance(child, Feature) and child.SUBSET): #Don't add pre-defined features, they are already added as attributes\n                e.append( child.xml() )\n        return e"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef append(self, child, *args, **kwargs):\n        #Accept Word instances instead of WordReference, references will be automagically used upon serialisation\n        if isinstance(child, (Word, Morpheme, Phoneme)) and WordReference in self.ACCEPTED_DATA:\n            #We don't really append but do an insertion so all references are in proper order\n            insertionpoint = len(self.data)\n            for i, sibling in enumerate(self.data):\n                if isinstance(sibling, (Word, Morpheme, Phoneme)):\n                    try:\n                        if not sibling.precedes(child):\n                            insertionpoint = i\n                    except: #happens if we can't determine common ancestors\n                        pass\n\n            self.data.insert(insertionpoint, child)\n            return child\n        elif isinstance(child, AbstractSpanAnnotation): #(covers span roles just as well)\n            insertionpoint = len(self.data)\n            try:\n                firstword = child.wrefs(0)\n            except IndexError:\n                #we have no basis to determine an insertionpoint for this child, just append it then\n                return super(AbstractSpanAnnotation,self).append(child, *args, **kwargs)\n\n            insertionpoint = len(self.data)\n            for i, sibling in enumerate(self.data):\n                if isinstance(sibling, (Word, Morpheme, Phoneme)):\n                    try:\n                        if not sibling.precedes(firstword):\n                            insertionpoint = i\n                    except: #happens if we can't determine common ancestors\n                        pass\n            return super(AbstractSpanAnnotation,self).insert(insertionpoint, child, *args, **kwargs)\n        else:\n            return super(AbstractSpanAnnotation,self).append(child, *args, **kwargs)", "response": "Append a child to the list of related elements."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the span of the span element anew erases all data inside.", "response": "def setspan(self, *args):\n        \"\"\"Sets the span of the span element anew, erases all data inside.\n\n        Arguments:\n            *args: Instances of :class:`Word`, :class:`Morpheme` or :class:`Phoneme`\n        \"\"\"\n        self.data = []\n        for child in args:\n            self.append(child)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hasannotation(self,Class,set=None):\n        return self.count(Class,set,True,default_ignore_annotations)", "response": "Returns an integer indicating whether such as annotation exists and how many."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _helper_wrefs(self, targets, recurse=True):\n        for c in self:\n            if isinstance(c,Word) or isinstance(c,Morpheme) or isinstance(c, Phoneme):\n                targets.append(c)\n            elif isinstance(c,WordReference):\n                try:\n                    targets.append(self.doc[c.id]) #try to resolve\n                except KeyError:\n                    targets.append(c) #add unresolved\n            elif isinstance(c, AbstractSpanAnnotation) and recurse:\n                #recursion\n                c._helper_wrefs(targets) #pylint: disable=protected-access\n            elif isinstance(c, Correction) and c.auth: #recurse into corrections\n                for e in c:\n                    if isinstance(e, AbstractCorrectionChild) and e.auth:\n                        for e2 in e:\n                            if isinstance(e2, AbstractSpanAnnotation):\n                                #recursion\n                                e2._helper_wrefs(targets)", "response": "Internal helper function that recursively walks the list of words and Phoneme objects and adds them to the list of targets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of word references", "response": "def wrefs(self, index = None, recurse=True):\n        \"\"\"Returns a list of word references, these can be Words but also Morphemes or Phonemes.\n\n        Arguments:\n            index (int or None): If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning the list of all\n        \"\"\"\n        targets =[]\n        self._helper_wrefs(targets, recurse)\n        if index is None:\n            return targets\n        else:\n            return targets[index]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addtoindex(self,norecurse=None):\n        if not norecurse: norecurse = (Word, Morpheme, Phoneme)\n        if self.id:\n            self.doc.index[self.id] = self\n        for e in self.data:\n            if all([not isinstance(e, C) for C in norecurse]):\n                try:\n                    e.addtoindex(norecurse)\n                except AttributeError:\n                    pass", "response": "Makes sure this element and all subelements are properly added to the index"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef xml(self, attribs = None,elements = None, skipchildren = False):\n        if self.set is False or self.set is None:\n            if len(self.data) == 0: #just skip if there are no children\n                return None\n            else:\n                raise ValueError(\"No set specified or derivable for annotation layer \" + self.__class__.__name__)\n        return super(AbstractAnnotationLayer, self).xml(attribs, elements, skipchildren)", "response": "Returns the XML representation of this annotation layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef append(self, child, *args, **kwargs):\n        #if no set is associated with the layer yet, we learn it from span annotation elements that are added\n        if self.set is False or self.set is None:\n            if inspect.isclass(child):\n                if issubclass(child,AbstractSpanAnnotation):\n                    if 'set' in kwargs:\n                        self.set = kwargs['set']\n            elif isinstance(child, AbstractSpanAnnotation):\n                if child.set:\n                    self.set = child.set\n            elif isinstance(child, Correction):\n                #descend into corrections to find the proper set for this layer (derived from span annotation elements)\n                for e in itertools.chain( child.new(), child.original(), child.suggestions() ):\n                    if isinstance(e, AbstractSpanAnnotation) and e.set:\n                        self.set = e.set\n                        break\n\n        return super(AbstractAnnotationLayer, self).append(child, *args, **kwargs)", "response": "Append a child to the layer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef alternatives(self, Class=None, set=None):\n\n        for e in self.select(AlternativeLayers,None, True, ['Original','Suggestion']): #pylint: disable=too-many-nested-blocks\n            if Class is None:\n                yield e\n            elif len(e) >= 1: #child elements?\n                for e2 in e:\n                    try:\n                        if isinstance(e2, Class):\n                            try:\n                                if set is None or e2.set == set:\n                                    yield e #not e2\n                                    break #yield an alternative only once (in case there are multiple matches)\n                            except AttributeError:\n                                continue\n                    except AttributeError:\n                        continue", "response": "Generator over alternatives of a specific annotation type and optionally restrained by set."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the span element which spans over the specified words or morphemes.", "response": "def findspan(self, *words):\n        \"\"\"Returns the span element which spans over the specified words or morphemes.\n\n        See also:\n            :meth:`Word.findspans`\n        \"\"\"\n\n        for span in self.select(AbstractSpanAnnotation,None,True):\n            if tuple(span.wrefs()) == words:\n                return span\n        raise NoSuchAnnotation"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef relaxng(cls, includechildren=True,extraattribs = None, extraelements=None, origclass = None):\n        E = ElementMaker(namespace=\"http://relaxng.org/ns/structure/1.0\",nsmap={None:'http://relaxng.org/ns/structure/1.0' , 'folia': \"http://ilk.uvt.nl/folia\", 'xml' : \"http://www.w3.org/XML/1998/namespace\",'a':\"http://relaxng.org/ns/annotation/0.9\" })\n        if not extraattribs:\n            extraattribs = []\n        extraattribs.append(E.optional(E.attribute(E.text(), name='set')) )\n        return AbstractElement.relaxng(includechildren, extraattribs, extraelements, cls)", "response": "Returns a RelaxNG definition for this element."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes the correction define new corrected annotations?", "response": "def hasnew(self,allowempty=False):\n        \"\"\"Does the correction define new corrected annotations?\"\"\"\n        for e in  self.select(New,None,False, False):\n            if not allowempty and len(e) == 0: continue\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hasoriginal(self,allowempty=False):\n        for e in self.select(Original,None,False, False):\n            if not allowempty and len(e) == 0: continue\n            return True\n        return False", "response": "Does the correction record the old annotations prior to correction?"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hascurrent(self, allowempty=False):\n        for e in self.select(Current,None,False, False):\n            if not allowempty and len(e) == 0: continue\n            return True\n        return False", "response": "Does the correction record the current authoritative annotation?"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hassuggestions(self,allowempty=False):\n        for e in self.select(Suggestion,None,False, False):\n            if not allowempty and len(e) == 0: continue\n            return True\n        return False", "response": "Does the correction propose suggestions for correction?"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the text content of the element.", "response": "def textcontent(self, cls='current', correctionhandling=CorrectionHandling.CURRENT):\n        \"\"\"See :meth:`AbstractElement.textcontent`\"\"\"\n        if cls == 'original': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, New) or isinstance(e, Current):\n                    return e.textcontent(cls,correctionhandling)\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, Original):\n                    return e.textcontent(cls,correctionhandling)\n        raise NoSuchText"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the phoncontent of the element.", "response": "def phoncontent(self, cls='current', correctionhandling=CorrectionHandling.CURRENT):\n        \"\"\"See :meth:`AbstractElement.phoncontent`\"\"\"\n        if cls == 'original': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, New) or isinstance(e, Current):\n                    return e.phoncontent(cls, correctionhandling)\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, Original):\n                    return e.phoncontent(cls, correctionhandling)\n        raise NoSuchPhon"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the class is current False otherwise.", "response": "def hastext(self, cls='current',strict=True, correctionhandling=CorrectionHandling.CURRENT):\n        \"\"\"See :meth:`AbstractElement.hastext`\"\"\"\n        if cls == 'original': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, New) or isinstance(e, Current):\n                    return e.hastext(cls,strict, correctionhandling)\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, Original):\n                    return e.hastext(cls,strict, correctionhandling)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the text of the element in the current language.", "response": "def text(self, cls = 'current', retaintokenisation=False, previousdelimiter=\"\",strict=False, correctionhandling=CorrectionHandling.CURRENT, normalize_spaces=False):\n        \"\"\"See :meth:`AbstractElement.text`\"\"\"\n        if cls == 'original': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, New) or isinstance(e, Current):\n                    s = previousdelimiter + e.text(cls, retaintokenisation,\"\", strict, correctionhandling)\n                    if normalize_spaces:\n                        return norm_spaces(s)\n                    else:\n                        return s\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, Original):\n                    s =  previousdelimiter + e.text(cls, retaintokenisation,\"\", strict, correctionhandling)\n                    if normalize_spaces:\n                        return norm_spaces(s)\n                    else:\n                        return s\n        raise NoSuchText"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef phon(self, cls = 'current', previousdelimiter=\"\",strict=False, correctionhandling=CorrectionHandling.CURRENT):\n        if cls == 'original': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, New) or isinstance(e, Current):\n                    return previousdelimiter + e.phon(cls, \"\", strict, correctionhandling)\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, Original):\n                    return previousdelimiter + e.phon(cls, \"\", correctionhandling)\n        raise NoSuchPhon", "response": "Returns the phonical name of the current or original element."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the gettext delimiter for the current or new elements.", "response": "def gettextdelimiter(self, retaintokenisation=False):\n        \"\"\"See :meth:`AbstractElement.gettextdelimiter`\"\"\"\n        for e in self:\n            if isinstance(e, New) or isinstance(e, Current):\n                return e.gettextdelimiter(retaintokenisation)\n        return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the new corrected annotation.", "response": "def new(self,index = None):\n        \"\"\"Get the new corrected annotation.\n\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\n\n        Returns:\n            an annotation element (:class:`AbstractElement`)\n\n        Raises:\n            :class:`NoSuchAnnotation`\n        \"\"\"\n\n        if index is None:\n            try:\n                return next(self.select(New,None,False))\n            except StopIteration:\n                raise NoSuchAnnotation\n        else:\n            for e in self.select(New,None,False):\n                return e[index]\n            raise NoSuchAnnotation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the old annotation prior to correction.", "response": "def original(self,index=None):\n        \"\"\"Get the old annotation prior to correction.\n\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\n\n        Returns:\n            an annotation element (:class:`AbstractElement`)\n\n        Raises:\n            :class:`NoSuchAnnotation`\n        \"\"\"\n        if index is None:\n            try:\n                return next(self.select(Original,None,False, False))\n            except StopIteration:\n                raise NoSuchAnnotation\n        else:\n            for e in self.select(Original,None,False, False):\n                return e[index]\n            raise NoSuchAnnotation"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef current(self,index=None):\n        if index is None:\n            try:\n                return next(self.select(Current,None,False))\n            except StopIteration:\n                raise NoSuchAnnotation\n        else:\n            for e in self.select(Current,None,False):\n                return e[index]\n            raise NoSuchAnnotation", "response": "Get the current authoritative annotation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget suggestions for correction.", "response": "def suggestions(self,index=None):\n        \"\"\"Get suggestions for correction.\n\n        Yields:\n            :class:`Suggestion` element that encapsulate the suggested annotations (if index is ``None``, default)\n\n        Returns:\n            a :class:`Suggestion` element that encapsulate the suggested annotations (if index is set)\n\n        Raises:\n            :class:`IndexError`\n        \"\"\"\n        if index is None:\n            return self.select(Suggestion,None,False, False)\n        else:\n            for i, e in enumerate(self.select(Suggestion,None,False, False)):\n                if index == i:\n                    return e\n            raise IndexError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting elements of the specified class from the subdoc.", "response": "def select(self, Class, set=None, recursive=True,  ignore=True, node=None):\n        \"\"\"See :meth:`AbstractElement.select`\"\"\"\n        if self.include:\n            return self.subdoc.data[0].select(Class,set,recursive, ignore, node) #pass it on to the text node of the subdoc\n        else:\n            return iter([])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialise the FoLiA element to XML by returning an XML Element for this element and all its children. For string output consider the xmlstring method instead.", "response": "def xml(self, attribs = None,elements = None, skipchildren = False):\n        \"\"\"Serialises the FoLiA element to XML, by returning an XML Element (in lxml.etree) for this element and all its children. For string output, consider the xmlstring() method instead.\"\"\"\n        E = ElementMaker(namespace=NSFOLIA,nsmap={None: NSFOLIA, 'xml' : \"http://www.w3.org/XML/1998/namespace\"})\n\n        if not attribs: attribs = {}\n        if not elements: elements = []\n\n        if self.id:\n            attribs['id'] = self.id\n            try:\n                w = self.doc[self.id]\n                attribs['t'] = w.text()\n            except KeyError:\n                pass\n\n        e  = makeelement(E, '{' + NSFOLIA + '}' + self.XMLTAG, **attribs)\n        return e"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef annotation(self, type, set=None):\n        l = self.count(type,set,True,default_ignore_annotations)\n        if len(l) >= 1:\n            return l[0]\n        else:\n            raise NoSuchAnnotation()", "response": "Will return a single annotation if there are multiple. Raises a NoSuchAnnotation exception if there are multiple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind spans of the specified type that include this word", "response": "def findspans(self, type,set=None):\n        \"\"\"Find span annotation of the specified type that include this word\"\"\"\n        if issubclass(type, AbstractAnnotationLayer):\n            layerclass = type\n        else:\n            layerclass = ANNOTATIONTYPE2LAYERCLASS[type.ANNOTATIONTYPE]\n        e = self\n        while True:\n            if not e.parent: break\n            e = e.parent\n            for layer in e.select(layerclass,set,False):\n                for e2 in layer:\n                    if isinstance(e2, AbstractSpanAnnotation):\n                        if self in e2.wrefs():\n                            yield e2"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef correctwords(self, originalwords, newwords, **kwargs):\n        for w in originalwords:\n            if not isinstance(w, Word):\n                raise Exception(\"Original word is not a Word instance: \" + str(type(w)))\n            elif w.sentence() != self:\n                raise Exception(\"Original not found as member of sentence!\")\n        for w in newwords:\n            if not isinstance(w, Word):\n                raise Exception(\"New word is not a Word instance: \" + str(type(w)))\n        if 'suggest' in kwargs and kwargs['suggest']:\n            del kwargs['suggest']\n            return self.correct(suggestion=newwords,current=originalwords, **kwargs)\n        else:\n            return self.correct(original=originalwords, new=newwords, **kwargs)", "response": "Generic correction method for words."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting a word into two words.", "response": "def splitword(self, originalword, *newwords, **kwargs):\n        \"\"\"TODO: Write documentation\"\"\"\n        if isstring(originalword):\n            originalword = self.doc[u(originalword)]\n        return self.correctwords([originalword], newwords, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mergewords(self, newword, *originalwords, **kwargs):\n        return self.correctwords(originalwords, [newword], **kwargs)", "response": "This function is used to correct the mergewords of the original words."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deleteword(self, word, **kwargs):\n        if isstring(word):\n            word = self.doc[u(word)]\n        return self.correctwords([word], [], **kwargs)", "response": "Delete a word from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insertwordleft(self, newword, nextword, **kwargs):\n        if nextword:\n            if isstring(nextword):\n                nextword = self.doc[u(nextword)]\n            if not nextword in self or not isinstance(nextword, Word):\n                raise Exception(\"Next word not found or not instance of Word!\")\n            if isinstance(newword, list) or isinstance(newword, tuple):\n                if not all([ isinstance(x, Word) for x in newword ]):\n                    raise Exception(\"New word (iterable) constains non-Word instances!\")\n            elif not isinstance(newword, Word):\n                raise Exception(\"New word no instance of Word!\")\n\n            kwargs['insertindex'] = self.getindex(nextword)\n        else:\n            kwargs['insertindex'] = 0\n        kwargs['nooriginal'] = True\n        if isinstance(newword, list) or isinstance(newword, tuple):\n            return self.correctwords([], newword, **kwargs)\n        else:\n            return self.correctwords([], [newword], **kwargs)", "response": "Inserts a word as a correction before an existing word."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve(self,size, distribution):\n        if not self.variablesize():\n            raise Exception(\"Can only resize patterns with * wildcards\")\n\n        nrofwildcards = 0\n        for x in self.sequence:\n            if x == '*':\n                nrofwildcards += 1\n\n        assert (len(distribution) == nrofwildcards)\n\n        wildcardnr = 0\n        newsequence = []\n        for x in self.sequence:\n            if x == '*':\n                newsequence += [True] * distribution[wildcardnr]\n                wildcardnr += 1\n            else:\n                newsequence.append(x)\n        d = { 'matchannotation':self.matchannotation, 'matchannotationset':self.matchannotationset, 'casesensitive':self.casesensitive }\n        yield Pattern(*newsequence, **d )", "response": "Resolve a variable sized pattern to all patterns of a certain fixed size"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(self, filename):\n        #if LXE and self.mode != Mode.XPATH:\n        #    #workaround for xml:id problem (disabled)\n        #    #f = open(filename)\n        #    #s = f.read().replace(' xml:id=', ' id=')\n        #    #f.close()\n        #    self.tree = ElementTree.parse(filename)\n        #else:\n        self.tree = xmltreefromfile(filename)\n        self.parsexml(self.tree.getroot())\n        if self.mode != Mode.XPATH:\n            #XML Tree is now obsolete (only needed when partially loaded for xpath queries)\n            self.tree = None", "response": "Loads a FoLiA XML file into the internal object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef items(self):\n        l = []\n        for e in self.data:\n            l += e.items()\n        return l", "response": "Returns a depth - first flat list of all items in the document"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun Xpath expression and parse the resulting elements.", "response": "def xpath(self, query):\n        \"\"\"Run Xpath expression and parse the resulting elements. Don't forget to use the FoLiA namesapace in your expressions, using folia: or the short form f: \"\"\"\n        for result in self.tree.xpath(query,namespaces={'f': 'http://ilk.uvt.nl/folia','folia': 'http://ilk.uvt.nl/folia' }):\n            yield self.parsexml(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the alias for a set if applicable returns the unaltered set otherwise iff fallback is enabled", "response": "def alias(self, annotationtype, set, fallback=False):\n        \"\"\"Return the alias for a set (if applicable, returns the unaltered set otherwise iff fallback is enabled)\"\"\"\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\n        if annotationtype in self.set_alias and set in self.set_alias[annotationtype]:\n            return self.set_alias[annotationtype][set]\n        elif fallback:\n            return set\n        else:\n            raise KeyError(\"No alias for set \" + set)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the set for an alias", "response": "def unalias(self, annotationtype, alias):\n        \"\"\"Return the set for an alias (if applicable, raises an exception otherwise)\"\"\"\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\n        return self.alias_set[annotationtype][alias]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the document to file.", "response": "def save(self, filename=None):\n        \"\"\"Save the document to file.\n\n        Arguments:\n            * filename (str): The filename to save to. If not set (``None``, default), saves to the same file as loaded from.\n        \"\"\"\n        if not filename:\n            filename = self.filename\n        if not filename:\n            raise Exception(\"No filename specified\")\n        if filename[-4:].lower() == '.bz2':\n            f = bz2.BZ2File(filename,'wb')\n            f.write(self.xmlstring().encode('utf-8'))\n            f.close()\n        elif filename[-3:].lower() == '.gz':\n            f = gzip.GzipFile(filename,'wb') #pylint: disable=redefined-variable-type\n            f.write(self.xmlstring().encode('utf-8'))\n            f.close()\n        else:\n            f = io.open(filename,'w',encoding='utf-8')\n            f.write(self.xmlstring())\n            f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef append(self,text):\n        if text is Text:\n            text = Text(self, id=self.id + '.text.' + str(len(self.data)+1) )\n        elif text is Speech:\n            text = Speech(self, id=self.id + '.speech.' + str(len(self.data)+1) ) #pylint: disable=redefined-variable-type\n        else:\n            assert isinstance(text, Text) or isinstance(text, Speech)\n        self.data.append(text)\n        return text", "response": "Add a text or speech to the document."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all declarations in a form ready to be serialised to JSON.", "response": "def jsondeclarations(self):\n        \"\"\"Return all declarations in a form ready to be serialised to JSON.\n\n        Returns:\n            list of dict\n        \"\"\"\n        l = []\n        for annotationtype, set in self.annotations:\n            label = None\n            #Find the 'label' for the declarations dynamically (aka: AnnotationType --> String)\n            for key, value in vars(AnnotationType).items():\n                if value == annotationtype:\n                    label = key\n                    break\n            #gather attribs\n\n            if (annotationtype == AnnotationType.TEXT or annotationtype == AnnotationType.PHON) and set == 'undefined' and len(self.annotationdefaults[annotationtype][set]) == 0:\n                #this is the implicit TextContent declaration, no need to output it explicitly\n                continue\n\n            jsonnode = {'annotationtype': label.lower()}\n            if set and set != 'undefined':\n                jsonnode['set'] = set\n\n\n            for key, value in self.annotationdefaults[annotationtype][set].items():\n                if key == 'annotatortype':\n                    if value == AnnotatorType.MANUAL:\n                        jsonnode[key] = 'manual'\n                    elif value == AnnotatorType.AUTO:\n                        jsonnode[key] = 'auto'\n                elif key == 'datetime':\n                    jsonnode[key] = value.strftime(\"%Y-%m-%dT%H:%M:%S\") #proper iso-formatting\n                elif value:\n                    jsonnode[key] = value\n            if label:\n                l.append( jsonnode  )\n            else:\n                raise Exception(\"Invalid annotation type\")\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xml(self):\n\n        self.pendingvalidation()\n\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={'xml' : \"http://www.w3.org/XML/1998/namespace\", 'xlink':\"http://www.w3.org/1999/xlink\"})\n        attribs = {}\n        attribs['{http://www.w3.org/XML/1998/namespace}id'] = self.id\n\n        #if self.version:\n        #    attribs['version'] = self.version\n        #else:\n        attribs['version'] = FOLIAVERSION\n\n        attribs['generator'] = 'pynlpl.formats.folia-v' + LIBVERSION\n\n        metadataattribs = {}\n        metadataattribs['{' + NSFOLIA + '}type'] = self.metadatatype\n\n        if isinstance(self.metadata, ExternalMetaData):\n            metadataattribs['{' + NSFOLIA + '}src'] = self.metadata.url\n\n        e = E.FoLiA(\n            E.metadata(\n                E.annotations(\n                    *self.xmldeclarations()\n                ),\n                *self.xmlmetadata(),\n                **metadataattribs\n            )\n            , **attribs)\n        for text in self.data:\n            e.append(text.xml())\n        return e", "response": "Serialise the document to XML."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nserialising the document to a dict ready for serialisation to JSON.", "response": "def json(self):\n        \"\"\"Serialise the document to a ``dict`` ready for serialisation to JSON.\n\n        Example::\n\n            import json\n            jsondoc = json.dumps(doc.json())\n        \"\"\"\n        self.pendingvalidation()\n\n        jsondoc = {'id': self.id, 'children': [], 'declarations': self.jsondeclarations() }\n        if self.version:\n            jsondoc['version'] = self.version\n        else:\n            jsondoc['version'] = FOLIAVERSION\n        jsondoc['generator'] = 'pynlpl.formats.folia-v' + LIBVERSION\n\n        for text in self.data:\n            jsondoc['children'].append(text.json())\n        return jsondoc"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef xmlmetadata(self):\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={None: \"http://ilk.uvt.nl/folia\", 'xml' : \"http://www.w3.org/XML/1998/namespace\"})\n        elements = []\n        if self.metadatatype == \"native\":\n            if isinstance(self.metadata, NativeMetaData):\n                for key, value in self.metadata.items():\n                    elements.append(E.meta(value,id=key) )\n        else:\n            if isinstance(self.metadata, ForeignData):\n                #in-document\n                m = self.metadata\n                while m is not None:\n                    elements.append(m.xml())\n                    m = m.next\n        for metadata_id, submetadata in self.submetadata.items():\n            subelements = []\n            attribs = {\n                \"{http://www.w3.org/XML/1998/namespace}id\": metadata_id,\n                \"type\": self.submetadatatype[metadata_id] }\n            if isinstance(submetadata, NativeMetaData):\n                for key, value in submetadata.items():\n                    subelements.append(E.meta(value,id=key) )\n            elif isinstance(submetadata, ExternalMetaData):\n                attribs['src'] = submetadata.url\n            elif isinstance(submetadata, ForeignData):\n                #in-document\n                m = submetadata\n                while m is not None:\n                    subelements.append(m.xml())\n                    m = m.next\n            elements.append( E.submetadata(*subelements, **attribs))\n        return elements", "response": "Internal method to serialize metadata to XML"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parsexmldeclarations(self, node):\n        if self.debug >= 1:\n            print(\"[PyNLPl FoLiA DEBUG] Processing Annotation Declarations\",file=stderr)\n        self.declareprocessed = True\n        for subnode in node: #pylint: disable=too-many-nested-blocks\n            if not isinstance(subnode.tag, str): continue\n            if subnode.tag[:25] == '{' + NSFOLIA + '}' and subnode.tag[-11:] == '-annotation':\n                prefix = subnode.tag[25:][:-11]\n                type = None\n                if prefix.upper() in vars(AnnotationType):\n                    type = vars(AnnotationType)[prefix.upper()]\n                else:\n                    raise Exception(\"Unknown declaration: \" + subnode.tag)\n\n                if 'set' in subnode.attrib and subnode.attrib['set']:\n                    set = subnode.attrib['set']\n                else:\n                    set = 'undefined'\n\n                if (type,set) in self.annotations:\n                    if type == AnnotationType.TEXT:\n                        #explicit Text declaration, remove the implicit declaration:\n                        a = []\n                        for t,s in self.annotations:\n                            if not (t == AnnotationType.TEXT and s == 'undefined'):\n                                a.append( (t,s) )\n                        self.annotations = a\n                    #raise ValueError(\"Double declaration of \" + subnode.tag + \", set '\" + set + \"' + is already declared\")    //doubles are okay says Ko\n                else:\n                    self.annotations.append( (type, set) )\n\n                #Load set definition\n                if set and self.loadsetdefinitions and set not in self.setdefinitions:\n                    if set[:7] == \"http://\" or set[:8] == \"https://\" or set[:6] == \"ftp://\":\n                        try:\n                            self.setdefinitions[set] = SetDefinition(set,verbose=self.verbose) #will raise exception on error\n                        except DeepValidationError:\n                            print(\"WARNING: Set \" + set + \" could not be downloaded, ignoring!\",file=sys.stderr) #warning and ignore\n\n                #Set defaults\n                if type in self.annotationdefaults and set in self.annotationdefaults[type]:\n                    #handle duplicate. If ambiguous: remove defaults\n                    if 'annotator' in subnode.attrib:\n                        if not ('annotator' in self.annotationdefaults[type][set]):\n                            self.annotationdefaults[type][set]['annotator'] = subnode.attrib['annotator']\n                        elif self.annotationdefaults[type][set]['annotator'] != subnode.attrib['annotator']:\n                            del self.annotationdefaults[type][set]['annotator']\n                    if 'annotatortype' in subnode.attrib:\n                        if not ('annotatortype' in self.annotationdefaults[type][set]):\n                            self.annotationdefaults[type][set]['annotatortype'] = subnode.attrib['annotatortype']\n                        elif self.annotationdefaults[type][set]['annotatortype'] != subnode.attrib['annotatortype']:\n                            del self.annotationdefaults[type][set]['annotatortype']\n                else:\n                    defaults = {}\n                    if 'annotator' in subnode.attrib:\n                        defaults['annotator'] = subnode.attrib['annotator']\n                    if 'annotatortype' in subnode.attrib:\n                        if subnode.attrib['annotatortype'] == 'auto':\n                            defaults['annotatortype'] = AnnotatorType.AUTO\n                        else:\n                            defaults['annotatortype'] = AnnotatorType.MANUAL\n                    if 'datetime' in subnode.attrib:\n                        if isinstance(subnode.attrib['datetime'], datetime):\n                            defaults['datetime'] = subnode.attrib['datetime']\n                        else:\n                            defaults['datetime'] = parse_datetime(subnode.attrib['datetime'])\n\n                    if not type in self.annotationdefaults:\n                        self.annotationdefaults[type] = {}\n                    self.annotationdefaults[type][set] = defaults\n\n\n                if 'external' in subnode.attrib and subnode.attrib['external']:\n                    if self.debug >= 1:\n                        print(\"[PyNLPl FoLiA DEBUG] Loading external document: \" + subnode.attrib['external'],file=stderr)\n                    if not type in self.standoffdocs:\n                        self.standoffdocs[type] = {}\n                    self.standoffdocs[type][set] = {}\n\n                    #check if it is already loaded, if multiple references are made to the same doc we reuse the instance\n                    standoffdoc = None\n                    for t in self.standoffdocs:\n                        for s in self.standoffdocs[t]:\n                            for source in self.standoffdocs[t][s]:\n                                if source == subnode.attrib['external']:\n                                    standoffdoc = self.standoffdocs[t][s]\n                                    break\n                            if standoffdoc: break\n                        if standoffdoc: break\n\n                    if not standoffdoc:\n                        if subnode.attrib['external'][:7] == 'http://' or subnode.attrib['external'][:8] == 'https://':\n                            #document is remote, download (in memory)\n                            try:\n                                f = urlopen(subnode.attrib['external'])\n                            except:\n                                raise DeepValidationError(\"Unable to download standoff document: \" + subnode.attrib['external'])\n                            try:\n                                content = u(f.read())\n                            except IOError:\n                                raise DeepValidationError(\"Unable to download standoff document: \" + subnode.attrib['external'])\n                            f.close()\n                            standoffdoc = Document(string=content, parentdoc=self, setdefinitions=self.setdefinitions)\n                        elif os.path.exists(subnode.attrib['external']):\n                            #document is on disk:\n                            standoffdoc = Document(file=subnode.attrib['external'], parentdoc=self, setdefinitions=self.setdefinitions)\n                        else:\n                            #document not found\n                            raise DeepValidationError(\"Unable to find standoff document: \" + subnode.attrib['external'])\n\n                    self.standoffdocs[type][set][subnode.attrib['external']] = standoffdoc\n                    standoffdoc.parentdoc = self\n\n                if self.debug >= 1:\n                    print(\"[PyNLPl FoLiA DEBUG] Found declared annotation \" + subnode.tag + \". Defaults: \" + repr(defaults),file=stderr)", "response": "Internal method to parse XML declarations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the metadata from the IMDI node.", "response": "def setimdi(self, node): #OBSOLETE\n        \"\"\"OBSOLETE\"\"\"\n        ns = {'imdi': 'http://www.mpi.nl/IMDI/Schema/IMDI'}\n        self.metadatatype = MetaDataType.IMDI\n        if LXE:\n            self.metadata = ElementTree.tostring(node, xml_declaration=False, pretty_print=True, encoding='utf-8')\n        else:\n            self.metadata = ElementTree.tostring(node, encoding='utf-8')\n        n = node.xpath('imdi:Session/imdi:Title', namespaces=ns)\n        if n and n[0].text: self._title = n[0].text\n        n = node.xpath('imdi:Session/imdi:Date', namespaces=ns)\n        if n and n[0].text: self._date = n[0].text\n        n = node.xpath('//imdi:Source/imdi:Access/imdi:Publisher', namespaces=ns)\n        if n and n[0].text: self._publisher = n[0].text\n        n = node.xpath('//imdi:Source/imdi:Access/imdi:Availability', namespaces=ns)\n        if n and n[0].text: self._license = n[0].text\n        n = node.xpath('//imdi:Languages/imdi:Language/imdi:ID', namespaces=ns)\n        if n and n[0].text: self._language = n[0].text"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeclares a new annotation type to be used in the document.", "response": "def declare(self, annotationtype, set, **kwargs):\n        \"\"\"Declare a new annotation type to be used in the document.\n\n        Keyword arguments can be used to set defaults for any annotation of this type and set.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n            set (str): the set, should formally be a URL pointing to the set definition\n\n        Keyword Arguments:\n            annotator (str): Sets a default annotator\n            annotatortype: Should be either ``AnnotatorType.MANUAL`` or ``AnnotatorType.AUTO``, indicating whether the annotation was performed manually or by an automated process.\n            datetime (datetime.datetime): Sets the default datetime\n            alias (str): Defines alias that may be used in set attribute of elements instead of the full set name\n\n        Example::\n\n            doc.declare(folia.PosAnnotation, 'http://some/path/brown-tag-set', annotator=\"mytagger\", annotatortype=folia.AnnotatorType.AUTO)\n        \"\"\"\n        if (sys.version > '3' and not isinstance(set,str)) or (sys.version < '3' and not isinstance(set,(str,unicode))):\n            raise ValueError(\"Set parameter for declare() must be a string\")\n\n        if inspect.isclass(annotationtype):\n            annotationtype = annotationtype.ANNOTATIONTYPE\n        if annotationtype in self.alias_set and set in self.alias_set[annotationtype]:\n            raise ValueError(\"Set \" + set + \" conflicts with alias, may not be equal!\")\n        if not (annotationtype, set) in self.annotations:\n            self.annotations.append( (annotationtype,set) )\n            if set and self.loadsetdefinitions and not set in self.setdefinitions:\n                if set[:7] == \"http://\" or set[:8] == \"https://\" or set[:6] == \"ftp://\":\n                    self.setdefinitions[set] = SetDefinition(set,verbose=self.verbose) #will raise exception on error\n        if not annotationtype in self.annotationdefaults:\n            self.annotationdefaults[annotationtype] = {}\n        self.annotationdefaults[annotationtype][set] = kwargs\n        if 'alias' in kwargs:\n            if annotationtype in self.set_alias and set in self.set_alias[annotationtype] and self.set_alias[annotationtype][set] != kwargs['alias']:\n                raise ValueError(\"Redeclaring set \" + set + \" with another alias ('\"+kwargs['alias']+\"') is not allowed!\")\n            if annotationtype in self.alias_set and kwargs['alias'] in self.alias_set[annotationtype] and self.alias_set[annotationtype][kwargs['alias']] != set:\n                raise ValueError(\"Redeclaring alias \" + kwargs['alias'] + \" with another set ('\"+set+\"') is not allowed!\")\n            if annotationtype in self.set_alias and kwargs['alias'] in self.set_alias[annotationtype]:\n                raise ValueError(\"Alias \" + kwargs['alias'] + \" conflicts with set name, may not be equal!\")\n            if annotationtype not in self.alias_set:\n                self.alias_set[annotationtype] = {}\n            if annotationtype not in self.set_alias:\n                self.set_alias[annotationtype] = {}\n            self.alias_set[annotationtype][kwargs['alias']] = set\n            self.set_alias[annotationtype][set] = kwargs['alias']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the annotation type is present in the document.", "response": "def declared(self, annotationtype, set):\n        \"\"\"Checks if the annotation type is present (i.e. declared) in the document.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n            set (str): the set, should formally be a URL pointing to the set definition (aliases are also supported)\n\n        Example::\n\n            if doc.declared(folia.PosAnnotation, 'http://some/path/brown-tag-set'):\n                ..\n\n        Returns:\n            bool\n        \"\"\"\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\n        return ( (annotationtype,set) in self.annotations) or (set in self.alias_set and self.alias_set[set] and (annotationtype, self.alias_set[set]) in self.annotations )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef defaultset(self, annotationtype):\n\n        if inspect.isclass(annotationtype) or isinstance(annotationtype,AbstractElement): annotationtype = annotationtype.ANNOTATIONTYPE\n        try:\n            return list(self.annotationdefaults[annotationtype].keys())[0]\n        except KeyError:\n            raise NoDefaultError\n        except IndexError:\n            raise NoDefaultError", "response": "Obtain the default set for the specified annotation type."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nobtains the default annotator for the specified annotation type and set.", "response": "def defaultannotator(self, annotationtype, set=None):\n        \"\"\"Obtain the default annotator for the specified annotation type and set.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n            set (str): the set, should formally be a URL pointing to the set definition\n\n        Returns:\n            the set (str)\n\n        Raises:\n            :class:`NoDefaultError` if the annotation type does not exist or if there is ambiguity (multiple sets for the same type)\n        \"\"\"\n\n        if inspect.isclass(annotationtype) or isinstance(annotationtype,AbstractElement): annotationtype = annotationtype.ANNOTATIONTYPE\n        if not set: set = self.defaultset(annotationtype)\n        try:\n            return self.annotationdefaults[annotationtype][set]['annotator']\n        except KeyError:\n            raise NoDefaultError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting or set the document s title from the metadata object", "response": "def title(self, value=None):\n        \"\"\"Get or set the document's title from/in the metadata\n\n           No arguments: Get the document's title from metadata\n           Argument: Set the document's title in metadata\n        \"\"\"\n        if not (value is None):\n            if (self.metadatatype == \"native\"):\n                self.metadata['title'] = value\n            else:\n                self._title = value\n        if (self.metadatatype == \"native\"):\n            if 'title' in self.metadata:\n                return self.metadata['title']\n            else:\n                return None\n        else:\n            return self._title"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef date(self, value=None):\n        if not (value is None):\n            if (self.metadatatype == \"native\"):\n                self.metadata['date'] = value\n            else:\n                self._date = value\n        if (self.metadatatype == \"native\"):\n            if 'date' in self.metadata:\n                return self.metadata['date']\n            else:\n                return None\n        else:\n            return self._date", "response": "Get or set the document s date from the metadata."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef publisher(self, value=None):\n        if not (value is None):\n            if (self.metadatatype == \"native\"):\n                self.metadata['publisher'] = value\n            else:\n                self._publisher = value\n        if (self.metadatatype == \"native\"):\n            if 'publisher' in self.metadata:\n                return self.metadata['publisher']\n            else:\n                return None\n        else:\n            return self._publisher", "response": "Get the document s publisher from metadata\n          "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the document s license from metadata Argument is not set", "response": "def license(self, value=None):\n        \"\"\"No arguments: Get the document's license from metadata\n           Argument: Set the document's license in metadata\n        \"\"\"\n        if not (value is None):\n            if (self.metadatatype == \"native\"):\n                self.metadata['license'] = value\n            else:\n                self._license = value\n        if (self.metadatatype == \"native\"):\n            if 'license' in self.metadata:\n                return self.metadata['license']\n            else:\n                return None\n        else:\n            return self._license"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the document s language from metadata", "response": "def language(self, value=None):\n        \"\"\"No arguments: Get the document's language (ISO-639-3) from metadata\n           Argument: Set the document's language (ISO-639-3) in metadata\n        \"\"\"\n        if not (value is None):\n            if (self.metadatatype == \"native\"):\n                self.metadata['language'] = value\n            else:\n                self._language = value\n        if self.metadatatype == \"native\":\n            if 'language' in self.metadata:\n                return self.metadata['language']\n            else:\n                return None\n        else:\n            return self._language"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parsemetadata(self, node):\n\n        if 'type' in node.attrib:\n            self.metadatatype = node.attrib['type']\n        else:\n            #no type specified, default to native\n            self.metadatatype = \"native\"\n\n        if 'src' in node.attrib:\n            self.metadata = ExternalMetaData(node.attrib['src'])\n        elif self.metadatatype == \"native\":\n            self.metadata = NativeMetaData()\n        else:\n            self.metadata = None #may be set below to ForeignData\n\n        for subnode in node:\n            if subnode.tag == '{' + NSFOLIA + '}annotations':\n                self.parsexmldeclarations(subnode)\n            elif subnode.tag == '{' + NSFOLIA + '}meta':\n                if self.metadatatype == \"native\":\n                    if subnode.text:\n                        self.metadata[subnode.attrib['id']] = subnode.text\n                else:\n                    raise MetaDataError(\"Encountered a meta element but metadata type is not native!\")\n            elif subnode.tag == '{' + NSFOLIA + '}provenance':\n                #forward compatibility with FoLiA 2.0; ignore provenance\n                print(\"WARNING: Ignoring provenance data. Use foliapy instead of pynlpl.formats.folia for FoLiA v2.0 compatibility!\",file=sys.stderr)\n                pass\n            elif subnode.tag == '{' + NSFOLIA + '}foreign-data':\n                if self.metadatatype == \"native\":\n                    raise MetaDataError(\"Encountered a foreign-data element but metadata type is native!\")\n                elif self.metadata is not None:\n                    #multiple foreign-data elements, chain:\n                    e = self.metadata\n                    while e.next is not None:\n                        e = e.next\n                    e.next = ForeignData(self, node=subnode)\n                else:\n                    self.metadata = ForeignData(self, node=subnode)\n            elif subnode.tag == '{' + NSFOLIA + '}submetadata':\n                self.parsesubmetadata(subnode)\n            elif subnode.tag == '{http://www.mpi.nl/IMDI/Schema/IMDI}METATRANSCRIPT': #backward-compatibility for old IMDI without foreign-key\n                E = ElementMaker(namespace=NSFOLIA,nsmap={None: NSFOLIA, 'xml' : \"http://www.w3.org/XML/1998/namespace\"})\n                self.metadatatype = \"imdi\"\n                self.metadata = ForeignData(self, node=subnode)", "response": "Internal method to parse the metadata from the XML node node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming any pending validations on the current object.", "response": "def pendingvalidation(self, warnonly=None):\n        \"\"\"Perform any pending validations\n\n        Parameters:\n            warnonly (bool): Warn only (True) or raise exceptions (False). If set to None then this value will be determined based on the document's FoLiA version (Warn only before FoLiA v1.5)\n\n        Returns:\n            bool\n        \"\"\"\n        if self.debug: print(\"[PyNLPl FoLiA DEBUG] Processing pending validations (if any)\",file=stderr)\n\n        if warnonly is None and self and self.version:\n            warnonly = (checkversion(self.version, '1.5.0') < 0) #warn only for documents older than FoLiA v1.5\n        if self.textvalidation:\n            while self.offsetvalidationbuffer:\n                structureelement, textclass = self.offsetvalidationbuffer.pop()\n\n                if self.debug: print(\"[PyNLPl FoLiA DEBUG] Performing offset validation on \" + repr(structureelement) + \" textclass \" + textclass,file=stderr)\n\n                #validate offsets\n                tc = structureelement.textcontent(textclass)\n                if tc.offset is not None:\n                    try:\n                        tc.getreference(validate=True)\n                    except UnresolvableTextContent:\n                        msg = \"Text for \" + structureelement.__class__.__name__ + \", ID \" + str(structureelement.id) + \", textclass \" + textclass  + \", has incorrect offset \" + str(tc.offset) + \" or invalid reference\"\n                        print(\"TEXT VALIDATION ERROR: \" + msg,file=sys.stderr)\n                        if not warnonly:\n                            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an iterator over the elements of the given class.", "response": "def select(self, Class, set=None, recursive=True,  ignore=True):\n        \"\"\"See :meth:`AbstractElement.select`\"\"\"\n        if self.mode == Mode.MEMORY:\n            for t in self.data:\n                if Class.__name__ == 'Text':\n                    yield t\n                else:\n                    for e in t.select(Class,set,recursive,ignore):\n                        yield e"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef count(self, Class, set=None, recursive=True,ignore=True):\n        if self.mode == Mode.MEMORY:\n            s = 0\n            for t in self.data:\n                s +=  sum( 1 for e in t.select(Class,recursive,True ) )\n            return s", "response": "Count the number of elements of a given class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a generator of all paragraphs found in the document.", "response": "def paragraphs(self, index = None):\n        \"\"\"Return a generator of all paragraphs found in the document.\n\n        If an index is specified, return the n'th paragraph only (starting at 0)\"\"\"\n        if index is None:\n            return self.select(Paragraph)\n        else:\n            if index < 0:\n                index = sum(t.count(Paragraph) for t in self.data) + index\n            for t in self.data:\n                for i,e in enumerate(t.select(Paragraph)) :\n                    if i == index:\n                        return e\n            raise IndexError"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sentences(self, index = None):\n        if index is None:\n            return self.select(Sentence,None,True,[Quote])\n        else:\n            if index < 0:\n                index = sum(t.count(Sentence,None,True,[Quote]) for t in self.data) + index\n            for t in self.data:\n                for i,e in enumerate(t.select(Sentence,None,True,[Quote])) :\n                    if i == index:\n                        return e\n            raise IndexError", "response": "Return a generator of all sentences found in the document. Except for sentences in quotes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef text(self, cls='current', retaintokenisation=False):\n\n        #backward compatibility, old versions didn't have cls as first argument, so if a boolean is passed first we interpret it as the 2nd:\n        if cls is True or cls is False:\n            retaintokenisation = cls\n            cls = 'current'\n\n        s = \"\"\n        for c in self.data:\n            if s: s += \"\\n\\n\\n\"\n            try:\n                s += c.text(cls, retaintokenisation)\n            except NoSuchText:\n                continue\n        return s", "response": "Returns the text of the entire document"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _states(self, state, processedstates=[]): #pylint: disable=dangerous-default-value\n        processedstates.append(state)\n\n        for nextstate in state.epsilon:\n            if not nextstate in processedstates:\n                self._states(nextstate, processedstates)\n\n        for _, nextstate in state.transitions:\n            if not nextstate in processedstates:\n                self._states(nextstate, processedstates)\n\n        return processedstates", "response": "Iterate over all states in no particular order"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef searchbest(self):\n        finalsolution = None\n        bestscore = None\n        for solution in self:\n            if bestscore == None:\n                bestscore = solution.score()\n                finalsolution = solution\n            elif self.minimize:\n                score = solution.score()\n                if score < bestscore:\n                    bestscore = score\n                    finalsolution = solution\n            elif not self.minimize:\n                score = solution.score()\n                if score > bestscore:\n                    bestscore = score\n                    finalsolution = solution                \n        return finalsolution", "response": "Returns the single best result if multiple matches have the same score"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef searchtop(self,n=10):\n        solutions = PriorityQueue([], lambda x: x.score, self.minimize, length=n, blockworse=False, blockequal=False,duplicates=False)\n        for solution in self:\n            solutions.append(solution)\n        return solutions", "response": "Return the top n best resulta or possibly less if not enough is found"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the last n results.", "response": "def searchlast(self,n=10):\n        \"\"\"Return the last n results (or possibly less if not found). Note that the last results are not necessarily the best ones! Depending on the search type.\"\"\"            \n        solutions = deque([], n)\n        for solution in self:\n            solutions.append(solution)\n        return solutions"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of synset IDs based on a given lemma", "response": "def get_syn_ids_by_lemma(self, lemma):\n        \"\"\"Returns a list of synset IDs based on a lemma\"\"\"\n        if not isinstance(lemma,unicode):\n            lemma = unicode(lemma,'utf-8')\n\n\n        http, resp, content = self.connect()\n\n        params   = \"\"\n        fragment = \"\"\n\n        path = \"cdb_syn\"\n        if self.debug:\n            printf( \"cornettodb/views/query_remote_syn_lemma: db_opt: %s\" % path )\n\n        query_opt = \"dict_search\"\n        if self.debug:\n            printf( \"cornettodb/views/query_remote_syn_lemma: query_opt: %s\" % query_opt )\n    \n        qdict = {}\n        qdict[ \"action\" ] = \"queryList\"\n        qdict[ \"word\" ]   = lemma.encode('utf-8')\n\n\n        query = urllib.urlencode( qdict )\n\n        db_url_tuple = ( self.scheme, self.host + ':' + str(self.port), path, params, query, fragment )\n        db_url = urlparse.urlunparse( db_url_tuple )\n        if self.debug:\n            printf( \"db_url: %s\" % db_url )\n\n        resp, content = http.request( db_url, \"GET\" )\n        if self.debug:\n            printf( \"resp:\\n%s\" % resp )\n            printf( \"content:\\n%s\" % content )\n        #    printf( \"content is of type: %s\" % type( content ) )\n\n        dict_list = []\n        dict_list = eval( content )        # string to list\n\n        synsets = []\n        items = len( dict_list )\n        if self.debug:\n            printf( \"items: %d\" % items )\n\n        # syn dict: like lu dict, but without pos: part-of-speech\n        for dict in dict_list:\n            if self.debug:\n                printf( dict )\n\n            seq_nr = dict[ \"seq_nr\" ]   # sense number\n            value  = dict[ \"value\" ]    # lexical unit identifier\n            form   = dict[ \"form\" ]     # lemma\n            label  = dict[ \"label\" ]    # label to be shown\n\n            if self.debug:\n                printf( \"seq_nr: %s\" % seq_nr )\n                printf( \"value:  %s\" % value )\n                printf( \"form:   %s\" % form )\n                printf( \"label:  %s\" % label )\n\n            if value != \"\":\n                synsets.append( value )\n\n        return synsets"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls cdb_syn with synset identifier -> returns the synset xml ;", "response": "def get_synset_xml(self,syn_id):\n        \"\"\"\n        call cdb_syn with synset identifier -> returns the synset xml;\n        \"\"\"\n\n        http, resp, content = self.connect()\n\n        params   = \"\"\n        fragment = \"\"\n\n        path = \"cdb_syn\"\n        if self.debug:\n            printf( \"cornettodb/views/query_remote_syn_id: db_opt: %s\" % path )\n\n        # output_opt: plain, html, xml\n        # 'xml' is actually xhtml (with markup), but it is not valid xml!\n        # 'plain' is actually valid xml (without markup)\n        output_opt = \"plain\"\n        if self.debug:\n            printf( \"cornettodb/views/query_remote_syn_id: output_opt: %s\" % output_opt )\n\n        action = \"runQuery\"\n        if self.debug:\n            printf( \"cornettodb/views/query_remote_syn_id: action: %s\" % action )\n            printf( \"cornettodb/views/query_remote_syn_id: query: %s\" % syn_id )\n\n        qdict = {}\n        qdict[ \"action\" ]  = action\n        qdict[ \"query\" ]   = syn_id\n        qdict[ \"outtype\" ] = output_opt\n\n        query = urllib.urlencode( qdict )\n\n        db_url_tuple = ( self.scheme, self.host + ':' + str(self.port), path, params, query, fragment )\n        db_url = urlparse.urlunparse( db_url_tuple )\n        if self.debug:\n            printf( \"db_url: %s\" % db_url )\n\n        resp, content = http.request( db_url, \"GET\" )\n        if self.debug:\n            printf( \"resp:\\n%s\" % resp )\n        #    printf( \"content:\\n%s\" % content )\n        #    printf( \"content is of type: %s\" % type( content ) )        #<type 'str'>\n\n        xml_data = eval( content )\n        return etree.fromstring( xml_data )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_lus_from_synset(self, syn_id):\n\n        root = self.get_synset_xml(syn_id)\n        elem_synonyms = root.find( \".//synonyms\" )\n\n\n        lus = []\n        for elem_synonym in elem_synonyms:\n            synonym_str = elem_synonym.get( \"c_lu_id-previewtext\" )        # get \"c_lu_id-previewtext\" attribute\n            # synonym_str ends with \":<num>\"\n            synonym = synonym_str.split( ':' )[ 0 ].strip()\n            lus.append( (synonym, elem_synonym.get( \"c_lu_id\") ) )\n        return lus", "response": "Returns a list of word lu_id tuples given a synset ID"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a lu_id and synonyms given a synset ID and a lemma", "response": "def get_lu_from_synset(self, syn_id, lemma = None):\n        \"\"\"Returns (lu_id, synonyms=[(word, lu_id)] ) tuple given a synset ID and a lemma\"\"\"\n        if not lemma:\n            return self.get_lus_from_synset(syn_id) #alias\n        if not isinstance(lemma,unicode):\n            lemma = unicode(lemma,'utf-8')\n\n        root = self.get_synset_xml(syn_id)\n        elem_synonyms = root.find( \".//synonyms\" )\n\n        lu_id = None\n        synonyms = []\n        for elem_synonym in elem_synonyms:\n            synonym_str = elem_synonym.get( \"c_lu_id-previewtext\" )        # get \"c_lu_id-previewtext\" attribute\n            # synonym_str ends with \":<num>\"\n            synonym = synonym_str.split( ':' )[ 0 ].strip()\n\n            if synonym != lemma:\n                synonyms.append( (synonym, elem_synonym.get(\"c_lu_id\")) )\n                if self.debug:\n                    printf( \"synonym add: %s\" % synonym )\n            else:\n                lu_id = elem_synonym.get( \"c_lu_id\" )        # get \"c_lu_id\" attribute\n                if self.debug:\n                    printf( \"lu_id: %s\" % lu_id )\n                    printf( \"synonym skip lemma: %s\" % synonym )\n        return lu_id, synonyms"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of all predicted senses", "response": "def senses(self, bestonly=False):\n        \"\"\"Returns a list of all predicted senses\"\"\"\n        l = []\n        for word_id, senses,distance in self:\n            for sense, confidence in senses:\n                if not sense in l: l.append(sense)\n                if bestonly:\n                    break\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the input data and return the Frog output as a list of tuples.", "response": "def process(self,input_data, source_encoding=\"utf-8\", return_unicode = True, oldfrog=False):\n        \"\"\"Receives input_data in the form of a str or unicode object, passes this to the server, with proper consideration for the encodings, and returns the Frog output as a list of tuples: (word,pos,lemma,morphology), each of these is a proper unicode object unless return_unicode is set to False, in which case raw strings will be returned. Return_unicode is no longer optional, it is fixed to True, parameter is still there only for backwards-compatibility.\"\"\"\n        if isinstance(input_data, list) or isinstance(input_data, tuple):\n            input_data = \" \".join(input_data)\n\n\n\n        input_data = u(input_data, source_encoding) #decode (or preferably do this in an earlier stage)\n        input_data = input_data.strip(' \\t\\n')\n\n        s = input_data.encode(self.server_encoding) +b'\\r\\n'\n        if not oldfrog: s += b'EOT\\r\\n'\n        self.socket.sendall(s) #send to socket in desired encoding\n        output = []\n\n        done = False\n        while not done:\n            data = b\"\"\n            while not data.endswith(b'\\n'):\n                moredata = self.socket.recv(self.BUFSIZE)\n                if not moredata: break\n                data += moredata\n\n\n            data = u(data,self.server_encoding)\n\n\n            for line in data.strip(' \\t\\r\\n').split('\\n'):\n                if line == \"READY\":\n                    done = True\n                    break\n                elif line:\n                    line = line.split('\\t') #split on tab\n                    if len(line) > 4 and line[0].isdigit(): #first column is token number\n                        if line[0] == '1' and output:\n                            if self.returnall:\n                                output.append( (None,None,None,None, None,None,None, None) )\n                            else:\n                                output.append( (None,None,None,None) )\n                        fields = line[1:]\n                        parse1=parse2=ner=chunk=\"\"\n                        word,lemma,morph,pos = fields[0:4]\n                        if len(fields) > 5:\n                            ner = fields[5]\n                        if len(fields) > 6:\n                            chunk = fields[6]\n                        if len(fields) >= 8:\n                            parse1 = fields[7]\n                            parse2 = fields[8]\n\n                        if len(fields) < 5:\n                            raise Exception(\"Can't process response line from Frog: \", repr(line), \" got unexpected number of fields \", str(len(fields) + 1))\n\n                        if self.returnall:\n                            output.append( (word,lemma,morph,pos,ner,chunk,parse1,parse2) )\n                        else:\n                            output.append( (word,lemma,morph,pos) )\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef align(self,inputwords, outputwords):\n        alignment = []\n        cursor = 0\n        for inputword in inputwords:\n            if len(outputwords) > cursor and outputwords[cursor] == inputword:\n                alignment.append(cursor)\n                cursor += 1\n            elif len(outputwords) > cursor+1 and outputwords[cursor+1] == inputword:\n                alignment.append(cursor+1)\n                cursor += 2\n            else:\n                alignment.append(None)\n                cursor += 1\n        return alignment", "response": "For each inputword provides the index of the outputword"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the overlap between two sequences.", "response": "def calculate_overlap(haystack, needle, allowpartial=True):\n    \"\"\"Calculate the overlap between two sequences. Yields (overlap, placement) tuples (multiple because there may be multiple overlaps!). The former is the part of the sequence that overlaps, and the latter is -1 if the overlap is on the left side, 0 if it is a subset, 1 if it overlaps on the right side, 2 if its an identical match\"\"\"\n    needle = tuple(needle)\n    haystack = tuple(haystack)\n    solutions = []\n\n    #equality check\n    if needle == haystack:\n        return [(needle, 2)]\n\n    if allowpartial:\n        minl =1\n    else:\n        minl = len(needle)\n\n    for l in range(minl,min(len(needle), len(haystack))+1):\n        #print \"LEFT-DEBUG\", l,\":\", needle[-l:], \" vs \", haystack[:l]\n        #print \"RIGHT-DEBUG\", l,\":\", needle[:l], \" vs \", haystack[-l:]\n        #Search for overlap left (including partial overlap!)\n        if needle[-l:] == haystack[:l]:\n            #print \"LEFT MATCH\"\n            solutions.append( (needle[-l:], -1) )\n        #Search for overlap right (including partial overlap!)\n        if needle[:l] == haystack[-l:]:\n            #print \"RIGHT MATCH\"\n            solutions.append( (needle[:l], 1) )\n\n    if len(needle) <= len(haystack):\n        options = list(iter(Windower(haystack,len(needle),beginmarker=None,endmarker=None)))\n        for option in options[1:-1]:\n            if option == needle:\n                #print \"SUBSET MATCH\"\n                solutions.append( (needle, 0) )\n\n    return solutions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tokenize(text, regexps=TOKENIZERRULES):\n\n    for i,regexp in list(enumerate(regexps)):\n        if isstring(regexp):\n            regexps[i] = re.compile(regexp)\n\n    tokens = []\n    begin = 0\n    for i, c in enumerate(text):\n        if begin > i:\n            continue\n        elif i == begin:\n            m = False\n            for regexp in regexps:\n                m = regexp.findall(text[i:i+300])\n                if m:\n                    tokens.append(m[0])\n                    begin = i + len(m[0])\n                    break\n            if m: continue\n\n        if c in string.punctuation or c in WHITESPACE:\n            prev = text[i-1] if i > 0 else \"\"\n            next = text[i+1] if i < len(text)-1 else \"\"\n\n            if (c == '.' or c == ',') and prev.isdigit() and next.isdigit():\n                #punctuation in between numbers, keep as one token\n                pass\n            elif (c == \"'\" or c == \"`\") and prev.isalpha() and next.isalpha():\n                #quote in between chars, keep...\n                pass\n            elif c not in WHITESPACE and next == c: #group clusters of identical punctuation together\n                continue\n            elif c == '\\r' and prev == '\\n':\n                #ignore\n                begin = i+1\n                continue\n            else:\n                token = text[begin:i]\n                if token: tokens.append(token)\n\n                if c not in WHITESPACE:\n                    tokens.append(c) #anything but spaces and newlines (i.e. punctuation) counts as a token too\n                begin = i + 1 #set the begin cursor\n\n    if begin <= len(text) - 1:\n        token = text[begin:]\n        tokens.append(token)\n\n    return tokens", "response": "Tokenizes a string and returns a list of tokens"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstripping characters with diacritics and return a flat ascii representation", "response": "def strip_accents(s, encoding= 'utf-8'):\n    \"\"\"Strip characters with diacritics and return a flat ascii representation\"\"\"\n    if sys.version < '3':\n        if isinstance(s,unicode):\n           return unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore')\n        else:\n           return unicodedata.normalize('NFKD', unicode(s,encoding)).encode('ASCII', 'ignore')\n    else:\n        if isinstance(s,bytes): s = str(s,encoding)\n        return str(unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore'),'ascii')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a swap operation on a sequence of tokens exhaustively swapping all tokens up to the maximum specified distance. This operation is a subset of all permutations.", "response": "def swap(tokens, maxdist=2):\n    \"\"\"Perform a swap operation on a sequence of tokens, exhaustively swapping all tokens up to the maximum specified distance. This is a subset of all permutations.\"\"\"\n    assert maxdist >= 2\n    tokens = list(tokens)\n    if maxdist > len(tokens):\n        maxdist = len(tokens)\n    l = len(tokens)\n    for i in range(0,l - 1):\n        for permutation in permutations(tokens[i:i+maxdist]):\n            if permutation != tuple(tokens[i:i+maxdist]):\n                newtokens = tokens[:i]\n                newtokens += permutation\n                newtokens += tokens[i+maxdist:]\n                yield newtokens\n        if maxdist == len(tokens):\n            break"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_keyword_in_context(tokens, keyword, contextsize=1):\n    if isinstance(keyword,tuple) and isinstance(keyword,list):\n        l = len(keyword)\n    else:\n        keyword = (keyword,)\n        l = 1\n    n = l + contextsize*2\n    focuspos = contextsize + 1\n    for ngram in Windower(tokens,n,None,None):\n        if ngram[focuspos:focuspos+l] == keyword:\n            yield ngram[:focuspos], ngram[focuspos:focuspos+l],ngram[focuspos+l+1:]", "response": "Find a keyword in a particular sequence of tokens and return the local context."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the next element in line", "response": "def pop(self):\n        \"\"\"Retrieve the next element in line, this will remove it from the queue\"\"\"\n        e = self.data[self.start]\n        self.start += 1\n        if self.start > 5 and self.start > len(self.data)//2:\n            self.data = self.data[self.start:]\n            self.start = 0\n        return e"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append(self, item):\n        f = self.f(item)\n        if callable(f):\n            score = f()\n        else:\n            score = f\n\n        if not self.duplicates:\n            for s, i in self.data:\n                if s == score and item == i:\n                    #item is a duplicate, don't add it\n                    return False\n\n        if self.length and len(self.data) == self.length:\n                #Fixed-length priority queue, abort when queue is full and new item scores worst than worst scoring item.\n                if self.minimize:\n                    worstscore = self.data[-1][0]\n                    if score >= worstscore:\n                        return False\n                else:\n                    worstscore = self.data[0][0]\n                    if score <= worstscore:\n                        return False\n\n        if self.blockworse and self.bestscore != None:\n            if self.minimize:\n                if score > self.bestscore:\n                    return False\n            else:\n                if score < self.bestscore:\n                    return False\n        if self.blockequal and self.bestscore != None:\n            if self.bestscore == score:\n                return False\n        if (self.bestscore == None) or (self.minimize and score < self.bestscore) or (not self.minimize and score > self.bestscore):\n            self.bestscore = score\n        bisect.insort(self.data, (score, item))\n        if self.length:\n            #fixed length queue: queue is now too long, delete worst items\n            while len(self.data) > self.length:\n                if self.minimize:\n                    del self.data[-1]\n                else:\n                    del self.data[0]\n        return True", "response": "Adds an item to the priority queue. Returns True if successfull False if the item was blocked"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the next element in line", "response": "def pop(self):\n        \"\"\"Retrieve the next element in line, this will remove it from the queue\"\"\"\n        if self.minimize:\n            return self.data.pop(0)[1]\n        else:\n            return self.data.pop()[1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef score(self, i):\n        if self.minimize:\n            return self.data[i][0]\n        else:\n            return self.data[(-1 * i) - 1][0]", "response": "Return the score for item x"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprunes all but the first n items", "response": "def prune(self, n):\n        \"\"\"prune all but the first (=best) n items\"\"\"\n        if self.minimize:\n            self.data = self.data[:n]\n        else:\n            self.data = self.data[-1 * n:]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef randomprune(self,n):\n        self.data = random.sample(self.data, n)", "response": "prune down to n items at random disregarding their score"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove all items below or above a certain score from the queue depending on whether minimize is True or False.", "response": "def prunebyscore(self, score, retainequalscore=False):\n        \"\"\"Deletes all items below/above a certain score from the queue, depending on whether minimize is True or False. Note: It is recommended (more efficient) to use blockworse=True / blockequal=True instead! Preventing the addition of 'worse' items.\"\"\"\n        if retainequalscore:\n            if self.minimize:\n                f = lambda x: x[0] <= score\n            else:\n                f = lambda x: x[0] >= score\n        else:\n            if self.minimize:\n                f = lambda x: x[0] < score\n            else:\n                f = lambda x: x[0] > score\n        self.data = filter(f, self.data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef append(self, item):\n        if not isinstance(item, Tree):\n            return ValueError(\"Can only append items of type Tree\")\n        if not self.children: self.children = []\n        item.parent = self\n        self.children.append(item)", "response": "Add an item to the Tree"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef size(self):\n        if self.children:\n            return sum( ( c.size() for c in self.children.values() ) ) + 1\n        else:\n            return 1", "response": "Size is number of nodes under the trie including the current node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over all sentences in the document", "response": "def sentences(self):\n        \"\"\"Iterate over all sentences (sentence_id, sentence) in the document, sentence is a list of 4-tuples (word,id,pos,lemma)\"\"\"\n        prevp = 0\n        prevs = 0\n        sentence = [];\n        sentence_id = \"\"\n        for word, id, pos, lemma in iter(self):\n            try:\n                doc_id, ptype, p, s, w = re.findall('([\\w\\d-]+)\\.(p|head)\\.(\\d+)\\.s\\.(\\d+)\\.w\\.(\\d+)',id)[0]\n                if ((p != prevp) or (s != prevs)) and sentence:\n                    yield sentence_id, sentence\n                    sentence = []\n                    sentence_id = doc_id + '.' + ptype + '.' + str(p) + '.s.' + str(s)\n                prevp = p\n            except IndexError:\n                doc_id, s, w = re.findall('([\\w\\d-]+)\\.s\\.(\\d+)\\.w\\.(\\d+)',id)[0]\n                if s != prevs and sentence:\n                    yield sentence_id, sentence\n                    sentence = []\n                    sentence_id = doc_id + '.s.' + str(s)\n            sentence.append( (word,id,pos,lemma) )\n            prevs = s\n        if sentence:\n            yield sentence_id, sentence"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef paragraphs(self, with_id = False):\n        prevp = 0\n        partext = []\n        for word, id, pos, lemma in iter(self):\n            doc_id, ptype, p, s, w = re.findall('([\\w\\d-]+)\\.(p|head)\\.(\\d+)\\.s\\.(\\d+)\\.w\\.(\\d+)',id)[0]\n            if prevp != p and partext:\n                    yield ( doc_id + \".\" + ptype + \".\" + prevp , \" \".join(partext) )\n                    partext = []\n            partext.append(word)\n            prevp = p\n        if partext:\n            yield (doc_id + \".\" + ptype + \".\" + prevp, \" \".join(partext) )", "response": "Extracts paragraphs returns list of plain - text (!) paragraphs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate(self, formats_dir=\"../formats/\"):\n        #TODO: download XSD from web\n        if self.inline:\n            xmlschema = ElementTree.XMLSchema(ElementTree.parse(StringIO(\"\\n\".join(open(formats_dir+\"dcoi-dsc.xsd\").readlines()))))\n            xmlschema.assertValid(self.tree)\n            #return xmlschema.validate(self)\n        else:\n            xmlschema = ElementTree.XMLSchema(ElementTree.parse(StringIO(\"\\n\".join(open(formats_dir+\"dutchsemcor-standalone.xsd\").readlines()))))\n            xmlschema.assertValid(self.tree)", "response": "checks if the document is valid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef xpath(self, expression):\n        global namespaces\n        return self.tree.xpath(expression, namespaces=namespaces)", "response": "Executes an xpath expression using the correct namespaces"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef align(self, referencewords, datatuple):\n        targetwords = []\n        for i, (word,lemma,postag) in enumerate(zip(datatuple[0],datatuple[1],datatuple[2])):\n            if word:\n                subwords = word.split(\"_\")\n                for w in subwords: #split multiword expressions\n                    targetwords.append( (w, lemma, postag, i, len(subwords) > 1 ) ) #word, lemma, pos, index, multiword? \n\n        referencewords = [ w.lower() for w in referencewords ]          \n        alignment = []\n        for i, referenceword in enumerate(referencewords):\n            found = False\n            best = 0  \n            distance = 999999          \n            for j, (targetword, lemma, pos, index, multiword) in enumerate(targetwords):\n                if referenceword == targetword and abs(i-j) < distance:\n                    found = True\n                    best = j\n                    distance = abs(i-j)\n\n            if found:\n                alignment.append(targetwords[best])\n            else:                \n                alignment.append((None,None,None,None,False)) #no alignment found        \n        \n        return alignment", "response": "align the reference sentence with the tagged data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mainset(self):\n        if self.mainsetcache:\n            return self.mainsetcache\n        set_uri = self.get_set_uri()\n        for row in self.graph.query(\"SELECT ?seturi ?setid ?setlabel ?setopen ?setempty WHERE { ?seturi rdf:type skos:Collection . OPTIONAL { ?seturi skos:notation ?setid } OPTIONAL { ?seturi skos:prefLabel ?setlabel } OPTIONAL { ?seturi fsd:open ?setopen } OPTIONAL { ?seturi fsd:empty ?setempty } FILTER NOT EXISTS { ?y skos:member ?seturi . ?y rdf:type skos:Collection } }\"):\n            self.mainsetcache = {'uri': str(row.seturi), 'id': str(row.setid), 'label': str(row.setlabel) if row.setlabel else \"\", 'open': bool(row.setopen), 'empty': bool(row.setempty) }\n            return self.mainsetcache\n        raise DeepValidationError(\"Unable to find main set (set_uri=\" + str(set_uri)+\"), this should not happen\")", "response": "Returns information regarding the main set"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subset(self, subset_id):\n        if subset_id in self.subsetcache:\n            return self.subsetcache[subset_id]\n        set_uri = self.get_set_uri(subset_id)\n        for row in self.graph.query(\"SELECT ?seturi ?setid ?setlabel ?setopen WHERE { ?seturi rdf:type skos:Collection . OPTIONAL { ?seturi skos:notation ?setid } OPTIONAL { ?seturi skos:prefLabel ?setlabel } OPTIONAL { ?seturi fsd:open ?setopen } FILTER (?seturi = <\" + str(set_uri)+\">) }\"):\n            self.subsetcache[str(row.setid)] = {'uri': str(row.seturi), 'id': str(row.setid), 'label': str(row.setlabel) if row.setlabel else \"\", 'open': bool(row.setopen) }\n            return self.subsetcache[str(row.setid)]\n        raise DeepValidationError(\"Unable to find subset (set_uri=\" + str(set_uri)+\")\")", "response": "Returns information regarding the set"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef orderedclasses(self, set_uri_or_id=None, nestedhierarchy=False):\n        classes = self.classes(set_uri_or_id, nestedhierarchy)\n        for classid in self.classorder(classes):\n            yield classes[classid]", "response": "Higher - order generator function that yields class information in the right order."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of classes for the specified sub - set.", "response": "def classes(self, set_uri_or_id=None, nestedhierarchy=False):\n        \"\"\"Returns a dictionary of classes for the specified (sub)set (if None, default, the main set is selected)\"\"\"\n        if set_uri_or_id and set_uri_or_id.startswith(('http://','https://')):\n            set_uri = set_uri_or_id\n        else:\n            set_uri = self.get_set_uri(set_uri_or_id)\n\n        assert set_uri is not None\n\n        classes= {}\n        uri2idmap = {}\n        for row in self.graph.query(\"SELECT ?classuri ?classid ?classlabel ?parentclass ?seqnr  WHERE { ?classuri rdf:type skos:Concept ; skos:notation ?classid. <\" + str(set_uri) + \"> skos:member ?classuri . OPTIONAL { ?classuri skos:prefLabel ?classlabel } OPTIONAL { ?classuri skos:narrower ?parentclass } OPTIONAL { ?classuri fsd:sequenceNumber ?seqnr } }\"):\n            classinfo = {'uri': str(row.classuri), 'id': str(row.classid),'label': str(row.classlabel) if row.classlabel else \"\" }\n            if nestedhierarchy:\n                uri2idmap[str(row.classuri)] = str(row.classid)\n            if row.parentclass:\n                classinfo['parentclass'] =  str(row.parentclass) #uri\n            if row.seqnr:\n                classinfo['seqnr'] =  int(row.seqnr)\n            classes[str(row.classid)] = classinfo\n\n        if nestedhierarchy:\n            #build hierarchy\n            removekeys = []\n            for classid, classinfo in classes.items():\n                if 'parentclass' in classinfo:\n                    removekeys.append(classid)\n                    parentclassid = uri2idmap[classinfo['parentclass']]\n                    if 'subclasses' not in classes[parentclassid]:\n                        classes[parentclassid]['subclasses'] = {}\n                    classes[parentclassid]['subclasses'][classid] = classinfo\n            for key in removekeys:\n                del classes[key]\n        return classes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef classorder(self,classes):\n        return [ classid for classid, classitem in sorted( ((classid, classitem) for classid, classitem in classes.items() if 'seqnr' in classitem) , key=lambda pair: pair[1]['seqnr'] )] + \\\n               [ classid for classid, classitem in sorted( ((classid, classitem) for classid, classitem in classes.items() if 'seqnr' not in classitem) , key=lambda pair: pair[1]['label'] if 'label' in pair[1] else pair[1]['id']) ]", "response": "Return a list of class IDs in order for presentational purposes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake a variable name from a XML tree.", "response": "def make_varname(tree):\n    \"\"\"\n    <left> tree </left>\n    \"\"\"\n    if tree.tag == 'identifier':\n        return tree.attrib['name']\n\n    if tree.tag in ('string', 'boolean'):\n        return tree.text\n\n    if tree.tag == 'number':\n        return tree.attrib['value']\n\n    if tree.tag in ('property', 'object'):\n        return make_varname(_xpath_one(tree, '*'))\n\n    if tree.tag.endswith('accessor'):\n        kind = tree.tag[:-len('accessor')]\n        obj = make_varname(_xpath_one(tree, 'object'))\n        prop = make_varname(_xpath_one(tree, 'property'))\n        if kind == 'dot':\n            fmt = '%s.%s'\n        elif kind == 'bracket':\n            fmt = '%s[%s]'\n        else:\n            raise ValueError(\"Unknown accessor: %s\" % tree.tag)\n        return fmt % (obj, prop)\n\n    raise ValueError(\"Unknown tag: %s\" % tree.tag)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _users_from_environ(env_prefix=''):\n    auth_string = os.environ.get(env_prefix + 'WSGI_AUTH_CREDENTIALS')\n    if not auth_string:\n        return {}\n\n    result = {}\n    for credentials in auth_string.split('|'):\n        username, password = credentials.split(':', 1)\n        result[username] = password\n    return result", "response": "Get the users from the environment variable WSGI_AUTH_CREDENTIALS."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _exclude_paths_from_environ(env_prefix=''):\n    paths = os.environ.get(env_prefix + 'WSGI_AUTH_EXCLUDE_PATHS')\n    if not paths:\n        return []\n    return paths.split(';')", "response": "Get the list of paths to exclude from the WSGI_AUTH_EXCLUDE_PATHS environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget include paths from environment variables.", "response": "def _include_paths_from_environ(env_prefix=''):\n    \"\"\"Environment value via `/login;/register`\"\"\"\n    paths = os.environ.get(env_prefix + 'WSGI_AUTH_PATHS')\n    if not paths:\n        return []\n    return paths.split(';')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the user is authenticated for the given request.", "response": "def is_authorized(self, request):\n        \"\"\"Check if the user is authenticated for the given request.\n\n        The include_paths and exclude_paths are first checked. If\n        authentication is required then the Authorization HTTP header is\n        checked against the credentials.\n\n        \"\"\"\n        if self._is_request_in_include_path(request):\n            if self._is_request_in_exclude_path(request):\n                return True\n            else:\n                auth = request.authorization\n                if auth and auth[0] == 'Basic':\n                    credentials = b64decode(auth[1]).decode('UTF-8')\n                    username, password = credentials.split(':', 1)\n                    return self._users.get(username) == password\n                else:\n                    return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a login response back to the client.", "response": "def _login(self, environ, start_response):\n        \"\"\"Send a login response back to the client.\"\"\"\n        response = HTTPUnauthorized()\n        response.www_authenticate = ('Basic', {'realm': self._realm})\n        return response(environ, start_response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the request path is in the include paths list.", "response": "def _is_request_in_include_path(self, request):\n        \"\"\"Check if the request path is in the `_include_paths` list.\n\n        If no specific include paths are given then we assume that\n        authentication is required for all paths.\n\n        \"\"\"\n        if self._include_paths:\n            for path in self._include_paths:\n                if request.path.startswith(path):\n                    return True\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _is_request_in_exclude_path(self, request):\n        if self._exclude_paths:\n            for path in self._exclude_paths:\n                if request.path.startswith(path):\n                    return True\n            return False\n        else:\n            return False", "response": "Check if the request path is in the _exclude_paths list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbootstraps prompt_toolkit kwargs or use user defined values.", "response": "def bootstrap_prompt(prompt_kwargs, group):\n    \"\"\"\n    Bootstrap prompt_toolkit kwargs or use user defined values.\n\n    :param prompt_kwargs: The user specified prompt kwargs.\n    \"\"\"\n    prompt_kwargs = prompt_kwargs or {}\n\n    defaults = {\n        \"history\": InMemoryHistory(),\n        \"completer\": ClickCompleter(group),\n        \"message\": u\"> \",\n    }\n\n    for key in defaults:\n        default_value = defaults[key]\n        if key not in prompt_kwargs:\n            prompt_kwargs[key] = default_value\n\n    return prompt_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef repl(  # noqa: C901\n    old_ctx,\n    prompt_kwargs=None,\n    allow_system_commands=True,\n    allow_internal_commands=True,\n):\n    \"\"\"\n    Start an interactive shell. All subcommands are available in it.\n\n    :param old_ctx: The current Click context.\n    :param prompt_kwargs: Parameters passed to\n        :py:func:`prompt_toolkit.shortcuts.prompt`.\n\n    If stdin is not a TTY, no prompt will be printed, but only commands read\n    from stdin.\n\n    \"\"\"\n    # parent should be available, but we're not going to bother if not\n    group_ctx = old_ctx.parent or old_ctx\n    group = group_ctx.command\n    isatty = sys.stdin.isatty()\n\n    # Delete the REPL command from those available, as we don't want to allow\n    # nesting REPLs (note: pass `None` to `pop` as we don't want to error if\n    # REPL command already not present for some reason).\n    repl_command_name = old_ctx.command.name\n    if isinstance(group_ctx.command, click.CommandCollection):\n        available_commands = {\n            cmd_name: cmd_obj\n            for source in group_ctx.command.sources\n            for cmd_name, cmd_obj in source.commands.items()\n        }\n    else:\n        available_commands = group_ctx.command.commands\n    available_commands.pop(repl_command_name, None)\n\n    prompt_kwargs = bootstrap_prompt(prompt_kwargs, group)\n\n    if isatty:\n\n        def get_command():\n            return prompt(**prompt_kwargs)\n\n    else:\n        get_command = sys.stdin.readline\n\n    while True:\n        try:\n            command = get_command()\n        except KeyboardInterrupt:\n            continue\n        except EOFError:\n            break\n\n        if not command:\n            if isatty:\n                continue\n            else:\n                break\n\n        if allow_system_commands and dispatch_repl_commands(command):\n            continue\n\n        if allow_internal_commands:\n            try:\n                result = handle_internal_commands(command)\n                if isinstance(result, six.string_types):\n                    click.echo(result)\n                    continue\n            except ExitReplException:\n                break\n\n        try:\n            args = shlex.split(command)\n        except ValueError as e:\n            click.echo(\"{}: {}\".format(type(e).__name__, e))\n            continue\n\n        try:\n            with group.make_context(None, args, parent=group_ctx) as ctx:\n                group.invoke(ctx)\n                ctx.exit()\n        except click.ClickException as e:\n            e.show()\n        except ClickExit:\n            pass\n        except SystemExit:\n            pass\n        except ExitReplException:\n            break", "response": "Start an interactive shell."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_repl(group, name=\"repl\"):\n    group.command(name=name)(click.pass_context(repl))", "response": "Register repl as sub - command of group."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_internal_commands(command):\n    if command.startswith(\":\"):\n        target = _get_registered_target(command[1:], default=None)\n        if target:\n            return target()", "response": "Run repl - internal commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fit(self, X):\n        D = self._initialize(X)\n        for i in range(self.max_iter):\n            gamma = self._transform(D, X)\n            e = np.linalg.norm(X - gamma.dot(D))\n            if e < self.tol:\n                break\n            D, gamma = self._update_dict(X, D, gamma)\n\n        self.components_ = D\n        return self", "response": "Fits the classifier to obtain the set of components for a set of modules."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_global_id(global_id):\n    '''\n    Takes the \"global ID\" created by toGlobalID, and retuns the type name and ID\n    used to create it.\n    '''\n    unbased_global_id = unbase64(global_id)\n    _type, _id = unbased_global_id.split(':', 1)\n    return _type, _id", "response": "Takes a global ID and returns the type name and ID of the ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a configuration for an id field on a node using to_global_id.", "response": "def global_id_field(type_name, id_fetcher=None):\n    '''\n    Creates the configuration for an id field on a node, using `to_global_id` to\n    construct the ID from the provided typename. The type-specific ID is fetcher\n    by calling id_fetcher on the object, or if not provided, by accessing the `id`\n    property on the object.\n    '''\n    return GraphQLField(\n        GraphQLNonNull(GraphQLID),\n        description='The ID of an object',\n        resolver=lambda obj, args, context, info: to_global_id(\n            type_name or info.parent_type.name,\n            id_fetcher(obj, context, info) if id_fetcher else obj.id\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connection_from_promised_list(data_promise, args=None, **kwargs):\n    '''\n    A version of `connectionFromArray` that takes a promised array, and returns a\n    promised connection.\n    '''\n    return data_promise.then(lambda data: connection_from_list(data, args, **kwargs))", "response": "A version of connectionFromArray that takes a promised array and returns a new connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a slice of an array returns a connection object for use in the GraphQL.", "response": "def connection_from_list_slice(list_slice, args=None, connection_type=None,\n                               edge_type=None, pageinfo_type=None,\n                               slice_start=0, list_length=0, list_slice_length=None):\n    '''\n    Given a slice (subset) of an array, returns a connection object for use in\n    GraphQL.\n    This function is similar to `connectionFromArray`, but is intended for use\n    cases where you know the cardinality of the connection, consider it too large\n    to materialize the entire array, and instead wish pass in a slice of the\n    total result large enough to cover the range specified in `args`.\n    '''\n    connection_type = connection_type or Connection\n    edge_type = edge_type or Edge\n    pageinfo_type = pageinfo_type or PageInfo\n\n    args = args or {}\n\n    before = args.get('before')\n    after = args.get('after')\n    first = args.get('first')\n    last = args.get('last')\n    if list_slice_length is None:\n        list_slice_length = len(list_slice)\n    slice_end = slice_start + list_slice_length\n    before_offset = get_offset_with_default(before, list_length)\n    after_offset = get_offset_with_default(after, -1)\n\n    start_offset = max(\n        slice_start - 1,\n        after_offset,\n        -1\n    ) + 1\n    end_offset = min(\n        slice_end,\n        before_offset,\n        list_length\n    )\n    if isinstance(first, int):\n        end_offset = min(\n            end_offset,\n            start_offset + first\n        )\n    if isinstance(last, int):\n        start_offset = max(\n            start_offset,\n            end_offset - last\n        )\n\n    # If supplied slice is too large, trim it down before mapping over it.\n    _slice = list_slice[\n        max(start_offset - slice_start, 0):\n        list_slice_length - (slice_end - end_offset)\n    ]\n    edges = [\n        edge_type(\n            node=node,\n            cursor=offset_to_cursor(start_offset + i)\n        )\n        for i, node in enumerate(_slice)\n    ]\n\n\n    first_edge_cursor = edges[0].cursor if edges else None\n    last_edge_cursor = edges[-1].cursor if edges else None\n    lower_bound = after_offset + 1 if after else 0\n    upper_bound = before_offset if before else list_length\n\n    return connection_type(\n        edges=edges,\n        page_info=pageinfo_type(\n            start_cursor=first_edge_cursor,\n            end_cursor=last_edge_cursor,\n            has_previous_page=isinstance(last, int) and start_offset > lower_bound,\n            has_next_page=isinstance(first, int) and end_offset < upper_bound\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cursor_for_object_in_connection(data, _object):\n    '''\n    Return the cursor associated with an object in an array.\n    '''\n    if _object not in data:\n        return None\n\n    offset = data.index(_object)\n    return offset_to_cursor(offset)", "response": "Return the cursor associated with an object in an array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_offset_with_default(cursor=None, default_offset=0):\n    '''\n    Given an optional cursor and a default offset, returns the offset\n    to use; if the cursor contains a valid offset, that will be used,\n    otherwise it will be the default.\n    '''\n    if not is_str(cursor):\n        return default_offset\n\n    offset = cursor_to_offset(cursor)\n    try:\n        return int(offset)\n    except:\n        return default_offset", "response": "Returns the offset of the base object that is used for the base object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw(data, size=(600, 400), node_size=2.0, edge_size=0.25,\n         default_node_color=0x5bc0de, default_edge_color=0xaaaaaa, z=100,\n         shader='basic', optimize=True, directed=True, display_html=True,\n         show_save=False):\n    \"\"\"Draws an interactive 3D visualization of the inputted graph.\n\n    Args:\n        data: Either an adjacency list of tuples (ie. [(1,2),...]) or object\n        size: (Optional) Dimensions of visualization, in pixels\n        node_size: (Optional) Defaults to 2.0\n        edge_size: (Optional) Defaults to 0.25\n        default_node_color: (Optional) If loading data without specified\n            'color' properties, this will be used. Default is 0x5bc0de\n        default_edge_color: (Optional) If loading data without specified\n            'color' properties, this will be used. Default is 0xaaaaaa\n        z: (Optional) Starting z position of the camera. Default is 100.\n        shader: (Optional) Specifies shading algorithm to use. Can be 'toon',\n            'basic', 'phong', or 'lambert'. Default is 'basic'.\n        optimize: (Optional) Runs a force-directed layout algorithm on the\n            graph. Default True.\n        directed: (Optional) Includes arrows on edges to indicate direction.\n            Default True.\n        display_html: If True (default), embed the html in a IPython display.\n            If False, return the html as a string.\n        show_save: If True, displays a save icon for rendering graph as an\n            image.\n\n    Inputting an adjacency list into `data` results in a 'default' graph type.\n    For more customization, use the more expressive object format.\n    \"\"\"\n    # Catch errors on string-based input before getting js involved\n    shader_options = ['toon', 'basic', 'phong', 'lambert']\n    if shader not in shader_options:\n        raise Exception('Invalid shader! Please use one of: ' +\n                        ', '.join(shader_options))\n\n    if isinstance(default_edge_color, int):\n        default_edge_color = hex(default_edge_color)\n    if isinstance(default_node_color, int):\n        default_node_color = hex(default_node_color)\n\n    # Guess the input format and handle accordingly\n    if isinstance(data, list):\n        graph = json_formatter.dumps(generate(data, iterations=1))\n    elif isinstance(data, dict):\n        # Convert color hex to string for json handling\n        for node_key in data['nodes']:\n            node = data['nodes'][node_key]\n            if 'color' in node and isinstance(node['color'], int):\n                node['color'] = hex(node['color'])\n        for edge in data['edges']:\n            if 'color' in edge and isinstance(edge['color'], int):\n                edge['color'] = hex(edge['color'])\n        graph = json_formatter.dumps(data)\n    else:\n        # Support both files and strings\n        try:\n            with open(data) as in_file:\n                graph = in_file.read()\n        except:\n            graph = data\n\n    div_id = uuid.uuid4()\n    html = '''<div id=\"graph-%(id)s\"></div>\n           <script type=\"text/javascript\">\n           require.config({baseUrl: '/',\n                             paths: {jgraph: ['%(local)s', '%(remote)s']}});\n           require(['jgraph'], function () {\n               var $d = $('#graph-%(id)s');\n               $d.width(%(w)d); $d.height(%(h)d);\n               $d.jgraph = jQuery.extend({}, jgraph);\n               $d.jgraph.create($d, {nodeSize: %(node_size)f,\n                                     edgeSize: %(edge_size)f,\n                                     defaultNodeColor: '%(node_color)s',\n                                     defaultEdgeColor: '%(edge_color)s',\n                                     shader: '%(shader)s',\n                                     z: %(z)d,\n                                     runOptimization: %(optimize)s,\n                                     directed: %(directed)s,\n                                     showSave: %(show_save)s});\n               $d.jgraph.draw(%(graph)s);\n\n               $d.resizable({\n                   aspectRatio: %(w)d / %(h)d,\n                   resize: function (evt, ui) {\n                       $d.jgraph.renderer.setSize(ui.size.width,\n                                                  ui.size.height);\n                   }\n               });\n           });\n           </script>''' % dict(id=div_id, local=local_path[:-3],\n                               remote=remote_path[:-3], w=size[0], h=size[1],\n                               node_size=node_size, edge_size=edge_size,\n                               node_color=default_node_color,\n                               edge_color=default_edge_color, shader=shader,\n                               z=z, graph=graph,\n                               optimize='true' if optimize else 'false',\n                               directed='true' if directed else 'false',\n                               show_save='true' if show_save else 'false')\n\n    # Execute js and display the results in a div (see script for more)\n    if display_html:\n        display(HTML(html))\n    else:\n        return html", "response": "Draws an interactive 3D visualization of the inputted graph."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate(data, iterations=1000, force_strength=5.0, dampening=0.01,\n             max_velocity=2.0, max_distance=50, is_3d=True):\n    \"\"\"Runs a force-directed algorithm on a graph, returning a data structure.\n\n    Args:\n        data: An adjacency list of tuples (ie. [(1,2),...])\n        iterations: (Optional) Number of FDL iterations to run in coordinate\n            generation\n        force_strength: (Optional) Strength of Coulomb and Hooke forces\n            (edit this to scale the distance between nodes)\n        dampening: (Optional) Multiplier to reduce force applied to nodes\n        max_velocity: (Optional) Maximum distance a node can move in one step\n        max_distance: (Optional) The maximum inter-node distance considered\n        is_3d: (Optional) Generates three-dimensional coordinates\n\n    Outputs a json-serializable Python object. To visualize, pass the output to\n    `jgraph.draw(...)`.\n    \"\"\"\n\n    edges = [{'source': s, 'target': t} for s, t in data]\n    nodes = force_directed_layout.run(edges, iterations, force_strength,\n                                      dampening, max_velocity, max_distance,\n                                      is_3d)\n    return {'edges': edges, 'nodes': nodes}", "response": "Runs a force - directed algorithm on a graph and returns a json - serializable Python object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noutput json without whitespace.", "response": "def compress(obj):\n    \"\"\"Outputs json without whitespace.\"\"\"\n    return json.dumps(obj, sort_keys=True, separators=(',', ':'),\n                      cls=CustomEncoder)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noutputs json with formatting edits + object handling.", "response": "def dumps(obj):\n    \"\"\"Outputs json with formatting edits + object handling.\"\"\"\n    return json.dumps(obj, indent=4, sort_keys=True, cls=CustomEncoder)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode(self, obj):\n        s = super(CustomEncoder, self).encode(obj)\n        # If uncompressed, postprocess for formatting\n        if len(s.splitlines()) > 1:\n            s = self.postprocess(s)\n        return s", "response": "Fired for every object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay each entry on its own line.", "response": "def postprocess(self, json_string):\n        \"\"\"Displays each entry on its own line.\"\"\"\n        is_compressing, is_hash, compressed, spaces = False, False, [], 0\n        for row in json_string.split('\\n'):\n            if is_compressing:\n                if (row[:spaces + 5] == ' ' * (spaces + 4) +\n                        ('\"' if is_hash else '{')):\n                    compressed.append(row.rstrip())\n                elif (len(row) > spaces and row[:spaces] == ' ' * spaces and\n                        re.match('[\\]\\}],?', row[spaces:].rstrip())):\n                    compressed.append(row.rstrip())\n                    is_compressing = False\n                else:\n                    compressed[-1] += ' ' + row.strip()\n            else:\n                compressed.append(row.rstrip())\n                if any(a in row for a in ['edges', 'nodes']):\n                    # Fix to handle issues that arise with empty lists\n                    if '[]' in row:\n                        continue\n                    spaces = sum(1 for _ in takewhile(str.isspace, row))\n                    is_compressing, is_hash = True, '{' in row\n        return '\\n'.join(compressed)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun a force - directed - layout algorithm on the input graph.", "response": "def run(edges, iterations=1000, force_strength=5.0, dampening=0.01,\n        max_velocity=2.0, max_distance=50, is_3d=True):\n    \"\"\"Runs a force-directed-layout algorithm on the input graph.\n\n    iterations - Number of FDL iterations to run in coordinate generation\n    force_strength - Strength of Coulomb and Hooke forces\n                     (edit this to scale the distance between nodes)\n    dampening - Multiplier to reduce force applied to nodes\n    max_velocity - Maximum distance a node can move in one step\n    max_distance - The maximum distance considered for interactions\n    \"\"\"\n\n    # Get a list of node ids from the edge data\n    nodes = set(e['source'] for e in edges) | set(e['target'] for e in edges)\n\n    # Convert to a data-storing object and initialize some values\n    d = 3 if is_3d else 2\n    nodes = {n: {'velocity': [0.0] * d, 'force': [0.0] * d} for n in nodes}\n\n    # Repeat n times (is there a more Pythonic way to do this?)\n    for _ in repeat(None, iterations):\n\n        # Add in Coulomb-esque node-node repulsive forces\n        for node1, node2 in combinations(nodes.values(), 2):\n            _coulomb(node1, node2, force_strength, max_distance)\n\n        # And Hooke-esque edge spring forces\n        for edge in edges:\n            _hooke(nodes[edge['source']], nodes[edge['target']],\n                   force_strength * edge.get('size', 1), max_distance)\n\n        # Move by resultant force\n        for node in nodes.values():\n            # Constrain the force to the bounds specified by input parameter\n            force = [_constrain(dampening * f, -max_velocity, max_velocity)\n                     for f in node['force']]\n            # Update velocities and reset force\n            node['velocity'] = [v + dv\n                                for v, dv in zip(node['velocity'], force)]\n            node['force'] = [0] * d\n\n    # Clean and return\n    for node in nodes.values():\n        del node['force']\n        node['location'] = node['velocity']\n        del node['velocity']\n        # Even if it's 2D, let's specify three dimensions\n        if not is_3d:\n            node['location'] += [0.0]\n    return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _coulomb(n1, n2, k, r):\n    # Get relevant positional data\n    delta = [x2 - x1 for x1, x2 in zip(n1['velocity'], n2['velocity'])]\n    distance = sqrt(sum(d ** 2 for d in delta))\n\n    # If the deltas are too small, use random values to keep things moving\n    if distance < 0.1:\n        delta = [uniform(0.1, 0.2) for _ in repeat(None, 3)]\n        distance = sqrt(sum(d ** 2 for d in delta))\n\n    # If the distance isn't huge (ie. Coulomb is negligible), calculate\n    if distance < r:\n        force = (k / distance) ** 2\n        n1['force'] = [f - force * d for f, d in zip(n1['force'], delta)]\n        n2['force'] = [f + force * d for f, d in zip(n2['force'], delta)]", "response": "Calculates Coulomb forces and updates node data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwipes the entire context.", "response": "def run_step(context):\n    \"\"\"Wipe the entire context.\n\n    Args:\n        Context is a dictionary or dictionary-like.\n        Does not require any specific keys in context.\n    \"\"\"\n    logger.debug(\"started\")\n\n    context.clear()\n    logger.info(f\"Context wiped. New context size: {len(context)}\")\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse input context string and returns context as dictionary.", "response": "def get_parsed_context(context_arg):\n    \"\"\"Parse input context string and returns context as dictionary.\"\"\"\n    assert context_arg, (\"pipeline must be invoked with context arg set. For \"\n                         \"this json parser you're looking for something \"\n                         \"like: \"\n                         \"pypyr pipelinename './myjsonfile.json'\")\n    logger.debug(\"starting\")\n    # open the json file on disk so that you can initialize the dictionary\n    logger.debug(f\"attempting to open file: {context_arg}\")\n    with open(context_arg) as json_file:\n        payload = json.load(json_file)\n\n    logger.debug(f\"json file loaded into context. Count: {len(payload)}\")\n    logger.debug(\"done\")\n    return payload"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_step(context):\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='pathCheck', caller=__name__)\n\n    paths_to_check = context['pathCheck']\n\n    if not paths_to_check:\n        raise KeyInContextHasNoValueError(\"context['pathCheck'] must have a \"\n                                          f\"value for {__name__}.\")\n\n    # pathsToCheck can be a string or a list in case there are multiple paths\n    if isinstance(paths_to_check, list):\n        check_me = paths_to_check\n    else:\n        # assuming it's a str/path at this point\n        check_me = [paths_to_check]\n\n    out = {}\n    total_found = 0\n\n    for path in check_me:\n        logger.debug(f\"checking path: {path}\")\n        formatted_path = context.get_formatted_string(path)\n        found_paths = pypyr.utils.filesystem.get_glob(formatted_path)\n        no_of_paths = len(found_paths)\n        out[path] = {\n            'exists': no_of_paths > 0,\n            'count': no_of_paths,\n            'found': found_paths\n        }\n        total_found = total_found + no_of_paths\n\n    context['pathCheckOut'] = out\n\n    logger.info(f'checked {len(out)} path(s) and found {total_found}')\n    logger.debug(\"done\")", "response": "This function creates a keyCheckOut in pypyr that checks if a file or directory path exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_step(context):\n    logger.debug(\"started\")\n    context.assert_child_key_has_value('fileWriteJson', 'path', __name__)\n\n    out_path = context.get_formatted_string(context['fileWriteJson']['path'])\n    # doing it like this to safeguard against accidentally dumping all context\n    # with potentially sensitive values in it to disk if payload exists but is\n    # None.\n    is_payload_specified = 'payload' in context['fileWriteJson']\n\n    logger.debug(f\"opening destination file for writing: {out_path}\")\n    os.makedirs(os.path.abspath(os.path.dirname(out_path)), exist_ok=True)\n    with open(out_path, 'w') as outfile:\n        if is_payload_specified:\n            payload = context['fileWriteJson']['payload']\n            formatted_iterable = context.get_formatted_iterable(payload)\n        else:\n            formatted_iterable = context.get_formatted_iterable(context)\n\n        json.dump(formatted_iterable, outfile, indent=2, ensure_ascii=False)\n\n    logger.info(f\"formatted context content and wrote to {out_path}\")\n    logger.debug(\"done\")", "response": "Write payload to json file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_step(context):\n    logger.debug(\"started\")\n\n    (pipeline_name,\n     use_parent_context,\n     pipe_arg,\n     skip_parse,\n     raise_error,\n     loader,\n     ) = get_arguments(context)\n\n    try:\n        if use_parent_context:\n            logger.info(f\"pyping {pipeline_name}, using parent context.\")\n            pipelinerunner.load_and_run_pipeline(\n                pipeline_name=pipeline_name,\n                pipeline_context_input=pipe_arg,\n                context=context,\n                parse_input=not skip_parse,\n                loader=loader\n            )\n        else:\n            logger.info(f\"pyping {pipeline_name}, without parent context.\")\n            pipelinerunner.load_and_run_pipeline(\n                pipeline_name=pipeline_name,\n                pipeline_context_input=pipe_arg,\n                working_dir=context.working_dir,\n                parse_input=not skip_parse,\n                loader=loader\n            )\n\n        logger.info(f\"pyped {pipeline_name}.\")\n    except Exception as ex_info:\n        # yes, yes, don't catch Exception. Have to, though, in order to swallow\n        # errs if !raise_error\n        logger.error(f\"Something went wrong pyping {pipeline_name}. \"\n                     f\"{type(ex_info).__name__}: {ex_info}\")\n\n        if raise_error:\n            logger.debug(\"Raising original exception to caller.\")\n            raise\n        else:\n            logger.debug(\n                f\"raiseError is False. Swallowing error in {pipeline_name}.\")\n\n    logger.debug(\"done\")", "response": "Runs another pipeline from this step."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_arguments(context):\n    context.assert_key_has_value(key='pype', caller=__name__)\n    pype = context.get_formatted('pype')\n\n    try:\n        pipeline_name = pype['name']\n\n        if pipeline_name is None:\n            raise KeyInContextHasNoValueError(\n                \"pypyr.steps.pype ['pype']['name'] exists but is empty.\")\n    except KeyError as err:\n        raise KeyNotInContextError(\n            \"pypyr.steps.pype missing 'name' in the 'pype' context item. \"\n            \"You need to specify the pipeline name to run another \"\n            \"pipeline.\") from err\n\n    use_parent_context = pype.get('useParentContext', True)\n    pipe_arg = pype.get('pipeArg', None)\n    skip_parse = pype.get('skipParse', True)\n    raise_error = pype.get('raiseError', True)\n    loader = pype.get('loader', None)\n\n    return (\n        pipeline_name,\n        use_parent_context,\n        pipe_arg,\n        skip_parse,\n        raise_error,\n        loader,\n    )", "response": "Parse arguments for pype from context and assign default values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlook for the pipeline in the various places it could be.", "response": "def get_pipeline_path(pipeline_name, working_directory):\n    \"\"\"Look for the pipeline in the various places it could be.\n\n    First checks the cwd. Then checks pypyr/pipelines dir.\n\n    Args:\n        pipeline_name: string. Name of pipeline to find\n        working_directory: string. Path in which to look for pipeline_name.yaml\n\n    Returns:\n        Absolute path to the pipeline_name.yaml file\n\n    Raises:\n        PipelineNotFoundError: if pipeline_name.yaml not found in working_dir\n                               or in {pypyr install dir}/pipelines.\n\n    \"\"\"\n    logger.debug(\"starting\")\n\n    # look for name.yaml in the pipelines/ sub-directory\n    logger.debug(f\"current directory is {working_directory}\")\n\n    # looking for {cwd}/pipelines/[pipeline_name].yaml\n    pipeline_path = os.path.abspath(os.path.join(\n        working_directory,\n        'pipelines',\n        pipeline_name + '.yaml'))\n\n    if os.path.isfile(pipeline_path):\n        logger.debug(f\"Found {pipeline_path}\")\n    else:\n        logger.debug(f\"{pipeline_name} not found in current \"\n                     \"directory/pipelines folder. Looking in pypyr install \"\n                     \"directory instead.\")\n        pypyr_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n        logger.debug(f\"pypyr installation directory is: {pypyr_dir}\")\n        pipeline_path = os.path.abspath(os.path.join(\n            pypyr_dir,\n            'pipelines',\n            pipeline_name + '.yaml'))\n\n        if os.path.isfile(pipeline_path):\n            logger.debug(f\"Found {pipeline_path}\")\n        else:\n            raise PipelineNotFoundError(f\"{pipeline_name}.yaml not found in \"\n                                        f\"either \"\n                                        f\"{working_directory}/pipelines \"\n                                        f\"or {pypyr_dir}/pipelines\")\n\n    logger.debug(\"done\")\n    return pipeline_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_pipeline_definition(pipeline_name, working_dir):\n    logger.debug(\"starting\")\n\n    pipeline_path = get_pipeline_path(\n        pipeline_name=pipeline_name,\n        working_directory=working_dir)\n\n    logger.debug(f\"Trying to open pipeline at path {pipeline_path}\")\n    try:\n        with open(pipeline_path) as yaml_file:\n            pipeline_definition = pypyr.yaml.get_pipeline_yaml(\n                yaml_file)\n            logger.debug(\n                f\"found {len(pipeline_definition)} stages in pipeline.\")\n    except FileNotFoundError:\n        logger.error(\n            \"The pipeline doesn't exist. Looking for a file here: \"\n            f\"{pipeline_name}.yaml in the /pipelines sub directory.\")\n        raise\n\n    logger.debug(\"pipeline definition loaded\")\n\n    logger.debug(\"done\")\n    return pipeline_definition", "response": "Open and parse the pipeline definition yaml and return the dictionary representing the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_value(self, context):\n        if self.value:\n            return expressions.eval_string(self.value, context)\n        else:\n            # Empty input raises cryptic EOF syntax err, this more human\n            # friendly\n            raise ValueError('!py string expression is empty. It must be a '\n                             'valid python expression instead.')", "response": "Run python eval on the input string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef foreach_loop(self, context):\n        logger.debug(\"starting\")\n\n        # Loop decorators only evaluated once, not for every step repeat\n        # execution.\n        foreach = context.get_formatted_iterable(self.foreach_items)\n\n        foreach_length = len(foreach)\n\n        logger.info(f\"foreach decorator will loop {foreach_length} times.\")\n\n        for i in foreach:\n            logger.info(f\"foreach: running step {i}\")\n            # the iterator must be available to the step when it executes\n            context['i'] = i\n            # conditional operators apply to each iteration, so might be an\n            # iteration run, skips or swallows.\n            self.run_conditional_decorators(context)\n            logger.debug(f\"foreach: done step {i}\")\n\n        logger.debug(f\"foreach decorator looped {foreach_length} times.\")\n        logger.debug(\"done\")", "response": "Run once for each item in foreach_items."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef invoke_step(self, context):\n        logger.debug(\"starting\")\n\n        logger.debug(f\"running step {self.module}\")\n\n        self.run_step_function(context)\n\n        logger.debug(f\"step {self.module} done\")", "response": "Invoke the run_step function in the dynamically loaded step module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nevaluates the step decorators to decide whether to run step or not.", "response": "def run_conditional_decorators(self, context):\n        \"\"\"Evaluate the step decorators to decide whether to run step or not.\n\n        Use pypyr.dsl.Step.run_step if you intend on executing the step the\n        same way pypyr does.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.\n        \"\"\"\n        logger.debug(\"starting\")\n\n        # The decorator attributes might contain formatting expressions that\n        # change whether they evaluate True or False, thus apply formatting at\n        # last possible instant.\n        run_me = context.get_formatted_as_type(self.run_me, out_type=bool)\n        skip_me = context.get_formatted_as_type(self.skip_me, out_type=bool)\n        swallow_me = context.get_formatted_as_type(self.swallow_me,\n                                                   out_type=bool)\n\n        if run_me:\n            if not skip_me:\n                try:\n                    if self.retry_decorator:\n                        self.retry_decorator.retry_loop(context,\n                                                        self.invoke_step)\n                    else:\n                        self.invoke_step(context=context)\n                except Exception as ex_info:\n                    if swallow_me:\n                        logger.error(\n                            f\"{self.name} Ignoring error because swallow \"\n                            \"is True for this step.\\n\"\n                            f\"{type(ex_info).__name__}: {ex_info}\")\n                    else:\n                        raise\n            else:\n                logger.info(\n                    f\"{self.name} not running because skip is True.\")\n        else:\n            logger.info(f\"{self.name} not running because run is False.\")\n\n        logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the foreach sequence or the conditional evaluation.", "response": "def run_foreach_or_conditional(self, context):\n        \"\"\"Run the foreach sequence or the conditional evaluation.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.\n        \"\"\"\n        logger.debug(\"starting\")\n        # friendly reminder [] list obj (i.e empty) evals False\n        if self.foreach_items:\n            self.foreach_loop(context)\n        else:\n            # since no looping required, don't pollute output with looping info\n            self.run_conditional_decorators(context)\n\n        logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_step(self, context):\n        logger.debug(\"starting\")\n        # the in params should be added to context before step execution.\n        self.set_step_input_context(context)\n\n        if self.while_decorator:\n            self.while_decorator.while_loop(context,\n                                            self.run_foreach_or_conditional)\n        else:\n            self.run_foreach_or_conditional(context)\n\n        logger.debug(\"done\")", "response": "Run a single pipeline step."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_step_input_context(self, context):\n        logger.debug(\"starting\")\n        if self.in_parameters is not None:\n            parameter_count = len(self.in_parameters)\n            if parameter_count > 0:\n                logger.debug(\n                    f\"Updating context with {parameter_count} 'in' \"\n                    \"parameters.\")\n                context.update(self.in_parameters)\n\n        logger.debug(\"done\")", "response": "Set the input context for this step."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a single retry iteration. This method abides by the signature invoked by poll.while_until_true, which is to say (counter, *args, **kwargs). In a normal execution chain, this method's args passed by self.retry_loop where context and step_method set. while_until_true injects counter as a 1st arg. Args: counter. int. loop counter, which number of iteration is this. context: (pypyr.context.Context) The pypyr context. This arg will mutate - after method execution will contain the new updated context. step_method: (method/function) This is the method/function that will execute on every loop iteration. Signature is: function(context) Returns: bool. True if step execution completed without error. False if error occured during step execution.", "response": "def exec_iteration(self, counter, context, step_method):\n        \"\"\"Run a single retry iteration.\n\n        This method abides by the signature invoked by poll.while_until_true,\n        which is to say (counter, *args, **kwargs). In a normal execution\n        chain, this method's args passed by self.retry_loop where context\n        and step_method set. while_until_true injects counter as a 1st arg.\n\n        Args:\n            counter. int. loop counter, which number of iteration is this.\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)\n\n         Returns:\n            bool. True if step execution completed without error.\n                  False if error occured during step execution.\n\n        \"\"\"\n        logger.debug(\"starting\")\n        context['retryCounter'] = counter\n\n        logger.info(f\"retry: running step with counter {counter}\")\n        try:\n            step_method(context)\n            result = True\n        except Exception as ex_info:\n            if self.max:\n                if counter == self.max:\n                    logger.debug(f\"retry: max {counter} retries exhausted. \"\n                                 \"raising error.\")\n                    # arguably shouldn't be using errs for control of flow.\n                    # but would lose the err info if not, so lesser of 2 evils.\n                    raise\n\n            if self.stop_on or self.retry_on:\n                error_name = get_error_name(ex_info)\n                if self.stop_on:\n                    formatted_stop_list = context.get_formatted_iterable(\n                        self.stop_on)\n                    if error_name in formatted_stop_list:\n                        logger.error(f\"{error_name} in stopOn. Raising error \"\n                                     \"and exiting retry.\")\n                        raise\n                    else:\n                        logger.debug(f\"{error_name} not in stopOn. Continue.\")\n\n                if self.retry_on:\n                    formatted_retry_list = context.get_formatted_iterable(\n                        self.retry_on)\n                    if error_name not in formatted_retry_list:\n                        logger.error(f\"{error_name} not in retryOn. Raising \"\n                                     \"error and exiting retry.\")\n                        raise\n                    else:\n                        logger.debug(f\"{error_name} in retryOn. Retry again.\")\n\n            result = False\n            logger.error(f\"retry: ignoring error because retryCounter < max.\\n\"\n                         f\"{type(ex_info).__name__}: {ex_info}\")\n\n        logger.debug(f\"retry: done step with counter {counter}\")\n\n        logger.debug(\"done\")\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retry_loop(self, context, step_method):\n        logger.debug(\"starting\")\n\n        context['retryCounter'] = 0\n\n        sleep = context.get_formatted_as_type(self.sleep, out_type=float)\n        if self.max:\n            max = context.get_formatted_as_type(self.max, out_type=int)\n\n            logger.info(f\"retry decorator will try {max} times at {sleep}s \"\n                        \"intervals.\")\n        else:\n            max = None\n            logger.info(f\"retry decorator will try indefinitely at {sleep}s \"\n                        \"intervals.\")\n\n        # this will never be false. because on counter == max,\n        # exec_iteration raises an exception, breaking out of the loop.\n        # pragma because cov doesn't know the implied else is impossible.\n        # unit test cov is 100%, though.\n        if poll.while_until_true(interval=sleep,\n                                 max_attempts=max)(\n                self.exec_iteration)(context=context,\n                                     step_method=step_method\n                                     ):  # pragma: no cover\n            logger.debug(\"retry loop complete, reporting success.\")\n\n        logger.debug(\"retry loop done\")\n\n        logger.debug(\"done\")", "response": "Run a single iteration of the retry loop."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exec_iteration(self, counter, context, step_method):\n        logger.debug(\"starting\")\n        context['whileCounter'] = counter\n\n        logger.info(f\"while: running step with counter {counter}\")\n        step_method(context)\n        logger.debug(f\"while: done step {counter}\")\n\n        result = False\n        # if no stop, just iterating to max)\n        if self.stop:\n            # dynamically evaluate stop after step execution, since the step\n            # might have changed True/False status for stop.\n            result = context.get_formatted_as_type(self.stop, out_type=bool)\n\n        logger.debug(\"done\")\n        return result", "response": "This method is called by poll. while_until_true. This method is called by poll. while_until_true."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef while_loop(self, context, step_method):\n        logger.debug(\"starting\")\n\n        context['whileCounter'] = 0\n\n        if self.stop is None and self.max is None:\n            # the ctor already does this check, but guess theoretically\n            # consumer could have messed with the props since ctor\n            logger.error(f\"while decorator missing both max and stop.\")\n            raise PipelineDefinitionError(\"the while decorator must have \"\n                                          \"either max or stop, or both. \"\n                                          \"But not neither.\")\n\n        error_on_max = context.get_formatted_as_type(\n            self.error_on_max, out_type=bool)\n        sleep = context.get_formatted_as_type(self.sleep, out_type=float)\n        if self.max is None:\n            max = None\n            logger.info(f\"while decorator will loop until {self.stop} \"\n                        f\"evaluates to True at {sleep}s intervals.\")\n        else:\n            max = context.get_formatted_as_type(self.max, out_type=int)\n\n            if max < 1:\n                logger.info(\n                    f\"max {self.max} is {max}. while only runs when max > 0.\")\n                logger.debug(\"done\")\n                return\n\n            if self.stop is None:\n                logger.info(f\"while decorator will loop {max} times at \"\n                            f\"{sleep}s intervals.\")\n            else:\n                logger.info(f\"while decorator will loop {max} times, or \"\n                            f\"until {self.stop} evaluates to True at \"\n                            f\"{sleep}s intervals.\")\n\n        if not poll.while_until_true(interval=sleep,\n                                     max_attempts=max)(\n                self.exec_iteration)(context=context,\n                                     step_method=step_method):\n            # False means loop exhausted and stop never eval-ed True.\n            if error_on_max:\n                logger.error(f\"exhausted {max} iterations of while loop, \"\n                             \"and errorOnMax is True.\")\n                if self.stop and max:\n                    raise LoopMaxExhaustedError(\"while loop reached \"\n                                                f\"{max} and {self.stop} \"\n                                                \"never evaluated to True.\")\n                else:\n                    raise LoopMaxExhaustedError(f\"while loop reached {max}.\")\n            else:\n                if self.stop and max:\n                    logger.info(\n                        f\"while decorator looped {max} times, \"\n                        f\"and {self.stop} never evaluated to True.\")\n\n            logger.debug(\"while loop done\")\n        else:\n            logger.info(f\"while loop done, stop condition {self.stop} \"\n                        \"evaluated True.\")\n\n        logger.debug(\"done\")", "response": "This is the main loop for the while loop."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a yaml file into the pypyr context.", "response": "def run_step(context):\n    \"\"\"Load a yaml file into the pypyr context.\n\n    Yaml parsed from the file will be merged into the pypyr context. This will\n    overwrite existing values if the same keys are already in there.\n    I.e if file yaml has {'eggs' : 'boiled'} and context {'eggs': 'fried'}\n    already exists, returned context['eggs'] will be 'boiled'.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key must exist\n                - fetchYaml\n                    - path. path-like. Path to file on disk.\n                    - key. string. If exists, write yaml to this context key.\n                      Else yaml writes to context root.\n\n    All inputs support formatting expressions.\n\n    Also supports a passing path as string to fetchYaml, but in this case you\n    won't be able to specify a key.\n\n    Returns:\n        None. updates context arg.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: fetchYamlPath missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fetchYamlPath exists but is\n                                                  None.\n\n    \"\"\"\n    logger.debug(\"started\")\n\n    deprecated(context)\n\n    context.assert_key_has_value(key='fetchYaml', caller=__name__)\n\n    fetch_yaml_input = context.get_formatted('fetchYaml')\n\n    if isinstance(fetch_yaml_input, str):\n        file_path = fetch_yaml_input\n        destination_key_expression = None\n    else:\n        context.assert_child_key_has_value(parent='fetchYaml',\n                                           child='path',\n                                           caller=__name__)\n        file_path = fetch_yaml_input['path']\n        destination_key_expression = fetch_yaml_input.get('key', None)\n\n    logger.debug(f\"attempting to open file: {file_path}\")\n    with open(file_path) as yaml_file:\n        yaml_loader = yaml.YAML(typ='safe', pure=True)\n        payload = yaml_loader.load(yaml_file)\n\n    if destination_key_expression:\n        destination_key = context.get_formatted_iterable(\n            destination_key_expression)\n        logger.debug(f\"yaml file loaded. Writing to context {destination_key}\")\n        context[destination_key] = payload\n    else:\n        if not isinstance(payload, MutableMapping):\n            raise TypeError(\n                \"yaml input should describe a dictionary at the top \"\n                \"level when fetchYamlKey isn't specified. You should have \"\n                \"something like \\n'key1: value1'\\n key2: value2'\\n\"\n                \"in the yaml top-level, not \\n'- value1\\n - value2'\")\n\n        logger.debug(\"yaml file loaded. Merging into pypyr context. . .\")\n        context.update(payload)\n\n    logger.info(f\"yaml file written into pypyr context. Count: {len(payload)}\")\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses input yaml file and substitute tokens from context.", "response": "def run_step(context):\n    \"\"\"Parse input yaml file and substitute {tokens} from context.\n\n    Loads yaml into memory to do parsing, so be aware of big files.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n        - fileFormatYaml\n            - in. mandatory.\n              str, path-like, or an iterable (list/tuple) of\n              strings/paths. Each str/path can be a glob, relative or\n              absolute path.\n            - out. optional. path-like.\n              Can refer to a file or a directory.\n              will create directory structure if it doesn't exist. If\n              in-path refers to >1 file (e.g it's a glob or list), out\n              path can only be a directory - it doesn't make sense to\n              write >1 file to the same single file (this is not an\n              appender.) To ensure out_path is read as a directory and\n              not a file, be sure to have the path separator (/) at the\n              end.\n              If out_path is not specified or None, will in-place edit\n              and overwrite the in-files.\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: fileFormatYaml or\n            fileFormatYaml['in'] missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fileFormatYaml or\n            fileFormatYaml['in'] exists but is None.\n\n    \"\"\"\n    logger.debug(\"started\")\n\n    deprecated(context)\n\n    ObjectRewriterStep(__name__, 'fileFormatYaml', context).run_step(\n        YamlRepresenter())\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse input file and substitutes tokens from context.", "response": "def run_step(context):\n    \"\"\"Parse input file and substitutes {tokens} from context.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileFormat\n                    - in. mandatory.\n                      str, path-like, or an iterable (list/tuple) of\n                      strings/paths. Each str/path can be a glob, relative or\n                      absolute path.\n                    - out. optional. path-like.\n                      Can refer to a file or a directory.\n                      will create directory structure if it doesn't exist. If\n                      in-path refers to >1 file (e.g it's a glob or list), out\n                      path can only be a directory - it doesn't make sense to\n                      write >1 file to the same single file (this is not an\n                      appender.) To ensure out_path is read as a directory and\n                      not a file, be sure to have the path separator (/) at the\n                      end.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: fileFormat missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: in or out exists but is None.\n\n    \"\"\"\n    logger.debug(\"started\")\n    deprecated(context)\n\n    StreamRewriterStep(__name__, 'fileFormat', context).run_step()\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating new style in params from deprecated.", "response": "def deprecated(context):\n    \"\"\"Create new style in params from deprecated.\"\"\"\n    if 'fileFormatIn' in context:\n        context.assert_keys_have_values(__name__,\n                                        'fileFormatIn',\n                                        'fileFormatOut')\n\n        context['fileFormat'] = {'in': context['fileFormatIn'],\n                                 'out': context['fileFormatOut']}\n\n        logger.warning(\"fileFormatIn and fileFormatOut \"\n                       \"are deprecated. They will stop working upon the next \"\n                       \"major release. Use the new context key fileFormat \"\n                       \"instead. It's a lot better, promise! For the moment \"\n                       \"pypyr is creating the new fileFormat key for you \"\n                       \"under the hood.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_step(context):\n    logger.debug(\"started\")\n    assert context, f\"context must have value for {__name__}\"\n    deprecated(context)\n\n    context.assert_key_has_value('env', __name__)\n\n    found_get = env_get(context)\n    found_set = env_set(context)\n    found_unset = env_unset(context)\n\n    # at least 1 of envGet, envSet or envUnset must exist in context\n    if not (found_get or found_set or found_unset):\n        raise KeyNotInContextError(\n            \"context must contain any combination of \"\n            \"env['get'], env['set'] or env['unset'] for \"\n            f\"{__name__}\")\n\n    logger.debug(\"done\")", "response": "This step is used to run the Get Set and Unset steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef env_get(context):\n    get = context['env'].get('get', None)\n\n    exists = False\n    if get:\n        logger.debug(\"start\")\n\n        for k, v in get.items():\n            logger.debug(f\"setting context {k} to $ENV {v}\")\n            context[k] = os.environ[v]\n\n        logger.info(f\"saved {len(get)} $ENVs to context.\")\n        exists = True\n\n        logger.debug(\"done\")\n    return exists", "response": "Get the names of the environment variables in the pypyr context."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the environment variables to specified string. from the pypyr context.", "response": "def env_set(context):\n    \"\"\"Set $ENVs to specified string. from the pypyr context.\n\n    Args:\n        context: is dictionary-like. context is mandatory.\n                 context['env']['set'] must exist. It's a dictionary.\n                 Values are strings to write to $ENV.\n                 Keys are the names of the $ENV values to which to write.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3: value3\n        env:\n            set:\n                MYVAR1: {key1}\n                MYVAR2: before_{key3}_after\n                MYVAR3: arbtexthere\n\n    This will result in the following $ENVs:\n    $MYVAR1 = value1\n    $MYVAR2 = before_value3_after\n    $MYVAR3 = arbtexthere\n\n    Note that the $ENVs are not persisted system-wide, they only exist for\n    pypyr sub-processes, and as such for the following steps during this pypyr\n    pipeline execution. If you set an $ENV here, don't expect to see it in your\n    system environment variables after the pipeline finishes running.\n    \"\"\"\n    env_set = context['env'].get('set', None)\n\n    exists = False\n    if env_set:\n        logger.debug(\"started\")\n\n        for k, v in env_set.items():\n            logger.debug(f\"setting ${k} to context[{v}]\")\n            os.environ[k] = context.get_formatted_string(v)\n\n        logger.info(f\"set {len(env_set)} $ENVs from context.\")\n        exists = True\n\n        logger.debug(\"done\")\n\n    return exists"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef env_unset(context):\n    unset = context['env'].get('unset', None)\n\n    exists = False\n    if unset:\n        logger.debug(\"started\")\n\n        for env_var_name in unset:\n            logger.debug(f\"unsetting ${env_var_name}\")\n            try:\n                del os.environ[env_var_name]\n            except KeyError:\n                # If user is trying to get rid of the $ENV, if it doesn't\n                # exist, no real point in throwing up an error that the thing\n                # you're trying to be rid off isn't there anyway.\n                logger.debug(\n                    f\"${env_var_name} doesn't exist anyway. As you were.\")\n\n        logger.info(f\"unset {len(unset)} $ENVs.\")\n        exists = True\n\n        logger.debug(\"done\")\n    return exists", "response": "Unset the environment variables in the base object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deprecated(context):\n    env = context.get('env', None)\n\n    get_info, set_info, unset_info = context.keys_of_type_exist(\n        ('envGet', dict),\n        ('envSet', dict),\n        ('envUnset', list)\n    )\n\n    found_at_least_one = (get_info.key_in_context or set_info.key_in_context\n                          or unset_info.key_in_context)\n\n    if found_at_least_one:\n        env = context['env'] = {}\n    else:\n        return\n\n    if get_info.key_in_context and get_info.is_expected_type:\n        env['get'] = context[get_info.key]\n\n    if set_info.key_in_context and set_info.is_expected_type:\n        env['set'] = context[set_info.key]\n\n    if unset_info.key_in_context and unset_info.is_expected_type:\n        env['unset'] = context[unset_info.key]\n\n    logger.warning(\"envGet, envSet and envUnset are deprecated. They will \"\n                   \"stop working upon the next major release. \"\n                   \"Use the new context key env instead. It's a lot \"\n                   \"better, promise! For the moment pypyr is creating the \"\n                   \"new env key for you under the hood.\")", "response": "Handle deprecated context input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_step(context):\n    logger.debug(\"started\")\n    assert context, f\"context must have value for {__name__}\"\n\n    deprecated(context)\n    context.assert_key_has_value('assert', __name__)\n\n    assert_this = context['assert']['this']\n    is_equals_there = 'equals' in context['assert']\n    if is_equals_there:\n        assert_equals = context['assert']['equals']\n        # compare assertThis to assertEquals\n        logger.debug(\"comparing assert['this'] to assert['equals'].\")\n        assert_result = (context.get_formatted_iterable(assert_this)\n                         == context.get_formatted_iterable(assert_equals))\n    else:\n        # nothing to compare means treat assertThis as a bool.\n        logger.debug(\"evaluating assert['this'] as a boolean.\")\n        assert_result = context.get_formatted_as_type(assert_this,\n                                                      out_type=bool)\n\n    logger.info(f\"assert evaluated to {assert_result}\")\n\n    if not assert_result:\n        if is_equals_there:\n            # emit type to help user, but not the actual field contents.\n            type_this = (\n                type(context.get_formatted_iterable(assert_this)).__name__)\n            type_equals = (\n                type(context.get_formatted_iterable(assert_equals)).__name__)\n            error_text = (\n                f\"assert assert['this'] is of type {type_this} \"\n                f\"and does not equal assert['equals'] of type {type_equals}.\")\n        else:\n            # if it's a bool it's presumably not a sensitive value.\n            error_text = (\n                f\"assert {assert_this} evaluated to False.\")\n        raise ContextError(error_text)\n\n    logger.debug(\"done\")", "response": "Assert that something is True or equal to something else."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deprecated(context):\n    assert_context = context.get('assert', None)\n    # specifically do \"key in dict\" to avoid python bool eval thinking\n    # None/Empty values mean the key isn't there.\n    if 'assertThis' in context:\n        assert_this = context['assertThis']\n        assert_context = context['assert'] = {'this': assert_this}\n\n        if 'assertEquals' in context:\n            assert_equals = context['assertEquals']\n            assert_context['equals'] = assert_equals\n\n        logger.warning(\"assertThis and assertEquals are deprecated. They will \"\n                       \"stop working upon the next major release. \"\n                       \"Use the new context key assert instead. It's a lot \"\n                       \"better, promise! For the moment pypyr is creating the \"\n                       \"new assert key for you under the hood.\")", "response": "Handle deprecated context input."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\narchive and/or extract tars with or without compression. Args: context: dictionary-like. Mandatory. Expects the following context: tar: extract: - in: /path/my.tar out: /out/path archive: - in: /dir/to/archive out: /out/destination.tar format: '' tar['format'] - if not specified, defaults to lzma/xz Available options: - '' - no compression - gz (gzip) - bz2 (bzip2) - xz (lzma) This step will run whatever combination of Extract and Archive you specify. Regardless of combination, execution order is Extract, Archive. Source and destination paths support {key} string interpolation. Never extract archives from untrusted sources without prior inspection. It is possible that files are created outside of path, e.g. members that have absolute filenames starting with \"/\" or filenames with two dots \"..\".", "response": "def run_step(context):\n    \"\"\"Archive and/or extract tars with or without compression.\n\n    Args:\n        context: dictionary-like. Mandatory.\n\n        Expects the following context:\n        tar:\n            extract:\n                - in: /path/my.tar\n                  out: /out/path\n            archive:\n                - in: /dir/to/archive\n                  out: /out/destination.tar\n            format: ''\n\n        tar['format'] - if not specified, defaults to lzma/xz\n                       Available options:\n                        - '' - no compression\n                        - gz (gzip)\n                        - bz2 (bzip2)\n                        - xz (lzma)\n\n    This step will run whatever combination of Extract and Archive you specify.\n    Regardless of combination, execution order is Extract, Archive.\n\n    Source and destination paths support {key} string interpolation.\n\n    Never extract archives from untrusted sources without prior inspection.\n    It is possible that files are created outside of path, e.g. members that\n    have absolute filenames starting with \"/\" or filenames with two dots \"..\".\n    \"\"\"\n    logger.debug(\"started\")\n\n    assert context, f\"context must have value for {__name__}\"\n\n    deprecated(context)\n    found_at_least_one = False\n\n    context.assert_key_has_value('tar', __name__)\n\n    tar = context['tar']\n    if 'extract' in tar:\n        found_at_least_one = True\n        tar_extract(context)\n\n    if 'archive' in tar:\n        found_at_least_one = True\n        tar_archive(context)\n\n    if not found_at_least_one:\n        # This will raise exception on first item with a problem.\n        raise KeyNotInContextError('pypyr.steps.tar must have either extract '\n                                   'or archive specified under the tar key. '\n                                   'Or both of these. It has neither.')\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting file mode for reading from tar. Format", "response": "def get_file_mode_for_reading(context):\n    \"\"\"Get file mode for reading from tar['format'].\n\n    This should return r:*, r:gz, r:bz2 or r:xz. If user specified something\n    wacky in tar.Format, that's their business.\n\n    In theory r:* will auto-deduce the correct format.\n    \"\"\"\n    format = context['tar'].get('format', None)\n\n    if format or format == '':\n        mode = f\"r:{context.get_formatted_string(format)}\"\n    else:\n        mode = 'r:*'\n\n    return mode"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_file_mode_for_writing(context):\n    format = context['tar'].get('format', None)\n    # slightly weird double-check because falsy format could mean either format\n    # doesn't exist in input, OR that it exists and is empty. Exists-but-empty\n    # has special meaning - default to no compression.\n    if format or format == '':\n        mode = f\"w:{context.get_formatted_string(format)}\"\n    else:\n        mode = 'w:xz'\n\n    return mode", "response": "Get file mode for writing from tar. Format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tar_archive(context):\n    logger.debug(\"start\")\n\n    mode = get_file_mode_for_writing(context)\n\n    for item in context['tar']['archive']:\n        # value is the destination tar. Allow string interpolation.\n        destination = context.get_formatted_string(item['out'])\n        # key is the source to archive\n        source = context.get_formatted_string(item['in'])\n        with tarfile.open(destination, mode) as archive_me:\n            logger.debug(f\"Archiving '{source}' to '{destination}'\")\n\n            archive_me.add(source, arcname='.')\n            logger.info(f\"Archived '{source}' to '{destination}'\")\n\n    logger.debug(\"end\")", "response": "Archive the specified path to a tar archive."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tar_extract(context):\n    logger.debug(\"start\")\n\n    mode = get_file_mode_for_reading(context)\n\n    for item in context['tar']['extract']:\n        # in is the path to the tar to extract. Allows string interpolation.\n        source = context.get_formatted_string(item['in'])\n        # out is the outdir, dhur. Allows string interpolation.\n        destination = context.get_formatted_string(item['out'])\n        with tarfile.open(source, mode) as extract_me:\n            logger.debug(f\"Extracting '{source}' to '{destination}'\")\n\n            extract_me.extractall(destination)\n            logger.info(f\"Extracted '{source}' to '{destination}'\")\n\n    logger.debug(\"end\")", "response": "Extract all members of tar archive to specified path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle deprecated context input.", "response": "def deprecated(context):\n    \"\"\"Handle deprecated context input.\"\"\"\n    tar = context.get('tar', None)\n\n    # at least 1 of tarExtract or tarArchive must exist in context\n    tar_extract, tar_archive = context.keys_of_type_exist(\n        ('tarExtract', list),\n        ('tarArchive', list))\n\n    found_at_least_one = (tar_extract.key_in_context\n                          or tar_archive.key_in_context)\n\n    if tar and not found_at_least_one:\n        return\n    elif found_at_least_one:\n        tar = context['tar'] = {}\n\n    if tar_extract.key_in_context and tar_extract.is_expected_type:\n        tar['extract'] = context[tar_extract.key]\n\n    if tar_archive.key_in_context and tar_archive.is_expected_type:\n        tar['archive'] = context[tar_archive.key]\n\n    if 'tarFormat' in context:\n        tar['format'] = context['tarFormat']\n\n    logger.warning(\"tarExtract and tarArchive are deprecated. They will \"\n                   \"stop working upon the next major release. \"\n                   \"Use the new context key env instead. It's a lot \"\n                   \"better, promise! For the moment pypyr is creating the \"\n                   \"new env key for you under the hood.\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_step(context):\n    logger.debug(\"started\")\n\n    CmdStep(name=__name__, context=context).run_step(is_shell=True)\n\n    logger.debug(\"done\")", "response": "Run a shell command without shell interpolation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_step(context):\n    logger.debug(\"started\")\n    assert context, f\"context must have value for {__name__}\"\n\n    context.assert_key_has_value('envGet', __name__)\n\n    # allow a list OR a single getenv dict\n    if isinstance(context['envGet'], list):\n        get_items = context['envGet']\n    else:\n        get_items = [context['envGet']]\n\n    get_count = 0\n\n    for get_me in get_items:\n        (env, key, has_default, default) = get_args(get_me)\n\n        logger.debug(f\"setting context {key} to $ENV {env}\")\n        formatted_key = context.get_formatted_string(key)\n        formatted_env = context.get_formatted_string(env)\n\n        if formatted_env in os.environ:\n            context[formatted_key] = os.environ[formatted_env]\n            get_count += 1\n        else:\n            logger.debug(f\"$ENV {env} not found.\")\n            if has_default:\n                logger.debug(f\"Using default value for {env} instead.\")\n                formatted_default = context.get_formatted_iterable(default)\n                context[formatted_key] = os.environ.get(formatted_env,\n                                                        formatted_default)\n                get_count += 1\n            else:\n                logger.debug(\n                    f\"No default value for {env} found. Doin nuthin'.\")\n\n    logger.info(f\"saved {get_count} $ENVs to context.\")", "response": "Get environment variables and set context properties from environment variables and specify a default value for the key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse env key has_default default out of input dict.", "response": "def get_args(get_item):\n    \"\"\"Parse env, key, default out of input dict.\n\n    Args:\n        get_item: dict. contains keys env/key/default\n\n    Returns:\n        (env, key, has_default, default) tuple, where\n            env: str. env var name.\n            key: str. save env value to this context key.\n            has_default: bool. True if default specified.\n            default: the value of default, if specified.\n\n    Raises:\n        ContextError: envGet is not a list of dicts.\n        KeyNotInContextError: If env or key not found in get_config.\n\n    \"\"\"\n    if not isinstance(get_item, dict):\n        raise ContextError('envGet must contain a list of dicts.')\n\n    env = get_item.get('env', None)\n\n    if not env:\n        raise KeyNotInContextError(\n            'context envGet[env] must exist in context for envGet.')\n\n    key = get_item.get('key', None)\n\n    if not key:\n        raise KeyNotInContextError(\n            'context envGet[key] must exist in context for envGet.')\n\n    if 'default' in get_item:\n        has_default = True\n        default = get_item['default']\n    else:\n        has_default = False\n        default = None\n\n    return (env, key, has_default, default)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes dynamic python code and updates the pipeline context.", "response": "def run_step(context):\n    \"\"\"Executes dynamic python code.\n\n    Context is a dictionary or dictionary-like.\n    Context must contain key 'pycode'\n    Will exec context['pycode'] as dynamically interpreted python statements.\n\n    context is mandatory. When you execute the pipeline, it should look\n    something like this:\n        pipeline-runner [name here] 'pycode=print(1+1)'.\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='pycode', caller=__name__)\n\n    logger.debug(f\"Executing python string: {context['pycode']}\")\n    locals_dictionary = locals()\n    exec(context['pycode'], globals(), locals_dictionary)\n\n    # It looks like this dance might be unnecessary in python 3.6\n    logger.debug(\"looking for context update in exec\")\n    exec_context = locals_dictionary['context']\n    context.update(exec_context)\n    logger.debug(\"exec output context merged with pipeline context\")\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_parsed_context(context_arg):\n    assert context_arg, (\"pipeline must be invoked with context arg set. For \"\n                         \"this yaml parser you're looking for something \"\n                         \"like: \"\n                         \"pypyr pipelinename './myyamlfile.yaml'\")\n    logger.debug(\"starting\")\n    logger.debug(f\"attempting to open file: {context_arg}\")\n    with open(context_arg) as yaml_file:\n        yaml_loader = yaml.YAML(typ='safe', pure=True)\n        payload = yaml_loader.load(yaml_file)\n\n    logger.debug(f\"yaml file parsed. Count: {len(payload)}\")\n\n    if not isinstance(payload, MutableMapping):\n        raise TypeError(\"yaml input should describe a dictionary at the top \"\n                        \"level. You should have something like \"\n                        \"\\n'key1: value1'\\n key2: value2'\\n\"\n                        \"in the yaml top-level, not \\n'- value1\\n - value2'\")\n\n    logger.debug(\"done\")\n    return payload", "response": "Parse input context string and returns context as dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn ArgumentParser for pypyr cli.", "response": "def get_parser():\n    \"\"\"Return ArgumentParser for pypyr cli.\"\"\"\n    parser = argparse.ArgumentParser(\n        allow_abbrev=True,\n        description='pypyr pipeline runner')\n    parser.add_argument('pipeline_name',\n                        help='Name of pipeline to run. It should exist in the '\n                        './pipelines directory.')\n    parser.add_argument(dest='pipeline_context',\n                        nargs='?',\n                        help='String for context values. Parsed by the '\n                        'pipeline\\'s context_parser function.')\n    parser.add_argument('--dir', dest='working_dir', default=os.getcwd(),\n                        help='Working directory. Use if your pipelines '\n                        'directory is elsewhere. Defaults to cwd.')\n    parser.add_argument('--log', '--loglevel', dest='log_level', type=int,\n                        default=20,\n                        help='Integer log level. Defaults to 20 (INFO). '\n                        '10=DEBUG\\n20=INFO\\n30=WARNING\\n40=ERROR\\n50=CRITICAL'\n                        '.\\n Log Level < 10 gives full traceback on errors.')\n    parser.add_argument('--logpath', dest='log_path',\n                        help='Log-file path. Append log output to this path')\n    parser.add_argument('--version', action='version',\n                        help='Echo version number.',\n                        version=f'{pypyr.version.get_version()}')\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_step(self, is_shell):\n        assert is_shell is not None, (\"is_shell param must exist for CmdStep.\")\n\n        # why? If shell is True, it is recommended to pass args as a string\n        # rather than as a sequence.\n        if is_shell:\n            args = self.cmd_text\n        else:\n            args = shlex.split(self.cmd_text)\n\n        if self.is_save:\n            completed_process = subprocess.run(args,\n                                               cwd=self.cwd,\n                                               shell=is_shell,\n                                               # capture_output=True,only>py3.7\n                                               stdout=subprocess.PIPE,\n                                               stderr=subprocess.PIPE,\n                                               # text=True, only>=py3.7,\n                                               universal_newlines=True)\n            self.context['cmdOut'] = {\n                'returncode': completed_process.returncode,\n                'stdout': completed_process.stdout,\n                'stderr': completed_process.stderr\n            }\n\n            # when capture is true, output doesn't write to stdout\n            self.logger.info(f\"stdout: {completed_process.stdout}\")\n            if completed_process.stderr:\n                self.logger.error(f\"stderr: {completed_process.stderr}\")\n\n            # don't swallow the error, because it's the Step swallow decorator\n            # responsibility to decide to ignore or not.\n            completed_process.check_returncode()\n        else:\n            # check=True throws CalledProcessError if exit code != 0\n            subprocess.run(args, shell=is_shell, check=True, cwd=self.cwd)", "response": "Runs a command in a separate process."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_step(context):\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='contextClear', caller=__name__)\n\n    for k in context['contextClear']:\n        logger.debug(f\"removing {k} from context\")\n        # slightly unorthodox pop returning None means you don't get a KeyError\n        # if key doesn't exist\n        context.pop(k, None)\n        logger.info(f\"removed {k} from context\")\n\n    logger.debug(\"done\")", "response": "Remove specified keys from the context."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns command program or executable.", "response": "def run_step(context):\n    \"\"\"Run command, program or executable.\n\n    Context is a dictionary or dictionary-like.\n\n    Context must contain the following keys:\n    cmd: <<cmd string>> (command + args to execute.)\n\n    OR, as a dict\n    cmd:\n        run: str. mandatory. <<cmd string>> command + args to execute.\n        save: bool. defaults False. save output to cmdOut.\n\n    Will execute the command string in the shell as a sub-process.\n    Escape curly braces: if you want a literal curly brace, double it like\n    {{ or }}.\n\n    If save is True, will save the output to context as follows:\n        cmdOut:\n            returncode: 0\n            stdout: 'stdout str here. None if empty.'\n            stderr: 'stderr str here. None if empty.'\n\n    cmdOut.returncode is the exit status of the called process. Typically 0\n    means OK. A negative value -N indicates that the child was terminated by\n    signal N (POSIX only).\n\n    context['cmd'] will interpolate anything in curly braces for values\n    found in context. So if your context looks like this:\n        key1: value1\n        key2: value2\n        cmd: mything --arg1 {key1}\n\n    The cmd passed to the shell will be \"mything --arg value1\"\n    \"\"\"\n    logger.debug(\"started\")\n\n    pypyr.steps.cmd.run_step(context)\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets hierarchy into context with substitutions if it doesn t exist yet.", "response": "def run_step(context):\n    \"\"\"Set hierarchy into context with substitutions if it doesn't exist yet.\n\n    context is a dictionary or dictionary-like.\n    context['defaults'] must exist. It's a dictionary.\n\n    Will iterate context['defaults'] and add these as new values where\n    their keys don't already exist. While it's doing so, it will leave\n    all other values in the existing hierarchy untouched.\n\n    List merging is purely additive, with no checks for uniqueness or already\n    existing list items. E.g context [0,1,2] with contextMerge=[2,3,4]\n    will result in [0,1,2,2,3,4]\n\n    Keep this in mind especially where complex types like\n    dicts nest inside a list - a merge will always add a new dict list item,\n    not merge it into whatever dicts might exist on the list already.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3:\n            k31: value31\n            k32: value32\n        defaults:\n            key2: 'aaa_{key1}_zzz'\n            key3:\n                k33: value33\n            key4: 'bbb_{key2}_yyy'\n\n    This will result in return context:\n        key1: value1\n        key2: value2\n        key3:\n            k31: value31\n            k32: value32\n            k33: value33\n        key4: bbb_value2_yyy\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='defaults', caller=__name__)\n\n    context.set_defaults(context['defaults'])\n\n    logger.info(f\"set {len(context['defaults'])} context item defaults.\")\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the steps attribute of the module pipeline.", "response": "def get_pipeline_steps(pipeline, steps_group):\n    \"\"\"Get the steps attribute of module pipeline.\n\n    If there is no steps sequence on the pipeline, return None. Guess you\n    could theoretically want to run a pipeline with nothing in it.\n    \"\"\"\n    logger.debug(\"starting\")\n    assert pipeline\n    assert steps_group\n\n    logger.debug(f\"retrieving {steps_group} steps from pipeline\")\n    if steps_group in pipeline:\n        steps = pipeline[steps_group]\n\n        if steps is None:\n            logger.warn(\n                f\"{steps_group}: sequence has no elements. So it won't do \"\n                \"anything.\")\n            logger.debug(\"done\")\n            return None\n\n        steps_count = len(steps)\n\n        logger.debug(f\"{steps_count} steps found under {steps_group} in \"\n                     \"pipeline definition.\")\n\n        logger.debug(\"done\")\n        return steps\n    else:\n        logger.debug(\n            f\"pipeline doesn't have a {steps_group} collection. Add a \"\n            f\"{steps_group}: sequence to the yaml if you want {steps_group} \"\n            \"actually to do something.\")\n        logger.debug(\"done\")\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_failure_step_group(pipeline, context):\n    logger.debug(\"starting\")\n    try:\n        assert pipeline\n        # if no on_failure exists, it'll do nothing.\n        run_step_group(pipeline_definition=pipeline,\n                       step_group_name='on_failure',\n                       context=context)\n    except Exception as exception:\n        logger.error(\"Failure handler also failed. Swallowing.\")\n        logger.error(exception)\n\n    logger.debug(\"done\")", "response": "Run the on_failure step group if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_pipeline_steps(steps, context):\n    logger.debug(\"starting\")\n    assert isinstance(\n        context, dict), \"context must be a dictionary, even if empty {}.\"\n\n    if steps is None:\n        logger.debug(\"No steps found to execute.\")\n    else:\n        step_count = 0\n\n        for step in steps:\n            step_instance = Step(step)\n            step_instance.run_step(context)\n            step_count += 1\n\n        logger.debug(f\"executed {step_count} steps\")\n\n    logger.debug(\"done\")", "response": "Execute the run_step method of each step in steps."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the specified step group from the pipeline and run its steps.", "response": "def run_step_group(pipeline_definition, step_group_name, context):\n    \"\"\"Get the specified step group from the pipeline and run its steps.\"\"\"\n    logger.debug(f\"starting {step_group_name}\")\n    assert step_group_name\n\n    steps = get_pipeline_steps(pipeline=pipeline_definition,\n                               steps_group=step_group_name)\n\n    run_pipeline_steps(steps=steps, context=context)\n\n    logger.debug(f\"done {step_group_name}\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate all parent directories of path if they don t exist.", "response": "def ensure_dir(path):\n    \"\"\"Create all parent directories of path if they don't exist.\n\n    Args:\n        path. Path-like object. Create parent dirs to this path.\n\n    Return:\n        None.\n\n    \"\"\"\n    os.makedirs(os.path.abspath(os.path.dirname(path)), exist_ok=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses the input path and return a list of files AND directories that match the glob.", "response": "def get_glob(path):\n    \"\"\"Process the input path, applying globbing and formatting.\n\n    Do note that this will returns files AND directories that match the glob.\n\n    No tilde expansion is done, but *, ?, and character ranges expressed with\n    [] will be correctly matched.\n\n    Escape all special characters ('?', '*' and '['). For a literal match, wrap\n    the meta-characters in brackets. For example, '[?]' matches the character\n    '?'.\n\n    If passing in an iterable of paths, will expand matches for each path in\n    the iterable. The function will return all the matches for each path\n    glob expression combined into a single list.\n\n    Args:\n        path: Path-like string, or iterable (list or tuple ) of paths.\n\n    Returns:\n        Combined list of paths found for input glob.\n\n    \"\"\"\n    if isinstance(path, str):\n        return glob.glob(path, recursive=True)\n    if isinstance(path, os.PathLike):\n        # hilariously enough, glob doesn't like path-like. Gotta be str.\n        return glob.glob(str(path), recursive=True)\n    elif isinstance(path, (list, tuple)):\n        # each glob returns a list, so chain all the lists into one big list\n        return list(chain.from_iterable(\n            glob.glob(str(p), recursive=True) for p in path))\n    else:\n        raise TypeError(\"path should be string, path-like or a list. Instead, \"\n                        f\"it's a {type(path)}\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_same_file(path1, path2):\n    return (\n        path1 and path2\n        and os.path.isfile(path1) and os.path.isfile(path2)\n        and os.path.samefile(path1, path2))", "response": "Return True if path1 is the same file as path2."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove source file to destination file.", "response": "def move_file(src, dest):\n    \"\"\"Move source file to destination.\n\n    Overwrites dest.\n\n    Args:\n        src: str or path-like. source file\n        dest: str or path-like. destination file\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: out path parent doesn't exist.\n        OSError: if any IO operations go wrong.\n\n    \"\"\"\n    try:\n        os.replace(src, dest)\n    except Exception as ex_replace:\n        logger.error(f\"error moving file {src} to \"\n                     f\"{dest}. {ex_replace}\")\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef move_temp_file(src, dest):\n    try:\n        move_file(src, dest)\n    except Exception:\n        try:\n            os.remove(src)\n        except Exception as ex_clean:\n            # at this point, something's deeply wrong, so log error.\n            # raising the original error, though, not this error in the\n            # error handler, as the 1st was the initial cause of all of\n            # this.\n            logger.error(f\"error removing temp file {src}. \"\n                         f\"{ex_clean}\")\n\n        raise", "response": "Move src to dest. Delete src if something goes wrong."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef files_in_to_out(self, in_path, out_path=None):\n        in_paths = get_glob(in_path)\n\n        in_count = len(in_paths)\n        if in_count == 0:\n            logger.debug(f'in path found {in_count} paths.')\n        else:\n            logger.debug(f'in path found {in_count} paths:')\n            for path in in_paths:\n                logger.debug(f'{path}')\n            logger.debug(\n                'herewith ends the paths. will now process each file.')\n\n        if in_paths:\n            # derive the destination directory, ensure it's ready for writing\n            basedir_out = None\n            is_outfile_name_known = False\n            if out_path:\n                # outpath could be a file, or a dir\n                pathlib_out = Path(out_path)\n                # yep, Path() strips trailing /, hence check original string\n                if isinstance(out_path, str) and out_path.endswith(os.sep):\n                    # ensure dir - mimic posix mkdir -p\n                    pathlib_out.mkdir(parents=True, exist_ok=True)\n                    basedir_out = pathlib_out\n                elif pathlib_out.is_dir():\n                    basedir_out = pathlib_out\n                else:\n                    if len(in_paths) > 1:\n                        raise Error(\n                            f'{in_path} resolves to {len(in_paths)} files, '\n                            'but you specified only a single file as out '\n                            f'{out_path}. If the outpath is meant to be a '\n                            'directory, put a / at the end.')\n\n                    # at this point it must be a file (not dir) path\n                    # make sure that the parent dir exists\n                    basedir_out = pathlib_out.parent\n                    basedir_out.parent.mkdir(parents=True, exist_ok=True)\n                    is_outfile_name_known = True\n\n            # loop through all the in files and write them to the out dir\n            file_counter = 0\n            is_edit = False\n            for path in in_paths:\n                actual_in = Path(path)\n                # recursive glob returns dirs too, only interested in files\n                if actual_in.is_file():\n                    if basedir_out:\n                        if is_outfile_name_known:\n                            actual_out = pathlib_out\n                        else:\n                            # default to original src file name if only out dir\n                            # specified without an out file name\n                            actual_out = basedir_out.joinpath(actual_in.name)\n\n                        logger.debug(f\"writing {path} to {actual_out}\")\n                        self.in_to_out(in_path=actual_in, out_path=actual_out)\n                    else:\n                        logger.debug(f\"editing {path}\")\n                        self.in_to_out(in_path=actual_in)\n                        is_edit = True\n                    file_counter += 1\n\n            if is_edit:\n                logger.info(\n                    f\"edited & wrote {file_counter} file(s) at {in_path}\")\n            else:\n                logger.info(\n                    f\"read {in_path}, formatted and wrote {file_counter} \"\n                    f\"file(s) to {out_path}\")\n        else:\n            logger.info(f\"{in_path} found no files\")", "response": "Write in files to out."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef in_to_out(self, in_path, out_path=None):\n        if is_same_file(in_path, out_path):\n            logger.debug(\n                \"in path and out path are the same file. writing to temp \"\n                \"file and then replacing in path with the temp file.\")\n            out_path = None\n\n        logger.debug(f\"opening source file: {in_path}\")\n        with open(in_path) as infile:\n            obj = self.object_representer.load(infile)\n\n        if out_path:\n            logger.debug(\n                f\"opening destination file for writing: {out_path}\")\n            ensure_dir(out_path)\n            with open(out_path, 'w') as outfile:\n                self.object_representer.dump(outfile, self.formatter(obj))\n            return\n        else:\n            logger.debug(\"opening temp file for writing...\")\n            with NamedTemporaryFile(mode='w+t',\n                                    dir=os.path.dirname(in_path),\n                                    delete=False) as outfile:\n                self.object_representer.dump(outfile, self.formatter(obj))\n\n            logger.debug(f\"moving temp file to: {in_path}\")\n\n            move_temp_file(outfile.name, infile.name)", "response": "Load a file into object formats writes object to out."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite a single file in to out running self. formatter on each line.", "response": "def in_to_out(self, in_path, out_path=None):\n        \"\"\"Write a single file in to out, running self.formatter on each line.\n\n        If in_path and out_path point to the same thing it will in-place edit\n        and overwrite the in path. Even easier, if you do want to edit a file\n        in place, don't specify out_path, or set it to None.\n\n        Args:\n            in_path: str or path-like. Must refer to a single existing file.\n            out_path: str or path-like. Must refer to a single destination file\n                      location. will create directory structure if it doesn't\n                      exist.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n\n        Returns:\n            None.\n\n        \"\"\"\n        is_in_place_edit = False\n        if is_same_file(in_path, out_path):\n            logger.debug(\n                \"in path and out path are the same file. writing to temp \"\n                \"file and then replacing in path with the temp file.\")\n            out_path = None\n            is_in_place_edit = True\n\n        logger.debug(f\"opening source file: {in_path}\")\n        with open(in_path) as infile:\n            if out_path:\n                logger.debug(\n                    f\"opening destination file for writing: {out_path}\")\n                ensure_dir(out_path)\n                with open(out_path, 'w') as outfile:\n                    outfile.writelines(self.formatter(infile))\n                return\n            else:\n                logger.debug(\"opening temp file for writing...\")\n                with NamedTemporaryFile(mode='w+t',\n                                        dir=os.path.dirname(in_path),\n                                        delete=False) as outfile:\n                    outfile.writelines(self.formatter(infile))\n\n                is_in_place_edit = True\n\n        # only replace infile AFTER it's closed, outside the with.\n        # pragma exclude because func actually returns on 287 in if out_path,\n        # and cov not smart enough to realize that !is_in_place_edit won't ever\n        # happen here (the function will have exited already)\n        if is_in_place_edit:    # pragma: no branch\n            logger.debug(f\"moving temp file to: {in_path}\")\n            move_temp_file(outfile.name, infile.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump(self, file, payload):\n        json.dump(payload, file, indent=2, ensure_ascii=False)", "response": "Dump the object to a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses input file and replace a search string. This also does string substitutions from context on the fileReplacePairs. It does this before it search & replaces the in file. Be careful of order. If fileReplacePairs is not an ordered collection, replacements could evaluate in any given order. If this is coming in from pipeline yaml it will be an ordered dictionary, so life is good. Args: context: pypyr.context.Context. Mandatory. The following context keys expected: - fileReplace - in. mandatory. str, path-like, or an iterable (list/tuple) of strings/paths. Each str/path can be a glob, relative or absolute path. - out. optional. path-like. Can refer to a file or a directory. will create directory structure if it doesn't exist. If in-path refers to >1 file (e.g it's a glob or list), out path can only be a directory - it doesn't make sense to write >1 file to the same single file (this is not an appender.) To ensure out_path is read as a directory and not a file, be sure to have the path separator (/) at the end. If out_path is not specified or None, will in-place edit and overwrite the in-files. - replacePairs. mandatory. Dictionary where items are: 'find_string': 'replace_string' Returns: None. Raises: FileNotFoundError: take a guess pypyr.errors.KeyNotInContextError: Any of the required keys missing in context. pypyr.errors.KeyInContextHasNoValueError: Any of the required keys exists but is None.", "response": "def run_step(context):\n    \"\"\"Parse input file and replace a search string.\n\n    This also does string substitutions from context on the fileReplacePairs.\n    It does this before it search & replaces the in file.\n\n    Be careful of order. If fileReplacePairs is not an ordered collection,\n    replacements could evaluate in any given order. If this is coming in from\n    pipeline yaml it will be an ordered dictionary, so life is good.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileReplace\n                    - in. mandatory.\n                      str, path-like, or an iterable (list/tuple) of\n                      strings/paths. Each str/path can be a glob, relative or\n                      absolute path.\n                    - out. optional. path-like.\n                      Can refer to a file or a directory.\n                      will create directory structure if it doesn't exist. If\n                      in-path refers to >1 file (e.g it's a glob or list), out\n                      path can only be a directory - it doesn't make sense to\n                      write >1 file to the same single file (this is not an\n                      appender.) To ensure out_path is read as a directory and\n                      not a file, be sure to have the path separator (/) at the\n                      end.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n                    - replacePairs. mandatory. Dictionary where items are:\n                      'find_string': 'replace_string'\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: Any of the required keys missing in\n                                          context.\n        pypyr.errors.KeyInContextHasNoValueError: Any of the required keys\n                                                  exists but is None.\n\n    \"\"\"\n    logger.debug(\"started\")\n    deprecated(context)\n    StreamReplacePairsRewriterStep(__name__, 'fileReplace', context).run_step()\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing input json file and substitute tokens from context.", "response": "def run_step(context):\n    \"\"\"Parse input json file and substitute {tokens} from context.\n\n    Loads json into memory to do parsing, so be aware of big files.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                - fileFormatJson\n                    - in. mandatory.\n                      str, path-like, or an iterable (list/tuple) of\n                      strings/paths. Each str/path can be a glob, relative or\n                      absolute path.\n                    - out. optional. path-like.\n                      Can refer to a file or a directory.\n                      will create directory structure if it doesn't exist. If\n                      in-path refers to >1 file (e.g it's a glob or list), out\n                      path can only be a directory - it doesn't make sense to\n                      write >1 file to the same single file (this is not an\n                      appender.) To ensure out_path is read as a directory and\n                      not a file, be sure to have the path separator (/) at the\n                      end.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: fileFormatJson or\n            fileFormatJson['in'] missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fileFormatJson or\n            fileFormatJson['in'] exists but is None.\n\n    \"\"\"\n    logger.debug(\"started\")\n\n    deprecated(context)\n\n    ObjectRewriterStep(__name__, 'fileFormatJson', context).run_step(\n        JsonRepresenter())\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_logging_config(log_level, handlers):\n    logging.basicConfig(\n        format='%(asctime)s %(levelname)s:%(name)s:%(funcName)s: %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S',\n        level=log_level,\n        handlers=handlers)", "response": "Set python logging library config."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_root_logger(root_log_level, log_path=None):\n    handlers = []\n\n    console_handler = logging.StreamHandler()\n    handlers.append(console_handler)\n    if log_path:\n        file_handler = logging.FileHandler(log_path)\n        handlers.append(file_handler)\n\n    set_logging_config(root_log_level, handlers=handlers)\n    root_logger = logging.getLogger(\"pypyr\")\n    root_logger.debug(\n        f\"Root logger {root_logger.name} configured with level \"\n        f\"{root_log_level}\")", "response": "Set the root logger."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse input context string and returns context as dictionary.", "response": "def get_parsed_context(context_arg):\n    \"\"\"Parse input context string and returns context as dictionary.\"\"\"\n    if not context_arg:\n        logger.debug(\"pipeline invoked without context arg set. For \"\n                     \"this json parser you're looking for something \"\n                     \"like: \"\n                     \"pypyr pipelinename '{\\\"key1\\\":\\\"value1\\\",\"\n                     \"\\\"key2\\\":\\\"value2\\\"}'\")\n        return None\n\n    logger.debug(\"starting\")\n    # deserialize the input context string into json\n    return json.loads(context_arg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_parsed_context(pipeline, context_in_string):\n    logger.debug(\"starting\")\n\n    if 'context_parser' in pipeline:\n        parser_module_name = pipeline['context_parser']\n        logger.debug(f\"context parser found: {parser_module_name}\")\n        parser_module = pypyr.moduleloader.get_module(parser_module_name)\n\n        try:\n            logger.debug(f\"running parser {parser_module_name}\")\n            result_context = parser_module.get_parsed_context(\n                context_in_string)\n            logger.debug(f\"step {parser_module_name} done\")\n            # Downstream steps likely to expect context not to be None, hence\n            # empty rather than None.\n            if result_context is None:\n                logger.debug(f\"{parser_module_name} returned None. Using \"\n                             \"empty context instead\")\n                return pypyr.context.Context()\n            else:\n                return pypyr.context.Context(result_context)\n        except AttributeError:\n            logger.error(f\"The parser {parser_module_name} doesn't have a \"\n                         \"get_parsed_context(context) function.\")\n            raise\n    else:\n        logger.debug(\"pipeline does not have custom context parser. Using \"\n                     \"empty context.\")\n        logger.debug(\"done\")\n        # initialize to an empty dictionary because you want to be able to run\n        # with no context.\n        return pypyr.context.Context()", "response": "Execute get_parsed_context handler if specified on pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_context(pipeline, context_in_string, context):\n    logger.debug(\"starting\")\n\n    parsed_context = get_parsed_context(\n        pipeline=pipeline,\n        context_in_string=context_in_string)\n\n    context.update(parsed_context)\n\n    logger.debug(\"done\")", "response": "Prepare the context for a pipeline run."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_and_run_pipeline(pipeline_name,\n                          pipeline_context_input=None,\n                          working_dir=None,\n                          context=None,\n                          parse_input=True,\n                          loader=None):\n    \"\"\"Load and run the specified pypyr pipeline.\n\n    This function runs the actual pipeline by name. If you are running another\n    pipeline from within a pipeline, call this, not main(). Do call main()\n    instead for your 1st pipeline if there are pipelines calling pipelines.\n\n    By default pypyr uses file loader. This means that pipeline_name.yaml\n    should be in the working_dir/pipelines/ directory.\n\n    Args:\n        pipeline_name (str): Name of pipeline, sans .yaml at end.\n        pipeline_context_input (str): Initialize the pypyr context with this\n                                 string.\n        working_dir (path): Look for pipelines and modules in this directory.\n                     If context arg passed, will use context.working_dir and\n                     ignore this argument. If context is None, working_dir\n                     must be specified.\n        context (pypyr.context.Context): Use if you already have a\n                 Context object, such as if you are running a pipeline from\n                 within a pipeline and you want to re-use the same context\n                 object for the child pipeline. Any mutations of the context by\n                 the pipeline will be against this instance of it.\n        parse_input (bool): run context_parser in pipeline.\n        loader (str): str. optional. Absolute name of pipeline loader module.\n                If not specified will use pypyr.pypeloaders.fileloader.\n\n    Returns:\n        None\n\n    \"\"\"\n    logger.debug(f\"you asked to run pipeline: {pipeline_name}\")\n    if loader:\n        logger.debug(f\"you set the pype loader to: {loader}\")\n    else:\n        loader = 'pypyr.pypeloaders.fileloader'\n        logger.debug(f\"use default pype loader: {loader}\")\n\n    logger.debug(f\"you set the initial context to: {pipeline_context_input}\")\n\n    if context is None:\n        context = pypyr.context.Context()\n        context.working_dir = working_dir\n    else:\n        working_dir = context.working_dir\n\n    # pipeline loading deliberately outside of try catch. The try catch will\n    # try to run a failure-handler from the pipeline, but if the pipeline\n    # doesn't exist there is no failure handler that can possibly run so this\n    # is very much a fatal stop error.\n    loader_module = pypyr.moduleloader.get_module(loader)\n\n    try:\n        get_pipeline_definition = getattr(\n            loader_module, 'get_pipeline_definition'\n        )\n    except AttributeError:\n        logger.error(\n            f\"The pipeline loader {loader_module} doesn't have a \"\n            \"get_pipeline_definition(pipeline_name, working_dir) function.\")\n        raise\n\n    logger.debug(f\"loading the pipeline definition with {loader_module}\")\n    pipeline_definition = get_pipeline_definition(\n        pipeline_name=pipeline_name,\n        working_dir=working_dir\n    )\n    logger.debug(f\"{loader_module} done\")\n\n    run_pipeline(\n        pipeline=pipeline_definition,\n        pipeline_context_input=pipeline_context_input,\n        context=context,\n        parse_input=parse_input\n    )", "response": "Load and run a pipeline from a file or directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_pipeline(pipeline,\n                 context,\n                 pipeline_context_input=None,\n                 parse_input=True):\n    \"\"\"Run the specified pypyr pipeline.\n\n    This function runs the actual pipeline. If you are running another\n    pipeline from within a pipeline, call this, not main(). Do call main()\n    instead for your 1st pipeline if there are pipelines calling pipelines.\n\n    Pipeline and context should be already loaded.\n\n    Args:\n        pipeline (dict): Dictionary representing the pipeline.\n        context (pypyr.context.Context): Reusable context object.\n        pipeline_context_input (str): Initialize the pypyr context with this\n                                string.\n        parse_input (bool): run context_parser in pipeline.\n\n    Returns:\n        None\n\n    \"\"\"\n    logger.debug(\"starting\")\n\n    try:\n        if parse_input:\n            logger.debug(\"executing context_parser\")\n            prepare_context(pipeline=pipeline,\n                            context_in_string=pipeline_context_input,\n                            context=context)\n        else:\n            logger.debug(\"skipping context_parser\")\n\n        # run main steps\n        pypyr.stepsrunner.run_step_group(\n            pipeline_definition=pipeline,\n            step_group_name='steps',\n            context=context)\n\n        # if nothing went wrong, run on_success\n        logger.debug(\"pipeline steps complete. Running on_success steps now.\")\n        pypyr.stepsrunner.run_step_group(\n            pipeline_definition=pipeline,\n            step_group_name='on_success',\n            context=context)\n    except Exception:\n        # yes, yes, don't catch Exception. Have to, though, to run the failure\n        # handler. Also, it does raise it back up.\n        logger.error(\"Something went wrong. Will now try to run on_failure.\")\n\n        # failure_step_group will log but swallow any errors\n        pypyr.stepsrunner.run_failure_step_group(\n            pipeline=pipeline,\n            context=context)\n        logger.debug(\"Raising original exception to caller.\")\n        raise\n\n    logger.debug(\"done\")", "response": "This function runs the specified pypyr pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_step(context):\n    logger.debug(\"started\")\n    context.assert_child_key_has_value('fileWriteYaml', 'path', __name__)\n\n    out_path = context.get_formatted_string(context['fileWriteYaml']['path'])\n    # doing it like this to safeguard against accidentally dumping all context\n    # with potentially sensitive values in it to disk if payload exists but is\n    # None.\n    is_payload_specified = 'payload' in context['fileWriteYaml']\n\n    yaml_writer = pypyr.yaml.get_yaml_parser_roundtrip_for_context()\n\n    logger.debug(f\"opening destination file for writing: {out_path}\")\n    os.makedirs(os.path.abspath(os.path.dirname(out_path)), exist_ok=True)\n    with open(out_path, 'w') as outfile:\n        if is_payload_specified:\n            payload = context['fileWriteYaml']['payload']\n            formatted_iterable = context.get_formatted_iterable(payload)\n        else:\n            formatted_iterable = context.get_formatted_iterable(context)\n\n        yaml_writer.dump(formatted_iterable, outfile)\n\n    logger.info(f\"formatted context content and wrote to {out_path}\")\n    logger.debug(\"done\")", "response": "Write payload to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint debug info to console.", "response": "def run_step(context):\n    \"\"\"Print debug info to console.\n\n    context is a dictionary or dictionary-like.\n\n    If you use pypyr.steps.debug as a simple step (i.e you do NOT specify the\n    debug input context), it will just dump the entire context to stdout.\n\n    Configure the debug step with the following optional context item:\n        debug:\n            keys: str (for single key) or list (of str keys). Only dump the\n                  specified keys.\n            format: bool. Defaults False. Applies formatting expressions on\n                    dump.\n    \"\"\"\n    logger.debug(\"started\")\n\n    debug = context.get('debug', None)\n\n    if debug:\n        keys = debug.get('keys', None)\n        format = debug.get('format', False)\n\n        if keys:\n            logger.debug(f\"Writing to output: {keys}\")\n            if isinstance(keys, str):\n                payload = {keys: context[keys]}\n            else:\n                payload = {k: context[k] for k in keys}\n        else:\n            logger.debug(\n                \"No keys specified. Writing entire context to output.\")\n            payload = context\n\n        if format:\n            payload = context.get_formatted_iterable(payload)\n    else:\n        payload = context\n\n    logger.info(f'\\n{json.dumps(payload, indent=2, ensure_ascii=False)}')\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_error_name(error):\n    error_type = type(error)\n    if error_type.__module__ in ['__main__', 'builtins']:\n        return error_type.__name__\n    else:\n        return f'{error_type.__module__}.{error_type.__name__}'", "response": "Return canonical error name as string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_module(module_abs_import):\n    logger.debug(\"starting\")\n    logger.debug(f\"loading module {module_abs_import}\")\n    try:\n        imported_module = importlib.import_module(module_abs_import)\n        logger.debug(\"done\")\n        return imported_module\n    except ModuleNotFoundError as err:\n        msg = (\"The module doesn't exist. Looking for a file like this: \"\n               f\"{module_abs_import}\")\n\n        extended_msg = (f\"{module_abs_import}.py should be in your working \"\n                        \"dir or it should be installed to the python path.\"\n                        \"\\nIf you have 'package.sub.mod' your current working \"\n                        \"dir should contain ./package/sub/mod.py\\n\"\n                        \"If you specified 'mymodulename', your current \"\n                        \"working dir should contain ./mymodulename.py\\n\"\n                        \"If the module is not in your current working dir, it \"\n                        \"must exist in your current python path - so you \"\n                        \"should have run pip install or setup.py\")\n        logger.error(msg)\n        raise PyModuleNotFoundError(extended_msg) from err", "response": "Use importlib to get the module dynamically."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a working directory to sys. paths.", "response": "def set_working_directory(working_directory):\n    \"\"\"Add working_directory to sys.paths.\n\n    This allows dynamic loading of arbitrary python modules in cwd.\n\n    Args:\n        working_directory: string. path to add to sys.paths\n\n    \"\"\"\n    logger.debug(\"starting\")\n\n    logger.debug(f\"adding {working_directory} to sys.paths\")\n    sys.path.append(working_directory)\n\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assert_child_key_has_value(self, parent, child, caller):\n        assert parent, (\"parent parameter must be specified.\")\n        assert child, (\"child parameter must be specified.\")\n        self.assert_key_has_value(parent, caller)\n\n        try:\n            child_exists = child in self[parent]\n        except TypeError as err:\n            # This happens if parent isn't iterable\n            raise ContextError(\n                f\"context['{parent}'] must be iterable and contain '{child}' \"\n                f\"for {caller}. {err}\") from err\n\n        if child_exists:\n            if self[parent][child] is None:\n                raise KeyInContextHasNoValueError(\n                    f\"context['{parent}']['{child}'] must have a value for \"\n                    f\"{caller}.\")\n        else:\n            raise KeyNotInContextError(\n                f\"context['{parent}']['{child}'] doesn't \"\n                f\"exist. It must exist for {caller}.\")", "response": "Assert that context contains key that has child which has a value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assert_key_has_value(self, key, caller):\n        assert key, (\"key parameter must be specified.\")\n        self.assert_key_exists(key, caller)\n\n        if self[key] is None:\n            raise KeyInContextHasNoValueError(\n                f\"context['{key}'] must have a value for {caller}.\")", "response": "Assert that context contains key which also has a value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assert_key_type_value(self,\n                              context_item,\n                              caller,\n                              extra_error_text=''):\n        \"\"\"Assert that keys exist of right type and has a value.\n\n        Args:\n             context_item: ContextItemInfo tuple\n             caller: string. calling function name - this used to construct\n                     error messages\n             extra_error_text: append to end of error message.\n\n        Raises:\n            AssertionError: if context_item None.\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None or the wrong\n                                         type.\n\n        \"\"\"\n        assert context_item, (\"context_item parameter must be specified.\")\n\n        if extra_error_text is None or extra_error_text == '':\n            append_error_text = ''\n        else:\n            append_error_text = f' {extra_error_text}'\n\n        if not context_item.key_in_context:\n            raise KeyNotInContextError(f'{caller} couldn\\'t find '\n                                       f'{context_item.key} in context.'\n                                       f'{append_error_text}')\n\n        if not context_item.has_value:\n            raise KeyInContextHasNoValueError(\n                f'{caller} found {context_item.key} in '\n                f'context but it doesn\\'t have a value.'\n                f'{append_error_text}')\n\n        if not context_item.is_expected_type:\n            raise KeyInContextHasNoValueError(\n                f'{caller} found {context_item.key} in context, but it\\'s '\n                f'not a {context_item.expected_type}.'\n                f'{append_error_text}')", "response": "Assert that a key exists of right type and has a value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assert_keys_exist(self, caller, *keys):\n        assert keys, (\"*keys parameter must be specified.\")\n        for key in keys:\n            self.assert_key_exists(key, caller)", "response": "Assert that the keys in the context exist in the caller."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks that keys list are all in context and all have values.", "response": "def assert_keys_have_values(self, caller, *keys):\n        \"\"\"Check that keys list are all in context and all have values.\n\n        Args:\n            *keys: Will check each of these keys in context\n            caller: string. Calling function name - just used for informational\n                    messages\n\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None\n            AssertionError: if *keys is None\n\n        \"\"\"\n        for key in keys:\n            self.assert_key_has_value(key, caller)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assert_keys_type_value(self,\n                               caller,\n                               extra_error_text,\n                               *context_items):\n        \"\"\"Assert that keys exist of right type and has a value.\n\n        Args:\n             caller: string. calling function name - this used to construct\n                     error messages\n             extra_error_text: append to end of error message. This can happily\n                               be None or ''.\n            *context_items: ContextItemInfo tuples\n\n        Raises:\n            AssertionError: if context_items None.\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None or the wrong\n                                         type.\n\n        \"\"\"\n        assert context_items, (\"context_items parameter must be specified.\")\n\n        for context_item in context_items:\n            self.assert_key_type_value(context_item, caller, extra_error_text)", "response": "Assert that keys exist of right type and has a value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_formatted(self, key):\n        val = self[key]\n\n        if isinstance(val, str):\n            try:\n                return self.get_processed_string(val)\n            except KeyNotInContextError as err:\n                # Wrapping the KeyError into a less cryptic error for end-user\n                # friendliness\n                raise KeyNotInContextError(\n                    f'Unable to format \\'{val}\\' at context[\\'{key}\\'], '\n                    f'because {err}'\n                ) from err\n        elif isinstance(val, SpecialTagDirective):\n            return val.get_value(self)\n        else:\n            # any sort of complex type will work with get_formatted_iterable.\n            return self.get_formatted_iterable(val)", "response": "Returns a formatted value for the key in the context dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a formatted string for the given string.", "response": "def get_formatted_string(self, input_string):\n        \"\"\"Return formatted value for input_string.\n\n        get_formatted gets a context[key] value.\n        get_formatted_string is for any arbitrary string that is not in the\n        context.\n\n        Only valid if input_string is a type string.\n        Return a string interpolated from the context dictionary.\n\n        If input_string='Piping {key1} the {key2} wild'\n        And context={'key1': 'down', 'key2': 'valleys', 'key3': 'value3'}\n\n        Then this will return string: \"Piping down the valleys wild\"\n\n        Args:\n            input_string: string to parse for substitutions.\n\n        Returns:\n            Formatted string.\n\n        Raises:\n            KeyNotInContextError: context[key] has {somekey} where somekey does\n                                  not exist in context dictionary.\n            TypeError: Attempt operation on a non-string type.\n\n        \"\"\"\n        if isinstance(input_string, str):\n            try:\n                return self.get_processed_string(input_string)\n            except KeyNotInContextError as err:\n                # Wrapping the KeyError into a less cryptic error for end-user\n                # friendliness\n                raise KeyNotInContextError(\n                    f'Unable to format \\'{input_string}\\' because {err}'\n                ) from err\n        elif isinstance(input_string, SpecialTagDirective):\n            return input_string.get_value(self)\n        else:\n            raise TypeError(f\"can only format on strings. {input_string} is a \"\n                            f\"{type(input_string)} instead.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning formatted value for input value returns as out_type.", "response": "def get_formatted_as_type(self, value, default=None, out_type=str):\n        \"\"\"Return formatted value for input value, returns as out_type.\n\n        Caveat emptor: if out_type is bool and value a string,\n        return will be True if str is 'True'. It will be False for all other\n        cases.\n\n        Args:\n            value: the value to format\n            default: if value is None, set to this\n            out_type: cast return as this type\n\n        Returns:\n            Formatted value of type out_type\n\n        \"\"\"\n        if value is None:\n            value = default\n\n        if isinstance(value, SpecialTagDirective):\n            result = value.get_value(self)\n            return types.cast_to_type(result, out_type)\n        if isinstance(value, str):\n            result = self.get_formatted_string(value)\n            result_type = type(result)\n            if out_type is result_type:\n                # get_formatted_string result is already a string\n                return result\n            elif out_type is bool and result_type is str:\n                # casting a str to bool is always True, hence special case. If\n                # the str value is 'False'/'false', presumably user can\n                # reasonably expect a bool False response.\n                return result.lower() in ['true', '1', '1.0']\n            else:\n                return out_type(result)\n        else:\n            return out_type(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a string into a formatted string.", "response": "def get_processed_string(self, input_string):\n        \"\"\"Run token substitution on input_string against context.\n\n        You probably don't want to call this directly yourself - rather use\n        get_formatted, get_formatted_iterable, or get_formatted_string because\n        these contain more friendly error handling plumbing and context logic.\n\n        If you do want to call it yourself, go for it, it doesn't touch state.\n\n        If input_string='Piping {key1} the {key2} wild'\n        And context={'key1': 'down', 'key2': 'valleys', 'key3': 'value3'}\n\n        An input string with a single formatting expression and nothing else\n        will return the object at that context path: input_string='{key1}'.\n        This means that the return obj will be the same type as the source\n        object. This return object in itself has token substitions run on it\n        iteratively.\n\n        By comparison, multiple formatting expressions and/or the inclusion of\n        literal text will result in a string return type:\n        input_string='{key1} literal text {key2}'\n\n        Then this will return string: \"Piping down the valleys wild\"\n\n        Args:\n            input_string: string to Parse\n\n        Returns:\n            any given type: Formatted string with {substitutions} made from\n            context. If it's a !sic string, x from !sic x, with no\n            substitutions made on x. If input_string was a single expression\n            (e.g '{field}'), then returns the object with {substitutions} made\n            for its attributes.\n\n        Raises:\n            KeyNotInContextError: input_string is not a sic string and has\n                                  {somekey} where somekey does not exist in\n                                  context dictionary.\n\n        \"\"\"\n        # arguably, this doesn't really belong here, or at least it makes a\n        # nonsense of the function name. given how py and strings\n        # look and feel pretty much like strings from user's perspective, and\n        # given legacy code back when sic strings were in fact just strings,\n        # keep in here for backwards compatibility.\n        if isinstance(input_string, SpecialTagDirective):\n            return input_string.get_value(self)\n        else:\n            # is this a special one field formatstring? i.e \"{field}\", with\n            # nothing else?\n            out = None\n            is_out_set = False\n            expr_count = 0\n            # parse finds field format expressions and/or literals in input\n            for expression in formatter.parse(input_string):\n                # parse tuple:\n                # (literal_text, field_name, format_spec, conversion)\n                # it's a single '{field}' if no literal_text but field_name\n                # no literal, field name exists, and no previous expr found\n                if (not expression[0] and expression[1] and not expr_count):\n                    # get_field tuple: (obj, used_key)\n                    out = formatter.get_field(expression[1], None, self)[0]\n                    # second flag necessary because a literal with no format\n                    # expression will still result in expr_count == 1\n                    is_out_set = True\n\n                expr_count += 1\n\n                # this is a little bit clumsy, but you have to consume the\n                # iterator to get the count. Interested in 1 and only 1 field\n                # expressions with no literal text: have to loop to see if\n                # there is >1.\n                if expr_count > 1:\n                    break\n\n            if is_out_set and expr_count == 1:\n                # found 1 and only 1. but this could be an iterable obj\n                # that needs formatting rules run on it in itself\n                return self.get_formatted_iterable(out)\n            else:\n                return input_string.format_map(self)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if keys exist in context and if types are as expected.", "response": "def keys_of_type_exist(self, *keys):\n        \"\"\"Check if keys exist in context and if types are as expected.\n\n        Args:\n            *keys: *args for keys to check in context.\n                   Each arg is a tuple (str, type)\n\n        Returns:\n            Tuple of namedtuple ContextItemInfo, same order as *keys.\n            ContextItemInfo(key,\n                            key_in_context,\n                            expected_type,\n                            is_expected_type)\n\n            Remember if there is only one key in keys, the return assignment\n            needs an extra comma to remind python that it's a tuple:\n            # one\n            a, = context.keys_of_type_exist('a')\n            # > 1\n            a, b = context.keys_of_type_exist('a', 'b')\n\n        \"\"\"\n        # k[0] = key name, k[1] = exists, k2 = expected type\n        keys_exist = [(key, key in self.keys(), expected_type)\n                      for key, expected_type in keys]\n\n        return tuple(ContextItemInfo(\n            key=k[0],\n            key_in_context=k[1],\n            expected_type=k[2],\n            is_expected_type=isinstance(self[k[0]], k[2])\n            if k[1] else None,\n            has_value=k[1] and not self[k[0]] is None\n        ) for k in keys_exist)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge add_me into context and applies interpolation.", "response": "def merge(self, add_me):\n        \"\"\"Merge add_me into context and applies interpolation.\n\n        Bottom-up merge where add_me merges into context. Applies string\n        interpolation where the type is a string. Where a key exists in\n        context already, add_me's value will overwrite what's in context\n        already.\n\n        Supports nested hierarchy. add_me can contains dicts/lists/enumerables\n        that contain other enumerables et. It doesn't restrict levels of\n        nesting, so if you really want to go crazy with the levels you can, but\n        you might blow your stack.\n\n        If something from add_me exists in context already, but add_me's value\n        is of a different type, add_me will overwrite context. Do note this.\n        i.e if you had context['int_key'] == 1 and\n        add_me['int_key'] == 'clearly not a number', the end result would be\n        context['int_key'] == 'clearly not a number'\n\n        If add_me contains lists/sets/tuples, this merges these\n        additively, meaning it appends values from add_me to the existing\n        sequence.\n\n        Args:\n            add_me: dict. Merge this dict into context.\n\n        Returns:\n            None. All operations mutate this instance of context.\n\n        \"\"\"\n        def merge_recurse(current, add_me):\n            \"\"\"Walk the current context tree in recursive inner function.\n\n            On 1st iteration, current = self (i.e root of context)\n            On subsequent recursive iterations, current is wherever you're at\n            in the nested context hierarchy.\n\n            Args:\n                current: dict. Destination of merge.\n                add_me: dict. Merge this to current.\n            \"\"\"\n            for k, v in add_me.items():\n                # key supports interpolation\n                k = self.get_formatted_string(k)\n\n                # str not mergable, so it doesn't matter if it exists in dest\n                if isinstance(v, str):\n                    # just overwrite dest - str adds/edits indiscriminately\n                    current[k] = self.get_formatted_string(v)\n                elif isinstance(v, (bytes, bytearray)):\n                    # bytes aren't mergable or formattable\n                    # only here to prevent the elif on enumerables catching it\n                    current[k] = v\n                # deal with things that are mergable - exists already in dest\n                elif k in current:\n                    if types.are_all_this_type(Mapping, current[k], v):\n                        # it's dict-y, thus recurse through it to merge since\n                        # it exists in dest\n                        merge_recurse(current[k], v)\n                    elif types.are_all_this_type(list, current[k], v):\n                        # it's list-y. Extend mutates existing list since it\n                        # exists in dest\n                        current[k].extend(\n                            self.get_formatted_iterable(v))\n                    elif types.are_all_this_type(tuple, current[k], v):\n                        # concatenate tuples\n                        current[k] = (\n                            current[k] + self.get_formatted_iterable(v))\n                    elif types.are_all_this_type(Set, current[k], v):\n                        # join sets\n                        current[k] = (\n                            current[k] | self.get_formatted_iterable(v))\n                    else:\n                        # at this point it's not mergable nor a known iterable\n                        current[k] = v\n                else:\n                    # at this point it's not mergable, nor in context\n                    current[k] = self.get_formatted_iterable(v)\n\n        # first iteration starts at context dict root\n        merge_recurse(self, add_me)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the values in the context dictionary to the values in the context dictionary.", "response": "def set_defaults(self, defaults):\n        \"\"\"Set defaults in context if keys do not exist already.\n\n        Adds the input dict (defaults) into the context, only where keys in\n        defaults do not already exist in context. Supports nested hierarchies.\n\n        Example:\n        Given a context like this:\n            key1: value1\n            key2:\n                key2.1: value2.1\n            key3: None\n\n        And defaults input like this:\n            key1: 'updated value here won't overwrite since it already exists'\n            key2:\n                key2.2: value2.2\n            key3: 'key 3 exists so I won't overwrite\n\n        Will result in context:\n            key1: value1\n            key2:\n                key2.1: value2.1\n                key2.2: value2.2\n            key3: None\n\n        Args:\n            defaults: dict. Add this dict into context.\n\n        Returns:\n            None. All operations mutate this instance of context.\n\n        \"\"\"\n        def defaults_recurse(current, defaults):\n            \"\"\"Walk the current context tree in recursive inner function.\n\n            On 1st iteration, current = self (i.e root of context)\n            On subsequent recursive iterations, current is wherever you're at\n            in the nested context hierarchy.\n\n            Args:\n                current: dict. Destination of merge.\n                defaults: dict. Add this to current if keys don't exist\n                                already.\n\n            \"\"\"\n            for k, v in defaults.items():\n                # key supports interpolation\n                k = self.get_formatted_string(k)\n\n                if k in current:\n                    if types.are_all_this_type(Mapping, current[k], v):\n                        # it's dict-y, thus recurse through it to check if it\n                        # contains child items that don't exist in dest\n                        defaults_recurse(current[k], v)\n                else:\n                    # since it's not in context already, add the default\n                    current[k] = self.get_formatted_iterable(v)\n\n        # first iteration starts at context dict root\n        defaults_recurse(self, defaults)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_step(self, rewriter):\n        assert rewriter, (\"FileRewriter instance required to run \"\n                          \"FileInRewriterStep.\")\n        rewriter.files_in_to_out(in_path=self.path_in, out_path=self.path_out)", "response": "Do the file in to out rewrite."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_step(self, representer):\n        assert representer, (\"ObjectRepresenter instance required to run \"\n                             \"ObjectRewriterStep.\")\n        rewriter = ObjectRewriter(self.context.get_formatted_iterable,\n                                  representer)\n        super().run_step(rewriter)", "response": "Do the object in - out rewrite."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_step(self):\n        rewriter = StreamRewriter(self.context.iter_formatted_strings)\n        super().run_step(rewriter)", "response": "Do the file in - out rewrite."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_step(self):\n        formatted_replacements = self.context.get_formatted_iterable(\n            self.replace_pairs)\n\n        iter = StreamReplacePairsRewriterStep.iter_replace_strings(\n            formatted_replacements)\n        rewriter = StreamRewriter(iter)\n        super().run_step(rewriter)", "response": "Write in to out replacing strings per the replace_pairs."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a function that returns a generator that yields a formatted string from each line in iterable_strings.", "response": "def iter_replace_strings(replacements):\n        \"\"\"Create a function that uses replacement pairs to process a string.\n\n        The returned function takes an iterator and yields on each processed\n        line.\n\n        Args:\n            replacements: Dict containing 'find_string': 'replace_string' pairs\n\n        Returns:\n            function with signature: iterator of strings = function(iterable)\n\n        \"\"\"\n        def function_iter_replace_strings(iterable_strings):\n            \"\"\"Yield a formatted string from iterable_strings using a generator.\n\n            Args:\n                iterable_strings: Iterable containing strings. E.g a file-like\n                                  object.\n\n            Returns:\n                Yields formatted line.\n\n            \"\"\"\n            for string in iterable_strings:\n                yield reduce((lambda s, kv: s.replace(*kv)),\n                             replacements.items(),\n                             string)\n\n        return function_iter_replace_strings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_step(context):\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='contextSetf', caller=__name__)\n\n    for k, v in context['contextSetf'].items():\n        logger.debug(f\"setting context {k} to value from context {v}\")\n        context[context.get_formatted_iterable(\n            k)] = context.get_formatted_iterable(v)\n\n    logger.info(f\"Set {len(context['contextSetf'])} context items.\")\n\n    logger.debug(\"done\")", "response": "Set new context keys from formatting expressions with substitutions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncast obj to out_type if it s not out_type already.", "response": "def cast_to_type(obj, out_type):\n    \"\"\"Cast obj to out_type if it's not out_type already.\n\n    If the obj happens to be out_type already, it just returns obj as is.\n\n    Args:\n        obj: input object\n        out_type: type.\n\n    Returns:\n        obj cast to out_type. Usual python conversion / casting rules apply.\n\n    \"\"\"\n    in_type = type(obj)\n    if out_type is in_type:\n        # no need to cast.\n        return obj\n    else:\n        return out_type(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_pipeline_yaml(file):\n    tag_representers = [PyString, SicString]\n\n    yaml_loader = get_yaml_parser_safe()\n\n    for representer in tag_representers:\n        yaml_loader.register_class(representer)\n\n    pipeline_definition = yaml_loader.load(file)\n    return pipeline_definition", "response": "Return pipeline yaml from open file object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the yaml parser object with this factory method.", "response": "def get_yaml_parser_roundtrip():\n    \"\"\"Create the yaml parser object with this factory method.\n\n    The round-trip parser preserves:\n    - comments\n    - block style and key ordering are kept, so you can diff the round-tripped\n      source\n    - flow style sequences ( \u2018a: b, c, d\u2019) (based on request and test by\n      Anthony Sottile)\n    - anchor names that are hand-crafted (i.e. not of the form``idNNN``)\n    - merges in dictionaries are preserved\n\n    Returns:\n        ruamel.yaml.YAML object with round-trip loader\n\n    \"\"\"\n    yaml_writer = yamler.YAML(typ='rt', pure=True)\n    # if this isn't here the yaml doesn't format nicely indented for humans\n    yaml_writer.indent(mapping=2, sequence=4, offset=2)\n    return yaml_writer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_yaml_parser_roundtrip_for_context():\n    yaml_writer = get_yaml_parser_roundtrip()\n\n    # Context is a dict data structure, so can just use a dict representer\n    yaml_writer.Representer.add_representer(\n        Context,\n        yamler.representer.RoundTripRepresenter.represent_dict)\n\n    return yaml_writer", "response": "Create a yaml parser that can serialize the pypyr Context"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_parsed_context(context_arg):\n    if not context_arg:\n        logger.debug(\"pipeline invoked without context arg set. For \"\n                     \"this keyvaluepairs parser you're looking for \"\n                     \"something like: \"\n                     \"pypyr pipelinename 'key1=value1,key2=value2'.\")\n        return None\n\n    logger.debug(\"starting\")\n    # for each comma-delimited element, project key=value\n    return dict(element.split('=') for element in context_arg.split(','))", "response": "Parse input context string and returns context as dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a json file into the pypyr context.", "response": "def run_step(context):\n    \"\"\"Load a json file into the pypyr context.\n\n    json parsed from the file will be merged into the pypyr context. This will\n    overwrite existing values if the same keys are already in there.\n    I.e if file json has {'eggs' : 'boiled'} and context {'eggs': 'fried'}\n    already exists, returned context['eggs'] will be 'boiled'.\n\n    The json should not be an array [] on the top level, but rather an Object.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key must exist\n                - fetchJson\n                    - path. path-like. Path to file on disk.\n                    - key. string. If exists, write json structure to this\n                      context key. Else json writes to context root.\n\n    Also supports a passing path as string to fetchJson, but in this case you\n    won't be able to specify a key.\n\n    All inputs support formatting expressions.\n\n    Returns:\n        None. updates context arg.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: fetchJson.path missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fetchJson.path exists but is\n                                                  None.\n\n    \"\"\"\n    logger.debug(\"started\")\n\n    deprecated(context)\n\n    context.assert_key_has_value(key='fetchJson', caller=__name__)\n\n    fetch_json_input = context.get_formatted('fetchJson')\n\n    if isinstance(fetch_json_input, str):\n        file_path = fetch_json_input\n        destination_key_expression = None\n    else:\n        context.assert_child_key_has_value(parent='fetchJson',\n                                           child='path',\n                                           caller=__name__)\n        file_path = fetch_json_input['path']\n        destination_key_expression = fetch_json_input.get('key', None)\n\n    logger.debug(f\"attempting to open file: {file_path}\")\n    with open(file_path) as json_file:\n        payload = json.load(json_file)\n\n    if destination_key_expression:\n        destination_key = context.get_formatted_iterable(\n            destination_key_expression)\n        logger.debug(f\"json file loaded. Writing to context {destination_key}\")\n        context[destination_key] = payload\n    else:\n        if not isinstance(payload, MutableMapping):\n            raise TypeError(\n                'json input should describe an object at the top '\n                'level when fetchJsonKey isn\\'t specified. You should have '\n                'something like {\"key1\": \"value1\", \"key2\": \"value2\"} '\n                'in the json top-level, not [\"value1\", \"value2\"]')\n\n        logger.debug(\"json file loaded. Merging into pypyr context. . .\")\n        context.update(payload)\n\n    logger.info(f\"json file written into pypyr context. Count: {len(payload)}\")\n    logger.debug(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates new style in params from deprecated.", "response": "def deprecated(context):\n    \"\"\"Create new style in params from deprecated.\"\"\"\n    if 'fetchJsonPath' in context:\n        context.assert_key_has_value(key='fetchJsonPath', caller=__name__)\n\n        context['fetchJson'] = {'path': context['fetchJsonPath']}\n\n        if 'fetchJsonKey' in context:\n            context['fetchJson']['key'] = context.get('fetchJsonKey', None)\n\n        logger.warning(\"fetchJsonPath and fetchJsonKey \"\n                       \"are deprecated. They will stop working upon the next \"\n                       \"major release. Use the new context key fetchJson \"\n                       \"instead. It's a lot better, promise! For the moment \"\n                       \"pypyr is creating the new fetchJson key for you \"\n                       \"under the hood.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck to see if we should ignore the request.", "response": "def _ignore_request(self, path):\n        \"\"\"Check to see if we should ignore the request.\"\"\"\n        return any([\n            re.match(pattern, path) for pattern in QC_SETTINGS['IGNORE_REQUEST_PATTERNS']\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ignore_sql(self, query):\n        return any([\n            re.search(pattern, query.get('sql')) for pattern in QC_SETTINGS['IGNORE_SQL_PATTERNS']\n        ])", "response": "Check to see if we should ignore the sql query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nappending the most common duplicate queries to the given output.", "response": "def _duplicate_queries(self, output):\n        \"\"\"Appends the most common duplicate queries to the given output.\"\"\"\n        if QC_SETTINGS['DISPLAY_DUPLICATES']:\n            for query, count in self.queries.most_common(QC_SETTINGS['DISPLAY_DUPLICATES']):\n                lines = ['\\nRepeated {0} times.'.format(count)]\n                lines += wrap(query)\n                lines = \"\\n\".join(lines) + \"\\n\"\n                output += self._colorize(lines, count)\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calculate_num_queries(self):\n        request_totals = self._totals(\"request\")\n        response_totals = self._totals(\"response\")\n\n        return request_totals[2] + response_totals[2]", "response": "Calculate the total number of request and response queries."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_settings(**kwargs):\n\n    # If we are in this method due to a signal, only reload for our settings\n    setting_name = kwargs.get('setting', None) \n    if setting_name is not None and setting_name != 'QUERYCOUNT':\n        return\n\n    # Support the old-style settings\n    if getattr(settings, 'QUERYCOUNT_THRESHOLDS', False):\n        QC_SETTINGS['THRESHOLDS'] = settings.QUERYCOUNT_THRESHOLDS\n\n    # Apply new-style settings\n    if not getattr(settings, 'QUERYCOUNT', False):\n        return\n\n    # Duplicate display is a special case, configure it specifically\n    if 'DISPLAY_DUPLICATES' in settings.QUERYCOUNT:\n        duplicate_settings = settings.QUERYCOUNT['DISPLAY_DUPLICATES']\n        if duplicate_settings is not None:\n            duplicate_settings = int(duplicate_settings)\n        QC_SETTINGS['DISPLAY_DUPLICATES'] = duplicate_settings\n\n    # Apply the rest of the setting overrides\n    for key in ['THRESHOLDS',\n                'IGNORE_REQUEST_PATTERNS',\n                'IGNORE_SQL_PATTERNS',\n                'IGNORE_PATTERNS',\n                'RESPONSE_HEADER']:\n        if key in settings.QUERYCOUNT:\n            QC_SETTINGS[key] = settings.QUERYCOUNT[key]", "response": "Apply user supplied settings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlogin to a phone or email.", "response": "def login(password, phone=None, email=None, rememberLogin=True):\n    \"\"\" \u767b\u5f55\u63a5\u53e3\uff0c\u8fd4\u56de :class:'Response' \u5bf9\u8c61\n    :param password: \u7f51\u6613\u4e91\u97f3\u4e50\u7684\u5bc6\u7801\n    :param phone: (optional) \u624b\u673a\u767b\u5f55\n    :param email: (optional) \u90ae\u7bb1\u767b\u5f55\n    :param rememberLogin: (optional) \u662f\u5426\u8bb0\u4f4f\u5bc6\u7801\uff0c\u9ed8\u8ba4 True\n    \"\"\"\n    if (phone is None) and (email is None):\n        raise ParamsError()\n    if password is None:\n        raise ParamsError()\n    r = NCloudBot()\n    # r.username = phone or email\n\n    md5 = hashlib.md5()\n    md5.update(password)\n    password = md5.hexdigest()\n    print password\n    r.data = {'password': password, 'rememberLogin': rememberLogin}\n    if phone is not None:\n        r.data['phone'] = phone\n        r.method = 'LOGIN'\n    else:\n        r.data['username'] = email\n        r.method = 'EMAIL_LOGIN'\n    r.send()\n\n    return r.response"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget user s play list", "response": "def user_play_list(uid, offset=0, limit=1000):\n    \"\"\"\u83b7\u53d6\u7528\u6237\u6b4c\u5355\uff0c\u5305\u542b\u6536\u85cf\u7684\u6b4c\u5355\n    \n    :param uid: \u7528\u6237\u7684ID\uff0c\u53ef\u901a\u8fc7\u767b\u5f55\u6216\u8005\u5176\u4ed6\u63a5\u53e3\u83b7\u53d6\n    :param offset: (optional) \u5206\u6bb5\u8d77\u59cb\u4f4d\u7f6e\uff0c\u9ed8\u8ba4 0\n    :param limit: (optional) \u6570\u636e\u4e0a\u9650\u591a\u5c11\u884c\uff0c\u9ed8\u8ba4 1000\n    \"\"\"\n    if uid is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'USER_PLAY_LIST'\n    r.data = {'offset': offset, 'uid': uid, 'limit': limit, 'csrf_token': ''}\n    r.send()\n    return r.response"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a user s base dj.", "response": "def user_dj(uid, offset=0, limit=30):\n    \"\"\"\u83b7\u53d6\u7528\u6237\u7535\u53f0\u6570\u636e\n\n    :param uid: \u7528\u6237\u7684ID\uff0c\u53ef\u901a\u8fc7\u767b\u5f55\u6216\u8005\u5176\u4ed6\u63a5\u53e3\u83b7\u53d6\n    :param offset: (optional) \u5206\u6bb5\u8d77\u59cb\u4f4d\u7f6e\uff0c\u9ed8\u8ba4 0\n    :param limit: (optional) \u6570\u636e\u4e0a\u9650\u591a\u5c11\u884c\uff0c\u9ed8\u8ba4 30\n    \"\"\"\n    if uid is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'USER_DJ'\n    r.data = {'offset': offset, 'limit': limit, \"csrf_token\": \"\"}\n    r.params = {'uid': uid}\n    r.send()\n\n    return r.response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching the NCloud API for a specific resource.", "response": "def search(keyword, type=1, offset=0, limit=30):\n    \"\"\"\u641c\u7d22\u6b4c\u66f2\uff0c\u652f\u6301\u641c\u7d22\u6b4c\u66f2\u3001\u6b4c\u624b\u3001\u4e13\u8f91\u7b49\n\n    :param keyword: \u5173\u952e\u8bcd\n    :param type: (optional) \u641c\u7d22\u7c7b\u578b\uff0c1: \u5355\u66f2, 100: \u6b4c\u624b, 1000: \u6b4c\u5355, 1002: \u7528\u6237\n    :param offset: (optional) \u5206\u6bb5\u8d77\u59cb\u4f4d\u7f6e\uff0c\u9ed8\u8ba4 0\n    :param limit: (optional) \u6570\u636e\u4e0a\u9650\u591a\u5c11\u884c\uff0c\u9ed8\u8ba4 30\n    \"\"\"\n    if keyword is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'SEARCH'\n    r.data = {\n        's': keyword,\n        'limit': str(limit),\n        'type': str(type),\n        'offset': str(offset)\n    }\n    r.send()\n\n    return r.response"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets user follow list", "response": "def user_follows(uid, offset='0', limit=30):\n    \"\"\"\u83b7\u53d6\u7528\u6237\u5173\u6ce8\u5217\u8868\n\n    :param uid: \u7528\u6237\u7684ID\uff0c\u53ef\u901a\u8fc7\u767b\u5f55\u6216\u8005\u5176\u4ed6\u63a5\u53e3\u83b7\u53d6\n    :param offset: (optional) \u5206\u6bb5\u8d77\u59cb\u4f4d\u7f6e\uff0c\u9ed8\u8ba4 0\n    :param limit: (optional) \u6570\u636e\u4e0a\u9650\u591a\u5c11\u884c\uff0c\u9ed8\u8ba4 30\n    \"\"\"\n    if uid is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'USER_FOLLOWS'\n    r.params = {'uid': uid}\n    r.data = {'offset': offset, 'limit': limit, 'order': True}\n    r.send()\n\n    return r.response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef user_event(uid):\n    if uid is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'USER_EVENT'\n    r.params = {'uid': uid}\n    r.data = {'time': -1, 'getcounts': True, \"csrf_token\": \"\"}\n    r.send()\n\n    return r.response", "response": "\u83b7\u53d6\u7528\u6237\u52a8\u6001\n\n    :param uid: \u7528\u6237\u7684ID\uff0c\u53ef\u901a\u8fc7\u767b\u5f55\u6216\u8005\u5176\u4ed6\u63a5\u53e3\u83b7\u53d6"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets event from NCloudBot", "response": "def event():\n    \"\"\"\u83b7\u53d6\u597d\u53cb\u7684\u52a8\u6001\uff0c\u5305\u62ec\u5206\u4eab\u89c6\u9891\u3001\u97f3\u4e50\u3001\u52a8\u6001\u7b49\n\n    \"\"\"\n    r = NCloudBot()\n    r.method = 'EVENT'\n    r.data = {\"csrf_token\": \"\"}\n    r.send()\n\n    return r.response"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef play_list_detail(id, limit=20):\n    if id is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'PLAY_LIST_DETAIL'\n    r.data = {'id': id, 'limit': limit, \"csrf_token\": \"\"}\n    r.send()\n\n    return r.response", "response": "Get a list of detail items."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef music_url(ids=[]):\n    if not isinstance(ids, list):\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'MUSIC_URL'\n    r.data = {'ids': ids, 'br': 999000, \"csrf_token\": \"\"}\n    r.send()\n\n    return r.response", "response": "Music URL \u83b7\u53d6\u6b4c\u66f2 ID"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting lyric from id", "response": "def lyric(id):\n    \"\"\"\u901a\u8fc7\u6b4c\u66f2 ID \u83b7\u53d6\u6b4c\u66f2\u6b4c\u8bcd\u5730\u5740\n\n    :param id: \u6b4c\u66f2ID\n    \"\"\"\n    if id is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'LYRIC'\n    r.params = {'id': id}\n    r.send()\n\n    return r.response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef music_comment(id, offset=0, limit=20):\n    if id is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'MUSIC_COMMENT'\n    r.params = {'id': id}\n    r.data = {'offset': offset, 'limit': limit, 'rid': id, \"csrf_token\": \"\"}\n    r.send()\n\n    return r.response", "response": "get a music comment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef song_detail(ids):\n    if not isinstance(ids, list):\n        raise ParamsError()\n    c = []\n    for id in ids:\n        c.append({'id': id})\n    r = NCloudBot()\n    r.method = 'SONG_DETAIL'\n    r.data = {'c': json.dumps(c), 'ids': c, \"csrf_token\": \"\"}\n    r.send()\n\n    return r.response", "response": "\u901a\u8fc7\u6b4c\u66f2 ID \u83b7\u53d6\u6b4c\u66f2\u7684\u8be6\u7ec6\u4fe1\u606f\n\n    :param ids: \u6b4c\u66f2 ID \u7684 list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef personal_fm():\n    r = NCloudBot()\n    r.method = 'PERSONAL_FM'\n    r.data = {\"csrf_token\": \"\"}\n    r.send()\n    return r.response", "response": "get a personal FM"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_webapi_requests(self):\n\n        headers = {\n            'Accept':\n            '*/*',\n            'Accept-Language':\n            'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4',\n            'Connection':\n            'keep-alive',\n            'Content-Type':\n            'application/x-www-form-urlencoded',\n            'Referer':\n            'http://music.163.com',\n            'Host':\n            'music.163.com',\n            'User-Agent':\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.152 Safari/537.36'\n        }\n        NCloudBot.req.headers.update(headers)\n        return NCloudBot.req", "response": "Update headers of webapi for Requests."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_response(self, resp):\n        # rememberLogin\n        # if self.method is 'LOGIN' and resp.json().get('code') == 200:\n        #     cookiesJar.save_cookies(resp, NCloudBot.username)\n        self.response.content = resp.content\n        self.response.status_code = resp.status_code\n        self.response.headers = resp.headers", "response": "Build internal Response object from given response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef json(self):\n\n        if not self.headers and len(self.content) > 3:\n            encoding = get_encoding_from_headers(self.headers)\n            if encoding is not None:\n                return json.loads(self.content.decode(encoding))\n        return json.loads(self.content)", "response": "Returns the json - encoded content of a response if any."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the value of an option in the globals", "response": "def set_option(name, value):\n    \"\"\"\n    Set plydata option\n\n    Parameters\n    ----------\n    name : str\n        Name of the option\n    value : object\n        New value of the option\n\n    Returns\n    -------\n    old : object\n        Old value of the option\n\n    See also\n    --------\n    :class:`options`\n    \"\"\"\n    old = get_option(name)\n    globals()[name] = value\n    return old"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _join(verb):\n    data = pd.merge(verb.x, verb.y, **verb.kwargs)\n\n    # Preserve x groups\n    if isinstance(verb.x, GroupedDataFrame):\n        data.plydata_groups = list(verb.x.plydata_groups)\n    return data", "response": "Join a single verb with another verb"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef groupby(self, by=None, **kwargs):\n        if by is None:\n            by = self.plydata_groups\n\n        # Turn off sorting by groups messes with some verbs\n        if 'sort' not in kwargs:\n            kwargs['sort'] = False\n\n        return super().groupby(by, **kwargs)", "response": "Group by and do not sort"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group_indices(self):\n        # No groups\n        if not self.plydata_groups:\n            return np.ones(len(self), dtype=int)\n\n        grouper = self.groupby()\n        indices = np.empty(len(self), dtype=int)\n        for i, (_, idx) in enumerate(sorted(grouper.indices.items())):\n            indices[idx] = i\n        return indices", "response": "Return the indices of the entries in the n - th entry table that are in the n - th entry table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates function that prepares a verb for the current node.", "response": "def _make_verb_helper(verb_func, add_groups=False):\n    \"\"\"\n    Create function that prepares verb for the verb function\n\n    The functions created add expressions to be evaluated to\n    the verb, then call the core verb function\n\n    Parameters\n    ----------\n    verb_func : function\n        Core verb function. This is the function called after\n        expressions created and added to the verb. The core\n        function should be one of those that implement verbs that\n        evaluate expressions.\n    add_groups : bool\n        If True, a groups attribute is added to the verb. The\n        groups are the columns created after evaluating the\n        expressions.\n\n    Returns\n    -------\n    out : function\n        A function that implements a helper verb.\n    \"\"\"\n\n    @wraps(verb_func)\n    def _verb_func(verb):\n        verb.expressions, new_columns = build_expressions(verb)\n        if add_groups:\n            verb.groups = new_columns\n        return verb_func(verb)\n\n    return _verb_func"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_base_dataframe(df):\n    if isinstance(df, GroupedDataFrame):\n        base_df = GroupedDataFrame(\n            df.loc[:, df.plydata_groups], df.plydata_groups,\n            copy=True)\n    else:\n        base_df = pd.DataFrame(index=df.index)\n    return base_df", "response": "Returns a dataframe that contains all columns other than those grouped on\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd group columns to data with a value from the grouped dataframe gdf", "response": "def _add_group_columns(data, gdf):\n    \"\"\"\n    Add group columns to data with a value from the grouped dataframe\n\n    It is assumed that the grouped dataframe contains a single group\n\n    >>> data = pd.DataFrame({\n    ...     'x': [5, 6, 7]})\n    >>> gdf = GroupedDataFrame({\n    ...     'g': list('aaa'),\n    ...     'x': range(3)}, groups=['g'])\n    >>> _add_group_columns(data, gdf)\n       g  x\n    0  a  5\n    1  a  6\n    2  a  7\n    \"\"\"\n    n = len(data)\n    if isinstance(gdf, GroupedDataFrame):\n        for i, col in enumerate(gdf.plydata_groups):\n            if col not in data:\n                group_values = [gdf[col].iloc[0]] * n\n                # Need to be careful and maintain the dtypes\n                # of the group columns\n                if pdtypes.is_categorical_dtype(gdf[col]):\n                    col_values = pd.Categorical(\n                        group_values,\n                        categories=gdf[col].cat.categories,\n                        ordered=gdf[col].cat.ordered\n                    )\n                else:\n                    col_values = pd.Series(\n                        group_values,\n                        index=data.index,\n                        dtype=gdf[col].dtype\n                    )\n                # Group columns come first\n                data.insert(i, col, col_values)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a column in dataframe that contains the object with the specified name and value.", "response": "def _create_column(data, col, value):\n    \"\"\"\n    Create column in dataframe\n\n    Helper method meant to deal with problematic\n    column values. e.g When the series index does\n    not match that of the data.\n\n    Parameters\n    ----------\n    data : pandas.DataFrame\n        dataframe in which to insert value\n    col : column label\n        Column name\n    value : object\n        Value to assign to column\n\n    Returns\n    -------\n    data : pandas.DataFrame\n        Modified original dataframe\n\n    >>> df = pd.DataFrame({'x': [1, 2, 3]})\n    >>> y = pd.Series([11, 12, 13], index=[21, 22, 23])\n\n    Data index and value index do not match\n\n    >>> _create_column(df, 'y', y)\n       x   y\n    0  1  11\n    1  2  12\n    2  3  13\n\n    Non-empty dataframe, scalar value\n\n    >>> _create_column(df, 'z', 3)\n       x   y  z\n    0  1  11  3\n    1  2  12  3\n    2  3  13  3\n\n    Empty dataframe, scalar value\n\n    >>> df = pd.DataFrame()\n    >>> _create_column(df, 'w', 3)\n       w\n    0  3\n    >>> _create_column(df, 'z', 'abc')\n       w    z\n    0  3  abc\n    \"\"\"\n    with suppress(AttributeError):\n        # If the index of a series and the dataframe\n        # in which the series will be assigned to a\n        # column do not match, missing values/NaNs\n        # are created. We do not want that.\n        if not value.index.equals(data.index):\n            if len(value) == len(data):\n                value.index = data.index\n            else:\n                value.reset_index(drop=True, inplace=True)\n\n    # You cannot assign a scalar value to a dataframe\n    # without an index. You need an interable value.\n    if data.index.empty:\n        try:\n            len(value)\n        except TypeError:\n            scalar = True\n        else:\n            scalar = isinstance(value, str)\n\n        if scalar:\n            value = [value]\n\n    data[col] = value\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the expressions for the given verb and returns the resulting list of expressions.", "response": "def build_expressions(verb):\n    \"\"\"\n    Build expressions for helper verbs\n\n    Parameters\n    ----------\n    verb : verb\n        A verb with a *functions* attribute.\n\n    Returns\n    -------\n    out : tuple\n        (List of Expressions, New columns). The expressions and the\n        new columns in which the results of those expressions will\n        be stored. Even when a result will stored in a column with\n        an existing label, that column is still considered new,\n        i.e An expression ``x='x+1'``, will create a new_column `x`\n        to replace an old column `x`.\n    \"\"\"\n    def partial(func, col, *args, **kwargs):\n        \"\"\"\n        Make a function that acts on a column in a dataframe\n\n        Parameters\n        ----------\n        func : callable\n            Function\n        col : str\n            Column\n        args : tuple\n            Arguments to pass to func\n        kwargs : dict\n            Keyword arguments to func\n\n        Results\n        -------\n        new_func : callable\n            Function that takes a dataframe, and calls the\n            original function on a column in the dataframe.\n        \"\"\"\n        def new_func(gdf):\n            return func(gdf[col], *args, **kwargs)\n\n        return new_func\n\n    def make_statement(func, col):\n        \"\"\"\n        A statement of function called on a column in a dataframe\n\n        Parameters\n        ----------\n        func : str or callable\n            Function to call on a dataframe column\n        col : str\n            Column\n        \"\"\"\n        if isinstance(func, str):\n            expr = '{}({})'.format(func, col)\n        elif callable(func):\n            expr = partial(func, col, *verb.args, **verb.kwargs)\n        else:\n            raise TypeError(\"{} is not a function\".format(func))\n        return expr\n\n    def func_name(func):\n        \"\"\"\n        Return name of a function.\n\n        If the function is `np.sin`, we return `sin`.\n        \"\"\"\n        if isinstance(func, str):\n            return func\n\n        try:\n            return func.__name__\n        except AttributeError:\n            return ''\n\n    # Generate function names. They act as identifiers (postfixed\n    # to the original columns) in the new_column names.\n    if isinstance(verb.functions, (tuple, list)):\n        names = (func_name(func) for func in verb.functions)\n        names_and_functions = zip(names, verb.functions)\n    else:\n        names_and_functions = verb.functions.items()\n\n    # Create statements for the expressions\n    # and postfix identifiers\n    columns = Selector.get(verb)  # columns to act on\n    postfixes = []\n    stmts = []\n    for name, func in names_and_functions:\n        postfixes.append(name)\n        for col in columns:\n            stmts.append(make_statement(func, col))\n\n    if not stmts:\n        stmts = columns\n\n    # Names of the new columns\n    # e.g col1_mean, col2_mean, col1_std, col2_std\n    add_postfix = (isinstance(verb.functions, dict) or\n                   len(verb.functions) > 1)\n    if add_postfix:\n        fmt = '{}_{}'.format\n        new_columns = [fmt(c, p) for p in postfixes for c in columns]\n    else:\n        new_columns = columns\n\n    expressions = [Expression(stmt, col)\n                   for stmt, col in zip(stmts, new_columns)]\n    return expressions, new_columns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the expressions and return the resulting dataframe", "response": "def process(self):\n        \"\"\"\n        Run the expressions\n\n        Returns\n        -------\n        out : pandas.DataFrame\n            Resulting data\n        \"\"\"\n        # Short cut\n        if self._all_expressions_evaluated():\n            if self.drop:\n                # Drop extra columns. They do not correspond to\n                # any expressions.\n                columns = [expr.column for expr in self.expressions]\n                self.data = self.data.loc[:, columns]\n            return self.data\n\n        # group_by\n        # evaluate expressions\n        # combine columns\n        # concat evalutated group data and clean up index and group\n        gdfs = self._get_group_dataframes()\n        egdfs = self._evaluate_expressions(gdfs)\n        edata = self._concat(egdfs)\n        return edata"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _all_expressions_evaluated(self):\n        def present(expr):\n            return expr.stmt == expr.column and expr.column in self.data\n        return all(present(expr) for expr in self.expressions)", "response": "Return True if all expressions match with the columns\n            Return False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting group dataframes Returns ------- out : tuple or generator Group dataframes", "response": "def _get_group_dataframes(self):\n        \"\"\"\n        Get group dataframes\n\n        Returns\n        -------\n        out : tuple or generator\n            Group dataframes\n        \"\"\"\n        if isinstance(self.data, GroupedDataFrame):\n            grouper = self.data.groupby()\n            # groupby on categorical columns uses the categories\n            # even if they are not present in the data. This\n            # leads to empty groups. We exclude them.\n            return (gdf for _, gdf in grouper if not gdf.empty)\n        else:\n            return (self.data, )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _evaluate_group_dataframe(self, gdf):\n        gdf._is_copy = None\n        result_index = gdf.index if self.keep_index else []\n        data = pd.DataFrame(index=result_index)\n        for expr in self.expressions:\n            value = expr.evaluate(gdf, self.env)\n            if isinstance(value, pd.DataFrame):\n                data = value\n                break\n            else:\n                _create_column(data, expr.column, value)\n        data = _add_group_columns(data, gdf)\n        return data", "response": "Evaluate a single group dataframe and return a pandas. DataFrame with the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconcatenates evaluated group dataframes into a single DataFrame.", "response": "def _concat(self, egdfs):\n        \"\"\"\n        Concatenate evaluated group dataframes\n\n        Parameters\n        ----------\n        egdfs : iterable\n            Evaluated dataframes\n\n        Returns\n        -------\n        edata : pandas.DataFrame\n            Evaluated data\n        \"\"\"\n        egdfs = list(egdfs)\n        edata = pd.concat(egdfs, axis=0, ignore_index=False, copy=False)\n\n        # groupby can mixup the rows. We try to maintain the original\n        # order, but we can only do that if the result has a one to\n        # one relationship with the original\n        one2one = (\n            self.keep_index and\n            not any(edata.index.duplicated()) and\n            len(edata.index) == len(self.data.index))\n        if one2one:\n            edata = edata.sort_index()\n        else:\n            edata.reset_index(drop=True, inplace=True)\n\n        # Maybe this should happen in the verb functions\n        if self.keep_groups and self.groups:\n            edata = GroupedDataFrame(edata, groups=self.groups)\n        return edata"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts any slices into column names", "response": "def _resolve_slices(data_columns, names):\n        \"\"\"\n        Convert any slices into column names\n\n        Parameters\n        ----------\n        data_columns : pandas.Index\n            Dataframe columns\n        names : tuple\n            Names (including slices) of columns in the\n            dataframe.\n\n        Returns\n        -------\n        out : tuple\n            Names of columns in the dataframe. Has no\n            slices.\n        \"\"\"\n        def _get_slice_cols(sc):\n            \"\"\"\n            Convert slice to list of names\n            \"\"\"\n            # Just like pandas.DataFrame.loc the stop\n            # column is included\n            idx_start = data_columns.get_loc(sc.start)\n            idx_stop = data_columns.get_loc(sc.stop) + 1\n            return data_columns[idx_start:idx_stop:sc.step]\n\n        result = []\n        for col in names:\n            if isinstance(col, slice):\n                result.extend(_get_slice_cols(col))\n            else:\n                result.append(col)\n        return tuple(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the selected columns for the verb.", "response": "def select(cls, verb):\n        \"\"\"\n        Return selected columns for the select verb\n\n        Parameters\n        ----------\n        verb : object\n            verb with the column selection attributes:\n\n                - names\n                - startswith\n                - endswith\n                - contains\n                - matches\n        \"\"\"\n        columns = verb.data.columns\n        contains = verb.contains\n        matches = verb.matches\n\n        groups = _get_groups(verb)\n        names = cls._resolve_slices(columns, verb.names)\n        names_set = set(names)\n        groups_set = set(groups)\n        lst = [[]]\n\n        if names or groups:\n            # group variable missing from the selection are prepended\n            missing = [g for g in groups if g not in names_set]\n            missing_set = set(missing)\n            c1 = missing + [x for x in names if x not in missing_set]\n            lst.append(c1)\n\n        if verb.startswith:\n            c2 = [x for x in columns\n                  if isinstance(x, str) and x.startswith(verb.startswith)]\n            lst.append(c2)\n\n        if verb.endswith:\n            c3 = [x for x in columns if\n                  isinstance(x, str) and x.endswith(verb.endswith)]\n            lst.append(c3)\n\n        if contains:\n            c4 = []\n            for col in columns:\n                if (isinstance(col, str) and\n                        any(s in col for s in contains)):\n                    c4.append(col)\n            lst.append(c4)\n\n        if matches:\n            c5 = []\n            patterns = [x if hasattr(x, 'match') else re.compile(x)\n                        for x in matches]\n            for col in columns:\n                if isinstance(col, str):\n                    if any(bool(p.match(col)) for p in patterns):\n                        c5.append(col)\n\n            lst.append(c5)\n\n        selected = unique(list(itertools.chain(*lst)))\n\n        if verb.drop:\n            to_drop = [col for col in selected if col not in groups_set]\n            selected = [col for col in columns if col not in to_drop]\n\n        return selected"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _all(cls, verb):\n        groups = set(_get_groups(verb))\n        return [col for col in verb.data if col not in groups]", "response": "A helper function to return all the columns that are not in groups"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_verb_function(data, verb):\n    try:\n        module = type_lookup[type(data)]\n    except KeyError:\n        # Some guess work for subclasses\n        for type_, mod in type_lookup.items():\n            if isinstance(data, type_):\n                module = mod\n                break\n    try:\n        return getattr(module, verb)\n    except (NameError, AttributeError):\n        msg = \"Data source of type '{}' is not supported.\"\n        raise TypeError(msg.format(type(data)))", "response": "Return function that implements the verb for given data type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an appropriate Expression given the arguments passed to the Expression class.", "response": "def Expression(*args, **kwargs):\n    \"\"\"\n    Return an appropriate Expression given the arguments\n\n    Parameters\n    ----------\n    args : tuple\n        Positional arguments passed to the Expression class\n    kwargs : dict\n        Keyword arguments passed to the Expression class\n    \"\"\"\n    # dispatch\n    if not hasattr(args[0], '_Expression'):\n        return BaseExpression(*args, *kwargs)\n    else:\n        return args[0]._Expression(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef evaluate(self, data, env):\n        def n():\n            \"\"\"\n            Return number of rows in groups\n\n            This function is part of the public API\n            \"\"\"\n            return len(data)\n\n        if isinstance(self.stmt, str):\n            # Add function n() that computes the\n            # size of the group data to the inner namespace.\n            if self._has_n_func:\n                namespace = dict(data, n=n)\n            else:\n                namespace = data\n            # Avoid obvious keywords e.g if a column\n            # is named class\n            if self.stmt not in KEYWORDS:\n                value = env.eval(\n                    self.stmt,\n                    source_name='Expression.evaluate',\n                    inner_namespace=namespace)\n            else:\n                value = namespace[self.stmt]\n        elif callable(self.stmt):\n            value = self.stmt(data)\n        else:\n            value = self.stmt\n        return value", "response": "Evaluate the statement and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating the predicates and values of the pv_expressions and return the result as a list of numpy arrays.", "response": "def evaluate(self, data, env):\n        \"\"\"\n        Evaluate the predicates and values\n        \"\"\"\n        # For each predicate-value, we keep track of the positions\n        # that have been copied to the result, so that the later\n        # more general values do not overwrite the previous ones.\n        result = np.repeat(None, len(data))\n        copied = np.repeat(False, len(data))\n        for pred_expr, value_expr in self.pv_expressions:\n            bool_idx = pred_expr.evaluate(data, env)\n            if not pdtypes.is_bool_dtype(np.asarray(bool_idx)):\n                raise TypeError(\n                    \"The predicate keys must return a boolean array, \"\n                    \"or a boolean value.\")\n            value = value_expr.evaluate(data, env)\n            mask = (copied ^ bool_idx) & bool_idx\n            copied |= bool_idx\n            idx = np.where(mask)[0]\n            result[idx] = self.nice_value(value, idx)\n        return np.array(list(result))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef evaluate(self, data, env):\n        bool_idx = self.predicate_expr.evaluate(data, env)\n        true_value = self.true_value_expr.evaluate(data, env)\n        false_value = self.false_value_expr.evaluate(data, env)\n        true_idx = np.where(bool_idx)[0]\n        false_idx = np.where(~bool_idx)[0]\n        result = np.repeat(None, len(data))\n        result[true_idx] = self.nice_value(true_value, true_idx)\n        result[false_idx] = self.nice_value(false_value, false_idx)\n        return np.array(list(result))", "response": "Evaluate the predicates and values\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new EvalEnvironment with an extra namespace added.", "response": "def with_outer_namespace(self, outer_namespace):\n        \"\"\"Return a new EvalEnvironment with an extra namespace added.\n        This namespace will be used only for variables that are not found in\n        any existing namespace, i.e., it is \"outside\" them all.\"\"\"\n        return self.__class__(self._namespaces + [outer_namespace],\n                              self.flags)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate some Python code in the encapsulated environment.", "response": "def eval(self, expr, source_name=\"<string>\", inner_namespace={}):\n        \"\"\"Evaluate some Python code in the encapsulated environment.\n        :arg expr: A string containing a Python expression.\n        :arg source_name: A name for this string, for use in tracebacks.\n        :arg inner_namespace: A dict-like object that will be checked first\n          when `expr` attempts to access any variables.\n        :returns: The value of `expr`.\n        \"\"\"\n        code = compile(expr, source_name, \"eval\", self.flags, False)\n        return eval(code, {}, VarLookupDict([inner_namespace]\n                                            + self._namespaces))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef capture(cls, eval_env=0, reference=0):\n        if isinstance(eval_env, cls):\n            return eval_env\n        elif isinstance(eval_env, numbers.Integral):\n            depth = eval_env + reference\n        else:\n            raise TypeError(\"Parameter 'eval_env' must be either an integer \"\n                            \"or an instance of patsy.EvalEnvironment.\")\n        frame = inspect.currentframe()\n        try:\n            for i in range(depth + 1):\n                if frame is None:\n                    raise ValueError(\"call-stack is not that deep!\")\n                frame = frame.f_back\n            return cls([frame.f_locals, frame.f_globals],\n                       frame.f_code.co_flags & _ALL_FUTURE_FLAGS)\n        # The try/finally is important to avoid a potential reference cycle --\n        # any exception traceback will carry a reference to *our* frame, which\n        # contains a reference to our local variables, which would otherwise\n        # carry a reference to some parent frame, where the exception was\n        # caught...:\n        finally:\n            del frame", "response": "Capture an execution environment from the stack."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subset(self, names):\n        vld = VarLookupDict(self._namespaces)\n        new_ns = dict((name, vld[name]) for name in names)\n        return EvalEnvironment([new_ns], self.flags)", "response": "Creates a new flat EvalEnvironment that contains only\n        the variables specified."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nquotes a variable name A way to 'quote' variable names, especially ones that do not otherwise meet Python's variable name rules. Parameters ---------- name : str Name of variable Returns ------- value : object Value of variable Examples -------- >>> import pandas as pd >>> from plydata import define >>> df = pd.DataFrame({'class': [10, 20, 30]}) Since ``class`` is a reserved python keyword it cannot be a variable name, and therefore cannot be used in an expression without quoting it. >>> df >> define(y='class+1') Traceback (most recent call last): File \"<string>\", line 1 class+1 ^ SyntaxError: invalid syntax >>> df >> define(y='Q(\"class\")+1') class y 0 10 11 1 20 21 2 30 31 Note that it is ``'Q(\"some name\")'`` and not ``'Q(some name)'``. As in the above example, you do not need to ``import`` ``Q`` before you can use it.", "response": "def Q(name):\n    \"\"\"\n    Quote a variable name\n\n    A way to 'quote' variable names, especially ones that do not otherwise\n    meet Python's variable name rules.\n\n    Parameters\n    ----------\n    name : str\n        Name of variable\n\n    Returns\n    -------\n    value : object\n        Value of variable\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> from plydata import define\n    >>> df = pd.DataFrame({'class': [10, 20, 30]})\n\n    Since ``class`` is a reserved python keyword it cannot be a variable\n    name, and therefore cannot be used in an expression without quoting it.\n\n    >>> df >> define(y='class+1')\n    Traceback (most recent call last):\n      File \"<string>\", line 1\n        class+1\n            ^\n    SyntaxError: invalid syntax\n\n    >>> df >> define(y='Q(\"class\")+1')\n       class   y\n    0     10  11\n    1     20  21\n    2     30  31\n\n    Note that it is ``'Q(\"some name\")'`` and not ``'Q(some name)'``.\n    As in the above example, you do not need to ``import`` ``Q`` before\n    you can use it.\n    \"\"\"\n    env = EvalEnvironment.capture(1)\n    try:\n        return env.namespace[name]\n    except KeyError:\n        raise NameError(\"No data named {!r} found\".format(name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange & restore the indices of dataframes Dataframe with duplicate values can be hard to work with. When split and recombined, you cannot restore the row order. This can be the case even if the index has unique but irregular/unordered. This contextmanager resets the unordered indices of any dataframe passed to it, on exit it restores the original index. A regular index is of the form:: RangeIndex(start=0, stop=n, step=1) Parameters ---------- dfs : tuple Dataframes Yields ------ dfs : tuple Dataframe Examples -------- Create dataframes with different indices >>> df1 = pd.DataFrame([4, 3, 2, 1]) >>> df2 = pd.DataFrame([3, 2, 1], index=[3, 0, 0]) >>> df3 = pd.DataFrame([11, 12, 13], index=[11, 12, 13]) Within the contexmanager all frames have nice range indices >>> with regular_index(df1, df2, df3): ... print(df1.index) ... print(df2.index) ... print(df3.index) RangeIndex(start=0, stop=4, step=1) RangeIndex(start=0, stop=3, step=1) RangeIndex(start=0, stop=3, step=1) Indices restored >>> df1.index RangeIndex(start=0, stop=4, step=1) >>> df2.index Int64Index([3, 0, 0], dtype='int64') >>> df3.index Int64Index([11, 12, 13], dtype='int64')", "response": "def regular_index(*dfs):\n    \"\"\"\n    Change & restore the indices of dataframes\n\n    Dataframe with duplicate values can be hard to work with.\n    When split and recombined, you cannot restore the row order.\n    This can be the case even if the index has unique but\n    irregular/unordered. This contextmanager resets the unordered\n    indices of any dataframe passed to it, on exit it restores\n    the original index.\n\n    A regular index is of the form::\n\n        RangeIndex(start=0, stop=n, step=1)\n\n    Parameters\n    ----------\n    dfs : tuple\n        Dataframes\n\n    Yields\n    ------\n    dfs : tuple\n        Dataframe\n\n    Examples\n    --------\n    Create dataframes with different indices\n\n    >>> df1 = pd.DataFrame([4, 3, 2, 1])\n    >>> df2 = pd.DataFrame([3, 2, 1], index=[3, 0, 0])\n    >>> df3 = pd.DataFrame([11, 12, 13], index=[11, 12, 13])\n\n    Within the contexmanager all frames have nice range indices\n\n    >>> with regular_index(df1, df2, df3):\n    ...     print(df1.index)\n    ...     print(df2.index)\n    ...     print(df3.index)\n    RangeIndex(start=0, stop=4, step=1)\n    RangeIndex(start=0, stop=3, step=1)\n    RangeIndex(start=0, stop=3, step=1)\n\n    Indices restored\n\n    >>> df1.index\n    RangeIndex(start=0, stop=4, step=1)\n    >>> df2.index\n    Int64Index([3, 0, 0], dtype='int64')\n    >>> df3.index\n    Int64Index([11, 12, 13], dtype='int64')\n    \"\"\"\n    original_index = [df.index for df in dfs]\n    have_bad_index = [not isinstance(df.index, pd.RangeIndex)\n                      for df in dfs]\n\n    for df, bad in zip(dfs, have_bad_index):\n        if bad:\n            df.reset_index(drop=True, inplace=True)\n\n    try:\n        yield dfs\n    finally:\n        for df, bad, idx in zip(dfs, have_bad_index, original_index):\n            if bad and len(df.index) == len(idx):\n                df.index = idx"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unique(lst):\n    seen = set()\n\n    def make_seen(x):\n        seen.add(x)\n        return x\n\n    return [make_seen(x) for x in lst if x not in seen]", "response": "Returns unique elements in the order that they appear in the input list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the nth value of an array of n - th item.", "response": "def _nth(arr, n):\n    \"\"\"\n    Return the nth value of array\n\n    If it is missing return NaN\n    \"\"\"\n    try:\n        return arr.iloc[n]\n    except (KeyError, IndexError):\n        return np.nan"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert time to milliseconds.", "response": "def make_time(h=0, m=0, s=0, ms=0, frames=None, fps=None):\n    \"\"\"\n    Convert time to milliseconds.\n\n    See :func:`pysubs2.time.times_to_ms()`. When both frames and fps are specified,\n    :func:`pysubs2.time.frames_to_ms()` is called instead.\n\n    Raises:\n        ValueError: Invalid fps, or one of frames/fps is missing.\n\n    Example:\n        >>> make_time(s=1.5)\n        1500\n        >>> make_time(frames=50, fps=25)\n        2000\n\n    \"\"\"\n    if frames is None and fps is None:\n        return times_to_ms(h, m, s, ms)\n    elif frames is not None and fps is not None:\n        return frames_to_ms(frames, fps)\n    else:\n        raise ValueError(\"Both fps and frames must be specified\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef timestamp_to_ms(groups):\n    h, m, s, frac = map(int, groups)\n    ms = frac * 10**(3 - len(groups[-1]))\n    ms += s * 1000\n    ms += m * 60000\n    ms += h * 3600000\n    return ms", "response": "Convert a list of groups from pysubs2. time. TIMESTAMP match to milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef times_to_ms(h=0, m=0, s=0, ms=0):\n    ms += s * 1000\n    ms += m * 60000\n    ms += h * 3600000\n    return int(round(ms))", "response": "Convert seconds in hours minutes seconds to milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting frame - based duration to milliseconds.", "response": "def frames_to_ms(frames, fps):\n    \"\"\"\n    Convert frame-based duration to milliseconds.\n    \n    Arguments:\n        frames: Number of frames (should be int).\n        fps: Framerate (must be a positive number, eg. 23.976).\n    \n    Returns:\n        Number of milliseconds (rounded to int).\n        \n    Raises:\n        ValueError: fps was negative or zero.\n    \n    \"\"\"\n    if fps <= 0:\n        raise ValueError(\"Framerate must be positive number (%f).\" % fps)\n\n    return int(round(frames * (1000 / fps)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting milliseconds to number of frames.", "response": "def ms_to_frames(ms, fps):\n    \"\"\"\n    Convert milliseconds to number of frames.\n    \n    Arguments:\n        ms: Number of milliseconds (may be int, float or other numeric class).\n        fps: Framerate (must be a positive number, eg. 23.976).\n    \n    Returns:\n        Number of frames (int).\n        \n    Raises:\n        ValueError: fps was negative or zero.\n    \n    \"\"\"\n    if fps <= 0:\n        raise ValueError(\"Framerate must be positive number (%f).\" % fps)\n\n    return int(round((ms / 1000) * fps))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ms_to_times(ms):\n    ms = int(round(ms))\n    h, ms = divmod(ms, 3600000)\n    m, ms = divmod(ms, 60000)\n    s, ms = divmod(ms, 1000)\n    return Times(h, m, s, ms)", "response": "Convert milliseconds to Times."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts milliseconds to a timestamp in the format HMMSS. cc.", "response": "def ms_to_timestamp(ms):\n    \"\"\"Convert ms to 'H:MM:SS.cc'\"\"\"\n    # XXX throw on overflow/underflow?\n    if ms < 0: ms = 0\n    if ms > MAX_REPRESENTABLE_TIME: ms = MAX_REPRESENTABLE_TIME\n    h, m, s, ms = ms_to_times(ms)\n    return \"%01d:%02d:%02d.%02d\" % (h, m, s, ms//10)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the text into fragments with computed SSAStyles.", "response": "def parse_tags(text, style=SSAStyle.DEFAULT_STYLE, styles={}):\n    \"\"\"\n    Split text into fragments with computed SSAStyles.\n    \n    Returns list of tuples (fragment, style), where fragment is a part of text\n    between two brace-delimited override sequences, and style is the computed\n    styling of the fragment, ie. the original style modified by all override\n    sequences before the fragment.\n    \n    Newline and non-breakable space overrides are left as-is.\n    \n    Supported override tags:\n    \n    - i, b, u, s\n    - r (with or without style name)\n    \n    \"\"\"\n    \n    fragments = SSAEvent.OVERRIDE_SEQUENCE.split(text)\n    if len(fragments) == 1:\n        return [(text, style)]\n    \n    def apply_overrides(all_overrides):\n        s = style.copy()\n        for tag in re.findall(r\"\\\\[ibus][10]|\\\\r[a-zA-Z_0-9 ]*\", all_overrides):\n            if tag == r\"\\r\":\n                s = style.copy() # reset to original line style\n            elif tag.startswith(r\"\\r\"):\n                name = tag[2:]\n                if name in styles:\n                    s = styles[name].copy() # reset to named style\n            else:\n                if \"i\" in tag: s.italic = \"1\" in tag\n                elif \"b\" in tag: s.bold = \"1\" in tag\n                elif \"u\" in tag: s.underline = \"1\" in tag\n                elif \"s\" in tag: s.strikeout = \"1\" in tag\n        return s\n    \n    overrides = SSAEvent.OVERRIDE_SEQUENCE.findall(text)\n    overrides_prefix_sum = [\"\".join(overrides[:i]) for i in range(len(overrides) + 1)]\n    computed_styles = map(apply_overrides, overrides_prefix_sum)\n    return list(zip(fragments, computed_styles))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plaintext(self):\n        text = self.text\n        text = self.OVERRIDE_SEQUENCE.sub(\"\", text)\n        text = text.replace(r\"\\h\", \" \")\n        text = text.replace(r\"\\n\", \"\\n\")\n        text = text.replace(r\"\\N\", \"\\n\")\n        return text", "response": "Return the text of the current SSA event as a multi - line string with no tags."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shift(self, h=0, m=0, s=0, ms=0, frames=None, fps=None):\n        delta = make_time(h=h, m=m, s=s, ms=ms, frames=frames, fps=fps)\n        self.start += delta\n        self.end += delta", "response": "Shift start and end times."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfield - based equality for SSAEvents.", "response": "def equals(self, other):\n        \"\"\"Field-based equality for SSAEvents.\"\"\"\n        if isinstance(other, SSAEvent):\n            return self.as_dict() == other.as_dict()\n        else:\n            raise TypeError(\"Cannot compare to non-SSAEvent object\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a new SSAFile from a file.", "response": "def load(cls, path, encoding=\"utf-8\", format_=None, fps=None, **kwargs):\n        \"\"\"\n        Load subtitle file from given path.\n\n        Arguments:\n            path (str): Path to subtitle file.\n            encoding (str): Character encoding of input file.\n                Defaults to UTF-8, you may need to change this.\n            format_ (str): Optional, forces use of specific parser\n                (eg. `\"srt\"`, `\"ass\"`). Otherwise, format is detected\n                automatically from file contents. This argument should\n                be rarely needed.\n            fps (float): Framerate for frame-based formats (MicroDVD),\n                for other formats this argument is ignored. Framerate might\n                be detected from the file, in which case you don't need\n                to specify it here (when given, this argument overrides\n                autodetection).\n            kwargs: Extra options for the parser.\n\n        Returns:\n            SSAFile\n\n        Raises:\n            IOError\n            UnicodeDecodeError\n            pysubs2.exceptions.UnknownFPSError\n            pysubs2.exceptions.UnknownFormatIdentifierError\n            pysubs2.exceptions.FormatAutodetectionError\n\n        Note:\n            pysubs2 may autodetect subtitle format and/or framerate. These\n            values are set as :attr:`SSAFile.format` and :attr:`SSAFile.fps`\n            attributes.\n\n        Example:\n            >>> subs1 = pysubs2.load(\"subrip-subtitles.srt\")\n            >>> subs2 = pysubs2.load(\"microdvd-subtitles.sub\", fps=23.976)\n\n        \"\"\"\n        with open(path, encoding=encoding) as fp:\n            return cls.from_file(fp, format_, fps=fps, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a new object from a string.", "response": "def from_string(cls, string, format_=None, fps=None, **kwargs):\n        \"\"\"\n        Load subtitle file from string.\n\n        See :meth:`SSAFile.load()` for full description.\n\n        Arguments:\n            string (str): Subtitle file in a string. Note that the string\n                must be Unicode (in Python 2).\n\n        Returns:\n            SSAFile\n\n        Example:\n            >>> text = '''\n            ... 1\n            ... 00:00:00,000 --> 00:00:05,000\n            ... An example SubRip file.\n            ... '''\n            >>> subs = SSAFile.from_string(text)\n\n        \"\"\"\n        fp = io.StringIO(string)\n        return cls.from_file(fp, format_, fps=fps, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_file(cls, fp, format_=None, fps=None, **kwargs):\n        if format_ is None:\n            # Autodetect subtitle format, then read again using correct parser.\n            # The file might be a pipe and we need to read it twice,\n            # so just buffer everything.\n            text = fp.read()\n            fragment = text[:10000]\n            format_ = autodetect_format(fragment)\n            fp = io.StringIO(text)\n\n        impl = get_format_class(format_)\n        subs = cls() # an empty subtitle file\n        subs.format = format_\n        subs.fps = fps\n        impl.from_file(subs, fp, format_, fps=fps, **kwargs)\n        return subs", "response": "Read a new SSAFile from a file object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, path, encoding=\"utf-8\", format_=None, fps=None, **kwargs):\n        if format_ is None:\n            ext = os.path.splitext(path)[1].lower()\n            format_ = get_format_identifier(ext)\n\n        with open(path, \"w\", encoding=encoding) as fp:\n            self.to_file(fp, format_, fps=fps, **kwargs)", "response": "Save the current object to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the current object s name as a string.", "response": "def to_string(self, format_, fps=None, **kwargs):\n        \"\"\"\n        Get subtitle file as a string.\n\n        See :meth:`SSAFile.save()` for full description.\n\n        Returns:\n            str\n\n        \"\"\"\n        fp = io.StringIO()\n        self.to_file(fp, format_, fps=fps, **kwargs)\n        return fp.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the current entry to a file object.", "response": "def to_file(self, fp, format_, fps=None, **kwargs):\n        \"\"\"\n        Write subtitle file to file object.\n\n        See :meth:`SSAFile.save()` for full description.\n\n        Note:\n            This is a low-level method. Usually, one of :meth:`SSAFile.save()`\n            or :meth:`SSAFile.to_string()` is preferable.\n\n        Arguments:\n            fp (file object): A file object, ie. :class:`io.TextIOBase` instance.\n                Note that the file must be opened in text mode (as opposed to binary).\n\n        \"\"\"\n        impl = get_format_class(format_)\n        impl.to_file(self, fp, format_, fps=fps, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transform_framerate(self, in_fps, out_fps):\n        if in_fps <= 0 or out_fps <= 0:\n            raise ValueError(\"Framerates must be positive, cannot transform %f -> %f\" % (in_fps, out_fps))\n\n        ratio = in_fps / out_fps\n        for line in self:\n            line.start = int(round(line.start * ratio))\n            line.end = int(round(line.end * ratio))", "response": "Transforms all timestamps in the log file into a new framerate."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrenaming a style in the log.", "response": "def rename_style(self, old_name, new_name):\n        \"\"\"\n        Rename a style, including references to it.\n\n        Arguments:\n            old_name (str): Style to be renamed.\n            new_name (str): New name for the style (must be unused).\n\n        Raises:\n            KeyError: No style named old_name.\n            ValueError: new_name is not a legal name (cannot use commas)\n                or new_name is taken.\n\n        \"\"\"\n        if old_name not in self.styles:\n            raise KeyError(\"Style %r not found\" % old_name)\n        if new_name in self.styles:\n            raise ValueError(\"There is already a style called %r\" % new_name)\n        if not is_valid_field_content(new_name):\n            raise ValueError(\"%r is not a valid name\" % new_name)\n\n        self.styles[new_name] = self.styles[old_name]\n        del self.styles[old_name]\n\n        for line in self:\n            # XXX also handle \\r override tag\n            if line.style == old_name:\n                line.style = new_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging in styles from other SSAFile.", "response": "def import_styles(self, subs, overwrite=True):\n        \"\"\"\n        Merge in styles from other SSAFile.\n\n        Arguments:\n            subs (SSAFile): Subtitle file imported from.\n            overwrite (bool): On name conflict, use style from the other file\n                (default: True).\n\n        \"\"\"\n        if not isinstance(subs, SSAFile):\n            raise TypeError(\"Must supply an SSAFile.\")\n\n        for name, style in subs.styles.items():\n            if name not in self.styles or overwrite:\n                self.styles[name] = style"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef equals(self, other):\n\n        if isinstance(other, SSAFile):\n            for key in set(chain(self.info.keys(), other.info.keys())) - {\"ScriptType\"}:\n                sv, ov = self.info.get(key), other.info.get(key)\n                if sv is None:\n                    logging.debug(\"%r missing in self.info\", key)\n                    return False\n                elif ov is None:\n                    logging.debug(\"%r missing in other.info\", key)\n                    return False\n                elif sv != ov:\n                    logging.debug(\"info %r differs (self=%r, other=%r)\", key, sv, ov)\n                    return False\n\n            for key in set(chain(self.styles.keys(), other.styles.keys())):\n                sv, ov = self.styles.get(key), other.styles.get(key)\n                if sv is None:\n                    logging.debug(\"%r missing in self.styles\", key)\n                    return False\n                elif ov is None:\n                    logging.debug(\"%r missing in other.styles\", key)\n                    return False\n                elif sv != ov:\n                    for k in sv.FIELDS:\n                        if getattr(sv, k) != getattr(ov, k): logging.debug(\"difference in field %r\", k)\n                    logging.debug(\"style %r differs (self=%r, other=%r)\", key, sv.as_dict(), ov.as_dict())\n                    return False\n\n            if len(self) != len(other):\n                logging.debug(\"different # of subtitles (self=%d, other=%d)\", len(self), len(other))\n                return False\n\n            for i, (se, oe) in enumerate(zip(self.events, other.events)):\n                if not se.equals(oe):\n                    for k in se.FIELDS:\n                        if getattr(se, k) != getattr(oe, k): logging.debug(\"difference in field %r\", k)\n                    logging.debug(\"event %d differs (self=%r, other=%r)\", i, se.as_dict(), oe.as_dict())\n                    return False\n\n            return True\n        else:\n            raise TypeError(\"Cannot compare to non-SSAFile object\")", "response": "Returns True if self and other are equal."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_file_extension(format_):\n    if format_ not in FORMAT_IDENTIFIER_TO_FORMAT_CLASS:\n        raise UnknownFormatIdentifierError(format_)\n\n    for ext, f in FILE_EXTENSION_TO_FORMAT_IDENTIFIER.items():\n        if f == format_:\n            return ext\n\n    raise RuntimeError(\"No file extension for format %r\" % format_)", "response": "Get the file extension for a given format identifier"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef autodetect_format(content):\n    formats = set()\n    for impl in FORMAT_IDENTIFIER_TO_FORMAT_CLASS.values():\n        guess = impl.guess_format(content)\n        if guess is not None:\n            formats.add(guess)\n\n    if len(formats) == 1:\n        return formats.pop()\n    elif not formats:\n        raise FormatAutodetectionError(\"No suitable formats\")\n    else:\n        raise FormatAutodetectionError(\"Multiple suitable formats (%r)\" % formats)", "response": "Return format identifier for given fragment or raise FormatAutodetectionError."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmodifies the main app to make development easier.", "response": "def modify_main_app(app, config: Config):\n    \"\"\"\n    Modify the app we're serving to make development easier, eg.\n    * modify responses to add the livereload snippet\n    * set ``static_root_url`` on the app\n    * setup the debug toolbar\n    \"\"\"\n    app._debug = True\n    dft_logger.debug('livereload enabled: %s', '\u2713' if config.livereload else '\u2716')\n\n    def get_host(request):\n        if config.infer_host:\n            return request.headers.get('host', 'localhost').split(':', 1)[0]\n        else:\n            return config.host\n\n    if config.livereload:\n        async def on_prepare(request, response):\n            if (not request.path.startswith('/_debugtoolbar') and\n                    'text/html' in response.content_type and\n                    getattr(response, 'body', False)):\n                lr_snippet = LIVE_RELOAD_HOST_SNIPPET.format(get_host(request), config.aux_port)\n                dft_logger.debug('appending live reload snippet \"%s\" to body', lr_snippet)\n                response.body += lr_snippet.encode()\n        app.on_response_prepare.append(on_prepare)\n\n    static_path = config.static_url.strip('/')\n    if config.infer_host and config.static_path is not None:\n        # we set the app key even in middleware to make the switch to production easier and for backwards compat.\n        @web.middleware\n        async def static_middleware(request, handler):\n            static_url = 'http://{}:{}/{}'.format(get_host(request), config.aux_port, static_path)\n            dft_logger.debug('settings app static_root_url to \"%s\"', static_url)\n            request.app['static_root_url'].change(static_url)\n            return await handler(request)\n\n        app.middlewares.insert(0, static_middleware)\n\n    if config.static_path is not None:\n        static_url = 'http://{}:{}/{}'.format(config.host, config.aux_port, static_path)\n        dft_logger.debug('settings app static_root_url to \"%s\"', static_url)\n        app['static_root_url'] = MutableValue(static_url)\n\n    if config.debug_toolbar and aiohttp_debugtoolbar:\n        aiohttp_debugtoolbar.setup(app, intercept_redirects=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def src_reload(app, path: str = None):\n    cli_count = len(app[WS])\n    if cli_count == 0:\n        return 0\n\n    is_html = None\n    if path:\n        path = str(Path(app['static_url']) / Path(path).relative_to(app['static_path']))\n        is_html = mimetypes.guess_type(path)[0] == 'text/html'\n\n    reloads = 0\n    aux_logger.debug('prompting source reload for %d clients', cli_count)\n    for ws, url in app[WS]:\n        if path and is_html and path not in {url, url + '.html', url.rstrip('/') + '/index.html'}:\n            aux_logger.debug('skipping reload for client at %s', url)\n            continue\n        aux_logger.debug('reload client at %s', url)\n        data = {\n            'command': 'reload',\n            'path': path or url,\n            'liveCSS': True,\n            'liveImg': True,\n        }\n        try:\n            await ws.send_str(json.dumps(data))\n        except RuntimeError as e:\n            # eg. \"RuntimeError: websocket connection is closing\"\n            aux_logger.error('Error broadcasting change to %s, RuntimeError: %s', path or url, e)\n        else:\n            reloads += 1\n\n    if reloads:\n        s = '' if reloads == 1 else 's'\n        aux_logger.info('prompted reload of %s on %d client%s', path or 'page', reloads, s)\n    return reloads", "response": "prompt each connected browser to reload the source of a single file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmodify the request object with the current URL.", "response": "def modify_request(self, request):\n        \"\"\"\n        Apply common path conventions eg. / > /index.html, /foobar > /foobar.html\n        \"\"\"\n        filename = URL.build(path=request.match_info['filename'], encoded=True).path\n        raw_path = self._directory.joinpath(filename)\n        try:\n            filepath = raw_path.resolve()\n            if not filepath.exists():\n                # simulate strict=True for python 3.6 which is not permitted with 3.5\n                raise FileNotFoundError()\n        except FileNotFoundError:\n            try:\n                html_file = raw_path.with_name(raw_path.name + '.html').resolve().relative_to(self._directory)\n            except (FileNotFoundError, ValueError):\n                pass\n            else:\n                request.match_info['filename'] = str(html_file)\n        else:\n            if filepath.is_dir():\n                index_file = filepath / 'index.html'\n                if index_file.exists():\n                    try:\n                        request.match_info['filename'] = str(index_file.relative_to(self._directory))\n                    except ValueError:\n                        # path is not not relative to self._directory\n                        pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubstituting environment variables into settings.", "response": "def substitute_environ(self):\n        \"\"\"\n        Substitute environment variables into settings.\n        \"\"\"\n        for attr_name in dir(self):\n            if attr_name.startswith('_') or attr_name.upper() != attr_name:\n                continue\n\n            orig_value = getattr(self, attr_name)\n            is_required = isinstance(orig_value, Required)\n            orig_type = orig_value.v_type if is_required else type(orig_value)\n            env_var_name = self._ENV_PREFIX + attr_name\n            env_var = os.getenv(env_var_name, None)\n            if env_var is not None:\n                if issubclass(orig_type, bool):\n                    env_var = env_var.upper() in ('1', 'TRUE')\n                elif issubclass(orig_type, int):\n                    env_var = int(env_var)\n                elif issubclass(orig_type, Path):\n                    env_var = Path(env_var)\n                elif issubclass(orig_type, bytes):\n                    env_var = env_var.encode()\n                # could do floats here and lists etc via json\n                setattr(self, attr_name, env_var)\n            elif is_required and attr_name not in self._custom_settings:\n                raise RuntimeError('The required environment variable \"{0}\" is currently not set, '\n                                   'you\\'ll need to run `source activate.settings.sh` '\n                                   'or you can set that single environment variable with '\n                                   '`export {0}=\"<value>\"`'.format(env_var_name))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a fresh database and run migrations.", "response": "def prepare_database(delete_existing: bool) -> bool:\n    \"\"\"\n    (Re)create a fresh database and run migrations.\n\n    :param delete_existing: whether or not to drop an existing database if it exists\n    :return: whether or not a database has been (re)created\n    \"\"\"\n    settings = Settings()\n\n    conn = psycopg2.connect(\n        password=settings.DB_PASSWORD,\n        host=settings.DB_HOST,\n        port=settings.DB_PORT,\n        user=settings.DB_USER,\n    )\n    conn.autocommit = True\n    cur = conn.cursor()\n    db_name = settings.DB_NAME\n    cur.execute('SELECT EXISTS (SELECT datname FROM pg_catalog.pg_database WHERE datname=%s)', (db_name,))\n    already_exists = bool(cur.fetchone()[0])\n    if already_exists:\n        if not delete_existing:\n            print('database \"{}\" already exists, skipping'.format(db_name))\n            return False\n        else:\n            print('dropping database \"{}\" as it already exists...'.format(db_name))\n            cur.execute('DROP DATABASE {}'.format(db_name))\n    else:\n        print('database \"{}\" does not yet exist'.format(db_name))\n\n    print('creating database \"{}\"...'.format(db_name))\n    cur.execute('CREATE DATABASE {}'.format(db_name))\n    cur.close()\n    conn.close()\n\n    # {% if database.is_pg_sqlalchemy %}\n    engine = create_engine(pg_dsn(settings))\n    print('creating tables from model definition...')\n    Base.metadata.create_all(engine)\n    engine.dispose()\n    # {% else %}\n    # TODO\n    # {% endif %}\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def message_data(request):\n    messages = []\n    # {% if database.is_none %}\n    if request.app['settings'].MESSAGE_FILE.exists():\n        # read the message file, process it and populate the \"messages\" list\n        with request.app['settings'].MESSAGE_FILE.open() as msg_file:\n            for line in msg_file:\n                if not line:\n                    # ignore blank lines eg. end of file\n                    continue\n                # split the line into it constituent parts, see process_form above\n                username, ts, message = line.split('|', 2)\n                # parse the datetime string and render it in a more readable format.\n                ts = '{:%Y-%m-%d %H:%M:%S}'.format(datetime.strptime(ts, '%Y-%m-%dT%H:%M:%S.%f'))\n                messages.append({'username': username, 'timestamp': ts, 'message': message})\n        messages.reverse()\n    # {% elif database.is_pg_sqlalchemy %}\n\n    async with request.app['pg_engine'].acquire() as conn:\n        async for row in conn.execute(sa_messages.select().order_by(sa_messages.c.timestamp.desc())):\n            ts = '{:%Y-%m-%d %H:%M:%S}'.format(row.timestamp)\n            messages.append({'username': row.username, 'timestamp': ts, 'message': row.message})\n    # {% endif %}\n    return json_response(messages)", "response": "Return the list of messages for the current user in the current language."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a DSN url suitable for sqlalchemy and aiopg.", "response": "def pg_dsn(settings: Settings) -> str:\n    \"\"\"\n    :param settings: settings including connection settings\n    :return: DSN url suitable for sqlalchemy and aiopg.\n    \"\"\"\n    return str(URL(\n        database=settings.DB_NAME,\n        password=settings.DB_PASSWORD,\n        host=settings.DB_HOST,\n        port=settings.DB_PORT,\n        username=settings.DB_USER,\n        drivername='postgres',\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serve(path, livereload, port, verbose):\n    setup_logging(verbose)\n    run_app(*serve_static(static_path=path, livereload=livereload, port=port))", "response": "Serve static files from a directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef runserver(**config):\n    active_config = {k: v for k, v in config.items() if v is not None}\n    setup_logging(config['verbose'])\n    try:\n        run_app(*_runserver(**active_config))\n    except AiohttpDevException as e:\n        if config['verbose']:\n            tb = click.style(traceback.format_exc().strip('\\n'), fg='white', dim=True)\n            main_logger.warning('AiohttpDevException traceback:\\n%s', tb)\n        main_logger.error('Error: %s', e)\n        sys.exit(2)", "response": "Run a development server for an aiohttp apps."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(*, path, name, verbose, **kwargs):\n    setup_logging(verbose)\n    try:\n        check_dir_clean(Path(path))\n        if name is None:\n            name = Path(path).name\n\n        for kwarg_name, choice_enum in DECISIONS:\n            docs = dedent(choice_enum.__doc__).split('\\n')\n            title, *help_text = filter(bool, docs)\n            click.secho('\\n' + title, fg='green')\n            if kwargs[kwarg_name] is None:\n                click.secho('\\n'.join(help_text), dim=True)\n                choices = _display_enum_choices(choice_enum)\n                kwargs[kwarg_name] = click.prompt(\n                    'choose which {} to use {}'.format(kwarg_name, choices),\n                    type=EnumChoice(choice_enum),\n                    show_default=False,\n                    default=enum_default(choice_enum),\n                )\n            click.echo('using: {}'.format(click.style(kwargs[kwarg_name], bold=True)))\n            continue\n\n        StartProject(path=path, name=name, **kwargs)\n    except AiohttpDevException as e:\n        main_logger.error('Error: %s', e)\n        sys.exit(2)", "response": "Start a new Aiohttp app."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimport attribute and class from a python module. Raise AdevConfigError if the import failed.", "response": "def import_app_factory(self):\n        \"\"\"\n        Import attribute/class from from a python module. Raise AdevConfigError if the import failed.\n\n        :return: (attribute, Path object for directory of file)\n        \"\"\"\n        rel_py_file = self.py_file.relative_to(self.python_path)\n        module_path = '.'.join(rel_py_file.with_suffix('').parts)\n\n        sys.path.append(str(self.python_path))\n        try:\n            module = import_module(module_path)\n        except ImportError as e:\n            raise AdevConfigError('error importing \"{}\" '\n                                  'from \"{}\": {}'.format(module_path, self.python_path, e)) from e\n\n        logger.debug('successfully loaded \"%s\" from \"%s\"', module_path, self.python_path)\n\n        if self.app_factory_name is None:\n            try:\n                self.app_factory_name = next(an for an in APP_FACTORY_NAMES if hasattr(module, an))\n            except StopIteration as e:\n                raise AdevConfigError('No name supplied and no default app factory '\n                                      'found in {s.py_file.name}'.format(s=self)) from e\n            else:\n                logger.debug('found default attribute \"%s\" in module \"%s\"',\n                             self.app_factory_name, module)\n\n        try:\n            attr = getattr(module, self.app_factory_name)\n        except AttributeError as e:\n            raise AdevConfigError('Module \"{s.py_file.name}\" '\n                                  'does not define a \"{s.app_factory_name}\" attribute/class'.format(s=self)) from e\n\n        self.watch_path = self.watch_path or Path(module.__file__).parent\n        return attr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting the development server.", "response": "def runserver(**config_kwargs):\n    \"\"\"\n    Prepare app ready to run development server.\n\n    :param config_kwargs: see config.Config for more details\n    :return: tuple (auxiliary app, auxiliary app port, event loop)\n    \"\"\"\n    # force a full reload in sub processes so they load an updated version of code, this must be called only once\n    set_start_method('spawn')\n\n    config = Config(**config_kwargs)\n    config.import_app_factory()\n    loop = asyncio.get_event_loop()\n\n    loop.run_until_complete(check_port_open(config.main_port, loop))\n\n    aux_app = create_auxiliary_app(\n        static_path=config.static_path_str,\n        static_url=config.static_url,\n        livereload=config.livereload,\n    )\n\n    main_manager = AppTask(config, loop)\n    aux_app.on_startup.append(main_manager.start)\n    aux_app.on_shutdown.append(main_manager.close)\n\n    if config.static_path:\n        static_manager = LiveReloadTask(config.static_path, loop)\n        logger.debug('starting livereload to watch %s', config.static_path_str)\n        aux_app.on_startup.append(static_manager.start)\n        aux_app.on_shutdown.append(static_manager.close)\n\n    url = 'http://{0.host}:{0.aux_port}'.format(config)\n    logger.info('Starting aux server at %s \u25c6', url)\n\n    if config.static_path:\n        rel_path = config.static_path.relative_to(os.getcwd())\n        logger.info('serving static files from ./%s/ at %s%s', rel_path, url, config.static_url)\n\n    return aux_app, config.aux_port, loop, AuxAccessLogger"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a synchronous request.", "response": "def request(endpoint, verb='GET', session_options=None, **options):\n    \"\"\"Performs a synchronous request.\n\n    Uses a dedicated event loop and aiohttp.ClientSession object.\n\n    Options:\n\n    - endpoint: the endpoint to call\n    - verb: the HTTP verb to use (defaults: GET)\n    - session_options: a dict containing options to initialize the session\n      (defaults: None)\n    - options: extra options for the request (defaults: None)\n\n    Returns a dict object with the following keys:\n\n    - content: the content of the response\n    - status: the status\n    - headers: a dict with all the response headers\n    \"\"\"\n    req = functools.partial(_request, endpoint, verb, session_options,\n                            **options)\n    return _run_in_fresh_loop(req)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef json_request(endpoint, verb='GET', session_options=None, **options):\n    req = functools.partial(_request, endpoint, verb, session_options,\n                            json=True, **options)\n    return _run_in_fresh_loop(req)", "response": "Like molotov. request but extracts json from the response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a global variable given its name.", "response": "def get_var(name, factory=None):\n    \"\"\"Gets a global variable given its name.\n\n    If factory is not None and the variable is not set, factory\n    is a callable that will set the variable.\n\n    If not set, returns None.\n    \"\"\"\n    if name not in _VARS and factory is not None:\n        _VARS[name] = factory()\n    return _VARS.get(name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsingles scenario call. When it returns 1, it works. -1 the script failed, 0 the test is stopping or needs to stop.", "response": "async def step(self, step_id, session, scenario=None):\n        \"\"\" single scenario call.\n\n        When it returns 1, it works. -1 the script failed,\n        0 the test is stopping or needs to stop.\n        \"\"\"\n        if scenario is None:\n            scenario = pick_scenario(self.wid, step_id)\n        try:\n            await self.send_event('scenario_start', scenario=scenario)\n\n            await scenario['func'](session, *scenario['args'],\n                                   **scenario['kw'])\n\n            await self.send_event('scenario_success', scenario=scenario)\n\n            if scenario['delay'] > 0.:\n                await cancellable_sleep(scenario['delay'])\n            return 1\n        except Exception as exc:\n            await self.send_event('scenario_failure',\n                                  scenario=scenario,\n                                  exception=exc)\n            if self.args.verbose > 0:\n                self.console.print_error(exc)\n                await self.console.flush()\n\n        return -1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_directory(directory, show_warnings=True):\n    errors = []\n\n    def onerror(function, path, excinfo):\n        if show_warnings:\n            print 'Cannot delete %s: %s' % (os.path.relpath(directory), excinfo[1])\n        errors.append((function, path, excinfo))\n\n    if os.path.exists(directory):\n        if not os.path.isdir(directory):\n            raise NotADirectoryError(directory)\n        shutil.rmtree(directory, onerror=onerror)\n\n    return errors", "response": "Deletes a directory and its contents. Returns a list of errors in form ( function path excinfo"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy_files(source_files, target_directory, source_directory=None):\n    try:\n        os.makedirs(target_directory)\n    except:     # TODO: specific exception?\n        pass\n    for f in source_files:\n        source = os.path.join(source_directory, f) if source_directory else f\n        target = os.path.join(target_directory, f)\n        shutil.copy2(source, target)", "response": "Copies a list of files to the specified directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget user input and returns True for yes and False for no.", "response": "def yes_or_no(message):\n    \"\"\"Gets user input and returns True for yes and False for no.\"\"\"\n    while True:\n        print message, '(yes/no)',\n        line = raw_input()\n        if line is None:\n            return None\n        line = line.lower()\n        if line == 'y' or line == 'ye' or line == 'yes':\n            return True\n        if line == 'n' or line == 'no':\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_plugins(directory=None):\n    repo = require_repo(directory)\n    plugins = get_value(repo, 'plugins')\n    if not plugins or not isinstance(plugins, dict):\n        return None\n    return plugins.keys()", "response": "Gets a list of the installed themes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the specified plugin. This returns False if it was already added.", "response": "def add_plugin(plugin, directory=None):\n    \"\"\"Adds the specified plugin. This returns False if it was already added.\"\"\"\n    repo = require_repo(directory)\n    plugins = get_value(repo, 'plugins', expect_type=dict)\n    if plugin in plugins:\n        return False\n\n    plugins[plugin] = {}\n    set_value(repo, 'plugins', plugins)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_plugin_settings(plugin, directory=None):\n    repo = require_repo(directory)\n    plugins = get_value(repo, 'plugins')\n    return plugins.get(plugin) if isinstance(plugins, dict) else None", "response": "Gets the settings for the specified plugin."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning a local server to preview the working directory of a repository.", "response": "def preview(directory=None, host=None, port=None, watch=True):\n    \"\"\"Runs a local server to preview the working directory of a repository.\"\"\"\n    directory = directory or '.'\n    host = host or '127.0.0.1'\n    port = port or 5000\n\n    # TODO: admin interface\n\n    # TODO: use cache_only to keep from modifying output directly\n    out_directory = build(directory)\n\n    # Serve generated site\n    os.chdir(out_directory)\n    Handler = SimpleHTTPServer.SimpleHTTPRequestHandler\n    httpd = SocketServer.TCPServer((host, port), Handler)\n    print ' * Serving on http://%s:%s/' % (host, port)\n    httpd.serve_forever()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef require_repo(directory=None):\n    if directory and not os.path.isdir(directory):\n        raise ValueError('Directory not found: ' + repr(directory))\n    repo = repo_path(directory)\n    if not os.path.isdir(repo):\n        raise RepositoryNotFoundError(directory)\n    return repo", "response": "Checks for a presentation repository and raises an exception if not found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init(directory=None):\n    repo = repo_path(directory)\n    if os.path.isdir(repo):\n        raise RepositoryAlreadyExistsError(directory, repo)\n\n    # Initialize repository with default template\n    shutil.copytree(default_template_path, repo)\n\n    message = '\"Default presentation content.\"'\n    subprocess.call(['git', 'init', '-q', repo])\n    subprocess.call(['git', 'add', '.'], cwd=repo)\n    subprocess.call(['git', 'commit', '-q', '-m', message], cwd=repo)\n\n    return repo", "response": "Initializes a Gitpress presentation repository at the specified directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iterate_presentation_files(path=None, excludes=None, includes=None):\n\n    # Defaults\n    if includes is None:\n        includes = []\n    if excludes is None:\n        excludes = []\n\n    # Transform glob patterns to regular expressions\n    includes_pattern = r'|'.join([fnmatch.translate(x) for x in includes]) or r'$.'\n    excludes_pattern = r'|'.join([fnmatch.translate(x) for x in excludes]) or r'$.'\n    includes_re = re.compile(includes_pattern)\n    excludes_re = re.compile(excludes_pattern)\n\n    def included(root, name):\n        \"\"\"Returns True if the specified file is a presentation file.\"\"\"\n        full_path = os.path.join(root, name)\n        # Explicitly included files takes priority\n        if includes_re.match(full_path):\n            return True\n        # Ignore special and excluded files\n        return (not specials_re.match(name)\n            and not excludes_re.match(full_path))\n\n    # Get a filtered list of paths to be built\n    for root, dirs, files in os.walk(path):\n        dirs[:] = [d for d in dirs if included(root, d)]\n        files = [f for f in files if included(root, f)]\n        for f in files:\n            yield os.path.relpath(os.path.join(root, f), path)", "response": "Iterate the repository presentation files relative to path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the configuration from the specified file.", "response": "def read_config_file(path):\n    \"\"\"Returns the configuration from the specified file.\"\"\"\n    try:\n        with open(path, 'r') as f:\n            return json.load(f, object_pairs_hook=OrderedDict)\n    except IOError as ex:\n        if ex != errno.ENOENT:\n            raise\n    return {}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_config(repo_directory, config):\n    return write_config_file(os.path.join(repo_directory, config_file), config)", "response": "Writes the specified configuration to the presentation repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_config_file(path, config):\n    contents = json.dumps(config, indent=4, separators=(',', ': ')) + '\\n'\n    try:\n        with open(path, 'w') as f:\n            f.write(contents)\n        return True\n    except IOError as ex:\n        if ex != errno.ENOENT:\n            raise\n    return False", "response": "Writes the specified configuration to the specified file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_value(repo_directory, key, expect_type=None):\n    config = read_config(repo_directory)\n    value = config.get(key)\n    if expect_type and value is not None and not isinstance(value, expect_type):\n        raise ConfigSchemaError('Expected config variable %s to be type %s, got %s'\n            % (repr(key), repr(expect_type), repr(type(value))))\n    return value", "response": "Gets the value of the specified key in the config file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_value(repo_directory, key, value, strict=True):\n    if value is None:\n        raise ValueError('Argument \"value\" must not be None.')\n\n    # Read values and do nothing if not making any changes\n    config = read_config(repo_directory)\n    old = config.get(key)\n    if old == value:\n        return old\n\n    # Check schema\n    if strict and old is not None and not isinstance(old, type(value)):\n        raise ConfigSchemaError('Expected config variable %s to be type %s, got %s'\n            % (repr(key), repr(type(value)), repr(type(old))))\n\n    # Set new value and save results\n    config[key] = value\n    write_config(repo_directory, config)\n    return old", "response": "Sets the value of a particular key in the config file. This has no effect when setting to the same value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds the site from its content and presentation repository.", "response": "def build(content_directory=None, out_directory=None):\n    \"\"\"Builds the site from its content and presentation repository.\"\"\"\n    content_directory = content_directory or '.'\n    out_directory = os.path.abspath(out_directory or default_out_directory)\n    repo = require_repo(content_directory)\n\n    # Prevent user mistakes\n    if out_directory == '.':\n        raise ValueError('Output directory must be different than the source directory: ' + repr(out_directory))\n    if os.path.basename(os.path.relpath(out_directory, content_directory)) == '..':\n        raise ValueError('Output directory must not contain the source directory: ' + repr(out_directory))\n\n    # TODO: read config\n    # TODO: use virtualenv\n    # TODO: init and run plugins\n    # TODO: process with active theme\n\n    # Collect and copy static files\n    files = presentation_files(repo)\n    remove_directory(out_directory)\n    copy_files(files, out_directory, repo)\n\n    return out_directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute(args):\n\n    def info(*message):\n        \"\"\"Displays a message unless -q was specified.\"\"\"\n        if not args['-q']:\n            print ' '.join(map(str, message))\n\n    if args['init']:\n        try:\n            repo = init(args['<directory>'])\n            info('Initialized Gitpress repository in', repo)\n        except RepositoryAlreadyExistsError as ex:\n            info('Gitpress repository already exists in', ex.repo)\n        return 0\n\n    if args['preview']:\n        directory, address = resolve(args['<directory>'], args['<address>'])\n        host, port = split_address(address)\n        if address and not host and not port:\n            error('Invalid address', repr(address))\n        return preview(directory, host=host, port=port)\n\n    if args['build']:\n        require_repo(args['<directory>'])\n        info('Building site', os.path.abspath(args['<directory>'] or '.'))\n        try:\n            out_directory = build(args['<directory>'], args['--out'])\n        except NotADirectoryError as ex:\n            error(ex)\n        info('Site built in', os.path.abspath(out_directory))\n        return 0\n\n    if args['themes']:\n        theme = args['<theme>']\n        if args['use']:\n            try:\n                switched = use_theme(theme)\n            except ConfigSchemaError as ex:\n                error('Could not modify config:', ex)\n                return 1\n            except ThemeNotFoundError as ex:\n                error('Theme %s is not currently installed.' % repr(theme))\n                return 1\n            info('Switched to theme %s' if switched else 'Already using %s' % repr(theme))\n        elif args['install']:\n            # TODO: implement\n            raise NotImplementedError()\n        elif args['uninstall']:\n            # TODO: implement\n            raise NotImplementedError()\n        else:\n            themes = list_themes()\n            if themes:\n                info('Installed themes:')\n                info('  ' + '\\n  '.join(themes))\n            else:\n                info('No themes installed.')\n        return 0\n\n    if args['plugins']:\n        plugin = args['<plugin>']\n        if args['add']:\n            try:\n                added = add_plugin(plugin)\n            except ConfigSchemaError as ex:\n                error('Could not modify config:', ex)\n                return 1\n            info(('Added plugin %s' if added else\n                'Plugin %s has already been added.') % repr(plugin))\n        elif args['remove']:\n            settings = get_plugin_settings(plugin)\n            if not args['-f'] and settings and isinstance(settings, dict):\n                warning = 'Plugin %s contains settings. Remove?' % repr(plugin)\n                if not yes_or_no(warning):\n                    return 0\n            try:\n                removed = remove_plugin(plugin)\n            except ConfigSchemaError as ex:\n                error('Error: Could not modify config:', ex)\n            info(('Removed plugin %s' if removed else\n                'Plugin %s has already been removed.') % repr(plugin))\n        else:\n            plugins = list_plugins()\n            info('Installed plugins:\\n  ' + '\\n  '.join(plugins) if plugins else\n                'No plugins installed.')\n        return 0\n\n    return 1", "response": "Executes the command specified by the specified parsed arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gpp(argv=None):\n    if argv is None:\n        argv = sys.argv[1:]\n    argv.insert(0, 'preview')\n    return main(argv)", "response": "Shortcut function for running the previewing command."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a list of the installed themes.", "response": "def list_themes(directory=None):\n    \"\"\"Gets a list of the installed themes.\"\"\"\n    repo = require_repo(directory)\n    path = os.path.join(repo, themes_dir)\n    return os.listdir(path) if os.path.isdir(path) else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef use_theme(theme, directory=None):\n    repo = require_repo(directory)\n    if theme not in list_themes(directory):\n        raise ThemeNotFoundError(theme)\n\n    old_theme = set_value(repo, 'theme', theme)\n    return old_theme != theme", "response": "Switches to the specified theme. This returns False if the specified theme is already active."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef size(value):\n        if value.value:\n            _assert_is_type('size.value', value.value, int)\n            if value.value < 0:\n                raise ValueError('size cannot be negative')", "response": "ValueRef : number area of the mark in pixels\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef baseline(value):\n        if value.value:\n            _assert_is_type('shape.value', value.value, str_types)\n            if value.value not in PropertySet._valid_baseline:\n                raise ValueError(value.value + ' is not a valid baseline')", "response": "ValueRef : string vertical alignment of mark\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef type(value):\n\n        valid_transforms = frozenset([\n            'array', 'copy', 'cross', 'facet', 'filter',\n            'flatten', 'fold', 'formula', 'slice', 'sort', 'stats',\n            'truncate', 'unique', 'window', 'zip', 'force', 'geo', 'geopath',\n            'link', 'pie', 'stack', 'treemap', 'wordcloud'\n        ])\n\n        if value not in valid_transforms:\n            raise ValueError('Transform type must be'\n                             ' one of {0}'.format(str(valid_transforms)))", "response": "string - property name of the computed transform\n        value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef data_type(data, grouped=False, columns=None, key_on='idx', iter_idx=None):\n    '''Data type check for automatic import'''\n    if iter_idx:\n        return Data.from_mult_iters(idx=iter_idx, **data)\n    if pd:\n        if isinstance(data, (pd.Series, pd.DataFrame)):\n            return Data.from_pandas(data, grouped=grouped, columns=columns,\n                                    key_on=key_on)\n    if isinstance(data, (list, tuple, dict)):\n            return Data.from_iter(data)\n    else:\n        raise ValueError('This data type is not supported by Vincent.')", "response": "Data type check for automatic import"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rebind(self, column=None, brew='GnBu'):\n        self.data['table'] = Data.keypairs(\n            self.raw_data, columns=[self.data_key, column])\n        domain = [Data.serialize(self.raw_data[column].min()),\n                  Data.serialize(self.raw_data[column].quantile(0.95))]\n        scale = Scale(name='color', type='quantize', domain=domain,\n                      range=brews[brew])\n        self.scales['color'] = scale", "response": "Bind a new column to the data map."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndefines the padding around the visualization .", "response": "def padding(value):\n        \"\"\"int or dict : Padding around visualization\n\n        The padding defines the distance between the edge of the\n        visualization canvas to the visualization box. It does not count as\n        part of the visualization width/height. Values cannot be negative.\n\n        If a dict, padding must have all keys ``''top'``, ``'left'``,\n        ``'right'``, and ``'bottom'`` with int values.\n        \"\"\"\n        if isinstance(value, dict):\n            required_keys = ['top', 'left', 'right', 'bottom']\n            for key in required_keys:\n                if key not in value:\n                    error = ('Padding must have keys \"{0}\".'\n                             .format('\", \"'.join(required_keys)))\n                    raise ValueError(error)\n                _assert_is_type('padding: {0}'.format(key), value[key], int)\n                if value[key] < 0:\n                    raise ValueError('Padding cannot be negative.')\n        elif isinstance(value, int):\n            if value < 0:\n                raise ValueError('Padding cannot be negative.')\n        else:\n            if value not in (\"auto\", \"strict\"):\n                raise ValueError('Padding can only be auto or strict.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data(value):\n        for i, entry in enumerate(value):\n            _assert_is_type('data[{0}]'.format(i), entry,  Data)", "response": "list or KeyedList of Data definitions\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist or KeyedList of Scale definitions", "response": "def scales(value):\n        \"\"\"list or KeyedList of ``Scale`` : Scale definitions\n\n        Scales map the data from the domain of the data to some\n        visualization space (such as an x-axis). See the :class:`Scale`\n        class for details.\n        \"\"\"\n        for i, entry in enumerate(value):\n            _assert_is_type('scales[{0}]'.format(i), entry, Scale)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting or KeyedList of Axis definitions", "response": "def axes(value):\n        \"\"\"list or KeyedList of ``Axis`` : Axis definitions\n\n        Axes define the locations of the data being mapped by the scales.\n        See the :class:`Axis` class for details.\n        \"\"\"\n        for i, entry in enumerate(value):\n            _assert_is_type('axes[{0}]'.format(i), entry, Axis)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef marks(value):\n        for i, entry in enumerate(value):\n            _assert_is_type('marks[{0}]'.format(i), entry, Mark)", "response": "list or KeyedList of Mark definitions that are the visual objects that represent the data in the visualization space."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting or KeyedList of Legends", "response": "def legends(value):\n        \"\"\"list or KeyedList of ``Legends`` : Legend definitions\n\n        Legends visualize scales, and take one or more scales as their input.\n        They can be customized via a LegendProperty object.\n        \"\"\"\n        for i, entry in enumerate(value):\n            _assert_is_type('legends[{0}]'.format(i), entry, Legend)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef axis_titles(self, x=None, y=None):\n        keys = self.axes.get_keys()\n\n        if keys:\n            for key in keys:\n                if key == 'x':\n                    self.axes[key].title = x\n                elif key == 'y':\n                    self.axes[key].title = y\n        else:\n            self.axes.extend([Axis(type='x', title=x),\n                              Axis(type='y', title=y)])\n        return self", "response": "Apply axis titles to the figure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_axis_properties(self, axis):\n        if not getattr(axis, 'properties'):\n            axis.properties = AxisProperties()\n            for prop in ['ticks', 'axis', 'major_ticks', 'minor_ticks',\n                         'title', 'labels']:\n                setattr(axis.properties, prop, PropertySet())", "response": "Set AxisProperties and PropertySets for the axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_all_axis_color(self, axis, color):\n        for prop in ['ticks', 'axis', 'major_ticks', 'minor_ticks', 'title',\n                     'labels']:\n            prop_set = getattr(axis.properties, prop)\n            if color and prop in ['title', 'labels']:\n                prop_set.fill = ValueRef(value=color)\n            elif color and prop in ['axis', 'major_ticks', 'minor_ticks',\n                                    'ticks']:\n                prop_set.stroke = ValueRef(value=color)", "response": "Set axis ticks title labels to given color"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef common_axis_properties(self, color=None, title_size=None):\n        if self.axes:\n            for axis in self.axes:\n                self._set_axis_properties(axis)\n                self._set_all_axis_color(axis, color)\n                if title_size:\n                    ref = ValueRef(value=title_size)\n                    axis.properties.title.font_size = ref\n        else:\n            raise ValueError('This Visualization has no axes!')\n        return self", "response": "Set common axis properties such as color\n            Hex color str etc."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef x_axis_properties(self, title_size=None, title_offset=None,\n                          label_angle=None, label_align=None, color=None):\n        \"\"\"Change x-axis title font size and label angle\n\n        Parameters\n        ----------\n        title_size: int, default None\n            Title size, in px\n        title_offset: int, default None\n            Pixel offset from given axis\n        label_angle: int, default None\n            label angle in degrees\n        label_align: str, default None\n            Label alignment\n        color: str, default None\n            Hex color\n        \"\"\"\n        self._axis_properties('x', title_size, title_offset, label_angle,\n                              label_align, color)\n        return self", "response": "Change x - axis title font size and label angle."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchanges y - axis title font size and label angle", "response": "def y_axis_properties(self, title_size=None, title_offset=None,\n                          label_angle=None, label_align=None, color=None):\n        \"\"\"Change y-axis title font size and label angle\n\n        Parameters\n        ----------\n        title_size: int, default None\n            Title size, in px\n        title_offset: int, default None\n            Pixel offset from given axis\n        label_angle: int, default None\n            label angle in degrees\n        label_align: str, default None\n            Label alignment\n        color: str, default None\n            Hex color\n        \"\"\"\n        self._axis_properties('y', title_size, title_offset, label_angle,\n                              label_align, color)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating the contents of the visualization.", "response": "def validate(self, require_all=True, scale='colors'):\n        \"\"\"Validate the visualization contents.\n\n        Parameters\n        ----------\n        require_all : boolean, default True\n            If True (default), then all fields ``data``, ``scales``,\n            ``axes``, and ``marks`` must be defined. The user is allowed to\n            disable this if the intent is to define the elements\n            client-side.\n\n        If the contents of the visualization are not valid Vega, then a\n        :class:`ValidationError` is raised.\n        \"\"\"\n        super(self.__class__, self).validate()\n        required_attribs = ('data', 'scales', 'axes', 'marks')\n        for elem in required_attribs:\n            attr = getattr(self, elem)\n            if attr:\n                # Validate each element of the sets of data, etc\n                for entry in attr:\n                    entry.validate()\n                names = [a.name for a in attr]\n                if len(names) != len(set(names)):\n                    raise ValidationError(elem + ' has duplicate names')\n            elif require_all:\n                raise ValidationError(\n                    elem + ' must be defined for valid visualization')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _repr_html_(self):\n        vis_id = str(uuid4()).replace(\"-\", \"\")\n        html = \"\"\"<div id=\"vis%s\"></div>\n<script>\n   ( function() {\n     var _do_plot = function() {\n       if (typeof vg === 'undefined') {\n         window.addEventListener('vincent_libs_loaded', _do_plot)\n         return;\n       }\n       vg.parse.spec(%s, function(chart) {\n         chart({el: \"#vis%s\"}).update();\n       });\n     };\n     _do_plot();\n   })();\n</script>\n<style>.vega canvas {width: 100%%;}</style>\n        \"\"\" % (vis_id, self.to_json(pretty_print=False), vis_id)\n        return html", "response": "Build the HTML representation for IPython."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays the visualization inline in the IPython notebook.", "response": "def display(self):\n        \"\"\"Display the visualization inline in the IPython notebook.\n\n        This is deprecated, use the following instead::\n\n            from IPython.display import display\n            display(viz)\n        \"\"\"\n        from IPython.core.display import display, HTML\n        display(HTML(self._repr_html_()))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating contents of class", "response": "def validate(self, *args):\n        \"\"\"Validate contents of class\n        \"\"\"\n        super(self.__class__, self).validate(*args)\n        if not self.name:\n            raise ValidationError('name is required for Data')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert an object into a JSON - serializable value.", "response": "def serialize(obj):\n        \"\"\"Convert an object into a JSON-serializable value\n\n        This is used by the ``from_pandas`` and ``from_numpy`` functions to\n        convert data to JSON-serializable types when loading.\n        \"\"\"\n        if isinstance(obj, str_types):\n            return obj\n        elif hasattr(obj, 'timetuple'):\n            return int(time.mktime(obj.timetuple())) * 1000\n        elif hasattr(obj, 'item'):\n            return obj.item()\n        elif hasattr(obj, '__float__'):\n            if isinstance(obj, int):\n                return int(obj)\n            else:\n                return float(obj)\n        elif hasattr(obj, '__int__'):\n            return int(obj)\n        else:\n            raise LoadError('cannot serialize index of type '\n                            + type(obj).__name__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_pandas(cls, data, columns=None, key_on='idx', name=None,\n                    series_key='data', grouped=False, records=False, **kwargs):\n        \"\"\"Load values from a pandas ``Series`` or ``DataFrame`` object\n\n        Parameters\n        ----------\n        data : pandas ``Series`` or ``DataFrame``\n            Pandas object to import data from.\n        columns: list, default None\n            DataFrame columns to convert to Data. Keys default to col names.\n            If columns are given and on_index is False, x-axis data will\n            default to the first column.\n        key_on: string, default 'index'\n            Value to key on for x-axis data. Defaults to index.\n        name : string, default None\n            Applies to the ``name`` attribute of the generated class. If\n            ``None`` (default), then the ``name`` attribute of ``pd_obj`` is\n            used if it exists, or ``'table'`` if it doesn't.\n        series_key : string, default 'data'\n            Applies only to ``Series``. If ``None`` (default), then defaults to\n            data.name. For example, if ``series_key`` is ``'x'``, then the\n            entries of the ``values`` list\n            will be ``{'idx': ..., 'col': 'x', 'val': ...}``.\n        grouped: boolean, default False\n            Pass true for an extra grouping parameter\n        records: boolean, defaule False\n            Requires Pandas 0.12 or greater. Writes the Pandas DataFrame\n            using the df.to_json(orient='records') formatting.\n        **kwargs : dict\n            Additional arguments passed to the :class:`Data` constructor.\n        \"\"\"\n        # Note: There's an experimental JSON encoder floating around in\n        # pandas land that hasn't made it into the main branch. This\n        # function should be revisited if it ever does.\n        if not pd:\n            raise LoadError('pandas could not be imported')\n        if not hasattr(data, 'index'):\n            raise ValueError('Please load a Pandas object.')\n\n        if name:\n            vega_data = cls(name=name, **kwargs)\n        else:\n            vega_data = cls(name='table', **kwargs)\n\n        pd_obj = data.copy()\n        if columns:\n            pd_obj = data[columns]\n        if key_on != 'idx':\n            pd_obj.index = data[key_on]\n        if records:\n            # The worst\n            vega_data.values = json.loads(pd_obj.to_json(orient='records'))\n            return vega_data\n\n        vega_data.values = []\n\n        if isinstance(pd_obj, pd.Series):\n            data_key = data.name or series_key\n            for i, v in pd_obj.iteritems():\n                value = {}\n                value['idx'] = cls.serialize(i)\n                value['col'] = data_key\n                value['val'] = cls.serialize(v)\n                vega_data.values.append(value)\n\n        elif isinstance(pd_obj, pd.DataFrame):\n            # We have to explicitly convert the column names to strings\n            # because the json serializer doesn't allow for integer keys.\n            for i, row in pd_obj.iterrows():\n                for num, (k, v) in enumerate(row.iteritems()):\n                    value = {}\n                    value['idx'] = cls.serialize(i)\n                    value['col'] = cls.serialize(k)\n                    value['val'] = cls.serialize(v)\n                    if grouped:\n                        value['group'] = num\n                    vega_data.values.append(value)\n        else:\n            raise ValueError('cannot load from data type '\n                             + type(pd_obj).__name__)\n        return vega_data", "response": "Load values from a pandas DataFrame or Series object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload values from a numpy array containing a set of key - value pairs.", "response": "def from_numpy(cls, np_obj, name, columns, index=None, index_key=None,\n                   **kwargs):\n        \"\"\"Load values from a numpy array\n\n        Parameters\n        ----------\n        np_obj : numpy.ndarray\n            numpy array to load data from\n        name : string\n            ``name`` field for the data\n        columns : iterable\n            Sequence of column names, from left to right. Must have same\n            length as the number of columns of ``np_obj``.\n        index : iterable, default None\n            Sequence of indices from top to bottom. If ``None`` (default),\n            then the indices are integers starting at 0. Must have same\n            length as the number of rows of ``np_obj``.\n        index_key : string, default None\n            Key to use for the index. If ``None`` (default), ``idx`` is\n            used.\n        **kwargs : dict\n            Additional arguments passed to the :class:`Data` constructor\n\n        Notes\n        -----\n        The individual elements of ``np_obj``, ``columns``, and ``index``\n        must return valid values from :func:`Data.serialize`.\n        \"\"\"\n        if not np:\n            raise LoadError('numpy could not be imported')\n\n        _assert_is_type('numpy object', np_obj, np.ndarray)\n\n        # Integer index if none is provided\n        index = index or range(np_obj.shape[0])\n        # Explicitly map dict-keys to strings for JSON serializer.\n        columns = list(map(str, columns))\n\n        index_key = index_key or cls._default_index_key\n\n        if len(index) != np_obj.shape[0]:\n            raise LoadError(\n                'length of index must be equal to number of rows of array')\n        elif len(columns) != np_obj.shape[1]:\n            raise LoadError(\n                'length of columns must be equal to number of columns of '\n                'array')\n\n        data = cls(name=name, **kwargs)\n        data.values = [\n            dict([(index_key, cls.serialize(idx))] +\n                 [(col, x) for col, x in zip(columns, row)])\n            for idx, row in zip(index, np_obj.tolist())]\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading values from multiple iters into a new Data object.", "response": "def from_mult_iters(cls, name=None, idx=None, **kwargs):\n        \"\"\"Load values from multiple iters\n\n        Parameters\n        ----------\n        name : string, default None\n            Name of the data set. If None (default), the name will be set to\n            ``'table'``.\n        idx: string, default None\n            Iterable to use for the data index\n        **kwargs : dict of iterables\n            The ``values`` field will contain dictionaries with keys for\n            each of the iterables provided. For example,\n\n                d = Data.from_iters(idx='x', x=[0, 1, 5], y=(10, 20, 30))\n\n            would result in ``d`` having a ``values`` field with\n\n                [{'idx': 0, 'col': 'y', 'val': 10},\n                 {'idx': 1, 'col': 'y', 'val': 20}\n\n            If the iterables are not the same length, then ValueError is\n            raised.\n        \"\"\"\n        if not name:\n            name = 'table'\n\n        lengths = [len(v) for v in kwargs.values()]\n\n        if len(set(lengths)) != 1:\n            raise ValueError('Iterables must all be same length')\n\n        if not idx:\n            raise ValueError('Must provide iter name index reference')\n\n        index = kwargs.pop(idx)\n        vega_vals = []\n        for k, v in sorted(kwargs.items()):\n            for idx, val in zip(index, v):\n                value = {}\n                value['idx'] = idx\n                value['col'] = k\n                value['val'] = val\n                vega_vals.append(value)\n\n        return cls(name, values=vega_vals)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _numpy_to_values(data):\n        '''Convert a NumPy array to values attribute'''\n        def to_list_no_index(xvals, yvals):\n            return [{\"x\": x, \"y\": np.asscalar(y)}\n                    for x, y in zip(xvals, yvals)]\n\n        if len(data.shape) == 1 or data.shape[1] == 1:\n            xvals = range(data.shape[0] + 1)\n            values = to_list_no_index(xvals, data)\n        elif len(data.shape) == 2:\n            if data.shape[1] == 2:\n                # NumPy arrays and matrices have different iteration rules.\n                if isinstance(data, np.matrix):\n                    xidx = (0, 0)\n                    yidx = (0, 1)\n                else:\n                    xidx = 0\n                    yidx = 1\n\n                xvals = [np.asscalar(row[xidx]) for row in data]\n                yvals = [np.asscalar(row[yidx]) for row in data]\n                values = [{\"x\": x, \"y\": y} for x, y in zip(xvals, yvals)]\n            else:\n                raise ValueError('arrays with > 2 columns not supported')\n        else:\n            raise ValueError('invalid dimensions for ndarray')\n\n        return values", "response": "Convert a NumPy array to values attribute"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_json(self, validate=False, pretty_print=True, data_path=None):\n        # TODO: support writing to separate file\n        return super(self.__class__, self).to_json(validate=validate,\n                                                   pretty_print=pretty_print)", "response": "Convert data to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize_notebook():\n    try:\n        from IPython.core.display import display, HTML\n    except ImportError:\n        print(\"IPython Notebook could not be loaded.\")\n\n    # Thanks to @jakevdp:\n    # https://github.com/jakevdp/mpld3/blob/master/mpld3/_display.py#L85\n    load_lib = \"\"\"\n                function vct_load_lib(url, callback){\n                      if(\n                typeof d3 !== 'undefined' &&\n                url === '//cdnjs.cloudflare.com/ajax/libs/d3/3.5.3/d3.min.js'){\n                        callback()\n                      }\n                      var s = document.createElement('script');\n                      s.src = url;\n                      s.async = true;\n                      s.onreadystatechange = s.onload = callback;\n                      s.onerror = function(){\n                        console.warn(\"failed to load library \" + url);\n                        };\n                      document.getElementsByTagName(\"head\")[0].appendChild(s);\n                };\n                var vincent_event = new CustomEvent(\n                  \"vincent_libs_loaded\",\n                  {bubbles: true, cancelable: true}\n                );\n                \"\"\"\n    lib_urls = [\n        \"'//cdnjs.cloudflare.com/ajax/libs/d3/3.5.3/d3.min.js'\",\n        (\"'//cdnjs.cloudflare.com/ajax/libs/d3-geo-projection/0.2.9/\"\n         \"d3.geo.projection.min.js'\"),\n        \"'//wrobstory.github.io/d3-cloud/d3.layout.cloud.js'\",\n        \"'//wrobstory.github.io/vega/vega.v1.3.3.js'\"\n    ]\n    get_lib = \"\"\"vct_load_lib(%s, function(){\n                  %s\n                  });\"\"\"\n    load_js = get_lib\n    ipy_trigger = \"window.dispatchEvent(vincent_event);\"\n    for elem in lib_urls[:-1]:\n        load_js = load_js % (elem, get_lib)\n    load_js = load_js % (lib_urls[-1], ipy_trigger)\n    html = \"\"\"\n           <script>\n               %s\n               function load_all_libs(){\n                  console.log('Loading Vincent libs...')\n                  %s\n               };\n               if(typeof define === \"function\" && define.amd){\n                    if (window['d3'] === undefined ||\n                        window['topojson'] === undefined){\n                        require.config(\n                            {paths: {\n    d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.5.3/d3.min',\n    topojson: '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.9/topojson.min'\n                              }\n                            }\n                          );\n                        require([\"d3\"], function(d3){\n                            console.log('Loading Vincent from require.js...')\n                            window.d3 = d3;\n                            require([\"topojson\"], function(topojson){\n                                window.topojson = topojson;\n                                load_all_libs();\n                            });\n                        });\n                    } else {\n                        load_all_libs();\n                    };\n               }else{\n                    console.log('Require.js not found, loading manually...')\n                    load_all_libs();\n               };\n\n           </script>\"\"\" % (load_lib, load_js,)\n    return display(HTML(html))", "response": "Initialize the IPython notebook display elements"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nasserts that a value is a given type.", "response": "def _assert_is_type(name, value, value_type):\n    \"\"\"Assert that a value must be a given type.\"\"\"\n    if not isinstance(value, value_type):\n        if type(value_type) is tuple:\n            types = ', '.join(t.__name__ for t in value_type)\n            raise ValueError('{0} must be one of ({1})'.format(name, types))\n        else:\n            raise ValueError('{0} must be {1}'\n                             .format(name, value_type.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self):\n        for key, val in self.grammar.items():\n            try:\n                setattr(self, key, val)\n            except ValueError as e:\n                raise ValidationError('invalid contents: ' + e.args[0])", "response": "Validate the contents of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_json(self, path=None, html_out=False,\n                html_path='vega_template.html', validate=False,\n                pretty_print=True):\n        \"\"\"Convert object to JSON\n\n        Parameters\n        ----------\n        path: string, default None\n            Path to write JSON out. If there is no path provided, JSON\n            will be returned as a string to the console.\n        html_out: boolean, default False\n            If True, vincent will output an simple HTML scaffold to\n            visualize the vega json output.\n        html_path: string, default 'vega_template.html'\n            Path for the html file (if html_out=True)\n        validate : boolean\n            If True, call the object's `validate` method before\n            serializing. Default is False.\n        pretty_print : boolean\n            If True (default), JSON is printed in more-readable form with\n            indentation and spaces.\n\n        Returns\n        -------\n        string\n            JSON serialization of the class's grammar properties.\n        \"\"\"\n        if validate:\n            self.validate()\n\n        if pretty_print:\n            dumps_args = {'indent': 2, 'separators': (',', ': ')}\n        else:\n            dumps_args = {}\n\n        def encoder(obj):\n            if hasattr(obj, 'grammar'):\n                return obj.grammar\n\n        if html_out:\n            template = Template(\n                str(resource_string('vincent', 'vega_template.html')))\n            with open(html_path, 'w') as f:\n                f.write(template.substitute(path=path))\n\n        if path:\n            with open(path, 'w') as f:\n                json.dump(self.grammar, f, default=encoder, sort_keys=True,\n                          **dumps_args)\n        else:\n            return json.dumps(self.grammar, default=encoder, sort_keys=True,\n                              **dumps_args)", "response": "Convert object to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef useful_mimetype(text):\n    if text is None:\n        return False\n    mimetype = normalize_mimetype(text)\n    return mimetype not in [DEFAULT, PLAIN, None]", "response": "Check to see if the given mime type is a MIME type which is useful in terms of how to treat this file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_extension(extension):\n    extension = decode_path(extension)\n    if extension is None:\n        return\n    if extension.startswith('.'):\n        extension = extension[1:]\n    if '.' in extension:\n        _, extension = os.path.splitext(extension)\n    extension = slugify(extension, sep='')\n    if extension is None:\n        return\n    if len(extension):\n        return extension", "response": "Normalise a file name extension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetch(url: str, **kwargs) -> Selector:\n    kwargs.setdefault('headers', DEFAULT_HEADERS)\n    try:\n        res = requests.get(url, **kwargs)\n        res.raise_for_status()\n    except requests.RequestException as e:\n        print(e)\n    else:\n        html = res.text\n        tree = Selector(text=html)\n        return tree", "response": "Send HTTP request and parse it as a DOM tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndo the fetch in an async style. Args: url (str): The url of the site. Returns: Selector: allows you to select parts of HTML text using CSS or XPath expressions.", "response": "async def async_fetch(url: str, **kwargs) -> Selector:\n    \"\"\"\n    Do the fetch in an async style.\n\n    Args:\n        url (str): The url of the site.\n\n    Returns:\n        Selector: allows you to select parts of HTML text using CSS or XPath expressions.\n    \"\"\"\n    kwargs.setdefault('headers', DEFAULT_HEADERS)\n    async with aiohttp.ClientSession(**kwargs) as ses:\n        async with ses.get(url, **kwargs) as res:\n            html = await res.text()\n            tree = Selector(text=html)\n            return tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nviews the page whether rendered properly.", "response": "def view(url: str, **kwargs) -> bool:\n    \"\"\"\n    View the page whether rendered properly. (ensure the <base> tag to make external links work)\n\n    Args:\n        url (str): The url of the site.\n    \"\"\"\n    kwargs.setdefault('headers', DEFAULT_HEADERS)\n    html = requests.get(url, **kwargs).content\n    if b'<base' not in html:\n        repl = f'<head><base href=\"{url}\">'\n        html = html.replace(b'<head>', repl.encode('utf-8'))\n    fd, fname = tempfile.mkstemp('.html')\n    os.write(fd, html)\n    os.close(fd)\n    return webbrowser.open(f'file://{fname}')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the links of the page.", "response": "def links(res: requests.models.Response,\n          search: str = None,\n          pattern: str = None) -> list:\n    \"\"\"Get the links of the page.\n\n    Args:\n        res (requests.models.Response): The response of the page.\n        search (str, optional): Defaults to None. Search the links you want.\n        pattern (str, optional): Defaults to None. Search the links use a regex pattern.\n\n    Returns:\n        list: All the links of the page.\n    \"\"\"\n    hrefs = [link.to_text() for link in find_all_links(res.text)]\n    if search:\n        hrefs = [href for href in hrefs if search in href]\n    if pattern:\n        hrefs = [href for href in hrefs if re.findall(pattern, href)]\n    return list(set(hrefs))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving what you crawled as a json file.", "response": "def save_as_json(total: list,\n                 name='data.json',\n                 sort_by: str = None,\n                 no_duplicate=False,\n                 order='asc'):\n    \"\"\"Save what you crawled as a json file.\n\n    Args:\n        total (list): Total of data you crawled.\n        name (str, optional): Defaults to 'data.json'. The name of the file.\n        sort_by (str, optional): Defaults to None. Sort items by a specific key.\n        no_duplicate (bool, optional): Defaults to False. If True, it will remove duplicated data.\n        order (str, optional): Defaults to 'asc'. The opposite option is 'desc'.\n    \"\"\"\n    if sort_by:\n        reverse = order == 'desc'\n        total = sorted(total, key=itemgetter(sort_by), reverse=reverse)\n    if no_duplicate:\n        total = [key for key, _ in groupby(total)]\n    data = json.dumps(total, ensure_ascii=False)\n    Path(name).write_text(data, encoding='utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_value_tuple(self):\n        retval = tuple()\n        for val in self.VALUES:\n            retval += (getattr(self, val),)\n        return retval", "response": "Returns a tuple of the color s values in order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_observer(self, observer):\n        observer = str(observer)\n        if observer not in color_constants.OBSERVERS:\n            raise InvalidObserverError(self)\n        self.observer = observer", "response": "Validates and sets the color s observer angle."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate and sets the color s illuminant.", "response": "def set_illuminant(self, illuminant):\n        \"\"\"\n        Validates and sets the color's illuminant.\n\n        .. note:: This only changes the illuminant. It does no conversion\n            of the color's coordinates. For this, you'll want to refer to\n            :py:meth:`XYZColor.apply_adaptation <colormath.color_objects.XYZColor.apply_adaptation>`.\n\n        .. tip:: Call this after setting your observer.\n\n        :param str illuminant: One of the various illuminants.\n        \"\"\"\n        illuminant = illuminant.lower()\n        if illuminant not in color_constants.ILLUMINANTS[self.observer]:\n            raise InvalidIlluminantError(illuminant)\n        self.illuminant = illuminant"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the XYZ values for an observer and illuminant.", "response": "def get_illuminant_xyz(self, observer=None, illuminant=None):\n        \"\"\"\n        :param str observer: Get the XYZ values for another observer angle. Must\n            be either '2' or '10'.\n        :param str illuminant: Get the XYZ values for another illuminant.\n        :returns: the color's illuminant's XYZ values.\n        \"\"\"\n        try:\n            if observer is None:\n                observer = self.observer\n\n            illums_observer = color_constants.ILLUMINANTS[observer]\n        except KeyError:\n            raise InvalidObserverError(self)\n\n        try:\n            if illuminant is None:\n                illuminant = self.illuminant\n\n            illum_xyz = illums_observer[illuminant]\n        except (KeyError, AttributeError):\n            raise InvalidIlluminantError(illuminant)\n\n        return {'X': illum_xyz[0], 'Y': illum_xyz[1], 'Z': illum_xyz[2]}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndump this color into NumPy array.", "response": "def get_numpy_array(self):\n        \"\"\"\n        Dump this color into NumPy array.\n        \"\"\"\n        # This holds the obect's spectral data, and will be passed to\n        # numpy.array() to create a numpy array (matrix) for the matrix math\n        # that will be done during the conversion to XYZ.\n        values = []\n\n        # Use the required value list to build this dynamically. Default to\n        # 0.0, since that ultimately won't affect the outcome due to the math\n        # involved.\n        for val in self.VALUES:\n            values.append(getattr(self, val, 0.0))\n\n        # Create and the actual numpy array/matrix from the spectral list.\n        color_array = numpy.array([values])\n        return color_array"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_density(self, density_standard=None):\n        if density_standard is not None:\n            return density.ansi_density(self, density_standard)\n        else:\n            return density.auto_density(self)", "response": "Calculates the density of the SpectralColor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply an adaptation matrix to change the XYZ color s illuminant.", "response": "def apply_adaptation(self, target_illuminant, adaptation='bradford'):\n        \"\"\"\n        This applies an adaptation matrix to change the XYZ color's illuminant.\n        You'll most likely only need this during RGB conversions.\n        \"\"\"\n        logger.debug(\"  \\- Original illuminant: %s\", self.illuminant)\n        logger.debug(\"  \\- Target illuminant: %s\", target_illuminant)\n\n        # If the XYZ values were taken with a different reference white than the\n        # native reference white of the target RGB space, a transformation matrix\n        # must be applied.\n        if self.illuminant != target_illuminant:\n            logger.debug(\"  \\* Applying transformation from %s to %s \",\n                         self.illuminant, target_illuminant)\n            # Sets the adjusted XYZ values, and the new illuminant.\n            apply_chromatic_adaptation_on_color(\n                color=self,\n                targ_illum=target_illuminant,\n                adaptation=adaptation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _clamp_rgb_coordinate(self, coord):\n        if not self.is_upscaled:\n            return min(max(coord, 0.0), 1.0)\n        else:\n            return min(max(coord, 1), 255)", "response": "Clamps an RGB coordinate taking into account whether or not the color is upscaled or not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_upscaled_value_tuple(self):\n        # Scale up to 0-255 values.\n        rgb_r = int(math.floor(0.5 + self.rgb_r * 255))\n        rgb_g = int(math.floor(0.5 + self.rgb_g * 255))\n        rgb_b = int(math.floor(0.5 + self.rgb_b * 255))\n\n        return rgb_r, rgb_g, rgb_b", "response": "Returns a tuple of the upscaled RGB color values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts the RGB value to a hex value in the form of R G B B", "response": "def get_rgb_hex(self):\n        \"\"\"\n        Converts the RGB value to a hex value in the form of: #RRGGBB\n\n        :rtype: str\n        \"\"\"\n        rgb_r, rgb_g, rgb_b = self.get_upscaled_value_tuple()\n        return '#%02x%02x%02x' % (rgb_r, rgb_g, rgb_b)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an RGB hex string like RGGBB to a sRGBColor object.", "response": "def new_from_rgb_hex(cls, hex_str):\n        \"\"\"\n        Converts an RGB hex string like #RRGGBB and assigns the values to\n        this sRGBColor object.\n\n        :rtype: sRGBColor\n        \"\"\"\n        colorstring = hex_str.strip()\n        if colorstring[0] == '#':\n            colorstring = colorstring[1:]\n        if len(colorstring) != 6:\n            raise ValueError(\"input #%s is not in #RRGGBB format\" % colorstring)\n        r, g, b = colorstring[:2], colorstring[2:4], colorstring[4:]\n        r, g, b = [int(n, 16) / 255.0 for n in (r, g, b)]\n        return cls(r, g, b)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the Delta E between lab_color_vector and all colors in lab_color_matrix.", "response": "def delta_e_cie1976(lab_color_vector, lab_color_matrix):\n    \"\"\"\n    Calculates the Delta E (CIE1976) between `lab_color_vector` and all\n    colors in `lab_color_matrix`.\n    \"\"\"\n    return numpy.sqrt(\n        numpy.sum(numpy.power(lab_color_vector - lab_color_matrix, 2), axis=1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delta_e_cie1994(lab_color_vector, lab_color_matrix,\n                    K_L=1, K_C=1, K_H=1, K_1=0.045, K_2=0.015):\n    \"\"\"\n    Calculates the Delta E (CIE1994) of two colors.\n\n    K_l:\n      0.045 graphic arts\n      0.048 textiles\n    K_2:\n      0.015 graphic arts\n      0.014 textiles\n    K_L:\n      1 default\n      2 textiles\n    \"\"\"\n    C_1 = numpy.sqrt(numpy.sum(numpy.power(lab_color_vector[1:], 2)))\n    C_2 = numpy.sqrt(numpy.sum(numpy.power(lab_color_matrix[:, 1:], 2), axis=1))\n\n    delta_lab = lab_color_vector - lab_color_matrix\n\n    delta_L = delta_lab[:, 0].copy()\n    delta_C = C_1 - C_2\n    delta_lab[:, 0] = delta_C\n\n    delta_H_sq = numpy.sum(numpy.power(delta_lab, 2) * numpy.array([-1, 1, 1]), axis=1)\n    # noinspection PyArgumentList\n    delta_H = numpy.sqrt(delta_H_sq.clip(min=0))\n\n    S_L = 1\n    S_C = 1 + K_1 * C_1\n    S_H = 1 + K_2 * C_1\n\n    LCH = numpy.vstack([delta_L, delta_C, delta_H])\n    params = numpy.array([[K_L * S_L], [K_C * S_C], [K_H * S_H]])\n\n    return numpy.sqrt(numpy.sum(numpy.power(LCH / params, 2), axis=0))", "response": "Calculates the Delta E of two colors."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delta_e_cmc(lab_color_vector, lab_color_matrix, pl=2, pc=1):\n    L, a, b = lab_color_vector\n\n    C_1 = numpy.sqrt(numpy.sum(numpy.power(lab_color_vector[1:], 2)))\n    C_2 = numpy.sqrt(numpy.sum(numpy.power(lab_color_matrix[:, 1:], 2), axis=1))\n\n    delta_lab = lab_color_vector - lab_color_matrix\n\n    delta_L = delta_lab[:, 0].copy()\n    delta_C = C_1 - C_2\n    delta_lab[:, 0] = delta_C\n\n    H_1 = numpy.degrees(numpy.arctan2(b, a))\n\n    if H_1 < 0:\n        H_1 += 360\n\n    F = numpy.sqrt(numpy.power(C_1, 4) / (numpy.power(C_1, 4) + 1900.0))\n\n    # noinspection PyChainedComparisons\n    if 164 <= H_1 and H_1 <= 345:\n        T = 0.56 + abs(0.2 * numpy.cos(numpy.radians(H_1 + 168)))\n    else:\n        T = 0.36 + abs(0.4 * numpy.cos(numpy.radians(H_1 + 35)))\n\n    if L < 16:\n        S_L = 0.511\n    else:\n        S_L = (0.040975 * L) / (1 + 0.01765 * L)\n\n    S_C = ((0.0638 * C_1) / (1 + 0.0131 * C_1)) + 0.638\n    S_H = S_C * (F * T + 1 - F)\n\n    delta_C = C_1 - C_2\n\n    delta_H_sq = numpy.sum(numpy.power(delta_lab, 2) * numpy.array([-1, 1, 1]), axis=1)\n    # noinspection PyArgumentList\n    delta_H = numpy.sqrt(delta_H_sq.clip(min=0))\n\n    LCH = numpy.vstack([delta_L, delta_C, delta_H])\n    params = numpy.array([[pl * S_L], [pc * S_C], [S_H]])\n\n    return numpy.sqrt(numpy.sum(numpy.power(LCH / params, 2), axis=0))", "response": "Calculates the Delta E of two colors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the Delta E of two colors.", "response": "def delta_e_cie2000(lab_color_vector, lab_color_matrix, Kl=1, Kc=1, Kh=1):\n    \"\"\"\n    Calculates the Delta E (CIE2000) of two colors.\n    \"\"\"\n    L, a, b = lab_color_vector\n\n    avg_Lp = (L + lab_color_matrix[:, 0]) / 2.0\n\n    C1 = numpy.sqrt(numpy.sum(numpy.power(lab_color_vector[1:], 2)))\n    C2 = numpy.sqrt(numpy.sum(numpy.power(lab_color_matrix[:, 1:], 2), axis=1))\n\n    avg_C1_C2 = (C1 + C2) / 2.0\n\n    G = 0.5 * (1 - numpy.sqrt(numpy.power(avg_C1_C2, 7.0) / (numpy.power(avg_C1_C2, 7.0) + numpy.power(25.0, 7.0))))\n\n    a1p = (1.0 + G) * a\n    a2p = (1.0 + G) * lab_color_matrix[:, 1]\n\n    C1p = numpy.sqrt(numpy.power(a1p, 2) + numpy.power(b, 2))\n    C2p = numpy.sqrt(numpy.power(a2p, 2) + numpy.power(lab_color_matrix[:, 2], 2))\n\n    avg_C1p_C2p = (C1p + C2p) / 2.0\n\n    h1p = numpy.degrees(numpy.arctan2(b, a1p))\n    h1p += (h1p < 0) * 360\n\n    h2p = numpy.degrees(numpy.arctan2(lab_color_matrix[:, 2], a2p))\n    h2p += (h2p < 0) * 360\n\n    avg_Hp = (((numpy.fabs(h1p - h2p) > 180) * 360) + h1p + h2p) / 2.0\n\n    T = 1 - 0.17 * numpy.cos(numpy.radians(avg_Hp - 30)) + \\\n        0.24 * numpy.cos(numpy.radians(2 * avg_Hp)) + \\\n        0.32 * numpy.cos(numpy.radians(3 * avg_Hp + 6)) - \\\n        0.2 * numpy.cos(numpy.radians(4 * avg_Hp - 63))\n\n    diff_h2p_h1p = h2p - h1p\n    delta_hp = diff_h2p_h1p + (numpy.fabs(diff_h2p_h1p) > 180) * 360\n    delta_hp -= (h2p > h1p) * 720\n\n    delta_Lp = lab_color_matrix[:, 0] - L\n    delta_Cp = C2p - C1p\n    delta_Hp = 2 * numpy.sqrt(C2p * C1p) * numpy.sin(numpy.radians(delta_hp) / 2.0)\n\n    S_L = 1 + ((0.015 * numpy.power(avg_Lp - 50, 2)) / numpy.sqrt(20 + numpy.power(avg_Lp - 50, 2.0)))\n    S_C = 1 + 0.045 * avg_C1p_C2p\n    S_H = 1 + 0.015 * avg_C1p_C2p * T\n\n    delta_ro = 30 * numpy.exp(-(numpy.power(((avg_Hp - 275) / 25), 2.0)))\n    R_C = numpy.sqrt((numpy.power(avg_C1p_C2p, 7.0)) / (numpy.power(avg_C1p_C2p, 7.0) + numpy.power(25.0, 7.0)))\n    R_T = -2 * R_C * numpy.sin(2 * numpy.radians(delta_ro))\n\n    return numpy.sqrt(\n        numpy.power(delta_Lp / (S_L * Kl), 2) +\n        numpy.power(delta_Cp / (S_C * Kc), 2) +\n        numpy.power(delta_Hp / (S_H * Kh), 2) +\n        R_T * (delta_Cp / (S_C * Kc)) * (delta_Hp / (S_H * Kh)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the density for the given color and density standard.", "response": "def ansi_density(color, density_standard):\n    \"\"\"\n    Calculates density for the given SpectralColor using the spectral weighting\n    function provided. For example, ANSI_STATUS_T_RED. These may be found in\n    :py:mod:`colormath.density_standards`.\n\n    :param SpectralColor color: The SpectralColor object to calculate\n        density for.\n    :param numpy.ndarray density_standard: NumPy array of filter of choice\n        from :py:mod:`colormath.density_standards`.\n    :rtype: float\n    :returns: The density value for the given color and density standard.\n    \"\"\"\n    # Load the spec_XXXnm attributes into a Numpy array.\n    sample = color.get_numpy_array()\n    # Matrix multiplication\n    intermediate = sample * density_standard\n\n    # Sum the products.\n    numerator = intermediate.sum()\n    # This is the denominator in the density equation.\n    sum_of_standard_wavelengths = density_standard.sum()\n\n    # This is the top level of the density formula.\n    return -1.0 * log10(numerator / sum_of_standard_wavelengths)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a SpectralColor, automatically choose the correct ANSI T filter. Returns a tuple with a string representation of the filter the calculated density. :param SpectralColor color: The SpectralColor object to calculate density for. :rtype: float :returns: The density value, with the filter selected automatically.", "response": "def auto_density(color):\n    \"\"\"\n    Given a SpectralColor, automatically choose the correct ANSI T filter.\n    Returns a tuple with a string representation of the filter the\n    calculated density.\n\n    :param SpectralColor color: The SpectralColor object to calculate\n        density for.\n    :rtype: float\n    :returns: The density value, with the filter selected automatically.\n    \"\"\"\n    blue_density = ansi_density(color, ANSI_STATUS_T_BLUE)\n    green_density = ansi_density(color, ANSI_STATUS_T_GREEN)\n    red_density = ansi_density(color, ANSI_STATUS_T_RED)\n\n    densities = [blue_density, green_density, red_density]\n    min_density = min(densities)\n    max_density = max(densities)\n    density_range = max_density - min_density\n\n    # See comments in density_standards.py for VISUAL_DENSITY_THRESH to\n    # understand what this is doing.\n    if density_range <= VISUAL_DENSITY_THRESH:\n        return ansi_density(color, ISO_VISUAL)\n    elif blue_density > green_density and blue_density > red_density:\n        return blue_density\n    elif green_density > blue_density and green_density > red_density:\n        return green_density\n    else:\n        return red_density"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_lab_color1_vector(color):\n    if not color.__class__.__name__ == 'LabColor':\n        raise ValueError(\n            \"Delta E functions can only be used with two LabColor objects.\")\n    return numpy.array([color.lab_l, color.lab_a, color.lab_b])", "response": "Converts an LabColor into a NumPy vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the Delta E of two colors.", "response": "def delta_e_cie1976(color1, color2):\n    \"\"\"\n    Calculates the Delta E (CIE1976) of two colors.\n    \"\"\"\n    color1_vector = _get_lab_color1_vector(color1)\n    color2_matrix = _get_lab_color2_matrix(color2)\n    delta_e = color_diff_matrix.delta_e_cie1976(color1_vector, color2_matrix)[0]\n    return numpy.asscalar(delta_e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delta_e_cie1994(color1, color2, K_L=1, K_C=1, K_H=1, K_1=0.045, K_2=0.015):\n    color1_vector = _get_lab_color1_vector(color1)\n    color2_matrix = _get_lab_color2_matrix(color2)\n    delta_e = color_diff_matrix.delta_e_cie1994(\n        color1_vector, color2_matrix, K_L=K_L, K_C=K_C, K_H=K_H, K_1=K_1, K_2=K_2)[0]\n    return numpy.asscalar(delta_e)", "response": "Calculates the Delta E of two colors."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the Delta E of two colors.", "response": "def delta_e_cie2000(color1, color2, Kl=1, Kc=1, Kh=1):\n    \"\"\"\n    Calculates the Delta E (CIE2000) of two colors.\n    \"\"\"\n    color1_vector = _get_lab_color1_vector(color1)\n    color2_matrix = _get_lab_color2_matrix(color2)\n    delta_e = color_diff_matrix.delta_e_cie2000(\n        color1_vector, color2_matrix, Kl=Kl, Kc=Kc, Kh=Kh)[0]\n    return numpy.asscalar(delta_e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delta_e_cmc(color1, color2, pl=2, pc=1):\n    color1_vector = _get_lab_color1_vector(color1)\n    color2_matrix = _get_lab_color2_matrix(color2)\n    delta_e = color_diff_matrix.delta_e_cmc(\n        color1_vector, color2_matrix, pl=pl, pc=pc)[0]\n    return numpy.asscalar(delta_e)", "response": "Calculates the Delta E of two colors."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the correct transformation matrix based on origin and target illuminants.", "response": "def _get_adaptation_matrix(wp_src, wp_dst, observer, adaptation):\n    \"\"\"\n    Calculate the correct transformation matrix based on origin and target\n    illuminants. The observer angle must be the same between illuminants.\n\n    See colormath.color_constants.ADAPTATION_MATRICES for a list of possible\n    adaptations.\n\n    Detailed conversion documentation is available at:\n    http://brucelindbloom.com/Eqn_ChromAdapt.html\n    \"\"\"\n    # Get the appropriate transformation matrix, [MsubA].\n    m_sharp = color_constants.ADAPTATION_MATRICES[adaptation]\n\n    # In case the white-points are still input as strings\n    # Get white-points for illuminant\n    if isinstance(wp_src, str):\n        orig_illum = wp_src.lower()\n        wp_src = color_constants.ILLUMINANTS[observer][orig_illum]\n    elif hasattr(wp_src, '__iter__'):\n        wp_src = wp_src\n\n    if isinstance(wp_dst, str):\n        targ_illum = wp_dst.lower()\n        wp_dst = color_constants.ILLUMINANTS[observer][targ_illum]\n    elif hasattr(wp_dst, '__iter__'):\n        wp_dst = wp_dst\n\n    # Sharpened cone responses ~ rho gamma beta ~ sharpened r g b\n    rgb_src = numpy.dot(m_sharp, wp_src)\n    rgb_dst = numpy.dot(m_sharp, wp_dst)\n\n    # Ratio of whitepoint sharpened responses\n    m_rat = numpy.diag(rgb_dst / rgb_src)\n\n    # Final transformation matrix\n    m_xfm = numpy.dot(numpy.dot(pinv(m_sharp), m_rat), m_sharp)\n\n    return m_xfm"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying a chromatic adaptation matrix to convert XYZ values between the original illuminant and the target illuminant.", "response": "def apply_chromatic_adaptation(val_x, val_y, val_z, orig_illum, targ_illum,\n                               observer='2', adaptation='bradford'):\n    \"\"\"\n    Applies a chromatic adaptation matrix to convert XYZ values between\n    illuminants. It is important to recognize that color transformation results\n    in color errors, determined by how far the original illuminant is from the\n    target illuminant. For example, D65 to A could result in very high maximum\n    deviance.\n\n    An informative article with estimate average Delta E values for each\n    illuminant conversion may be found at:\n\n    http://brucelindbloom.com/ChromAdaptEval.html\n    \"\"\"\n    # It's silly to have to do this, but some people may want to call this\n    # function directly, so we'll protect them from messing up upper/lower case.\n    adaptation = adaptation.lower()\n\n    # Get white-points for illuminant\n    if isinstance(orig_illum, str):\n        orig_illum = orig_illum.lower()\n        wp_src = color_constants.ILLUMINANTS[observer][orig_illum]\n    elif hasattr(orig_illum, '__iter__'):\n        wp_src = orig_illum\n\n    if isinstance(targ_illum, str):\n        targ_illum = targ_illum.lower()\n        wp_dst = color_constants.ILLUMINANTS[observer][targ_illum]\n    elif hasattr(targ_illum, '__iter__'):\n        wp_dst = targ_illum\n\n    logger.debug(\"  \\* Applying adaptation matrix: %s\", adaptation)\n    # Retrieve the appropriate transformation matrix from the constants.\n    transform_matrix = _get_adaptation_matrix(wp_src, wp_dst,\n                                              observer, adaptation)\n\n    # Stuff the XYZ values into a NumPy matrix for conversion.\n    XYZ_matrix = numpy.array((val_x, val_y, val_z))\n    # Perform the adaptation via matrix multiplication.\n    result_matrix = numpy.dot(transform_matrix, XYZ_matrix)\n\n    # Return individual X, Y, and Z coordinates.\n    return result_matrix[0], result_matrix[1], result_matrix[2]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_chromatic_adaptation_on_color(color, targ_illum, adaptation='bradford'):\n    xyz_x = color.xyz_x\n    xyz_y = color.xyz_y\n    xyz_z = color.xyz_z\n    orig_illum = color.illuminant\n    targ_illum = targ_illum.lower()\n    observer = color.observer\n    adaptation = adaptation.lower()\n\n    # Return individual X, Y, and Z coordinates.\n    color.xyz_x, color.xyz_y, color.xyz_z = apply_chromatic_adaptation(\n        xyz_x, xyz_y, xyz_z, orig_illum, targ_illum,\n        observer=observer, adaptation=adaptation)\n    color.set_illuminant(targ_illum)\n\n    return color", "response": "Convenience function to apply an adaptation to a Color object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef example_lchab_to_lchuv():\n\n    print(\"=== Complex Example: LCHab->LCHuv ===\")\n    # Instantiate an LCHab color object with the given values.\n    lchab = LCHabColor(0.903, 16.447, 352.252)\n    # Show a string representation.\n    print(lchab)\n    # Convert to LCHuv.\n    lchuv = convert_color(lchab, LCHuvColor)\n    print(lchuv)\n    print(\"=== End Example ===\\n\")", "response": "This function shows how complex LCHab to LCHuv are used in action."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_RGB_matrix(var1, var2, var3, rgb_type, convtype=\"xyz_to_rgb\"):\n    convtype = convtype.lower()\n    # Retrieve the appropriate transformation matrix from the constants.\n    rgb_matrix = rgb_type.conversion_matrices[convtype]\n\n    logger.debug(\"  \\* Applying RGB conversion matrix: %s->%s\",\n                 rgb_type.__class__.__name__, convtype)\n    # Stuff the RGB/XYZ values into a NumPy matrix for conversion.\n    var_matrix = numpy.array((\n        var1, var2, var3\n    ))\n    # Perform the adaptation via matrix multiplication.\n    result_matrix = numpy.dot(rgb_matrix, var_matrix)\n    rgb_r, rgb_g, rgb_b = result_matrix\n    # Clamp these values to a valid range.\n    rgb_r = max(rgb_r, 0.0)\n    rgb_g = max(rgb_g, 0.0)\n    rgb_b = max(rgb_b, 0.0)\n    return rgb_r, rgb_g, rgb_b", "response": "Applies an RGB working matrix to convert from XYZ to RGB."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Spectral_to_XYZ(cobj, illuminant_override=None, *args, **kwargs):\n    # If the user provides an illuminant_override numpy array, use it.\n    if illuminant_override:\n        reference_illum = illuminant_override\n    else:\n        # Otherwise, look up the illuminant from known standards based\n        # on the value of 'illuminant' pulled from the SpectralColor object.\n        try:\n            reference_illum = spectral_constants.REF_ILLUM_TABLE[cobj.illuminant]\n        except KeyError:\n            raise InvalidIlluminantError(cobj.illuminant)\n\n    # Get the spectral distribution of the selected standard observer.\n    if cobj.observer == '10':\n        std_obs_x = spectral_constants.STDOBSERV_X10\n        std_obs_y = spectral_constants.STDOBSERV_Y10\n        std_obs_z = spectral_constants.STDOBSERV_Z10\n    else:\n        # Assume 2 degree, since it is theoretically the only other possibility.\n        std_obs_x = spectral_constants.STDOBSERV_X2\n        std_obs_y = spectral_constants.STDOBSERV_Y2\n        std_obs_z = spectral_constants.STDOBSERV_Z2\n\n    # This is a NumPy array containing the spectral distribution of the color.\n    sample = cobj.get_numpy_array()\n\n    # The denominator is constant throughout the entire calculation for X,\n    # Y, and Z coordinates. Calculate it once and re-use.\n    denom = std_obs_y * reference_illum\n\n    # This is also a common element in the calculation whereby the sample\n    # NumPy array is multiplied by the reference illuminant's power distribution\n    # (which is also a NumPy array).\n    sample_by_ref_illum = sample * reference_illum\n\n    # Calculate the numerator of the equation to find X.\n    x_numerator = sample_by_ref_illum * std_obs_x\n    y_numerator = sample_by_ref_illum * std_obs_y\n    z_numerator = sample_by_ref_illum * std_obs_z\n\n    xyz_x = x_numerator.sum() / denom.sum()\n    xyz_y = y_numerator.sum() / denom.sum()\n    xyz_z = z_numerator.sum() / denom.sum()\n\n    return XYZColor(\n        xyz_x, xyz_y, xyz_z, observer=cobj.observer, illuminant=cobj.illuminant)", "response": "Converts a SpectralColor object to XYZ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Lab_to_LCHab(cobj, *args, **kwargs):\n    lch_l = cobj.lab_l\n    lch_c = math.sqrt(\n        math.pow(float(cobj.lab_a), 2) + math.pow(float(cobj.lab_b), 2))\n    lch_h = math.atan2(float(cobj.lab_b), float(cobj.lab_a))\n\n    if lch_h > 0:\n        lch_h = (lch_h / math.pi) * 180\n    else:\n        lch_h = 360 - (math.fabs(lch_h) / math.pi) * 180\n\n    return LCHabColor(\n        lch_l, lch_c, lch_h, observer=cobj.observer, illuminant=cobj.illuminant)", "response": "Convert from CIE Lab to LCHab."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Lab_to_XYZ(cobj, *args, **kwargs):\n    illum = cobj.get_illuminant_xyz()\n    xyz_y = (cobj.lab_l + 16.0) / 116.0\n    xyz_x = cobj.lab_a / 500.0 + xyz_y\n    xyz_z = xyz_y - cobj.lab_b / 200.0\n\n    if math.pow(xyz_y, 3) > color_constants.CIE_E:\n        xyz_y = math.pow(xyz_y, 3)\n    else:\n        xyz_y = (xyz_y - 16.0 / 116.0) / 7.787\n\n    if math.pow(xyz_x, 3) > color_constants.CIE_E:\n        xyz_x = math.pow(xyz_x, 3)\n    else:\n        xyz_x = (xyz_x - 16.0 / 116.0) / 7.787\n\n    if math.pow(xyz_z, 3) > color_constants.CIE_E:\n        xyz_z = math.pow(xyz_z, 3)\n    else:\n        xyz_z = (xyz_z - 16.0 / 116.0) / 7.787\n\n    xyz_x = (illum[\"X\"] * xyz_x)\n    xyz_y = (illum[\"Y\"] * xyz_y)\n    xyz_z = (illum[\"Z\"] * xyz_z)\n\n    return XYZColor(\n        xyz_x, xyz_y, xyz_z, observer=cobj.observer, illuminant=cobj.illuminant)", "response": "Convert from Lab to XYZColor"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Luv_to_LCHuv(cobj, *args, **kwargs):\n    lch_l = cobj.luv_l\n    lch_c = math.sqrt(math.pow(cobj.luv_u, 2.0) + math.pow(cobj.luv_v, 2.0))\n    lch_h = math.atan2(float(cobj.luv_v), float(cobj.luv_u))\n\n    if lch_h > 0:\n        lch_h = (lch_h / math.pi) * 180\n    else:\n        lch_h = 360 - (math.fabs(lch_h) / math.pi) * 180\n    return LCHuvColor(\n        lch_l, lch_c, lch_h, observer=cobj.observer, illuminant=cobj.illuminant)", "response": "Convert from CIE Luv to LCHuv."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Luv_to_XYZ(cobj, *args, **kwargs):\n    illum = cobj.get_illuminant_xyz()\n    # Without Light, there is no color. Short-circuit this and avoid some\n    # zero division errors in the var_a_frac calculation.\n    if cobj.luv_l <= 0.0:\n        xyz_x = 0.0\n        xyz_y = 0.0\n        xyz_z = 0.0\n        return XYZColor(\n            xyz_x, xyz_y, xyz_z,\n            observer=cobj.observer, illuminant=cobj.illuminant)\n\n    # Various variables used throughout the conversion.\n    cie_k_times_e = color_constants.CIE_K * color_constants.CIE_E\n    u_sub_0 = (4.0 * illum[\"X\"]) / (illum[\"X\"] + 15.0 * illum[\"Y\"] + 3.0 * illum[\"Z\"])\n    v_sub_0 = (9.0 * illum[\"Y\"]) / (illum[\"X\"] + 15.0 * illum[\"Y\"] + 3.0 * illum[\"Z\"])\n    var_u = cobj.luv_u / (13.0 * cobj.luv_l) + u_sub_0\n    var_v = cobj.luv_v / (13.0 * cobj.luv_l) + v_sub_0\n\n    # Y-coordinate calculations.\n    if cobj.luv_l > cie_k_times_e:\n        xyz_y = math.pow((cobj.luv_l + 16.0) / 116.0, 3.0)\n    else:\n        xyz_y = cobj.luv_l / color_constants.CIE_K\n\n    # X-coordinate calculation.\n    xyz_x = xyz_y * 9.0 * var_u / (4.0 * var_v)\n    # Z-coordinate calculation.\n    xyz_z = xyz_y * (12.0 - 3.0 * var_u - 20.0 * var_v) / (4.0 * var_v)\n\n    return XYZColor(\n        xyz_x, xyz_y, xyz_z, illuminant=cobj.illuminant, observer=cobj.observer)", "response": "Convert from Luv to XYZ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef LCHab_to_Lab(cobj, *args, **kwargs):\n    lab_l = cobj.lch_l\n    lab_a = math.cos(math.radians(cobj.lch_h)) * cobj.lch_c\n    lab_b = math.sin(math.radians(cobj.lch_h)) * cobj.lch_c\n    return LabColor(\n        lab_l, lab_a, lab_b, illuminant=cobj.illuminant, observer=cobj.observer)", "response": "Convert from LCH to Lab."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert from LCH to Luv.", "response": "def LCHuv_to_Luv(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from LCH(uv) to Luv.\n    \"\"\"\n    luv_l = cobj.lch_l\n    luv_u = math.cos(math.radians(cobj.lch_h)) * cobj.lch_c\n    luv_v = math.sin(math.radians(cobj.lch_h)) * cobj.lch_c\n    return LuvColor(\n        luv_l, luv_u, luv_v, illuminant=cobj.illuminant, observer=cobj.observer)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef xyY_to_XYZ(cobj, *args, **kwargs):\n    # avoid division by zero\n    if cobj.xyy_y == 0.0:\n        xyz_x = 0.0\n        xyz_y = 0.0\n        xyz_z = 0.0\n    else:\n        xyz_x = (cobj.xyy_x * cobj.xyy_Y) / cobj.xyy_y\n        xyz_y = cobj.xyy_Y\n        xyz_z = ((1.0 - cobj.xyy_x - cobj.xyy_y) * xyz_y) / cobj.xyy_y\n\n    return XYZColor(\n        xyz_x, xyz_y, xyz_z, illuminant=cobj.illuminant, observer=cobj.observer)", "response": "Convert from xyY to XYZ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert from XYZ to xyY.", "response": "def XYZ_to_xyY(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from XYZ to xyY.\n    \"\"\"\n    xyz_sum = cobj.xyz_x + cobj.xyz_y + cobj.xyz_z\n    # avoid division by zero\n    if xyz_sum == 0.0:\n        xyy_x = 0.0\n        xyy_y = 0.0\n    else:\n        xyy_x = cobj.xyz_x / xyz_sum\n        xyy_y = cobj.xyz_y / xyz_sum\n    xyy_Y = cobj.xyz_y\n\n    return xyYColor(\n        xyy_x, xyy_y, xyy_Y, observer=cobj.observer, illuminant=cobj.illuminant)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting from XYZ to Luv", "response": "def XYZ_to_Luv(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from XYZ to Luv\n    \"\"\"\n    temp_x = cobj.xyz_x\n    temp_y = cobj.xyz_y\n    temp_z = cobj.xyz_z\n    denom = temp_x + (15.0 * temp_y) + (3.0 * temp_z)\n    # avoid division by zero\n    if denom == 0.0:\n        luv_u = 0.0\n        luv_v = 0.0\n    else:\n        luv_u = (4.0 * temp_x) / denom\n        luv_v = (9.0 * temp_y) / denom\n\n    illum = cobj.get_illuminant_xyz()\n    temp_y = temp_y / illum[\"Y\"]\n    if temp_y > color_constants.CIE_E:\n        temp_y = math.pow(temp_y, (1.0 / 3.0))\n    else:\n        temp_y = (7.787 * temp_y) + (16.0 / 116.0)\n\n    ref_U = (4.0 * illum[\"X\"]) / (illum[\"X\"] + (15.0 * illum[\"Y\"]) + (3.0 * illum[\"Z\"]))\n    ref_V = (9.0 * illum[\"Y\"]) / (illum[\"X\"] + (15.0 * illum[\"Y\"]) + (3.0 * illum[\"Z\"]))\n\n    luv_l = (116.0 * temp_y) - 16.0\n    luv_u = 13.0 * luv_l * (luv_u - ref_U)\n    luv_v = 13.0 * luv_l * (luv_v - ref_V)\n\n    return LuvColor(\n        luv_l, luv_u, luv_v, observer=cobj.observer, illuminant=cobj.illuminant)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef XYZ_to_Lab(cobj, *args, **kwargs):\n    illum = cobj.get_illuminant_xyz()\n    temp_x = cobj.xyz_x / illum[\"X\"]\n    temp_y = cobj.xyz_y / illum[\"Y\"]\n    temp_z = cobj.xyz_z / illum[\"Z\"]\n\n    if temp_x > color_constants.CIE_E:\n        temp_x = math.pow(temp_x, (1.0 / 3.0))\n    else:\n        temp_x = (7.787 * temp_x) + (16.0 / 116.0)\n\n    if temp_y > color_constants.CIE_E:\n        temp_y = math.pow(temp_y, (1.0 / 3.0))\n    else:\n        temp_y = (7.787 * temp_y) + (16.0 / 116.0)\n\n    if temp_z > color_constants.CIE_E:\n        temp_z = math.pow(temp_z, (1.0 / 3.0))\n    else:\n        temp_z = (7.787 * temp_z) + (16.0 / 116.0)\n\n    lab_l = (116.0 * temp_y) - 16.0\n    lab_a = 500.0 * (temp_x - temp_y)\n    lab_b = 200.0 * (temp_y - temp_z)\n    return LabColor(\n        lab_l, lab_a, lab_b, observer=cobj.observer, illuminant=cobj.illuminant)", "response": "Converts XYZ to Lab."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef XYZ_to_RGB(cobj, target_rgb, *args, **kwargs):\n    temp_X = cobj.xyz_x\n    temp_Y = cobj.xyz_y\n    temp_Z = cobj.xyz_z\n\n    logger.debug(\"  \\- Target RGB space: %s\", target_rgb)\n    target_illum = target_rgb.native_illuminant\n    logger.debug(\"  \\- Target native illuminant: %s\", target_illum)\n    logger.debug(\"  \\- XYZ color's illuminant: %s\", cobj.illuminant)\n\n    # If the XYZ values were taken with a different reference white than the\n    # native reference white of the target RGB space, a transformation matrix\n    # must be applied.\n    if cobj.illuminant != target_illum:\n        logger.debug(\"  \\* Applying transformation from %s to %s \",\n                     cobj.illuminant, target_illum)\n        # Get the adjusted XYZ values, adapted for the target illuminant.\n        temp_X, temp_Y, temp_Z = apply_chromatic_adaptation(\n            temp_X, temp_Y, temp_Z,\n            orig_illum=cobj.illuminant, targ_illum=target_illum)\n        logger.debug(\"  \\*   New values: %.3f, %.3f, %.3f\",\n                     temp_X, temp_Y, temp_Z)\n\n    # Apply an RGB working space matrix to the XYZ values (matrix mul).\n    rgb_r, rgb_g, rgb_b = apply_RGB_matrix(\n        temp_X, temp_Y, temp_Z,\n        rgb_type=target_rgb, convtype=\"xyz_to_rgb\")\n\n    # v\n    linear_channels = dict(r=rgb_r, g=rgb_g, b=rgb_b)\n    # V\n    nonlinear_channels = {}\n    if target_rgb == sRGBColor:\n        for channel in ['r', 'g', 'b']:\n            v = linear_channels[channel]\n            if v <= 0.0031308:\n                nonlinear_channels[channel] = v * 12.92\n            else:\n                nonlinear_channels[channel] = 1.055 * math.pow(v, 1 / 2.4) - 0.055\n    elif target_rgb == BT2020Color:\n        if kwargs.get('is_12_bits_system'):\n            a, b = 1.0993, 0.0181\n        else:\n            a, b = 1.099, 0.018\n        for channel in ['r', 'g', 'b']:\n            v = linear_channels[channel]\n            if v < b:\n                nonlinear_channels[channel] = v * 4.5\n            else:\n                nonlinear_channels[channel] = a * math.pow(v, 0.45) - (a - 1)\n    else:\n        # If it's not sRGB...\n        for channel in ['r', 'g', 'b']:\n            v = linear_channels[channel]\n            nonlinear_channels[channel] = math.pow(v, 1 / target_rgb.rgb_gamma)\n\n    return target_rgb(\n        nonlinear_channels['r'], nonlinear_channels['g'], nonlinear_channels['b'])", "response": "XYZ to RGB conversion."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert from RGB to Hue", "response": "def __RGB_to_Hue(var_R, var_G, var_B, var_min, var_max):\n    \"\"\"\n    For RGB_to_HSL and RGB_to_HSV, the Hue (H) component is calculated in\n    the same way.\n    \"\"\"\n    if var_max == var_min:\n        return 0.0\n    elif var_max == var_R:\n        return (60.0 * ((var_G - var_B) / (var_max - var_min)) + 360) % 360.0\n    elif var_max == var_G:\n        return 60.0 * ((var_B - var_R) / (var_max - var_min)) + 120\n    elif var_max == var_B:\n        return 60.0 * ((var_R - var_G) / (var_max - var_min)) + 240.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef RGB_to_HSV(cobj, *args, **kwargs):\n    var_R = cobj.rgb_r\n    var_G = cobj.rgb_g\n    var_B = cobj.rgb_b\n\n    var_max = max(var_R, var_G, var_B)\n    var_min = min(var_R, var_G, var_B)\n\n    var_H = __RGB_to_Hue(var_R, var_G, var_B, var_min, var_max)\n\n    if var_max == 0:\n        var_S = 0\n    else:\n        var_S = 1.0 - (var_min / var_max)\n\n    var_V = var_max\n\n    return HSVColor(\n        var_H, var_S, var_V)", "response": "Converts from RGB to HSV."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef RGB_to_HSL(cobj, *args, **kwargs):\n    var_R = cobj.rgb_r\n    var_G = cobj.rgb_g\n    var_B = cobj.rgb_b\n\n    var_max = max(var_R, var_G, var_B)\n    var_min = min(var_R, var_G, var_B)\n\n    var_H = __RGB_to_Hue(var_R, var_G, var_B, var_min, var_max)\n    var_L = 0.5 * (var_max + var_min)\n\n    if var_max == var_min:\n        var_S = 0\n    elif var_L <= 0.5:\n        var_S = (var_max - var_min) / (2.0 * var_L)\n    else:\n        var_S = (var_max - var_min) / (2.0 - (2.0 * var_L))\n\n    return HSLColor(\n        var_H, var_S, var_L)", "response": "Convert from RGB to HSL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __Calc_HSL_to_RGB_Components(var_q, var_p, C):\n    if C < 0:\n        C += 1.0\n    if C > 1:\n        C -= 1.0\n\n    # Computing C of vector (Color R, Color G, Color B)\n    if C < (1.0 / 6.0):\n        return var_p + ((var_q - var_p) * 6.0 * C)\n    elif (1.0 / 6.0) <= C < 0.5:\n        return var_q\n    elif 0.5 <= C < (2.0 / 3.0):\n        return var_p + ((var_q - var_p) * 6.0 * ((2.0 / 3.0) - C))\n    else:\n        return var_p", "response": "This function calculates the HSL components of the color space."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef HSV_to_RGB(cobj, target_rgb, *args, **kwargs):\n    H = cobj.hsv_h\n    S = cobj.hsv_s\n    V = cobj.hsv_v\n\n    h_floored = int(math.floor(H))\n    h_sub_i = int(h_floored / 60) % 6\n    var_f = (H / 60.0) - (h_floored // 60)\n    var_p = V * (1.0 - S)\n    var_q = V * (1.0 - var_f * S)\n    var_t = V * (1.0 - (1.0 - var_f) * S)\n\n    if h_sub_i == 0:\n        rgb_r = V\n        rgb_g = var_t\n        rgb_b = var_p\n    elif h_sub_i == 1:\n        rgb_r = var_q\n        rgb_g = V\n        rgb_b = var_p\n    elif h_sub_i == 2:\n        rgb_r = var_p\n        rgb_g = V\n        rgb_b = var_t\n    elif h_sub_i == 3:\n        rgb_r = var_p\n        rgb_g = var_q\n        rgb_b = V\n    elif h_sub_i == 4:\n        rgb_r = var_t\n        rgb_g = var_p\n        rgb_b = V\n    elif h_sub_i == 5:\n        rgb_r = V\n        rgb_g = var_p\n        rgb_b = var_q\n    else:\n        raise ValueError(\"Unable to convert HSL->RGB due to value error.\")\n\n    # TODO: Investigate intent of following code block.\n    # In the event that they define an HSV color and want to convert it to\n    # a particular RGB space, let them override it here.\n    # if target_rgb is not None:\n    #     rgb_type = target_rgb\n    # else:\n    #     rgb_type = cobj.rgb_type\n\n    return target_rgb(rgb_r, rgb_g, rgb_b)", "response": "HSV to RGB conversion."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef RGB_to_CMY(cobj, *args, **kwargs):\n    cmy_c = 1.0 - cobj.rgb_r\n    cmy_m = 1.0 - cobj.rgb_g\n    cmy_y = 1.0 - cobj.rgb_b\n\n    return CMYColor(cmy_c, cmy_m, cmy_y)", "response": "RGB to CMY conversion."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting CMY to RGB via simple subtraction.", "response": "def CMY_to_RGB(cobj, target_rgb, *args, **kwargs):\n    \"\"\"\n    Converts CMY to RGB via simple subtraction.\n\n    NOTE: Returned values are in the range of 0-255.\n    \"\"\"\n    rgb_r = 1.0 - cobj.cmy_c\n    rgb_g = 1.0 - cobj.cmy_m\n    rgb_b = 1.0 - cobj.cmy_y\n\n    return target_rgb(rgb_r, rgb_g, rgb_b)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting from CMY to CMYK.", "response": "def CMY_to_CMYK(cobj, *args, **kwargs):\n    \"\"\"\n    Converts from CMY to CMYK.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0\n    \"\"\"\n    var_k = 1.0\n    if cobj.cmy_c < var_k:\n        var_k = cobj.cmy_c\n    if cobj.cmy_m < var_k:\n        var_k = cobj.cmy_m\n    if cobj.cmy_y < var_k:\n        var_k = cobj.cmy_y\n\n    if var_k == 1:\n        cmyk_c = 0.0\n        cmyk_m = 0.0\n        cmyk_y = 0.0\n    else:\n        cmyk_c = (cobj.cmy_c - var_k) / (1.0 - var_k)\n        cmyk_m = (cobj.cmy_m - var_k) / (1.0 - var_k)\n        cmyk_y = (cobj.cmy_y - var_k) / (1.0 - var_k)\n    cmyk_k = var_k\n\n    return CMYKColor(cmyk_c, cmyk_m, cmyk_y, cmyk_k)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts CMYK to CMY.", "response": "def CMYK_to_CMY(cobj, *args, **kwargs):\n    \"\"\"\n    Converts CMYK to CMY.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0\n    \"\"\"\n    cmy_c = cobj.cmyk_c * (1.0 - cobj.cmyk_k) + cobj.cmyk_k\n    cmy_m = cobj.cmyk_m * (1.0 - cobj.cmyk_k) + cobj.cmyk_k\n    cmy_y = cobj.cmyk_y * (1.0 - cobj.cmyk_k) + cobj.cmyk_k\n\n    return CMYColor(cmy_c, cmy_m, cmy_y)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert XYZ to IPT.", "response": "def XYZ_to_IPT(cobj, *args, **kwargs):\n    \"\"\"\n    Converts XYZ to IPT.\n\n    NOTE: XYZ values need to be adapted to 2 degree D65\n\n    Reference:\n    Fairchild, M. D. (2013). Color appearance models, 3rd Ed. (pp. 271-272). John Wiley & Sons.\n    \"\"\"\n    if cobj.illuminant != 'd65' or cobj.observer != '2':\n        raise ValueError('XYZColor for XYZ->IPT conversion needs to be D65 adapted.')\n    xyz_values = numpy.array(cobj.get_value_tuple())\n    lms_values = numpy.dot(\n        IPTColor.conversion_matrices['xyz_to_lms'],\n        xyz_values)\n\n    lms_prime = numpy.sign(lms_values) * numpy.abs(lms_values) ** 0.43\n\n    ipt_values = numpy.dot(\n        IPTColor.conversion_matrices['lms_to_ipt'],\n        lms_prime)\n    return IPTColor(*ipt_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts IPT to XYZ.", "response": "def IPT_to_XYZ(cobj, *args, **kwargs):\n    \"\"\"\n    Converts IPT to XYZ.\n    \"\"\"\n    ipt_values = numpy.array(cobj.get_value_tuple())\n    lms_values = numpy.dot(\n        numpy.linalg.inv(IPTColor.conversion_matrices['lms_to_ipt']),\n        ipt_values)\n\n    lms_prime = numpy.sign(lms_values) * numpy.abs(lms_values) ** (1 / 0.43)\n\n    xyz_values = numpy.dot(\n        numpy.linalg.inv(IPTColor.conversion_matrices['xyz_to_lms']),\n        lms_prime)\n    return XYZColor(*xyz_values, observer='2', illuminant='d65')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a color to the designated color space.", "response": "def convert_color(color, target_cs, through_rgb_type=sRGBColor,\n                  target_illuminant=None, *args, **kwargs):\n    \"\"\"\n    Converts the color to the designated color space.\n\n    :param color: A Color instance to convert.\n    :param target_cs: The Color class to convert to. Note that this is not\n        an instance, but a class.\n    :keyword BaseRGBColor through_rgb_type: If during your conversion between\n        your original and target color spaces you have to pass through RGB,\n        this determines which kind of RGB to use. For example, XYZ->HSL.\n        You probably don't need to specify this unless you have a special\n        usage case.\n    :type target_illuminant: None or str\n    :keyword target_illuminant: If during conversion from RGB to a reflective\n        color space you want to explicitly end up with a certain illuminant,\n        pass this here. Otherwise the RGB space's native illuminant\n        will be used.\n    :returns: An instance of the type passed in as ``target_cs``.\n    :raises: :py:exc:`colormath.color_exceptions.UndefinedConversionError`\n        if conversion between the two color spaces isn't possible.\n    \"\"\"\n    if isinstance(target_cs, str):\n        raise ValueError(\"target_cs parameter must be a Color object.\")\n    if not issubclass(target_cs, ColorBase):\n        raise ValueError(\"target_cs parameter must be a Color object.\")\n\n    conversions = _conversion_manager.get_conversion_path(color.__class__, target_cs)\n\n    logger.debug('Converting %s to %s', color, target_cs)\n    logger.debug(' @ Conversion path: %s', conversions)\n\n    # Start with original color in case we convert to the same color space.\n    new_color = color\n\n    if issubclass(target_cs, BaseRGBColor):\n        # If the target_cs is an RGB color space of some sort, then we\n        # have to set our through_rgb_type to make sure the conversion returns\n        # the expected RGB colorspace (instead of defaulting to sRGBColor).\n        through_rgb_type = target_cs\n\n    # We have to be careful to use the same RGB color space that created\n    # an object (if it was created by a conversion) in order to get correct\n    # results. For example, XYZ->HSL via Adobe RGB should default to Adobe\n    # RGB when taking that generated HSL object back to XYZ.\n    # noinspection PyProtectedMember\n    if through_rgb_type != sRGBColor:\n        # User overrides take priority over everything.\n        # noinspection PyProtectedMember\n        target_rgb = through_rgb_type\n    elif color._through_rgb_type:\n        # Otherwise, a value on the color object is the next best thing,\n        # when available.\n        # noinspection PyProtectedMember\n        target_rgb = color._through_rgb_type\n    else:\n        # We could collapse this into a single if statement above,\n        # but I think this reads better.\n        target_rgb = through_rgb_type\n\n    # Iterate through the list of functions for the conversion path, storing\n    # the results in a dictionary via update(). This way the user has access\n    # to all of the variables involved in the conversion.\n    for func in conversions:\n        # Execute the function in this conversion step and store the resulting\n        # Color object.\n        logger.debug(' * Conversion: %s passed to %s()',\n                     new_color.__class__.__name__, func)\n        logger.debug(' |->  in %s', new_color)\n\n        if func:\n            # This can be None if you try to convert a color to the color\n            # space that is already in. IE: XYZ->XYZ.\n            new_color = func(\n                new_color,\n                target_rgb=target_rgb,\n                target_illuminant=target_illuminant,\n                *args, **kwargs)\n\n        logger.debug(' |-< out %s', new_color)\n\n    # If this conversion had something other than the default sRGB color space\n    # requested,\n    if through_rgb_type != sRGBColor:\n        new_color._through_rgb_type = through_rgb_type\n\n    return new_color"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters a conversion function between two color spaces.", "response": "def add_type_conversion(self, start_type, target_type, conversion_function):\n        \"\"\"\n        Register a conversion function between two color spaces.\n        :param start_type: Starting color space.\n        :param target_type: Target color space.\n        :param conversion_function: Conversion function.\n        \"\"\"\n        self.registered_color_spaces.add(start_type)\n        self.registered_color_spaces.add(target_type)\n        logger.debug(\n            'Registered conversion from %s to %s', start_type, target_type)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _adaptation(self, f_l, l_a, xyz, xyz_w, xyz_b, xyz_p=None, p=None, helson_judd=False, discount_illuminant=True):\n        rgb = self.xyz_to_rgb(xyz)\n        logger.debug('RGB: {}'.format(rgb))\n        rgb_w = self.xyz_to_rgb(xyz_w)\n        logger.debug('RGB_W: {}'.format(rgb_w))\n        y_w = xyz_w[1]\n        y_b = xyz_b[1]\n\n        h_rgb = 3 * rgb_w / (rgb_w.sum())\n        logger.debug('H_RGB: {}'.format(h_rgb))\n\n        # Chromatic adaptation factors\n        if not discount_illuminant:\n            f_rgb = (1 + (l_a ** (1 / 3)) + h_rgb) / (1 + (l_a ** (1 / 3)) + (1 / h_rgb))\n        else:\n            f_rgb = numpy.ones(numpy.shape(h_rgb))\n        logger.debug('F_RGB: {}'.format(f_rgb))\n\n        # Adaptation factor\n        if helson_judd:\n            d_rgb = self._f_n((y_b / y_w) * f_l * f_rgb[1]) - self._f_n((y_b / y_w) * f_l * f_rgb)\n            assert d_rgb[1] == 0\n        else:\n            d_rgb = numpy.zeros(numpy.shape(f_rgb))\n        logger.debug('D_RGB: {}'.format(d_rgb))\n\n        # Cone bleaching factors\n        rgb_b = (10 ** 7) / ((10 ** 7) + 5 * l_a * (rgb_w / 100))\n        logger.debug('B_RGB: {}'.format(rgb_b))\n\n        if xyz_p is not None and p is not None:\n            logger.debug('Account for simultaneous chromatic contrast')\n            rgb_p = self.xyz_to_rgb(xyz_p)\n            rgb_w = self.adjust_white_for_scc(rgb_p, rgb_b, rgb_w, p)\n\n        # Adapt rgb using modified\n        rgb_a = 1 + rgb_b * (self._f_n(f_l * f_rgb * rgb / rgb_w) + d_rgb)\n        logger.debug('RGB_A: {}'.format(rgb_a))\n\n        return rgb_a", "response": "Internal function to compute the adaptation factor and the chromatic adaptation factors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadjust the white point for simultaneous chromatic contrast.", "response": "def adjust_white_for_scc(cls, rgb_p, rgb_b, rgb_w, p):\n        \"\"\"\n        Adjust the white point for simultaneous chromatic contrast.\n\n        :param rgb_p: Cone signals of proxima field.\n        :param rgb_b: Cone signals of background.\n        :param rgb_w: Cone signals of reference white.\n        :param p: Simultaneous contrast/assimilation parameter.\n        :return: Adjusted cone signals for reference white.\n        \"\"\"\n        p_rgb = rgb_p / rgb_b\n        rgb_w = rgb_w * (((1 - p) * p_rgb + (1 + p) / p_rgb) ** 0.5) / (((1 + p) * p_rgb + (1 - p) / p_rgb) ** 0.5)\n        return rgb_w"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the correlated color temperatures across the entire gamut of daylight and skylight chromaticities.", "response": "def _get_cct(x, y, z):\n        \"\"\"\n        Reference\n        Hernandez-Andres, J., Lee, R. L., & Romero, J. (1999).\n        Calculating correlated color temperatures across the entire gamut of daylight and skylight chromaticities.\n        Applied Optics, 38(27), 5703-5709.\n        \"\"\"\n        x_e = 0.3320\n        y_e = 0.1858\n\n        n = ((x / (x + z + z)) - x_e) / ((y / (x + z + z)) - y_e)\n\n        a_0 = -949.86315\n        a_1 = 6253.80338\n        a_2 = 28.70599\n        a_3 = 0.00004\n\n        t_1 = 0.92159\n        t_2 = 0.20039\n        t_3 = 0.07125\n\n        cct = a_0 + a_1 * numpy.exp(-n / t_1) + a_2 * numpy.exp(-n / t_2) + a_3 * numpy.exp(-n / t_3)\n        return cct"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the adaptation factor for a set of components.", "response": "def _compute_adaptation(self, xyz, xyz_w, f_l, d):\n        \"\"\"\n        Modified adaptation procedure incorporating simultaneous chromatic contrast from Hunt model.\n\n        :param xyz: Stimulus XYZ.\n        :param xyz_w: Reference white XYZ.\n        :param f_l: Luminance adaptation factor\n        :param d: Degree of adaptation.\n        :return: Tuple of adapted rgb and rgb_w arrays.\n        \"\"\"\n        # Transform input colors to cone responses\n        rgb = self._xyz_to_rgb(xyz)\n        logger.debug(\"RGB: {}\".format(rgb))\n\n        rgb_b = self._xyz_to_rgb(self._xyz_b)\n        rgb_w = self._xyz_to_rgb(xyz_w)\n        rgb_w = Hunt.adjust_white_for_scc(rgb, rgb_b, rgb_w, self._p)\n        logger.debug(\"RGB_W: {}\".format(rgb_w))\n\n        # Compute adapted tristimulus-responses\n        rgb_c = self._white_adaption(rgb, rgb_w, d)\n        logger.debug(\"RGB_C: {}\".format(rgb_c))\n        rgb_cw = self._white_adaption(rgb_w, rgb_w, d)\n        logger.debug(\"RGB_CW: {}\".format(rgb_cw))\n\n        # Convert adapted tristimulus-responses to Hunt-Pointer-Estevez fundamentals\n        rgb_p = self._compute_hunt_pointer_estevez_fundamentals(rgb_c)\n        logger.debug(\"RGB': {}\".format(rgb_p))\n        rgb_wp = self._compute_hunt_pointer_estevez_fundamentals(rgb_cw)\n        logger.debug(\"RGB'_W: {}\".format(rgb_wp))\n\n        # Compute post-adaptation non-linearities\n        rgb_ap = self._compute_nonlinearities(f_l, rgb_p)\n        rgb_awp = self._compute_nonlinearities(f_l, rgb_wp)\n\n        return rgb_ap, rgb_awp"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef errors(self):\n        if self.ok():\n            return {}\n\n        errors = self.content\n\n        if(not isinstance(errors, dict)):\n            errors = {\"error\": errors}  # convert to dict for consistency\n        elif('errors' in errors):\n            errors = errors['errors']\n\n        return errors", "response": "return error dict if no success"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, email, phone, country_code=1, send_install_link_via_sms=False):\n        data = {\n            \"user\": {\n                \"email\": email,\n                \"cellphone\": phone,\n                \"country_code\": country_code\n            },\n\n            'send_install_link_via_sms': send_install_link_via_sms\n        }\n\n        resp = self.post(\"/protected/json/users/new\", data)\n\n        return User(self, resp)", "response": "Creates a new user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verification_start(self, phone_number, country_code, via='sms',\n                           locale=None, code_length=4):\n        \"\"\"\n        :param string phone_number: stored in your databse or you provided while creating new user.\n        :param string country_code: stored in your databse or you provided while creating new user.\n        :param string via: verification method either sms or call\n        :param string locale: optional default none\n        :param number code_length: optional default 4\n        :return:\n        \"\"\"\n\n        if via != 'sms' and via != 'call':\n            raise AuthyFormatException(\"Invalid Via. Expected 'sms' or 'call'.\")\n\n        options = {\n            'phone_number': phone_number,\n            'country_code': country_code,\n            'via': via\n        }\n\n        if locale:\n            options['locale'] = locale\n\n        try:\n            cl = int(code_length)\n            if cl < 4 or cl > 10:\n                raise ValueError\n            options['code_length'] = cl\n        except ValueError:\n            raise AuthyFormatException(\n                \"Invalid code_length. Expected numeric value from 4-10.\")\n\n        resp = self.post(\"/protected/json/phones/verification/start\", options)\n        return Phone(self, resp)", "response": "Start verification of a phone number."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Phone object with the phone number and verification code.", "response": "def verification_check(self, phone_number, country_code, verification_code):\n        \"\"\"\n        :param phone_number:\n        :param country_code:\n        :param verification_code:\n        :return:\n        \"\"\"\n        options = {\n            'phone_number': phone_number,\n            'country_code': country_code,\n            'verification_code': verification_code\n        }\n        resp = self.get(\"/protected/json/phones/verification/check\", options)\n        return Phone(self, resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a request to OneTouch.", "response": "def send_request(self, user_id, message, seconds_to_expire=None, details={}, hidden_details={}, logos=[]):\n        \"\"\"\n        OneTouch verification request. Sends a request for Auth App. For more info https://www.twilio.com/docs/api/authy/authy-onetouch-api\n        :param string user_id: user_id User's authy id stored in your database\n        :param string message: Required, the message shown to the user when the approval request arrives.\n        :param number seconds_to_expire: Optional, defaults to 120 (two minutes).\n        :param dict details:  For example details['Requested by'] = 'MacBook Pro, Chrome'; it will be displayed on Authy app\n        :param dict hidden_details: Same usage as detail except this detail is not shown in Authy app\n        :param list logos: Contains the logos that will be shown to user. The logos parameter is expected to be an array of objects, each object with two fields: res (values are default,low,med,high) and url\n        :return OneTouchResponse: the server response Json Object\n        \"\"\"\n\n        self._validate_request(user_id, message)\n\n        data = {\n            \"message\": message[:MAX_STRING_SIZE],\n            \"seconds_to_expire\": seconds_to_expire,\n            \"details\": self.__clean_inputs(details),\n            'hidden_details': self.__clean_inputs(hidden_details),\n            'logos': self.clean_logos(logos)\n        }\n\n        request_url = \"/onetouch/json/users/{0}/approval_requests\".format(\n            user_id)\n        response = self.post(request_url, data)\n        return OneTouchResponse(self, response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean_logos(self, logos):\n        if not len(logos):\n            return logos  # Allow nil hash\n        if not isinstance(logos, list):\n            raise AuthyFormatException(\n                'Invalid logos list. Only res and url required')\n\n        temp_array = {}\n        clean_logos = []\n\n        for logo in logos:\n\n            if not isinstance(logo, dict):\n                raise AuthyFormatException('Invalid logo type')\n\n            for l in logo:\n                # We ignore any additional parameter on the logos, and truncate\n                # string size to the maximum allowed.\n                if l == 'res':\n                    temp_array['res'] = logo[l][:MAX_STRING_SIZE]\n                elif l == 'url':\n                    temp_array['url'] = logo[l][:MAX_STRING_SIZE]\n                else:\n                    raise AuthyFormatException(\n                        'Invalid logos list. Only res and url required')\n\n            clean_logos.append(temp_array)\n            temp_array = {}\n\n        return clean_logos", "response": "Validate logos input.\n        :param list logos:\n        :return list logos:"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the status of an ApprovalRequest.", "response": "def get_approval_status(self, uuid):\n        \"\"\"\n        OneTouch verification request. Sends a request for Auth App. For more info https://www.twilio.com/docs/api/authy/authy-onetouch-api\n        :param string uuid Required. The approval request ID. (Obtained from the response to an ApprovalRequest):\n        :return OneTouchResponse the server response Json Object:\n        \"\"\"\n        request_url = \"/onetouch/json/approval_requests/{0}\".format(uuid)\n        response = self.get(request_url)\n        return OneTouchResponse(self, response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_one_touch_signature(self, signature, nonce, method, url, params):\n        if not signature or not isinstance(signature, str):\n            raise AuthyFormatException(\n                \"Invalid signature - should not be empty. It is required\")\n\n        if not nonce:\n            raise AuthyFormatException(\n                \"Invalid nonce - should not be empty. It is required\")\n\n        if not method or not ('get' == method.lower() or 'post' == method.lower()):\n            raise AuthyFormatException(\n                \"Invalid method - should not be empty. It is required\")\n\n        if not params or not isinstance(params, dict):\n            raise AuthyFormatException(\n                \"Invalid params - should not be empty. It is required\")\n\n        query_params = self.__make_http_query(params)\n        # Sort and replace encoded  params in case-sensitive order\n        sorted_params = '&'.join(sorted(query_params.replace(\n            '/', '%2F').replace('%20', '+').split('&')))\n        sorted_params = re.sub(\"\\\\%5B([0-9])*\\\\%5D\", \"%5B%5D\", sorted_params)\n        sorted_params = re.sub(\"\\\\=None\", \"=\", sorted_params)\n        data = nonce + \"|\" + method + \"|\" + url + \"|\" + sorted_params\n        try:\n            calculated_signature = base64.b64encode(\n                hmac.new(self.api_key.encode(), data.encode(), hashlib.sha256).digest())\n            return calculated_signature.decode() == signature\n        except:\n            calculated_signature = base64.b64encode(\n                hmac.new(self.api_key, data, hashlib.sha256).digest())\n            return calculated_signature == signature", "response": "Function to validate the signature in OneTouch headers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to covert params into url encoded query string", "response": "def __make_http_query(self, params, topkey=''):\n        \"\"\"\n        Function to covert params into url encoded query string\n        :param dict params: Json string sent  by Authy.\n        :param string topkey: params key\n        :return string: url encoded Query.\n        \"\"\"\n        if len(params) == 0:\n            return \"\"\n        result = \"\"\n        # is a dictionary?\n        if type(params) is dict:\n            for key in params.keys():\n                newkey = quote(key)\n                if topkey != '':\n                    newkey = topkey + quote('[' + key + ']')\n                if type(params[key]) is dict:\n                    result += self.__make_http_query(params[key], newkey)\n                elif type(params[key]) is list:\n                    i = 0\n                    for val in params[key]:\n                        if type(val) is dict:\n                            result += self.__make_http_query(\n                                val, newkey + quote('['+str(i)+']'))\n                        else:\n                            result += newkey + \\\n                                quote('['+str(i)+']') + \"=\" + \\\n                                quote(str(val)) + \"&\"\n                        i = i + 1\n                # boolean should have special treatment as well\n                elif type(params[key]) is bool:\n                    result += newkey + \"=\" + \\\n                        quote(str(params[key]).lower()) + \"&\"\n                # assume string (integers and floats work well)\n                else:\n                    result += newkey + \"=\" + quote(str(params[key])) + \"&\"\n        # remove the last '&'\n        if (result) and (topkey == '') and (result[-1] == '&'):\n            result = result[:-1]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompile a jq script retuning a script object.", "response": "def compile(script, vars={}, library_paths=[]):\n    \"\"\"\n    Compile a jq script, retuning a script object.\n\n    library_paths is a list of strings that defines the module search path.\n    \"\"\"\n\n    return _pyjq.Script(script.encode('utf-8'), vars=vars,\n                        library_paths=library_paths)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransform value by script returning all results as list.", "response": "def apply(script, value=None, vars={}, url=None, opener=default_opener, library_paths=[]):\n    \"\"\"\n    Transform value by script, returning all results as list.\n    \"\"\"\n    return all(script, value, vars, url, opener, library_paths)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms object by jq script returning the first result.", "response": "def first(script, value=None, default=None, vars={}, url=None, opener=default_opener, library_paths=[]):\n    \"\"\"\n    Transform object by jq script, returning the first result.\n    Return default if result is empty.\n    \"\"\"\n    return compile(script, vars, library_paths).first(_get_value(value, url, opener), default)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef one(script, value=None, vars={}, url=None, opener=default_opener, library_paths=[]):\n    return compile(script, vars, library_paths).one(_get_value(value, url, opener))", "response": "Transform object by jq script returning the first result."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_mypypath() -> List[str]:\n\n    typeshed_root = None\n    count = 0\n    started = time.time()\n    for parent in itertools.chain(\n        # Look in current script's parents, useful for zipapps.\n        Path(__file__).parents,\n        # Look around site-packages, useful for virtualenvs.\n        Path(mypy.api.__file__).parents,\n        # Look in global paths, useful for globally installed.\n        Path(os.__file__).parents,\n    ):\n        count += 1\n        candidate = parent / 'lib' / 'mypy' / 'typeshed'\n        if candidate.is_dir():\n            typeshed_root = candidate\n            break\n\n        # Also check the non-installed path, useful for `setup.py develop`.\n        candidate = parent / 'typeshed'\n        if candidate.is_dir():\n            typeshed_root = candidate\n            break\n\n    LOG.debug(\n        'Checked %d paths in %.2fs looking for typeshed. Found %s',\n        count,\n        time.time() - started,\n        typeshed_root,\n    )\n\n    if not typeshed_root:\n        return []\n\n    stdlib_dirs = ('3.7', '3.6', '3.5', '3.4', '3.3', '3.2', '3', '2and3')\n    stdlib_stubs = [\n        typeshed_root / 'stdlib' / stdlib_dir\n        for stdlib_dir in stdlib_dirs\n    ]\n    third_party_dirs = ('3.7', '3.6', '3', '2and3')\n    third_party_stubs = [\n        typeshed_root / 'third_party' / tp_dir\n        for tp_dir in third_party_dirs\n    ]\n    return [\n        str(p) for p in stdlib_stubs + third_party_stubs\n    ]", "response": "Return MYPYPATH so that stubs have precedence over local sources."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a message to a channel", "response": "def send(channel, message, **kwargs):\n    \"\"\"\n    Site: http://www.pubnub.com/\n    API: https://www.mashape.com/pubnub/pubnub-network\n    Desc: real-time browser notifications\n\n    Installation and usage:\n    pip install -U pubnub\n    Tests for browser notification http://127.0.0.1:8000/browser_notification/\n    \"\"\"\n\n    pubnub = Pubnub(\n        publish_key=settings.PUBNUB_PUB_KEY,\n        subscribe_key=settings.PUBNUB_SUB_KEY,\n        secret_key=settings.PUBNUB_SEC_KEY,\n        ssl_on=kwargs.pop('ssl_on', False), **kwargs)\n    return pubnub.publish(channel=channel, message={\"text\": message})"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send(token, title, **kwargs):\n    headers = {\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n    }\n\n    data = {\n        \"user_credentials\": token,\n        \"notification[title]\": from_unicode(title),\n        \"notification[sound]\": \"notifier-2\"\n    }\n\n    for k, v in kwargs.items():\n        data['notification[%s]' % k] = from_unicode(v)\n\n    http = HTTPSConnection(kwargs.pop(\"api_url\", \"new.boxcar.io\"))\n    http.request(\n        \"POST\", \"/api/notifications\",\n        headers=headers,\n        body=urlencode(data))\n    response = http.getresponse()\n\n    if response.status != 201:\n        raise BoxcarError(response.reason)\n    return True", "response": "Send a notification to the system administrators"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a message to a specific channel.", "response": "def send(channel, message, **kwargs):\n    \"\"\"\n    Site: https://slack.com\n    API: https://api.slack.com\n    Desc: real-time messaging\n    \"\"\"\n    headers = {\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n    }\n\n    username = from_unicode(kwargs.pop(\"username\", settings.SLACK_USERNAME))\n    hook_url = from_unicode(kwargs.pop(\"hook_url\", settings.SLACK_HOOCK_URL))\n    channel = from_unicode(channel or settings.SLACK_CHANNEL)\n    emoji = from_unicode(kwargs.pop(\"emoji\", \"\"))\n    message = from_unicode(message)\n\n    data = {\n        \"channel\": channel,\n        \"username\": username,\n        \"text\": message,\n        \"icon_emoji\": emoji,\n    }\n\n    _data = kwargs.pop('data', None)\n    if _data is not None:\n        data.update(_data)\n\n    up = urlparse(hook_url)\n    http = HTTPSConnection(up.netloc)\n    http.request(\n        \"POST\", up.path,\n        headers=headers,\n        body=urlencode({\"payload\": dumps(data)}))\n    response = http.getresponse()\n\n    if response.status != 200:\n        raise SlackError(response.reason)\n\n    body = response.read()\n    if body != \"ok\":\n        raise SlackError(repr(body))\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends SMS to SMSAERO API", "response": "def send(sms_to, sms_body, **kwargs):\n    \"\"\"\n    Site: http://smsaero.ru/\n    API: http://smsaero.ru/api/\n    \"\"\"\n    headers = {\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n    }\n\n    kwargs.update({\n        'user': settings.SMSAERO_LOGIN,\n        'password': settings.SMSAERO_MD5_PASSWORD,\n        'from': kwargs.pop('sms_from', settings.SMSAERO_FROM),\n        'to': sms_to.replace('+', ''),\n        'text': from_unicode(sms_body),\n        'answer': 'json',\n    })\n\n    http = HTTPConnection(kwargs.pop(\"api_url\", \"gate.smsaero.ru\"))\n    http.request(\"GET\", \"/send/?\" + urlencode(kwargs), headers=headers)\n    response = http.getresponse()\n\n    if response.status != 200:\n        raise AeroSmsError(response.reason)\n\n    read = response.read().decode(response.headers.get_content_charset())\n    data = json.loads(read)\n\n    status = None\n    if 'result' in data:\n        status = data['result']\n\n    sms_id = None\n    if 'id' in data:\n        sms_id = data['id']\n\n    if sms_id and status == 'accepted':\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef email_list_to_email_dict(email_list):\n    if email_list is None:\n        return {}\n    result = {}\n    for value in email_list:\n        realname, address = email.utils.parseaddr(value)\n        result[address] = realname if realname and address else address\n    return result", "response": "Convert a list of email to a dict of email."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an email address to a list.", "response": "def email_address_to_list(email_address):\n    \"\"\"Convert an email address to a list.\"\"\"\n    realname, address = email.utils.parseaddr(email_address)\n    return (\n        [address, realname] if realname and address else\n        [email_address, email_address]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a transactional email using SendInBlue API.", "response": "def send(sender_instance):\n    \"\"\"Send a transactional email using SendInBlue API.\n\n    Site: https://www.sendinblue.com\n    API: https://apidocs.sendinblue.com/\n    \"\"\"\n    m = Mailin(\n        \"https://api.sendinblue.com/v2.0\",\n        sender_instance._kwargs.get(\"api_key\")\n    )\n    data = {\n        \"to\": email_list_to_email_dict(sender_instance._recipient_list),\n        \"cc\": email_list_to_email_dict(sender_instance._cc),\n        \"bcc\": email_list_to_email_dict(sender_instance._bcc),\n        \"from\": email_address_to_list(sender_instance._from_email),\n        \"subject\": sender_instance._subject,\n    }\n    if sender_instance._template.is_html:\n        data.update({\n            \"html\": sender_instance._message,\n            \"headers\": {\"Content-Type\": \"text/html; charset=utf-8\"}\n        })\n    else:\n        data.update({\"text\": sender_instance._message})\n    if \"attachments\" in sender_instance._kwargs:\n        data[\"attachment\"] = {}\n        for attachment in sender_instance._kwargs[\"attachments\"]:\n            data[\"attachment\"][attachment[0]] = base64.b64encode(attachment[1])\n    result = m.send_email(data)\n    if result[\"code\"] != \"success\":\n        raise SendInBlueError(result[\"message\"])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(sms_to, sms_body, **kwargs):\n    headers = {\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n        'Authorization': 'Basic %s' % b64encode(\n            \"%s:%s\" % (\n                settings.IQSMS_API_LOGIN, settings.IQSMS_API_PASSWORD\n            )).decode(\"ascii\")\n    }\n\n    kwargs.update({\n        'phone': sms_to,\n        'text': from_unicode(sms_body),\n        'sender': kwargs.pop('sms_from', settings.IQSMS_FROM)\n    })\n\n    http = HTTPConnection(kwargs.pop(\"api_url\", \"gate.iqsms.ru\"))\n    http.request(\"GET\", \"/send/?\" + urlencode(kwargs), headers=headers)\n    response = http.getresponse()\n\n    if response.status != 200:\n        raise IQSMSError(response.reason)\n\n    body = response.read().strip()\n    if '=accepted' not in body:\n        raise IQSMSError(body)\n\n    return int(body.split('=')[0])", "response": "Send SMS to another phone number."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend an alert to the parse. com API.", "response": "def send(device_id, description, **kwargs):\n    \"\"\"\n    Site: http://parse.com\n    API: https://www.parse.com/docs/push_guide#scheduled/REST\n    Desc: Best app for system administrators\n    \"\"\"\n    headers = {\n        \"X-Parse-Application-Id\": settings.PARSE_APP_ID,\n        \"X-Parse-REST-API-Key\": settings.PARSE_API_KEY,\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n        \"Content-type\": \"application/json\",\n    }\n\n    data = {\n        \"where\": {\n            \"user_id\": device_id,\n        },\n        \"data\": {\n            \"alert\": description,\n            \"title\": kwargs.pop(\"event\")\n        }\n    }\n\n    _data = kwargs.pop('data', None)\n    if _data is not None:\n        data.update(_data)\n\n    http = HTTPSConnection(kwargs.pop(\"api_url\", \"api.parse.com\"))\n    http.request(\n        \"POST\", \"/1/push\",\n        headers=headers,\n        body=dumps(data))\n    response = http.getresponse()\n\n    if response.status != 200:\n        raise ParseComError(response.reason)\n\n    body = loads(response.read())\n    if body['error']:\n        raise ParseComError(body['error'])\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a new entry to the system administrators", "response": "def send(api_key, description, **kwargs):\n    \"\"\"\n    Site: http://prowlapp.com\n    API: http://prowlapp.com/api.php\n    Desc: Best app for system administrators\n    \"\"\"\n    headers = {\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n        \"Content-type\": \"application/x-www-form-urlencoded\"\n    }\n\n    application = from_unicode(kwargs.pop(\"app\", settings.PROWL_APP), 256)\n    event = from_unicode(kwargs.pop(\"event\", 'Alert'), 1024)\n    description = from_unicode(description, 10000)\n\n    data = {\n        \"apikey\": api_key,\n        \"application\": application,\n        \"event\": event,\n        \"description\": description,\n        \"priority\": kwargs.pop(\"priority\", 1)\n    }\n\n    provider_key = kwargs.pop(\"providerkey\", None)\n    url = kwargs.pop('url', None)\n\n    if provider_key is not None:\n        data[\"providerkey\"] = provider_key\n\n    if url is not None:\n        data[\"url\"] = url[0:512]\n\n    http = HTTPSConnection(kwargs.pop(\"api_url\", \"api.prowlapp.com\"))\n    http.request(\n        \"POST\", \"/publicapi/add\",\n        headers=headers,\n        body=urlencode(data))\n    response = http.getresponse()\n\n    if response.status != 200:\n        raise ProwlError(response.reason)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a message to the APKs.", "response": "def send(token_hex, message, **kwargs):\n    \"\"\"\n    Site: https://apple.com\n    API: https://developer.apple.com\n    Desc: iOS notifications\n    \"\"\"\n    is_enhanced = kwargs.pop('is_enhanced', False)\n    identifier = kwargs.pop('identifier', 0)\n    expiry = kwargs.pop('expiry', 0)\n\n    alert = {\n        \"title\": kwargs.pop(\"event\"),\n        \"body\": message,\n        \"action\": kwargs.pop(\n            'apns_action', defaults.APNS_PROVIDER_DEFAULT_ACTION)\n    }\n\n    data = {\n        \"aps\": {\n            'alert': alert,\n            'content-available': kwargs.pop('content_available', 0) and 1\n        }\n    }\n    data['aps'].update(kwargs)\n    payload = dumps(data, separators=(',', ':'))\n\n    token = a2b_hex(token_hex)\n    if is_enhanced is True:\n        fmt = '!BIIH32sH%ds' % len(payload)\n        expiry = expiry and time() + expiry\n        notification = pack(\n            fmt, 1, identifier, expiry,\n            32, token, len(payload), payload)\n    else:\n        token_length_bin = pack('>H', len(token))\n        payload_length_bin = pack('>H', len(payload))\n        zero_byte = bytes('\\0', 'utf-8') if PY3 is True else '\\0'\n        payload = bytes(payload, 'utf-8') if PY3 is True else payload\n        notification = (\n            zero_byte + token_length_bin + token +\n            payload_length_bin + payload)\n\n    sock = socket(AF_INET, SOCK_STREAM)\n    sock.settimeout(3)\n    sock.connect((settings.APNS_GW_HOST, settings.APNS_GW_PORT))\n    ssl = wrap_socket(\n        sock, settings.APNS_KEY_FILE,\n        settings.APNS_CERT_FILE,\n        do_handshake_on_connect=False)\n\n    result = ssl.write(notification)\n\n    sock.close()\n    ssl.close()\n\n    if not result:\n        raise APNsError\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(ch, message, **kwargs):\n    params = {\n        'type': kwargs.pop('req_type', 'self'),\n        'key': settings.PUSHALL_API_KEYS[ch]['key'],\n        'id': settings.PUSHALL_API_KEYS[ch]['id'],\n        'title': kwargs.pop(\n            \"title\", settings.PUSHALL_API_KEYS[ch].get('title') or \"\"),\n        'text': message,\n        'priority': kwargs.pop(\n            \"priority\", settings.PUSHALL_API_KEYS[ch].get('priority') or \"0\"),\n    }\n    if kwargs:\n        params.update(**kwargs)\n\n    response = urlopen(\n        Request('https://pushall.ru/api.php'),\n        urlencode(params),\n        timeout=10\n    )\n\n    if response.code != 200:\n        raise PushAllError(response.read())\n\n    json = loads(response.read())\n    if json.get('error'):\n        raise PushAllError(json.get('error'))\n\n    return True", "response": "Send a message to the specified channel"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send(to, message, **kwargs):\n\n    available_kwargs_keys = [\n        'parse_mode',\n        'disable_web_page_preview',\n        'disable_notification',\n        'reply_to_message_id',\n        'reply_markup'\n    ]\n\n    available_kwargs = {\n        k: v for k, v in kwargs.iteritems() if k in available_kwargs_keys\n    }\n\n    bot = telepot.Bot(settings.TELEGRAM_BOT_TOKEN)\n    return bot.sendMessage(to, message, **available_kwargs)", "response": "Send a message to a user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send(reg_id, message, **kwargs):\n\n    subscription_info = kwargs.pop('subscription_info')\n\n    payload = {\n        \"title\": kwargs.pop(\"event\"),\n        \"body\": message,\n        \"url\": kwargs.pop(\"push_url\", None)\n    }\n    payload.update(kwargs)\n\n    wp = WebPusher(subscription_info)\n    response = wp.send(\n        dumps(payload), gcm_key=settings.GCM_KEY,\n        ttl=kwargs.pop(\"ttl\", 60))\n\n    if not response.ok or (\n            response.text and loads(response.text).get(\"failure\") > 0):\n        raise GCMError(response.text)\n    return True", "response": "Send a message to a new node in the registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a message to the specified token.", "response": "def send(token_hex, message, **kwargs):\n    \"\"\"\n    Site: https://apple.com\n    API: https://developer.apple.com\n    Desc: iOS notifications\n\n    Installation and usage:\n    pip install hyper\n    \"\"\"\n\n    priority = kwargs.pop('priority', 10)\n    topic = kwargs.pop('topic', None)\n\n    alert = {\n        \"title\": kwargs.pop(\"event\"),\n        \"body\": message,\n        \"action\": kwargs.pop(\n            'apns_action', defaults.APNS_PROVIDER_DEFAULT_ACTION)\n    }\n\n    data = {\n        \"aps\": {\n            'alert': alert,\n            'content-available': kwargs.pop('content_available', 0) and 1\n        }\n    }\n    data['aps'].update(kwargs)\n    payload = dumps(data, separators=(',', ':'))\n\n    headers = {\n        'apns-priority': priority\n    }\n    if topic is not None:\n        headers['apns-topic'] = topic\n\n    ssl_context = init_context()\n    ssl_context.load_cert_chain(settings.APNS_CERT_FILE)\n    connection = HTTP20Connection(\n        settings.APNS_GW_HOST, settings.APNS_GW_PORT, ssl_context=ssl_context)\n\n    stream_id = connection.request(\n        'POST', '/3/device/{}'.format(token_hex), payload, headers)\n    response = connection.get_response(stream_id)\n    if response.status != 200:\n        raise APNsError(response.read())\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending SMS to a specific SMS to a specific SMS to the specified SMS.", "response": "def send(sms_to, sms_body, **kwargs):\n    \"\"\"\n    Site: https://www.twilio.com/\n    API: https://www.twilio.com/docs/api/rest/sending-messages\n    \"\"\"\n    headers = {\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n        'Authorization': 'Basic %s' % b64encode(\n            \"%s:%s\" % (\n                settings.TWILIO_ACCOUNT_SID, settings.TWILIO_AUTH_TOKEN\n            )).decode(\"ascii\")\n\n    }\n\n    kwargs.update({\n        'From': kwargs.pop('sms_from', settings.TWILIO_FROM),\n        'To': sms_to,\n        'Body': from_unicode(sms_body)\n    })\n\n    http = HTTPSConnection(kwargs.pop(\"api_url\", \"api.twilio.com\"))\n    http.request(\n        \"POST\",\n        \"/2010-04-01/Accounts/%s/Messages.json\" % settings.TWILIO_ACCOUNT_SID,\n        headers=headers,\n        body=urlencode(kwargs))\n\n    response = http.getresponse()\n    if response.status != 201:\n        raise TwilioSmsError(response.reason)\n\n    return loads(response.read()).get('sid')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a message to a pushover site.", "response": "def send(user, message, **kwargs):\n    \"\"\"\n    Site: https://pushover.net/\n    API: https://pushover.net/api\n    Desc: real-time notifications\n    \"\"\"\n    headers = {\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n    }\n\n    title = from_unicode(kwargs.pop(\"title\", settings.PUSHOVER_APP))\n    message = from_unicode(message)\n\n    data = {\n        \"token\": settings.PUSHOVER_TOKEN,\n        \"user\": user,\n        \"message\": message,\n        \"title\": title,\n        \"priority\": kwargs.pop(\"priority\", 0)\n    }\n\n    _data = kwargs.pop('data', None)\n    if _data is not None:\n        data.update(_data)\n\n    http = HTTPSConnection(kwargs.pop(\"api_url\", \"api.pushover.net\"))\n    http.request(\n        \"POST\", \"/1/messages.json\",\n        headers=headers,\n        body=urlencode(data))\n    response = http.getresponse()\n\n    if response.status != 200:\n        raise PushOverError(response.reason)\n\n    body = loads(response.read())\n    if body.get('status') != 1:\n        raise PushOverError(repr(body))\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a message to the user.", "response": "def send(user, message, **kwargs):\n    \"\"\"\n    Site: https://developers.google.com\n    API: https://developers.google.com/cloud-messaging/\n    Desc: Android notifications\n    \"\"\"\n\n    headers = {\n        \"Content-type\": \"application/json\",\n        \"Authorization\": \"key=\" + kwargs.pop(\"gcm_key\", settings.GCM_KEY)\n    }\n\n    hook_url = 'https://android.googleapis.com/gcm/send'\n\n    data = {\n        \"registration_ids\": [user],\n        \"data\": {\n            \"title\": kwargs.pop(\"event\"),\n            'message': message,\n        }\n    }\n    data['data'].update(kwargs)\n\n    up = urlparse(hook_url)\n    http = HTTPSConnection(up.netloc)\n    http.request(\n        \"POST\", up.path,\n        headers=headers,\n        body=dumps(data))\n    response = http.getresponse()\n\n    if response.status != 200:\n        raise GCMError(response.reason)\n\n    body = response.read()\n    if loads(body).get(\"failure\") > 0:\n        raise GCMError(repr(body))\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rotate_lv(*, device, size, debug, forward):\n\n    import augeas\n    class Augeas(augeas.Augeas):\n        def get_int(self, key):\n            return int(self.get(key + '/int'))\n\n        def set_int(self, key, val):\n            return self.set(key + '/int', '%d' % val)\n\n        def incr(self, key, by=1):\n            orig = self.get_int(key)\n            self.set_int(key, orig + by)\n\n        def decr(self, key):\n            self.incr(key, by=-1)\n\n    lv_info = subprocess.check_output(\n        'lvm lvs --noheadings --rows --units=b --nosuffix '\n        '-o vg_name,vg_uuid,lv_name,lv_uuid,lv_attr --'.split()\n        + [device.devpath], universal_newlines=True).splitlines()\n    vgname, vg_uuid, lvname, lv_uuid, lv_attr = (fi.lstrip() for fi in lv_info)\n    active = lv_attr[4] == 'a'\n\n    # Make sure the volume isn't in use by unmapping it\n    quiet_call(\n        ['lvm', 'lvchange', '-an', '--', '{}/{}'.format(vgname, lvname)])\n\n    with tempfile.TemporaryDirectory(suffix='.blocks') as tdname:\n        vgcfgname = tdname + '/vg.cfg'\n        print('Loading LVM metadata... ', end='', flush=True)\n        quiet_call(\n            ['lvm', 'vgcfgbackup', '--file', vgcfgname, '--', vgname])\n        aug = Augeas(\n            loadpath=pkg_resources.resource_filename('blocks', 'augeas'),\n            root='/dev/null',\n            flags=augeas.Augeas.NO_MODL_AUTOLOAD | augeas.Augeas.SAVE_NEWFILE)\n        vgcfg = open(vgcfgname)\n        vgcfg_orig = vgcfg.read()\n        aug.set('/raw/vgcfg', vgcfg_orig)\n\n        aug.text_store('LVM.lns', '/raw/vgcfg', '/vg')\n        print('ok')\n\n        # There is no easy way to quote for XPath, so whitelist\n        assert all(ch in ASCII_ALNUM_WHITELIST for ch in vgname), vgname\n        assert all(ch in ASCII_ALNUM_WHITELIST for ch in lvname), lvname\n\n        aug.defvar('vg', '/vg/{}/dict'.format(vgname))\n        assert aug.get('$vg/id/str') == vg_uuid\n        aug.defvar('lv', '$vg/logical_volumes/dict/{}/dict'.format(lvname))\n        assert aug.get('$lv/id/str') == lv_uuid\n\n        rotate_aug(aug, forward, size)\n        aug.text_retrieve('LVM.lns', '/raw/vgcfg', '/vg', '/raw/vgcfg.new')\n        open(vgcfgname + '.new', 'w').write(aug.get('/raw/vgcfg.new'))\n        rotate_aug(aug, not forward, size)\n        aug.text_retrieve('LVM.lns', '/raw/vgcfg', '/vg', '/raw/vgcfg.backagain')\n        open(vgcfgname + '.backagain', 'w').write(aug.get('/raw/vgcfg.backagain'))\n\n        if debug:\n            print('CHECK STABILITY')\n            subprocess.call(\n                ['git', '--no-pager', 'diff', '--no-index', '--patience', '--color-words',\n                 '--', vgcfgname, vgcfgname + '.backagain'])\n            if forward:\n                print('CHECK CORRECTNESS (forward)')\n            else:\n                print('CHECK CORRECTNESS (backward)')\n            subprocess.call(\n                ['git', '--no-pager', 'diff', '--no-index', '--patience', '--color-words',\n                 '--', vgcfgname, vgcfgname + '.new'])\n\n        if forward:\n            print(\n                'Rotating the second extent to be the first... ',\n                end='', flush=True)\n        else:\n            print(\n                'Rotating the last extent to be the first... ',\n                end='', flush=True)\n        quiet_call(\n            ['lvm', 'vgcfgrestore', '--file', vgcfgname + '.new', '--', vgname])\n        # Make sure LVM updates the mapping, this is pretty critical\n        quiet_call(\n            ['lvm', 'lvchange', '--refresh', '--', '{}/{}'.format(vgname, lvname)])\n        if active:\n            quiet_call(\n                ['lvm', 'lvchange', '-ay', '--', '{}/{}'.format(vgname, lvname)])\n        print('ok')", "response": "Rotate a logical volume by a single PE."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _mpv_coax_proptype(value, proptype=str):\n    if type(value) is bytes:\n        return value;\n    elif type(value) is bool:\n        return b'yes' if value else b'no'\n    elif proptype in (str, int, float):\n        return str(proptype(value)).encode('utf-8')\n    else:\n        raise TypeError('Cannot coax value of type {} into property type {}'.format(type(value), proptype))", "response": "Intelligently coax the given python value into something that can be understood as a proptype property."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake a list of python objects and make a MPV string node array from it.", "response": "def _make_node_str_list(l):\n    \"\"\"Take a list of python objects and make a MPV string node array from it.\n\n    As an example, the python list ``l = [ \"foo\", 23, false ]`` will result in the following MPV node object::\n\n        struct mpv_node {\n            .format = MPV_NODE_ARRAY,\n            .u.list = *(struct mpv_node_array){\n                .num = len(l),\n                .keys = NULL,\n                .values = struct mpv_node[len(l)] {\n                    { .format = MPV_NODE_STRING, .u.string = l[0] },\n                    { .format = MPV_NODE_STRING, .u.string = l[1] },\n                    ...\n                }\n            }\n        }\n    \"\"\"\n    char_ps = [ c_char_p(_mpv_coax_proptype(e, str)) for e in l ]\n    node_list = MpvNodeList(\n        num=len(l),\n        keys=None,\n        values=( MpvNode * len(l))( *[ MpvNode(\n                format=MpvFormat.STRING,\n                val=MpvNodeUnion(string=p))\n            for p in char_ps ]))\n    node = MpvNode(\n        format=MpvFormat.NODE_ARRAY,\n        val=MpvNodeUnion(list=pointer(node_list)))\n    return char_ps, node_list, node, cast(pointer(node), c_void_p)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait until cond evaluates to a truthy value on the named property.", "response": "def wait_for_property(self, name, cond=lambda val: val, level_sensitive=True):\n        \"\"\"Waits until ``cond`` evaluates to a truthy value on the named property. This can be used to wait for\n        properties such as ``idle_active`` indicating the player is done with regular playback and just idling around\n        \"\"\"\n        sema = threading.Semaphore(value=0)\n        def observer(name, val):\n            if cond(val):\n                sema.release()\n        self.observe_property(name, observer)\n        if not level_sensitive or not cond(getattr(self, name.replace('-', '_'))):\n            sema.acquire()\n        self.unobserve_property(name, observer)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef terminate(self):\n        self.handle, handle = None, self.handle\n        if threading.current_thread() is self._event_thread:\n            # Handle special case to allow event handle to be detached.\n            # This is necessary since otherwise the event thread would deadlock itself.\n            grim_reaper = threading.Thread(target=lambda: _mpv_terminate_destroy(handle))\n            grim_reaper.start()\n        else:\n            _mpv_terminate_destroy(handle)\n            if self._event_thread:\n                self._event_thread.join()", "response": "Properly terminates this player instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a raw command.", "response": "def command(self, name, *args):\n        \"\"\"Execute a raw command.\"\"\"\n        args = [name.encode('utf-8')] + [ (arg if type(arg) is bytes else str(arg).encode('utf-8'))\n                for arg in args if arg is not None ] + [None]\n        _mpv_command(self.handle, (c_char_p*len(args))(*args))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef seek(self, amount, reference=\"relative\", precision=\"default-precise\"):\n        self.command('seek', amount, reference, precision)", "response": "seek amount in the current mpv file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef screenshot_to_file(self, filename, includes='subtitles'):\n        self.command('screenshot_to_file', filename.encode(fs_enc), includes)", "response": "Mapped mpv screenshot_to_file command"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef screenshot_raw(self, includes='subtitles'):\n        from PIL import Image\n        res = self.node_command('screenshot-raw', includes)\n        if res['format'] != 'bgr0':\n            raise ValueError('Screenshot in unknown format \"{}\". Currently, only bgr0 is supported.'\n                    .format(res['format']))\n        img = Image.frombytes('RGBA', (res['w'], res['h']), res['data'])\n        b,g,r,a = img.split()\n        return Image.merge('RGB', (r,g,b))", "response": "Returns a PIL Image object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a file into the current mpv instance.", "response": "def loadfile(self, filename, mode='replace', **options):\n        \"\"\"Mapped mpv loadfile command, see man mpv(1).\"\"\"\n        self.command('loadfile', filename.encode(fs_enc), mode, MPV._encode_options(options))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef loadlist(self, playlist, mode='replace'):\n        self.command('loadlist', playlist.encode(fs_enc), mode)", "response": "Mapped mpv loadlist command, see man mpv(1)."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef overlay_add(self, overlay_id, x, y, file_or_fd, offset, fmt, w, h, stride):\n        self.command('overlay_add', overlay_id, x, y, file_or_fd, offset, fmt, w, h, stride)", "response": "add a new overlay"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering an observer on the named property.", "response": "def observe_property(self, name, handler):\n        \"\"\"Register an observer on the named property. An observer is a function that is called with the new property\n        value every time the property's value is changed. The basic function signature is ``fun(property_name,\n        new_value)`` with new_value being the decoded property value as a python object. This function can be used as a\n        function decorator if no handler is given.\n\n        To unregister the observer, call either of ``mpv.unobserve_property(name, handler)``,\n        ``mpv.unobserve_all_properties(handler)`` or the handler's ``unregister_mpv_properties`` attribute::\n\n            @player.observe_property('volume')\n            def my_handler(new_volume, *):\n                print(\"It's loud!\", volume)\n\n            my_handler.unregister_mpv_properties()\n        \"\"\"\n        self._property_handlers[name].append(handler)\n        _mpv_observe_property(self._event_handle, hash(name)&0xffffffffffffffff, name.encode('utf-8'), MpvFormat.NODE)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef property_observer(self, name):\n        def wrapper(fun):\n            self.observe_property(name, fun)\n            fun.unobserve_mpv_properties = lambda: self.unobserve_property(name, fun)\n            return fun\n        return wrapper", "response": "Decorator to register a property observer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unobserve_all_properties(self, handler):\n        for name in self._property_handlers:\n            self.unobserve_property(name, handler)", "response": "Unregister a property observer from all observed properties."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction decorator to register a blanket event callback for the given event types. Event types can be given as str (e.g. 'start-file'), integer or MpvEventID object. WARNING: Due to the way this is filtering events, this decorator cannot be chained with itself. To unregister the event callback, call its ``unregister_mpv_events`` function:: player = mpv.MPV() @player.event_callback('shutdown') def my_handler(event): print('It ded.') my_handler.unregister_mpv_events()", "response": "def event_callback(self, *event_types):\n        \"\"\"Function decorator to register a blanket event callback for the given event types. Event types can be given\n        as str (e.g.  'start-file'), integer or MpvEventID object.\n\n        WARNING: Due to the way this is filtering events, this decorator cannot be chained with itself.\n\n        To unregister the event callback, call its ``unregister_mpv_events`` function::\n\n            player = mpv.MPV()\n            @player.event_callback('shutdown')\n            def my_handler(event):\n                print('It ded.')\n\n            my_handler.unregister_mpv_events()\n        \"\"\"\n        def register(callback):\n            types = [MpvEventID.from_str(t) if isinstance(t, str) else t for t in event_types] or MpvEventID.ANY\n            @wraps(callback)\n            def wrapper(event, *args, **kwargs):\n                if event['event_id'] in types:\n                    callback(event, *args, **kwargs)\n            self._event_callbacks.append(wrapper)\n            wrapper.unregister_mpv_events = partial(self.unregister_event_callback, wrapper)\n            return wrapper\n        return register"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction decorator to register a simplified key binding. The callback is called whenever the key given is *pressed*. To unregister the callback function, you can call its ``unregister_mpv_key_bindings`` attribute:: player = mpv.MPV() @player.on_key_press('Q') def binding(): print('blep') binding.unregister_mpv_key_bindings() WARNING: For a single keydef only a single callback/command can be registered at the same time. If you register a binding multiple times older bindings will be overwritten and there is a possibility of references leaking. So don't do that. The BIG FAT WARNING regarding untrusted keydefs from the key_binding method applies here as well.", "response": "def on_key_press(self, keydef, mode='force'):\n        \"\"\"Function decorator to register a simplified key binding. The callback is called whenever the key given is\n        *pressed*.\n\n        To unregister the callback function, you can call its ``unregister_mpv_key_bindings`` attribute::\n\n            player = mpv.MPV()\n            @player.on_key_press('Q')\n            def binding():\n                print('blep')\n\n            binding.unregister_mpv_key_bindings()\n\n        WARNING: For a single keydef only a single callback/command can be registered at the same time. If you register\n        a binding multiple times older bindings will be overwritten and there is a possibility of references leaking. So\n        don't do that.\n\n        The BIG FAT WARNING regarding untrusted keydefs from the key_binding method applies here as well.\n        \"\"\"\n        def register(fun):\n            @self.key_binding(keydef, mode)\n            @wraps(fun)\n            def wrapper(state='p-', name=None):\n                if state[0] in ('d', 'p'):\n                    fun()\n            return wrapper\n        return register"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction decorator to register a key binding.", "response": "def key_binding(self, keydef, mode='force'):\n        \"\"\"Function decorator to register a low-level key binding.\n\n        The callback function signature is ``fun(key_state, key_name)`` where ``key_state`` is either ``'U'`` for \"key\n        up\" or ``'D'`` for \"key down\".\n\n        The keydef format is: ``[Shift+][Ctrl+][Alt+][Meta+]<key>`` where ``<key>`` is either the literal character the\n        key produces (ASCII or Unicode character), or a symbolic name (as printed by ``mpv --input-keylist``).\n\n        To unregister the callback function, you can call its ``unregister_mpv_key_bindings`` attribute::\n\n            player = mpv.MPV()\n            @player.key_binding('Q')\n            def binding(state, name):\n                print('blep')\n\n            binding.unregister_mpv_key_bindings()\n\n        WARNING: For a single keydef only a single callback/command can be registered at the same time. If you register\n        a binding multiple times older bindings will be overwritten and there is a possibility of references leaking. So\n        don't do that.\n\n        BIG FAT WARNING: mpv's key binding mechanism is pretty powerful.  This means, you essentially get arbitrary code\n        exectution through key bindings. This interface makes some limited effort to sanitize the keydef given in the\n        first parameter, but YOU SHOULD NOT RELY ON THIS IN FOR SECURITY. If your input comes from config files, this is\n        completely fine--but, if you are about to pass untrusted input into this parameter, better double-check whether\n        this is secure in your case.\n        \"\"\"\n        def register(fun):\n            fun.mpv_key_bindings = getattr(fun, 'mpv_key_bindings', []) + [keydef]\n            def unregister_all():\n                for keydef in fun.mpv_key_bindings:\n                    self.unregister_key_binding(keydef)\n            fun.unregister_mpv_key_bindings = unregister_all\n\n            self.register_key_binding(keydef, fun, mode)\n            return fun\n        return register"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a key binding.", "response": "def register_key_binding(self, keydef, callback_or_cmd, mode='force'):\n        \"\"\"Register a key binding. This takes an mpv keydef and either a string containing a mpv command or a python\n        callback function.  See ``MPV.key_binding`` for details.\n        \"\"\"\n        if not re.match(r'(Shift+)?(Ctrl+)?(Alt+)?(Meta+)?(.|\\w+)', keydef):\n            raise ValueError('Invalid keydef. Expected format: [Shift+][Ctrl+][Alt+][Meta+]<key>\\n'\n                    '<key> is either the literal character the key produces (ASCII or Unicode character), or a '\n                    'symbolic name (as printed by --input-keylist')\n        binding_name = MPV._binding_name(keydef)\n        if callable(callback_or_cmd):\n            self._key_binding_handlers[binding_name] = callback_or_cmd\n            self.register_message_handler('key-binding', self._handle_key_binding_message)\n            self.command('define-section',\n                    binding_name, '{} script-binding py_event_handler/{}'.format(keydef, binding_name), mode)\n        elif isinstance(callback_or_cmd, str):\n            self.command('define-section', binding_name, '{} {}'.format(keydef, callback_or_cmd), mode)\n        else:\n            raise TypeError('register_key_binding expects either an str with an mpv command or a python callable.')\n        self.command('enable-section', binding_name, 'allow-hide-cursor+allow-vo-dragging')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unregister_key_binding(self, keydef):\n        binding_name = MPV._binding_name(keydef)\n        self.command('disable-section', binding_name)\n        self.command('define-section', binding_name, '')\n        if binding_name in self._key_binding_handlers:\n            del self._key_binding_handlers[binding_name]\n            if not self._key_binding_handlers:\n                self.unregister_message_handler('key-binding')", "response": "Unregister a key binding by keydef."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninspect the request and adds the Access - Control - Allow - Origin HTTP header.", "response": "def _process_origin(self, req, resp, origin):\n        \"\"\"Inspects the request and adds the Access-Control-Allow-Origin\n        header if the requested origin is allowed.\n\n        Returns:\n            ``True`` if the header was added and the requested origin\n            is allowed, ``False`` if the origin is not allowed and the\n            header has not been added.\n        \"\"\"\n        if self._cors_config['allow_all_origins']:\n            if self.supports_credentials:\n                self._set_allow_origin(resp, origin)\n            else:\n                self._set_allow_origin(resp, '*')\n            return True\n\n        if origin in self._cors_config['allow_origins_list']:\n            self._set_allow_origin(resp, origin)\n            return True\n\n        regex = self._cors_config['allow_origins_regex']\n        if regex is not None:\n            if regex.match(origin):\n                self._set_allow_origin(resp, origin)\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the Access - Control - Allow - Headers header to the response.", "response": "def _process_allow_headers(self, req, resp, requested_headers):\n        \"\"\"Adds the Access-Control-Allow-Headers header to the response,\n        using the cors settings to determine which headers are allowed.\n\n        Returns:\n            True if all the headers the client requested are allowed.\n            False if some or none of the headers the client requested are allowed.\n        \"\"\"\n        if not requested_headers:\n            return True\n        elif self._cors_config['allow_all_headers']:\n            self._set_allowed_headers(resp, requested_headers)\n            return True\n\n        approved_headers = []\n        for header in requested_headers:\n            if header.lower() in self._cors_config['allow_headers_list']:\n                approved_headers.append(header)\n            elif self._cors_config.get('allow_headers_regex'):\n                if self._cors_config['allow_headers_regex'].match(header):\n                    approved_headers.append(header)\n\n        if len(approved_headers) == len(requested_headers):\n            self._set_allowed_headers(resp, approved_headers)\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the Access - Control - Allow - Methods header to the response and returns True if the request is allowed.", "response": "def _process_methods(self, req, resp, resource):\n        \"\"\"Adds the Access-Control-Allow-Methods header to the response,\n        using the cors settings to determine which methods are allowed.\n        \"\"\"\n        requested_method = self._get_requested_method(req)\n        if not requested_method:\n            return False\n\n        if self._cors_config['allow_all_methods']:\n            allowed_methods = self._get_resource_methods(resource)\n            self._set_allowed_methods(resp, allowed_methods)\n            if requested_method in allowed_methods:\n                return True\n        elif requested_method in self._cors_config['allow_methods_list']:\n            resource_methods = self._get_resource_methods(resource)\n            # Only list methods as allowed if they exist\n            # on the resource AND are in the allowed_methods_list\n            allowed_methods = [\n                method for method in resource_methods\n                if method in self._cors_config['allow_methods_list']\n            ]\n            self._set_allowed_methods(resp, allowed_methods)\n            if requested_method in allowed_methods:\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_credentials(self, req, resp, origin):\n        if self._cors_config['allow_credentials_all_origins']:\n            self._set_allow_credentials(resp)\n            return True\n\n        if origin in self._cors_config['allow_credentials_origins_list']:\n            self._set_allow_credentials(resp)\n            return True\n\n        credentials_regex = self._cors_config['allow_credentials_origins_regex']\n        if credentials_regex:\n            if credentials_regex.match(origin):\n                self._set_allow_credentials(resp)\n                return True\n\n        return False", "response": "Adds the Access - Control - Allow - Credentials to the response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_messages(self, email_messages):\n        if not email_messages:\n            return\n\n        sent_message_count = 0\n\n        for email_message in email_messages:\n            if self._send(email_message):\n                sent_message_count += 1\n        return sent_message_count", "response": "Sends one or more Django EmailMessage objects and returns the number of email messages sent."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _send(self, email_message):\n        pre_send.send(self.__class__, message=email_message)\n\n        if not email_message.recipients():\n            return False\n\n        from_email = sanitize_address(email_message.from_email,\n                                      email_message.encoding)\n        recipients = [sanitize_address(addr, email_message.encoding)\n                      for addr in email_message.recipients()]\n        message = email_message.message().as_bytes(linesep='\\r\\n')\n\n        try:\n            result = self.conn.send_raw_email(\n                Source=from_email,\n                Destinations=recipients,\n                RawMessage={\n                    'Data': message\n                }\n            )\n            message_id = result['MessageId']\n            post_send.send(\n                self.__class__,\n                message=email_message,\n                message_id=message_id\n            )\n        except ClientError:\n            if not self.fail_silently:\n                raise\n            return False\n        return True", "response": "Sends an individual message via the SES HTTP API."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef serve(request, path, document_root=None, show_indexes=False, default=''):\n\n    # Clean up given path to only allow serving files below document_root.\n    path = posixpath.normpath(unquote(path))\n    path = path.lstrip('/')\n    newpath = ''\n    for part in path.split('/'):\n        if not part:\n            # Strip empty path components.\n            continue\n        drive, part = os.path.splitdrive(part)\n        head, part = os.path.split(part)\n        if part in (os.curdir, os.pardir):\n            # Strip '.' and '..' in path.\n            continue\n        newpath = os.path.join(newpath, part).replace('\\\\', '/')\n    if newpath and path != newpath:\n        return HttpResponseRedirect(newpath)\n    fullpath = os.path.join(document_root, newpath)\n    if os.path.isdir(fullpath) and default:\n        defaultpath = os.path.join(fullpath, default)\n        if os.path.exists(defaultpath):\n            fullpath = defaultpath\n    if os.path.isdir(fullpath):\n        if show_indexes:\n            return directory_index(newpath, fullpath)\n        raise Http404(\"Directory indexes are not allowed here.\")\n    if not os.path.exists(fullpath):\n        raise Http404('\"%s\" does not exist' % fullpath)\n    # Respect the If-Modified-Since header.\n    statobj = os.stat(fullpath)\n    mimetype = mimetypes.guess_type(fullpath)[0] or 'application/octet-stream'\n    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\n                              statobj[stat.ST_MTIME], statobj[stat.ST_SIZE]):\n        if django.VERSION > (1, 6):\n            return HttpResponseNotModified(content_type=mimetype)\n        else:\n            return HttpResponseNotModified(mimetype=mimetype)\n    contents = open(fullpath, 'rb').read()\n    if django.VERSION > (1, 6):\n        response = HttpResponse(contents, content_type=mimetype)\n    else:\n        response = HttpResponse(contents, mimetype=mimetype)\n    response[\"Last-Modified\"] = http_date(statobj[stat.ST_MTIME])\n    response[\"Content-Length\"] = len(contents)\n    return response", "response": "Serve static files below a given point in the directory structure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if the item was modified since the user last downloaded it.", "response": "def was_modified_since(header=None, mtime=0, size=0):\n    \"\"\"\n    Was something modified since the user last downloaded it?\n    header\n      This is the value of the If-Modified-Since header.  If this is None,\n      I'll just return True.\n    mtime\n      This is the modification time of the item we're talking about.\n    size\n      This is the size of the item we're talking about.\n    \"\"\"\n    try:\n        if header is None:\n            raise ValueError\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header,\n                           re.IGNORECASE)\n        header_mtime = parse_http_date(matches.group(1))\n        header_len = matches.group(3)\n        if header_len and int(header_len) != size:\n            raise ValueError\n        if int(mtime) > header_mtime:\n            raise ValueError\n    except (AttributeError, ValueError, OverflowError):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the URL for the detail page.", "response": "def get_url(self, obj):\n        \"\"\"\n        The URL at which the detail page should appear.\n        \"\"\"\n        if not hasattr(obj, 'get_absolute_url') or not obj.get_absolute_url():\n            raise ImproperlyConfigured(\"No URL configured. You must either \\\nset a ``get_absolute_url`` method on the %s model or override the %s view's \\\n``get_url`` method\" % (obj.__class__.__name__, self.__class__.__name__))\n        return obj.get_absolute_url()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unbuild_object(self, obj):\n        logger.debug(\"Unbuilding %s\" % obj)\n        target_path = os.path.split(self.get_build_path(obj))[0]\n        if self.fs.exists(target_path):\n            logger.debug(\"Removing {}\".format(target_path))\n            self.fs.removetree(target_path)", "response": "Removes the object from the filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unpublish_object(content_type_pk, obj_pk):\n    ct = ContentType.objects.get_for_id(content_type_pk)\n    obj = ct.get_object_for_this_type(pk=obj_pk)\n    try:\n        # Unbuild the object\n        logger.info(\"unpublish_object task has received %s\" % obj)\n        obj.unbuild()\n        # Run the `publish` management command unless the\n        # ALLOW_BAKERY_AUTO_PUBLISHING variable is explictly set to False.\n        if not getattr(settings, 'ALLOW_BAKERY_AUTO_PUBLISHING', True):\n            logger.info(\"Not running publish command because \\\nALLOW_BAKERY_AUTO_PUBLISHING is False\")\n        else:\n            management.call_command(\"publish\")\n    except Exception:\n        # Log the error if this crashes\n        logger.error(\"Task Error: unpublish_object\", exc_info=True)", "response": "Unpublishes a bakery object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prep_directory(self, target_dir):\n        dirname = path.dirname(target_dir)\n        if dirname:\n            dirname = path.join(settings.BUILD_DIR, dirname)\n            if not self.fs.exists(dirname):\n                logger.debug(\"Creating directory at {}{}\".format(self.fs_name, dirname))\n                self.fs.makedirs(dirname)", "response": "Pre - creates a new directory for the given target_dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_file(self, target_path, html):\n        logger.debug(\"Building to {}{}\".format(self.fs_name, target_path))\n        with self.fs.open(smart_text(target_path), 'wb') as outfile:\n            outfile.write(six.binary_type(html))\n            outfile.close()", "response": "Writes out the provided HTML to the provided path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_gzippable(self, path):\n        # First check if gzipping is allowed by the global setting\n        if not getattr(settings, 'BAKERY_GZIP', False):\n            return False\n        # Then check if the content type of this particular file is gzippable\n        whitelist = getattr(\n            settings,\n            'GZIP_CONTENT_TYPES',\n            DEFAULT_GZIP_CONTENT_TYPES\n        )\n        return mimetypes.guess_type(path)[0] in whitelist", "response": "Returns a boolean indicating if the provided file path is a candidate\n        for gzipping."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gzip_file(self, target_path, html):\n        logger.debug(\"Gzipping to {}{}\".format(self.fs_name, target_path))\n\n        # Write GZIP data to an in-memory buffer\n        data_buffer = six.BytesIO()\n        kwargs = dict(\n            filename=path.basename(target_path),\n            mode='wb',\n            fileobj=data_buffer\n        )\n        if float(sys.version[:3]) >= 2.7:\n            kwargs['mtime'] = 0\n        with gzip.GzipFile(**kwargs) as f:\n            f.write(six.binary_type(html))\n\n        # Write that buffer out to the filesystem\n        with self.fs.open(smart_text(target_path), 'wb') as outfile:\n            outfile.write(data_buffer.getvalue())\n            outfile.close()", "response": "Zips up the provided HTML as a companion for the provided path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the URL redirect to. Keyword arguments from the URL pattern match generating the redirect request are passed as kwargs to this method.", "response": "def get_redirect_url(self, *args, **kwargs):\n        \"\"\"\n        Return the URL redirect to. Keyword arguments from the\n        URL pattern match generating the redirect request\n        are provided as kwargs to this method.\n        \"\"\"\n        if self.url:\n            url = self.url % kwargs\n        elif self.pattern_name:\n            try:\n                url = reverse(self.pattern_name, args=args, kwargs=kwargs)\n            except NoReverseMatch:\n                return None\n        else:\n            return None\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build(self):\n        for detail_view in self.detail_views:\n            view = self._get_view(detail_view)\n            view().build_object(self)\n        self._build_extra()\n        self._build_related()", "response": "Builds the object with the attributes of this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate through the views pointed to by self.detail_views, runs unbuild_object with `self`, and calls _build_extra() and _build_related().", "response": "def unbuild(self):\n        \"\"\"\n        Iterates through the views pointed to by self.detail_views, runs\n        unbuild_object with `self`, and calls _build_extra()\n        and _build_related().\n        \"\"\"\n        for detail_view in self.detail_views:\n            view = self._get_view(detail_view)\n            view().unbuild_object(self)\n        self._unbuild_extra()\n        # _build_related again to kill the object from RSS etc.\n        self._build_related()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self, *args, **kwargs):\n        from bakery import tasks\n        from django.contrib.contenttypes.models import ContentType\n        # if obj.save(publish=False) has been passed, we skip everything.\n        if not kwargs.pop('publish', True):\n            super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n        # Otherwise, for the standard obj.save(), here we go...\n        else:\n            # First figure out if the record is an addition, or an edit of\n            # a preexisting record.\n            try:\n                preexisting = self.__class__.objects.get(pk=self.pk)\n            except self.__class__.DoesNotExist:\n                preexisting = None\n            # If this is an addition...\n            if not preexisting:\n                # We will publish if that's the boolean\n                if self.get_publication_status():\n                    action = 'publish'\n                # Otherwise we will do nothing do nothing\n                else:\n                    action = None\n            # If this is an edit...\n            else:\n                # If it's being unpublished...\n                if not self.get_publication_status() and \\\n                        preexisting.get_publication_status():\n                    action = 'unpublish'\n                # If it's being published...\n                elif self.get_publication_status():\n                    action = 'publish'\n                # If it's remaining unpublished...\n                else:\n                    action = None\n            # Now, no matter what, save it normally inside of a dedicated\n            # database transaction so that we are sure that the save will\n            # be complete before we trigger any task\n            with transaction.atomic():\n                super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n            # Finally, depending on the action, fire off a task\n            ct = ContentType.objects.get_for_model(self.__class__)\n            if action == 'publish':\n                tasks.publish_object.delay(ct.pk, self.pk)\n            elif action == 'unpublish':\n                tasks.unpublish_object.delay(ct.pk, self.pk)", "response": "A custom save that publishes or unpublishes the object where appropriate."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes the object from the database.", "response": "def delete(self, *args, **kwargs):\n        \"\"\"\n        Triggers a task that will unpublish the object after it is deleted.\n\n        Save with keyword argument obj.delete(unpublish=False) to skip it.\n        \"\"\"\n        from bakery import tasks\n        from django.contrib.contenttypes.models import ContentType\n        # if obj.save(unpublish=False) has been passed, we skip the task.\n        unpublish = kwargs.pop('unpublish', True)\n        # Delete it from the database\n        super(AutoPublishingBuildableModel, self).delete(*args, **kwargs)\n        if unpublish:\n            ct = ContentType.objects.get_for_model(self.__class__)\n            tasks.unpublish_object.delay(ct.pk, self.pk)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling the build process.", "response": "def handle(self, *args, **options):\n        \"\"\"\n        Making it happen.\n        \"\"\"\n        logger.info(\"Build started\")\n\n        # Set options\n        self.set_options(*args, **options)\n\n        # Get the build directory ready\n        if not options.get(\"keep_build_dir\"):\n            self.init_build_dir()\n\n        # Build up static files\n        if not options.get(\"skip_static\"):\n            self.build_static()\n\n        # Build the media directory\n        if not options.get(\"skip_media\"):\n            self.build_media()\n\n        # Build views\n        self.build_views()\n\n        # Close out\n        logger.info(\"Build finished\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_options(self, *args, **options):\n        self.verbosity = int(options.get('verbosity', 1))\n\n        # Figure out what build directory to use\n        if options.get(\"build_dir\"):\n            self.build_dir = options.get(\"build_dir\")\n            settings.BUILD_DIR = self.build_dir\n        else:\n            if not hasattr(settings, 'BUILD_DIR'):\n                raise CommandError(self.build_unconfig_msg)\n            self.build_dir = settings.BUILD_DIR\n\n        # Get the datatypes right so fs will be happy\n        self.build_dir = smart_text(self.build_dir)\n        self.static_root = smart_text(settings.STATIC_ROOT)\n        self.media_root = smart_text(settings.MEDIA_ROOT)\n\n        # Connect the BUILD_DIR with our filesystem backend\n        self.app = apps.get_app_config(\"bakery\")\n        self.fs = self.app.filesystem\n        self.fs_name = self.app.filesystem_name\n\n        # If the build dir doesn't exist make it\n        if not self.fs.exists(self.build_dir):\n            self.fs.makedirs(self.build_dir)\n\n        # Figure out what views we'll be using\n        if options.get('view_list'):\n            self.view_list = options['view_list']\n        else:\n            if not hasattr(settings, 'BAKERY_VIEWS'):\n                raise CommandError(self.views_unconfig_msg)\n            self.view_list = settings.BAKERY_VIEWS\n\n        # Are we pooling?\n        self.pooling = options.get('pooling')", "response": "Set the options for this command."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_build_dir(self):\n        # Destroy the build directory, if it exists\n        logger.debug(\"Initializing %s\" % self.build_dir)\n        if self.verbosity > 1:\n            self.stdout.write(\"Initializing build directory\")\n        if self.fs.exists(self.build_dir):\n            self.fs.removetree(self.build_dir)\n        # Then recreate it from scratch\n        self.fs.makedirs(self.build_dir)", "response": "Initialize the build directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds the static files directory and robots. txt and favicon. ico files.", "response": "def build_static(self, *args, **options):\n        \"\"\"\n        Builds the static files directory as well as robots.txt and favicon.ico\n        \"\"\"\n        logger.debug(\"Building static directory\")\n        if self.verbosity > 1:\n            self.stdout.write(\"Building static directory\")\n        management.call_command(\n            \"collectstatic\",\n            interactive=False,\n            verbosity=0\n        )\n\n        # Set the target directory inside the filesystem.\n        target_dir = path.join(\n            self.build_dir,\n            settings.STATIC_URL.lstrip('/')\n        )\n        target_dir = smart_text(target_dir)\n\n        if os.path.exists(self.static_root) and settings.STATIC_URL:\n            if getattr(settings, 'BAKERY_GZIP', False):\n                self.copytree_and_gzip(self.static_root, target_dir)\n            # if gzip isn't enabled, just copy the tree straight over\n            else:\n                logger.debug(\"Copying {}{} to {}{}\".format(\"osfs://\", self.static_root, self.fs_name, target_dir))\n                copy.copy_dir(\"osfs:///\", self.static_root, self.fs, target_dir)\n\n        # If they exist in the static directory, copy the robots.txt\n        # and favicon.ico files down to the root so they will work\n        # on the live website.\n        robots_src = path.join(target_dir, 'robots.txt')\n        if self.fs.exists(robots_src):\n            robots_target = path.join(self.build_dir, 'robots.txt')\n            logger.debug(\"Copying {}{} to {}{}\".format(self.fs_name, robots_src, self.fs_name, robots_target))\n            self.fs.copy(robots_src, robots_target)\n\n        favicon_src = path.join(target_dir, 'favicon.ico')\n        if self.fs.exists(favicon_src):\n            favicon_target = path.join(self.build_dir, 'favicon.ico')\n            logger.debug(\"Copying {}{} to {}{}\".format(self.fs_name, favicon_src, self.fs_name, favicon_target))\n            self.fs.copy(favicon_src, favicon_target)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_media(self):\n        logger.debug(\"Building media directory\")\n        if self.verbosity > 1:\n            self.stdout.write(\"Building media directory\")\n        if os.path.exists(self.media_root) and settings.MEDIA_URL:\n            target_dir = path.join(self.fs_name, self.build_dir, settings.MEDIA_URL.lstrip('/'))\n            logger.debug(\"Copying {}{} to {}{}\".format(\"osfs://\", self.media_root, self.fs_name, target_dir))\n            copy.copy_dir(\"osfs:///\", smart_text(self.media_root), self.fs, smart_text(target_dir))", "response": "Build the media files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_views(self):\n        # Then loop through and run them all\n        for view_str in self.view_list:\n            logger.debug(\"Building %s\" % view_str)\n            if self.verbosity > 1:\n                self.stdout.write(\"Building %s\" % view_str)\n            view = get_callable(view_str)\n            self.get_view_instance(view).build_method()", "response": "Build all the views that are in the list of views."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copytree_and_gzip(self, source_dir, target_dir):\n        # Figure out what we're building...\n        build_list = []\n        # Walk through the source directory...\n        for (dirpath, dirnames, filenames) in os.walk(source_dir):\n            for f in filenames:\n                # Figure out what is going where\n                source_path = os.path.join(dirpath, f)\n                rel_path = os.path.relpath(dirpath, source_dir)\n                target_path = os.path.join(target_dir, rel_path, f)\n                # Add it to our list to build\n                build_list.append((source_path, target_path))\n\n        logger.debug(\"Gzipping {} files\".format(len(build_list)))\n\n        # Build em all\n        if not getattr(self, 'pooling', False):\n            [self.copyfile_and_gzip(*u) for u in build_list]\n        else:\n            cpu_count = multiprocessing.cpu_count()\n            logger.debug(\"Pooling build on {} CPUs\".format(cpu_count))\n            pool = ThreadPool(processes=cpu_count)\n            pool.map(self.pooled_copyfile_and_gzip, build_list)", "response": "Copies the provided source directory to the provided target directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copyfile_and_gzip(self, source_path, target_path):\n        # And then where we want to copy it to.\n        target_dir = path.dirname(target_path)\n        if not self.fs.exists(target_dir):\n            try:\n                self.fs.makedirs(target_dir)\n            except OSError:\n                pass\n\n        # determine the mimetype of the file\n        guess = mimetypes.guess_type(source_path)\n        content_type = guess[0]\n        encoding = guess[1]\n\n        # If it isn't a file want to gzip...\n        if content_type not in self.gzip_file_match:\n            # just copy it to the target.\n            logger.debug(\"Copying {}{} to {}{} because its filetype isn't on the whitelist\".format(\n                \"osfs://\",\n                source_path,\n                self.fs_name,\n                target_path\n            ))\n            copy.copy_file(\"osfs:///\", smart_text(source_path), self.fs, smart_text(target_path))\n\n        # # if the file is already gzipped\n        elif encoding == 'gzip':\n            logger.debug(\"Copying {}{} to {}{} because it's already gzipped\".format(\n                \"osfs://\",\n                source_path,\n                self.fs_name,\n                target_path\n            ))\n            copy.copy_file(\"osfs:///\", smart_text(source_path), self.fs, smart_text(target_path))\n\n        # If it is one we want to gzip...\n        else:\n            # ... let the world know ...\n            logger.debug(\"Gzipping {}{} to {}{}\".format(\n                \"osfs://\",\n                source_path,\n                self.fs_name,\n                target_path\n            ))\n            # Open up the source file from the OS\n            with open(source_path, 'rb') as source_file:\n                # Write GZIP data to an in-memory buffer\n                data_buffer = six.BytesIO()\n                kwargs = dict(\n                    filename=path.basename(target_path),\n                    mode='wb',\n                    fileobj=data_buffer\n                )\n                if float(sys.version[:3]) >= 2.7:\n                    kwargs['mtime'] = 0\n                with gzip.GzipFile(**kwargs) as f:\n                    f.write(six.binary_type(source_file.read()))\n\n                # Write that buffer out to the filesystem\n                with self.fs.open(smart_text(target_path), 'wb') as outfile:\n                    outfile.write(data_buffer.getvalue())\n                    outfile.close()", "response": "Copy the file from source_path to target_path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle(self, *args, **options):\n        # Counts and such we can use to keep tabs on this as they progress\n        self.uploaded_files = 0\n        self.uploaded_file_list = []\n        self.deleted_files = 0\n        self.deleted_file_list = []\n        self.start_time = time.time()\n\n        # Configure all the options we're going to use\n        self.set_options(options)\n\n        # Initialize the boto connection\n        logger.debug(\"Connecting to s3\")\n        if self.verbosity > 2:\n            self.stdout.write(\"Connecting to s3\")\n        self.s3_client, self.s3_resource = get_s3_client()\n\n        # Grab our bucket\n        logger.debug(\"Retriving bucket {}\".format(self.aws_bucket_name))\n        if self.verbosity > 2:\n            self.stdout.write(\"Retriving bucket {}\".format(self.aws_bucket_name))\n        self.bucket = self.s3_resource.Bucket(self.aws_bucket_name)\n\n        # Get a list of all keys in our s3 bucket ...\n        # ...nunless you're this is case where we're blindly pushing\n        if self.force_publish and self.no_delete:\n            self.blind_upload = True\n            logger.debug(\"Skipping object retrieval. We won't need to because we're blinding uploading everything.\")\n            self.s3_obj_dict = {}\n        else:\n            self.blind_upload = False\n            logger.debug(\"Retrieving objects now published in bucket\")\n            if self.verbosity > 2:\n                self.stdout.write(\"Retrieving objects now published in bucket\")\n            self.s3_obj_dict = {}\n            self.s3_obj_dict = self.get_bucket_file_list()\n\n        # Get a list of all the local files in our build directory\n        logger.debug(\"Retrieving files built locally\")\n        if self.verbosity > 2:\n            self.stdout.write(\"Retrieving files built locally\")\n        self.local_file_list = self.get_local_file_list()\n\n        # Sync local files with s3 bucket\n        logger.debug(\"Syncing local files with bucket\")\n        if self.verbosity > 2:\n            self.stdout.write(\"Syncing local files with bucket\")\n        self.sync_with_s3()\n\n        # Delete anything that's left in our keys dict\n        if not self.dry_run and not self.no_delete:\n            self.deleted_file_list = list(self.s3_obj_dict.keys())\n            self.deleted_files = len(self.deleted_file_list)\n            if self.deleted_files:\n                logger.debug(\"Deleting %s keys\" % self.deleted_files)\n                if self.verbosity > 0:\n                    self.stdout.write(\"Deleting %s keys\" % self.deleted_files)\n                self.batch_delete_s3_objects(\n                    self.deleted_file_list,\n                    self.aws_bucket_name\n                )\n\n        # Run any post publish hooks on the views\n        if not hasattr(settings, 'BAKERY_VIEWS'):\n            raise CommandError(self.views_unconfig_msg)\n        for view_str in settings.BAKERY_VIEWS:\n            view = get_callable(view_str)()\n            if hasattr(view, 'post_publish'):\n                getattr(view, 'post_publish')(self.bucket)\n\n        # We're finished, print the final output\n        elapsed_time = time.time() - self.start_time\n        msg = \"Publish completed, %d uploaded and %d deleted files in %.2f seconds\" % (\n            self.uploaded_files,\n            self.deleted_files,\n            elapsed_time\n        )\n        logger.info(msg)\n        if self.verbosity > 0:\n            self.stdout.write(msg)\n\n        if self.dry_run:\n            logger.info(\"Publish executed with the --dry-run option. No content was changed on S3.\")\n            if self.verbosity > 0:\n                self.stdout.write(\"Publish executed with the --dry-run option. No content was changed on S3.\")", "response": "Sync files in the build directory to a specified S3 bucket"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconfiguring all the many options we need to make this happen.", "response": "def set_options(self, options):\n        \"\"\"\n        Configure all the many options we'll need to make this happen.\n        \"\"\"\n        self.verbosity = int(options.get('verbosity'))\n\n        # Will we be gzipping?\n        self.gzip = getattr(settings, 'BAKERY_GZIP', False)\n\n        # And if so what content types will we be gzipping?\n        self.gzip_content_types = getattr(\n            settings,\n            'GZIP_CONTENT_TYPES',\n            DEFAULT_GZIP_CONTENT_TYPES\n        )\n\n        # What ACL (i.e. security permissions) will be giving the files on S3?\n        self.acl = getattr(settings, 'DEFAULT_ACL', self.DEFAULT_ACL)\n\n        # Should we set cache-control headers?\n        self.cache_control = getattr(settings, 'BAKERY_CACHE_CONTROL', {})\n\n        # If the user specifies a build directory...\n        if options.get('build_dir'):\n            # ... validate that it is good.\n            if not os.path.exists(options.get('build_dir')):\n                raise CommandError(self.build_missing_msg)\n            # Go ahead and use it\n            self.build_dir = options.get(\"build_dir\")\n        # If the user does not specify a build dir...\n        else:\n            # Check if it is set in settings.py\n            if not hasattr(settings, 'BUILD_DIR'):\n                raise CommandError(self.build_unconfig_msg)\n            # Then make sure it actually exists\n            if not os.path.exists(settings.BUILD_DIR):\n                raise CommandError(self.build_missing_msg)\n            # Go ahead and use it\n            self.build_dir = settings.BUILD_DIR\n\n        # If the user provides a bucket name, use that.\n        if options.get(\"aws_bucket_name\"):\n            self.aws_bucket_name = options.get(\"aws_bucket_name\")\n        else:\n            # Otherwise try to find it the settings\n            if not hasattr(settings, 'AWS_BUCKET_NAME'):\n                raise CommandError(self.bucket_unconfig_msg)\n            self.aws_bucket_name = settings.AWS_BUCKET_NAME\n\n        # The bucket prefix, if it exists\n        self.aws_bucket_prefix = options.get(\"aws_bucket_prefix\")\n\n        # If the user sets the --force option\n        if options.get('force'):\n            self.force_publish = True\n        else:\n            self.force_publish = False\n\n        # set the --dry-run option\n        if options.get('dry_run'):\n            self.dry_run = True\n            if self.verbosity > 0:\n                logger.info(\"Executing with the --dry-run option set.\")\n        else:\n            self.dry_run = False\n\n        self.no_delete = options.get('no_delete')\n        self.no_pooling = options.get('no_pooling')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_bucket_file_list(self):\n        logger.debug(\"Retrieving bucket object list\")\n\n        paginator = self.s3_client.get_paginator('list_objects')\n        options = {\n            'Bucket': self.aws_bucket_name\n        }\n        if self.aws_bucket_prefix:\n            logger.debug(\"Adding prefix {} to bucket list as a filter\".format(self.aws_bucket_prefix))\n            options['Prefix'] = self.aws_bucket_prefix\n        page_iterator = paginator.paginate(**options)\n\n        obj_dict = {}\n        for page in page_iterator:\n            obj_dict.update(get_bucket_page(page))\n\n        return obj_dict", "response": "Returns a dictionary of all objects in given bucket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwalking the local build directory and create a list of relative and absolute paths to files.", "response": "def get_local_file_list(self):\n        \"\"\"\n        Walk the local build directory and create a list of relative and\n        absolute paths to files.\n        \"\"\"\n        file_list = []\n        for (dirpath, dirnames, filenames) in os.walk(self.build_dir):\n            for fname in filenames:\n                # relative path, to sync with the S3 key\n                local_key = os.path.join(\n                    os.path.relpath(dirpath, self.build_dir),\n                    fname\n                )\n                if local_key.startswith('./'):\n                    local_key = local_key[2:]\n                file_list.append(local_key)\n        return file_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsynchronizing the local files with the S3 bucket.", "response": "def sync_with_s3(self):\n        \"\"\"\n        Walk through our self.local_files list, and match them with the list\n        of keys in the S3 bucket.\n        \"\"\"\n        # Create a list to put all the files we're going to update\n        self.update_list = []\n\n        # Figure out which files need to be updated and upload all these files\n        logger.debug(\"Comparing {} local files with {} bucket files\".format(\n            len(self.local_file_list),\n            len(self.s3_obj_dict.keys())\n        ))\n        if self.no_pooling:\n            [self.compare_local_file(f) for f in self.local_file_list]\n        else:\n            cpu_count = multiprocessing.cpu_count()\n            logger.debug(\"Pooling local file comparison on {} CPUs\".format(cpu_count))\n            pool = ThreadPool(processes=cpu_count)\n            pool.map(self.compare_local_file, self.local_file_list)\n\n        logger.debug(\"Uploading {} new or updated files to bucket\".format(len(self.update_list)))\n        if self.no_pooling:\n            [self.upload_to_s3(*u) for u in self.update_list]\n        else:\n            logger.debug(\"Pooling s3 uploads on {} CPUs\".format(cpu_count))\n            pool = ThreadPool(processes=cpu_count)\n            pool.map(self.pooled_upload_to_s3, self.update_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_md5(self, filename):\n        with open(filename, 'rb') as f:\n            m = hashlib.md5(f.read())\n        return m.hexdigest()", "response": "Returns the md5 checksum of the provided file name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_multipart_md5(self, filename, chunk_size=8 * 1024 * 1024):\n        # Loop through the file contents ...\n        md5s = []\n        with open(filename, 'rb') as fp:\n            while True:\n                # Break it into chunks\n                data = fp.read(chunk_size)\n                # Finish when there are no more\n                if not data:\n                    break\n                # Generate a md5 hash for each chunk\n                md5s.append(hashlib.md5(data))\n\n        # Combine the chunks\n        digests = b\"\".join(m.digest() for m in md5s)\n\n        # Generate a new hash using them\n        new_md5 = hashlib.md5(digests)\n\n        # Create the ETag as Amazon will\n        new_etag = '\"%s-%s\"' % (new_md5.hexdigest(), len(md5s))\n\n        # Trim it down and pass it back for comparison\n        return new_etag.strip('\"').strip(\"'\")", "response": "Returns the md5 checksum of the provided file name after breaking it into chunks and then generates a new md5 hash using the md5 checksum of the file name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompares a local version of a file with what s already published.", "response": "def compare_local_file(self, file_key):\n        \"\"\"\n        Compares a local version of a file with what's already published.\n\n        If an update is needed, the file's key is added self.update_list.\n        \"\"\"\n        # Where is the file?\n        file_path = os.path.join(self.build_dir, file_key)\n\n        # If we're in force_publish mode just add it\n        if self.force_publish:\n            self.update_list.append((file_key, file_path))\n            # And quit now\n            return\n\n        # Does it exist in our s3 object list?\n        if file_key in self.s3_obj_dict:\n\n            # Get the md5 stored in Amazon's header\n            s3_md5 = self.s3_obj_dict[file_key].get('ETag').strip('\"').strip(\"'\")\n\n            # If there is a multipart ETag on S3, compare that to our local file after its chunked up.\n            # We are presuming this file was uploaded in multiple parts.\n            if \"-\" in s3_md5:\n                local_md5 = self.get_multipart_md5(file_path)\n            # Other, do it straight for the whole file\n            else:\n                local_md5 = self.get_md5(file_path)\n\n            # If their md5 hexdigests match, do nothing\n            if s3_md5 == local_md5:\n                pass\n            # If they don't match, we want to add it\n            else:\n                logger.debug(\"{} has changed\".format(file_key))\n                self.update_list.append((file_key, file_path))\n\n            # Remove the file from the s3 dict, we don't need it anymore\n            del self.s3_obj_dict[file_key]\n\n        # If the file doesn't exist, queue it for creation\n        else:\n            logger.debug(\"{} has been added\".format(file_key))\n            self.update_list.append((file_key, file_path))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nuploading the file to S3.", "response": "def upload_to_s3(self, key, filename):\n        \"\"\"\n        Set the content type and gzip headers if applicable\n        and upload the item to S3\n        \"\"\"\n        extra_args = {'ACL': self.acl}\n        # determine the mimetype of the file\n        guess = mimetypes.guess_type(filename)\n        content_type = guess[0]\n        encoding = guess[1]\n\n        if content_type:\n            extra_args['ContentType'] = content_type\n\n        # add the gzip headers, if necessary\n        if (self.gzip and content_type in self.gzip_content_types) or encoding == 'gzip':\n            extra_args['ContentEncoding'] = 'gzip'\n\n        # add the cache-control headers if necessary\n        if content_type in self.cache_control:\n            extra_args['CacheControl'] = ''.join((\n                'max-age=',\n                str(self.cache_control[content_type])\n            ))\n\n        # access and write the contents from the file\n        if not self.dry_run:\n            logger.debug(\"Uploading %s\" % filename)\n            if self.verbosity > 0:\n                self.stdout.write(\"Uploading %s\" % filename)\n            s3_obj = self.s3_resource.Object(self.aws_bucket_name, key)\n            s3_obj.upload_file(filename, ExtraArgs=extra_args)\n\n        # Update counts\n        self.uploaded_files += 1\n        self.uploaded_file_list.append(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_bakery_dynamic_attr(self, attname, obj, args=None, default=None):\n        try:\n            attr = getattr(self, attname)\n        except AttributeError:\n            return default\n\n        if callable(attr) or args:\n            args = args[:] if args else []\n\n            # Check co_argcount rather than try/excepting the function and\n            # catching the TypeError, because something inside the function\n            # may raise the TypeError. This technique is more accurate.\n            try:\n                code = six.get_function_code(attr)\n            except AttributeError:\n                code = six.get_function_code(attr.__call__)\n            if code.co_argcount == 2 + len(args):  # one argument is 'self'\n                args.append(obj)\n            return attr(*args)\n\n        return attr", "response": "This method is used to get a dynamic attribute from the Feed object. It is used to get the attribute name from the Feed object and the object itself."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the year from the database in the format expected by the URL.", "response": "def get_year(self):\n        \"\"\"\n        Return the year from the database in the format expected by the URL.\n        \"\"\"\n        year = super(BuildableYearArchiveView, self).get_year()\n        fmt = self.get_year_format()\n        return date(int(year), 1, 1).strftime(fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_year(self, dt):\n        self.year = str(dt.year)\n        logger.debug(\"Building %s\" % self.year)\n        self.request = self.create_request(self.get_url())\n        target_path = self.get_build_path()\n        self.build_file(target_path, self.get_content())", "response": "Build the page for the provided year."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild pages for all years in the queryset.", "response": "def build_dated_queryset(self):\n        \"\"\"\n        Build pages for all years in the queryset.\n        \"\"\"\n        qs = self.get_dated_queryset()\n        years = self.get_date_list(qs)\n        [self.build_year(dt) for dt in years]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_month(self):\n        year = super(BuildableMonthArchiveView, self).get_year()\n        month = super(BuildableMonthArchiveView, self).get_month()\n        fmt = self.get_month_format()\n        return date(int(year), int(month), 1).strftime(fmt)", "response": "Return the month from the database in the format expected by the URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_dated_queryset(self):\n        qs = self.get_dated_queryset()\n        months = self.get_date_list(qs)\n        [self.build_month(dt) for dt in months]", "response": "Build pages for all years in the queryset."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unbuild_month(self, dt):\n        self.year = str(dt.year)\n        self.month = str(dt.month)\n        logger.debug(\"Building %s-%s\" % (self.year, self.month))\n        target_path = os.path.split(self.get_build_path())[0]\n        if self.fs.exists(target_path):\n            logger.debug(\"Removing {}\".format(target_path))\n            self.fs.removetree(target_path)", "response": "Unbuilds the ISO - 8601 calendar."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the year from the database in the format expected by the URL.", "response": "def get_year(self):\n        \"\"\"\n        Return the year from the database in the format expected by the URL.\n        \"\"\"\n        year = super(BuildableDayArchiveView, self).get_year()\n        fmt = self.get_year_format()\n        dt = date(int(year), 1, 1)\n        return dt.strftime(fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_month(self):\n        year = super(BuildableDayArchiveView, self).get_year()\n        month = super(BuildableDayArchiveView, self).get_month()\n        fmt = self.get_month_format()\n        dt = date(int(year), int(month), 1)\n        return dt.strftime(fmt)", "response": "Return the month from the database in the format expected by the URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_day(self):\n        year = super(BuildableDayArchiveView, self).get_year()\n        month = super(BuildableDayArchiveView, self).get_month()\n        day = super(BuildableDayArchiveView, self).get_day()\n        fmt = self.get_day_format()\n        dt = date(int(year), int(month), int(day))\n        return dt.strftime(fmt)", "response": "Return the day from the database in the format expected by the URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_url(self):\n        return os.path.join(\n            '/archive',\n            self.get_year(),\n            self.get_month(),\n            self.get_day()\n        )", "response": "Returns the URL at which the detail page should appear."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the path where the build file should be located.", "response": "def get_build_path(self):\n        \"\"\"\n        Used to determine where to build the page. Override this if you\n        would like your page at a different location. By default it\n        will be built at self.get_url() + \"/index.html\"\n        \"\"\"\n        target_path = path.join(settings.BUILD_DIR, self.get_url().lstrip('/'))\n        if not self.fs.exists(target_path):\n            logger.debug(\"Creating {}\".format(target_path))\n            self.fs.makedirs(target_path)\n        return os.path.join(target_path, 'index.html')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the page for the provided day.", "response": "def build_day(self, dt):\n        \"\"\"\n        Build the page for the provided day.\n        \"\"\"\n        self.month = str(dt.month)\n        self.year = str(dt.year)\n        self.day = str(dt.day)\n        logger.debug(\"Building %s-%s-%s\" % (self.year, self.month, self.day))\n        self.request = self.create_request(self.get_url())\n        path = self.get_build_path()\n        self.build_file(path, self.get_content())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_dated_queryset(self):\n        qs = self.get_dated_queryset()\n        days = self.get_date_list(qs, date_type='day')\n        [self.build_day(dt) for dt in days]", "response": "Build pages for all years in the queryset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a boto3 client and s3 resource", "response": "def get_s3_client():\n    \"\"\"\n    A DRY place to make sure AWS credentials in settings override\n    environment based credentials.  Boto3 will fall back to:\n    http://boto3.readthedocs.io/en/latest/guide/configuration.html\n    \"\"\"\n    session_kwargs = {}\n    if hasattr(settings, 'AWS_ACCESS_KEY_ID'):\n        session_kwargs['aws_access_key_id'] = settings.AWS_ACCESS_KEY_ID\n\n    if hasattr(settings, 'AWS_SECRET_ACCESS_KEY'):\n        session_kwargs['aws_secret_access_key'] = settings.AWS_SECRET_ACCESS_KEY\n    boto3.setup_default_session(**session_kwargs)\n\n    s3_kwargs = {}\n    if hasattr(settings, 'AWS_S3_ENDPOINT'):\n        s3_kwargs['endpoint_url'] = settings.AWS_S3_ENDPOINT\n    elif hasattr(settings, 'AWS_S3_HOST'):\n        if hasattr(settings, 'AWS_S3_USE_SSL') and settings.AWS_S3_USE_SSL is False:\n            protocol = \"http://\"\n        else:\n            protocol = \"https://\"\n        s3_kwargs['endpoint_url'] = \"{}{}\".format(\n            protocol,\n            settings.AWS_S3_HOST\n        )\n    if hasattr(settings, \"AWS_REGION\"):\n        s3_kwargs['region_name'] = settings.AWS_REGION\n    s3_client = boto3.client('s3', **s3_kwargs)\n    s3_resource = boto3.resource('s3', **s3_kwargs)\n    return s3_client, s3_resource"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_bucket_page(page):\n    key_list = page.get('Contents', [])\n    logger.debug(\"Retrieving page with {} keys\".format(\n        len(key_list),\n    ))\n    return dict((k.get('Key'), k) for k in key_list)", "response": "Returns all the keys in a s3 bucket paginator page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_objects_in_bucket(\n        aws_bucket_name,\n        s3_client=None,\n        max_keys=1000\n):\n    \"\"\"\n    Little utility method that handles pagination and returns\n    all objects in given bucket.\n    \"\"\"\n    logger.debug(\"Retrieving bucket object list\")\n\n    if not s3_client:\n        s3_client, s3_resource = get_s3_client()\n\n    obj_dict = {}\n    paginator = s3_client.get_paginator('list_objects')\n    page_iterator = paginator.paginate(Bucket=aws_bucket_name)\n    for page in page_iterator:\n        key_list = page.get('Contents', [])\n        logger.debug(\"Loading page with {} keys\".format(len(key_list)))\n        for obj in key_list:\n            obj_dict[obj.get('Key')] = obj\n    return obj_dict", "response": "Returns a dictionary of all objects in a given bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef batch_delete_s3_objects(\n        keys,\n        aws_bucket_name,\n        chunk_size=100,\n        s3_client=None\n):\n    \"\"\"\n    Utility method that batch deletes objects in given bucket.\n    \"\"\"\n    if s3_client is None:\n        s3_client, s3_resource = get_s3_client()\n\n    key_chunks = []\n    for i in range(0, len(keys), chunk_size):\n        chunk = []\n        for key in (list(keys)[i:i+100]):\n            chunk.append({'Key': key})\n        key_chunks.append(chunk)\n    for chunk in key_chunks:\n        s3_client.delete_objects(\n            Bucket=aws_bucket_name,\n            Delete={'Objects': chunk}\n        )", "response": "Utility method that batch deletes objects in given bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a regex that matches a token in a sequence of tokens.", "response": "def lexeme(p):\n    \"\"\"\n    From a parser (or string), make a parser that consumes\n    whitespace on either side.\n    \"\"\"\n    if isinstance(p, str):\n        p = string(p)\n    return regex(r'\\s*') >> p << regex(r'\\s*')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_present(p):\n    return lexeme(p).optional().map(lambda v: False if v is None else True)", "response": "Given a parser or string return a parser that returns\n    True if the parser matches False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a string or list of tokens and return the result or raise a ParseError.", "response": "def parse(self, stream):\n        \"\"\"Parse a string or list of tokens and return the result or raise a ParseError.\"\"\"\n        (result, _) = (self << eof).parse_partial(stream)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_partial(self, stream):\n        result = self(stream, 0)\n\n        if result.status:\n            return (result.value, stream[result.index:])\n        else:\n            raise ParseError(result.expected, stream, result.furthest)", "response": "Parse the longest possible prefix of a given string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_key_values(array_value, separators=(';', ',', ':'), **kwargs):\n    items_sep, fields_sep, keys_sep = separators\n    return items_sep.join(fields_sep.join(keys_sep.join(x) for x in sorted(it.items()))\n                          for it in array_value)", "response": "Serialize array of objects with simple key - values\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Flatson instance from a schemafile", "response": "def from_schemafile(cls, schemafile):\n        \"\"\"Create a Flatson instance from a schemafile\n        \"\"\"\n        with open(schemafile) as f:\n            return cls(json.load(f))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters a custom serialization method that can be used by schema configuration.", "response": "def register_serialization_method(self, name, serialize_func):\n        \"\"\"Register a custom serialization method that can be\n        used via schema configuration\n        \"\"\"\n        if name in self._default_serialization_methods:\n            raise ValueError(\"Can't replace original %s serialization method\")\n        self._serialization_methods[name] = serialize_func"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list with the field values", "response": "def flatten(self, obj):\n        \"\"\"Return a list with the field values\n        \"\"\"\n        return [self._serialize(f, obj) for f in self.fields]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an OrderedDict dict preserving order of keys in fieldnames", "response": "def flatten_dict(self, obj):\n        \"\"\"Return an OrderedDict dict preserving order of keys in fieldnames\n        \"\"\"\n        return OrderedDict(zip(self.fieldnames, self.flatten(obj)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose the connection :raises: ConnectionBusyError", "response": "def close(self):\n        \"\"\"Close the connection\n\n        :raises: ConnectionBusyError\n\n        \"\"\"\n        LOGGER.debug('Connection %s closing', self.id)\n        if self.busy:\n            raise ConnectionBusyError(self)\n        with self._lock:\n            if not self.handle.closed:\n                try:\n                    self.handle.close()\n                except psycopg2.InterfaceError as error:\n                    LOGGER.error('Error closing socket: %s', error)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns if the connection is currently executing a query or is locked by a session that still exists.", "response": "def busy(self):\n        \"\"\"Return if the connection is currently executing a query or is locked\n        by a session that still exists.\n\n        :rtype: bool\n\n        \"\"\"\n        if self.handle.isexecuting():\n            return True\n        elif self.used_by is None:\n            return False\n        return not self.used_by() is None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef free(self):\n        LOGGER.debug('Connection %s freeing', self.id)\n        if self.handle.isexecuting():\n            raise ConnectionBusyError(self)\n        with self._lock:\n            self.used_by = None\n        LOGGER.debug('Connection %s freed', self.id)", "response": "Remove the lock on the connection if the connection is not active\n\n        :raises: ConnectionBusyError"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlock the connection ensuring that it is not busy and storing a weakref for the session.", "response": "def lock(self, session):\n        \"\"\"Lock the connection, ensuring that it is not busy and storing\n        a weakref for the session.\n\n        :param queries.Session session: The session to lock the connection with\n        :raises: ConnectionBusyError\n\n        \"\"\"\n        if self.busy:\n            raise ConnectionBusyError(self)\n        with self._lock:\n            self.used_by = weakref.ref(session)\n        LOGGER.debug('Connection %s locked', self.id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new connection to the pool.", "response": "def add(self, connection):\n        \"\"\"Add a new connection to the pool\n\n        :param connection: The connection to add to the pool\n        :type connection: psycopg2.extensions.connection\n        :raises: PoolFullError\n\n        \"\"\"\n        if id(connection) in self.connections:\n            raise ValueError('Connection already exists in pool')\n\n        if len(self.connections) == self.max_size:\n            LOGGER.warning('Race condition found when adding new connection')\n            try:\n                connection.close()\n            except (psycopg2.Error, psycopg2.Warning) as error:\n                LOGGER.error('Error closing the conn that cant be used: %s',\n                             error)\n            raise PoolFullError(self)\n        with self._lock:\n            self.connections[id(connection)] = Connection(connection)\n        LOGGER.debug('Pool %s added connection %s', self.id, id(connection))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of active and busy connections.", "response": "def busy_connections(self):\n        \"\"\"Return a list of active/busy connections\n\n        :rtype: list\n\n        \"\"\"\n        return [c for c in self.connections.values()\n                if c.busy and not c.closed]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclean the pool by removing any closed connections and removing any idle connections and closing all connections.", "response": "def clean(self):\n        \"\"\"Clean the pool by removing any closed connections and if the pool's\n        idle has exceeded its idle TTL, remove all connections.\n\n        \"\"\"\n        LOGGER.debug('Cleaning the pool')\n        for connection in [self.connections[k] for k in self.connections if\n                           self.connections[k].closed]:\n            LOGGER.debug('Removing %s', connection.id)\n            self.remove(connection.handle)\n\n        if self.idle_duration > self.idle_ttl:\n            self.close()\n\n        LOGGER.debug('Pool %s cleaned', self.id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self):\n        for cid in list(self.connections.keys()):\n            self.remove(self.connections[cid].handle)\n        LOGGER.debug('Pool %s closed', self.id)", "response": "Close the pool by closing all of the connections and removing all of the connections."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfrees the connection from use by the session that was using it.", "response": "def free(self, connection):\n        \"\"\"Free the connection from use by the session that was using it.\n\n        :param connection: The connection to free\n        :type connection: psycopg2.extensions.connection\n        :raises: ConnectionNotFoundError\n\n        \"\"\"\n        LOGGER.debug('Pool %s freeing connection %s', self.id, id(connection))\n        try:\n            self.connection_handle(connection).free()\n        except KeyError:\n            raise ConnectionNotFoundError(self.id, id(connection))\n\n        if self.idle_connections == list(self.connections.values()):\n            with self._lock:\n                self.idle_start = self.time_method()\n        LOGGER.debug('Pool %s freed connection %s', self.id, id(connection))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an idle connection and assign the session to the connection", "response": "def get(self, session):\n        \"\"\"Return an idle connection and assign the session to the connection\n\n        :param queries.Session session: The session to assign\n        :rtype: psycopg2.extensions.connection\n        :raises: NoIdleConnectionsError\n\n        \"\"\"\n        idle = self.idle_connections\n        if idle:\n            connection = idle.pop(0)\n            connection.lock(session)\n            if self.idle_start:\n                with self._lock:\n                    self.idle_start = None\n            return connection.handle\n        raise NoIdleConnectionsError(self.id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of idle connections.", "response": "def idle_connections(self):\n        \"\"\"Return a list of idle connections\n\n        :rtype: list\n\n        \"\"\"\n        return [c for c in self.connections.values()\n                if not c.busy and not c.closed]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lock(self, connection, session):\n        cid = id(connection)\n        try:\n            self.connection_handle(connection).lock(session)\n        except KeyError:\n            raise ConnectionNotFoundError(self.id, cid)\n        else:\n            if self.idle_start:\n                with self._lock:\n                    self.idle_start = None\n        LOGGER.debug('Pool %s locked connection %s', self.id, cid)", "response": "Explicitly lock the specified connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove(self, connection):\n        cid = id(connection)\n        if cid not in self.connections:\n            raise ConnectionNotFoundError(self.id, cid)\n        self.connection_handle(connection).close()\n        with self._lock:\n            del self.connections[cid]\n        LOGGER.debug('Pool %s removed connection %s', self.id, cid)", "response": "Removes the connection from the pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef report(self):\n        return {\n            'connections': {\n                'busy': len(self.busy_connections),\n                'closed': len(self.closed_connections),\n                'executing': len(self.executing_connections),\n                'idle': len(self.idle_connections),\n                'locked': len(self.busy_connections)\n            },\n            'exceptions': sum([c.exceptions\n                               for c in self.connections.values()]),\n            'executions': sum([c.executions\n                               for c in self.connections.values()]),\n            'full': self.is_full,\n            'idle': {\n                'duration': self.idle_duration,\n                'ttl': self.idle_ttl\n            },\n            'max_size': self.max_size\n        }", "response": "Return a report about the pool state and configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shutdown(self):\n        with self._lock:\n            for cid in list(self.connections.keys()):\n                if self.connections[cid].executing:\n                    raise ConnectionBusyError(cid)\n                if self.connections[cid].locked:\n                    self.connections[cid].free()\n                self.connections[cid].close()\n                del self.connections[cid]", "response": "Forcefully shutdown all non - executing contexts."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a new connection and session to a pool.", "response": "def add(cls, pid, connection):\n        \"\"\"Add a new connection and session to a pool.\n\n        :param str pid: The pool id\n        :type connection: psycopg2.extensions.connection\n        :param connection: The connection to add to the pool\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].add(connection)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean(cls, pid):\n        with cls._lock:\n            try:\n                cls._ensure_pool_exists(pid)\n            except KeyError:\n                LOGGER.debug('Pool clean invoked against missing pool %s', pid)\n                return\n            cls._pools[pid].clean()\n            cls._maybe_remove_pool(pid)", "response": "Clean the specified pool removing any closed connections or stale locks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(cls, pid, idle_ttl=DEFAULT_IDLE_TTL, max_size=DEFAULT_MAX_SIZE,\n               time_method=None):\n        \"\"\"Create a new pool, with the ability to pass in values to override\n        the default idle TTL and the default maximum size.\n\n        A pool's idle TTL defines the amount of time that a pool can be open\n        without any sessions before it is removed.\n\n        A pool's max size defines the maximum number of connections that can\n        be added to the pool to prevent unbounded open connections.\n\n        :param str pid: The pool ID\n        :param int idle_ttl: Time in seconds for the idle TTL\n        :param int max_size: The maximum pool size\n        :param callable time_method: Override the use of :py:meth:`time.time`\n            method for time values.\n        :raises: KeyError\n\n        \"\"\"\n        if pid in cls._pools:\n            raise KeyError('Pool %s already exists' % pid)\n        with cls._lock:\n            LOGGER.debug(\"Creating Pool: %s (%i/%i)\", pid, idle_ttl, max_size)\n            cls._pools[pid] = Pool(pid, idle_ttl, max_size, time_method)", "response": "Create a new pool with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfreeing a connection that was locked by a session .", "response": "def free(cls, pid, connection):\n        \"\"\"Free a connection that was locked by a session\n\n        :param str pid: The pool ID\n        :param connection: The connection to remove\n        :type connection: psycopg2.extensions.connection\n\n        \"\"\"\n        with cls._lock:\n            LOGGER.debug('Freeing %s from pool %s', id(connection), pid)\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].free(connection)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(cls, pid, session):\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            return cls._pools[pid].get(session)", "response": "Get an idle unused connection from the pool."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the specified connection from the specified pool ID .", "response": "def get_connection(cls, pid, connection):\n        \"\"\"Return the specified :class:`~queries.pool.Connection` from the\n        pool.\n\n        :param str pid: The pool ID\n        :param connection: The connection to return for\n        :type connection: psycopg2.extensions.connection\n        :rtype: queries.pool.Connection\n\n        \"\"\"\n        with cls._lock:\n            return cls._pools[pid].connection_handle(connection)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck to see if a connection in a pool with the specified ID.", "response": "def has_connection(cls, pid, connection):\n        \"\"\"Check to see if a pool has the specified connection\n\n        :param str pid: The pool ID\n        :param connection: The connection to check for\n        :type connection: psycopg2.extensions.connection\n        :rtype: bool\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            return connection in cls._pools[pid]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_idle_connection(cls, pid):\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            return bool(cls._pools[pid].idle_connections)", "response": "Check to see if a pool has an idle connection"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a bool indicating if the specified pool is full.", "response": "def is_full(cls, pid):\n        \"\"\"Return a bool indicating if the specified pool is full\n\n        :param str pid: The pool id\n        :rtype: bool\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            return cls._pools[pid].is_full"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a pool from the pool list.", "response": "def remove(cls, pid):\n        \"\"\"Remove a pool, closing all connections\n\n        :param str pid: The pool ID\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].close()\n            del cls._pools[pid]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_connection(cls, pid, connection):\n        cls._ensure_pool_exists(pid)\n        cls._pools[pid].remove(connection)", "response": "Removes a connection from the pool closing it if is open."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_idle_ttl(cls, pid, ttl):\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].set_idle_ttl(ttl)", "response": "Set the idle TTL for a pool after which it will be destroyed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_max_size(cls, pid, size):\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].set_max_size(size)", "response": "Set the maximum number of connections for the specified pool."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shutdown(cls):\n        for pid in list(cls._pools.keys()):\n            cls._pools[pid].shutdown()\n        LOGGER.info('Shutdown complete, all pooled connections closed')", "response": "Close all connections on all pools"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef size(cls, pid):\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            return len(cls._pools[pid])", "response": "Return the number of connections in the pool with the given pid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the state of the all of the registered pools.", "response": "def report(cls):\n        \"\"\"Return the state of the all of the registered pools.\n\n        :rtype: dict\n\n        \"\"\"\n        return {\n            'timestamp': datetime.datetime.utcnow().isoformat(),\n            'process': os.getpid(),\n            'pools': dict([(i, p.report()) for i, p in cls._pools.items()])\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves the pool from the dict if it has no open connections.", "response": "def _maybe_remove_pool(cls, pid):\n        \"\"\"If the pool has no open connections, remove it\n\n        :param str pid: The pool id to clean\n\n        \"\"\"\n        if not len(cls._pools[pid]):\n            del cls._pools[pid]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef callproc(self, name, args=None):\n        try:\n            self._cursor.callproc(name, args)\n        except psycopg2.Error as err:\n            self._incr_exceptions()\n            raise err\n        finally:\n            self._incr_executions()\n        return results.Results(self._cursor)", "response": "Call a stored procedure on the server and return the results in a\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pid(self):\n        return hashlib.md5(':'.join([self.__class__.__name__,\n                                     self._uri]).encode('utf-8')).hexdigest()", "response": "Return the pool ID used for connection pooling."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query(self, sql, parameters=None):\n        try:\n            self._cursor.execute(sql, parameters)\n        except psycopg2.Error as err:\n            self._incr_exceptions()\n            raise err\n        finally:\n            self._incr_executions()\n        return results.Results(self._cursor)", "response": "A generator to issue a query on the server and return the result data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the client encoding for the session if the value specified is different than the current client encoding.", "response": "def set_encoding(self, value=DEFAULT_ENCODING):\n        \"\"\"Set the client encoding for the session if the value specified\n        is different than the current client encoding.\n\n        :param str value: The encoding value to use\n\n        \"\"\"\n        if self._conn.encoding != value:\n            self._conn.set_client_encoding(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the connection from the stack closing out the cursor and freeing the connection in the pool", "response": "def _cleanup(self):\n        \"\"\"Remove the connection from the stack, closing out the cursor\"\"\"\n        if self._cursor:\n            LOGGER.debug('Closing the cursor on %s', self.pid)\n            self._cursor.close()\n            self._cursor = None\n\n        if self._conn:\n            LOGGER.debug('Freeing %s in the pool', self.pid)\n            try:\n                pool.PoolManager.instance().free(self.pid, self._conn)\n            except pool.ConnectionNotFoundError:\n                pass\n            self._conn = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _connect(self):\n        # Attempt to get a cached connection from the connection pool\n        try:\n            connection = self._pool_manager.get(self.pid, self)\n            LOGGER.debug(\"Re-using connection for %s\", self.pid)\n        except pool.NoIdleConnectionsError:\n            if self._pool_manager.is_full(self.pid):\n                raise\n\n            # Create a new PostgreSQL connection\n            kwargs = utils.uri_to_kwargs(self._uri)\n            LOGGER.debug(\"Creating a new connection for %s\", self.pid)\n            connection = self._psycopg2_connect(kwargs)\n\n            self._pool_manager.add(self.pid, connection)\n            self._pool_manager.lock(self.pid, connection, self)\n\n            # Added in because psycopg2ct connects and leaves the connection in\n            # a weird state: consts.STATUS_DATESTYLE, returning from\n            # Connection._setup without setting the state as const.STATUS_OK\n            if utils.PYPY:\n                connection.reset()\n\n            # Register the custom data types\n            self._register_unicode(connection)\n            self._register_uuid(connection)\n\n        return connection", "response": "Connect to the PostgreSQL server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a cursor for the given connection. Specify a name to use server - side cursors.", "response": "def _get_cursor(self, connection, name=None):\n        \"\"\"Return a cursor for the given cursor_factory. Specify a name to\n        use server-side cursors.\n\n        :param connection: The connection to create a cursor on\n        :type connection: psycopg2.extensions.connection\n        :param str name: A cursor name for a server side cursor\n        :rtype: psycopg2.extensions.cursor\n\n        \"\"\"\n        cursor = connection.cursor(name=name,\n                                   cursor_factory=self._cursor_factory)\n        if name is not None:\n            cursor.scrollable = True\n            cursor.withhold = True\n        return cursor"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _incr_exceptions(self):\n        self._pool_manager.get_connection(self.pid, self._conn).exceptions += 1", "response": "Increment the number of exceptions for the current connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nincrement the number of executions for the current connection.", "response": "def _incr_executions(self):\n        \"\"\"Increment the number of executions for the current connection.\"\"\"\n        self._pool_manager.get_connection(self.pid, self._conn).executions += 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _register_unicode(connection):\n        psycopg2.extensions.register_type(psycopg2.extensions.UNICODE,\n                                          connection)\n        psycopg2.extensions.register_type(psycopg2.extensions.UNICODEARRAY,\n                                          connection)", "response": "Register the cursor to be able to receive Unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _status(self):\n        if self._conn.status == psycopg2.extensions.STATUS_BEGIN:\n            return self.READY\n        return self._conn.status", "response": "Return the current connection status as an integer value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a single row result as a dictionary.", "response": "def as_dict(self):\n        \"\"\"Return a single row result as a dictionary. If the results contain\n        multiple rows, a :py:class:`ValueError` will be raised.\n\n        :return: dict\n        :raises: ValueError\n\n        \"\"\"\n        if not self.cursor.rowcount:\n            return {}\n\n        self._rewind()\n        if self.cursor.rowcount == 1:\n            return dict(self.cursor.fetchone())\n        else:\n            raise ValueError('More than one row')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef items(self):\n        if not self.cursor.rowcount:\n            return []\n\n        self.cursor.scroll(0, 'absolute')\n        return self.cursor.fetchall()", "response": "Return all the rows that are in the result set."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_current_user():\n    if pwd is None:\n        return getpass.getuser()\n    else:\n        try:\n            return pwd.getpwuid(os.getuid())[0]\n        except KeyError as error:\n            LOGGER.error('Could not get logged-in user: %s', error)", "response": "Return the current username for the logged in user"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef uri(host='localhost', port=5432, dbname='postgres', user='postgres',\n        password=None):\n    \"\"\"Return a PostgreSQL connection URI for the specified values.\n\n    :param str host: Host to connect to\n    :param int port: Port to connect on\n    :param str dbname: The database name\n    :param str user: User to connect as\n    :param str password: The password to use, None for no password\n    :return str: The PostgreSQL connection URI\n\n     \"\"\"\n    if port:\n        host = '%s:%s' % (host, port)\n    if password:\n        return 'postgresql://%s:%s@%s/%s' % (user, password, host, dbname)\n    return 'postgresql://%s@%s/%s' % (user, host, dbname)", "response": "Return a PostgreSQL connection URI for the specified values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a URI to a dictionary of keyword arguments for connecting to PostgreSQL with psycopg2.", "response": "def uri_to_kwargs(uri):\n    \"\"\"Return a URI as kwargs for connecting to PostgreSQL with psycopg2,\n    applying default values for non-specified areas of the URI.\n\n    :param str uri: The connection URI\n    :rtype: dict\n\n    \"\"\"\n    parsed = urlparse(uri)\n    default_user = get_current_user()\n    password = unquote(parsed.password) if parsed.password else None\n    kwargs = {'host': parsed.hostname,\n              'port': parsed.port,\n              'dbname': parsed.path[1:] or default_user,\n              'user': parsed.username or default_user,\n              'password': password}\n    values = parse_qs(parsed.query)\n    if 'host' in values:\n        kwargs['host'] = values['host'][0]\n    for k in [k for k in values if k in KEYWORDS]:\n        kwargs[k] = values[k][0] if len(values[k]) == 1 else values[k]\n        try:\n            if kwargs[k].isdigit():\n                kwargs[k] = int(kwargs[k])\n        except AttributeError:\n            pass\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef urlparse(url):\n    value = 'http%s' % url[5:] if url[:5] == 'postgresql' else url\n    parsed = _urlparse.urlparse(value)\n    path, query = parsed.path, parsed.query\n    hostname = parsed.hostname if parsed.hostname else ''\n    return PARSED(parsed.scheme.replace('http', 'postgresql'),\n                  parsed.netloc,\n                  path,\n                  parsed.params,\n                  query,\n                  parsed.fragment,\n                  parsed.username,\n                  parsed.password,\n                  hostname.replace('%2f', '/'),\n                  parsed.port)", "response": "Parse a URL in a Python 2 independent fashion."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreleases the results and connection lock from the TornadoSession object.", "response": "def free(self):\n        \"\"\"Release the results and connection lock from the TornadoSession\n        object. This **must** be called after you finish processing the results\n        from :py:meth:`TornadoSession.query <queries.TornadoSession.query>` or\n        :py:meth:`TornadoSession.callproc <queries.TornadoSession.callproc>`\n        or the connection will not be able to be reused by other asynchronous\n        requests.\n\n        \"\"\"\n        self._freed = True\n        self._cleanup(self.cursor, self._fd)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the pool in the pool manager if it does not exist.", "response": "def _ensure_pool_exists(self):\n        \"\"\"Create the pool in the pool manager if it does not exist.\"\"\"\n        if self.pid not in self._pool_manager:\n            self._pool_manager.create(self.pid, self._pool_idle_ttl,\n                                      self._pool_max_size, self._ioloop.time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconnects to PostgreSQL, either by reusing a connection from the pool if possible, or by creating the new connection. :rtype: psycopg2.extensions.connection :raises: pool.NoIdleConnectionsError", "response": "def _connect(self):\n        \"\"\"Connect to PostgreSQL, either by reusing a connection from the pool\n        if possible, or by creating the new connection.\n\n        :rtype: psycopg2.extensions.connection\n        :raises: pool.NoIdleConnectionsError\n\n        \"\"\"\n        future = concurrent.Future()\n\n        # Attempt to get a cached connection from the connection pool\n        try:\n            connection = self._pool_manager.get(self.pid, self)\n            self._connections[connection.fileno()] = connection\n            future.set_result(connection)\n\n            # Add the connection to the IOLoop\n            self._ioloop.add_handler(connection.fileno(),\n                                     self._on_io_events,\n                                     ioloop.IOLoop.WRITE)\n        except pool.NoIdleConnectionsError:\n            self._create_connection(future)\n\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_connection(self, future):\n        LOGGER.debug('Creating a new connection for %s', self.pid)\n\n        # Create a new PostgreSQL connection\n        kwargs = utils.uri_to_kwargs(self._uri)\n\n        try:\n            connection = self._psycopg2_connect(kwargs)\n        except (psycopg2.Error, OSError, socket.error) as error:\n            future.set_exception(error)\n            return\n\n        # Add the connection for use in _poll_connection\n        fd = connection.fileno()\n        self._connections[fd] = connection\n\n        def on_connected(cf):\n            \"\"\"Invoked by the IOLoop when the future is complete for the\n            connection\n\n            :param Future cf: The future for the initial connection\n\n            \"\"\"\n            if cf.exception():\n                self._cleanup_fd(fd, True)\n                future.set_exception(cf.exception())\n\n            else:\n\n                try:\n                    # Add the connection to the pool\n                    LOGGER.debug('Connection established for %s', self.pid)\n                    self._pool_manager.add(self.pid, connection)\n                except (ValueError, pool.PoolException) as err:\n                    LOGGER.exception('Failed to add %r to the pool', self.pid)\n                    self._cleanup_fd(fd)\n                    future.set_exception(err)\n                    return\n\n                self._pool_manager.lock(self.pid, connection, self)\n\n                # Added in because psycopg2cffi connects and leaves the\n                # connection in a weird state: consts.STATUS_DATESTYLE,\n                # returning from Connection._setup without setting the state\n                # as const.STATUS_OK\n                if utils.PYPY:\n                    connection.status = extensions.STATUS_READY\n\n                # Register the custom data types\n                self._register_unicode(connection)\n                self._register_uuid(connection)\n\n                # Set the future result\n                future.set_result(connection)\n\n        # Add a future that fires once connected\n        self._futures[fd] = concurrent.Future()\n        self._ioloop.add_future(self._futures[fd], on_connected)\n\n        # Add the connection to the IOLoop\n        self._ioloop.add_handler(connection.fileno(),\n                                 self._on_io_events,\n                                 ioloop.IOLoop.WRITE)", "response": "Create a new PostgreSQL connection and lock it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nissue a query asynchronously on the server and return the result.", "response": "def _execute(self, method, query, parameters=None):\n        \"\"\"Issue a query asynchronously on the server, mogrifying the\n        parameters against the sql statement and yielding the results\n        as a :py:class:`Results <queries.tornado_session.Results>` object.\n\n        This function reduces duplicate code for callproc and query by getting\n        the class attribute for the method passed in as the function to call.\n\n        :param str method: The method attribute to use\n        :param str query: The SQL statement or Stored Procedure name\n        :param list|dict parameters: A dictionary of query parameters\n        :rtype: Results\n        :raises: queries.DataError\n        :raises: queries.DatabaseError\n        :raises: queries.IntegrityError\n        :raises: queries.InternalError\n        :raises: queries.InterfaceError\n        :raises: queries.NotSupportedError\n        :raises: queries.OperationalError\n        :raises: queries.ProgrammingError\n\n        \"\"\"\n        future = concurrent.Future()\n\n        def on_connected(cf):\n            \"\"\"Invoked by the future returned by self._connect\"\"\"\n            if cf.exception():\n                future.set_exception(cf.exception())\n                return\n\n            # Get the psycopg2 connection object and cursor\n            conn = cf.result()\n            cursor = self._get_cursor(conn)\n\n            def completed(qf):\n                \"\"\"Invoked by the IOLoop when the future has completed\"\"\"\n                if qf.exception():\n                    self._incr_exceptions(conn)\n                    err = qf.exception()\n                    LOGGER.debug('Cleaning cursor due to exception: %r', err)\n                    self._exec_cleanup(cursor, conn.fileno())\n                    future.set_exception(err)\n                else:\n                    self._incr_executions(conn)\n                    value = Results(cursor, self._exec_cleanup, conn.fileno())\n                    future.set_result(value)\n\n            # Setup a callback to wait on the query result\n            self._futures[conn.fileno()] = concurrent.Future()\n\n            # Add the future to the IOLoop\n            self._ioloop.add_future(self._futures[conn.fileno()],\n                                    completed)\n\n            # Get the cursor, execute the query\n            func = getattr(cursor, method)\n            try:\n                func(query, parameters)\n            except Exception as error:\n                future.set_exception(error)\n\n        # Ensure the pool exists for the connection\n        self._ensure_pool_exists()\n\n        # Grab a connection to PostgreSQL\n        self._ioloop.add_future(self._connect(), on_connected)\n\n        # Return the future for the query result\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _exec_cleanup(self, cursor, fd):\n        LOGGER.debug('Closing cursor and cleaning %s', fd)\n        try:\n            cursor.close()\n        except (psycopg2.Error, psycopg2.Warning) as error:\n            LOGGER.debug('Error closing the cursor: %s', error)\n\n        self._cleanup_fd(fd)\n\n        # If the cleanup callback exists, remove it\n        if self._cleanup_callback:\n            self._ioloop.remove_timeout(self._cleanup_callback)\n\n        # Create a new cleanup callback to clean the pool of idle connections\n        self._cleanup_callback = self._ioloop.add_timeout(\n            self._ioloop.time() + self._pool_idle_ttl + 1,\n            self._pool_manager.clean, self.pid)", "response": "Close the cursor remove any references to the fd from the ioloop and remove the fd from the ioloop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _cleanup_fd(self, fd, close=False):\n        self._ioloop.remove_handler(fd)\n        if fd in self._connections:\n            try:\n                self._pool_manager.free(self.pid, self._connections[fd])\n            except pool.ConnectionNotFoundError:\n                pass\n            if close:\n                self._connections[fd].close()\n            del self._connections[fd]\n        if fd in self._futures:\n            del self._futures[fd]", "response": "Remove the socket from the IOLoop the connection stack and futures stack."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _incr_exceptions(self, conn):\n        self._pool_manager.get_connection(self.pid, conn).exceptions += 1", "response": "Increment the number of exceptions for the current connection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _incr_executions(self, conn):\n        self._pool_manager.get_connection(self.pid, conn).executions += 1", "response": "Increment the number of executions for the current connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling by Tornado s IOLoop when there are events for the fd.", "response": "def _on_io_events(self, fd=None, _events=None):\n        \"\"\"Invoked by Tornado's IOLoop when there are events for the fd\n\n        :param int fd: The file descriptor for the event\n        :param int _events: The events raised\n\n        \"\"\"\n        if fd not in self._connections:\n            LOGGER.warning('Received IO event for non-existing connection')\n            return\n        self._poll_connection(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npolling the connection for the given fd.", "response": "def _poll_connection(self, fd):\n        \"\"\"Check with psycopg2 to see what action to take. If the state is\n        POLL_OK, we should have a pending callback for that fd.\n\n        :param int fd: The socket fd for the postgresql connection\n\n        \"\"\"\n        try:\n            state = self._connections[fd].poll()\n        except (OSError, socket.error) as error:\n            self._ioloop.remove_handler(fd)\n            if fd in self._futures and not self._futures[fd].done():\n                self._futures[fd].set_exception(\n                    psycopg2.OperationalError('Connection error (%s)' % error)\n                )\n        except (psycopg2.Error, psycopg2.Warning) as error:\n            if fd in self._futures and not self._futures[fd].done():\n                self._futures[fd].set_exception(error)\n        else:\n            if state == extensions.POLL_OK:\n                if fd in self._futures and not self._futures[fd].done():\n                    self._futures[fd].set_result(True)\n            elif state == extensions.POLL_WRITE:\n                self._ioloop.update_handler(fd, ioloop.IOLoop.WRITE)\n            elif state == extensions.POLL_READ:\n                self._ioloop.update_handler(fd, ioloop.IOLoop.READ)\n            elif state == extensions.POLL_ERROR:\n                self._ioloop.remove_handler(fd)\n                if fd in self._futures and not self._futures[fd].done():\n                    self._futures[fd].set_exception(\n                        psycopg2.Error('Poll Error'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    import codecs\n    setuptools.setup(\n        name='wcwidth',\n        version='0.1.7',\n        description=(\"Measures number of Terminal column cells \"\n                     \"of wide-character codes\"),\n        long_description=codecs.open(\n            os.path.join(HERE, 'README.rst'), 'r', 'utf8').read(),\n        author='Jeff Quast',\n        author_email='contact@jeffquast.com',\n        license='MIT',\n        packages=['wcwidth', 'wcwidth.tests'],\n        url='https://github.com/jquast/wcwidth',\n        include_package_data=True,\n        test_suite='wcwidth.tests',\n        zip_safe=True,\n        classifiers=[\n            'Intended Audience :: Developers',\n            'Natural Language :: English',\n            'Development Status :: 3 - Alpha',\n            'Environment :: Console',\n            'License :: OSI Approved :: MIT License',\n            'Operating System :: POSIX',\n            'Programming Language :: Python :: 2.7',\n            'Programming Language :: Python :: 3.4',\n            'Programming Language :: Python :: 3.5',\n            'Topic :: Software Development :: Libraries',\n            'Topic :: Software Development :: Localization',\n            'Topic :: Software Development :: Internationalization',\n            'Topic :: Terminals'\n            ],\n        keywords=['terminal', 'emulator', 'wcwidth', 'wcswidth', 'cjk',\n                  'combining', 'xterm', 'console', ],\n        cmdclass={'update': SetupUpdate},\n    )", "response": "A function that sets up the base setup. py module."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npatches README. rst to reflect the data files used in release.", "response": "def _do_readme_update(self):\n        \"\"\"Patch README.rst to reflect the data files used in release.\"\"\"\n        import codecs\n        import glob\n\n        # read in,\n        data_in = codecs.open(\n            os.path.join(HERE, 'README.rst'), 'r', 'utf8').read()\n\n        # search for beginning and end positions,\n        pos_begin = data_in.find(self.README_PATCH_FROM)\n        assert pos_begin != -1, (pos_begin, self.README_PATCH_FROM)\n        pos_begin += len(self.README_PATCH_FROM)\n\n        pos_end = data_in.find(self.README_PATCH_TO)\n        assert pos_end != -1, (pos_end, self.README_PATCH_TO)\n\n        glob_pattern = os.path.join(HERE, 'data', '*.txt')\n        file_descriptions = [\n            self._describe_file_header(fpath)\n            for fpath in glob.glob(glob_pattern)]\n\n        # patch,\n        data_out = (\n            data_in[:pos_begin] +\n            '\\n\\n' +\n            '\\n'.join(file_descriptions) +\n            '\\n\\n' +\n            data_in[pos_end:]\n        )\n\n        # write.\n        print(\"patching {} ..\".format(self.README_RST))\n        codecs.open(\n            self.README_RST, 'w', 'utf8').write(data_out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _do_east_asian(self):\n        self._do_retrieve(self.EAW_URL, self.EAW_IN)\n        (version, date, values) = self._parse_east_asian(\n            fname=self.EAW_IN,\n            properties=(u'W', u'F',)\n        )\n        table = self._make_table(values)\n        self._do_write(self.EAW_OUT, 'WIDE_EASTASIAN', version, date, table)", "response": "Fetch and update east - asian tables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching and update zero width tables.", "response": "def _do_zero_width(self):\n        \"\"\"Fetch and update zero width tables.\"\"\"\n        self._do_retrieve(self.UCD_URL, self.UCD_IN)\n        (version, date, values) = self._parse_category(\n            fname=self.UCD_IN,\n            categories=('Me', 'Mn',)\n        )\n        table = self._make_table(values)\n        self._do_write(self.ZERO_OUT, 'ZERO_WIDTH', version, date, table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a tuple of lookup tables for given values.", "response": "def _make_table(values):\n        \"\"\"Return a tuple of lookup tables for given values.\"\"\"\n        import collections\n        table = collections.deque()\n        start, end = values[0], values[0]\n        for num, value in enumerate(values):\n            if num == 0:\n                table.append((value, value,))\n                continue\n            start, end = table.pop()\n            if end == value - 1:\n                table.append((start, value,))\n            else:\n                table.append((start, end,))\n                table.append((value, value,))\n        return tuple(table)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _do_retrieve(url, fname):\n        folder = os.path.dirname(fname)\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n            print(\"{}/ created.\".format(folder))\n        if not os.path.exists(fname):\n            with open(fname, 'wb') as fout:\n                print(\"retrieving {}.\".format(url))\n                resp = urlopen(url)\n                fout.write(resp.read())\n            print(\"{} saved.\".format(fname))\n        else:\n            print(\"re-using artifact {}\".format(fname))\n        return fname", "response": "Retrieve given url to target filepath fname."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses unicode east - asian width tables.", "response": "def _parse_east_asian(fname, properties=(u'W', u'F',)):\n        \"\"\"Parse unicode east-asian width tables.\"\"\"\n        version, date, values = None, None, []\n        print(\"parsing {} ..\".format(fname))\n        for line in open(fname, 'rb'):\n            uline = line.decode('utf-8')\n            if version is None:\n                version = uline.split(None, 1)[1].rstrip()\n                continue\n            elif date is None:\n                date = uline.split(':', 1)[1].rstrip()\n                continue\n            if uline.startswith('#') or not uline.lstrip():\n                continue\n            addrs, details = uline.split(';', 1)\n            if any(details.startswith(property)\n                   for property in properties):\n                start, stop = addrs, addrs\n                if '..' in addrs:\n                    start, stop = addrs.split('..')\n                values.extend(range(int(start, 16), int(stop, 16) + 1))\n        return version, date, sorted(values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse unicode category tables.", "response": "def _parse_category(fname, categories):\n        \"\"\"Parse unicode category tables.\"\"\"\n        version, date, values = None, None, []\n        print(\"parsing {} ..\".format(fname))\n        for line in open(fname, 'rb'):\n            uline = line.decode('utf-8')\n            if version is None:\n                version = uline.split(None, 1)[1].rstrip()\n                continue\n            elif date is None:\n                date = uline.split(':', 1)[1].rstrip()\n                continue\n            if uline.startswith('#') or not uline.lstrip():\n                continue\n            addrs, details = uline.split(';', 1)\n            addrs, details = addrs.rstrip(), details.lstrip()\n            if any(details.startswith('{} #'.format(value))\n                   for value in categories):\n                start, stop = addrs, addrs\n                if '..' in addrs:\n                    start, stop = addrs.split('..')\n                values.extend(range(int(start, 16), int(stop, 16) + 1))\n        return version, date, sorted(values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting combining tables to filesystem as python code.", "response": "def _do_write(fname, variable, version, date, table):\n        \"\"\"Write combining tables to filesystem as python code.\"\"\"\n        # pylint: disable=R0914\n        #         Too many local variables (19/15) (col 4)\n        print(\"writing {} ..\".format(fname))\n        import unicodedata\n        import datetime\n        import string\n        utc_now = datetime.datetime.utcnow()\n        indent = 4\n        with open(fname, 'w') as fout:\n            fout.write(\n                '\"\"\"{variable_proper} table. Created by setup.py.\"\"\"\\n'\n                \"# Generated: {iso_utc}\\n\"\n                \"# Source: {version}\\n\"\n                \"# Date: {date}\\n\"\n                \"{variable} = (\".format(iso_utc=utc_now.isoformat(),\n                                        version=version,\n                                        date=date,\n                                        variable=variable,\n                                        variable_proper=variable.title()))\n            for start, end in table:\n                ucs_start, ucs_end = unichr(start), unichr(end)\n                hex_start, hex_end = ('0x{0:04x}'.format(start),\n                                      '0x{0:04x}'.format(end))\n                try:\n                    name_start = string.capwords(unicodedata.name(ucs_start))\n                except ValueError:\n                    name_start = u''\n                try:\n                    name_end = string.capwords(unicodedata.name(ucs_end))\n                except ValueError:\n                    name_end = u''\n                fout.write('\\n' + (' ' * indent))\n                fout.write('({0}, {1},),'.format(hex_start, hex_end))\n                fout.write('  # {0:24s}..{1}'.format(\n                    name_start[:24].rstrip() or '(nil)',\n                    name_end[:24].rstrip()))\n            fout.write('\\n)\\n')\n        print(\"complete.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef report_ucs_msg(ucs, wcwidth_libc, wcwidth_local):\n    ucp = (ucs.encode('unicode_escape')[2:]\n           .decode('ascii')\n           .upper()\n           .lstrip('0'))\n    url = \"http://codepoints.net/U+{}\".format(ucp)\n    name = unicodedata.name(ucs)\n    return (u\"libc,ours={},{} [--o{}o--] name={} val={} {}\"\n            \" \".format(wcwidth_libc, wcwidth_local, ucs, name, ord(ucs), url))", "response": "Return string report of combining character differences."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(using_locale=('en_US', 'UTF-8',)):\n    all_ucs = (ucs for ucs in\n               [unichr(val) for val in range(sys.maxunicode)]\n               if is_named(ucs) and isnt_combining(ucs))\n\n    libc_name = ctypes.util.find_library('c')\n    if not libc_name:\n        raise ImportError(\"Can't find C library.\")\n\n    libc = ctypes.cdll.LoadLibrary(libc_name)\n    libc.wcwidth.argtypes = [ctypes.c_wchar, ]\n    libc.wcwidth.restype = ctypes.c_int\n\n    assert getattr(libc, 'wcwidth', None) is not None\n    assert getattr(libc, 'wcswidth', None) is not None\n\n    locale.setlocale(locale.LC_ALL, using_locale)\n\n    for ucs in all_ucs:\n        try:\n            _is_equal_wcwidth(libc, ucs)\n        except AssertionError as err:\n            print(err)", "response": "This is the main entry point for the unicode table. It loads the entire Unicode table into memory and checks that the Unicode table contains all of the Unicode characters that are not combining characters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate and return options provided by docopt parsing.", "response": "def validate_args(opts):\n    \"\"\"Validate and return options provided by docopt parsing.\"\"\"\n    if opts['--wide'] is None:\n        opts['--wide'] = 2\n    else:\n        assert opts['--wide'] in (\"1\", \"2\"), opts['--wide']\n    if opts['--alignment'] is None:\n        opts['--alignment'] = 'left'\n    else:\n        assert opts['--alignment'] in ('left', 'right'), opts['--alignment']\n    opts['--wide'] = int(opts['--wide'])\n    opts['character_factory'] = WcWideCharacterGenerator\n    if opts['--combining']:\n        opts['character_factory'] = WcCombinedCharacterGenerator\n    return opts"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hint_width(self):\n        return sum((len(self.style.delimiter),\n                    self.wide,\n                    len(self.style.delimiter),\n                    len(u' '),\n                    UCS_PRINTLEN + 2,\n                    len(u' '),\n                    self.style.name_len,))", "response": "Width of a column segment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef head_item(self):\n        delimiter = self.style.attr_minor(self.style.delimiter)\n        hint = self.style.header_hint * self.wide\n        heading = (u'{delimiter}{hint}{delimiter}'\n                   .format(delimiter=delimiter, hint=hint))\n        alignment = lambda *args: (\n            self.term.rjust(*args) if self.style.alignment == 'right' else\n            self.term.ljust(*args))\n        txt = alignment(heading, self.hint_width, self.style.header_fill)\n        return self.style.attr_major(txt)", "response": "Text of a single column heading."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef msg_intro(self):\n        delim = self.style.attr_minor(self.style.delimiter)\n        txt = self.intro_msg_fmt.format(delim=delim).rstrip()\n        return self.term.center(txt)", "response": "Introductory message disabled above heading."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnumber of columns displayed.", "response": "def num_columns(self):\n        \"\"\"Number of columns displayed.\"\"\"\n        if self.term.is_a_tty:\n            return self.term.width // self.hint_width\n        return 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsignaling handler for SIGWINCH.", "response": "def on_resize(self, *args):\n        \"\"\"Signal handler callback for SIGWINCH.\"\"\"\n        # pylint: disable=W0613\n        #         Unused argument 'args'\n        self.screen.style.name_len = min(self.screen.style.name_len,\n                                         self.term.width - 15)\n        assert self.term.width >= self.screen.hint_width, (\n            'Screen to small {}, must be at least {}'.format(\n                self.term.width, self.screen.hint_width))\n        self._set_lastpage()\n        self.dirty = self.STATE_REFRESH"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating value of class attribute last_page.", "response": "def _set_lastpage(self):\n        \"\"\"Calculate value of class attribute ``last_page``.\"\"\"\n        self.last_page = (len(self._page_data) - 1) // self.screen.page_size"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef display_initialize(self):\n        echo(self.term.home + self.term.clear)\n        echo(self.term.move_y(self.term.height // 2))\n        echo(self.term.center('Initializing page data ...').rstrip())\n        flushout()\n\n        if LIMIT_UCS == 0x10000:\n            echo('\\n\\n')\n            echo(self.term.blink_red(self.term.center(\n                'narrow Python build: upperbound value is {n}.'\n                .format(n=LIMIT_UCS)).rstrip()))\n            echo('\\n\\n')\n            flushout()", "response": "Display the initialization message and narrow build warning."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize_page_data(self):\n        if self.term.is_a_tty:\n            self.display_initialize()\n        self.character_generator = self.character_factory(self.screen.wide)\n        page_data = list()\n        while True:\n            try:\n                page_data.append(next(self.character_generator))\n            except StopIteration:\n                break\n        if LIMIT_UCS == 0x10000:\n            echo(self.term.center('press any key.').rstrip())\n            flushout()\n            self.term.inkey(timeout=None)\n        return page_data", "response": "Initialize the page data for the given screen."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef page_data(self, idx, offset):\n        size = self.screen.page_size\n\n        while offset < 0 and idx:\n            offset += size\n            idx -= 1\n        offset = max(0, offset)\n\n        while offset >= size:\n            offset -= size\n            idx += 1\n\n        if idx == self.last_page:\n            offset = 0\n        idx = min(max(0, idx), self.last_page)\n\n        start = (idx * self.screen.page_size) + offset\n        end = start + self.screen.page_size\n        return (idx, offset), self._page_data[start:end]", "response": "Return character data for given page index and offset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run_notty(self, writer):\n        page_idx = page_offset = 0\n        while True:\n            npage_idx, _ = self.draw(writer, page_idx + 1, page_offset)\n            if npage_idx == self.last_page:\n                # page displayed was last page, quit.\n                break\n            page_idx = npage_idx\n            self.dirty = self.STATE_DIRTY\n        return", "response": "Pager run method for terminals that are not a tty."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, writer, reader):\n        self._page_data = self.initialize_page_data()\n        self._set_lastpage()\n        if not self.term.is_a_tty:\n            self._run_notty(writer)\n        else:\n            self._run_tty(writer, reader)", "response": "Runs the pager entry point."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a keystroke and adjust screen parameters.", "response": "def process_keystroke(self, inp, idx, offset):\n        \"\"\"\n        Process keystroke ``inp``, adjusting screen parameters.\n\n        :param inp: return value of Terminal.inkey().\n        :type inp: blessed.keyboard.Keystroke\n        :param idx: page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page.\n        :type offset: int\n        :returns: tuple of next (idx, offset).\n        :rtype: (int, int)\n        \"\"\"\n        if inp.lower() in (u'q', u'Q'):\n            # exit\n            return (-1, -1)\n        self._process_keystroke_commands(inp)\n        idx, offset = self._process_keystroke_movement(inp, idx, offset)\n        return idx, offset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing keystrokes that issue commands (side effects ).", "response": "def _process_keystroke_commands(self, inp):\n        \"\"\"Process keystrokes that issue commands (side effects).\"\"\"\n        if inp in (u'1', u'2'):\n            # chose 1 or 2-character wide\n            if int(inp) != self.screen.wide:\n                self.screen.wide = int(inp)\n                self.on_resize(None, None)\n        elif inp in (u'_', u'-'):\n            # adjust name length -2\n            nlen = max(1, self.screen.style.name_len - 2)\n            if nlen != self.screen.style.name_len:\n                self.screen.style.name_len = nlen\n                self.on_resize(None, None)\n        elif inp in (u'+', u'='):\n            # adjust name length +2\n            nlen = min(self.term.width - 8, self.screen.style.name_len + 2)\n            if nlen != self.screen.style.name_len:\n                self.screen.style.name_len = nlen\n                self.on_resize(None, None)\n        elif inp == u'2' and self.screen.wide != 2:\n            # change 2 or 1-cell wide view\n            self.screen.wide = 2\n            self.on_resize(None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_keystroke_movement(self, inp, idx, offset):\n        term = self.term\n        if inp in (u'y', u'k') or inp.code in (term.KEY_UP,):\n            # scroll backward 1 line\n            idx, offset = (idx, offset - self.screen.num_columns)\n        elif inp in (u'e', u'j') or inp.code in (term.KEY_ENTER,\n                                                 term.KEY_DOWN,):\n            # scroll forward 1 line\n            idx, offset = (idx, offset + self.screen.num_columns)\n        elif inp in (u'f', u' ') or inp.code in (term.KEY_PGDOWN,):\n            # scroll forward 1 page\n            idx, offset = (idx + 1, offset)\n        elif inp == u'b' or inp.code in (term.KEY_PGUP,):\n            # scroll backward 1 page\n            idx, offset = (max(0, idx - 1), offset)\n        elif inp.code in (term.KEY_SDOWN,):\n            # scroll forward 10 pages\n            idx, offset = (max(0, idx + 10), offset)\n        elif inp.code in (term.KEY_SUP,):\n            # scroll forward 10 pages\n            idx, offset = (max(0, idx - 10), offset)\n        elif inp.code == term.KEY_HOME:\n            # top\n            idx, offset = (0, 0)\n        elif inp.code == term.KEY_END:\n            # bottom\n            idx, offset = (self.last_page, 0)\n        return idx, offset", "response": "Process keystrokes that adjust index and offset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndrawing the current page view to writer.", "response": "def draw(self, writer, idx, offset):\n        \"\"\"\n        Draw the current page view to ``writer``.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :type writer: callable\n        :param idx: current page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page.\n        :type offset: int\n        :returns: tuple of next (idx, offset).\n        :rtype: (int, int)\n        \"\"\"\n        # as our screen can be resized while we're mid-calculation,\n        # our self.dirty flag can become re-toggled; because we are\n        # not re-flowing our pagination, we must begin over again.\n        while self.dirty:\n            self.draw_heading(writer)\n            self.dirty = self.STATE_CLEAN\n            (idx, offset), data = self.page_data(idx, offset)\n            for txt in self.page_view(data):\n                writer(txt)\n        self.draw_status(writer, idx)\n        flushout()\n        return idx, offset"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw_heading(self, writer):\n        if self.dirty == self.STATE_REFRESH:\n            writer(u''.join(\n                (self.term.home, self.term.clear,\n                 self.screen.msg_intro, '\\n',\n                 self.screen.header, '\\n',)))\n            return True", "response": "Conditionally redraw screen when class attribute is valued REFRESH."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw_status(self, writer, idx):\n        if self.term.is_a_tty:\n            writer(self.term.hide_cursor())\n            style = self.screen.style\n            writer(self.term.move(self.term.height - 1))\n            if idx == self.last_page:\n                last_end = u'(END)'\n            else:\n                last_end = u'/{0}'.format(self.last_page)\n            txt = (u'Page {idx}{last_end} - '\n                   u'{q} to quit, [keys: {keyset}]'\n                   .format(idx=style.attr_minor(u'{0}'.format(idx)),\n                           last_end=style.attr_major(last_end),\n                           keyset=style.attr_major('kjfb12-='),\n                           q=style.attr_minor(u'q')))\n            writer(self.term.center(txt).rstrip())", "response": "Conditionally draw status bar when output terminal is a tty."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef page_view(self, data):\n        if self.term.is_a_tty:\n            yield self.term.move(self.screen.row_begins, 0)\n        # sequence clears to end-of-line\n        clear_eol = self.term.clear_eol\n        # sequence clears to end-of-screen\n        clear_eos = self.term.clear_eos\n\n        # track our current column and row, where column is\n        # the whole segment of unicode value text, and draw\n        # only self.screen.num_columns before end-of-line.\n        #\n        # use clear_eol at end of each row to erase over any\n        # \"ghosted\" text, and clear_eos at end of screen to\n        # clear the same, especially for the final page which\n        # is often short.\n        col = 0\n        for ucs, name in data:\n            val = self.text_entry(ucs, name)\n            col += 1\n            if col == self.screen.num_columns:\n                col = 0\n                if self.term.is_a_tty:\n                    val = u''.join((val, clear_eol, u'\\n'))\n                else:\n                    val = u''.join((val.rstrip(), u'\\n'))\n            yield val\n\n        if self.term.is_a_tty:\n            yield u''.join((clear_eol, u'\\n', clear_eos))", "response": "Generator yields unicode text to be displayed for the current unicode pageview."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef text_entry(self, ucs, name):\n        style = self.screen.style\n        if len(name) > style.name_len:\n            idx = max(0, style.name_len - len(style.continuation))\n            name = u''.join((name[:idx], style.continuation if idx else u''))\n        if style.alignment == 'right':\n            fmt = u' '.join(('0x{val:0>{ucs_printlen}x}',\n                             '{name:<{name_len}s}',\n                             '{delimiter}{ucs}{delimiter}'\n                             ))\n        else:\n            fmt = u' '.join(('{delimiter}{ucs}{delimiter}',\n                             '0x{val:0>{ucs_printlen}x}',\n                             '{name:<{name_len}s}'))\n        delimiter = style.attr_minor(style.delimiter)\n        if len(ucs) != 1:\n            # determine display of combining characters\n            val = ord(ucs[1])\n            # a combining character displayed of any fg color\n            # will reset the foreground character of the cell\n            # combined with (iTerm2, OSX).\n            disp_ucs = style.attr_major(ucs[0:2])\n            if len(ucs) > 2:\n                disp_ucs += ucs[2]\n        else:\n            # non-combining\n            val = ord(ucs)\n            disp_ucs = style.attr_major(ucs)\n\n        return fmt.format(name_len=style.name_len,\n                          ucs_printlen=UCS_PRINTLEN,\n                          delimiter=delimiter,\n                          name=name,\n                          ucs=disp_ucs,\n                          val=val)", "response": "Display a single column segment row describing a single unicode point."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun command cmd and return stdout or stderr if ret_err is True", "response": "def back_tick(cmd, ret_err=False, as_str=True, raise_err=None):\n    \"\"\" Run command `cmd`, return stdout, or stdout, stderr if `ret_err`\n\n    Roughly equivalent to ``check_output`` in Python 2.7\n\n    Parameters\n    ----------\n    cmd : sequence\n        command to execute\n    ret_err : bool, optional\n        If True, return stderr in addition to stdout.  If False, just return\n        stdout\n    as_str : bool, optional\n        Whether to decode outputs to unicode string on exit.\n    raise_err : None or bool, optional\n        If True, raise RuntimeError for non-zero return code. If None, set to\n        True when `ret_err` is False, False if `ret_err` is True\n\n    Returns\n    -------\n    out : str or tuple\n        If `ret_err` is False, return stripped string containing stdout from\n        `cmd`.  If `ret_err` is True, return tuple of (stdout, stderr) where\n        ``stdout`` is the stripped stdout, and ``stderr`` is the stripped\n        stderr.\n\n    Raises\n    ------\n    Raises RuntimeError if command returns non-zero exit code and `raise_err`\n    is True\n    \"\"\"\n    if raise_err is None:\n        raise_err = False if ret_err else True\n    cmd_is_seq = isinstance(cmd, (list, tuple))\n    proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=not cmd_is_seq)\n    out, err = proc.communicate()\n    retcode = proc.returncode\n    cmd_str = ' '.join(cmd) if cmd_is_seq else cmd\n    if retcode is None:\n        proc.terminate()\n        raise RuntimeError(cmd_str + ' process did not terminate')\n    if raise_err and retcode != 0:\n        raise RuntimeError('{0} returned code {1} with error {2}'.format(\n                           cmd_str, retcode, err.decode('latin-1')))\n    out = out.strip()\n    if as_str:\n        out = out.decode('latin-1')\n    if not ret_err:\n        return out\n    err = err.strip()\n    if as_str:\n        err = err.decode('latin-1')\n    return out, err"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of unique elements in the given sequence ordered by the index of the element in the sequence.", "response": "def unique_by_index(sequence):\n    \"\"\" unique elements in `sequence` in the order in which they occur\n\n    Parameters\n    ----------\n    sequence : iterable\n\n    Returns\n    -------\n    uniques : list\n        unique elements of sequence, ordered by the order in which the element\n        occurs in `sequence`\n    \"\"\"\n    uniques = []\n    for element in sequence:\n        if element not in uniques:\n            uniques.append(element)\n    return uniques"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn install names from library named in filename.", "response": "def get_install_names(filename):\n    \"\"\" Return install names from library named in `filename`\n\n    Returns tuple of install names\n\n    tuple will be empty if no install names, or if this is not an object file.\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n\n    Returns\n    -------\n    install_names : tuple\n        tuple of install names for library `filename`\n    \"\"\"\n    lines = _cmd_out_err(['otool', '-L', filename])\n    if not _line0_says_object(lines[0], filename):\n        return ()\n    names = tuple(parse_install_name(line)[0] for line in lines[1:])\n    install_id = get_install_id(filename)\n    if not install_id is None:\n        assert names[0] == install_id\n        return names[1:]\n    return names"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_install_id(filename):\n    lines = _cmd_out_err(['otool', '-D', filename])\n    if not _line0_says_object(lines[0], filename):\n        return None\n    if len(lines) == 1:\n        return None\n    if len(lines) != 2:\n        raise InstallNameError('Unexpected otool output ' + '\\n'.join(lines))\n    return lines[1].strip()", "response": "Get install id from library named in filename."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_install_name(filename, oldname, newname):\n    names = get_install_names(filename)\n    if oldname not in names:\n        raise InstallNameError('{0} not in install names for {1}'.format(\n            oldname, filename))\n    back_tick(['install_name_tool', '-change', oldname, newname, filename])", "response": "Set install name oldname to newname in library filename\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_install_id(filename, install_id):\n    if get_install_id(filename) is None:\n        raise InstallNameError('{0} has no install id'.format(filename))\n    back_tick(['install_name_tool', '-id', install_id, filename])", "response": "Set install id for library named in filename"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_rpaths(filename):\n    try:\n        lines = _cmd_out_err(['otool', '-l', filename])\n    except RuntimeError:\n        return ()\n    if not _line0_says_object(lines[0], filename):\n        return ()\n    lines = [line.strip() for line in lines]\n    paths = []\n    line_no = 1\n    while line_no < len(lines):\n        line = lines[line_no]\n        line_no += 1\n        if line != 'cmd LC_RPATH':\n            continue\n        cmdsize, path = lines[line_no:line_no+2]\n        assert cmdsize.startswith('cmdsize ')\n        paths.append(RPATH_RE.match(path).groups()[0])\n        line_no += 2\n    return tuple(paths)", "response": "Return a tuple of rpaths from the library filename."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dir2zip(in_dir, zip_fname):\n    z = zipfile.ZipFile(zip_fname, 'w',\n                        compression=zipfile.ZIP_DEFLATED)\n    for root, dirs, files in os.walk(in_dir):\n        for file in files:\n            in_fname = pjoin(root, file)\n            in_stat = os.stat(in_fname)\n            # Preserve file permissions, but allow copy\n            info = zipfile.ZipInfo(in_fname)\n            info.filename = relpath(in_fname, in_dir)\n            if os.path.sep == '\\\\':\n                # Make the path unix friendly on windows.\n                # PyPI won't accept wheels with windows path separators\n                info.filename = relpath(in_fname, in_dir).replace('\\\\', '/')\n            # Set time from modification time\n            info.date_time = time.localtime(in_stat.st_mtime)\n            # See https://stackoverflow.com/questions/434641/how-do-i-set-permissions-attributes-on-a-file-in-a-zip-file-using-pythons-zip/48435482#48435482\n            # Also set regular file permissions\n            perms = stat.S_IMODE(in_stat.st_mode) | stat.S_IFREG\n            info.external_attr = perms << 16\n            with open_readable(in_fname, 'rb') as fobj:\n                contents = fobj.read()\n            z.writestr(info, contents, zipfile.ZIP_DEFLATED)\n    z.close()", "response": "This function takes a directory and creates a zip file with the contents of it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_package_dirs(root_path):\n    package_sdirs = set()\n    for entry in os.listdir(root_path):\n        fname = entry if root_path == '.' else pjoin(root_path, entry)\n        if isdir(fname) and exists(pjoin(fname, '__init__.py')):\n            package_sdirs.add(fname)\n    return package_sdirs", "response": "Find python package directories in directory root_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cmp_contents(filename1, filename2):\n    with open_readable(filename1, 'rb') as fobj:\n        contents1 = fobj.read()\n    with open_readable(filename2, 'rb') as fobj:\n        contents2 = fobj.read()\n    return contents1 == contents2", "response": "Returns True if the contents of the files of filename1 and filename2 are the same as the contents of filename2."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_archs(libname):\n    if not exists(libname):\n        raise RuntimeError(libname + \" is not a file\")\n    try:\n        stdout = back_tick(['lipo', '-info', libname])\n    except RuntimeError:\n        return frozenset()\n    lines = [line.strip() for line in stdout.split('\\n') if line.strip()]\n    # For some reason, output from lipo -info on .a file generates this line\n    if lines[0] == \"input file {0} is not a fat file\".format(libname):\n        line = lines[1]\n    else:\n        assert len(lines) == 1\n        line = lines[0]\n    for reggie in (\n        'Non-fat file: {0} is architecture: (.*)'.format(libname),\n        'Architectures in the fat file: {0} are: (.*)'.format(libname)):\n        reggie = re.compile(reggie)\n        match = reggie.match(line)\n        if not match is None:\n            return frozenset(match.groups()[0].split(' '))\n    raise ValueError(\"Unexpected output: '{0}' for {1}\".format(\n        stdout, libname))", "response": "Return architectures from library libname."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates the signature of a file.", "response": "def validate_signature(filename):\n    \"\"\" Remove invalid signatures from a binary file\n\n    If the file signature is missing or valid then it will be ignored\n\n    Invalid signatures are replaced with an ad-hoc signature.  This is the\n    closest you can get to removing a signature on MacOS\n\n    Parameters\n    ----------\n    filename : str\n        Filepath to a binary file\n    \"\"\"\n    out, err = back_tick(['codesign', '--verify', filename],\n                         ret_err=True, as_str=True, raise_err=False)\n    if not err:\n        return # The existing signature is valid\n    if 'code object is not signed at all' in err:\n        return # File has no signature, and adding a new one isn't necessary\n\n    # This file's signature is invalid and needs to be replaced\n    replace_signature(filename, '-')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a relative version of a path", "response": "def os_path_relpath(path, start=os.path.curdir):\n    \"\"\"Return a relative version of a path\"\"\"\n\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    start_list = [x for x in os.path.abspath(start).split(os.path.sep) if x]\n    path_list = [x for x in os.path.abspath(path).split(os.path.sep) if x]\n\n    # Work out how much of the filepath is shared by start and path.\n    i = len(os.path.commonprefix([start_list, path_list]))\n\n    rel_list = [os.path.pardir] * (len(start_list)-i) + path_list[i:]\n    if not rel_list:\n        return os.path.curdir\n    return os.path.join(*rel_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfuses all files in from_tree into to_tree.", "response": "def fuse_trees(to_tree, from_tree, lib_exts=('.so', '.dylib', '.a')):\n    \"\"\" Fuse path `from_tree` into path `to_tree`\n\n    For each file in `from_tree` - check for library file extension (in\n    `lib_exts` - if present, check if there is a file with matching relative\n    path in `to_tree`, if so, use :func:`delocate.tools.lipo_fuse` to fuse the\n    two libraries together and write into `to_tree`.  If any of these\n    conditions are not met, just copy the file from `from_tree` to `to_tree`.\n\n    Parameters\n    ---------\n    to_tree : str\n        path of tree to fuse into (update into)\n    from_tree : str\n        path of tree to fuse from (update from)\n    lib_exts : sequence, optional\n        filename extensions for libraries\n    \"\"\"\n    for from_dirpath, dirnames, filenames in os.walk(from_tree):\n        to_dirpath = pjoin(to_tree, relpath(from_dirpath, from_tree))\n        # Copy any missing directories in to_path\n        for dirname in tuple(dirnames):\n            to_path = pjoin(to_dirpath, dirname)\n            if not exists(to_path):\n                from_path = pjoin(from_dirpath, dirname)\n                shutil.copytree(from_path, to_path)\n                # If copying, don't further analyze this directory\n                dirnames.remove(dirname)\n        for fname in filenames:\n            root, ext = splitext(fname)\n            from_path = pjoin(from_dirpath, fname)\n            to_path = pjoin(to_dirpath, fname)\n            if not exists(to_path):\n                _copyfile(from_path, to_path)\n            elif cmp_contents(from_path, to_path):\n                pass\n            elif ext in lib_exts:\n                # existing lib that needs fuse\n                lipo_fuse(from_path, to_path, to_path)\n            else:\n                # existing not-lib file not identical to source\n                _copyfile(from_path, to_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fuse_wheels(to_wheel, from_wheel, out_wheel):\n    to_wheel, from_wheel, out_wheel = [\n        abspath(w) for w in (to_wheel, from_wheel, out_wheel)]\n    with InTemporaryDirectory():\n        zip2dir(to_wheel, 'to_wheel')\n        zip2dir(from_wheel, 'from_wheel')\n        fuse_trees('to_wheel', 'from_wheel')\n        rewrite_record('to_wheel')\n        dir2zip('to_wheel', out_wheel)", "response": "Fuse from_wheel into to_wheel write to out_wheel"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove needed libraries in lib_dict into lib_path.", "response": "def delocate_tree_libs(lib_dict, lib_path, root_path):\n    \"\"\" Move needed libraries in `lib_dict` into `lib_path`\n\n    `lib_dict` has keys naming libraries required by the files in the\n    corresponding value.  Call the keys, \"required libs\".  Call the values\n    \"requiring objects\".\n\n    Copy all the required libs to `lib_path`.  Fix up the rpaths and install\n    names in the requiring objects to point to these new copies.\n\n    Exception: required libs within the directory tree pointed to by\n    `root_path` stay where they are, but we modify requiring objects to use\n    relative paths to these libraries.\n\n    Parameters\n    ----------\n    lib_dict : dict\n        Dictionary with (key, value) pairs of (``depended_lib_path``,\n        ``dependings_dict``) (see :func:`libsana.tree_libs`)\n    lib_path : str\n        Path in which to store copies of libs referred to in keys of\n        `lib_dict`.  Assumed to exist\n    root_path : str, optional\n        Root directory of tree analyzed in `lib_dict`.  Any required\n        library within the subtrees of `root_path` does not get copied, but\n        libraries linking to it have links adjusted to use relative path to\n        this library.\n\n    Returns\n    -------\n    copied_libs : dict\n        Filtered `lib_dict` dict containing only the (key, value) pairs from\n        `lib_dict` where the keys are the libraries copied to `lib_path``.\n    \"\"\"\n    copied_libs = {}\n    delocated_libs = set()\n    copied_basenames = set()\n    rp_root_path = realpath(root_path)\n    rp_lib_path = realpath(lib_path)\n    # Test for errors first to avoid getting half-way through changing the tree\n    for required, requirings in lib_dict.items():\n        if required.startswith('@'): # assume @rpath etc are correct\n            # But warn, because likely they are not\n            warnings.warn('Not processing required path {0} because it '\n                          'begins with @'.format(required))\n            continue\n        r_ed_base = basename(required)\n        if relpath(required, rp_root_path).startswith('..'):\n            # Not local, plan to copy\n            if r_ed_base in copied_basenames:\n                raise DelocationError('Already planning to copy library with '\n                                      'same basename as: ' + r_ed_base)\n            if not exists(required):\n                raise DelocationError('library \"{0}\" does not exist'.format(\n                    required))\n            copied_libs[required] = requirings\n            copied_basenames.add(r_ed_base)\n        else: # Is local, plan to set relative loader_path\n            delocated_libs.add(required)\n    # Modify in place now that we've checked for errors\n    for required in copied_libs:\n        shutil.copy(required, lib_path)\n        # Set rpath and install names for this copied library\n        for requiring, orig_install_name in lib_dict[required].items():\n            req_rel = relpath(rp_lib_path, dirname(requiring))\n            set_install_name(requiring, orig_install_name,\n                             '@loader_path/{0}/{1}'.format(\n                                 req_rel, basename(required)))\n    for required in delocated_libs:\n        # Set relative path for local library\n        for requiring, orig_install_name in lib_dict[required].items():\n            req_rel = relpath(required, dirname(requiring))\n            set_install_name(requiring, orig_install_name,\n                             '@loader_path/' + req_rel)\n    return copied_libs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nanalyzes `lib_path` for library dependencies and copy libraries `lib_path` is a directory containing libraries. The libraries might themselves have dependencies. This function analyzes the dependencies and copies library dependencies that match the filter `copy_filt_func`. It also adjusts the depending libraries to use the copy. It keeps iterating over `lib_path` until all matching dependencies (of dependencies of dependencies ...) have been copied. Parameters ---------- lib_path : str Directory containing libraries copy_filt_func : None or callable, optional If None, copy any library that found libraries depend on. If callable, called on each depended library name; copy where ``copy_filt_func(libname)`` is True, don't copy otherwise copied_libs : dict Dict with (key, value) pairs of (``copied_lib_path``, ``dependings_dict``) where ``copied_lib_path`` is the canonical path of a library that has been copied to `lib_path`, and ``dependings_dict`` is a dictionary with (key, value) pairs of (``depending_lib_path``, ``install_name``). ``depending_lib_path`` is the canonical path of the library depending on ``copied_lib_path``, ``install_name`` is the name that ``depending_lib_path`` uses to refer to ``copied_lib_path`` (in its install names). Returns ------- copied_libs : dict Input `copied_libs` dict with any extra libraries and / or dependencies added.", "response": "def copy_recurse(lib_path, copy_filt_func = None, copied_libs = None):\n    \"\"\" Analyze `lib_path` for library dependencies and copy libraries\n\n    `lib_path` is a directory containing libraries.  The libraries might\n    themselves have dependencies.  This function analyzes the dependencies and\n    copies library dependencies that match the filter `copy_filt_func`. It also\n    adjusts the depending libraries to use the copy. It keeps iterating over\n    `lib_path` until all matching dependencies (of dependencies of dependencies\n    ...) have been copied.\n\n    Parameters\n    ----------\n    lib_path : str\n        Directory containing libraries\n    copy_filt_func : None or callable, optional\n        If None, copy any library that found libraries depend on.  If callable,\n        called on each depended library name; copy where\n        ``copy_filt_func(libname)`` is True, don't copy otherwise\n    copied_libs : dict\n        Dict with (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``) where ``copied_lib_path`` is the canonical path of\n        a library that has been copied to `lib_path`, and ``dependings_dict``\n        is a dictionary with (key, value) pairs of (``depending_lib_path``,\n        ``install_name``).  ``depending_lib_path`` is the canonical path of the\n        library depending on ``copied_lib_path``, ``install_name`` is the name\n        that ``depending_lib_path`` uses to refer to ``copied_lib_path`` (in\n        its install names).\n\n    Returns\n    -------\n    copied_libs : dict\n        Input `copied_libs` dict with any extra libraries and / or dependencies\n        added.\n    \"\"\"\n    if copied_libs is None:\n        copied_libs = {}\n    else:\n        copied_libs = dict(copied_libs)\n    done = False\n    while not done:\n        in_len = len(copied_libs)\n        _copy_required(lib_path, copy_filt_func, copied_libs)\n        done = len(copied_libs) == in_len\n    return copied_libs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncopying libraries required for files in lib_path to lib_path.", "response": "def _copy_required(lib_path, copy_filt_func, copied_libs):\n    \"\"\" Copy libraries required for files in `lib_path` to `lib_path`\n\n    Augment `copied_libs` dictionary with any newly copied libraries, modifying\n    `copied_libs` in-place - see Notes.\n\n    This is one pass of ``copy_recurse``\n\n    Parameters\n    ----------\n    lib_path : str\n        Directory containing libraries\n    copy_filt_func : None or callable, optional\n        If None, copy any library that found libraries depend on.  If callable,\n        called on each library name; copy where ``copy_filt_func(libname)`` is\n        True, don't copy otherwise\n    copied_libs : dict\n        See :func:`copy_recurse` for definition.\n\n    Notes\n    -----\n    If we need to copy another library, add that (``depended_lib_path``,\n    ``dependings_dict``) to `copied_libs`.  ``dependings_dict`` has (key,\n    value) pairs of (``depending_lib_path``, ``install_name``).\n    ``depending_lib_path`` will be the original (canonical) library name, not\n    the copy in ``lib_path``.\n\n    Sometimes we copy a library, that further depends on a library we have\n    already copied. In this case update ``copied_libs[depended_lib]`` with the\n    extra dependency (as well as fixing up the install names for the depending\n    library).\n\n    For example, imagine we've start with a lib path like this::\n\n        my_lib_path/\n            libA.dylib\n            libB.dylib\n\n    Our input `copied_libs` has keys ``/sys/libA.dylib``, ``/sys/libB.lib``\n    telling us we previously copied those guys from the ``/sys`` folder.\n\n    On a first pass, we discover that ``libA.dylib`` depends on\n    ``/sys/libC.dylib``, so we copy that.\n\n    On a second pass, we discover now that ``libC.dylib`` also depends on\n    ``/sys/libB.dylib``.  `copied_libs` tells us that we already have a copy of\n    ``/sys/libB.dylib``, so we fix our copy of `libC.dylib`` to point to\n    ``my_lib_path/libB.dylib`` and add ``/sys/libC.dylib`` as a\n    ``dependings_dict`` entry for ``copied_libs['/sys/libB.dylib']``\n    \"\"\"\n    # Paths will be prepended with `lib_path`\n    lib_dict = tree_libs(lib_path)\n    # Map library paths after copy ('copied') to path before copy ('orig')\n    rp_lp = realpath(lib_path)\n    copied2orig = dict((pjoin(rp_lp, basename(c)), c) for c in copied_libs)\n    for required, requirings in lib_dict.items():\n        if not copy_filt_func is None and not copy_filt_func(required):\n            continue\n        if required.startswith('@'):\n            # May have been processed by us, or have some rpath, loader_path of\n            # its own. Either way, leave alone\n            continue\n        # Requiring names may well be the copies in lib_path.  Replace the copy\n        # names with the original names for entry into `copied_libs`\n        procd_requirings = {}\n        # Set requiring lib install names to point to local copy\n        for requiring, orig_install_name in requirings.items():\n            set_install_name(requiring,\n                             orig_install_name,\n                             '@loader_path/' + basename(required))\n            # Make processed version of ``dependings_dict``\n            mapped_requiring = copied2orig.get(requiring, requiring)\n            procd_requirings[mapped_requiring] = orig_install_name\n        if required in copied_libs:\n            # Have copied this already, add any new requirings\n            copied_libs[required].update(procd_requirings)\n            continue\n        # Haven't see this one before, add entry to copied_libs\n        out_path = pjoin(lib_path, basename(required))\n        if exists(out_path):\n            raise DelocationError(out_path + ' already exists')\n        shutil.copy(required, lib_path)\n        copied2orig[out_path] = required\n        copied_libs[required] = procd_requirings"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies required libraries for files in tree_path into lib_path.", "response": "def delocate_path(tree_path, lib_path,\n                  lib_filt_func = None,\n                  copy_filt_func = filter_system_libs):\n    \"\"\" Copy required libraries for files in `tree_path` into `lib_path`\n\n    Parameters\n    ----------\n    tree_path : str\n        Root path of tree to search for required libraries\n    lib_path : str\n        Directory into which we copy required libraries\n    lib_filt_func : None or str or callable, optional\n        If None, inspect all files for dependencies on dynamic libraries. If\n        callable, accepts filename as argument, returns True if we should\n        inspect the file, False otherwise. If str == \"dylibs-only\" then inspect\n        only files with known dynamic library extensions (``.dylib``, ``.so``).\n    copy_filt_func : None or callable, optional\n        If callable, called on each library name detected as a dependency; copy\n        where ``copy_filt_func(libname)`` is True, don't copy otherwise.\n        Default is callable rejecting only libraries beginning with\n        ``/usr/lib`` or ``/System``.  None means copy all libraries. This will\n        usually end up copying large parts of the system run-time.\n\n    Returns\n    -------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that was copied into `lib_sdir` of the wheel packages, and\n        ``dependings_dict`` is a dictionary with key, value pairs where the key\n        is a file in the path depending on ``copied_lib_path``, and the value\n        is the ``install_name`` of ``copied_lib_path`` in the depending\n        library.\n    \"\"\"\n    if lib_filt_func == \"dylibs-only\":\n        lib_filt_func = _dylibs_only\n    if not exists(lib_path):\n        os.makedirs(lib_path)\n    lib_dict = tree_libs(tree_path, lib_filt_func)\n    if not copy_filt_func is None:\n        lib_dict = dict((key, value) for key, value in lib_dict.items()\n                        if copy_filt_func(key))\n    copied = delocate_tree_libs(lib_dict, lib_path, tree_path)\n    return copy_recurse(lib_path, copy_filt_func, copied)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging lib_dict d2 into lib_dict d1.", "response": "def _merge_lib_dict(d1, d2):\n    \"\"\" Merges lib_dict `d2` into lib_dict `d1`\n    \"\"\"\n    for required, requirings in d2.items():\n        if required in d1:\n            d1[required].update(requirings)\n        else:\n            d1[required] = requirings\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy required libraries from in_wheel to out_wheel.", "response": "def delocate_wheel(in_wheel,\n                   out_wheel = None,\n                   lib_sdir = '.dylibs',\n                   lib_filt_func = None,\n                   copy_filt_func = filter_system_libs,\n                   require_archs = None,\n                   check_verbose = False,\n                  ):\n    \"\"\" Update wheel by copying required libraries to `lib_sdir` in wheel\n\n    Create `lib_sdir` in wheel tree only if we are copying one or more\n    libraries.\n\n    If `out_wheel` is None (the default), overwrite the wheel `in_wheel`\n    in-place.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to process\n    out_wheel : None or str\n        Filename of processed wheel to write.  If None, overwrite `in_wheel`\n    lib_sdir : str, optional\n        Subdirectory name in wheel package directory (or directories) to store\n        needed libraries.\n    lib_filt_func : None or str or callable, optional\n        If None, inspect all files for dependencies on dynamic libraries. If\n        callable, accepts filename as argument, returns True if we should\n        inspect the file, False otherwise. If str == \"dylibs-only\" then inspect\n        only files with known dynamic library extensions (``.dylib``, ``.so``).\n    copy_filt_func : None or callable, optional\n        If callable, called on each library name detected as a dependency; copy\n        where ``copy_filt_func(libname)`` is True, don't copy otherwise.\n        Default is callable rejecting only libraries beginning with\n        ``/usr/lib`` or ``/System``.  None means copy all libraries. This will\n        usually end up copying large parts of the system run-time.\n    require_archs : None or str or sequence, optional\n        If None, do no checks of architectures in libraries.  If sequence,\n        sequence of architectures (output from ``lipo -info``) that every\n        library in the wheels should have (e.g. ``['x86_64, 'i386']``). An\n        empty sequence results in checks that depended libraries have the same\n        archs as depending libraries.  If string, either \"intel\" (corresponds\n        to sequence ``['x86_64, 'i386']``) or name of required architecture\n        (e.g \"i386\" or \"x86_64\").\n    check_verbose : bool, optional\n        If True, print warning messages about missing required architectures\n\n    Returns\n    -------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that was copied into `lib_sdir` of the wheel packages, and\n        ``dependings_dict`` is a dictionary with key, value pairs where the key\n        is a path in the wheel depending on ``copied_lib_path``, and the value\n        is the ``install_name`` of ``copied_lib_path`` in the depending\n        library. The filenames in the keys are relative to the wheel root path.\n    \"\"\"\n    if lib_filt_func == \"dylibs-only\":\n        lib_filt_func = _dylibs_only\n    in_wheel = abspath(in_wheel)\n    if out_wheel is None:\n        out_wheel = in_wheel\n    else:\n        out_wheel = abspath(out_wheel)\n    in_place = in_wheel == out_wheel\n    with TemporaryDirectory() as tmpdir:\n        all_copied = {}\n        wheel_dir = realpath(pjoin(tmpdir, 'wheel'))\n        zip2dir(in_wheel, wheel_dir)\n        for package_path in find_package_dirs(wheel_dir):\n            lib_path = pjoin(package_path, lib_sdir)\n            lib_path_exists = exists(lib_path)\n            copied_libs = delocate_path(package_path, lib_path,\n                                        lib_filt_func, copy_filt_func)\n            if copied_libs and lib_path_exists:\n                raise DelocationError(\n                    '{0} already exists in wheel but need to copy '\n                    '{1}'.format(lib_path, '; '.join(copied_libs)))\n            if len(os.listdir(lib_path)) == 0:\n                shutil.rmtree(lib_path)\n            # Check architectures\n            if not require_archs is None:\n                stop_fast = not check_verbose\n                bads = check_archs(copied_libs, require_archs, stop_fast)\n                if len(bads) != 0:\n                    if check_verbose:\n                        print(bads_report(bads, pjoin(tmpdir, 'wheel')))\n                    raise DelocationError(\n                        \"Some missing architectures in wheel\")\n            # Change install ids to be unique within Python space\n            install_id_root = (DLC_PREFIX +\n                               relpath(package_path, wheel_dir) +\n                               '/')\n            for lib in copied_libs:\n                lib_base = basename(lib)\n                copied_path = pjoin(lib_path, lib_base)\n                set_install_id(copied_path, install_id_root + lib_base)\n                validate_signature(copied_path)\n            _merge_lib_dict(all_copied, copied_libs)\n        if len(all_copied):\n            rewrite_record(wheel_dir)\n        if len(all_copied) or not in_place:\n            dir2zip(wheel_dir, out_wheel)\n    return stripped_lib_dict(all_copied, wheel_dir + os.path.sep)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch_wheel(in_wheel, patch_fname, out_wheel=None):\n    in_wheel = abspath(in_wheel)\n    patch_fname = abspath(patch_fname)\n    if out_wheel is None:\n        out_wheel = in_wheel\n    else:\n        out_wheel = abspath(out_wheel)\n    if not exists(patch_fname):\n        raise ValueError(\"patch file {0} does not exist\".format(patch_fname))\n    with InWheel(in_wheel, out_wheel):\n        with open(patch_fname, 'rb') as fobj:\n            patch_proc = Popen(['patch', '-p1'],\n                               stdin = fobj,\n                               stdout = PIPE,\n                               stderr = PIPE)\n            stdout, stderr = patch_proc.communicate()\n            if patch_proc.returncode != 0:\n                raise RuntimeError(\"Patch failed with stdout:\\n\" +\n                                   stdout.decode('latin1'))", "response": "Apply - p1 style patch in patch_fname to contents of in_wheel and write out_wheel."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck compatibility of archs in copied_libs dict containing the libraries that are not in the architectures of the depending libraries.", "response": "def check_archs(copied_libs, require_archs=(), stop_fast=False):\n    \"\"\" Check compatibility of archs in `copied_libs` dict\n\n    Parameters\n    ----------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that has been copied during delocation, and ``dependings_dict`` is a\n        dictionary with key, value pairs where the key is a path in the target\n        being delocated (a wheel or path) depending on ``copied_lib_path``, and\n        the value is the ``install_name`` of ``copied_lib_path`` in the\n        depending library.\n    require_archs : str or sequence, optional\n        Architectures we require to be present in all library files in wheel.\n        If an empty sequence, just check that depended libraries do have the\n        architectures of the depending libraries, with no constraints on what\n        these architectures are. If a sequence, then a set of required\n        architectures e.g. ``['i386', 'x86_64']`` to specify dual Intel\n        architectures.  If a string, then a standard architecture name as\n        returned by ``lipo -info`` or the string \"intel\", corresponding to the\n        sequence ``['i386', 'x86_64']``\n    stop_fast : bool, optional\n        Whether to give up collecting errors after the first\n\n    Returns\n    -------\n    bads : set\n        set of length 2 or 3 tuples. A length 2 tuple is of form\n        ``(depending_lib, missing_archs)`` meaning that an arch in\n        `require_archs` was missing from ``depending_lib``.  A length 3 tuple\n        is of form ``(depended_lib, depending_lib, missing_archs)`` where\n        ``depended_lib`` is the filename of the library depended on,\n        ``depending_lib`` is the library depending on ``depending_lib`` and\n        ``missing_archs`` is a set of missing architecture strings giving\n        architectures present in ``depending_lib`` and missing in\n        ``depended_lib``.  An empty set means all architectures were present as\n        required.\n    \"\"\"\n    if isinstance(require_archs, string_types):\n        require_archs = (['i386', 'x86_64'] if require_archs == 'intel'\n                         else [require_archs])\n    require_archs = frozenset(require_archs)\n    bads = []\n    for depended_lib, dep_dict in copied_libs.items():\n        depended_archs = get_archs(depended_lib)\n        for depending_lib, install_name in dep_dict.items():\n            depending_archs = get_archs(depending_lib)\n            all_required = depending_archs | require_archs\n            all_missing = all_required.difference(depended_archs)\n            if len(all_missing) == 0:\n                continue\n            required_missing = require_archs.difference(depended_archs)\n            if len(required_missing):\n                bads.append((depending_lib, required_missing))\n            else:\n                bads.append((depended_lib, depending_lib, all_missing))\n            if stop_fast:\n                return set(bads)\n    return set(bads)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bads_report(bads, path_prefix=None):\n    path_processor = ((lambda x : x) if path_prefix is None\n                      else get_rp_stripper(path_prefix))\n    reports = []\n    for result in bads:\n        if len(result) == 3:\n            depended_lib, depending_lib, missing_archs = result\n            reports.append(\"{0} needs {1} {2} missing from {3}\".format(\n                path_processor(depending_lib),\n                'archs' if len(missing_archs) > 1 else 'arch',\n                ', '.join(sorted(missing_archs)),\n                path_processor(depended_lib)))\n        elif len(result) == 2:\n            depending_lib, missing_archs = result\n            reports.append(\"Required {0} {1} missing from {2}\".format(\n                'archs' if len(missing_archs) > 1 else 'arch',\n                ', '.join(sorted(missing_archs)),\n                path_processor(depending_lib)))\n        else:\n            raise ValueError('Report tuple should be length 2 or 3')\n    return '\\n'.join(sorted(reports))", "response": "Return a nice report of bad architectures in a set of bads."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn analysis of library dependencies within start_path.", "response": "def tree_libs(start_path, filt_func=None):\n    \"\"\" Return analysis of library dependencies within `start_path`\n\n    Parameters\n    ----------\n    start_path : str\n        root path of tree to search for libraries depending on other libraries.\n    filt_func : None or callable, optional\n        If None, inspect all files for library dependencies. If callable,\n        accepts filename as argument, returns True if we should inspect the\n        file, False otherwise.\n\n    Returns\n    -------\n    lib_dict : dict\n        dictionary with (key, value) pairs of (``libpath``,\n        ``dependings_dict``).\n\n        ``libpath`` is canonical (``os.path.realpath``) filename of library, or\n        library name starting with {'@rpath', '@loader_path',\n        '@executable_path'}.\n\n        ``dependings_dict`` is a dict with (key, value) pairs of\n        (``depending_libpath``, ``install_name``), where ``dependings_libpath``\n        is the canonical (``os.path.realpath``) filename of the library\n        depending on ``libpath``, and ``install_name`` is the \"install_name\" by\n        which ``depending_libpath`` refers to ``libpath``.\n\n    Notes\n    -----\n\n    See:\n\n    * https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/dyld.1.html\n    * http://matthew-brett.github.io/pydagogue/mac_runtime_link.html\n    \"\"\"\n    lib_dict = {}\n    for dirpath, dirnames, basenames in os.walk(start_path):\n        for base in basenames:\n            depending_libpath = realpath(pjoin(dirpath, base))\n            if not filt_func is None and not filt_func(depending_libpath):\n                continue\n            rpaths = get_rpaths(depending_libpath)\n            for install_name in get_install_names(depending_libpath):\n                lib_path = (install_name if install_name.startswith('@')\n                            else realpath(install_name))\n                lib_path = resolve_rpath(lib_path, rpaths)\n                if lib_path in lib_dict:\n                    lib_dict[lib_path][depending_libpath] = install_name\n                else:\n                    lib_dict[lib_path] = {depending_libpath: install_name}\n    return lib_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning lib_path with its @rpath resolved.", "response": "def resolve_rpath(lib_path, rpaths):\n    \"\"\" Return `lib_path` with its `@rpath` resolved\n\n    If the `lib_path` doesn't have `@rpath` then it's returned as is.\n\n    If `lib_path` has `@rpath` then returns the first `rpaths`/`lib_path`\n    combination found.  If the library can't be found in `rpaths` then a\n    detailed warning is printed and `lib_path` is returned as is.\n\n    Parameters\n    ----------\n    lib_path : str\n        The path to a library file, which may or may not start with `@rpath`.\n    rpaths : sequence of str\n        A sequence of search paths, usually gotten from a call to `get_rpaths`.\n\n    Returns\n    -------\n    lib_path : str\n        A str with the resolved libraries realpath.\n    \"\"\"\n    if not lib_path.startswith('@rpath/'):\n        return lib_path\n\n    lib_rpath = lib_path.split('/', 1)[1]\n    for rpath in rpaths:\n        rpath_lib = realpath(pjoin(rpath, lib_rpath))\n        if os.path.exists(rpath_lib):\n            return rpath_lib\n\n    warnings.warn(\n        \"Couldn't find {0} on paths:\\n\\t{1}\".format(\n            lib_path,\n            '\\n\\t'.join(realpath(path) for path in rpaths),\n            )\n        )\n    return lib_path"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_prefix_stripper(strip_prefix):\n    n = len(strip_prefix)\n    def stripper(path):\n        return path if not path.startswith(strip_prefix) else path[n:]\n    return stripper", "response": "Returns a function that strips the prefix from the beginning of a string if present."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stripped_lib_dict(lib_dict, strip_prefix):\n    relative_dict = {}\n    stripper = get_prefix_stripper(strip_prefix)\n\n    for lib_path, dependings_dict in lib_dict.items():\n        ding_dict = {}\n        for depending_libpath, install_name in dependings_dict.items():\n            ding_dict[stripper(depending_libpath)] = install_name\n        relative_dict[stripper(lib_path)] = ding_dict\n    return relative_dict", "response": "Return lib_dict with strip_prefix removed from start of paths\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn analysis of library dependencies with a Python wheel.", "response": "def wheel_libs(wheel_fname, filt_func = None):\n    \"\"\" Return analysis of library dependencies with a Python wheel\n\n    Use this routine for a dump of the dependency tree.\n\n    Parameters\n    ----------\n    wheel_fname : str\n        Filename of wheel\n    filt_func : None or callable, optional\n        If None, inspect all files for library dependencies. If callable,\n        accepts filename as argument, returns True if we should inspect the\n        file, False otherwise.\n\n    Returns\n    -------\n    lib_dict : dict\n        dictionary with (key, value) pairs of (``libpath``,\n        ``dependings_dict``).  ``libpath`` is library being depended on,\n        relative to wheel root path if within wheel tree.  ``dependings_dict``\n        is (key, value) of (``depending_lib_path``, ``install_name``).  Again,\n        ``depending_lib_path`` is library relative to wheel root path, if\n        within wheel tree.\n    \"\"\"\n    with TemporaryDirectory() as tmpdir:\n        zip2dir(wheel_fname, tmpdir)\n        lib_dict = tree_libs(tmpdir, filt_func)\n    return stripped_lib_dict(lib_dict, realpath(tmpdir) + os.path.sep)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening a file for CSV files.", "response": "def _open_for_csv(name, mode):\n    \"\"\" Deal with Python 2/3 open API differences \"\"\"\n    if sys.version_info[0] < 3:\n        return open_rw(name, mode + 'b')\n    return open_rw(name, mode, newline='', encoding='utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrewriting RECORD file with hashes for all files in wheel_sdir.", "response": "def rewrite_record(bdist_dir):\n    \"\"\" Rewrite RECORD file with hashes for all files in `wheel_sdir`\n\n    Copied from :method:`wheel.bdist_wheel.bdist_wheel.write_record`\n\n    Will also unsign wheel\n\n    Parameters\n    ----------\n    bdist_dir : str\n        Path of unpacked wheel file\n    \"\"\"\n    info_dirs = glob.glob(pjoin(bdist_dir, '*.dist-info'))\n    if len(info_dirs) != 1:\n        raise WheelToolsError(\"Should be exactly one `*.dist_info` directory\")\n    record_path = pjoin(info_dirs[0], 'RECORD')\n    record_relpath = relpath(record_path, bdist_dir)\n    # Unsign wheel - because we're invalidating the record hash\n    sig_path = pjoin(info_dirs[0], 'RECORD.jws')\n    if exists(sig_path):\n        os.unlink(sig_path)\n\n    def walk():\n        for dir, dirs, files in os.walk(bdist_dir):\n            for f in files:\n                yield pjoin(dir, f)\n\n    def skip(path):\n        \"\"\"Wheel hashes every possible file.\"\"\"\n        return (path == record_relpath)\n\n    with _open_for_csv(record_path, 'w+') as record_file:\n        writer = csv.writer(record_file)\n        for path in walk():\n            relative_path = relpath(path, bdist_dir)\n            if skip(relative_path):\n                hash = ''\n                size = ''\n            else:\n                with open(path, 'rb') as f:\n                    data = f.read()\n                digest = hashlib.sha256(data).digest()\n                hash = 'sha256=' + native(urlsafe_b64encode(digest))\n                size = len(data)\n            path_for_record = relpath(\n                path, bdist_dir).replace(psep, '/')\n            writer.writerow((path_for_record, hash, size))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding platforms to the WHEEL file.", "response": "def add_platforms(in_wheel, platforms, out_path=None, clobber=False):\n    \"\"\" Add platform tags `platforms` to `in_wheel` filename and WHEEL tags\n\n    Add any platform tags in `platforms` that are missing from `in_wheel`\n    filename.\n\n    Add any platform tags in `platforms` that are missing from `in_wheel`\n    ``WHEEL`` file.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to which to add platform tags\n    platforms : iterable\n        platform tags to add to wheel filename and WHEEL tags - e.g.\n        ``('macosx_10_9_intel', 'macosx_10_9_x86_64')\n    out_path : None or str, optional\n        Directory to which to write new wheel.  Default is directory containing\n        `in_wheel`\n    clobber : bool, optional\n        If True, overwrite existing output filename, otherwise raise error\n\n    Returns\n    -------\n    out_wheel : None or str\n        Absolute path of wheel file written, or None if no wheel file written.\n    \"\"\"\n    in_wheel = abspath(in_wheel)\n    out_path = dirname(in_wheel) if out_path is None else abspath(out_path)\n    wf = WheelFile(in_wheel)\n    info_fname = _get_wheelinfo_name(wf)\n    # Check what tags we have\n    in_fname_tags = wf.parsed_filename.groupdict()['plat'].split('.')\n    extra_fname_tags = [tag for tag in platforms if tag not in in_fname_tags]\n    in_wheel_base, ext = splitext(basename(in_wheel))\n    out_wheel_base = '.'.join([in_wheel_base] + list(extra_fname_tags))\n    out_wheel = pjoin(out_path, out_wheel_base + ext)\n    if exists(out_wheel) and not clobber:\n        raise WheelToolsError('Not overwriting {0}; set clobber=True '\n                              'to overwrite'.format(out_wheel))\n    with InWheelCtx(in_wheel) as ctx:\n        info = read_pkg_info(info_fname)\n        if info['Root-Is-Purelib'] == 'true':\n            raise WheelToolsError('Cannot add platforms to pure wheel')\n        in_info_tags = [tag for name, tag in info.items() if name == 'Tag']\n        # Python version, C-API version combinations\n        pyc_apis = ['-'.join(tag.split('-')[:2]) for tag in in_info_tags]\n        # unique Python version, C-API version combinations\n        pyc_apis = unique_by_index(pyc_apis)\n        # Add new platform tags for each Python version, C-API combination\n        required_tags = ['-'.join(tup) for tup in product(pyc_apis, platforms)]\n        needs_write = False\n        for req_tag in required_tags:\n            if req_tag in in_info_tags: continue\n            needs_write = True\n            info.add_header('Tag', req_tag)\n        if needs_write:\n            write_pkg_info(info_fname, info)\n            # Tell context manager to write wheel on exit by setting filename\n            ctx.out_wheel = out_wheel\n    return ctx.out_wheel"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef temporal_betweenness_centrality(tnet=None, paths=None, calc='time'):\n    '''\n    Returns temporal betweenness centrality per node.\n\n    Parameters\n    -----------\n\n    Input should be *either* tnet or paths.\n\n    data : array or dict\n\n        Temporal network input (graphlet or contact). nettype: 'bu', 'bd'.\n\n    calc : str\n\n        either 'global' or 'time'\n\n    paths : pandas dataframe\n\n        Output of TenetoBIDS.networkmeasure.shortest_temporal_paths\n\n\n    Returns\n    --------\n\n    :close: array\n\n        normalized temporal betweenness centrality.\n\n            If calc = 'time', returns (node,time)\n\n            If calc = 'global', returns (node)\n\n    '''\n\n    if tnet is not None and paths is not None:\n        raise ValueError('Only network or path input allowed.')\n    if tnet is None and paths is None:\n        raise ValueError('No input.')\n    # if shortest paths are not calculated, calculate them\n    if tnet is not None:\n        paths = shortest_temporal_path(tnet)\n\n    bet = np.zeros([paths[['from', 'to']].max().max() +\n                    1, paths['t_start'].max()+1])\n\n    for row in paths.iterrows():\n        if (np.isnan(row[1]['path includes'])).all():\n            pass\n        else:\n            nodes_in_path = np.unique(np.concatenate(\n                row[1]['path includes'])).astype(int).tolist()\n            nodes_in_path.remove(row[1]['from'])\n            nodes_in_path.remove(row[1]['to'])\n            if len(nodes_in_path) > 0:\n                bet[nodes_in_path, row[1]['t_start']] += 1\n\n    # Normalise bet\n    bet = (1/((bet.shape[0]-1)*(bet.shape[0]-2))) * bet\n\n    if calc == 'global':\n        bet = np.mean(bet, axis=1)\n\n    return bet", "response": "Returns temporal betweenness centrality per node."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the allegiance matrix for a given community.", "response": "def allegiance(community):\n    \"\"\"\n    Computes the allegiance matrix with values representing the probability that\n    nodes i and j were assigned to the same community by time-varying clustering methods.\n\n    parameters\n    ----------\n    community : array\n        array of community assignment of size node,time\n\n    returns\n    -------\n    P : array\n        module allegiance matrix, with P_ij probability that area i and j are in the same community\n\n    Reference:\n    ----------\n    Bassett, et al. (2013) \u201cRobust detection of dynamic community structure in networks\u201d, Chaos, 23, 1\n\n    \"\"\"\n    N = community.shape[0]\n    C = community.shape[1]\n    T = P = np.zeros([N, N])\n\n    for t in range(len(community[0, :])):\n        for i in range(len(community[:, 0])):\n            for j in range(len(community[:, 0])):\n                if i == j:\n                    continue\n                # T_ij indicates the number of times that i and j are assigned to the same community across time\n                if community[i][t] == community[j][t]:\n                    T[i, j] += 1\n\n    # module allegiance matrix, probability that ij were assigned to the same community\n    P = (1/C)*T\n\n    return P"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a random network where intervals between contacts are distributed by a Poisson distribution.", "response": "def rand_poisson(nnodes, ncontacts, lam=1, nettype='bu', netinfo=None, netrep='graphlet'):\n    \"\"\"\n    Generate a random network where intervals between contacts are distributed by a poisson distribution\n\n    Parameters\n    ----------\n\n    nnodes : int\n        Number of nodes in networks\n\n    ncontacts : int or list\n        Number of expected contacts (i.e. edges). If list, number of contacts for each node.\n        Any zeros drawn are ignored so returned degree of network can be smaller than ncontacts.\n\n    lam : int or list\n        Expectation of interval.\n\n    nettype : str\n        'bu' or 'bd'\n\n    netinfo : dict\n        Dictionary of additional information\n\n    netrep : str\n        How the output should be.\n\n    If ncontacts is a list, so should lam.\n\n    Returns\n    -------\n        net : array or dict\n            Random network with intervals between active edges being Poisson distributed.\n\n    \"\"\"\n\n    if isinstance(ncontacts, list):\n        if len(ncontacts) != nnodes:\n            raise ValueError(\n                'Number of contacts, if a list, should be one per node')\n    if isinstance(lam, list):\n        if len(lam) != nnodes:\n            raise ValueError(\n                'Lambda value of Poisson distribution, if a list, should be one per node')\n    if isinstance(lam, list) and not isinstance(ncontacts, list) or not isinstance(lam, list) and isinstance(ncontacts, list):\n        raise ValueError(\n            'When one of lambda or ncontacts is given as a list, the other argument must also be a list.')\n\n    if nettype == 'bu':\n        edgen = int((nnodes*(nnodes-1))/2)\n    elif nettype == 'bd':\n        edgen = int(nnodes*nnodes)\n\n    if not isinstance(lam, list) and not isinstance(ncontacts, list):\n        icts = np.random.poisson(lam, size=(edgen, ncontacts))\n        net = np.zeros([edgen, icts.sum(axis=1).max()+1])\n        for n in range(edgen):\n            net[n, np.unique(np.cumsum(icts[n]))] = 1\n    else:\n        icts = []\n        ict_max = 0\n        for n in range(edgen):\n            icts.append(np.random.poisson(lam[n], size=ncontacts[n]))\n            if sum(icts[-1]) > ict_max:\n                ict_max = sum(icts[-1])\n        net = np.zeros([nnodes, ict_max+1])\n        for n in range(nnodes):\n            net[n, np.unique(np.cumsum(icts[n]))] = 1\n\n    if nettype == 'bu':\n        nettmp = np.zeros([nnodes, nnodes, net.shape[-1]])\n        ind = np.triu_indices(nnodes, k=1)\n        nettmp[ind[0], ind[1], :] = net\n        net = nettmp + nettmp.transpose([1, 0, 2])\n    elif nettype == 'bd':\n        net = net.reshape([nnodes, nnodes, net.shape[-1]], order='F')\n        net = set_diagonal(net, 0)\n\n    if netrep == 'contact':\n        if not netinfo:\n            netinfo = {}\n        netinfo['nettype'] = 'b' + nettype[-1]\n        net = graphlet2contact(net, netinfo)\n\n    return net"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef temporal_participation_coeff(tnet, communities=None, decay=None, removeneg=False):\n    r'''\n    Temporal participation coefficient is a measure of diversity of connections across communities for individual nodes.\n\n    Parameters\n    ----------\n    tnet : array, dict\n        graphlet or contact sequence input. Only positive matrices considered.\n    communities : array\n        community vector. Either 1D (node) community index or 2D (node,time).\n    removeneg : bool (default false)\n        If true, all values < 0 are made to be 0.\n\n\n    Returns\n    -------\n    P : array\n        participation coefficient\n\n\n    Notes\n    -----\n\n    Static participatoin coefficient is:\n\n    .. math:: P_i = 1 - \\sum_s^{N_M}({{k_{is}}\\over{k_i}})^2\n\n    Where s is the index of each community (:math:`N_M`). :math:`k_i` is total degree of node. And :math:`k_{is}` is degree of connections within community.[part-1]_\n\n    This \"temporal\" version only loops through temporal snapshots and calculates :math:`P_i` for each t.\n\n    If directed, function sums axis=1, so tnet may need to be transposed before hand depending on what type of directed part_coef you are interested in.\n\n\n    References\n    ----------\n\n    .. [part-1] Guimera et al (2005) Functional cartography of complex metabolic networks. Nature. 433: 7028, p895-900. [`Link <http://doi.org/10.1038/nature03288>`_]\n    '''\n\n    if communities is None:\n        if isinstance(tnet, dict):\n            if 'communities' in tnet.keys():\n                communities = tnet['communities']\n            else:\n                raise ValueError('Community index not found')\n        else:\n            raise ValueError('Community must be provided for graphlet input')\n\n    # Get input in right format\n    tnet = process_input(tnet, ['C', 'G', 'TN'], 'TN')\n\n    if tnet.nettype[0] == 'w':\n        # TODO add contingency when hdf5 data has negative edges\n        if tnet.hdf5 == False:\n            if sum(tnet.network['weight'] < 0) > 0 and not removeneg:\n                print(\n                    'TENETO WARNING: negative edges exist when calculating participation coefficient.')\n            else:\n                tnet.network['weight'][tnet.network['weight'] < 0] = 0\n\n    part = np.zeros([tnet.netshape[0], tnet.netshape[1]])\n\n    if len(communities.shape) == 1:\n        for t in np.arange(0, tnet.netshape[1]):\n            C = communities\n            snapshot = tnet.get_network_when(t=t)\n            if tnet.nettype[1] == 'd':\n                i_at_t = snapshot['i'].values\n            else:\n                i_at_t = np.concatenate(\n                    [snapshot['i'].values, snapshot['j'].values])\n            i_at_t = np.unique(i_at_t).tolist()\n            i_at_t = list(map(int, i_at_t))\n            for i in i_at_t:\n                # Calculate degree of node\n                if tnet.nettype[1] == 'd':\n                    df = tnet.get_network_when(i=i, t=t)\n                    j_at_t = df['j'].values\n                    if tnet.nettype == 'wd':\n                        k_i = df['weight'].sum()\n                    elif tnet.nettype == 'bd':\n                        k_i = len(df)\n                elif tnet.nettype[1] == 'u':\n                    df = tnet.get_network_when(ij=i, t=t)\n                    j_at_t = np.concatenate([df['i'].values, df['j'].values])\n                    if tnet.nettype == 'wu':\n                        k_i = df['weight'].sum()\n                    elif tnet.nettype == 'bu':\n                        k_i = len(df)\n                j_at_t = list(map(int, j_at_t))\n                for c in np.unique(C[j_at_t]):\n                    ci = np.where(C == c)[0].tolist()\n                    k_is = tnet.get_network_when(i=i, j=ci, t=t)\n                    if tnet.nettype[1] == 'u':\n                        k_is2 = tnet.get_network_when(j=i, i=ci, t=t)\n                        k_is = pd.concat([k_is, k_is2])\n                    if len(k_is) > 0:\n                        if tnet.nettype[0] == 'b':\n                            k_is = len(k_is)\n                        else:\n                            k_is = k_is['weight'].sum()\n                        part[i, t] += np.square(k_is/k_i)\n            part[i_at_t, t] = 1 - part[i_at_t, t]\n            if decay is not None and t > 0:\n                part[i_at_t, t] += decay*part[i_at_t, t-1]\n    else:\n        for t in np.arange(0, tnet.netshape[1]):\n            snapshot = tnet.get_network_when(t=t)\n            if tnet.nettype[1] == 'd':\n                i_at_t = snapshot['i'].values\n            else:\n                i_at_t = np.concatenate(\n                    [snapshot['i'].values, snapshot['j'].values])\n            i_at_t = np.unique(i_at_t).tolist()\n            i_at_t = list(map(int, i_at_t))\n            for i in i_at_t:\n                for tc in np.arange(0, tnet.netshape[1]):\n                    C = communities[:, tc]\n                    # Calculate degree of node\n                    if tnet.nettype[1] == 'd':\n                        df = tnet.get_network_when(i=i, t=t)\n                        j_at_t = df['j'].values\n                        if tnet.nettype == 'wd':\n                            k_i = df['weight'].sum()\n                        elif tnet.nettype == 'bd':\n                            k_i = len(df)\n                    elif tnet.nettype[1] == 'u':\n                        df = tnet.get_network_when(ij=i, t=t)\n                        j_at_t = np.concatenate(\n                            [df['i'].values, df['j'].values])\n                        if tnet.nettype == 'wu':\n                            k_i = df['weight'].sum()\n                        elif tnet.nettype == 'bu':\n                            k_i = len(df)\n                    j_at_t = list(map(int, j_at_t))\n                    for c in np.unique(C[j_at_t]):\n                        ci = np.where(C == c)[0].tolist()\n                        k_is = tnet.get_network_when(i=i, j=ci, t=t)\n                        if tnet.nettype[1] == 'u':\n                            k_is2 = tnet.get_network_when(j=i, i=ci, t=t)\n                            k_is = pd.concat([k_is, k_is2])\n                        if tnet.nettype[0] == 'b':\n                            k_is = len(k_is)\n                        else:\n                            k_is = k_is['weight'].sum()\n                        part[i, t] += np.square(k_is/k_i)\n                part[i, t] = part[i, t] / tnet.netshape[1]\n            part[i_at_t, t] = 1 - part[i_at_t, t]\n            if decay is not None and t > 0:\n                part[i_at_t, t] += decay*part[i_at_t, t-1]\n\n    # Set any division by 0 to 0\n    part[np.isnan(part) == 1] = 0\n\n    return part", "response": "r Returns a sequence of temporal participation coefficient values for a given graphlet or contact sequence input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef graphlet_stack_plot(netin, ax, q=10, cmap='Reds', gridcolor='k', borderwidth=2, bordercolor=None, Fs=1, timeunit='', t0=1, sharpen='yes', vminmax='minmax'):\n    r'''\n    Returns matplotlib axis handle for graphlet_stack_plot. This is a row of transformed connectivity matrices to look like a 3D stack.\n\n    Parameters\n    ----------\n\n    netin : array, dict\n        network input (graphlet or contact)\n    ax : matplotlib ax handles.\n    q : int\n        Quality. Increaseing this will lead to smoother axis but take up more memory.\n    cmap : str\n        Colormap (matplotlib) of graphlets\n    Fs : int\n        Sampling rate. Same as contact-representation (if netin is contact, and input is unset, contact dictionary is used)\n    timeunit : str\n        Unit of time for xlabel. Same as contact-representation (if netin is contact, and input is unset, contact dictionary is used)\n    t0 : int\n        What should the first time point be called. Should be integer. Default 1.\n    gridcolor : str\n        The color of the grid section of the graphlets. Set to 'none' if not wanted.\n    borderwidth : int\n        Scales the size of border. (at the moment it cannot be set to 0.)\n    bordorcolor :\n        color of the border (at the moment it must be in RGB values between 0 and 1 -> this will be changed sometime in the future). Default: black.\n    vminmax : str\n         'maxabs', 'minmax' (default), or list/array with length of 2. Specifies the min and max colormap value of graphlets. Maxabs entails [-max(abs(G)),max(abs(G))], minmax entails [min(G), max(G)].\n\n    Returns\n    --------\n    ax : matplotlib ax handle\n\n    Note\n    ------\n    This function can require a lot of RAM with larger networks.\n\n    Note\n    ------\n    At the momenet bordercolor cannot be set to zero. To remove border, set bordorwidth=1 and bordercolor=[1,1,1] for temporay workaround.\n\n    Examples\n    -------\n\n    Create a network with some metadata\n\n    >>> import numpy as np\n    >>> import teneto\n    >>> import matplotlib.pyplot as plt\n    >>> np.random.seed(2017) # For reproduceability\n    >>> N = 5 # Number of nodes\n    >>> T = 10 # Number of timepoints\n    >>> # Probability of edge activation\n    >>> birth_rate = 0.2\n    >>> death_rate = .9\n    >>> # Add node names into the network and say time units are years, go 1 year per graphlet and startyear is 2007\n    >>> cfg={}\n    >>> cfg['Fs'] = 1\n    >>> cfg['timeunit'] = 'Years'\n    >>> cfg['t0'] = 2007 #First year in network\n    >>> #Generate network\n    >>> C = teneto.generatenetwork.rand_binomial([N,T],[birth_rate, death_rate],'contact','bu',netinfo=cfg)\n\n    Now this network can be plotted\n\n    >>> fig,ax = plt.subplots(figsize=(10,3))\n    >>> ax = teneto.plot.graphlet_stack_plot(C,ax,q=10,cmap='Greys')\n    >>> fig.show()\n\n    .. plot::\n\n        import numpy as np\n        import teneto\n        import matplotlib.pyplot as plt\n        np.random.seed(2017) # For reproduceability\n        N = 5 # Number of nodes\n        T = 10 # Number of timepoints\n        # Probability of edge activation\n        birth_rate = 0.2\n        death_rate = .9\n        # Add node names into the network and say time units are years, go 1 year per graphlet and startyear is 2007\n        cfg={}\n        cfg['Fs'] = 1\n        cfg['timeunit'] = 'Years'\n        cfg['t0'] = 2007 #First year in network\n        #Generate network\n        C = teneto.generatenetwork.rand_binomial([N,T],[birth_rate, death_rate],'contact','bu',netinfo=cfg)\n        fig,ax = plt.subplots(figsize=(10,3))\n        cmap = 'Greys'\n        ax = teneto.plot.graphlet_stack_plot(C,ax,q=10,cmap=cmap)\n        fig.show()\n\n    '''\n\n    # Get input type (C, G, TO)\n    inputType = checkInput(netin)\n\n    # Convert TO to C representation\n    if inputType == 'TO':\n        netin = netin.contact\n        inputType = 'C'\n    # Convert C representation to G\n    if inputType == 'C':\n        if timeunit == '':\n            timeunit = netin['timeunit']\n        if t0 == 1:\n            t0 = netin['t0']\n        if Fs == 1:\n            Fs = netin['Fs']\n        netin = contact2graphlet(netin)\n\n    if timeunit != '':\n        timeunit = ' (' + timeunit + ')'\n\n    if bordercolor == None:\n        bordercolor = [0, 0, 0]\n\n    if not isinstance(borderwidth, int):\n        borderwidth = int(borderwidth)\n        print('Warning: borderwidth should be an integer. Converting to integer.')\n\n    # x and y ranges for each of the graphlet plots\n    v = np.arange(0, netin.shape[0] + 1)\n    vr = np.arange(netin.shape[0], -1, -1)\n    # Preallocatie matrix\n\n    if vminmax == '' or vminmax == 'absmax' or vminmax == 'maxabs':\n        vminmax = [-np.nanmax(np.abs(netin)), np.nanmax(np.abs(netin))]\n    elif vminmax == 'minmax':\n        vminmax = [np.nanmin(netin), np.nanmax(netin)]\n\n    qb = q * borderwidth\n    figmat = np.zeros([80 * q + (qb * 2), int(((netin.shape[-1]) *\n                                               (80 * q) + (qb * 2)) - ((netin.shape[-1] - 1) * q * 80) / 2), 4])\n    for n in range(0, netin.shape[-1]):\n        # Create graphlet\n        figtmp, axtmp = plt.subplots(\n            1, facecolor='white', figsize=(q, q), dpi=80)\n        axtmp.pcolormesh(v, vr, netin[:, :, n], cmap=cmap, edgecolor=gridcolor,\n                         linewidth=q * 2, vmin=vminmax[0], vmax=vminmax[1])\n        axtmp.set_xticklabels('')\n        axtmp.set_yticklabels('')\n        axtmp.set_xticks([])\n        axtmp.set_yticks([])\n        x0, x1 = axtmp.get_xlim()\n        y0, y1 = axtmp.get_ylim()\n        axtmp.set_aspect((x1 - x0) / (y1 - y0))\n        axtmp.spines['left'].set_visible(False)\n        axtmp.spines['right'].set_visible(False)\n        axtmp.spines['top'].set_visible(False)\n        axtmp.spines['bottom'].set_visible(False)\n        plt.subplots_adjust(left=0, bottom=0, right=1,\n                            top=1, wspace=0, hspace=0)\n\n        # Convert graphlet to RGB values\n        figtmp.canvas.draw()\n        figmattmp = np.fromstring(\n            figtmp.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n        figmattmp = figmattmp.reshape(\n            figtmp.canvas.get_width_height()[::-1] + (3,))\n\n        # Close figure for memory\n        plt.close(figtmp)\n\n        # Manually add a border\n\n        figmattmp_withborder = np.zeros(\n            [figmattmp.shape[0] + (qb * 2), figmattmp.shape[1] + (qb * 2), 3]) + (np.array(bordercolor) * 255)\n        figmattmp_withborder[qb:-qb, qb:-qb, :] = figmattmp\n\n        # Make corners rounded. First make a circle and then take the relevant quarter for each corner.\n        y, x = np.ogrid[-qb: qb + 1, -qb: qb + 1]\n        mask = x * x + y * y <= qb * qb\n        # A little clumsy. Should improve\n        Mq1 = np.vstack([[mask[:qb, :qb] == 0], [mask[:qb, :qb] == 0], [\n                        mask[:qb, :qb] == 0]]).transpose([1, 2, 0])\n        figmattmp_withborder[:qb, :qb, :][Mq1] = 255\n        Mq1 = np.vstack([[mask[:qb, -qb:] == 0], [mask[:qb, -qb:]\n                                                  == 0], [mask[:qb, -qb:] == 0]]).transpose([1, 2, 0])\n        figmattmp_withborder[:qb, -qb:, :][Mq1] = 255\n        Mq1 = np.vstack([[mask[-qb:, :qb] == 0], [mask[-qb:, :qb]\n                                                  == 0], [mask[-qb:, :qb] == 0]]).transpose([1, 2, 0])\n        figmattmp_withborder[-qb:, :qb, :][Mq1] = 255\n        Mq1 = np.vstack([[mask[-qb:, -qb:] == 0], [mask[-qb:, -qb:]\n                                                   == 0], [mask[-qb:, -qb:] == 0]]).transpose([1, 2, 0])\n        figmattmp_withborder[-qb:, -qb:, :][Mq1] = 255\n\n        #scale and sheer\n        scale = np.matrix([[1.5, 0, 0], [0, 3, 0], [0, 0, 1]])\n        sheer = np.matrix([[1, np.tan(np.pi / 12), 0], [0, 1, 0], [0, 0, 1]])\n\n        # apply affine transformation\n        figmattmp = ndimage.affine_transform(\n            figmattmp_withborder, sheer * (scale), offset=[-35 * q, 0, 0], cval=255)\n\n        # At the moment the alpha part does not work if the background colour is anything but white.\n        # Also used for detecting where the graphlets are in the image.\n        trans = np.where(np.sum(figmattmp, axis=2) == 255 * 3)\n        alphamat = np.ones([figmattmp.shape[0], figmattmp.shape[0]])\n        alphamat[trans[0], trans[1]] = 0\n        figmattmp = np.dstack([figmattmp, alphamat])\n\n        # Add graphlet to matrix\n        if n == 0:\n            figmat[:, n * (80 * q):((n + 1) * (80 * q) + (qb * 2))] = figmattmp\n        else:\n            figmat[:, n * (80 * q) - int((n * q * 80) / 2):int(((n + 1)\n                                                                * (80 * q) + (qb * 2)) - (n * q * 80) / 2)] = figmattmp\n\n    # Fix colours - due to imshows weirdness when taking nxnx3\n    figmat[:, :, 0:3] = figmat[:, :, 0:3] / 255\n    # Cut end of matrix off that isn't need\n    figmat = figmat[:, :-int((q / 2) * 80), :]\n    fid = np.where(figmat[:, :, -1] > 0)\n    fargmin = np.argmin(fid[0])\n    ymax = np.max(fid[0])\n    yright = np.max(np.where(figmat[:, fid[1][fargmin], -1] > 0))\n    xtickloc = np.where(figmat[ymax, :, -1] > 0)[0]\n    # In case there are multiple cases of xtickloc in same graphlet (i.e. they all have the same lowest value)\n    xtickloc = np.delete(xtickloc, np.where(np.diff(xtickloc) == 1)[0] + 1)\n\n    fid = np.where(figmat[:, :, -1] > 0)\n    ymin = np.min(fid[0])\n    topfig = np.where(figmat[ymin, :, -1] > 0)[0]\n    topfig = topfig[0:len(topfig):int(len(topfig) / netin.shape[-1])]\n\n    # Make squares of non transparency around each figure (this fixes transparency issues when white is in the colormap)\n    # for n in range(0,len(topfig)):\n    # fid=np.where(figmat[ymin:ymax,xtickloc[n]:topfig[n],-1]==0)\n    # figmat[ymin:ymax,xtickloc[n]:topfig[n],:3][fid[0],fid[1]]=1\n    # figmat[ymin+q:ymax-q,xtickloc[n]+q:topfig[n]-q,-1]=1\n\n    # Create figure\n    # Sharped edges of figure with median filter\n    if sharpen == 'yes':\n        figmat[:, :, :-1] = ndimage.median_filter(figmat[:, :, :-1], 3)\n    ax.imshow(figmat[:, :, :-1], zorder=1)\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    L = int((((netin.shape[-1] - 3) + 1) * (80 * q) +\n             (qb * 2)) - ((netin.shape[-1] - 3) * q * 80) / 2 - q)\n    _ = [ax.plot(range(topfig[i], xt), np.zeros(len(range(topfig[i], xt))) + yright,\n                 color='k', linestyle=':', zorder=2) for i, xt in enumerate(xtickloc[1:])]\n    ax.plot(range(0, L), np.zeros(L) + ymax,\n            color='k', linestyle=':', zorder=2)\n    _ = [ax.plot(np.zeros(q * 10) + xt, np.arange(ymax, ymax + q * 10),\n                 color='k', linestyle=':', zorder=2) for xt in xtickloc]\n    _ = [ax.text(xt, ymax + q * 20, str(round((i + t0) * Fs, 5)),\n                 horizontalalignment='center',) for i, xt in enumerate(xtickloc)]\n\n    ylim = ax.axes.get_ylim()\n    xlim = ax.axes.get_xlim()\n    ax.set_ylim(ylim[0] + q * 15, 0)\n    ax.set_xlim(xlim[0] - q * 20, xlim[1])\n    ax.set_xlabel('Time' + timeunit)\n    return ax", "response": "r Plot a 3D stack of graphlets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndefining a network from a pandas dataframe.", "response": "def network_from_df(self, df):\n        \"\"\"\n        Defines a network from an array.\n\n        Parameters\n        ----------\n        array : array\n            Pandas dataframe. Should have columns: \\'i\\', \\'j\\', \\'t\\' where i and j are node indicies and t is the temporal index.\n            If weighted, should also include \\'weight\\'. Each row is an edge.\n        \"\"\"\n        teneto.utils.check_TemporalNetwork_input(df, 'df')\n        self.network = df\n        self._update_network()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndefining a network from an array of lists.", "response": "def network_from_edgelist(self, edgelist):\n        \"\"\"\n        Defines a network from an array.\n\n        Parameters\n        ----------\n        edgelist : list of lists.\n            A list of lists which are 3 or 4 in length. For binary networks each sublist should be [i, j ,t] where i and j are node indicies and t is the temporal index.\n            For weighted networks each sublist should be [i, j, t, weight].\n        \"\"\"\n        teneto.utils.check_TemporalNetwork_input(edgelist, 'edgelist')\n        if len(edgelist[0]) == 4:\n            colnames = ['i', 'j', 't', 'weight']\n        elif len(edgelist[0]) == 3:\n            colnames = ['i', 'j', 't']\n        self.network = pd.DataFrame(edgelist, columns=colnames)\n        self._update_network()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndropping duplicate entries from the network dataframe.", "response": "def _drop_duplicate_ij(self):\n        \"\"\"\n        Drops duplicate entries from the network dataframe.\n        \"\"\"\n        self.network['ij'] = list(map(lambda x: tuple(sorted(x)), list(\n            zip(*[self.network['i'].values, self.network['j'].values]))))\n        self.network.drop_duplicates(['ij', 't'], inplace=True)\n        self.network.reset_index(inplace=True, drop=True)\n        self.network.drop('ij', inplace=True, axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndrops self - contacts from the network dataframe.", "response": "def _drop_diagonal(self):\n        \"\"\"\n        Drops self-contacts from the network dataframe.\n        \"\"\"\n        self.network = self.network.where(\n            self.network['i'] != self.network['j']).dropna()\n        self.network.reset_index(inplace=True, drop=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_edge(self, edgelist):\n        if not isinstance(edgelist[0], list):\n            edgelist = [edgelist]\n        teneto.utils.check_TemporalNetwork_input(edgelist, 'edgelist')\n        if len(edgelist[0]) == 4:\n            colnames = ['i', 'j', 't', 'weight']\n        elif len(edgelist[0]) == 3:\n            colnames = ['i', 'j', 't']\n        if self.hdf5:\n            with pd.HDFStore(self.network) as hdf:\n                rows = hdf.get_storer('network').nrows\n                hdf.append('network', pd.DataFrame(edgelist, columns=colnames, index=np.arange(\n                    rows, rows+len(edgelist))), format='table', data_columns=True)\n            edgelist = np.array(edgelist)\n            if np.max(edgelist[:, :2]) > self.netshape[0]:\n                self.netshape[0] = np.max(edgelist[:, :2])\n            if np.max(edgelist[:, 2]) > self.netshape[1]:\n                self.netshape[1] = np.max(edgelist[:, 2])\n        else:\n            newedges = pd.DataFrame(edgelist, columns=colnames)\n            self.network = pd.concat(\n                [self.network, newedges], ignore_index=True, sort=True)\n            self._update_network()", "response": "Adds an edge from the network to the TenetoBIDS. network dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves an edge from the TenetoBIDS. network dataframe.", "response": "def drop_edge(self, edgelist):\n        \"\"\"\n        Removes an edge from network.\n\n        Parameters\n        ----------\n\n        edgelist : list\n            a list (or list of lists) containing the i,j and t indicies to be removes.\n\n        Returns\n        --------\n            Updates TenetoBIDS.network dataframe\n        \"\"\"\n        if not isinstance(edgelist[0], list):\n            edgelist = [edgelist]\n        teneto.utils.check_TemporalNetwork_input(edgelist, 'edgelist')\n        if self.hdf5:\n            with pd.HDFStore(self.network) as hdf:\n                for e in edgelist:\n                    hdf.remove(\n                        'network', 'i == ' + str(e[0]) + ' & ' + 'j == ' + str(e[1]) + ' & ' + 't == ' + str(e[2]))\n            print('HDF5 delete warning. This will not reduce the size of the file.')\n        else:\n            for e in edgelist:\n                idx = self.network[(self.network['i'] == e[0]) & (\n                    self.network['j'] == e[1]) & (self.network['t'] == e[2])].index\n                self.network.drop(idx, inplace=True)\n            self.network.reset_index(inplace=True, drop=True)\n            self._update_network()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_networkmeasure(self, networkmeasure, **measureparams):\n        availablemeasures = [f for f in dir(\n            teneto.networkmeasures) if not f.startswith('__')]\n        if networkmeasure not in availablemeasures:\n            raise ValueError(\n                'Unknown network measure. Available network measures are: ' + ', '.join(availablemeasures))\n        funs = inspect.getmembers(teneto.networkmeasures)\n        funs = {m[0]: m[1] for m in funs if not m[0].startswith('__')}\n        measure = funs[networkmeasure](self, **measureparams)\n        return measure", "response": "Calculates the network measure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a network from a function.", "response": "def generatenetwork(self, networktype, **networkparams):\n        \"\"\"\n        Generate a network\n\n        Parameters\n        -----------\n        networktype : str\n            Function to call. Functions available are in teneto.generatenetwork\n\n        measureparams : kwargs\n            kwargs for teneto.generatenetwork.[networktype]\n\n        Returns\n        --------\n        TenetoBIDS.network is made with the generated network.\n        \"\"\"\n        availabletypes = [f for f in dir(\n            teneto.generatenetwork) if not f.startswith('__')]\n        if networktype not in availabletypes:\n            raise ValueError(\n                'Unknown network measure. Available networks to generate are: ' + ', '.join(availabletypes))\n        funs = inspect.getmembers(teneto.generatenetwork)\n        funs = {m[0]: m[1] for m in funs if not m[0].startswith('__')}\n        network = funs[networktype](**networkparams)\n        self.network_from_array(network)\n        if self.nettype[1] == 'u':\n            self._drop_duplicate_ij()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_aspickle(self, fname):\n        if fname[-4:] != '.pkl':\n            fname += '.pkl'\n        with open(fname, 'wb') as f:\n            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)", "response": "Saves object as pickle."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef postpro_fisher(data, report=None):\n    if not report:\n        report = {}\n    # Due to rounding errors\n    data[data < -0.99999999999999] = -1\n    data[data > 0.99999999999999] = 1\n    fisher_data = 0.5 * np.log((1 + data) / (1 - data))\n    report['fisher'] = {}\n    report['fisher']['performed'] = 'yes'\n    #report['fisher']['diagonal'] = 'zeroed'\n    return fisher_data, report", "response": "Performs fisher transform on everything in data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms box cox transform on everything in data.", "response": "def postpro_boxcox(data, report=None):\n    \"\"\"\n    Performs box cox transform on everything in data.\n\n    If report variable is passed, this is added to the report.\n    \"\"\"\n    if not report:\n        report = {}\n    # Note the min value of all time series will now be at least 1.\n    mindata = 1 - np.nanmin(data)\n    data = data + mindata\n    ind = np.triu_indices(data.shape[0], k=1)\n\n    boxcox_list = np.array([sp.stats.boxcox(np.squeeze(\n        data[ind[0][n], ind[1][n], :])) for n in range(0, len(ind[0]))])\n\n    boxcox_data = np.zeros(data.shape)\n    boxcox_data[ind[0], ind[1], :] = np.vstack(boxcox_list[:, 0])\n    boxcox_data[ind[1], ind[0], :] = np.vstack(boxcox_list[:, 0])\n\n    bccheck = np.array(np.transpose(boxcox_data, [2, 0, 1]))\n    bccheck = (bccheck - bccheck.mean(axis=0)) / bccheck.std(axis=0)\n    bccheck = np.squeeze(np.mean(bccheck, axis=0))\n    np.fill_diagonal(bccheck, 0)\n\n    report['boxcox'] = {}\n    report['boxcox']['performed'] = 'yes'\n    report['boxcox']['lambda'] = [\n        tuple([ind[0][n], ind[1][n], boxcox_list[n, -1]]) for n in range(0, len(ind[0]))]\n    report['boxcox']['shift'] = mindata\n    report['boxcox']['shited_to'] = 1\n\n    if np.sum(np.isnan(bccheck)) > 0:\n        report['boxcox'] = {}\n        report['boxcox']['performed'] = 'FAILED'\n        report['boxcox']['failure_reason'] = (\n            'Box cox transform is returning edges with uniform values through time. '\n            'This is probabaly due to one or more outliers or a very skewed distribution. '\n            'Have you corrected for sources of noise (e.g. movement)? '\n            'If yes, some time-series might need additional transforms to approximate to Gaussian.'\n        )\n        report['boxcox']['failure_consequence'] = (\n            'Box cox transform was skipped from the postprocess pipeline.'\n        )\n        boxcox_data = data - mindata\n        error_msg = ('TENETO WARNING: Box Cox transform problem. \\n'\n                     'Box Cox transform not performed. \\n'\n                     'See report for more details.')\n        print(error_msg)\n\n    return boxcox_data, report"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef postpro_standardize(data, report=None):\n    if not report:\n        report = {}\n    # First make dim 1 = time.\n    data = np.transpose(data, [2, 0, 1])\n    standardized_data = (data - data.mean(axis=0)) / data.std(axis=0)\n    standardized_data = np.transpose(standardized_data, [1, 2, 0])\n    report['standardize'] = {}\n    report['standardize']['performed'] = 'yes'\n    report['standardize']['method'] = 'Z-score'\n    # The above makes self connections to nan, set to 1.\n    data = set_diagonal(data, 1)\n    return standardized_data, report", "response": "Standardizes everything in data (along axis -1).\n\n    If report variable is passed, this is added to the report."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef derive_temporalnetwork(data, params):\n    report = {}\n\n    if 'dimord' not in params.keys():\n        params['dimord'] = 'node,time'\n\n    if 'report' not in params.keys():\n        params['report'] = False\n\n    if 'analysis_id' not in params.keys():\n        params['analysis_id'] = ''\n\n    if 'postpro' not in params.keys():\n        params['postpro'] = 'no'\n\n    if params['report'] == 'yes' or params['report'] == True:\n\n        if 'analysis_id' not in params.keys():\n            params['analysis_id'] = ''\n\n        if 'report_path' not in params.keys():\n            params['report_path'] = './report/' + params['analysis_id']\n\n        if 'report_filename' not in params.keys():\n            params['report_filename'] = 'derivation_report.html'\n\n    if params['dimord'] == 'node,time':\n        data = data.transpose()\n\n    if isinstance(params['method'], str):\n        if params['method'] == 'jackknife':\n            weights, report = _weightfun_jackknife(data.shape[0], report)\n            relation = 'weight'\n        elif params['method'] == 'sliding window' or params['method'] == 'slidingwindow':\n            weights, report = _weightfun_sliding_window(\n                data.shape[0], params, report)\n            relation = 'weight'\n        elif params['method'] == 'tapered sliding window' or params['method'] == 'taperedslidingwindow':\n            weights, report = _weightfun_tapered_sliding_window(\n                data.shape[0], params, report)\n            relation = 'weight'\n        elif params['method'] == 'distance' or params['method'] == \"spatial distance\" or params['method'] == \"node distance\" or params['method'] == \"nodedistance\" or params['method'] == \"spatialdistance\":\n            weights, report = _weightfun_spatial_distance(data, params, report)\n            relation = 'weight'\n        elif params['method'] == 'mtd' or params['method'] == 'multiply temporal derivative' or params['method'] == 'multiplytemporalderivative' or params['method'] == 'temporal derivative' or params['method'] == \"temporalderivative\":\n            R, report = _temporal_derivative(data, params, report)\n            relation = 'coupling'\n        else:\n            raise ValueError(\n                'Unrecognoized method. See derive_with_weighted_pearson documentation for predefined methods or enter own weight matrix')\n    else:\n        try:\n            weights = np.array(params['method'])\n            relation = 'weight'\n        except:\n            raise ValueError(\n                'Unrecognoized method. See documentation for predefined methods')\n        if weights.shape[0] != weights.shape[1]:\n            raise ValueError(\"weight matrix should be square\")\n        if weights.shape[0] != data.shape[0]:\n            raise ValueError(\"weight matrix must equal number of time points\")\n\n    if relation == 'weight':\n        # Loop over each weight vector and calculate pearson correlation.\n        # Note, should see if this can be made quicker in future.\n        R = np.array(\n            [DescrStatsW(data, weights[i, :]).corrcoef for i in range(0, weights.shape[0])])\n        # Make node,node,time\n        R = R.transpose([1, 2, 0])\n\n    # Correct jackknife direction\n    if params['method'] == 'jackknife':\n        # Correct inversion\n        R = R * -1\n        jc_z = 0\n        if 'weight-var' in params.keys():\n            R = np.transpose(R, [2, 0, 1])\n            R = (R - R.mean(axis=0)) / R.std(axis=0)\n            jc_z = 1\n            R = R * params['weight-var']\n            R = R.transpose([1, 2, 0])\n        if 'weight-mean' in params.keys():\n            R = np.transpose(R, [2, 0, 1])\n            if jc_z == 0:\n                R = (R - R.mean(axis=0)) / R.std(axis=0)\n            R = R + params['weight-mean']\n            R = np.transpose(R, [1, 2, 0])\n        R = set_diagonal(R, 1)\n\n    if params['postpro'] != 'no':\n        R, report = postpro_pipeline(\n            R, params['postpro'], report)\n        R = set_diagonal(R, 1)\n\n    if params['report'] == 'yes' or params['report'] == True:\n        gen_report(report, params['report_path'], params['report_filename'])\n    return R", "response": "Derives a time - resolved network from the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _weightfun_jackknife(T, report):\n\n    weights = np.ones([T, T])\n    np.fill_diagonal(weights, 0)\n    report['method'] = 'jackknife'\n    report['jackknife'] = ''\n    return weights, report", "response": "Create the weights for the jackknife method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _weightfun_sliding_window(T, params, report):\n    weightat0 = np.zeros(T)\n    weightat0[0:params['windowsize']] = np.ones(params['windowsize'])\n    weights = np.array([np.roll(weightat0, i)\n                        for i in range(0, T + 1 - params['windowsize'])])\n    report['method'] = 'slidingwindow'\n    report['slidingwindow'] = params\n    report['slidingwindow']['taper'] = 'untapered/uniform'\n    return weights, report", "response": "Create the weights for the sliding window method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to create the weights for the tapered method.", "response": "def _weightfun_tapered_sliding_window(T, params, report):\n    \"\"\"\n    Creates the weights for the tapered method. See func: teneto.derive.derive.\n    \"\"\"\n    x = np.arange(-(params['windowsize'] - 1) / 2, (params['windowsize']) / 2)\n    distribution_parameters = ','.join(map(str, params['distribution_params']))\n    taper = eval('sps.' + params['distribution'] +\n                             '.pdf(x,' + distribution_parameters + ')')\n\n    weightat0 = np.zeros(T)\n    weightat0[0:params['windowsize']] = taper\n    weights = np.array([np.roll(weightat0, i)\n                        for i in range(0, T + 1 - params['windowsize'])])\n    report['method'] = 'slidingwindow'\n    report['slidingwindow'] = params\n    report['slidingwindow']['taper'] = taper\n    report['slidingwindow']['taper_window'] = x\n    return weights, report"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the weights for the spatial distance method.", "response": "def _weightfun_spatial_distance(data, params, report):\n    \"\"\"\n    Creates the weights for the spatial distance method. See func: teneto.derive.derive.\n    \"\"\"\n    distance = getDistanceFunction(params['distance'])\n    weights = np.array([distance(data[n, :], data[t, :]) for n in np.arange(\n        0, data.shape[0]) for t in np.arange(0, data.shape[0])])\n    weights = np.reshape(weights, [data.shape[0], data.shape[0]])\n    np.fill_diagonal(weights, np.nan)\n    weights = 1 / weights\n    weights = (weights - np.nanmin(weights)) / \\\n        (np.nanmax(weights) - np.nanmin(weights))\n    np.fill_diagonal(weights, 1)\n    return weights, report"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms mtd method. See func: teneto.derive.derive.", "response": "def _temporal_derivative(data, params, report):\n    \"\"\"\n    Performs mtd method. See func: teneto.derive.derive.\n    \"\"\"\n    # Data should be timexnode\n    report = {}\n\n    # Derivative\n    tdat = data[1:, :] - data[:-1, :]\n    # Normalize\n    tdat = tdat / np.std(tdat, axis=0)\n    # Coupling\n    coupling = np.array([tdat[:, i] * tdat[:, j] for i in np.arange(0,\n                                                                    tdat.shape[1]) for j in np.arange(0, tdat.shape[1])])\n    coupling = np.reshape(\n        coupling, [tdat.shape[1], tdat.shape[1], tdat.shape[0]])\n    # Average over window using strides\n    shape = coupling.shape[:-1] + (coupling.shape[-1] -\n                                   params['windowsize'] + 1, params['windowsize'])\n    strides = coupling.strides + (coupling.strides[-1],)\n    coupling_windowed = np.mean(np.lib.stride_tricks.as_strided(\n        coupling, shape=shape, strides=strides), -1)\n\n    report = {}\n    report['method'] = 'temporalderivative'\n    report['temporalderivative'] = {}\n    report['temporalderivative']['windowsize'] = params['windowsize']\n\n    return coupling_windowed, report"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef binarize_rdp(netin, level, sign='pos', axis='time'):\n    netin, netinfo = process_input(netin, ['C', 'G', 'TO'])\n    trajectory = rdp(netin, level)\n\n    contacts = []\n    # Use the trajectory points as threshold\n    for n in range(trajectory['index'].shape[0]):\n        if sign == 'pos':\n            sel = trajectory['trajectory_points'][n][trajectory['trajectory']\n                                                     [n][trajectory['trajectory_points'][n]] > 0]\n        elif sign == 'neg':\n            sel = trajectory['trajectory_points'][n][trajectory['trajectory']\n                                                     [n][trajectory['trajectory_points'][n]] < 0]\n        else:\n            sel = trajectory['trajectory_points']\n        i_ind = np.repeat(trajectory['index'][n, 0], len(sel))\n        j_ind = np.repeat(trajectory['index'][n, 1], len(sel))\n        contacts.append(np.array([i_ind, j_ind, sel]).transpose())\n    contacts = np.concatenate(contacts)\n\n    # Create output dictionary\n    netout = dict(netinfo)\n    netout['contacts'] = contacts\n    netout['nettype'] = 'b' + netout['nettype'][1]\n    netout['dimord'] = 'node,node,time'\n    netout['timetype'] = 'discrete'\n    netout['diagonal'] = 0\n    # If input is graphlet, output graphlet\n    if netinfo['inputtype'] == 'G':\n        netout = contact2graphlet(netout)\n    else:\n        netout.pop('inputtype')\n\n    return netout", "response": "Binarizes a network based on RDP compression."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking input network and checks what the input is.", "response": "def process_input(netIn, allowedformats, outputformat='G'):\n    \"\"\"\n    Takes input network and checks what the input is.\n\n    Parameters\n    ----------\n\n    netIn : array, dict, or TemporalNetwork\n        Network (graphlet, contact or object)\n    allowedformats : str\n        Which format of network objects that are allowed. Options: 'C', 'TN', 'G'.\n    outputformat: str, default=G\n        Target output format. Options: 'C' or 'G'.\n\n\n    Returns\n    -------\n\n    C : dict\n\n    OR\n\n    G : array\n        Graphlet representation.\n    netInfo : dict\n        Metainformation about network.\n\n    OR\n\n    tnet : object\n        object of TemporalNetwork class\n\n    \"\"\"\n    inputtype = checkInput(netIn)\n    # Convert TN to G representation\n    if inputtype == 'TN' and 'TN' in allowedformats and outputformat != 'TN':\n        G = netIn.df_to_array()\n        netInfo = {'nettype': netIn.nettype, 'netshape': netIn.netshape}\n    elif inputtype == 'TN' and 'TN' in allowedformats and outputformat == 'TN':\n        TN = netIn\n    elif inputtype == 'C' and 'C' in allowedformats and outputformat == 'G':\n        G = contact2graphlet(netIn)\n        netInfo = dict(netIn)\n        netInfo.pop('contacts')\n    elif inputtype == 'C' and 'C' in allowedformats and outputformat == 'TN':\n        TN = TemporalNetwork(from_dict=netIn)\n    elif inputtype == 'G' and 'G' in allowedformats and outputformat == 'TN':\n        TN = TemporalNetwork(from_array=netIn)\n    # Get network type if not set yet\n    elif inputtype == 'G' and 'G' in allowedformats:\n        netInfo = {}\n        netInfo['netshape'] = netIn.shape\n        netInfo['nettype'] = gen_nettype(netIn)\n        G = netIn\n    elif inputtype == 'C' and outputformat == 'C':\n        pass\n    else:\n        raise ValueError('Input invalid.')\n    if outputformat == 'TN' and not isinstance(TN.network, str):\n        TN.network['i'] = TN.network['i'].astype(int)\n        TN.network['j'] = TN.network['j'].astype(int)\n        TN.network['t'] = TN.network['t'].astype(int)\n    if outputformat == 'C' or outputformat == 'G':\n        netInfo['inputtype'] = inputtype\n    if inputtype != 'C' and outputformat == 'C':\n        C = graphlet2contact(G, netInfo)\n    if outputformat == 'G':\n        return G, netInfo\n    elif outputformat == 'C':\n        return C\n    elif outputformat == 'TN':\n        return TN"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean_community_indexes(communityID):\n    communityID = np.array(communityID)\n    cid_shape = communityID.shape\n    if len(cid_shape) > 1:\n        communityID = communityID.flatten()\n    new_communityID = np.zeros(len(communityID))\n    for i, n in enumerate(np.unique(communityID)):\n        new_communityID[communityID == n] = i\n    if len(cid_shape) > 1:\n        new_communityID = new_communityID.reshape(cid_shape)\n    return new_communityID", "response": "Takes input of community assignments. Returns reindexed community assignment by using smallest numbers possible."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef multiple_contacts_get_values(C):\n    d = collections.OrderedDict()\n    for c in C['contacts']:\n        ct = tuple(c)\n        if ct in d:\n            d[ct] += 1\n        else:\n            d[ct] = 1\n\n    new_contacts = []\n    new_values = []\n    for (key, value) in d.items():\n        new_values.append(value)\n        new_contacts.append(key)\n    C_out = C\n    C_out['contacts'] = new_contacts\n    C_out['values'] = new_values\n    return C_out", "response": "This function removes duplicates and creates a value\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a numpy array representation of the contact list of the specified network type", "response": "def df_to_array(df, netshape, nettype):\n    \"\"\"\n    Returns a numpy array (snapshot representation) from thedataframe contact list\n\n    Parameters:\n        df : pandas df\n            pandas df with columns, i,j,t.\n        netshape : tuple\n            network shape, format: (node, time)\n        nettype : str\n            'wu', 'wd', 'bu', 'bd'\n\n    Returns:\n    --------\n        G : array\n            (node,node,time) array for the network\n    \"\"\"\n    if len(df) > 0:\n        idx = np.array(list(map(list, df.values)))\n        G = np.zeros([netshape[0], netshape[0], netshape[1]])\n        if idx.shape[1] == 3:\n            if nettype[-1] == 'u':\n                idx = np.vstack([idx, idx[:, [1, 0, 2]]])\n            idx = idx.astype(int)\n            G[idx[:, 0], idx[:, 1], idx[:, 2]] = 1\n        elif idx.shape[1] == 4:\n            if nettype[-1] == 'u':\n                idx = np.vstack([idx, idx[:, [1, 0, 2, 3]]])\n            weights = idx[:, 3]\n            idx = np.array(idx[:, :3], dtype=int)\n            G[idx[:, 0], idx[:, 1], idx[:, 2]] = weights\n    else:\n        G = np.zeros([netshape[0], netshape[0], netshape[1]])\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions that checks distance_func_name given the type of the network and returns the distance function name.", "response": "def check_distance_funciton_input(distance_func_name, netinfo):\n    \"\"\"\n    Funciton checks distance_func_name, if it is specified as 'default'. Then given the type of the network selects a default distance function.\n\n    Parameters\n    ----------\n\n    distance_func_name : str\n        distance function name.\n\n    netinfo : dict\n        the output of utils.process_input\n\n    Returns\n    -------\n\n    distance_func_name : str\n        distance function name.\n    \"\"\"\n\n    if distance_func_name == 'default' and netinfo['nettype'][0] == 'b':\n        print('Default distance funciton specified. As network is binary, using Hamming')\n        distance_func_name = 'hamming'\n    elif distance_func_name == 'default' and netinfo['nettype'][0] == 'w':\n        distance_func_name = 'euclidean'\n        print(\n            'Default distance funciton specified. '\n            'As network is weighted, using Euclidean')\n\n    return distance_func_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads coordinates of included parcellations.", "response": "def load_parcellation_coords(parcellation_name):\n    \"\"\"\n    Loads coordinates of included parcellations.\n\n    Parameters\n    ----------\n\n    parcellation_name : str\n        options: 'gordon2014_333', 'power2012_264', 'shen2013_278'.\n\n    Returns\n    -------\n    parc : array\n        parcellation cordinates\n\n    \"\"\"\n\n    path = tenetopath[0] + '/data/parcellation/' + parcellation_name + '.csv'\n    parc = np.loadtxt(path, skiprows=1, delimiter=',', usecols=[1, 2, 3])\n\n    return parc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_parcellation(data_path, parcellation, parc_type=None, parc_params=None):\n\n    if isinstance(parcellation, str):\n        parcin = ''\n        if '+' in parcellation:\n            parcin = parcellation\n            parcellation = parcellation.split('+')[0]\n        if '+OH' in parcin:\n            subcortical = True\n        else:\n            subcortical = None\n        if '+SUIT' in parcin:\n            cerebellar = True\n        else:\n            cerebellar = None\n\n        if not parc_type or not parc_params:\n            path = tenetopath[0] + '/data/parcellation_defaults/defaults.json'\n            with open(path) as data_file:\n                defaults = json.load(data_file)\n        if not parc_type:\n            parc_type = defaults[parcellation]['type']\n            print('Using default parcellation type')\n        if not parc_params:\n            parc_params = defaults[parcellation]['params']\n            print('Using default parameters')\n\n    if parc_type == 'sphere':\n        parcellation = load_parcellation_coords(parcellation)\n        seed = NiftiSpheresMasker(np.array(parcellation), **parc_params)\n        data = seed.fit_transform(data_path)\n    elif parc_type == 'region':\n        path = tenetopath[0] + '/data/parcellation/' + parcellation + '.nii.gz'\n        region = NiftiLabelsMasker(path, **parc_params)\n        data = region.fit_transform(data_path)\n    else:\n        raise ValueError('Unknown parc_type specified')\n\n    if subcortical:\n        subatlas = fetch_atlas_harvard_oxford('sub-maxprob-thr0-2mm')['maps']\n        region = NiftiLabelsMasker(subatlas, **parc_params)\n        data_sub = region.fit_transform(data_path)\n        data = np.hstack([data, data_sub])\n\n    if cerebellar:\n        path = tenetopath[0] + '/data/parcellation/Cerebellum-SUIT_space-MNI152NLin2009cAsym.nii.gz'\n        region = NiftiLabelsMasker(path, **parc_params)\n        data_cerebellar = region.fit_transform(data_path)\n        data = np.hstack([data, data_cerebellar])\n\n    return data", "response": "This function creates a parcellation for a single site."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a trajectory range from start to stop.", "response": "def create_traj_ranges(start, stop, N):\n    \"\"\"\n    Fills in the trajectory range.\n\n    # Adapted from https://stackoverflow.com/a/40624614\n    \"\"\"\n    steps = (1.0/(N-1)) * (stop - start)\n    if np.isscalar(steps):\n        return steps*np.arange(N) + start\n    else:\n        return steps[:, None]*np.arange(N) + start[:, None]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the dimension order of a network measure.", "response": "def get_dimord(measure, calc=None, community=None):\n    \"\"\"\n    Get the dimension order of a network measure.\n\n    Parameters\n    ----------\n\n    measure : str\n        Name of funciton in teneto.networkmeasures.\n    calc : str, default=None\n        Calc parameter for the function\n    community : bool, default=None\n        If not null, then community property is assumed to be believed.\n\n    Returns\n    -------\n\n    dimord : str\n        Dimension order. So \"node,node,time\" would define the dimensions of the network measure.\n\n    \"\"\"\n\n    if not calc:\n        calc = ''\n    else:\n        calc = '_' + calc\n    if not community:\n        community = ''\n    else:\n        community = 'community'\n    if 'community' in calc and 'community' in community:\n        community = ''\n    if calc == 'community_avg' or calc == 'community_pairs':\n        community = ''\n\n    dimord_dict = {\n        'temporal_closeness_centrality': 'node',\n        'temporal_degree_centrality': 'node',\n        'temporal_degree_centralit_avg': 'node',\n        'temporal_degree_centrality_time': 'node,time',\n        'temporal_efficiency': 'global',\n        'temporal_efficiency_global': 'global',\n        'temporal_efficiency_node': 'node',\n        'temporal_efficiency_to': 'node',\n        'sid_global': 'global,time',\n        'community_pairs': 'community,community,time',\n        'community_avg': 'community,time',\n        'sid': 'community,community,time',\n        'reachability_latency_global': 'global',\n        'reachability_latency': 'global',\n        'reachability_latency_node': 'node',\n        'fluctuability': 'node',\n        'fluctuability_global': 'global',\n        'bursty_coeff': 'edge,edge',\n        'bursty_coeff_edge': 'edge,edge',\n        'bursty_coeff_node': 'node',\n        'bursty_coeff_meanEdgePerNode': 'node',\n        'volatility_global': 'time',\n    }\n    if measure + calc + community in dimord_dict:\n        return dimord_dict[measure + calc + community]\n    else:\n        print('WARNINGL: get_dimord() returned unknown dimension labels')\n        return 'unknown'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_network_when(tnet, i=None, j=None, t=None, ij=None, logic='and', copy=False, asarray=False):\n    if isinstance(tnet, pd.DataFrame):\n        network = tnet\n        hdf5 = False\n    # Can add hdfstore\n    elif isinstance(tnet, object):\n        network = tnet.network\n        hdf5 = tnet.hdf5\n    if ij is not None and (i is not None or j is not None):\n        raise ValueError('ij cannoed be specifed along with i or j')\n    # Make non list inputs a list\n    if i is not None and not isinstance(i, list):\n        i = [i]\n    if j is not None and not isinstance(j, list):\n        j = [j]\n    if t is not None and not isinstance(t, list):\n        t = [t]\n    if ij is not None and not isinstance(ij, list):\n        ij = [ij]\n    if hdf5:\n        if i is not None and j is not None and t is not None and logic == 'and':\n            isinstr = 'i in ' + str(i) + ' & ' + 'j in ' + \\\n                str(j) + ' & ' + 't in ' + str(t)\n        elif ij is not None and t is not None and logic == 'and':\n            isinstr = '(i in ' + str(ij) + ' | ' + 'j in ' + \\\n                str(ij) + ') & ' + 't in ' + str(t)\n        elif ij is not None and t is not None and logic == 'or':\n            isinstr = 'i in ' + str(ij) + ' | ' + 'j in ' + \\\n                str(ij) + ' | ' + 't in ' + str(t)\n        elif i is not None and j is not None and logic == 'and':\n            isinstr = 'i in ' + str(i) + ' & ' + 'j in ' + str(j)\n        elif i is not None and t is not None and logic == 'and':\n            isinstr = 'i in ' + str(i) + ' & ' + 't in ' + str(t)\n        elif j is not None and t is not None and logic == 'and':\n            isinstr = 'j in ' + str(j) + ' & ' + 't in ' + str(t)\n        elif i is not None and j is not None and t is not None and logic == 'or':\n            isinstr = 'i in ' + str(i) + ' | ' + 'j in ' + \\\n                str(j) + ' | ' + 't in ' + str(t)\n        elif i is not None and j is not None and logic == 'or':\n            isinstr = 'i in ' + str(i) + ' | ' + 'j in ' + str(j)\n        elif i is not None and t is not None and logic == 'or':\n            isinstr = 'i in ' + str(i) + ' | ' + 't in ' + str(t)\n        elif j is not None and t is not None and logic == 'or':\n            isinstr = 'j in ' + str(j) + ' | ' + 't in ' + str(t)\n        elif i is not None:\n            isinstr = 'i in ' + str(i)\n        elif j is not None:\n            isinstr = 'j in ' + str(j)\n        elif t is not None:\n            isinstr = 't in ' + str(t)\n        elif ij is not None:\n            isinstr = 'i in ' + str(ij) + ' | ' + 'j in ' + str(ij)\n        df = pd.read_hdf(network, where=isinstr)\n    else:\n        if i is not None and j is not None and t is not None and logic == 'and':\n            df = network[(network['i'].isin(i)) & (\n                network['j'].isin(j)) & (network['t'].isin(t))]\n        elif ij is not None and t is not None and logic == 'and':\n            df = network[((network['i'].isin(ij)) | (\n                network['j'].isin(ij))) & (network['t'].isin(t))]\n        elif ij is not None and t is not None and logic == 'or':\n            df = network[((network['i'].isin(ij)) | (\n                network['j'].isin(ij))) | (network['t'].isin(t))]\n        elif i is not None and j is not None and logic == 'and':\n            df = network[(network['i'].isin(i)) & (network['j'].isin(j))]\n        elif i is not None and t is not None and logic == 'and':\n            df = network[(network['i'].isin(i)) & (network['t'].isin(t))]\n        elif j is not None and t is not None and logic == 'and':\n            df = network[(network['j'].isin(j)) & (network['t'].isin(t))]\n        elif i is not None and j is not None and t is not None and logic == 'or':\n            df = network[(network['i'].isin(i)) | (\n                network['j'].isin(j)) | (network['t'].isin(t))]\n        elif i is not None and j is not None and logic == 'or':\n            df = network[(network['i'].isin(i)) | (network['j'].isin(j))]\n        elif i is not None and t is not None and logic == 'or':\n            df = network[(network['i'].isin(i)) | (network['t'].isin(t))]\n        elif j is not None and t is not None and logic == 'or':\n            df = network[(network['j'].isin(j)) | (network['t'].isin(t))]\n        elif i is not None:\n            df = network[network['i'].isin(i)]\n        elif j is not None:\n            df = network[network['j'].isin(j)]\n        elif t is not None:\n            df = network[network['t'].isin(t)]\n        elif ij is not None:\n            df = network[(network['i'].isin(ij)) | (network['j'].isin(ij))]\n        if copy:\n            df = df.copy()\n    if asarray:\n        df = df.values\n    return df", "response": "Returns a subset of dataframe that matches index i j t and ij."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_supraadjacency_matrix(tnet, intersliceweight=1):\n    newnetwork = tnet.network.copy()\n    newnetwork['i'] = (tnet.network['i']) + \\\n        ((tnet.netshape[0]) * (tnet.network['t']))\n    newnetwork['j'] = (tnet.network['j']) + \\\n        ((tnet.netshape[0]) * (tnet.network['t']))\n    if 'weight' not in newnetwork.columns:\n        newnetwork['weight'] = 1\n    newnetwork.drop('t', axis=1, inplace=True)\n    timepointconns = pd.DataFrame()\n    timepointconns['i'] = np.arange(0, (tnet.N*tnet.T)-tnet.N)\n    timepointconns['j'] = np.arange(tnet.N, (tnet.N*tnet.T))\n    timepointconns['weight'] = intersliceweight\n    supranet = pd.concat([newnetwork, timepointconns]).reset_index(drop=True)\n    return supranet", "response": "Creates a supraadjacency matrix from a temporal network structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a TNET dataframe to a NetworkX object.", "response": "def tnet_to_nx(df, t=None):\n    \"\"\"\n    Creates undirected networkx object\n    \"\"\"\n    if t is not None:\n        df = get_network_when(df, t=t)\n    if 'weight' in df.columns:\n        nxobj = nx.from_pandas_edgelist(\n            df, source='i', target='j', edge_attr='weight')\n    else:\n        nxobj = nx.from_pandas_edgelist(df, source='i', target='j')\n    return nxobj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef temporal_louvain(tnet, resolution=1, intersliceweight=1, n_iter=100, negativeedge='ignore', randomseed=None, consensus_threshold=0.5, temporal_consensus=True, njobs=1):\n\n    tnet = process_input(tnet, ['C', 'G', 'TN'], 'TN')\n    # Divide resolution by the number of timepoints\n    resolution = resolution / tnet.T\n    supranet = create_supraadjacency_matrix(\n        tnet, intersliceweight=intersliceweight)\n    if negativeedge == 'ignore':\n        supranet = supranet[supranet['weight'] > 0]\n    nxsupra = tnet_to_nx(supranet)\n    np.random.seed(randomseed)\n    while True:\n        comtmp = []\n        with ProcessPoolExecutor(max_workers=njobs) as executor:\n            job = {executor.submit(_run_louvain, nxsupra, resolution, tnet.N, tnet.T) for n in range(n_iter)}\n            for j in as_completed(job):\n                comtmp.append(j.result())\n        comtmp = np.stack(comtmp)\n        comtmp = comtmp.transpose()\n        comtmp = np.reshape(comtmp, [tnet.N, tnet.T, n_iter], order='F')\n        if n_iter == 1: \n            break        \n        nxsupra_old = nxsupra\n        nxsupra = make_consensus_matrix(comtmp, consensus_threshold)\n        # If there was no consensus, there are no communities possible, return\n        if nxsupra is None:\n            break\n        if (nx.to_numpy_array(nxsupra, nodelist=np.arange(tnet.N*tnet.T)) == nx.to_numpy_array(nxsupra_old, nodelist=np.arange(tnet.N*tnet.T))).all():\n            break\n    communities = comtmp[:, :, 0]\n    if temporal_consensus == True:\n        communities = make_temporal_consensus(communities)\n    return communities", "response": "r This function creates a Louvain clustering for a temporal network."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_temporal_consensus(com_membership):\n\n    com_membership = np.array(com_membership)\n    # make first indicies be between 0 and 1.\n    com_membership[:, 0] = clean_community_indexes(com_membership[:, 0])\n    # loop over all timepoints, get jacccard distance in greedy manner for largest community to time period before\n    for t in range(1, com_membership.shape[1]):\n        ct, counts_t = np.unique(com_membership[:, t], return_counts=True)\n        ct = ct[np.argsort(counts_t)[::-1]]\n        c1back = np.unique(com_membership[:, t-1])\n        new_index = np.zeros(com_membership.shape[0])\n        for n in ct:\n            if len(c1back) > 0:\n                d = np.ones(int(c1back.max())+1)\n                for m in c1back:\n                    v1 = np.zeros(com_membership.shape[0])\n                    v2 = np.zeros(com_membership.shape[0])\n                    v1[com_membership[:, t] == n] = 1\n                    v2[com_membership[:, t-1] == m] = 1\n                    d[int(m)] = jaccard(v1, v2)\n                bestval = np.argmin(d)\n            else:\n                bestval = new_index.max() + 1\n            new_index[com_membership[:, t] == n] = bestval\n            c1back = np.array(np.delete(c1back, np.where(c1back == bestval)))\n        com_membership[:, t] = new_index\n    return com_membership", "response": "r Creates a temporal consensus matrix for the given community labels accross time - points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the flexibility of a node changes community.", "response": "def flexibility(communities):\n    \"\"\"\n    Amount a node changes community\n\n    Parameters\n    ----------\n    communities : array\n        Community array of shape (node,time)\n\n    Returns\n    --------\n    flex : array\n        Size with the flexibility of each node.\n\n    Notes\n    -----\n    Flexbility calculates the number of times a node switches its community label during a time series. It is normalized by the number of possible changes which could occur. It is important to make sure that the different community labels accross time points are not artbirary.\n\n    References\n    -----------\n    Bassett, DS, Wymbs N, Porter MA, Mucha P, Carlson JM, Grafton ST. Dynamic reconfiguration of human brain networks during learning. PNAS, 2011, 108(18):7641-6.\n    \"\"\"\n\n    # Preallocate\n    flex = np.zeros(communities.shape[0])\n    # Go from the second time point to last, compare with time-point before\n    for t in range(1, communities.shape[1]):\n        flex[communities[:, t] != communities[:, t-1]] += 1\n    # Normalize\n    flex = flex / (communities.shape[1] - 1)\n    return flex"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef slice_plot(netin, ax, nodelabels=None, timelabels=None, communities=None, plotedgeweights=False, edgeweightscalar=1, timeunit='', linestyle='k-', cmap=None, nodesize=100, nodekwargs=None, edgekwargs=None):\n    r'''\n\n    Fuction draws \"slice graph\" and exports axis handles\n\n\n    Parameters\n    ----------\n\n    netin : array, dict\n        temporal network input (graphlet or contact)\n    ax : matplotlib figure handles.\n    nodelabels : list\n        nodes labels. List of strings.\n    timelabels : list\n        labels of dimension Graph is expressed across. List of strings.\n    communities : array\n        array of size: (time) or (node,time). Nodes will be coloured accordingly.\n    plotedgeweights : bool\n        if True, edges will vary in size (default False)\n    edgeweightscalar : int\n        scalar to multiply all edges if tweaking is needed.\n    timeunit : string\n        unit time axis is in.\n    linestyle : string\n        line style of Bezier curves.\n    nodesize : int\n        size of nodes\n    nodekwargs : dict\n        any additional kwargs for matplotlib.plt.scatter for the nodes\n    edgekwargs : dict\n        any additional kwargs for matplotlib.plt.plots for the edges\n\n\n    Returns\n    ---------\n    ax : axis handle of slice graph\n\n\n    Examples\n    ---------\n\n\n    Create a network with some metadata\n\n    >>> import numpy as np\n    >>> import teneto\n    >>> import matplotlib.pyplot as plt\n    >>> np.random.seed(2017) # For reproduceability\n    >>> N = 5 # Number of nodes\n    >>> T = 10 # Number of timepoints\n    >>> # Probability of edge activation\n    >>> birth_rate = 0.2\n    >>> death_rate = .9\n    >>> # Add node names into the network and say time units are years, go 1 year per graphlet and startyear is 2007\n    >>> cfg={}\n    >>> cfg['Fs'] = 1\n    >>> cfg['timeunit'] = 'Years'\n    >>> cfg['t0'] = 2007 #First year in network\n    >>> cfg['nodelabels'] = ['Ashley','Blake','Casey','Dylan','Elliot'] # Node names\n    >>> #Generate network\n    >>> C = teneto.generatenetwork.rand_binomial([N,T],[birth_rate, death_rate],'contact','bu',netinfo=cfg)\n\n    Now this network can be plotted\n\n    >>> fig,ax = plt.subplots(figsize=(10,3))\n    >>> ax = teneto.plot.slice_plot(C, ax, cmap='Pastel2')\n    >>> plt.tight_layout()\n    >>> fig.show()\n\n    .. plot::\n\n        import numpy as np\n        import teneto\n        import matplotlib.pyplot as plt\n        np.random.seed(2017) # For reproduceability\n        N = 5 # Number of nodes\n        T = 10 # Number of timepoints\n        # Probability of edge activation\n        birth_rate = 0.2\n        death_rate = .9\n        # Add node names into the network and say time units are years, go 1 year per graphlet and startyear is 2007\n        cfg={}\n        cfg['Fs'] = 1\n        cfg['timeunit'] = 'Years'\n        cfg['t0'] = 2007 #First year in network\n        cfg['nodelabels'] = ['Ashley','Blake','Casey','Dylan','Elliot']\n        #Generate network\n        C = teneto.generatenetwork.rand_binomial([N,T],[birth_rate, death_rate],'contact','bu',netinfo=cfg)\n        fig,ax = plt.subplots(figsize=(10,3))\n        cmap = 'Pastel2'\n        ax = teneto.plot.slice_plot(C,ax,cmap=cmap)\n        plt.tight_layout()\n        fig.show()\n\n\n    '''\n    # Get input type (C or G)\n    inputType = checkInput(netin)\n    # Convert C representation to G\n\n    if inputType == 'G':\n        netin = graphlet2contact(netin)\n        inputType = 'C'\n    edgelist = [tuple(np.array(e[0:2]) + e[2] * netin['netshape'][0])\n                for e in netin['contacts']]\n\n    if nodelabels is not None and len(nodelabels) == netin['netshape'][0]:\n        pass\n    elif nodelabels is not None and len(nodelabels) != netin['netshape'][0]:\n        raise ValueError('specified node label length does not match netshape')\n    elif nodelabels is None and netin['nodelabels'] == '':\n        nodelabels = np.arange(1, netin['netshape'][0] + 1)\n    else:\n        nodelabels = netin['nodelabels']\n\n    if timelabels is not None and len(timelabels) == netin['netshape'][-1]:\n        pass\n    elif timelabels is not None and len(timelabels) != netin['netshape'][-1]:\n        raise ValueError('specified time label length does not match netshape')\n    elif timelabels is None and str(netin['t0']) == '':\n        timelabels = np.arange(1, netin['netshape'][-1] + 1)\n    else:\n        timelabels = np.arange(netin['t0'], netin['Fs'] *\n                               netin['netshape'][-1] + netin['t0'], netin['Fs'])\n\n    if timeunit is None:\n        timeunit = netin['timeunit']\n\n    timeNum = len(timelabels)\n    nodeNum = len(nodelabels)\n    posy = np.tile(list(range(0, nodeNum)), timeNum)\n    posx = np.repeat(list(range(0, timeNum)), nodeNum)\n\n    if nodekwargs is None:\n        nodekwargs = {}\n    if edgekwargs is None:\n        edgekwargs = {}\n    if cmap:\n        nodekwargs['cmap'] = cmap\n    if 'c' not in nodekwargs:\n        nodekwargs['c'] = posy\n    if communities is not None:\n        # check if temporal or static\n        if len(communities.shape) == 1:\n            nodekwargs['c'] = np.tile(communities, timeNum)\n        else:\n            nodekwargs['c'] = communities.flatten(order='F')\n\n\n    # plt.plot(points)\n    # Draw Bezier vectors around egde positions\n    for ei, edge in enumerate(edgelist):\n        if plotedgeweights == True and netin['nettype'][0] == 'w':\n            edgekwargs['linewidth'] = netin['values'][ei] * edgeweightscalar\n        bvx, bvy = bezier_points(\n            (posx[edge[0]], posy[edge[0]]), (posx[edge[1]], posy[edge[1]]), nodeNum, 20)\n        ax.plot(bvx, bvy, linestyle, **edgekwargs)\n    ax.set_yticks(range(0, len(nodelabels)))\n    ax.set_xticks(range(0, len(timelabels)))\n    ax.set_yticklabels(nodelabels)\n    ax.set_xticklabels(timelabels)\n    ax.grid()\n    ax.set_frame_on(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.set_xlim([min(posx) - 1, max(posx) + 1])\n    ax.set_ylim([min(posy) - 1, max(posy) + 1])\n    ax.scatter(posx, posy, s=nodesize, zorder=10, **nodekwargs)\n    if timeunit != '':\n        timeunit = ' (' + timeunit + ')'\n    ax.set_xlabel('Time' + timeunit)\n\n    return ax", "response": "r Draws a slice graph and exports axis handles"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef local_variation(data):\n\n    ict = 0  # are ict present\n    if isinstance(data, dict):\n        # This could be done better\n        if [k for k in list(data.keys()) if k == 'intercontacttimes'] == ['intercontacttimes']:\n            ict = 1\n    # if shortest paths are not calculated, calculate them\n    if ict == 0:\n        data = intercontacttimes(data)\n\n    if data['nettype'][1] == 'u':\n        ind = np.triu_indices(data['intercontacttimes'].shape[0], k=1)\n    if data['nettype'][1] == 'd':\n        triu = np.triu_indices(data['intercontacttimes'].shape[0], k=1)\n        tril = np.tril_indices(data['intercontacttimes'].shape[0], k=-1)\n        ind = [[], []]\n        ind[0] = np.concatenate([tril[0], triu[0]])\n        ind[1] = np.concatenate([tril[1], triu[1]])\n        ind = tuple(ind)\n\n    ict_shape = data['intercontacttimes'].shape\n\n    lv = np.zeros(ict_shape)\n\n    for n in range(len(ind[0])):\n        icts = data['intercontacttimes'][ind[0][n], ind[1][n]]\n        # make sure there is some contact\n        if len(icts) > 0:\n            lv_nonnorm = np.sum(\n                np.power((icts[:-1] - icts[1:]) / (icts[:-1] + icts[1:]), 2))\n            lv[ind[0][n], ind[1][n]] = (3/len(icts)) * lv_nonnorm\n        else:\n            lv[ind[0][n], ind[1][n]] = np.nan\n\n    # Make symetric if undirected\n    if data['nettype'][1] == 'u':\n        lv = lv + lv.transpose()\n\n    for n in range(lv.shape[0]):\n        lv[n, n] = np.nan\n\n    return lv", "response": "r Calculates the local variation of the given data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a filename sub - 01_run - 01. gz it will return a list of filenames with the suffix.", "response": "def drop_bids_suffix(fname):\n    \"\"\"\n    Given a filename sub-01_run-01_preproc.nii.gz, it will return ['sub-01_run-01', '.nii.gz']\n\n    Parameters\n    ----------\n\n    fname : str\n        BIDS filename with suffice. Directories should not be included.\n\n    Returns\n    -------\n    fname_head : str\n        BIDS filename with\n    fileformat : str\n        The file format (text after suffix)\n\n    Note\n    ------\n    This assumes that there are no periods in the filename\n    \"\"\"\n    if '/' in fname:\n        split = fname.split('/')\n        dirnames = '/'.join(split[:-1]) + '/'\n        fname = split[-1]\n    else:\n        dirnames = ''\n    tags = [tag for tag in fname.split('_') if '-' in tag]\n    fname_head = '_'.join(tags)\n    fileformat = '.' + '.'.join(fname.split('.')[1:])\n    return dirnames + fname_head, fileformat"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_tabular_file(fname, return_meta=False, header=True, index_col=True):\n    if index_col:\n        index_col = 0\n    else:\n        index_col = None\n    if header:\n        header = 0\n    else:\n        header = None\n\n    df = pd.read_csv(fname, header=header, index_col=index_col, sep='\\t')\n\n    if return_meta:\n        json_fname = fname.replace('tsv', 'json')\n        meta = pd.read_json(json_fname)\n        return df, meta\n    else:\n        return df", "response": "Loads a single tab - delimited file into a pandas dataframe"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a single object from a file.", "response": "def get_sidecar(fname, allowedfileformats='default'):\n    \"\"\"\n    Loads sidecar or creates one\n    \"\"\"\n    if allowedfileformats == 'default':\n        allowedfileformats = ['.tsv', '.nii.gz']\n    for f in allowedfileformats:\n        fname = fname.split(f)[0]\n    fname += '.json'\n    if os.path.exists(fname):\n        with open(fname) as fs:\n            sidecar = json.load(fs)\n    else:\n        sidecar = {}\n    if 'filestatus' not in sidecar:\n        sidecar['filestatus'] = {}\n        sidecar['filestatus']['reject'] = False\n        sidecar['filestatus']['reason'] = []\n    return sidecar"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake an exclusion criteria string and returns the function and threshold for the exclusion criteria.", "response": "def process_exclusion_criteria(exclusion_criteria):\n    \"\"\"\n    Parses an exclusion critera string to get the function and threshold.\n\n    Parameters\n    ----------\n        exclusion_criteria : list\n            list of strings where each string is of the format [relation][threshold]. E.g. \\'<0.5\\' or \\'>=1\\'\n\n    Returns\n    -------\n        relfun : list\n            list of numpy functions for the exclusion criteria\n        threshold : list\n            list of floats for threshold for each relfun\n\n\n    \"\"\"\n    relfun = []\n    threshold = []\n    for ec in exclusion_criteria:\n        if ec[0:2] == '>=':\n            relfun.append(np.greater_equal)\n            threshold.append(float(ec[2:]))\n        elif ec[0:2] == '<=':\n            relfun.append(np.less_equal)\n            threshold.append(float(ec[2:]))\n        elif ec[0] == '>':\n            relfun.append(np.greater)\n            threshold.append(float(ec[1:]))\n        elif ec[0] == '<':\n            relfun.append(np.less)\n            threshold.append(float(ec[1:]))\n        else:\n            raise ValueError('exclusion crieria must being with >,<,>= or <=')\n    return relfun, threshold"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reachability_latency(tnet=None, paths=None, rratio=1, calc='global'):\n\n    if tnet is not None and paths is not None:\n        raise ValueError('Only network or path input allowed.')\n    if tnet is None and paths is None:\n        raise ValueError('No input.')\n    # if shortest paths are not calculated, calculate them\n    if tnet is not None:\n        paths = shortest_temporal_path(tnet)\n\n    pathmat = np.zeros([paths[['from', 'to']].max().max(\n    )+1, paths[['from', 'to']].max().max()+1, paths[['t_start']].max().max()+1]) * np.nan\n    pathmat[paths['from'].values, paths['to'].values,\n            paths['t_start'].values] = paths['temporal-distance']\n\n    netshape = pathmat.shape\n\n    edges_to_reach = netshape[0] - np.round(netshape[0] * rratio)\n\n    reach_lat = np.zeros([netshape[1], netshape[2]]) * np.nan\n    for t_ind in range(0, netshape[2]):\n        paths_sort = -np.sort(-pathmat[:, :, t_ind], axis=1)\n        reach_lat[:, t_ind] = paths_sort[:, edges_to_reach]\n    if calc == 'global':\n        reach_lat = np.nansum(reach_lat)\n        reach_lat = reach_lat / ((netshape[0]) * netshape[2])\n    elif calc == 'nodes':\n        reach_lat = np.nansum(reach_lat, axis=1)\n        reach_lat = reach_lat / (netshape[2])\n    return reach_lat", "response": "Returns the reachability latency for a given set of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fluctuability(netin, calc='global'):\n\n    # Get input type (C or G)\n    netin, _ = process_input(netin, ['C', 'G', 'TN'])\n\n    netin[netin != 0] = 1\n    unique_edges = np.sum(netin, axis=2)\n    unique_edges[unique_edges > 0] = 1\n    unique_edges[unique_edges == 0] = 0\n\n    fluct = (np.sum(unique_edges)) / np.sum(netin)\n    return fluct", "response": "r Returns the fluctuability of a temporal network."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the recruitment coefficient for each node in the tree.", "response": "def recruitment(temporalcommunities, staticcommunities):\n    \"\"\"\n    Calculates recruitment coefficient for each node. Recruitment coefficient is the average probability of nodes from the\n      same static communities being in the same temporal communities at other time-points or during different tasks.\n\n    Parameters:\n    ------------\n    temporalcommunities :  array\n        temporal communities vector (node,time)\n    staticcommunities : array\n        Static communities vector for each node\n\n    Returns:\n    -------\n    Rcoeff : array\n        recruitment coefficient for each node\n\n    References:\n    -----------\n\n    Danielle S. Bassett, Muzhi Yang, Nicholas F. Wymbs, Scott T. Grafton.\n    Learning-Induced Autonomy of Sensorimotor Systems. Nat Neurosci. 2015 May;18(5):744-51.\n\n    Marcelo Mattar, Michael W. Cole, Sharon Thompson-Schill, Danielle S. Bassett. A Functional\n    Cartography of Cognitive Systems. PLoS Comput Biol. 2015 Dec 2;11(12):e1004533.\n    \"\"\"\n\n    # make sure the static and temporal communities have the same number of nodes\n    if staticcommunities.shape[0] != temporalcommunities.shape[0]:\n        raise ValueError(   \n            'Temporal and static communities have different dimensions')\n\n    alleg = allegiance(temporalcommunities)\n\n    Rcoeff = np.zeros(len(staticcommunities))\n\n    for i, statcom in enumerate(staticcommunities):\n        Rcoeff[i] = np.mean(alleg[i, staticcommunities == statcom])\n\n    return Rcoeff"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the integration coefficient for each node.", "response": "def integration(temporalcommunities, staticcommunities):\n    \"\"\"\n    Calculates the integration coefficient for each node. Measures the average probability\n    that a node is in the same community as nodes from other systems.\n\n      Parameters:\n      ------------\n      temporalcommunities :  array\n          temporal communities vector (node,time)\n      staticcommunities : array\n          Static communities vector for each node\n\n      Returns:\n      -------\n      Icoeff : array\n          integration coefficient for each node\n\n    References:\n    ----------\n    Danielle S. Bassett, Muzhi Yang, Nicholas F. Wymbs, Scott T. Grafton.\n    Learning-Induced Autonomy of Sensorimotor Systems. Nat Neurosci. 2015 May;18(5):744-51.\n\n    Marcelo Mattar, Michael W. Cole, Sharon Thompson-Schill, Danielle S. Bassett.\n    A Functional Cartography of Cognitive Systems. PLoS Comput Biol. 2015 Dec\n    2;11(12):e1004533.\n    \"\"\"\n\n    # make sure the static and temporal communities have the same number of nodes\n    if staticcommunities.shape[0] != temporalcommunities.shape[0]:\n        raise ValueError(\n            'Temporal and static communities have different dimensions')\n\n    alleg = allegiance(temporalcommunities)\n\n    Icoeff = np.zeros(len(staticcommunities))\n\n    # calc integration for each node\n    for i, statcom in enumerate(len(staticcommunities)):\n        Icoeff[i] = np.mean(alleg[i, staticcommunities != statcom])\n\n    return Icoeff"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef intercontacttimes(tnet):\n\n    # Process input\n    tnet = process_input(tnet, ['C', 'G', 'TN'], 'TN')\n\n    if tnet.nettype[0] == 'w':\n        print('WARNING: assuming connections to be binary when computing intercontacttimes')\n\n    # Each time series is padded with a 0 at the start and end. Then t[0:-1]-[t:].\n    # Then discard the noninformative ones (done automatically)\n    # Finally return back as np array\n    contacts = np.array([[None] * tnet.netshape[0]] * tnet.netshape[0])\n\n    if tnet.nettype[1] == 'u':\n        for i in range(0, tnet.netshape[0]):\n            for j in range(i + 1, tnet.netshape[0]):\n                edge_on = tnet.get_network_when(i=i, j=j)['t'].values\n                if len(edge_on) > 0:\n                    edge_on_diff = edge_on[1:] - edge_on[:-1]\n                    contacts[i, j] = np.array(edge_on_diff)\n                    contacts[j, i] = np.array(edge_on_diff)\n                else:\n                    contacts[i, j] = []\n                    contacts[j, i] = []\n    elif tnet.nettype[1] == 'd':\n        for i in range(0, tnet.netshape[0]):\n            for j in range(0, tnet.netshape[0]):\n                edge_on = tnet.get_network_when(i=i, j=j)['t'].values\n                if len(edge_on) > 0:\n                    edge_on_diff = edge_on[1:] - edge_on[:-1]\n                    contacts[i, j] = np.array(edge_on_diff)\n                else:\n                    contacts[i, j] = []\n\n    out = {}\n    out['intercontacttimes'] = contacts\n    out['nettype'] = tnet.nettype\n    return out", "response": "This function calculates the inter - contact times of each edge in a network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gen_report(report, sdir='./', report_name='report.html'):\n\n    # Create report directory\n    if not os.path.exists(sdir):\n        os.makedirs(sdir)\n\n    # Add a slash to file directory if not included to avoid DirNameFleName\n    # instead of DirName/FileName being creaated\n    if sdir[-1] != '/':\n        sdir += '/'\n\n    report_html = '<html><body>'\n\n    if 'method' in report.keys():\n\n        report_html += \"<h1>Method: \" + report['method'] + \"</h1><p>\"\n\n        for i in report[report['method']]:\n\n            if i == 'taper_window':\n\n                fig, ax = plt.subplots(1)\n                ax.plot(report[report['method']]['taper_window'],\n                        report[report['method']]['taper'])\n                ax.set_xlabel('Window (time). 0 in middle of window.')\n                ax.set_title(\n                    'Taper from ' + report[report['method']]['distribution'] + ' distribution (PDF).')\n                fig.savefig(sdir + 'taper.png')\n\n                report_html += \"<img src='./taper.png' width=500>\" + \"<p>\"\n\n            else:\n\n                report_html += \"- <b>\" + i + \"</b>: \" + \\\n                    str(report[report['method']][i]) + \"<br>\"\n\n    if 'postprocess' in report.keys():\n\n        report_html += \"<p><h2>Postprocessing:</h2><p>\"\n\n        report_html += \"<b>Pipeline: </b>\"\n\n        for i in report['postprocess']:\n\n            report_html += \" \" + i + \",\"\n\n        for i in report['postprocess']:\n\n            report_html += \"<p><h3>\" + i + \"</h3><p>\"\n\n            for j in report[i]:\n\n                if j == 'lambda':\n\n                    report_html += \"- <b>\" + j + \"</b>: \" + \"<br>\"\n\n                    lambda_val = np.array(report['boxcox']['lambda'])\n                    fig, ax = plt.subplots(1)\n                    ax.hist(lambda_val[:, -1])\n                    ax.set_xlabel('lambda')\n                    ax.set_ylabel('frequency')\n                    ax.set_title('Histogram of lambda parameter')\n                    fig.savefig(sdir + 'boxcox_lambda.png')\n\n                    report_html += \"<img src='./boxcox_lambda.png' width=500>\" + \"<p>\"\n                    report_html += \"Data located in \" + sdir + \"boxcox_lambda.csv <p>\"\n\n                    np.savetxt(sdir + \"boxcox_lambda.csv\",\n                               lambda_val, delimiter=\",\")\n\n                else:\n\n                    report_html += \"- <b>\" + j + \"</b>: \" + \\\n                        str(report[i][j]) + \"<br>\"\n\n    report_html += '</body></html>'\n\n    with open(sdir + report_name, 'w') as file:\n        file.write(report_html)\n    file.close()", "response": "Generates a report of derivation and postprocessing steps in teneto. derive\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_history(self, fname, fargs, init=0):\n        if init == 1:\n            self.history = []\n        self.history.append([fname, fargs])", "response": "Adds a processing step to TenetoBIDS. history."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports the history of the current TenetoBIDS instance to dirname", "response": "def export_history(self, dirname):\n        \"\"\"\n        Exports TenetoBIDShistory.py, tenetoinfo.json, requirements.txt (modules currently imported) to dirname\n\n        Parameters\n        ---------\n        dirname : str\n            directory to export entire TenetoBIDS history.\n\n        \"\"\"\n        mods = [(m.__name__, m.__version__)\n                for m in sys.modules.values() if m if hasattr(m, '__version__')]\n        with open(dirname + '/requirements.txt', 'w') as f:\n            for m in mods:\n                m = list(m)\n                if not isinstance(m[1], str):\n                    m[1] = m[1].decode(\"utf-8\")\n                f.writelines(m[0] + ' == ' + m[1] + '\\n')\n\n        with open(dirname + '/TenetoBIDShistory.py', 'w') as f:\n            f.writelines('import teneto\\n')\n            for func, args in self.history:\n                f.writelines(func + '(**' + str(args) + ')\\n')\n\n        with open(dirname + '/tenetoinfo.json', 'w') as f:\n            json.dump(self.tenetoinfo, f)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef derive_temporalnetwork(self, params, update_pipeline=True, tag=None, njobs=1, confound_corr_report=True):\n        if not njobs:\n            njobs = self.njobs\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n\n        files = self.get_selected_files(quiet=1)\n        confound_files = self.get_selected_files(quiet=1, pipeline='confound')\n        if confound_files:\n            confounds_exist = True\n        else:\n            confounds_exist = False\n        if not confound_corr_report:\n            confounds_exist = False\n\n        if not tag:\n            tag = ''\n        else:\n            tag = 'desc-' + tag\n\n        with ProcessPoolExecutor(max_workers=njobs) as executor:\n            job = {executor.submit(self._derive_temporalnetwork, f, i, tag, params,\n                                   confounds_exist, confound_files) for i, f in enumerate(files) if f}\n            for j in as_completed(job):\n                j.result()\n\n        if update_pipeline == True:\n            if not self.confound_pipeline and len(self.get_selected_files(quiet=1, pipeline='confound')) > 0:\n                self.set_confound_pipeline = self.pipeline\n            self.set_pipeline('teneto_' + teneto.__version__)\n            self.set_pipeline_subdir('tvc')\n            self.set_bids_suffix('tvcconn')", "response": "Derive time - vary connectivity on the selected files."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions called by TenetoBIDS. derive_temporalnetwork for concurrent processing.", "response": "def _derive_temporalnetwork(self, f, i, tag, params, confounds_exist, confound_files):\n        \"\"\"\n        Funciton called by TenetoBIDS.derive_temporalnetwork for concurrent processing.\n        \"\"\"\n        data = load_tabular_file(f, index_col=True, header=True)\n\n        fs, _ = drop_bids_suffix(f)\n        save_name, save_dir, _ = self._save_namepaths_bids_derivatives(\n            fs, tag, 'tvc', 'tvcconn')\n        if 'weight-var' in params.keys():\n            if params['weight-var'] == 'from-subject-fc':\n                fc_files = self.get_selected_files(\n                    quiet=1, pipeline='functionalconnectivity', forfile=f)\n                if len(fc_files) == 1:\n                    # Could change to load_data call\n                    params['weight-var'] = load_tabular_file(\n                        fc_files[0]).values\n                else:\n                    raise ValueError('Cannot correctly find FC files')\n\n        if 'weight-mean' in params.keys():\n            if params['weight-mean'] == 'from-subject-fc':\n                fc_files = self.get_selected_files(\n                    quiet=1, pipeline='functionalconnectivity', forfile=f)\n                if len(fc_files) == 1:\n                    # Could change to load_data call\n                    params['weight-mean'] = load_tabular_file(\n                        fc_files[0]).values\n                else:\n                    raise ValueError('Cannot correctly find FC files')\n\n        params['report'] = 'yes'\n        params['report_path'] = save_dir + '/report/'\n        params['report_filename'] = save_name + '_derivationreport.html'\n\n        if not os.path.exists(params['report_path']):\n            os.makedirs(params['report_path'])\n\n        if 'dimord' not in params:\n            params['dimord'] = 'time,node'\n\n        dfc = teneto.timeseries.derive_temporalnetwork(data.values, params)\n        dfc_net = TemporalNetwork(from_array=dfc, nettype='wu')\n        dfc_net.network.to_csv(save_dir + save_name + '.tsv', sep='\\t')\n\n        sidecar = get_sidecar(f)\n        sidecar['tvc'] = params\n        if 'weight-var' in sidecar['tvc']:\n            sidecar['tvc']['weight-var'] = True\n            sidecar['tvc']['fc source'] = fc_files\n        if 'weight-mean' in sidecar['tvc']:\n            sidecar['tvc']['weight-mean'] = True\n            sidecar['tvc']['fc source'] = fc_files\n        sidecar['tvc']['inputfile'] = f\n        sidecar['tvc']['description'] = 'Time varying connectivity information.'\n        with open(save_dir + save_name + '.json', 'w') as fs:\n            json.dump(sidecar, fs)\n\n        if confounds_exist:\n            analysis_step = 'tvc-derive'\n            df = pd.read_csv(confound_files[i], sep='\\t')\n            df = df.fillna(df.median())\n            ind = np.triu_indices(dfc.shape[0], k=1)\n            dfc_df = pd.DataFrame(dfc[ind[0], ind[1], :].transpose())\n            # If windowed, prune df so that it matches with dfc_df\n            if len(df) != len(dfc_df):\n                df = df.iloc[int(np.round((params['windowsize']-1)/2)): int(np.round((params['windowsize']-1)/2)+len(dfc_df))]\n                df.reset_index(inplace=True, drop=True)\n            # NOW CORRELATE DF WITH DFC BUT ALONG INDEX NOT DF.\n            dfc_df_z = (dfc_df - dfc_df.mean())\n            df_z = (df - df.mean())\n            R_df = dfc_df_z.T.dot(df_z).div(len(dfc_df)).div(\n                df_z.std(ddof=0)).div(dfc_df_z.std(ddof=0), axis=0)\n            R_df_describe = R_df.describe()\n            desc_index = R_df_describe.index\n            confound_report_dir = params['report_path'] + \\\n                '/' + save_name + '_confoundcorr/'\n            confound_report_figdir = confound_report_dir + 'figures/'\n            if not os.path.exists(confound_report_figdir):\n                os.makedirs(confound_report_figdir)\n            report = '<html><body>'\n            report += '<h1> Correlation of ' + analysis_step + ' and confounds.</h1>'\n            for c in R_df.columns:\n                fig, ax = plt.subplots(1)\n                ax = sns.distplot(\n                    R_df[c], hist=False, color='m', ax=ax, kde_kws={\"shade\": True})\n                fig.savefig(confound_report_figdir + c + '.png')\n                plt.close(fig)\n                report += '<h2>' + c + '</h2>'\n                for ind_name, r in enumerate(R_df_describe[c]):\n                    report += str(desc_index[ind_name]) + ': '\n                    report += str(r) + '<br>'\n                report += 'Distribution of corrlation values:'\n                report += '<img src=' + \\\n                    os.path.abspath(confound_report_figdir) + \\\n                    '/' + c + '.png><br><br>'\n            report += '</body></html>'\n\n            with open(confound_report_dir + save_name + '_confoundcorr.html', 'w') as file:\n                file.write(report)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes connectivity matrix for each of the subjects.", "response": "def make_functional_connectivity(self, njobs=None, returngroup=False, file_hdr=None, file_idx=None):\n        \"\"\"\n        Makes connectivity matrix for each of the subjects.\n\n        Parameters\n        ----------\n        returngroup : bool, default=False\n            If true, returns the group average connectivity matrix.\n        njobs : int\n            How many parallel jobs to run\n        file_idx : bool\n            Default False, true if to ignore index column in loaded file.\n        file_hdr : bool\n            Default False, true if to ignore header row in loaded file.\n\n        Returns\n        -------\n        Saves data in derivatives/teneto_<version>/.../fc/\n        R_group : array\n            if returngroup is true, the average connectivity matrix is returned.\n\n        \"\"\"\n        if not njobs:\n            njobs = self.njobs\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n        files = self.get_selected_files(quiet=1)\n\n        R_group = []\n\n        with ProcessPoolExecutor(max_workers=njobs) as executor:\n            job = {executor.submit(\n                self._run_make_functional_connectivity, f, file_hdr, file_idx) for f in files}\n            for j in as_completed(job):\n                R_group.append(j.result())\n\n        if returngroup:\n            # Fisher tranform -> mean -> inverse fisher tranform\n            R_group = np.tanh(np.mean(np.arctanh(np.array(R_group)), axis=0))\n            return np.array(R_group)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _save_namepaths_bids_derivatives(self, f, tag, save_directory, suffix=None):\n        file_name = f.split('/')[-1].split('.')[0]\n        if tag != '':\n            tag = '_' + tag\n        if suffix:\n            file_name, _ = drop_bids_suffix(file_name)\n            save_name = file_name + tag\n            save_name += '_' + suffix\n        else:\n            save_name = file_name + tag\n        paths_post_pipeline = f.split(self.pipeline)\n        if self.pipeline_subdir:\n            paths_post_pipeline = paths_post_pipeline[1].split(self.pipeline_subdir)[\n                0]\n        else:\n            paths_post_pipeline = paths_post_pipeline[1].split(file_name)[0]\n        base_dir = self.BIDS_dir + '/derivatives/' + 'teneto_' + \\\n            teneto.__version__ + '/' + paths_post_pipeline + '/'\n        save_dir = base_dir + '/' + save_directory + '/'\n        if not os.path.exists(save_dir):\n            # A case has happened where this has been done in parallel and an error was raised. So do try/except\n            try:\n                os.makedirs(save_dir)\n            except:\n                # Wait 2 seconds so that the error does not try and save something in the directory before it is created\n                time.sleep(2)\n        if not os.path.exists(self.BIDS_dir + '/derivatives/' + 'teneto_' + teneto.__version__ + '/dataset_description.json'):\n            try:\n                with open(self.BIDS_dir + '/derivatives/' + 'teneto_' + teneto.__version__ + '/dataset_description.json', 'w') as fs:\n                    json.dump(self.tenetoinfo, fs)\n            except:\n                # Same as above, just in case parallel does duplicaiton\n                time.sleep(2)\n        return save_name, save_dir, base_dir", "response": "Save the namepaths of the BIDS derivatives for a specific tag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tags(self, tag, quiet=1):\n        if not self.pipeline:\n            print('Please set pipeline first.')\n            self.get_pipeline_alternatives(quiet)\n        else:\n            if tag == 'sub':\n                datapath = self.BIDS_dir + '/derivatives/' + self.pipeline + '/'\n                tag_alternatives = [\n                    f.split('sub-')[1] for f in os.listdir(datapath) if os.path.isdir(datapath + f) and 'sub-' in f]\n            elif tag == 'ses':\n                tag_alternatives = []\n                for sub in self.bids_tags['sub']:\n                    tag_alternatives += [f.split('ses-')[1] for f in os.listdir(\n                        self.BIDS_dir + '/derivatives/' + self.pipeline + '/' + 'sub-' + sub) if 'ses' in f]\n                tag_alternatives = set(tag_alternatives)\n            else:\n                files = self.get_selected_files(quiet=1)\n                tag_alternatives = []\n                for f in files:\n                    f = f.split('.')[0]\n                    f = f.split('/')[-1]\n                    tag_alternatives += [t.split('-')[1]\n                                         for t in f.split('_') if t.split('-')[0] == tag]\n                tag_alternatives = set(tag_alternatives)\n            if quiet == 0:\n                print(tag + ' alternatives: ' + ', '.join(tag_alternatives))\n            return list(tag_alternatives)", "response": "Returns which tag alternatives can be identified in the BIDS derivatives structure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the list of alternatives in the pipeline.", "response": "def get_pipeline_alternatives(self, quiet=0):\n        \"\"\"\n        The pipeline are the different outputs that are placed in the ./derivatives directory.\n\n        get_pipeline_alternatives gets those which are found in the specified BIDS directory structure.\n        \"\"\"\n        if not os.path.exists(self.BIDS_dir + '/derivatives/'):\n            print('Derivative directory not found. Is the data preprocessed?')\n        else:\n            pipeline_alternatives = os.listdir(self.BIDS_dir + '/derivatives/')\n            if quiet == 0:\n                print('Derivative alternatives: ' +\n                      ', '.join(pipeline_alternatives))\n            return list(pipeline_alternatives)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnoting ----- This function currently returns the wrong folders and will be fixed in the future. This function should return ./derivatives/pipeline/sub-xx/[ses-yy/][func/]/pipeline_subdir But it does not care about ses-yy at the moment.", "response": "def get_pipeline_subdir_alternatives(self, quiet=0):\n        \"\"\"\n        Note\n        -----\n\n        This function currently returns the wrong folders and will be fixed in the future.\n\n        This function should return ./derivatives/pipeline/sub-xx/[ses-yy/][func/]/pipeline_subdir\n        But it does not care about ses-yy at the moment.\n        \"\"\"\n        if not self.pipeline:\n            print('Please set pipeline first.')\n            self.get_pipeline_alternatives()\n        else:\n            pipeline_subdir_alternatives = []\n            for s in self.bids_tags['sub']:\n                derdir_files = os.listdir(\n                    self.BIDS_dir + '/derivatives/' + self.pipeline + '/sub-' + s + '/func/')\n                pipeline_subdir_alternatives += [\n                    f for f in derdir_files if os.path.isdir(self.BIDS_dir + '/derivatives/' + self.pipeline + '/sub-' + s + '/func/' + f)]\n            pipeline_subdir_alternatives = set(pipeline_subdir_alternatives)\n            if quiet == 0:\n                print('Pipeline_subdir alternatives: ' +\n                      ', '.join(pipeline_subdir_alternatives))\n            return list(pipeline_subdir_alternatives)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_selected_files(self, pipeline='pipeline', forfile=None, quiet=0, allowedfileformats='default'):\n        # This could be mnade better\n        file_dict = dict(self.bids_tags)\n        if allowedfileformats == 'default':\n            allowedfileformats = ['.tsv', '.nii.gz']\n        if forfile:\n            if isinstance(forfile, str):\n                forfile = get_bids_tag(forfile, 'all')\n            for n in forfile.keys():\n                file_dict[n] = [forfile[n]]\n        non_entries = []\n        for n in file_dict:\n            if not file_dict[n]:\n                non_entries.append(n)\n        for n in non_entries:\n            file_dict.pop(n)\n\n        # Only keep none empty elemenets\n        file_components = []\n        for k in ['sub', 'ses', 'task', 'run']:\n            if k in file_dict:\n                file_components.append([k + '-' + t for t in file_dict[k]])\n\n        file_list = list(itertools.product(*file_components))\n\n        # Specify main directory\n        if pipeline == 'pipeline':\n            mdir = self.BIDS_dir + '/derivatives/' + self.pipeline\n        elif pipeline == 'confound' and self.confound_pipeline:\n            mdir = self.BIDS_dir + '/derivatives/' + self.confound_pipeline\n        elif pipeline == 'confound':\n            mdir = self.BIDS_dir + '/derivatives/' + self.pipeline\n        elif pipeline == 'functionalconnectivity':\n            mdir = self.BIDS_dir + '/derivatives/teneto_' + teneto.__version__\n        else:\n            raise ValueError('unknown request')\n\n        found_files = []\n\n        for f in file_list:\n            wdir = str(mdir)\n            sub = [t for t in f if t.startswith('sub')]\n            ses = [t for t in f if t.startswith('ses')]\n            wdir += '/' + sub[0] + '/'\n            if ses:\n                wdir += '/' + ses[0] + '/'\n            wdir += '/func/'\n\n            if pipeline == 'pipeline':\n                wdir += '/' + self.pipeline_subdir + '/'\n                fileending = [self.bids_suffix +\n                              f for f in allowedfileformats]\n            elif pipeline == 'functionalconnectivity':\n                wdir += '/fc/'\n                fileending = ['conn' + f for f in allowedfileformats]\n            elif pipeline == 'confound':\n                fileending = ['confounds' + f for f in allowedfileformats]\n\n            if os.path.exists(wdir):\n                # make filenames\n                found = []\n                # Check that the tags are in the specified bids tags\n                for ff in os.listdir(wdir):\n                    ftags = get_bids_tag(ff, 'all')\n                    t = [t for t in ftags if t in file_dict and ftags[t]\n                         in file_dict[t]]\n                    if len(t) == len(file_dict):\n                        found.append(ff)\n                found = [f for f in found for e in fileending if f.endswith(e)]\n                # Include only if all analysis step tags are present\n                # Exclude if confounds tag is present\n                if pipeline == 'confound':\n                    found = [i for i in found if '_confounds' in i]\n                else:\n                    found = [i for i in found if '_confounds' not in i]\n                # Make full paths\n                found = list(\n                    map(str.__add__, [re.sub('/+', '/', wdir)]*len(found), found))\n                # Remove any files in bad files (could add json subcar reading here)\n                found = [i for i in found if not any(\n                    [bf in i for bf in self.bad_files])]\n                if found:\n                    found_files += found\n\n            if quiet == -1:\n                print(wdir)\n\n        found_files = list(set(found_files))\n        if quiet == 0:\n            print(found_files)\n        return found_files", "response": "Returns a list of all files currently selected with the current analysis pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_exclusion_file(self, confound, exclusion_criteria, confound_stat='mean'):\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n        if isinstance(confound, str):\n            confound = [confound]\n        if isinstance(exclusion_criteria, str):\n            exclusion_criteria = [exclusion_criteria]\n        if isinstance(confound_stat, str):\n            confound_stat = [confound_stat]\n        if len(exclusion_criteria) != len(confound):\n            raise ValueError(\n                'Same number of confound names and exclusion criteria must be given')\n        if len(confound_stat) != len(confound):\n            raise ValueError(\n                'Same number of confound names and confound stats must be given')\n        relex, crit = process_exclusion_criteria(exclusion_criteria)\n        files = sorted(self.get_selected_files(quiet=1))\n        confound_files = sorted(\n            self.get_selected_files(quiet=1, pipeline='confound'))\n        files, confound_files = confound_matching(files, confound_files)\n        bad_files = []\n        bs = 0\n        foundconfound = []\n        foundreason = []\n        for s, cfile in enumerate(confound_files):\n            df = load_tabular_file(cfile, index_col=None)\n            found_bad_subject = False\n            for i, _ in enumerate(confound):\n                if confound_stat[i] == 'median':\n                    if relex[i](df[confound[i]].median(), crit[i]):\n                        found_bad_subject = True\n                elif confound_stat[i] == 'mean':\n                    if relex[i](df[confound[i]].mean(), crit[i]):\n                        found_bad_subject = True\n                elif confound_stat[i] == 'std':\n                    if relex[i](df[i][confound[i]].std(), crit[i]):\n                        found_bad_subject = True\n                if found_bad_subject:\n                    foundconfound.append(confound[i])\n                    foundreason.append(exclusion_criteria[i])\n            if found_bad_subject:\n                bad_files.append(files[s])\n                bs += 1\n        self.set_bad_files(\n            bad_files, reason='excluded file (confound over specfied stat threshold)')\n        for i, f in enumerate(bad_files):\n            sidecar = get_sidecar(f)\n            sidecar['file_exclusion'] = {}\n            sidecar['confound'] = foundconfound[i]\n            sidecar['threshold'] = foundreason[i]\n            for af in ['.tsv', '.nii.gz']:\n                f = f.split(af)[0]\n            f += '.json'\n            with open(f, 'w') as fs:\n                json.dump(sidecar, fs)\n        print('Removed ' + str(bs) + ' files from inclusion.')", "response": "Set the exclusion file for a given confound."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the exclusion criteria for a specific timepoint in the teneto bIDS directory.", "response": "def set_exclusion_timepoint(self, confound, exclusion_criteria, replace_with, tol=1, overwrite=True, desc=None):\n        \"\"\"\n        Excludes subjects given a certain exclusion criteria. Does not work on nifti files, only csv, numpy or tsc. Assumes data is node,time\n\n        Parameters\n        ----------\n            confound : str or list\n                string or list of confound name(s) from confound files. Assumes data is node,time\n            exclusion_criteria  : str or list\n                for each confound, an exclusion_criteria should be expressed as a string. It starts with >,<,>= or <= then the numerical threshold. Ex. '>0.2' will entail every subject with the avg greater than 0.2 of confound will be rejected.\n            replace_with : str\n                Can be 'nan' (bad values become nans) or 'cubicspline' (bad values are interpolated). If bad value occurs at 0 or -1 index, then these values are kept and no interpolation occurs.\n            tol : float\n                Tolerance of exlcuded time-points allowed before becoming a bad subject.\n            overwrite : bool (default=True)\n                If true, if their are files in the teneto derivatives directory with the same name, these will be overwritten with this step.\n                The json sidecar is updated with the new information about the file.\n            desc : str\n                String to add desc tag to filenames if overwrite is set to true.\n\n        Returns\n        ------\n            Loads the TenetoBIDS.selected_files and replaces any instances of confound meeting the exclusion_criteria with replace_with.\n        \"\"\"\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n        if isinstance(confound, str):\n            confound = [confound]\n        if isinstance(exclusion_criteria, str):\n            exclusion_criteria = [exclusion_criteria]\n        if len(exclusion_criteria) != len(confound):\n            raise ValueError(\n                'Same number of confound names and exclusion criteria must be given')\n        relex, crit = process_exclusion_criteria(exclusion_criteria)\n        files = sorted(self.get_selected_files(quiet=1))\n        confound_files = sorted(\n            self.get_selected_files(quiet=1, pipeline='confound'))\n        files, confound_files = confound_matching(files, confound_files)\n        bad_files = []\n        for i, cfile in enumerate(confound_files):\n            data = load_tabular_file(files[i]).values\n            df = load_tabular_file(cfile, index_col=None)\n            ind = []\n            # Can't interpolate values if nanind is at the beginning or end. So keep these as their original values.\n            for ci, c in enumerate(confound):\n                ind = df[relex[ci](df[c], crit[ci])].index\n                if replace_with == 'cubicspline':\n                    if 0 in ind:\n                        ind = np.delete(ind, np.where(ind == 0))\n                    if df.index.max():\n                        ind = np.delete(ind, np.where(ind == df.index.max()))\n                data[:, ind.astype(int)] = np.nan\n            nanind = np.where(np.isnan(data[0, :]))[0]\n            badpoints_n = len(nanind)\n            # Bad file if the number of ratio bad points are greater than the tolerance.\n            if badpoints_n / np.array(len(df)) > tol:\n                bad_files.append(files[i])\n            nonnanind = np.where(np.isnan(data[0, :]) == 0)[0]\n            nanind = nanind[nanind > nonnanind.min()]\n            nanind = nanind[nanind < nonnanind.max()]\n            if replace_with == 'cubicspline':\n                for n in range(data.shape[0]):\n                    interp = interp1d(\n                        nonnanind, data[n, nonnanind], kind='cubic')\n                    data[n, nanind] = interp(nanind)\n            # only save if the subject is not excluded\n            data = pd.DataFrame(data)\n            sname, _ = drop_bids_suffix(files[i])\n            # Move files to teneto derivatives if the pipeline isn't already set to it\n            if self.pipeline != 'teneto_' + teneto.__version__:\n                sname = sname.split('/')[-1]\n                spath = self.BIDS_dir + '/derivatives/' + 'teneto_' + teneto.__version__ + '/'\n                tags = get_bids_tag(sname, ['sub', 'ses'])\n                spath += 'sub-' + tags['sub'] + '/'\n                if 'ses' in tags:\n                    spath += 'ses-' + tags['ses'] + '/'\n                spath += 'func/'\n                if self.pipeline_subdir:\n                    spath += self.pipeline_subdir + '/'\n                make_directories(spath)\n                sname = spath + sname\n            if 'desc' in sname and desc:\n                desctag = get_bids_tag(sname.split('/')[-1], 'desc')\n                sname = ''.join(sname.split('desc-' + desctag['desc']))\n                sname += '_desc-' + desc\n            if os.path.exists(sname + self.bids_suffix + '.tsv') and overwrite == False:\n                raise ValueError(\n                    'overwrite is set to False, but non-unique filename. Set unique desc tag')\n            data.to_csv(sname + '_' + self.bids_suffix + '.tsv', sep='\\t')\n            # Update json sidecar\n            sidecar = get_sidecar(files[i])\n            sidecar['scrubbed_timepoints'] = {}\n            sidecar['scrubbed_timepoints']['description'] = 'Scrubbing which censors timepoints where the confounds where above a certain time-points.\\\n                Censored time-points are replaced with replacement value (nans or cubic spline). \\\n                Output of teneto.TenetoBIDS.set_exclusion_timepoint.'\n            sidecar['scrubbed_timepoints']['confound'] = ','.join(confound)\n            sidecar['scrubbed_timepoints']['threshold'] = ','.join(\n                exclusion_criteria)\n            sidecar['scrubbed_timepoints']['replacement'] = replace_with\n            sidecar['scrubbed_timepoints']['badpoint_number'] = badpoints_n\n            sidecar['scrubbed_timepoints']['badpoint_ratio'] = badpoints_n / \\\n                np.array(len(df))\n            sidecar['scrubbed_timepoints']['file_exclusion_when_badpoint_ratio'] = tol\n            with open(sname + '_' + self.bids_suffix + '.json', 'w') as fs:\n                json.dump(sidecar, fs)\n        self.set_bad_files(\n            bad_files, reason='scrubbing (number of points over threshold)')\n        self.set_pipeline('teneto_' + teneto.__version__)\n        if desc:\n            self.set_bids_tags({'desc': desc.split('-')[1]})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a parcellation space for a specific voxel.", "response": "def make_parcellation(self, parcellation, parc_type=None, parc_params=None, network='defaults', update_pipeline=True, removeconfounds=False, tag=None, njobs=None, clean_params=None, yeonetworkn=None):\n        \"\"\"\n        Reduces the data from voxel to parcellation space. Files get saved in a teneto folder in the derivatives with a roi tag at the end.\n\n        Parameters\n        -----------\n\n        parcellation : str\n            specify which parcellation that you would like to use. For MNI: 'power2012_264', 'gordon2014_333'. TAL: 'shen2013_278'\n        parc_type : str\n            can be 'sphere' or 'region'. If nothing is specified, the default for that parcellation will be used.\n        parc_params : dict\n            **kwargs for nilearn functions\n        network : str\n            if \"defaults\", it selects static parcellation, _if available_ (other options will be made available soon).\n        removeconfounds : bool\n            if true, regresses out confounds that are specfied in self.set_confounds with linear regression.\n        update_pipeline : bool\n            TenetoBIDS gets updated with the parcellated files being selected.\n        tag : str or list\n            any additional tag that must be in file name. After the tag there must either be a underscore or period (following bids).\n        clean_params : dict\n            **kwargs for nilearn function nilearn.signal.clean\n        yeonetworkn : int (7 or 17)\n            Only relevant for when parcellation is schaeffer2018. Use 7 or 17 template networks\n        njobs : n\n            number of processes to run. Overrides TenetoBIDS.njobs\n\n        Returns\n        -------\n        Files are saved in ./BIDS_dir/derivatives/teneto_<version>/.../parcellation/.\n        To load these files call TenetoBIDS.load_parcellation.\n\n        NOTE\n        ----\n        These functions make use of nilearn. Please cite nilearn if used in a publicaiton.\n        \"\"\"\n        if not njobs:\n            njobs = self.njobs\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n\n        parc_name = parcellation.split('_')[0].lower()\n\n        # Check confounds have been specified\n        if not self.confounds and removeconfounds:\n            raise ValueError(\n                'Specified confounds are not found. Make sure that you have run self.set_confunds([\\'Confound1\\',\\'Confound2\\']) first.')\n\n        # Check confounds have been specified\n        if update_pipeline == False and removeconfounds:\n            raise ValueError(\n                'Pipeline must be updated in order to remove confounds within this funciton.')\n\n        # In theory these should be the same. So at the moment, it goes through each element and checks they are matched.\n        # A matching algorithem may be needed if cases arise where this isnt the case\n        files = self.get_selected_files(quiet=1)\n        # Load network communities, if possible.\n        self.set_network_communities(parcellation, netn=yeonetworkn)\n\n        if not tag:\n            tag = ''\n        else:\n            tag = 'desc-' + tag\n\n        if not parc_params:\n            parc_params = {}\n\n        with ProcessPoolExecutor(max_workers=njobs) as executor:\n            job = {executor.submit(self._run_make_parcellation, f, i, tag, parcellation,\n                                   parc_name, parc_type, parc_params) for i, f in enumerate(files)}\n            for j in as_completed(job):\n                j.result()\n\n        if update_pipeline == True:\n            if not self.confound_pipeline and len(self.get_selected_files(quiet=1, pipeline='confound')) > 0:\n                self.set_confound_pipeline(self.pipeline)\n            self.set_pipeline('teneto_' + teneto.__version__)\n            self.set_pipeline_subdir('parcellation')\n            if tag:\n                self.set_bids_tags({'desc': tag.split('-')[1]})\n            self.set_bids_suffix('roi')\n            if removeconfounds:\n                self.removeconfounds(\n                    clean_params=clean_params, transpose=None, njobs=njobs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall temporal_louvain_with_consensus on connectivity data.", "response": "def communitydetection(self, community_detection_params, community_type='temporal', tag=None, file_hdr=False, file_idx=False, njobs=None):\n        \"\"\"\n        Calls temporal_louvain_with_consensus on connectivity data\n\n        Parameters\n        ----------\n\n        community_detection_params : dict\n            kwargs for detection. See teneto.communitydetection.louvain.temporal_louvain_with_consensus\n        community_type : str\n            Either 'temporal' or 'static'. If temporal, community is made per time-point for each timepoint.\n        file_idx : bool (default false)\n            if true, index column present in data and this will be ignored\n        file_hdr : bool (default false)\n            if true, header row present in data and this will be ignored\n        njobs : int\n            number of processes to run. Overrides TenetoBIDS.njobs\n\n        Note\n        ----\n        All non-positive edges are made to zero.\n\n\n        Returns\n        -------\n        List of communities for each subject. Saved in BIDS_dir/derivatives/teneto/communitydetection/\n        \"\"\"\n        if not njobs:\n            njobs = self.njobs\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n\n        if not tag:\n            tag = ''\n        else:\n            tag = 'desc-' + tag\n\n        if community_type == 'temporal':\n            files = self.get_selected_files(quiet=True)\n            # Run check to make sure files are tvc input\n            for f in files:\n                if 'tvc' not in f:\n                    raise ValueError(\n                        'tvc tag not found in filename. TVC data must be used in communitydetection (perhaps run TenetoBIDS.derive first?).')\n        elif community_type == 'static':\n            files = self.get_selected_files(\n                quiet=True, pipeline='functionalconnectivity')\n\n        with ProcessPoolExecutor(max_workers=njobs) as executor:\n            job = {executor.submit(self._run_communitydetection, f, community_detection_params, community_type, file_hdr,\n                                   file_idx, tag) for i, f in enumerate(files) if all([t + '_' in f or t + '.' in f for t in tag])}\n            for j in as_completed(job):\n                j.result()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving specified confounds using nilearn.signal.clean Parameters ---------- confounds : list List of confounds. Can be prespecified in set_confounds clean_params : dict Dictionary of kawgs to pass to nilearn.signal.clean transpose : bool (default False) Default removeconfounds works on time,node dimensions. Pass transpose=True to transpose pre and post confound removal. njobs : int Number of jobs. Otherwise tenetoBIDS.njobs is run. update_pipeline : bool update pipeline with '_clean' tag for new files created overwrite : bool tag : str Returns ------- Says all TenetBIDS.get_selected_files with confounds removed with _rmconfounds at the end. Note ---- There may be some issues regarding loading non-cleaned data through the TenetoBIDS functions instead of the cleaned data. This depeneds on when you clean the data.", "response": "def removeconfounds(self, confounds=None, clean_params=None, transpose=None, njobs=None, update_pipeline=True, overwrite=True, tag=None):\n        \"\"\"\n        Removes specified confounds using nilearn.signal.clean\n\n        Parameters\n        ----------\n        confounds : list\n            List of confounds. Can be prespecified in set_confounds\n        clean_params : dict\n            Dictionary of kawgs to pass to nilearn.signal.clean\n        transpose : bool (default False)\n            Default removeconfounds works on time,node dimensions. Pass transpose=True to transpose pre and post confound removal.\n        njobs : int\n            Number of jobs. Otherwise tenetoBIDS.njobs is run.\n        update_pipeline : bool\n            update pipeline with '_clean' tag for new files created\n        overwrite : bool\n        tag : str\n\n        Returns\n        -------\n        Says all TenetBIDS.get_selected_files with confounds removed with _rmconfounds at the end.\n\n        Note\n        ----\n        There may be some issues regarding loading non-cleaned data through the TenetoBIDS functions instead of the cleaned data. This depeneds on when you clean the data.\n        \"\"\"\n        if not njobs:\n            njobs = self.njobs\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n\n        if not self.confounds and not confounds:\n            raise ValueError(\n                'Specified confounds are not found. Make sure that you have run self.set_confunds([\\'Confound1\\',\\'Confound2\\']) first or pass confounds as input to function.')\n\n        if not tag:\n            tag = ''\n        else:\n            tag = 'desc-' + tag\n\n        if confounds:\n            self.set_confounds(confounds)\n        files = sorted(self.get_selected_files(quiet=1))\n        confound_files = sorted(\n            self.get_selected_files(quiet=1, pipeline='confound'))\n        files, confound_files = confound_matching(files, confound_files)\n        if not clean_params:\n            clean_params = {}\n\n        with ProcessPoolExecutor(max_workers=njobs) as executor:\n            job = {executor.submit(\n                self._run_removeconfounds, f, confound_files[i], clean_params, transpose, overwrite, tag) for i, f in enumerate(files)}\n            for j in as_completed(job):\n                j.result()\n\n        self.set_pipeline('teneto_' + teneto.__version__)\n        self.set_bids_suffix('roi')\n        if tag:\n            self.set_bids_tags({'desc': tag.split('-')[1]})"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates a network measure for the specified function.", "response": "def networkmeasures(self, measure=None, measure_params=None, tag=None, njobs=None):\n        \"\"\"\n        Calculates a network measure\n\n        For available funcitons see: teneto.networkmeasures\n\n        Parameters\n        ----------\n\n        measure : str or list\n            Mame of function(s) from teneto.networkmeasures that will be run.\n\n        measure_params : dict or list of dctionaries)\n            Containing kwargs for the argument in measure.\n            See note regarding Communities key.\n\n        tag : str\n            Add additional tag to saved filenames.\n\n        Note\n        ----\n        In measure_params, if communities can equal 'template', 'static', or 'temporal'.\n        These options must be precalculated. If template, Teneto tries to load default for parcellation. If static, loads static communities\n        in BIDS_dir/teneto_<version>/sub-.../func/communities/..._communitytype-static....npy. If temporal, loads static communities\n        in BIDS_dir/teneto_<version>/sub-.../func/communities/..._communitytype-temporal....npy\n        Returns\n        -------\n\n        Saves in ./BIDS_dir/derivatives/teneto/sub-NAME/func//temporalnetwork/MEASURE/\n        Load the measure with tenetoBIDS.load_network_measure\n        \"\"\"\n        if not njobs:\n            njobs = self.njobs\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n\n        # measure can be string or list\n        if isinstance(measure, str):\n            measure = [measure]\n        # measure_params can be dictionaary or list of dictionaries\n        if isinstance(measure_params, dict):\n            measure_params = [measure_params]\n        if measure_params and len(measure) != len(measure_params):\n            raise ValueError('Number of identified measure_params (' + str(len(measure_params)) +\n                             ') differs from number of identified measures (' + str(len(measure)) + '). Leave black dictionary if default methods are wanted')\n\n        files = self.get_selected_files(quiet=1)\n\n        if not tag:\n            tag = ''\n        else:\n            tag = 'desc-' + tag\n\n        with ProcessPoolExecutor(max_workers=njobs) as executor:\n            job = {executor.submit(\n                self._run_networkmeasures, f, tag, measure, measure_params) for f in files}\n            for j in as_completed(job):\n                j.result()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the bids suffix for the log files.", "response": "def set_bids_suffix(self, bids_suffix):\n        \"\"\"\n        The last analysis step is the final tag that is present in files.\n        \"\"\"\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n        self.bids_suffix = bids_suffix"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_pipeline(self, pipeline):\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n        if not os.path.exists(self.BIDS_dir + '/derivatives/' + pipeline):\n            print('Specified direvative directory not found.')\n            self.get_pipeline_alternatives()\n        else:\n            # Todo: perform check that pipeline is valid\n            self.pipeline = pipeline", "response": "Set the pipeline for this instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the summary of the BIDS data and the files currently selected.", "response": "def print_dataset_summary(self):\n        \"\"\"\n        Prints information about the the BIDS data and the files currently selected.\n        \"\"\"\n\n        print('--- DATASET INFORMATION ---')\n\n        print('--- Subjects ---')\n        if self.raw_data_exists:\n            if self.BIDS.get_subjects():\n                print('Number of subjects (in dataset): ' +\n                      str(len(self.BIDS.get_subjects())))\n                print('Subjects (in dataset): ' +\n                      ', '.join(self.BIDS.get_subjects()))\n            else:\n                print('NO SUBJECTS FOUND (is the BIDS directory specified correctly?)')\n\n        print('Number of subjects (selected): ' +\n              str(len(self.bids_tags['sub'])))\n        print('Subjects (selected): ' + ', '.join(self.bids_tags['sub']))\n        if isinstance(self.bad_subjects, list):\n            print('Bad subjects: ' + ', '.join(self.bad_subjects))\n        else:\n            print('Bad subjects: 0')\n\n        print('--- Tasks ---')\n        if self.raw_data_exists:\n            if self.BIDS.get_tasks():\n                print('Number of tasks (in dataset): ' +\n                      str(len(self.BIDS.get_tasks())))\n                print('Tasks (in dataset): ' + ', '.join(self.BIDS.get_tasks()))\n        if 'task' in self.bids_tags:\n            print('Number of tasks (selected): ' +\n                  str(len(self.bids_tags['task'])))\n            print('Tasks (selected): ' + ', '.join(self.bids_tags['task']))\n        else:\n            print('No task names found')\n\n        print('--- Runs ---')\n        if self.raw_data_exists:\n            if self.BIDS.get_runs():\n                print('Number of runs (in dataset): ' +\n                      str(len(self.BIDS.get_runs())))\n                print('Runs (in dataset): ' + ', '.join(self.BIDS.get_runs()))\n        if 'run' in self.bids_tags:\n            print('Number of runs (selected): ' +\n                  str(len(self.bids_tags['run'])))\n            print('Rubs (selected): ' + ', '.join(self.bids_tags['run']))\n        else:\n            print('No run names found')\n\n        print('--- Sessions ---')\n        if self.raw_data_exists:\n            if self.BIDS.get_sessions():\n                print('Number of runs (in dataset): ' +\n                      str(len(self.BIDS.get_sessions())))\n                print('Sessions (in dataset): ' +\n                      ', '.join(self.BIDS.get_sessions()))\n        if 'ses' in self.bids_tags:\n            print('Number of sessions (selected): ' +\n                  str(len(self.bids_tags['ses'])))\n            print('Sessions (selected): ' + ', '.join(self.bids_tags['ses']))\n        else:\n            print('No session names found')\n\n        print('--- PREPROCESSED DATA (Pipelines/Derivatives) ---')\n\n        if not self.pipeline:\n            print('Derivative pipeline not set. To set, run TN.set_pipeline()')\n        else:\n            print('Pipeline: ' + self.pipeline)\n        if self.pipeline_subdir:\n            print('Pipeline subdirectories: ' + self.pipeline_subdir)\n\n        selected_files = self.get_selected_files(quiet=1)\n        if selected_files:\n            print('--- SELECTED DATA ---')\n            print('Numnber of selected files: ' + str(len(selected_files)))\n            print('\\n - '.join(selected_files))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_frompickle(cls, fname, reload_object=False):\n        if fname[-4:] != '.pkl':\n            fname += '.pkl'\n        with open(fname, 'rb') as f:\n            tnet = pickle.load(f)\n        if reload_object:\n            reloadnet = teneto.TenetoBIDS(tnet.BIDS_dir, pipeline=tnet.pipeline, pipeline_subdir=tnet.pipeline_subdir, bids_tags=tnet.bids_tags,  bids_suffix=tnet.bids_suffix,\n                                          bad_subjects=tnet.bad_subjects, confound_pipeline=tnet.confound_pipeline, raw_data_exists=tnet.raw_data_exists, njobs=tnet.njobs)\n            reloadnet.histroy = tnet.history\n            tnet = reloadnet\n        return tnet", "response": "Loads a TenetoBIDS object from a pickle file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads the data from the TenetoBIDS. derive function.", "response": "def load_data(self, datatype='tvc', tag=None, measure=''):\n        \"\"\"\n        Function loads time-varying connectivity estimates created by the TenetoBIDS.derive function.\n        The default grabs all data (in numpy arrays) in the teneto/../func/tvc/ directory.\n        Data is placed in teneto.tvc_data_\n\n        Parameters\n        ----------\n\n        datatype : str\n            \\'tvc\\', \\'parcellation\\', \\'participant\\', \\'temporalnetwork\\'\n\n        tag : str or list\n            any additional tag that must be in file name. After the tag there must either be a underscore or period (following bids).\n\n        measure : str\n            retquired when datatype is temporalnetwork. A networkmeasure that should be loaded.\n\n        Returns\n        -------\n\n        tvc_data_ : numpy array\n            Containing the parcellation data. Each file is appended to the first dimension of the numpy array.\n        tvc_trialinfo_ : pandas data frame\n            Containing the subject info (all BIDS tags) in the numpy array.\n        \"\"\"\n\n        if datatype == 'temporalnetwork' and not measure:\n            raise ValueError(\n                'When datatype is temporalnetwork, \\'measure\\' must also be specified.')\n\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n        data_list = []\n        trialinfo_list = []\n\n        for s in self.bids_tags['sub']:\n            # Define base folder\n            base_path, file_list, datainfo = self._get_filelist(\n                datatype, s, tag, measure=measure)\n            if base_path:\n                for f in file_list:\n                    # Include only if all analysis step tags are present\n                    # Get all BIDS tags. i.e. in 'sub-AAA', get 'sub' as key and 'AAA' as item.\n                    # Ignore if tsv file is empty\n                    try:\n                        filetags = get_bids_tag(f, 'all')\n                        data_list.append(load_tabular_file(base_path + f))\n                        # Only return trialinfo if datatype is trlinfo\n                        if datainfo == 'trlinfo':\n                            trialinfo_list.append(\n                                pd.DataFrame(filetags, index=[0]))\n                    except pd.errors.EmptyDataError:\n                        pass\n        # If group data and length of output is one, don't make it a list\n        if datatype == 'group' and len(data_list) == 1:\n            data_list = data_list[0]\n        if measure:\n            data_list = {measure: data_list}\n        setattr(self, datatype + '_data_', data_list)\n        if trialinfo_list:\n            out_trialinfo = pd.concat(trialinfo_list)\n            out_trialinfo.reset_index(inplace=True, drop=True)\n            setattr(self, datatype + '_trialinfo_', out_trialinfo)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn temporal closeness centrality per node.", "response": "def temporal_closeness_centrality(tnet=None, paths=None):\n    '''\n    Returns temporal closeness centrality per node.\n\n    Parameters\n    -----------\n\n    Input should be *either* tnet or paths.\n\n    data : array or dict\n\n        Temporal network input (graphlet or contact). nettype: 'bu', 'bd'.\n\n    paths : pandas dataframe\n\n        Output of TenetoBIDS.networkmeasure.shortest_temporal_paths\n\n\n    Returns\n    --------\n\n    :close: array\n\n        temporal closness centrality (nodal measure)\n\n    '''\n\n    if tnet is not None and paths is not None:\n        raise ValueError('Only network or path input allowed.')\n    if tnet is None and paths is None:\n        raise ValueError('No input.')\n    # if shortest paths are not calculated, calculate them\n    if tnet is not None:\n        paths = shortest_temporal_path(tnet)\n\n    pathmat = np.zeros([paths[['from', 'to']].max().max(\n    )+1, paths[['from', 'to']].max().max()+1, paths[['t_start']].max().max()+1]) * np.nan\n    pathmat[paths['from'].values, paths['to'].values,\n            paths['t_start'].values] = paths['temporal-distance']\n\n    closeness = np.nansum(1 / np.nanmean(pathmat, axis=2),\n                          axis=1) / (pathmat.shape[1] - 1)\n\n    return closeness"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sid(tnet, communities, axis=0, calc='global', decay=0):\n\n    tnet, netinfo = utils.process_input(tnet, ['C', 'G', 'TN'])\n    D = temporal_degree_centrality(\n        tnet, calc='time', communities=communities, decay=decay)\n\n    # Check network output (order of communitiesworks)\n    network_ids = np.unique(communities)\n    communities_size = np.array([sum(communities == n) for n in network_ids])\n\n    sid = np.zeros([network_ids.max()+1, network_ids.max()+1, tnet.shape[-1]])\n    for n in network_ids:\n        for m in network_ids:\n            betweenmodulescaling = 1/(communities_size[n]*communities_size[m])\n            if netinfo['nettype'][1] == 'd':\n                withinmodulescaling = 1 / \\\n                    (communities_size[n]*communities_size[n])\n            elif netinfo['nettype'][1] == 'u':\n                withinmodulescaling = 2 / \\\n                    (communities_size[n]*(communities_size[n]-1))\n                if n == m:\n                    betweenmodulescaling = withinmodulescaling\n            sid[n, m, :] = withinmodulescaling * \\\n                D[n, n, :] - betweenmodulescaling * D[n, m, :]\n    # If nans emerge than there is no connection between networks at time point, so make these 0.\n    sid[np.isnan(sid)] = 0\n\n    if calc == 'global':\n        return np.sum(np.sum(sid, axis=1), axis=0)\n    elif calc == 'communities_avg':\n        return np.sum(sid, axis=axis)\n    else:\n        return sid", "response": "r Returns a sequence of arrays representing the segergation integration difference of each community assignment and each global assignment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flatten(d, reducer='tuple', inverse=False):\n    if isinstance(reducer, str):\n        reducer = REDUCER_DICT[reducer]\n    flat_dict = {}\n\n    def _flatten(d, parent=None):\n        for key, value in six.viewitems(d):\n            flat_key = reducer(parent, key)\n            if isinstance(value, Mapping):\n                _flatten(value, flat_key)\n            else:\n                if inverse:\n                    flat_key, value = value, flat_key\n                if flat_key in flat_dict:\n                    raise ValueError(\"duplicated key '{}'\".format(flat_key))\n                flat_dict[flat_key] = value\n\n    _flatten(d)\n    return flat_dict", "response": "Flatten a dict - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets a value to a sequence of nested keys", "response": "def nested_set_dict(d, keys, value):\n    \"\"\"Set a value to a sequence of nested keys\n\n    Parameters\n    ----------\n    d: Mapping\n    keys: Sequence[str]\n    value: Any\n    \"\"\"\n    assert keys\n    key = keys[0]\n    if len(keys) == 1:\n        if key in d:\n            raise ValueError(\"duplicated key '{}'\".format(key))\n        d[key] = value\n        return\n    d = d.setdefault(key, {})\n    nested_set_dict(d, keys[1:], value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unflatten(d, splitter='tuple', inverse=False):\n    if isinstance(splitter, str):\n        splitter = SPLITTER_DICT[splitter]\n\n    unflattened_dict = {}\n    for flat_key, value in six.viewitems(d):\n        if inverse:\n            flat_key, value = value, flat_key\n        key_tuple = splitter(flat_key)\n        nested_set_dict(unflattened_dict, key_tuple, value)\n\n    return unflattened_dict", "response": "Unflatten a dict - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_pianoroll(ax, pianoroll, is_drum=False, beat_resolution=None,\n                   downbeats=None, preset='default', cmap='Blues', xtick='auto',\n                   ytick='octave', xticklabel=True, yticklabel='auto',\n                   tick_loc=None, tick_direction='in', label='both',\n                   grid='both', grid_linestyle=':', grid_linewidth=.5):\n    \"\"\"\n    Plot a pianoroll given as a numpy array.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes object\n        A :class:`matplotlib.axes.Axes` object where the pianoroll will be\n        plotted on.\n    pianoroll : np.ndarray\n        A pianoroll to be plotted. The values should be in [0, 1] when data\n        type is float, and in [0, 127] when data type is integer.\n\n        - For a 2D array, shape=(num_time_step, num_pitch).\n        - For a 3D array, shape=(num_time_step, num_pitch, num_channel),\n          where channels can be either RGB or RGBA.\n\n    is_drum : bool\n        A boolean number that indicates whether it is a percussion track.\n            Defaults to False.\n    beat_resolution : int\n        The number of time steps used to represent a beat. Required and only\n        effective when `xtick` is 'beat'.\n    downbeats : list\n        An array that indicates whether the time step contains a downbeat\n        (i.e., the first time step of a bar).\n    preset : {'default', 'plain', 'frame'}\n        A string that indicates the preset theme to use.\n\n        - In 'default' preset, the ticks, grid and labels are on.\n        - In 'frame' preset, the ticks and grid are both off.\n        - In 'plain' preset, the x- and y-axis are both off.\n\n    cmap :  `matplotlib.colors.Colormap`\n        The colormap to use in :func:`matplotlib.pyplot.imshow`. Defaults to\n        'Blues'. Only effective when `pianoroll` is 2D.\n    xtick : {'auto', 'beat', 'step', 'off'}\n        A string that indicates what to use as ticks along the x-axis. If\n        'auto' is given, automatically set to 'beat' if `beat_resolution` is\n        also given and set to 'step', otherwise. Defaults to 'auto'.\n    ytick : {'octave', 'pitch', 'off'}\n        A string that indicates what to use as ticks along the y-axis.\n        Defaults to 'octave'.\n    xticklabel : bool\n        Whether to add tick labels along the x-axis. Only effective when\n        `xtick` is not 'off'.\n    yticklabel : {'auto', 'name', 'number', 'off'}\n        If 'name', use octave name and pitch name (key name when `is_drum`\n        is True) as tick labels along the y-axis. If 'number', use pitch\n        number. If 'auto', set to 'name' when `ytick` is 'octave' and\n        'number' when `ytick` is 'pitch'. Defaults to 'auto'. Only effective\n        when `ytick` is not 'off'.\n    tick_loc : tuple or list\n        The locations to put the ticks. Availables elements are 'bottom',\n        'top', 'left' and 'right'. Defaults to ('bottom', 'left').\n    tick_direction : {'in', 'out', 'inout'}\n        A string that indicates where to put the ticks. Defaults to 'in'.\n        Only effective when one of `xtick` and `ytick` is on.\n    label : {'x', 'y', 'both', 'off'}\n        A string that indicates whether to add labels to the x-axis and\n        y-axis. Defaults to 'both'.\n    grid : {'x', 'y', 'both', 'off'}\n        A string that indicates whether to add grids to the x-axis, y-axis,\n        both or neither. Defaults to 'both'.\n    grid_linestyle : str\n        Will be passed to :meth:`matplotlib.axes.Axes.grid` as 'linestyle'\n        argument.\n    grid_linewidth : float\n        Will be passed to :meth:`matplotlib.axes.Axes.grid` as 'linewidth'\n        argument.\n\n    \"\"\"\n    if not HAS_MATPLOTLIB:\n        raise ImportError(\"matplotlib package is required for plotting \"\n                          \"supports.\")\n\n    if pianoroll.ndim not in (2, 3):\n        raise ValueError(\"`pianoroll` must be a 2D or 3D numpy array\")\n    if pianoroll.shape[1] != 128:\n        raise ValueError(\"The length of the second axis of `pianoroll` \"\n                         \"must be 128.\")\n    if xtick not in ('auto', 'beat', 'step', 'off'):\n        raise ValueError(\"`xtick` must be one of {'auto', 'beat', 'step', \"\n                         \"'none'}.\")\n    if xtick == 'beat' and beat_resolution is None:\n        raise ValueError(\"`beat_resolution` must be specified when `xtick` \"\n                         \"is 'beat'.\")\n    if ytick not in ('octave', 'pitch', 'off'):\n        raise ValueError(\"`ytick` must be one of {octave', 'pitch', 'off'}.\")\n    if not isinstance(xticklabel, bool):\n        raise TypeError(\"`xticklabel` must be bool.\")\n    if yticklabel not in ('auto', 'name', 'number', 'off'):\n        raise ValueError(\"`yticklabel` must be one of {'auto', 'name', \"\n                         \"'number', 'off'}.\")\n    if tick_direction not in ('in', 'out', 'inout'):\n        raise ValueError(\"`tick_direction` must be one of {'in', 'out',\"\n                         \"'inout'}.\")\n    if label not in ('x', 'y', 'both', 'off'):\n        raise ValueError(\"`label` must be one of {'x', 'y', 'both', 'off'}.\")\n    if grid not in ('x', 'y', 'both', 'off'):\n        raise ValueError(\"`grid` must be one of {'x', 'y', 'both', 'off'}.\")\n\n    # plotting\n    if pianoroll.ndim > 2:\n        to_plot = pianoroll.transpose(1, 0, 2)\n    else:\n        to_plot = pianoroll.T\n    if (np.issubdtype(pianoroll.dtype, np.bool_)\n            or np.issubdtype(pianoroll.dtype, np.floating)):\n        ax.imshow(to_plot, cmap=cmap, aspect='auto', vmin=0, vmax=1,\n                  origin='lower', interpolation='none')\n    elif np.issubdtype(pianoroll.dtype, np.integer):\n        ax.imshow(to_plot, cmap=cmap, aspect='auto', vmin=0, vmax=127,\n                  origin='lower', interpolation='none')\n    else:\n        raise TypeError(\"Unsupported data type for `pianoroll`.\")\n\n    # tick setting\n    if tick_loc is None:\n        tick_loc = ('bottom', 'left')\n    if xtick == 'auto':\n        xtick = 'beat' if beat_resolution is not None else 'step'\n    if yticklabel == 'auto':\n        yticklabel = 'name' if ytick == 'octave' else 'number'\n\n    if preset == 'plain':\n        ax.axis('off')\n    elif preset == 'frame':\n        ax.tick_params(direction=tick_direction, bottom=False, top=False,\n                       left=False, right=False, labelbottom=False,\n                       labeltop=False, labelleft=False, labelright=False)\n    else:\n        ax.tick_params(direction=tick_direction, bottom=('bottom' in tick_loc),\n                       top=('top' in tick_loc), left=('left' in tick_loc),\n                       right=('right' in tick_loc),\n                       labelbottom=(xticklabel != 'off'),\n                       labelleft=(yticklabel != 'off'),\n                       labeltop=False, labelright=False)\n\n    # x-axis\n    if xtick == 'beat' and preset != 'frame':\n        num_beat = pianoroll.shape[0]//beat_resolution\n        xticks_major = beat_resolution * np.arange(0, num_beat)\n        xticks_minor = beat_resolution * (0.5 + np.arange(0, num_beat))\n        xtick_labels = np.arange(1, 1 + num_beat)\n        ax.set_xticks(xticks_major)\n        ax.set_xticklabels('')\n        ax.set_xticks(xticks_minor, minor=True)\n        ax.set_xticklabels(xtick_labels, minor=True)\n        ax.tick_params(axis='x', which='minor', width=0)\n\n    # y-axis\n    if ytick == 'octave':\n        ax.set_yticks(np.arange(0, 128, 12))\n        if yticklabel == 'name':\n            ax.set_yticklabels(['C{}'.format(i - 2) for i in range(11)])\n    elif ytick == 'step':\n        ax.set_yticks(np.arange(0, 128))\n        if yticklabel == 'name':\n            if is_drum:\n                ax.set_yticklabels([pretty_midi.note_number_to_drum_name(i)\n                                    for i in range(128)])\n            else:\n                ax.set_yticklabels([pretty_midi.note_number_to_name(i)\n                                    for i in range(128)])\n\n    # axis labels\n    if label == 'x' or label == 'both':\n        if xtick == 'step' or not xticklabel:\n            ax.set_xlabel('time (step)')\n        else:\n            ax.set_xlabel('time (beat)')\n\n    if label == 'y' or label == 'both':\n        if is_drum:\n            ax.set_ylabel('key name')\n        else:\n            ax.set_ylabel('pitch')\n\n    # grid\n    if grid != 'off':\n        ax.grid(axis=grid, color='k', linestyle=grid_linestyle,\n                linewidth=grid_linewidth)\n\n    # downbeat boarder\n    if downbeats is not None and preset != 'plain':\n        for step in downbeats:\n            ax.axvline(x=step, color='k', linewidth=1)", "response": "Plots a pianoroll given as a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting a single track on the current figure.", "response": "def plot_track(track, filename=None, beat_resolution=None, downbeats=None,\n               preset='default', cmap='Blues', xtick='auto', ytick='octave',\n               xticklabel=True, yticklabel='auto', tick_loc=None,\n               tick_direction='in', label='both', grid='both',\n               grid_linestyle=':', grid_linewidth=.5):\n    \"\"\"\n    Plot the pianoroll or save a plot of the pianoroll.\n\n    Parameters\n    ----------\n    filename :\n        The filename to which the plot is saved. If None, save nothing.\n    beat_resolution : int\n        The number of time steps used to represent a beat. Required and only\n        effective when `xtick` is 'beat'.\n    downbeats : list\n        An array that indicates whether the time step contains a downbeat\n        (i.e., the first time step of a bar).\n\n    preset : {'default', 'plain', 'frame'}\n        A string that indicates the preset theme to use.\n\n        - In 'default' preset, the ticks, grid and labels are on.\n        - In 'frame' preset, the ticks and grid are both off.\n        - In 'plain' preset, the x- and y-axis are both off.\n\n    cmap :  `matplotlib.colors.Colormap`\n        The colormap to use in :func:`matplotlib.pyplot.imshow`. Defaults to\n        'Blues'. Only effective when `pianoroll` is 2D.\n    xtick : {'auto', 'beat', 'step', 'off'}\n        A string that indicates what to use as ticks along the x-axis. If\n        'auto' is given, automatically set to 'beat' if `beat_resolution` is\n        also given and set to 'step', otherwise. Defaults to 'auto'.\n    ytick : {'octave', 'pitch', 'off'}\n        A string that indicates what to use as ticks along the y-axis.\n        Defaults to 'octave'.\n    xticklabel : bool\n        Whether to add tick labels along the x-axis. Only effective when\n        `xtick` is not 'off'.\n    yticklabel : {'auto', 'name', 'number', 'off'}\n        If 'name', use octave name and pitch name (key name when `is_drum`\n        is True) as tick labels along the y-axis. If 'number', use pitch\n        number. If 'auto', set to 'name' when `ytick` is 'octave' and\n        'number' when `ytick` is 'pitch'. Defaults to 'auto'. Only effective\n        when `ytick` is not 'off'.\n    tick_loc : tuple or list\n        The locations to put the ticks. Availables elements are 'bottom',\n        'top', 'left' and 'right'. Defaults to ('bottom', 'left').\n    tick_direction : {'in', 'out', 'inout'}\n        A string that indicates where to put the ticks. Defaults to 'in'.\n        Only effective when one of `xtick` and `ytick` is on.\n    label : {'x', 'y', 'both', 'off'}\n        A string that indicates whether to add labels to the x-axis and\n        y-axis. Defaults to 'both'.\n    grid : {'x', 'y', 'both', 'off'}\n        A string that indicates whether to add grids to the x-axis, y-axis,\n        both or neither. Defaults to 'both'.\n    grid_linestyle : str\n        Will be passed to :meth:`matplotlib.axes.Axes.grid` as 'linestyle'\n        argument.\n    grid_linewidth : float\n        Will be passed to :meth:`matplotlib.axes.Axes.grid` as 'linewidth'\n        argument.\n\n    Returns\n    -------\n    fig : `matplotlib.figure.Figure` object\n        A :class:`matplotlib.figure.Figure` object.\n    ax : `matplotlib.axes.Axes` object\n        A :class:`matplotlib.axes.Axes` object.\n\n    \"\"\"\n    if not HAS_MATPLOTLIB:\n        raise ImportError(\"matplotlib package is required for plotting \"\n                          \"supports.\")\n\n    fig, ax = plt.subplots()\n    plot_pianoroll(ax, track.pianoroll, track.is_drum, beat_resolution,\n                   downbeats, preset=preset, cmap=cmap, xtick=xtick,\n                   ytick=ytick, xticklabel=xticklabel, yticklabel=yticklabel,\n                   tick_loc=tick_loc, tick_direction=tick_direction,\n                   label=label, grid=grid, grid_linestyle=grid_linestyle,\n                   grid_linewidth=grid_linewidth)\n\n    if filename is not None:\n        plt.savefig(filename)\n\n    return fig, ax"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_multitrack(multitrack, filename=None, mode='separate',\n                    track_label='name', preset='default', cmaps=None,\n                    xtick='auto', ytick='octave', xticklabel=True,\n                    yticklabel='auto', tick_loc=None, tick_direction='in',\n                    label='both', grid='both', grid_linestyle=':',\n                    grid_linewidth=.5):\n    \"\"\"\n    Plot the pianorolls or save a plot of them.\n\n    Parameters\n    ----------\n    filename : str\n        The filename to which the plot is saved. If None, save nothing.\n    mode : {'separate', 'stacked', 'hybrid'}\n        A string that indicate the plotting mode to use. Defaults to\n        'separate'.\n\n        - In 'separate' mode, all the tracks are plotted separately.\n        - In 'stacked' mode, a color is assigned based on `cmaps` to the\n            pianoroll of each track and the pianorolls are stacked and\n            plotted as a colored image with RGB channels.\n        - In 'hybrid' mode, the drum tracks are merged into a 'Drums' track,\n            while the other tracks are merged into an 'Others' track, and the\n            two merged tracks are then plotted separately.\n\n    track_label : {'name', 'program', 'family', 'off'}\n        A sting that indicates what to use as labels to the track. When\n        `mode` is 'hybrid', all options other than 'off' will label the two\n        track with 'Drums' and 'Others'.\n    preset : {'default', 'plain', 'frame'}\n        A string that indicates the preset theme to use.\n\n        - In 'default' preset, the ticks, grid and labels are on.\n        - In 'frame' preset, the ticks and grid are both off.\n        - In 'plain' preset, the x- and y-axis are both off.\n\n    cmaps :  tuple or list\n        The `matplotlib.colors.Colormap` instances or colormap codes to use.\n\n        - When `mode` is 'separate', each element will be passed to each\n            call of :func:`matplotlib.pyplot.imshow`. Defaults to ('Blues',\n            'Oranges', 'Greens', 'Reds', 'Purples', 'Greys').\n        - When `mode` is stacked, a color is assigned based on `cmaps` to\n            the pianoroll of each track. Defaults to ('hsv').\n        - When `mode` is 'hybrid', the first (second) element is used in the\n            'Drums' ('Others') track. Defaults to ('Blues', 'Greens').\n\n    xtick : {'auto', 'beat', 'step', 'off'}\n        A string that indicates what to use as ticks along the x-axis. If\n        'auto' is given, automatically set to 'beat' if `beat_resolution` is\n        also given and set to 'step', otherwise. Defaults to 'auto'.\n    ytick : {'octave', 'pitch', 'off'}\n        A string that indicates what to use as ticks along the y-axis.\n        Defaults to 'octave'.\n    xticklabel : bool\n        Whether to add tick labels along the x-axis. Only effective when\n        `xtick` is not 'off'.\n    yticklabel : {'auto', 'name', 'number', 'off'}\n        If 'name', use octave name and pitch name (key name when `is_drum`\n        is True) as tick labels along the y-axis. If 'number', use pitch\n        number. If 'auto', set to 'name' when `ytick` is 'octave' and\n        'number' when `ytick` is 'pitch'. Defaults to 'auto'. Only effective\n        when `ytick` is not 'off'.\n    tick_loc : tuple or list\n        The locations to put the ticks. Availables elements are 'bottom',\n        'top', 'left' and 'right'. Defaults to ('bottom', 'left').\n    tick_direction : {'in', 'out', 'inout'}\n        A string that indicates where to put the ticks. Defaults to 'in'.\n        Only effective when one of `xtick` and `ytick` is on.\n    label : {'x', 'y', 'both', 'off'}\n        A string that indicates whether to add labels to the x-axis and\n        y-axis. Defaults to 'both'.\n    grid : {'x', 'y', 'both', 'off'}\n        A string that indicates whether to add grids to the x-axis, y-axis,\n        both or neither. Defaults to 'both'.\n    grid_linestyle : str\n        Will be passed to :meth:`matplotlib.axes.Axes.grid` as 'linestyle'\n        argument.\n    grid_linewidth : float\n        Will be passed to :meth:`matplotlib.axes.Axes.grid` as 'linewidth'\n        argument.\n\n    Returns\n    -------\n    fig : `matplotlib.figure.Figure` object\n        A :class:`matplotlib.figure.Figure` object.\n    axs : list\n        List of :class:`matplotlib.axes.Axes` object.\n\n    \"\"\"\n    if not HAS_MATPLOTLIB:\n        raise ImportError(\"matplotlib package is required for plotting \"\n                          \"supports.\")\n\n    def get_track_label(track_label, track=None):\n        \"\"\"Convenient function to get track labels\"\"\"\n        if track_label == 'name':\n            return track.name\n        elif track_label == 'program':\n            return pretty_midi.program_to_instrument_name(track.program)\n        elif track_label == 'family':\n            return pretty_midi.program_to_instrument_class(track.program)\n        elif track is None:\n            return track_label\n\n    def add_tracklabel(ax, track_label, track=None):\n        \"\"\"Convenient function for adding track labels\"\"\"\n        if not ax.get_ylabel():\n            return\n        ax.set_ylabel(get_track_label(track_label, track) + '\\n\\n'\n                      + ax.get_ylabel())\n\n    multitrack.check_validity()\n    if not multitrack.tracks:\n        raise ValueError(\"There is no track to plot.\")\n    if mode not in ('separate', 'stacked', 'hybrid'):\n        raise ValueError(\"`mode` must be one of {'separate', 'stacked', \"\n                         \"'hybrid'}.\")\n    if track_label not in ('name', 'program', 'family', 'off'):\n        raise ValueError(\"`track_label` must be one of {'name', 'program', \"\n                         \"'family'}.\")\n\n    if cmaps is None:\n        if mode == 'separate':\n            cmaps = ('Blues', 'Oranges', 'Greens', 'Reds', 'Purples', 'Greys')\n        elif mode == 'stacked':\n            cmaps = ('hsv')\n        else:\n            cmaps = ('Blues', 'Greens')\n\n    num_track = len(multitrack.tracks)\n    downbeats = multitrack.get_downbeat_steps()\n\n    if mode == 'separate':\n        if num_track > 1:\n            fig, axs = plt.subplots(num_track, sharex=True)\n        else:\n            fig, ax = plt.subplots()\n            axs = [ax]\n\n        for idx, track in enumerate(multitrack.tracks):\n            now_xticklabel = xticklabel if idx < num_track else False\n            plot_pianoroll(axs[idx], track.pianoroll, False,\n                           multitrack.beat_resolution, downbeats, preset=preset,\n                           cmap=cmaps[idx%len(cmaps)], xtick=xtick, ytick=ytick,\n                           xticklabel=now_xticklabel, yticklabel=yticklabel,\n                           tick_loc=tick_loc, tick_direction=tick_direction,\n                           label=label, grid=grid,\n                           grid_linestyle=grid_linestyle,\n                           grid_linewidth=grid_linewidth)\n            if track_label != 'none':\n                add_tracklabel(axs[idx], track_label, track)\n\n        if num_track > 1:\n            fig.subplots_adjust(hspace=0)\n\n        if filename is not None:\n            plt.savefig(filename)\n\n        return (fig, axs)\n\n    elif mode == 'stacked':\n        is_all_drum = True\n        for track in multitrack.tracks:\n            if not track.is_drum:\n                is_all_drum = False\n\n        fig, ax = plt.subplots()\n        stacked = multitrack.get_stacked_pianorolls()\n\n        colormap = matplotlib.cm.get_cmap(cmaps[0])\n        cmatrix = colormap(np.arange(0, 1, 1 / num_track))[:, :3]\n        recolored = np.matmul(stacked.reshape(-1, num_track), cmatrix)\n        stacked = recolored.reshape(stacked.shape[:2] + (3, ))\n\n        plot_pianoroll(ax, stacked, is_all_drum, multitrack.beat_resolution,\n                       downbeats, preset=preset, xtick=xtick, ytick=ytick,\n                       xticklabel=xticklabel, yticklabel=yticklabel,\n                       tick_loc=tick_loc, tick_direction=tick_direction,\n                       label=label, grid=grid, grid_linestyle=grid_linestyle,\n                       grid_linewidth=grid_linewidth)\n\n        if track_label != 'none':\n            patches = [Patch(color=cmatrix[idx],\n                             label=get_track_label(track_label, track))\n                       for idx, track in enumerate(multitrack.tracks)]\n            plt.legend(handles=patches)\n\n        if filename is not None:\n            plt.savefig(filename)\n\n        return (fig, [ax])\n\n    elif mode == 'hybrid':\n        drums = [i for i, track in enumerate(multitrack.tracks)\n                 if track.is_drum]\n        others = [i for i in range(len(multitrack.tracks)) if i not in drums]\n        merged_drums = multitrack.get_merged_pianoroll(drums)\n        merged_others = multitrack.get_merged_pianoroll(others)\n\n        fig, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=True)\n        plot_pianoroll(ax1, merged_drums, True, multitrack.beat_resolution,\n                       downbeats, preset=preset, cmap=cmaps[0], xtick=xtick,\n                       ytick=ytick, xticklabel=xticklabel,\n                       yticklabel=yticklabel, tick_loc=tick_loc,\n                       tick_direction=tick_direction, label=label, grid=grid,\n                       grid_linestyle=grid_linestyle,\n                       grid_linewidth=grid_linewidth)\n        plot_pianoroll(ax2, merged_others, False, multitrack.beat_resolution,\n                       downbeats, preset=preset, cmap=cmaps[1], ytick=ytick,\n                       xticklabel=xticklabel, yticklabel=yticklabel,\n                       tick_loc=tick_loc, tick_direction=tick_direction,\n                       label=label, grid=grid, grid_linestyle=grid_linestyle,\n                       grid_linewidth=grid_linewidth)\n        fig.subplots_adjust(hspace=0)\n\n        if track_label != 'none':\n            add_tracklabel(ax1, 'Drums')\n            add_tracklabel(ax2, 'Others')\n\n        if filename is not None:\n            plt.savefig(filename)\n\n        return (fig, [ax1, ax2])", "response": "Plots the pianorolls of a multitrack."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves a pianoroll to a video or GIF file.", "response": "def save_animation(filename, pianoroll, window, hop=1, fps=None, is_drum=False,\n                   beat_resolution=None, downbeats=None, preset='default',\n                   cmap='Blues', xtick='auto', ytick='octave', xticklabel=True,\n                   yticklabel='auto', tick_loc=None, tick_direction='in',\n                   label='both', grid='both', grid_linestyle=':',\n                   grid_linewidth=.5, **kwargs):\n    \"\"\"\n    Save a pianoroll to an animation in video or GIF format.\n\n    Parameters\n    ----------\n    filename : str\n        The filename to which the animation is saved.\n    pianoroll : np.ndarray\n        A pianoroll to be plotted. The values should be in [0, 1] when data\n        type is float, and in [0, 127] when data type is integer.\n\n        - For a 2D array, shape=(num_time_step, num_pitch).\n        - For a 3D array, shape=(num_time_step, num_pitch, num_channel),\n          where channels can be either RGB or RGBA.\n\n    window : int\n        The window size to be applied to `pianoroll` for the animation.\n    hop : int\n        The hop size to be applied to `pianoroll` for the animation.\n    fps : int\n        The number of frames per second in the resulting video or GIF file.\n    is_drum : bool\n        A boolean number that indicates whether it is a percussion track.\n        Defaults to False.\n    beat_resolution : int\n        The number of time steps used to represent a beat. Required and only\n        effective when `xtick` is 'beat'.\n    downbeats : list\n        An array that indicates whether the time step contains a downbeat\n        (i.e., the first time step of a bar).\n\n    preset : {'default', 'plain', 'frame'}\n        A string that indicates the preset theme to use.\n\n        - In 'default' preset, the ticks, grid and labels are on.\n        - In 'frame' preset, the ticks and grid are both off.\n        - In 'plain' preset, the x- and y-axis are both off.\n\n    cmap :  `matplotlib.colors.Colormap`\n        The colormap to use in :func:`matplotlib.pyplot.imshow`. Defaults to\n        'Blues'. Only effective when `pianoroll` is 2D.\n    xtick : {'auto', 'beat', 'step', 'off'}\n        A string that indicates what to use as ticks along the x-axis. If\n        'auto' is given, automatically set to 'beat' if `beat_resolution` is\n        also given and set to 'step', otherwise. Defaults to 'auto'.\n    ytick : {'octave', 'pitch', 'off'}\n        A string that indicates what to use as ticks along the y-axis.\n        Defaults to 'octave'.\n    xticklabel : bool\n        Whether to add tick labels along the x-axis. Only effective when\n        `xtick` is not 'off'.\n    yticklabel : {'auto', 'name', 'number', 'off'}\n        If 'name', use octave name and pitch name (key name when `is_drum`\n        is True) as tick labels along the y-axis. If 'number', use pitch\n        number. If 'auto', set to 'name' when `ytick` is 'octave' and\n        'number' when `ytick` is 'pitch'. Defaults to 'auto'. Only effective\n        when `ytick` is not 'off'.\n    tick_loc : tuple or list\n        The locations to put the ticks. Availables elements are 'bottom',\n        'top', 'left' and 'right'. Defaults to ('bottom', 'left').\n    tick_direction : {'in', 'out', 'inout'}\n        A string that indicates where to put the ticks. Defaults to 'in'.\n        Only effective when one of `xtick` and `ytick` is on.\n    label : {'x', 'y', 'both', 'off'}\n        A string that indicates whether to add labels to the x-axis and\n        y-axis. Defaults to 'both'.\n    grid : {'x', 'y', 'both', 'off'}\n        A string that indicates whether to add grids to the x-axis, y-axis,\n        both or neither. Defaults to 'both'.\n    grid_linestyle : str\n        Will be passed to :meth:`matplotlib.axes.Axes.grid` as 'linestyle'\n        argument.\n    grid_linewidth : float\n        Will be passed to :meth:`matplotlib.axes.Axes.grid` as 'linewidth'\n        argument.\n\n    \"\"\"\n    if not HAS_MOVIEPY:\n        raise ImportError(\"moviepy package is required for animation supports.\")\n\n    def make_frame(t):\n        \"\"\"Return an image of the frame for time t.\"\"\"\n        fig = plt.gcf()\n        ax = plt.gca()\n        f_idx = int(t * fps)\n        start = hop * f_idx\n        end = start + window\n        to_plot = transposed[:, start:end]\n        extent = (start, end - 1, 0, 127)\n        ax.imshow(to_plot, cmap=cmap, aspect='auto', vmin=vmin, vmax=vmax,\n                  origin='lower', interpolation='none', extent=extent)\n\n        if xtick == 'beat':\n            next_major_idx = beat_resolution - start % beat_resolution\n            if start % beat_resolution < beat_resolution//2:\n                next_minor_idx = beat_resolution//2 - start % beat_resolution\n            else:\n                next_minor_idx = (beat_resolution//2 - start % beat_resolution\n                                  + beat_resolution)\n            xticks_major = np.arange(next_major_idx, window, beat_resolution)\n            xticks_minor = np.arange(next_minor_idx, window, beat_resolution)\n            if end % beat_resolution < beat_resolution//2:\n                last_minor_idx = beat_resolution//2 - end % beat_resolution\n            else:\n                last_minor_idx = (beat_resolution//2 - end % beat_resolution\n                                  + beat_resolution)\n            xtick_labels = np.arange((start + next_minor_idx)//beat_resolution,\n                                     (end + last_minor_idx)//beat_resolution)\n            ax.set_xticks(xticks_major)\n            ax.set_xticklabels('')\n            ax.set_xticks(xticks_minor, minor=True)\n            ax.set_xticklabels(xtick_labels, minor=True)\n            ax.tick_params(axis='x', which='minor', width=0)\n\n        return mplfig_to_npimage(fig)\n\n    if xtick == 'auto':\n        xtick = 'beat' if beat_resolution is not None else 'step'\n\n    fig, ax = plt.subplots()\n    plot_pianoroll(ax, pianoroll[:window], is_drum, beat_resolution, downbeats,\n                   preset=preset, cmap=cmap, xtick=xtick, ytick=ytick,\n                   xticklabel=xticklabel, yticklabel=yticklabel,\n                   tick_loc=tick_loc, tick_direction=tick_direction,\n                   label=label, grid=grid, grid_linestyle=grid_linestyle,\n                   grid_linewidth=grid_linewidth)\n\n    num_frame = int((pianoroll.shape[0] - window) / hop)\n    duration = int(num_frame / fps)\n\n    if (np.issubdtype(pianoroll.dtype, np.bool_)\n            or np.issubdtype(pianoroll.dtype, np.floating)):\n        vmax = 1\n    elif np.issubdtype(pianoroll.dtype, np.integer):\n        vmax = 127\n    else:\n        raise TypeError(\"Unsupported data type for `pianoroll`.\")\n    vmin = 0\n\n    transposed = pianoroll.T\n    animation = VideoClip(make_frame, duration=duration)\n    if filename.endswith('.gif'):\n        animation.write_gif(filename, fps, **kwargs)\n    else:\n        animation.write_videofile(filename, fps, **kwargs)\n    plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append_track(self, track=None, pianoroll=None, program=0, is_drum=False,\n                     name='unknown'):\n        \"\"\"\n        Append a multitrack.Track instance to the track list or create a new\n        multitrack.Track object and append it to the track list.\n\n        Parameters\n        ----------\n        track : pianoroll.Track\n            A :class:`pypianoroll.Track` instance to be appended to the track\n            list.\n        pianoroll : np.ndarray, shape=(n_time_steps, 128)\n            A pianoroll matrix. The first and second dimension represents time\n            and pitch, respectively. Available datatypes are bool, int and\n            float. Only effective when `track` is None.\n        program: int\n            A program number according to General MIDI specification [1].\n            Available values are 0 to 127. Defaults to 0 (Acoustic Grand Piano).\n            Only effective when `track` is None.\n        is_drum : bool\n            A boolean number that indicates whether it is a percussion track.\n            Defaults to False. Only effective when `track` is None.\n        name : str\n            The name of the track. Defaults to 'unknown'. Only effective when\n            `track` is None.\n\n        References\n        ----------\n        [1] https://www.midi.org/specifications/item/gm-level-1-sound-set\n\n        \"\"\"\n        if track is not None:\n            if not isinstance(track, Track):\n                raise TypeError(\"`track` must be a pypianoroll.Track instance.\")\n            track.check_validity()\n        else:\n            track = Track(pianoroll, program, is_drum, name)\n        self.tracks.append(track)", "response": "Append a track to the multitrack. Track list or create a new multitrack. Track object if one is not provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that the current object is valid.", "response": "def check_validity(self):\n        \"\"\"\n        Raise an error if any invalid attribute found.\n\n        Raises\n        ------\n        TypeError\n            If an attribute has an invalid type.\n        ValueError\n            If an attribute has an invalid value (of the correct type).\n\n        \"\"\"\n        # tracks\n        for track in self.tracks:\n            if not isinstance(track, Track):\n                raise TypeError(\"`tracks` must be a list of \"\n                                \"`pypianoroll.Track` instances.\")\n            track.check_validity()\n        # tempo\n        if not isinstance(self.tempo, np.ndarray):\n            raise TypeError(\"`tempo` must be int or a numpy array.\")\n        elif not np.issubdtype(self.tempo.dtype, np.number):\n            raise TypeError(\"Data type of `tempo` must be a subdtype of \"\n                            \"np.number.\")\n        elif self.tempo.ndim != 1:\n            raise ValueError(\"`tempo` must be a 1D numpy array.\")\n        if np.any(self.tempo <= 0.0):\n            raise ValueError(\"`tempo` should contain only positive numbers.\")\n        # downbeat\n        if self.downbeat is not None:\n            if not isinstance(self.downbeat, np.ndarray):\n                raise TypeError(\"`downbeat` must be a numpy array.\")\n            if not np.issubdtype(self.downbeat.dtype, np.bool_):\n                raise TypeError(\"Data type of `downbeat` must be bool.\")\n            if self.downbeat.ndim != 1:\n                raise ValueError(\"`downbeat` must be a 1D numpy array.\")\n        # beat_resolution\n        if not isinstance(self.beat_resolution, int):\n            raise TypeError(\"`beat_resolution` must be int.\")\n        if self.beat_resolution < 1:\n            raise ValueError(\"`beat_resolution` must be a positive integer.\")\n        # name\n        if not isinstance(self.name, string_types):\n            raise TypeError(\"`name` must be a string.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clip(self, lower=0, upper=127):\n        for track in self.tracks:\n            track.clip(lower, upper)", "response": "Clip the pianorolls of all tracks by the given lower and upper bounds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_active_length(self):\n        active_length = 0\n        for track in self.tracks:\n            now_length = track.get_active_length()\n            if active_length < track.get_active_length():\n                active_length = now_length\n        return active_length", "response": "Returns the maximum active length among the pianorolls of all tracks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the active pitch range of the pianorolls of all tracks as a tuple low highest.", "response": "def get_active_pitch_range(self):\n        \"\"\"\n        Return the active pitch range of the pianorolls of all tracks as a tuple\n        (lowest, highest).\n\n        Returns\n        -------\n        lowest : int\n            The lowest active pitch among the pianorolls of all tracks.\n        highest : int\n            The lowest highest pitch among the pianorolls of all tracks.\n\n        \"\"\"\n        lowest, highest = self.tracks[0].get_active_pitch_range()\n        if len(self.tracks) > 1:\n            for track in self.tracks[1:]:\n                low, high = track.get_active_pitch_range()\n                if low < lowest:\n                    lowest = low\n                if high > highest:\n                    highest = high\n        return lowest, highest"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the indices of time steps that contain downbeats.", "response": "def get_downbeat_steps(self):\n        \"\"\"\n        Return the indices of time steps that contain downbeats.\n\n        Returns\n        -------\n        downbeat_steps : list\n            The indices of time steps that contain downbeats.\n\n        \"\"\"\n        if self.downbeat is None:\n            return []\n        downbeat_steps = np.nonzero(self.downbeat)[0].tolist()\n        return downbeat_steps"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_empty_tracks(self):\n        empty_track_indices = [idx for idx, track in enumerate(self.tracks)\n                               if not np.any(track.pianoroll)]\n        return empty_track_indices", "response": "Returns the indices of tracks with empty pianorolls."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_max_length(self):\n        max_length = 0\n        for track in self.tracks:\n            if max_length < track.pianoroll.shape[0]:\n                max_length = track.pianoroll.shape[0]\n        return max_length", "response": "Returns the maximum length of the pianorolls along the time axis."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_merged_pianoroll(self, mode='sum'):\n        stacked = self.get_stacked_pianoroll()\n\n        if mode == 'any':\n            merged = np.any(stacked, axis=2)\n        elif mode == 'sum':\n            merged = np.sum(stacked, axis=2)\n        elif mode == 'max':\n            merged = np.max(stacked, axis=2)\n        else:\n            raise ValueError(\"`mode` must be one of {'max', 'sum', 'any'}.\")\n\n        return merged", "response": "Returns the merged pianoroll."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_stacked_pianoroll(self):\n        multitrack = deepcopy(self)\n        multitrack.pad_to_same()\n        stacked = np.stack([track.pianoroll for track in multitrack.tracks], -1)\n        return stacked", "response": "Returns a stacked multitrack pianoroll."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(self, filename):\n        def reconstruct_sparse(target_dict, name):\n            \"\"\"Return a reconstructed instance of `scipy.sparse.csc_matrix`.\"\"\"\n            return csc_matrix((target_dict[name+'_csc_data'],\n                               target_dict[name+'_csc_indices'],\n                               target_dict[name+'_csc_indptr']),\n                              shape=target_dict[name+'_csc_shape']).toarray()\n\n        with np.load(filename) as loaded:\n            if 'info.json' not in loaded:\n                raise ValueError(\"Cannot find 'info.json' in the npz file.\")\n            info_dict = json.loads(loaded['info.json'].decode('utf-8'))\n            self.name = info_dict['name']\n            self.beat_resolution = info_dict['beat_resolution']\n\n            self.tempo = loaded['tempo']\n            if 'downbeat' in loaded.files:\n                self.downbeat = loaded['downbeat']\n            else:\n                self.downbeat = None\n\n            idx = 0\n            self.tracks = []\n            while str(idx) in info_dict:\n                pianoroll = reconstruct_sparse(\n                    loaded, 'pianoroll_{}'.format(idx))\n                track = Track(pianoroll, info_dict[str(idx)]['program'],\n                              info_dict[str(idx)]['is_drum'],\n                              info_dict[str(idx)]['name'])\n                self.tracks.append(track)\n                idx += 1\n\n        self.check_validity()", "response": "Load a npz file into the internal state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_tracks(self, track_indices=None, mode='sum', program=0,\n                     is_drum=False, name='merged', remove_merged=False):\n        \"\"\"\n        Merge pianorolls of the tracks specified by `track_indices`. The merged\n        track will have program number as given by `program` and drum indicator\n        as given by `is_drum`. The merged track will be appended at the end of\n        the track list.\n\n        Parameters\n        ----------\n        track_indices : list\n            The indices of tracks to be merged. Defaults to all the tracks.\n        mode : {'sum', 'max', 'any'}\n            A string that indicates the merging strategy to apply along the\n            track axis. Default to 'sum'.\n\n            - In 'sum' mode, the merged pianoroll is the sum of the collected\n              pianorolls. Note that for binarized pianorolls, integer summation\n              is performed.\n            - In 'max' mode, for each pixel, the maximum value among the\n              collected pianorolls is assigned to the merged pianoroll.\n            - In 'any' mode, the value of a pixel in the merged pianoroll is\n              True if any of the collected pianorolls has nonzero value at that\n              pixel; False if all the collected pianorolls are inactive\n              (zero-valued) at that pixel.\n\n        program: int\n            A program number according to General MIDI specification [1].\n            Available values are 0 to 127. Defaults to 0 (Acoustic Grand Piano).\n        is_drum : bool\n            A boolean number that indicates whether it is a percussion track.\n            Defaults to False.\n        name : str\n            A name to be assigned to the merged track. Defaults to 'merged'.\n        remove_merged : bool\n            True to remove the source tracks from the track list. False to keep\n            them. Defaults to False.\n\n        References\n        ----------\n        [1] https://www.midi.org/specifications/item/gm-level-1-sound-set\n\n        \"\"\"\n        if mode not in ('max', 'sum', 'any'):\n            raise ValueError(\"`mode` must be one of {'max', 'sum', 'any'}.\")\n\n        merged = self[track_indices].get_merged_pianoroll(mode)\n\n        merged_track = Track(merged, program, is_drum, name)\n        self.append_track(merged_track)\n\n        if remove_merged:\n            self.remove_tracks(track_indices)", "response": "This method merges the pianorolls of the specified tracks into a single track. The tracks are assigned to the merged pianoroll."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npads shorter pianorolls with zeros at the end along the time axis to make the resulting pianoroll lengths the same as the maximum pianoroll lengths among all the tracks.", "response": "def pad_to_same(self):\n        \"\"\"Pad shorter pianorolls with zeros at the end along the time axis to\n        make the resulting pianoroll lengths the same as the maximum pianoroll\n        length among all the tracks.\"\"\"\n        max_length = self.get_max_length()\n        for track in self.tracks:\n            if track.pianoroll.shape[0] < max_length:\n                track.pad(max_length - track.pianoroll.shape[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_midi(self, filename, **kwargs):\n        pm = pretty_midi.PrettyMIDI(filename)\n        self.parse_pretty_midi(pm, **kwargs)", "response": "Parse a MIDI file and store the result in self."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_pretty_midi(self, pm, mode='max', algorithm='normal',\n                          binarized=False, skip_empty_tracks=True,\n                          collect_onsets_only=False, threshold=0,\n                          first_beat_time=None):\n        \"\"\"\n        Parse a :class:`pretty_midi.PrettyMIDI` object. The data type of the\n        resulting pianorolls is automatically determined (int if 'mode' is\n        'sum', np.uint8 if `mode` is 'max' and `binarized` is False, bool if\n        `mode` is 'max' and `binarized` is True).\n\n        Parameters\n        ----------\n        pm : `pretty_midi.PrettyMIDI` object\n            A :class:`pretty_midi.PrettyMIDI` object to be parsed.\n        mode : {'max', 'sum'}\n            A string that indicates the merging strategy to apply to duplicate\n            notes. Default to 'max'.\n        algorithm : {'normal', 'strict', 'custom'}\n            A string that indicates the method used to get the location of the\n            first beat. Notes before it will be dropped unless an incomplete\n            beat before it is found (see Notes for more information). Defaults\n            to 'normal'.\n\n            - The 'normal' algorithm estimates the location of the first beat by\n              :meth:`pretty_midi.PrettyMIDI.estimate_beat_start`.\n            - The 'strict' algorithm sets the first beat at the event time of\n              the first time signature change. Raise a ValueError if no time\n              signature change event is found.\n            - The 'custom' algorithm takes argument `first_beat_time` as the\n              location of the first beat.\n\n        binarized : bool\n            True to binarize the parsed pianorolls before merging duplicate\n            notes. False to use the original parsed pianorolls. Defaults to\n            False.\n        skip_empty_tracks : bool\n            True to remove tracks with empty pianorolls and compress the pitch\n            range of the parsed pianorolls. False to retain the empty tracks\n            and use the original parsed pianorolls. Deafault to True.\n        collect_onsets_only : bool\n            True to collect only the onset of the notes (i.e. note on events) in\n            all tracks, where the note off and duration information are dropped.\n            False to parse regular pianorolls.\n        threshold : int or float\n            A threshold used to binarize the parsed pianorolls. Only effective\n            when `binarized` is True. Defaults to zero.\n        first_beat_time : float\n            The location (in sec) of the first beat. Required and only effective\n            when using 'custom' algorithm.\n\n        Notes\n        -----\n        If an incomplete beat before the first beat is found, an additional beat\n        will be added before the (estimated) beat starting time. However, notes\n        before the (estimated) beat starting time for more than one beat are\n        dropped.\n\n        \"\"\"\n        if mode not in ('max', 'sum'):\n            raise ValueError(\"`mode` must be one of {'max', 'sum'}.\")\n        if algorithm not in ('strict', 'normal', 'custom'):\n            raise ValueError(\"`algorithm` must be one of {'normal', 'strict', \"\n                             \" 'custom'}.\")\n        if algorithm == 'custom':\n            if not isinstance(first_beat_time, (int, float)):\n                raise TypeError(\"`first_beat_time` must be int or float when \"\n                                \"using 'custom' algorithm.\")\n            if first_beat_time < 0.0:\n                raise ValueError(\"`first_beat_time` must be a positive number \"\n                                 \"when using 'custom' algorithm.\")\n\n        # Set first_beat_time for 'normal' and 'strict' modes\n        if algorithm == 'normal':\n            if pm.time_signature_changes:\n                pm.time_signature_changes.sort(key=lambda x: x.time)\n                first_beat_time = pm.time_signature_changes[0].time\n            else:\n                first_beat_time = pm.estimate_beat_start()\n        elif algorithm == 'strict':\n            if not pm.time_signature_changes:\n                raise ValueError(\"No time signature change event found. Unable \"\n                                 \"to set beat start time using 'strict' \"\n                                 \"algorithm.\")\n            pm.time_signature_changes.sort(key=lambda x: x.time)\n            first_beat_time = pm.time_signature_changes[0].time\n\n        # get tempo change event times and contents\n        tc_times, tempi = pm.get_tempo_changes()\n        arg_sorted = np.argsort(tc_times)\n        tc_times = tc_times[arg_sorted]\n        tempi = tempi[arg_sorted]\n\n        beat_times = pm.get_beats(first_beat_time)\n        if not len(beat_times):\n            raise ValueError(\"Cannot get beat timings to quantize pianoroll.\")\n        beat_times.sort()\n\n        n_beats = len(beat_times)\n        n_time_steps = self.beat_resolution * n_beats\n\n        # Parse downbeat array\n        if not pm.time_signature_changes:\n            self.downbeat = None\n        else:\n            self.downbeat = np.zeros((n_time_steps,), bool)\n            self.downbeat[0] = True\n            start = 0\n            end = start\n            for idx, tsc in enumerate(pm.time_signature_changes[:-1]):\n                end += np.searchsorted(beat_times[end:],\n                                       pm.time_signature_changes[idx+1].time)\n                start_idx = start * self.beat_resolution\n                end_idx = end * self.beat_resolution\n                stride = tsc.numerator * self.beat_resolution\n                self.downbeat[start_idx:end_idx:stride] = True\n                start = end\n\n        # Build tempo array\n        one_more_beat = 2 * beat_times[-1] - beat_times[-2]\n        beat_times_one_more = np.append(beat_times, one_more_beat)\n        bpm = 60. / np.diff(beat_times_one_more)\n        self.tempo = np.tile(bpm, (1, 24)).reshape(-1,)\n\n        # Parse pianoroll\n        self.tracks = []\n        for instrument in pm.instruments:\n            if binarized:\n                pianoroll = np.zeros((n_time_steps, 128), bool)\n            elif mode == 'max':\n                pianoroll = np.zeros((n_time_steps, 128), np.uint8)\n            else:\n                pianoroll = np.zeros((n_time_steps, 128), int)\n\n            pitches = np.array([note.pitch for note in instrument.notes\n                                if note.end > first_beat_time])\n            note_on_times = np.array([note.start for note in instrument.notes\n                                      if note.end > first_beat_time])\n            beat_indices = np.searchsorted(beat_times, note_on_times) - 1\n            remained = note_on_times - beat_times[beat_indices]\n            ratios = remained / (beat_times_one_more[beat_indices + 1]\n                                 - beat_times[beat_indices])\n            rounded = np.round((beat_indices + ratios) * self.beat_resolution)\n            note_ons = rounded.astype(int)\n\n            if collect_onsets_only:\n                pianoroll[note_ons, pitches] = True\n            elif instrument.is_drum:\n                if binarized:\n                    pianoroll[note_ons, pitches] = True\n                else:\n                    velocities = [note.velocity for note in instrument.notes\n                                  if note.end > first_beat_time]\n                    pianoroll[note_ons, pitches] = velocities\n            else:\n                note_off_times = np.array([note.end for note in instrument.notes\n                                           if note.end > first_beat_time])\n                beat_indices = np.searchsorted(beat_times, note_off_times) - 1\n                remained = note_off_times - beat_times[beat_indices]\n                ratios = remained / (beat_times_one_more[beat_indices + 1]\n                                     - beat_times[beat_indices])\n                note_offs = ((beat_indices + ratios)\n                             * self.beat_resolution).astype(int)\n\n                for idx, start in enumerate(note_ons):\n                    end = note_offs[idx]\n                    velocity = instrument.notes[idx].velocity\n\n                    if velocity < 1:\n                        continue\n                    if binarized and velocity <= threshold:\n                        continue\n\n                    if start > 0 and start < n_time_steps:\n                        if pianoroll[start - 1, pitches[idx]]:\n                            pianoroll[start - 1, pitches[idx]] = 0\n                    if end < n_time_steps - 1:\n                        if pianoroll[end, pitches[idx]]:\n                            end -= 1\n\n                    if binarized:\n                        if mode == 'sum':\n                            pianoroll[start:end, pitches[idx]] += 1\n                        elif mode == 'max':\n                            pianoroll[start:end, pitches[idx]] = True\n                    elif mode == 'sum':\n                        pianoroll[start:end, pitches[idx]] += velocity\n                    elif mode == 'max':\n                        maximum = np.maximum(\n                            pianoroll[start:end, pitches[idx]], velocity)\n                        pianoroll[start:end, pitches[idx]] = maximum\n\n            if skip_empty_tracks and not np.any(pianoroll):\n                continue\n\n            track = Track(pianoroll, int(instrument.program),\n                          instrument.is_drum, instrument.name)\n            self.tracks.append(track)\n\n        self.check_validity()", "response": "Parse a duplicate pianoroll file into a list of pianorolls."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving tracks specified by track_indices from the set of tracks.", "response": "def remove_tracks(self, track_indices):\n        \"\"\"\n        Remove tracks specified by `track_indices`.\n\n        Parameters\n        ----------\n        track_indices : list\n            The indices of the tracks to be removed.\n\n        \"\"\"\n        if isinstance(track_indices, int):\n            track_indices = [track_indices]\n        self.tracks = [track for idx, track in enumerate(self.tracks)\n                       if idx not in track_indices]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, filename, compressed=True):\n        def update_sparse(target_dict, sparse_matrix, name):\n            \"\"\"Turn `sparse_matrix` into a scipy.sparse.csc_matrix and update\n            its component arrays to the `target_dict` with key as `name`\n            suffixed with its component type string.\"\"\"\n            csc = csc_matrix(sparse_matrix)\n            target_dict[name+'_csc_data'] = csc.data\n            target_dict[name+'_csc_indices'] = csc.indices\n            target_dict[name+'_csc_indptr'] = csc.indptr\n            target_dict[name+'_csc_shape'] = csc.shape\n\n        self.check_validity()\n        array_dict = {'tempo': self.tempo}\n        info_dict = {'beat_resolution': self.beat_resolution,\n                     'name': self.name}\n\n        if self.downbeat is not None:\n            array_dict['downbeat'] = self.downbeat\n\n        for idx, track in enumerate(self.tracks):\n            update_sparse(array_dict, track.pianoroll,\n                          'pianoroll_{}'.format(idx))\n            info_dict[str(idx)] = {'program': track.program,\n                                   'is_drum': track.is_drum,\n                                   'name': track.name}\n\n        if not filename.endswith('.npz'):\n            filename += '.npz'\n        if compressed:\n            np.savez_compressed(filename, **array_dict)\n        else:\n            np.savez(filename, **array_dict)\n\n        compression = zipfile.ZIP_DEFLATED if compressed else zipfile.ZIP_STORED\n        with zipfile.ZipFile(filename, 'a') as zip_file:\n            zip_file.writestr('info.json', json.dumps(info_dict), compression)", "response": "Save the multitrack pianoroll to a npz file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the MIDI file into a pretty MIDI file.", "response": "def to_pretty_midi(self, constant_tempo=None, constant_velocity=100):\n        \"\"\"\n        Convert to a :class:`pretty_midi.PrettyMIDI` instance.\n\n        Notes\n        -----\n        - Only constant tempo is supported by now.\n        - The velocities of the converted pianorolls are clipped to [0, 127],\n          i.e. values below 0 and values beyond 127 are replaced by 127 and 0,\n          respectively.\n        - Adjacent nonzero values of the same pitch will be considered a single\n          note with their mean as its velocity.\n\n        Parameters\n        ----------\n        constant_tempo : int\n            The constant tempo value of the output object. Defaults to use the\n            first element of `tempo`.\n        constant_velocity : int\n            The constant velocity to be assigned to binarized tracks. Defaults\n            to 100.\n\n        Returns\n        -------\n        pm : `pretty_midi.PrettyMIDI` object\n            The converted :class:`pretty_midi.PrettyMIDI` instance.\n\n        \"\"\"\n        self.check_validity()\n        pm = pretty_midi.PrettyMIDI(initial_tempo=self.tempo[0])\n\n        # TODO: Add downbeat support -> time signature change events\n        # TODO: Add tempo support -> tempo change events\n        if constant_tempo is None:\n            constant_tempo = self.tempo[0]\n        time_step_size = 60. / constant_tempo / self.beat_resolution\n\n        for track in self.tracks:\n            instrument = pretty_midi.Instrument(\n                program=track.program, is_drum=track.is_drum, name=track.name)\n            copied = track.copy()\n            if copied.is_binarized():\n                copied.assign_constant(constant_velocity)\n            copied.clip()\n            clipped = copied.pianoroll.astype(np.uint8)\n            binarized = (clipped > 0)\n            padded = np.pad(binarized, ((1, 1), (0, 0)), 'constant')\n            diff = np.diff(padded.astype(np.int8), axis=0)\n\n            positives = np.nonzero((diff > 0).T)\n            pitches = positives[0]\n            note_ons = positives[1]\n            note_on_times = time_step_size * note_ons\n            note_offs = np.nonzero((diff < 0).T)[1]\n            note_off_times = time_step_size * note_offs\n\n            for idx, pitch in enumerate(pitches):\n                velocity = np.mean(clipped[note_ons[idx]:note_offs[idx], pitch])\n                note = pretty_midi.Note(\n                    velocity=int(velocity), pitch=pitch,\n                    start=note_on_times[idx], end=note_off_times[idx])\n                instrument.notes.append(note)\n\n            instrument.notes.sort(key=lambda x: x.start)\n            pm.instruments.append(instrument)\n\n        return pm"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransposing the pianorolls of all tracks by a number of semitones.", "response": "def transpose(self, semitone):\n        \"\"\"\n        Transpose the pianorolls of all tracks by a number of semitones, where\n        positive values are for higher key, while negative values are for lower\n        key. The drum tracks are ignored.\n\n        Parameters\n        ----------\n        semitone : int\n            The number of semitones to transpose the pianorolls.\n\n        \"\"\"\n        for track in self.tracks:\n            if not track.is_drum:\n                track.transpose(semitone)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrimming the trailing silences of the pianorolls of all tracks.", "response": "def trim_trailing_silence(self):\n        \"\"\"Trim the trailing silences of the pianorolls of all tracks. Trailing\n        silences are considered globally.\"\"\"\n        active_length = self.get_active_length()\n        for track in self.tracks:\n            track.pianoroll = track.pianoroll[:active_length]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self, filename):\n        if not filename.endswith(('.mid', '.midi', '.MID', '.MIDI')):\n            filename = filename + '.mid'\n        pm = self.to_pretty_midi()\n        pm.write(filename)", "response": "Writes the multitrack pianoroll to a MIDI file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the input array is a standard piano - roll matrix.", "response": "def check_pianoroll(arr):\n    \"\"\"\n    Return True if the array is a standard piano-roll matrix. Otherwise,\n    return False. Raise TypeError if the input object is not a numpy array.\n\n    \"\"\"\n    if not isinstance(arr, np.ndarray):\n        raise TypeError(\"`arr` must be of np.ndarray type\")\n    if not (np.issubdtype(arr.dtype, np.bool_)\n            or np.issubdtype(arr.dtype, np.number)):\n        return False\n    if arr.ndim != 2:\n        return False\n    if arr.shape[1] != 128:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef binarize(obj, threshold=0):\n    _check_supported(obj)\n    copied = deepcopy(obj)\n    copied.binarize(threshold)\n    return copied", "response": "Returns a copy of the object with binarized piano - rolls."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clip(obj, lower=0, upper=127):\n    _check_supported(obj)\n    copied = deepcopy(obj)\n    copied.clip(lower, upper)\n    return copied", "response": "Returns a copy of the object with piano - rolls clipped by a lower bound and an upper bound."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pad(obj, pad_length):\n    _check_supported(obj)\n    copied = deepcopy(obj)\n    copied.pad(pad_length)\n    return copied", "response": "Returns a copy of the object with piano - roll padded with zeros at the end\n    along the time axis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a copy of the object with its piano - roll padded with zeros at the end along the time axis with the minimal length that make the length of the resulting piano - roll a multiple of factor.", "response": "def pad_to_multiple(obj, factor):\n    \"\"\"\n    Return a copy of the object with its piano-roll padded with zeros at the\n    end along the time axis with the minimal length that make the length of\n    the resulting piano-roll a multiple of `factor`.\n\n    Parameters\n    ----------\n    factor : int\n        The value which the length of the resulting piano-roll will be\n        a multiple of.\n\n    \"\"\"\n    _check_supported(obj)\n    copied = deepcopy(obj)\n    copied.pad_to_multiple(factor)\n    return copied"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pad_to_same(obj):\n    if not isinstance(obj, Multitrack):\n        raise TypeError(\"Support only `pypianoroll.Multitrack` class objects\")\n    copied = deepcopy(obj)\n    copied.pad_to_same()\n    return copied", "response": "Returns a copy of the object with shorter piano - rolls padded with zeros\n    at the end along the time axis to the length of the piano - roll."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a MIDI file into a pypianoroll. Multitrack object.", "response": "def parse(filepath, beat_resolution=24, name='unknown'):\n    \"\"\"\n    Return a :class:`pypianoroll.Multitrack` object loaded from a MIDI\n    (.mid, .midi, .MID, .MIDI) file.\n\n    Parameters\n    ----------\n    filepath : str\n        The file path to the MIDI file.\n\n    \"\"\"\n    if not filepath.endswith(('.mid', '.midi', '.MID', '.MIDI')):\n        raise ValueError(\"Only MIDI files are supported\")\n    return Multitrack(filepath, beat_resolution=beat_resolution, name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the object to a. npz file.", "response": "def save(filepath, obj, compressed=True):\n    \"\"\"\n    Save the object to a .npz file.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to save the file.\n    obj: `pypianoroll.Multitrack` objects\n        The object to be saved.\n\n    \"\"\"\n    if not isinstance(obj, Multitrack):\n        raise TypeError(\"Support only `pypianoroll.Multitrack` class objects\")\n    obj.save(filepath, compressed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transpose(obj, semitone):\n    _check_supported(obj)\n    copied = deepcopy(obj)\n    copied.transpose(semitone)\n    return copied", "response": "Returns a copy of the object with piano - rolls transposed by semitones."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a copy of the object with trimmed trailing silence of the piano - rolls.", "response": "def trim_trailing_silence(obj):\n    \"\"\"\n    Return a copy of the object with trimmed trailing silence of the\n    piano-roll(s).\n\n    \"\"\"\n    _check_supported(obj)\n    copied = deepcopy(obj)\n    length = copied.get_active_length()\n    copied.pianoroll = copied.pianoroll[:length]\n    return copied"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the object to a MIDI file.", "response": "def write(obj, filepath):\n    \"\"\"\n    Write the object to a MIDI file.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to write the MIDI file.\n\n    \"\"\"\n    if not isinstance(obj, Multitrack):\n        raise TypeError(\"Support only `pypianoroll.Multitrack` class objects\")\n    obj.write(filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_pianoroll(pianoroll):\n    if not isinstance(pianoroll, np.ndarray):\n        raise TypeError(\"`pianoroll` must be of np.ndarray type.\")\n    if not (np.issubdtype(pianoroll.dtype, np.bool_)\n            or np.issubdtype(pianoroll.dtype, np.number)):\n        raise TypeError(\"The data type of `pianoroll` must be np.bool_ or a \"\n                        \"subdtype of  np.number.\")\n    if pianoroll.ndim != 2:\n        raise ValueError(\"`pianoroll` must have exactly two dimensions.\")\n    if pianoroll.shape[1] != 128:\n        raise ValueError(\"The length of the second axis of `pianoroll` must be \"\n                         \"128.\")", "response": "Raise an error if the input array is not a standard pianoroll."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _to_chroma(pianoroll):\n    _validate_pianoroll(pianoroll)\n    reshaped = pianoroll[:, :120].reshape(-1, 12, 10)\n    reshaped[..., :8] += pianoroll[:, 120:].reshape(-1, 1, 8)\n    return np.sum(reshaped, 1)", "response": "Return the unnormalized chroma features of a pianoroll."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef empty_beat_rate(pianoroll, beat_resolution):\n    _validate_pianoroll(pianoroll)\n    reshaped = pianoroll.reshape(-1, beat_resolution * pianoroll.shape[1])\n    n_empty_beats = np.count_nonzero(reshaped.any(1))\n    return n_empty_beats / len(reshaped)", "response": "Return the ratio of empty beats to the total number of beats in a\n    pianoroll."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef n_pitche_classes_used(pianoroll):\n    _validate_pianoroll(pianoroll)\n    chroma = _to_chroma(pianoroll)\n    return np.count_nonzero(np.any(chroma, 0))", "response": "Return the number of unique pitch classes used in a pianoroll."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the ratio of the number of the qualified notes longer than a threshold ( in time step ) to the total number of notes in a pianoroll.", "response": "def qualified_note_rate(pianoroll, threshold=2):\n    \"\"\"Return the ratio of the number of the qualified notes (notes longer than\n    `threshold` (in time step)) to the total number of notes in a pianoroll.\"\"\"\n    _validate_pianoroll(pianoroll)\n    if np.issubdtype(pianoroll.dtype, np.bool_):\n        pianoroll = pianoroll.astype(np.uint8)\n    padded = np.pad(pianoroll, ((1, 1), (0, 0)), 'constant')\n    diff = np.diff(padded, axis=0).reshape(-1)\n    onsets = (diff > 0).nonzero()[0]\n    offsets = (diff < 0).nonzero()[0]\n    n_qualified_notes = np.count_nonzero(offsets - onsets >= threshold)\n    return n_qualified_notes / len(onsets)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the ratio of the number of time steps where the number of pitches being played is larger than threshold to the total number of time steps in a pianoroll.", "response": "def polyphonic_rate(pianoroll, threshold=2):\n    \"\"\"Return the ratio of the number of time steps where the number of pitches\n    being played is larger than `threshold` to the total number of time steps\n    in a pianoroll.\"\"\"\n    _validate_pianoroll(pianoroll)\n    n_poly = np.count_nonzero(np.count_nonzero(pianoroll, 1) > threshold)\n    return n_poly / len(pianoroll)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the percentage of number of drum notes that lie on the drum tree pattern.", "response": "def drum_in_pattern_rate(pianoroll, beat_resolution, tolerance=0.1):\n    \"\"\"Return the ratio of the number of drum notes that lie on the drum\n    pattern (i.e., at certain time steps) to the total number of drum notes.\"\"\"\n    if beat_resolution not in (4, 6, 8, 9, 12, 16, 18, 24):\n        raise ValueError(\"Unsupported beat resolution. Only 4, 6, 8 ,9, 12, \"\n                         \"16, 18, 42 are supported.\")\n    _validate_pianoroll(pianoroll)\n\n    def _drum_pattern_mask(res, tol):\n        \"\"\"Return a drum pattern mask with the given tolerance.\"\"\"\n        if res == 24:\n            drum_pattern_mask = np.tile([1., tol, 0., 0., 0., tol], 4)\n        elif res == 12:\n            drum_pattern_mask = np.tile([1., tol, tol], 4)\n        elif res == 6:\n            drum_pattern_mask = np.tile([1., tol, tol], 2)\n        elif res == 18:\n            drum_pattern_mask = np.tile([1., tol, 0., 0., 0., tol], 3)\n        elif res == 9:\n            drum_pattern_mask = np.tile([1., tol, tol], 3)\n        elif res == 16:\n            drum_pattern_mask = np.tile([1., tol, 0., tol], 4)\n        elif res == 8:\n            drum_pattern_mask = np.tile([1., tol], 4)\n        elif res == 4:\n            drum_pattern_mask = np.tile([1., tol], 2)\n        return drum_pattern_mask\n\n    drum_pattern_mask = _drum_pattern_mask(beat_resolution, tolerance)\n    n_in_pattern = np.sum(drum_pattern_mask * np.count_nonzero(pianoroll, 1))\n    return n_in_pattern / np.count_nonzero(pianoroll)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef in_scale_rate(pianoroll, key=3, kind='major'):\n    if not isinstance(key, int):\n        raise TypeError(\"`key` must an integer.\")\n    if key > 11 or key < 0:\n        raise ValueError(\"`key` must be in an integer in between 0 and 11.\")\n    if kind not in ('major', 'minor'):\n        raise ValueError(\"`kind` must be one of 'major' or 'minor'.\")\n    _validate_pianoroll(pianoroll)\n\n    def _scale_mask(key, kind):\n        \"\"\"Return a scale mask for the given key. Default to C major scale.\"\"\"\n        if kind == 'major':\n            a_scale_mask = np.array([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1], bool)\n        else:\n            a_scale_mask = np.array([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1], bool)\n        return np.roll(a_scale_mask, key)\n\n    chroma = _to_chroma(pianoroll)\n    scale_mask = _scale_mask(key, kind)\n    n_in_scale = np.sum(scale_mask.reshape(-1, 12) * chroma)\n    return n_in_scale / np.count_nonzero(pianoroll)", "response": "Return the ratio of the number of nonzero entries that lie in a specific\n    scale to the total number of nonzero entries in a specific\n   . Default to C major scale."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the tonal distance between two pianorolls.", "response": "def tonal_distance(pianoroll_1, pianoroll_2, beat_resolution, r1=1.0, r2=1.0,\n                   r3=0.5):\n    \"\"\"Return the tonal distance [1] between the two input pianorolls.\n\n    [1] Christopher Harte, Mark Sandler, and Martin Gasser. Detecting\n        harmonic change in musical audio. In Proc. ACM Workshop on Audio and\n        Music Computing Multimedia, 2006.\n\n    \"\"\"\n    _validate_pianoroll(pianoroll_1)\n    _validate_pianoroll(pianoroll_2)\n    assert len(pianoroll_1) == len(pianoroll_2), (\n        \"Input pianorolls must have the same length.\")\n\n    def _tonal_matrix(r1, r2, r3):\n        \"\"\"Return a tonal matrix for computing the tonal distance.\"\"\"\n        tonal_matrix = np.empty((6, 12))\n        tonal_matrix[0] = r1 * np.sin(np.arange(12) * (7. / 6.) * np.pi)\n        tonal_matrix[1] = r1 * np.cos(np.arange(12) * (7. / 6.) * np.pi)\n        tonal_matrix[2] = r2 * np.sin(np.arange(12) * (3. / 2.) * np.pi)\n        tonal_matrix[3] = r2 * np.cos(np.arange(12) * (3. / 2.) * np.pi)\n        tonal_matrix[4] = r3 * np.sin(np.arange(12) * (2. / 3.) * np.pi)\n        tonal_matrix[5] = r3 * np.cos(np.arange(12) * (2. / 3.) * np.pi)\n        return tonal_matrix\n\n    def _to_tonal_space(pianoroll, tonal_matrix):\n        \"\"\"Return the tensor in tonal space where chroma features are normalized\n        per beat.\"\"\"\n        beat_chroma = _to_chroma(pianoroll).reshape(-1, beat_resolution, 12)\n        beat_chroma = beat_chroma / np.sum(beat_chroma, 2, keepdims=True)\n        return np.matmul(tonal_matrix, beat_chroma.T).T\n\n    tonal_matrix = _tonal_matrix(r1, r2, r3)\n    mapped_1 = _to_tonal_space(pianoroll_1, tonal_matrix)\n    mapped_2 = _to_tonal_space(pianoroll_2, tonal_matrix)\n    return np.linalg.norm(mapped_1 - mapped_2)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign a constant value to all nonzeros in the pianoroll.", "response": "def assign_constant(self, value, dtype=None):\n        \"\"\"\n        Assign a constant value to all nonzeros in the pianoroll. If the\n        pianoroll is not binarized, its data type will be preserved. If the\n        pianoroll is binarized, it will be casted to the type of `value`.\n\n        Arguments\n        ---------\n        value : int or float\n            The constant value to be assigned to all the nonzeros in the\n            pianoroll.\n\n        \"\"\"\n        if not self.is_binarized():\n            self.pianoroll[self.pianoroll.nonzero()] = value\n            return\n        if dtype is None:\n            if isinstance(value, int):\n                dtype = int\n            elif isinstance(value, float):\n                dtype = float\n        nonzero = self.pianoroll.nonzero()\n        self.pianoroll = np.zeros(self.pianoroll.shape, dtype)\n        self.pianoroll[nonzero] = value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nraise error if any invalid attribute found.\"\"\"", "response": "def check_validity(self):\n        \"\"\"\"Raise error if any invalid attribute found.\"\"\"\n        # pianoroll\n        if not isinstance(self.pianoroll, np.ndarray):\n            raise TypeError(\"`pianoroll` must be a numpy array.\")\n        if not (np.issubdtype(self.pianoroll.dtype, np.bool_)\n                or np.issubdtype(self.pianoroll.dtype, np.number)):\n            raise TypeError(\"The data type of `pianoroll` must be np.bool_ or \"\n                            \"a subdtype of np.number.\")\n        if self.pianoroll.ndim != 2:\n            raise ValueError(\"`pianoroll` must have exactly two dimensions.\")\n        if self.pianoroll.shape[1] != 128:\n            raise ValueError(\"The length of the second axis of `pianoroll` \"\n                             \"must be 128.\")\n        # program\n        if not isinstance(self.program, int):\n            raise TypeError(\"`program` must be int.\")\n        if self.program < 0 or self.program > 127:\n            raise ValueError(\"`program` must be in between 0 to 127.\")\n        # is_drum\n        if not isinstance(self.is_drum, bool):\n            raise TypeError(\"`is_drum` must be bool.\")\n        # name\n        if not isinstance(self.name, string_types):\n            raise TypeError(\"`name` must be a string.\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clip(self, lower=0, upper=127):\n        self.pianoroll = self.pianoroll.clip(lower, upper)", "response": "Clip the pianoroll by the given lower and upper bounds."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_active_length(self):\n        nonzero_steps = np.any(self.pianoroll, axis=1)\n        inv_last_nonzero_step = np.argmax(np.flip(nonzero_steps, axis=0))\n        active_length = self.pianoroll.shape[0] - inv_last_nonzero_step\n        return active_length", "response": "Returns the active length of the current pianoroll. The unit is time step."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_active_pitch_range(self):\n        if self.pianoroll.shape[1] < 1:\n            raise ValueError(\"Cannot compute the active pitch range for an \"\n                             \"empty pianoroll\")\n        lowest = 0\n        highest = 127\n        while lowest < highest:\n            if np.any(self.pianoroll[:, lowest]):\n                break\n            lowest += 1\n        if lowest == highest:\n            raise ValueError(\"Cannot compute the active pitch range for an \"\n                             \"empty pianoroll\")\n        while not np.any(self.pianoroll[:, highest]):\n            highest -= 1\n\n        return lowest, highest", "response": "Returns the active pitch range as a tuple ( lowest highest."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the pianoroll is already binarized False otherwise.", "response": "def is_binarized(self):\n        \"\"\"\n        Return True if the pianoroll is already binarized. Otherwise, return\n        False.\n\n        Returns\n        -------\n        is_binarized : bool\n            True if the pianoroll is already binarized; otherwise, False.\n\n        \"\"\"\n        is_binarized = np.issubdtype(self.pianoroll.dtype, np.bool_)\n        return is_binarized"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npadding the pianoroll with zeros at the end along the time axis.", "response": "def pad(self, pad_length):\n        \"\"\"\n        Pad the pianoroll with zeros at the end along the time axis.\n\n        Parameters\n        ----------\n        pad_length : int\n            The length to pad with zeros along the time axis.\n\n        \"\"\"\n        self.pianoroll = np.pad(\n            self.pianoroll, ((0, pad_length), (0, 0)), 'constant')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pad_to_multiple(self, factor):\n        remainder = self.pianoroll.shape[0] % factor\n        if remainder:\n            pad_width = ((0, (factor - remainder)), (0, 0))\n            self.pianoroll = np.pad(self.pianoroll, pad_width, 'constant')", "response": "Pad the pianoroll with zeros at the end along the time axis with zeros at the end along the time axis with the minimum length that makes the resulting pianoroll length a multiple of factor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transpose(self, semitone):\n        if semitone > 0 and semitone < 128:\n            self.pianoroll[:, semitone:] = self.pianoroll[:, :(128 - semitone)]\n            self.pianoroll[:, :semitone] = 0\n        elif semitone < 0 and semitone > -128:\n            self.pianoroll[:, :(128 + semitone)] = self.pianoroll[:, -semitone:]\n            self.pianoroll[:, (128 + semitone):] = 0", "response": "Transpose the pianoroll by a number of semitones where positive values are for higher key while negative values are for lower key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef trim_trailing_silence(self):\n        length = self.get_active_length()\n        self.pianoroll = self.pianoroll[:length]", "response": "Trim the trailing silence of the pianoroll."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_conv_weights(layer, figsize=(6, 6)):\n    W = layer.W.get_value()\n    shape = W.shape\n    nrows = np.ceil(np.sqrt(shape[0])).astype(int)\n    ncols = nrows\n\n    for feature_map in range(shape[1]):\n        figs, axes = plt.subplots(nrows, ncols, figsize=figsize, squeeze=False)\n\n        for ax in axes.flatten():\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.axis('off')\n\n        for i, (r, c) in enumerate(product(range(nrows), range(ncols))):\n            if i >= shape[0]:\n                break\n            axes[r, c].imshow(W[i, feature_map], cmap='gray',\n                              interpolation='none')\n    return plt", "response": "Plots the weights of a specific convolutional layer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot the acitivities of a specific layer.", "response": "def plot_conv_activity(layer, x, figsize=(6, 8)):\n    \"\"\"Plot the acitivities of a specific layer.\n\n    Only really makes sense with layers that work 2D data (2D\n    convolutional layers, 2D pooling layers ...).\n\n    Parameters\n    ----------\n    layer : lasagne.layers.Layer\n\n    x : numpy.ndarray\n      Only takes one sample at a time, i.e. x.shape[0] == 1.\n\n    \"\"\"\n    if x.shape[0] != 1:\n        raise ValueError(\"Only one sample can be plotted at a time.\")\n\n    # compile theano function\n    xs = T.tensor4('xs').astype(theano.config.floatX)\n    get_activity = theano.function([xs], get_output(layer, xs))\n\n    activity = get_activity(x)\n    shape = activity.shape\n    nrows = np.ceil(np.sqrt(shape[1])).astype(int)\n    ncols = nrows\n\n    figs, axes = plt.subplots(nrows + 1, ncols, figsize=figsize, squeeze=False)\n    axes[0, ncols // 2].imshow(1 - x[0][0], cmap='gray',\n                               interpolation='none')\n    axes[0, ncols // 2].set_title('original')\n\n    for ax in axes.flatten():\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.axis('off')\n\n    for i, (r, c) in enumerate(product(range(nrows), range(ncols))):\n        if i >= shape[1]:\n            break\n        ndim = activity[0][i].ndim\n        if ndim != 2:\n            raise ValueError(\"Wrong number of dimensions, image data should \"\n                             \"have 2, instead got {}\".format(ndim))\n        axes[r + 1, c].imshow(-activity[0][i], cmap='gray',\n                              interpolation='none')\n    return plt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef occlusion_heatmap(net, x, target, square_length=7):\n    if (x.ndim != 4) or x.shape[0] != 1:\n        raise ValueError(\"This function requires the input data to be of \"\n                         \"shape (1, c, x, y), instead got {}\".format(x.shape))\n    if square_length % 2 == 0:\n        raise ValueError(\"Square length has to be an odd number, instead \"\n                         \"got {}.\".format(square_length))\n\n    num_classes = get_output_shape(net.layers_[-1])[1]\n    img = x[0].copy()\n    bs, col, s0, s1 = x.shape\n\n    heat_array = np.zeros((s0, s1))\n    pad = square_length // 2 + 1\n    x_occluded = np.zeros((s1, col, s0, s1), dtype=img.dtype)\n    probs = np.zeros((s0, s1, num_classes))\n\n    # generate occluded images\n    for i in range(s0):\n        # batch s1 occluded images for faster prediction\n        for j in range(s1):\n            x_pad = np.pad(img, ((0, 0), (pad, pad), (pad, pad)), 'constant')\n            x_pad[:, i:i + square_length, j:j + square_length] = 0.\n            x_occluded[j] = x_pad[:, pad:-pad, pad:-pad]\n        y_proba = net.predict_proba(x_occluded)\n        probs[i] = y_proba.reshape(s1, num_classes)\n\n    # from predicted probabilities, pick only those of target class\n    for i in range(s0):\n        for j in range(s1):\n            heat_array[i, j] = probs[i, j, target]\n    return heat_array", "response": "This function tests an image for critical parts of a neural network and returns a heatmap of the correct class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot the occlusion heatmap for a neural net.", "response": "def plot_occlusion(net, X, target, square_length=7, figsize=(9, None)):\n    \"\"\"Plot which parts of an image are particularly import for the\n    net to classify the image correctly.\n\n    See paper: Zeiler, Fergus 2013\n\n    Parameters\n    ----------\n    net : NeuralNet instance\n      The neural net to test.\n\n    X : numpy.array\n      The input data, should be of shape (b, c, 0, 1). Only makes\n      sense with image data.\n\n    target : list or numpy.array of ints\n      The true values of the image. If the net makes several\n      predictions, say 10 classes, this indicates which one to look\n      at. If more than one sample is passed to X, each of them needs\n      its own target.\n\n    square_length : int (default=7)\n      The length of the side of the square that occludes the image.\n      Must be an odd number.\n\n    figsize : tuple (int, int)\n      Size of the figure.\n\n    Plots\n    -----\n    Figure with 3 subplots: the original image, the occlusion heatmap,\n    and both images super-imposed.\n\n    \"\"\"\n    return _plot_heat_map(\n        net, X, figsize, lambda net, X, n: occlusion_heatmap(\n            net, X, target[n], square_length))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the hex color for a given layer type.", "response": "def get_hex_color(layer_type):\n    \"\"\"\n    Determines the hex color for a layer.\n    :parameters:\n        - layer_type : string\n            Class name of the layer\n    :returns:\n        - color : string containing a hex color for filling block.\n    \"\"\"\n    COLORS = ['#4A88B3', '#98C1DE', '#6CA2C8', '#3173A2', '#17649B',\n              '#FFBB60', '#FFDAA9', '#FFC981', '#FCAC41', '#F29416',\n              '#C54AAA', '#E698D4', '#D56CBE', '#B72F99', '#B0108D',\n              '#75DF54', '#B3F1A0', '#91E875', '#5DD637', '#3FCD12']\n\n    hashed = int(hash(layer_type)) % 5\n\n    if \"conv\" in layer_type.lower():\n        return COLORS[:5][hashed]\n    if layer_type in lasagne.layers.pool.__all__:\n        return COLORS[5:10][hashed]\n    if layer_type in lasagne.layers.recurrent.__all__:\n        return COLORS[10:15][hashed]\n    else:\n        return COLORS[15:20][hashed]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a pydot graph containing the layers in the list of layers.", "response": "def make_pydot_graph(layers, output_shape=True, verbose=False):\n    \"\"\"\n    :parameters:\n        - layers : list\n            List of the layers, as obtained from lasagne.layers.get_all_layers\n        - output_shape: (default `True`)\n            If `True`, the output shape of each layer will be displayed.\n        - verbose: (default `False`)\n            If `True`, layer attributes like filter shape, stride, etc.\n            will be displayed.\n    :returns:\n        - pydot_graph : PyDot object containing the graph\n    \"\"\"\n    import pydotplus as pydot\n    pydot_graph = pydot.Dot('Network', graph_type='digraph')\n    pydot_nodes = {}\n    pydot_edges = []\n    for i, layer in enumerate(layers):\n        layer_name = getattr(layer, 'name', None)\n        if layer_name is None:\n            layer_name = layer.__class__.__name__\n        layer_type = '{0}'.format(layer_name)\n        key = repr(layer)\n        label = layer_type\n        color = get_hex_color(layer_type)\n        if verbose:\n            for attr in ['num_filters', 'num_units', 'ds',\n                         'filter_shape', 'stride', 'strides', 'p']:\n                if hasattr(layer, attr):\n                    label += '\\n{0}: {1}'.format(attr, getattr(layer, attr))\n            if hasattr(layer, 'nonlinearity'):\n                try:\n                    nonlinearity = layer.nonlinearity.__name__\n                except AttributeError:\n                    nonlinearity = layer.nonlinearity.__class__.__name__\n                label += '\\nnonlinearity: {0}'.format(nonlinearity)\n\n        if output_shape:\n            label += '\\nOutput shape: {0}'.format(layer.output_shape)\n\n        pydot_nodes[key] = pydot.Node(\n            key, label=label, shape='record', fillcolor=color, style='filled')\n\n        if hasattr(layer, 'input_layers'):\n            for input_layer in layer.input_layers:\n                pydot_edges.append([repr(input_layer), key])\n\n        if hasattr(layer, 'input_layer'):\n            pydot_edges.append([repr(layer.input_layer), key])\n\n    for node in pydot_nodes.values():\n        pydot_graph.add_node(node)\n\n    for edges in pydot_edges:\n        pydot_graph.add_edge(\n            pydot.Edge(pydot_nodes[edges[0]], pydot_nodes[edges[1]]))\n    return pydot_graph"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw_to_file(layers, filename, **kwargs):\n    layers = (layers.get_all_layers() if hasattr(layers, 'get_all_layers')\n              else layers)\n    dot = make_pydot_graph(layers, **kwargs)\n    ext = filename[filename.rfind('.') + 1:]\n    with io.open(filename, 'wb') as fid:\n        fid.write(dot.create(format=ext))", "response": "Draws a network diagram to a file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef draw_to_notebook(layers, **kwargs):\n    from IPython.display import Image\n    layers = (layers.get_all_layers() if hasattr(layers, 'get_all_layers')\n              else layers)\n    dot = make_pydot_graph(layers, **kwargs)\n    return Image(dot.create_png())", "response": "Draws a network diagram in an IPython notebook"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_real_filter(layers, img_size):\n    real_filter = np.zeros((len(layers), 2))\n    conv_mode = True\n    first_conv_layer = True\n    expon = np.ones((1, 2))\n\n    for i, layer in enumerate(layers[1:]):\n        j = i + 1\n        if not conv_mode:\n            real_filter[j] = img_size\n            continue\n\n        if is_conv2d(layer):\n            if not first_conv_layer:\n                new_filter = np.array(layer.filter_size) * expon\n                real_filter[j] = new_filter\n            else:\n                new_filter = np.array(layer.filter_size) * expon\n                real_filter[j] = new_filter\n                first_conv_layer = False\n        elif is_maxpool2d(layer):\n            real_filter[j] = real_filter[i]\n            expon *= np.array(layer.pool_size)\n        else:\n            conv_mode = False\n            real_filter[j] = img_size\n\n    real_filter[0] = img_size\n    return real_filter", "response": "Get the real filter sizes of each layer involved in\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the real filter sizes of each layer involved in .", "response": "def get_receptive_field(layers, img_size):\n    \"\"\"Get the real filter sizes of each layer involved in\n    convoluation. See Xudong Cao:\n    https://www.kaggle.com/c/datasciencebowl/forums/t/13166/happy-lantern-festival-report-and-code\n    This does not yet take into consideration feature pooling,\n    padding, striding and similar gimmicks.\n    \"\"\"\n    receptive_field = np.zeros((len(layers), 2))\n    conv_mode = True\n    first_conv_layer = True\n    expon = np.ones((1, 2))\n\n    for i, layer in enumerate(layers[1:]):\n        j = i + 1\n        if not conv_mode:\n            receptive_field[j] = img_size\n            continue\n\n        if is_conv2d(layer):\n            if not first_conv_layer:\n                last_field = receptive_field[i]\n                new_field = (last_field + expon *\n                             (np.array(layer.filter_size) - 1))\n                receptive_field[j] = new_field\n            else:\n                receptive_field[j] = layer.filter_size\n                first_conv_layer = False\n        elif is_maxpool2d(layer):\n            receptive_field[j] = receptive_field[i]\n            expon *= np.array(layer.pool_size)\n        else:\n            conv_mode = False\n            receptive_field[j] = img_size\n\n    receptive_field[0] = img_size\n    return receptive_field"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn image as expected by classify_direct = True.", "response": "def prepare_image(self, image):\n        \"\"\"Returns image of shape `(256, 256, 3)`, as expected by\n        `transform` when `classify_direct = True`.\n        \"\"\"\n        from decaf.util import transform  # soft dep\n        _JEFFNET_FLIP = True\n\n        # first, extract the 256x256 center.\n        image = transform.scale_and_extract(transform.as_rgb(image), 256)\n        # convert to [0,255] float32\n        image = image.astype(np.float32) * 255.\n        if _JEFFNET_FLIP:\n            # Flip the image if necessary, maintaining the c_contiguous order\n            image = image[::-1, :].copy()\n        # subtract the mean\n        image -= self.net_._data_mean\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndefaults implementation of the NeuralNet objective.", "response": "def objective(layers,\n              loss_function,\n              target,\n              aggregate=aggregate,\n              deterministic=False,\n              l1=0,\n              l2=0,\n              get_output_kw=None):\n    \"\"\"\n    Default implementation of the NeuralNet objective.\n\n    :param layers: The underlying layers of the NeuralNetwork\n    :param loss_function: The callable loss function to use\n    :param target: the expected output\n\n    :param aggregate: the aggregation function to use\n    :param deterministic: Whether or not to get a deterministic output\n    :param l1: Optional l1 regularization parameter\n    :param l2: Optional l2 regularization parameter\n    :param get_output_kw: optional kwargs to pass to\n                          :meth:`NeuralNetwork.get_output`\n    :return: The total calculated loss\n    \"\"\"\n    if get_output_kw is None:\n        get_output_kw = {}\n    output_layer = layers[-1]\n    network_output = get_output(\n        output_layer, deterministic=deterministic, **get_output_kw)\n    loss = aggregate(loss_function(network_output, target))\n\n    if l1:\n        loss += regularization.regularize_layer_params(\n            layers.values(), regularization.l1) * l1\n    if l2:\n        loss += regularization.regularize_layer_params(\n            layers.values(), regularization.l2) * l2\n    return loss"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize(self):\n        if getattr(self, '_initialized', False):\n            return\n\n        out = getattr(self, '_output_layers', None)\n        if out is None:\n            self.initialize_layers()\n        self._check_for_unused_kwargs()\n\n        iter_funcs = self._create_iter_funcs(\n            self.layers_, self.objective, self.update,\n            self.y_tensor_type,\n            )\n        self.train_iter_, self.eval_iter_, self.predict_iter_ = iter_funcs\n        self._initialized = True", "response": "Initializes the network.  Checks that no extra kwargs were\n        passed to the constructor, and compiles the train, predict,\n        and evaluation functions.\n\n        Subsequent calls to this function will return without any action."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the Lasagne layer objects.", "response": "def initialize_layers(self, layers=None):\n        \"\"\"Sets up the Lasagne layers\n\n        :param layers: The dictionary of layers, or a\n        :class:`lasagne.Layers` instance, describing the underlying\n        network\n\n        :return: the output layer of the underlying lasagne network.\n\n        :seealso: :ref:`layer-def`\n        \"\"\"\n        if layers is not None:\n            self.layers = layers\n        self.layers_ = Layers()\n\n        #If a Layer, or a list of Layers was passed in\n        if isinstance(self.layers[0], Layer):\n            for out_layer in self.layers:\n                for i, layer in enumerate(get_all_layers(out_layer)):\n                    if layer not in self.layers_.values():\n                        name = layer.name or self._layer_name(layer.__class__, i)\n                        self.layers_[name] = layer\n                        if self._get_params_for(name) != {}:\n                            raise ValueError(\n                                \"You can't use keyword params when passing a Lasagne \"\n                                \"instance object as the 'layers' parameter of \"\n                                \"'NeuralNet'.\"\n                                )\n            self._output_layers = self.layers\n            return self.layers\n\n        # 'self.layers' are a list of '(Layer class, kwargs)', so\n        # we'll have to actually instantiate the layers given the\n        # arguments:\n        layer = None\n        for i, layer_def in enumerate(self.layers):\n\n            if isinstance(layer_def[1], dict):\n                # Newer format: (Layer, {'layer': 'kwargs'})\n                layer_factory, layer_kw = layer_def\n                layer_kw = layer_kw.copy()\n            else:\n                # The legacy format: ('name', Layer)\n                layer_name, layer_factory = layer_def\n                layer_kw = {'name': layer_name}\n\n            if isinstance(layer_factory, str):\n                layer_factory = locate(layer_factory)\n                assert layer_factory is not None\n\n            if 'name' not in layer_kw:\n                layer_kw['name'] = self._layer_name(layer_factory, i)\n\n            more_params = self._get_params_for(layer_kw['name'])\n            layer_kw.update(more_params)\n\n            if layer_kw['name'] in self.layers_:\n                raise ValueError(\n                    \"Two layers with name {}.\".format(layer_kw['name']))\n\n            # Any layers that aren't subclasses of InputLayer are\n            # assumed to require an 'incoming' paramter.  By default,\n            # we'll use the previous layer as input:\n            try:\n                is_input_layer = issubclass(layer_factory, InputLayer)\n            except TypeError:\n                is_input_layer = False\n            if not is_input_layer:\n                if 'incoming' in layer_kw:\n                    layer_kw['incoming'] = self.layers_[\n                        layer_kw['incoming']]\n                elif 'incomings' in layer_kw:\n                    layer_kw['incomings'] = [\n                        self.layers_[name] for name in layer_kw['incomings']]\n                else:\n                    layer_kw['incoming'] = layer\n\n            # Deal with additional string parameters that may\n            # reference other layers; currently only 'mask_input'.\n            for param in self.layer_reference_params:\n                if param in layer_kw:\n                    val = layer_kw[param]\n                    if isinstance(val, basestring):\n                        layer_kw[param] = self.layers_[val]\n\n            for attr in ('W', 'b'):\n                if isinstance(layer_kw.get(attr), str):\n                    name = layer_kw[attr]\n                    layer_kw[attr] = getattr(self.layers_[name], attr, None)\n\n            try:\n                layer_wrapper = layer_kw.pop('layer_wrapper', None)\n                layer = layer_factory(**layer_kw)\n            except TypeError as e:\n                msg = (\"Failed to instantiate {} with args {}.\\n\"\n                       \"Maybe parameter names have changed?\".format(\n                           layer_factory, layer_kw))\n                chain_exception(TypeError(msg), e)\n            self.layers_[layer_kw['name']] = layer\n            if layer_wrapper is not None:\n                layer = layer_wrapper(layer)\n                self.layers_[\"LW_%s\" % layer_kw['name']] = layer\n\n        self._output_layers = [layer]\n        return [layer]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the training loop for a given number of epochs.", "response": "def fit(self, X, y, epochs=None):\n        \"\"\"\n        Runs the training loop for a given number of epochs\n\n        :param X:  The input data\n        :param y:  The ground truth\n        :param epochs: The number of epochs to run, if `None` runs for the\n                       network's :attr:`max_epochs`\n        :return: This instance\n        \"\"\"\n        if self.check_input:\n            X, y = self._check_good_input(X, y)\n\n        if self.use_label_encoder:\n            self.enc_ = LabelEncoder()\n            y = self.enc_.fit_transform(y).astype(np.int32)\n            self.classes_ = self.enc_.classes_\n        self.initialize()\n\n        try:\n            self.train_loop(X, y, epochs=epochs)\n        except KeyboardInterrupt:\n            pass\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef partial_fit(self, X, y, classes=None):\n        return self.fit(X, y, epochs=1)", "response": "Runs a single epoch using the provided data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches an instance lazily.", "response": "def _register(self, defaults=None, **kwargs):\n        \"\"\"Fetch (update or create)  an instance, lazily.\n\n        We're doing this lazily, so that it becomes possible to define\n        custom enums in your code, even before the Django ORM is fully\n        initialized.\n\n        Domain.objects.SHOPPING = Domain.objects.register(\n            ref='shopping',\n            name='Webshop')\n        Domain.objects.USERS = Domain.objects.register(\n            ref='users',\n            name='User Accounts')\n        \"\"\"\n        f = lambda: self.update_or_create(defaults=defaults, **kwargs)[0]\n        ret = SimpleLazyObject(f)\n        self._lazy_entries.append(ret)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupping - to including", "response": "def narrow(self, **kwargs):\n        \"\"\"Up-to including\"\"\"\n        from_date = kwargs.pop('from_date', None)\n        to_date = kwargs.pop('to_date', None)\n        date = kwargs.pop('date', None)\n        qs = self\n        if from_date:\n            qs = qs.filter(date__gte=from_date)\n        if to_date:\n            qs = qs.filter(date__lte=to_date)\n        if date:\n            qs = qs.filter(date=date)\n        return super(ByDateQuerySetMixin, qs).narrow(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_environment_variables(json_file_path):\n    if json_file_path:\n        with open(json_file_path) as json_file:\n            env_vars = json.loads(json_file.read())\n\n            export_variables(env_vars)", "response": "Read and set environment variables from a flat json file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the millis interval between start and end", "response": "def millis_interval(start, end):\n    \"\"\"start and end are datetime instances\"\"\"\n    diff = end - start\n    millis = diff.days * 24 * 60 * 60 * 1000\n    millis += diff.seconds * 1000\n    millis += diff.microseconds / 1000\n    return millis"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes the lua script and returns the result", "response": "def _execute_lua(self, keys, args, client):\n        \"\"\"\n        Sets KEYS and ARGV alongwith redis.call() function in lua globals\n        and executes the lua redis script\n        \"\"\"\n        lua, lua_globals = Script._import_lua(self.load_dependencies)\n        lua_globals.KEYS = self._python_to_lua(keys)\n        lua_globals.ARGV = self._python_to_lua(args)\n\n        def _call(*call_args):\n            # redis-py and native redis commands are mostly compatible argument\n            # wise, but some exceptions need to be handled here:\n            if str(call_args[0]).lower() == 'lrem':\n                response = client.call(\n                    call_args[0], call_args[1],\n                    call_args[3],  # \"count\", default is 0\n                    call_args[2])\n            else:\n                response = client.call(*call_args)\n            return self._python_to_lua(response)\n\n        lua_globals.redis = {\"call\": _call}\n        return self._lua_to_python(lua.execute(self.script), return_status=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimports lua and dependencies.", "response": "def _import_lua(load_dependencies=True):\n        \"\"\"\n        Import lua and dependencies.\n\n        :param load_dependencies: should Lua library dependencies be loaded?\n        :raises: RuntimeError if Lua is not available\n        \"\"\"\n        try:\n            import lua\n        except ImportError:\n            raise RuntimeError(\"Lua not installed\")\n\n        lua_globals = lua.globals()\n        if load_dependencies:\n            Script._import_lua_dependencies(lua, lua_globals)\n        return lua, lua_globals"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimport lua dependencies that are supported by redis.", "response": "def _import_lua_dependencies(lua, lua_globals):\n        \"\"\"\n        Imports lua dependencies that are supported by redis lua scripts.\n\n        The current implementation is fragile to the target platform and lua version\n        and may be disabled if these imports are not needed.\n\n        Included:\n            - cjson lib.\n        Pending:\n            - base lib.\n            - table lib.\n            - string lib.\n            - math lib.\n            - debug lib.\n            - cmsgpack lib.\n        \"\"\"\n        if sys.platform not in ('darwin', 'windows'):\n            import ctypes\n            ctypes.CDLL('liblua5.2.so', mode=ctypes.RTLD_GLOBAL)\n\n        try:\n            lua_globals.cjson = lua.eval('require \"cjson\"')\n        except RuntimeError:\n            raise RuntimeError(\"cjson not installed\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _lua_to_python(lval, return_status=False):\n        import lua\n        lua_globals = lua.globals()\n        if lval is None:\n            # Lua None --> Python None\n            return None\n        if lua_globals.type(lval) == \"table\":\n            # Lua table --> Python list\n            pval = []\n            for i in lval:\n                if return_status:\n                    if i == 'ok':\n                        return lval[i]\n                    if i == 'err':\n                        raise ResponseError(lval[i])\n                pval.append(Script._lua_to_python(lval[i]))\n            return pval\n        elif isinstance(lval, long):\n            # Lua number --> Python long\n            return long(lval)\n        elif isinstance(lval, float):\n            # Lua number --> Python float\n            return float(lval)\n        elif lua_globals.type(lval) == \"userdata\":\n            # Lua userdata --> Python string\n            return str(lval)\n        elif lua_globals.type(lval) == \"string\":\n            # Lua string --> Python string\n            return lval\n        elif lua_globals.type(lval) == \"boolean\":\n            # Lua boolean --> Python bool\n            return bool(lval)\n        raise RuntimeError(\"Invalid Lua type: \" + str(lua_globals.type(lval)))", "response": "Convert Lua object into Python object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert Python object into Lua object.", "response": "def _python_to_lua(pval):\n        \"\"\"\n        Convert Python object(s) into Lua object(s), as at times Python object(s)\n        are not compatible with Lua functions\n        \"\"\"\n        import lua\n        if pval is None:\n            # Python None --> Lua None\n            return lua.eval(\"\")\n        if isinstance(pval, (list, tuple, set)):\n            # Python list --> Lua table\n            # e.g.: in lrange\n            #     in Python returns: [v1, v2, v3]\n            #     in Lua returns: {v1, v2, v3}\n            lua_list = lua.eval(\"{}\")\n            lua_table = lua.eval(\"table\")\n            for item in pval:\n                lua_table.insert(lua_list, Script._python_to_lua(item))\n            return lua_list\n        elif isinstance(pval, dict):\n            # Python dict --> Lua dict\n            # e.g.: in hgetall\n            #     in Python returns: {k1:v1, k2:v2, k3:v3}\n            #     in Lua returns: {k1, v1, k2, v2, k3, v3}\n            lua_dict = lua.eval(\"{}\")\n            lua_table = lua.eval(\"table\")\n            for k, v in pval.iteritems():\n                lua_table.insert(lua_dict, Script._python_to_lua(k))\n                lua_table.insert(lua_dict, Script._python_to_lua(v))\n            return lua_dict\n        elif isinstance(pval, str):\n            # Python string --> Lua userdata\n            return pval\n        elif isinstance(pval, bool):\n            # Python bool--> Lua boolean\n            return lua.eval(str(pval).lower())\n        elif isinstance(pval, (int, long, float)):\n            # Python int --> Lua number\n            lua_globals = lua.globals()\n            return lua_globals.tonumber(str(pval))\n\n        raise RuntimeError(\"Invalid Python type: \" + str(type(pval)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nemulating ttl Even though the official redis commands documentation at http://www. redis. io does not provide a TTL in seconds. The redis - py lib does not return None for both these cases. The redis - py lib does not return None for both these cases.", "response": "def ttl(self, key):\n        \"\"\"\n        Emulate ttl\n\n        Even though the official redis commands documentation at http://redis.io/commands/ttl\n        states \"Return value: Integer reply: TTL in seconds, -2 when key does not exist or -1\n        when key does not have a timeout.\" the redis-py lib returns None for both these cases.\n        The lib behavior has been emulated here.\n\n        :param key: key for which ttl is requested.\n        :returns: the number of seconds till timeout, None if the key does not exist or if the\n                  key has no timeout(as per the redis-py lib behavior).\n        \"\"\"\n        value = self.pttl(key)\n        if value is None or value < 0:\n            return value\n        return value // 1000"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pttl(self, key):\n        \"\"\"\n        Returns time to live in milliseconds if output_ms is True, else returns seconds.\n        \"\"\"\n        key = self._encode(key)\n        if key not in self.redis:\n            # as of redis 2.8, -2 is returned if the key does not exist\n            return long(-2) if self.strict else None\n        if key not in self.timeouts:\n            # as of redis 2.8, -1 is returned if the key is persistent\n            # redis-py returns None; command docs say -1\n            return long(-1) if self.strict else None\n\n        time_to_live = get_total_milliseconds(self.timeouts[key] - self.clock.now())\n        return long(max(-1, time_to_live))", "response": "Emulate pttl\n\n        :param key: key for which pttl is requested.\n        :returns: the number of milliseconds till timeout, None if the key does not exist or if the\n                  key has no timeout(as per the redis-py lib behavior)."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_expire(self):\n        # Deep copy to avoid RuntimeError: dictionary changed size during iteration\n        _timeouts = deepcopy(self.timeouts)\n        for key, value in _timeouts.items():\n            if value - self.clock.now() < timedelta(0):\n                del self.timeouts[key]\n                # removing the expired key\n                if key in self.redis:\n                    self.redis.pop(key, None)", "response": "Expire all expired entries in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the value for the key in the context of the provided kwargs.", "response": "def set(self, key, value, ex=None, px=None, nx=False, xx=False):\n        \"\"\"\n        Set the ``value`` for the ``key`` in the context of the provided kwargs.\n\n        As per the behavior of the redis-py lib:\n        If nx and xx are both set, the function does nothing and None is returned.\n        If px and ex are both set, the preference is given to px.\n        If the key is not set for some reason, the lib function returns None.\n        \"\"\"\n        key = self._encode(key)\n        value = self._encode(value)\n\n        if nx and xx:\n            return None\n        mode = \"nx\" if nx else \"xx\" if xx else None\n        if self._should_set(key, mode):\n            expire = None\n            if ex is not None:\n                expire = ex if isinstance(ex, timedelta) else timedelta(seconds=ex)\n            if px is not None:\n                expire = px if isinstance(px, timedelta) else timedelta(milliseconds=px)\n\n            if expire is not None and expire.total_seconds() <= 0:\n                raise ResponseError(\"invalid expire time in SETEX\")\n\n            result = self._set(key, value)\n            if expire:\n                self._expire(key, expire)\n\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _should_set(self, key, mode):\n\n        if mode is None or mode not in [\"nx\", \"xx\"]:\n            return True\n\n        if mode == \"nx\":\n            if key in self.redis:\n                # nx means set only if key is absent\n                # false if the key already exists\n                return False\n        elif key not in self.redis:\n            # at this point mode can only be xx\n            # xx means set only if the key already exists\n            # false if is absent\n            return False\n        # for all other cases, return true\n        return True", "response": "Determine if a key should be set based on the mode."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the value of name to value that expires in time.", "response": "def setex(self, name, time, value):\n        \"\"\"\n        Set the value of ``name`` to ``value`` that expires in ``time``\n        seconds. ``time`` can be represented by an integer or a Python\n        timedelta object.\n        \"\"\"\n        if not self.strict:\n            # when not strict mode swap value and time args order\n            time, value = value, time\n        return self.set(name, value, ex=time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the value of key to value that expires in time milliseconds.", "response": "def psetex(self, key, time, value):\n        \"\"\"\n        Set the value of ``key`` to ``value`` that expires in ``time``\n        milliseconds. ``time`` can be represented by an integer or a Python\n        timedelta object.\n        \"\"\"\n        return self.set(key, value, px=time)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the value of key to value only if key does not exist", "response": "def setnx(self, key, value):\n        \"\"\"Set the value of ``key`` to ``value`` if key doesn't exist\"\"\"\n        return self.set(key, value, nx=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the value of the key - value pairs for the current locale.", "response": "def mset(self, *args, **kwargs):\n        \"\"\"\n        Sets key/values based on a mapping. Mapping can be supplied as a single\n        dictionary argument or as kwargs.\n        \"\"\"\n        mapping = kwargs\n        if args:\n            if len(args) != 1 or not isinstance(args[0], dict):\n                raise RedisError('MSET requires **kwargs or a single dict arg')\n            mapping.update(args[0])\n\n        if len(mapping) == 0:\n            raise ResponseError(\"wrong number of arguments for 'mset' command\")\n\n        for key, value in mapping.items():\n            self.set(key, value)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef msetnx(self, *args, **kwargs):\n        if args:\n            if len(args) != 1 or not isinstance(args[0], dict):\n                raise RedisError('MSETNX requires **kwargs or a single dict arg')\n            mapping = args[0]\n        else:\n            mapping = kwargs\n\n        if len(mapping) == 0:\n            raise ResponseError(\"wrong number of arguments for 'msetnx' command\")\n\n        for key in mapping.keys():\n            if self._encode(key) in self.redis:\n                return False\n        for key, value in mapping.items():\n            self.set(key, value)\n\n        return True", "response": "This command is used to set the keys and values based on a mapping."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setbit(self, key, offset, value):\n        key = self._encode(key)\n        index, bits, mask = self._get_bits_and_offset(key, offset)\n\n        if index >= len(bits):\n            bits.extend(b\"\\x00\" * (index + 1 - len(bits)))\n\n        prev_val = 1 if (bits[index] & mask) else 0\n\n        if value:\n            bits[index] |= mask\n        else:\n            bits[index] &= ~mask\n\n        self.redis[key] = bytes(bits)\n\n        return prev_val", "response": "Set the value of the bit at offset in key to value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getbit(self, key, offset):\n        key = self._encode(key)\n        index, bits, mask = self._get_bits_and_offset(key, offset)\n\n        if index >= len(bits):\n            return 0\n\n        return 1 if (bits[index] & mask) else 0", "response": "Returns the bit value at offset in key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsharing hincrby and hincrbyfloat routine", "response": "def _hincrby(self, hashkey, attribute, command, type_, increment):\n        \"\"\"Shared hincrby and hincrbyfloat routine\"\"\"\n        redis_hash = self._get_hash(hashkey, command, create=True)\n        attribute = self._encode(attribute)\n        previous_value = type_(redis_hash.get(attribute, '0'))\n        redis_hash[attribute] = self._encode(previous_value + increment)\n        return type_(redis_hash[attribute])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nemulate blocking pop functionality", "response": "def _blocking_pop(self, pop_func, keys, timeout):\n        \"\"\"Emulate blocking pop functionality\"\"\"\n        if not isinstance(timeout, (int, long)):\n            raise RuntimeError('timeout is not an integer or out of range')\n\n        if timeout is None or timeout == 0:\n            timeout = self.blocking_timeout\n\n        if isinstance(keys, basestring):\n            keys = [keys]\n        else:\n            keys = list(keys)\n\n        elapsed_time = 0\n        start = time.time()\n        while elapsed_time < timeout:\n            key, val = self._pop_first_available(pop_func, keys)\n            if val:\n                return key, val\n            # small delay to avoid high cpu utilization\n            time.sleep(self.blocking_sleep_interval)\n            elapsed_time = time.time() - start\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _common_scan(self, values_function, cursor='0', match=None, count=10, key=None):\n        if count is None:\n            count = 10\n        cursor = int(cursor)\n        count = int(count)\n        if not count:\n            raise ValueError('if specified, count must be > 0: %s' % count)\n\n        values = values_function()\n        if cursor + count >= len(values):\n            # we reached the end, back to zero\n            result_cursor = 0\n        else:\n            result_cursor = cursor + count\n\n        values = values[cursor:cursor+count]\n\n        if match is not None:\n            regex = re.compile(b'^' + re.escape(self._encode(match)).replace(b'\\\\*', b'.*') + b'$')\n            if not key:\n                key = lambda v: v\n            values = [v for v in values if regex.match(key(v))]\n\n        return [result_cursor, values]", "response": "Common scanning skeleton.\n\n        :param key: optional function used to identify what 'match' is applied to"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call(self, command, *args):\n        command = self._normalize_command_name(command)\n        args = self._normalize_command_args(command, *args)\n\n        redis_function = getattr(self, command)\n        value = redis_function(*args)\n        return self._normalize_command_response(command, value)", "response": "Sends a command to the function whose name is specified by command and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _normalize_command_args(self, command, *args):\n        if command == 'zadd' and not self.strict and len(args) >= 3:\n            # Reorder score and name\n            zadd_args = [x for tup in zip(args[2::2], args[1::2]) for x in tup]\n            return [args[0]] + zadd_args\n\n        if command in ('zrangebyscore', 'zrevrangebyscore'):\n            # expected format is: <command> name min max start num with_scores score_cast_func\n            if len(args) <= 3:\n                # just plain min/max\n                return args\n\n            start, num = None, None\n            withscores = False\n\n            for i, arg in enumerate(args[3:], 3):\n                # keywords are case-insensitive\n                lower_arg = self._encode(arg).lower()\n\n                # handle \"limit\"\n                if lower_arg == b\"limit\" and i + 2 < len(args):\n                    start, num = args[i + 1], args[i + 2]\n\n                # handle \"withscores\"\n                if lower_arg == b\"withscores\":\n                    withscores = True\n\n            # do not expect to set score_cast_func\n\n            return args[:3] + (start, num, withscores)\n\n        return args", "response": "Normalizes the command arguments to match the strictness of the redis client."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef config_get(self, pattern='*'):\n        result = {}\n        for name, value in self.redis_config.items():\n            if fnmatch.fnmatch(name, pattern):\n                try:\n                    result[name] = int(value)\n                except ValueError:\n                    result[name] = value\n        return result", "response": "Get one or more configuration parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list by name.", "response": "def _get_list(self, key, operation, create=False):\n        \"\"\"\n        Get (and maybe create) a list by name.\n        \"\"\"\n        return self._get_by_type(key, operation, create, b'list', [])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a set by name.", "response": "def _get_set(self, key, operation, create=False):\n        \"\"\"\n        Get (and maybe create) a set by name.\n        \"\"\"\n        return self._get_by_type(key, operation, create, b'set', set())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_hash(self, name, operation, create=False):\n        return self._get_by_type(name, operation, create, b'hash', {})", "response": "Get a hash by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_zset(self, name, operation, create=False):\n        return self._get_by_type(name, operation, create, b'zset', SortedSet(), return_default=False)", "response": "Get a sorted set by name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a redis data structure by name and type.", "response": "def _get_by_type(self, key, operation, create, type_, default, return_default=True):\n        \"\"\"\n        Get (and maybe create) a redis data structure by name and type.\n        \"\"\"\n        key = self._encode(key)\n        if self.type(key) in [type_, b'none']:\n            if create:\n                return self.redis.setdefault(key, default)\n            else:\n                return self.redis.get(key, default if return_default else None)\n\n        raise TypeError(\"{} requires a {}\".format(operation, type_))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _translate_range(self, len_, start, end):\n        if start < 0:\n            start += len_\n        start = max(0, min(start, len_))\n        if end < 0:\n            end += len_\n        end = max(-1, min(end, len_ - 1))\n        return start, end", "response": "Translate range to valid bounds."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntranslates limit to valid bounds.", "response": "def _translate_limit(self, len_, start, num):\n        \"\"\"\n        Translate limit to valid bounds.\n        \"\"\"\n        if start > len_ or num <= 0:\n            return 0, 0\n        return min(start, len_), num"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a suitable function from score_cast_func", "response": "def _range_func(self, withscores, score_cast_func):\n        \"\"\"\n        Return a suitable function from (score, member)\n        \"\"\"\n        if withscores:\n            return lambda score_member: (score_member[1], score_cast_func(self._encode(score_member[0])))  # noqa\n        else:\n            return lambda score_member: score_member[1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _aggregate_func(self, aggregate):\n        funcs = {\"sum\": add, \"min\": min, \"max\": max}\n        func_name = aggregate.lower() if aggregate else 'sum'\n        try:\n            return funcs[func_name]\n        except KeyError:\n            raise TypeError(\"Unsupported aggregate: {}\".format(aggregate))", "response": "Return a suitable aggregate score function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list combining keys and args.", "response": "def _list_or_args(self, keys, args):\n        \"\"\"\n        Shamelessly copied from redis-py.\n        \"\"\"\n        # returns a single list combining keys and args\n        try:\n            iter(keys)\n            # a string can be iterated, but indicates\n            # keys wasn't passed as a list\n            if isinstance(keys, basestring):\n                keys = [keys]\n        except TypeError:\n            keys = [keys]\n        if args:\n            keys.extend(args)\n        return keys"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _encode(self, value):\n        \"Return a bytestring representation of the value. Taken from redis-py connection.py\"\n        if isinstance(value, bytes):\n            return value\n        elif isinstance(value, (int, long)):\n            value = str(value).encode('utf-8')\n        elif isinstance(value, float):\n            value = repr(value).encode('utf-8')\n        elif not isinstance(value, basestring):\n            value = str(value).encode('utf-8')\n        else:\n            value = value.encode('utf-8', 'strict')\n        return value", "response": "Return a bytestring representation of the value. Taken from redis - py connection. py"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninserts a new entry into the score list.", "response": "def insert(self, member, score):\n        \"\"\"\n        Identical to __setitem__, but returns whether a member was\n        inserted (True) or updated (False)\n        \"\"\"\n        found = self.remove(member)\n        index = bisect_left(self._scores, (score, member))\n        self._scores.insert(index, (score, member))\n        self._members[member] = score\n        return not found"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(self, member):\n        if member not in self:\n            return False\n        score = self._members[member]\n        score_index = bisect_left(self._scores, (score, member))\n        del self._scores[score_index]\n        del self._members[member]\n        return True", "response": "Removes a member from the set."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the rank of a member.", "response": "def rank(self, member):\n        \"\"\"\n        Get the rank (index of a member).\n        \"\"\"\n        score = self._members.get(member)\n        if score is None:\n            return None\n        return bisect_left(self._scores, (score, member))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of score member pairs between start and end ranks.", "response": "def range(self, start, end, desc=False):\n        \"\"\"\n        Return (score, member) pairs between min and max ranks.\n        \"\"\"\n        if not self:\n            return []\n\n        if desc:\n            return reversed(self._scores[len(self) - end - 1:len(self) - start])\n        else:\n            return self._scores[start:end + 1]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scorerange(self, start, end, start_inclusive=True, end_inclusive=True):\n        if not self:\n            return []\n\n        left = bisect_left(self._scores, (start,))\n        right = bisect_right(self._scores, (end,))\n\n        if end_inclusive:\n            # end is inclusive\n            while right < len(self) and self._scores[right][0] == end:\n                right += 1\n\n        if not start_inclusive:\n            while left < right and self._scores[left][0] == start:\n                left += 1\n        return self._scores[left:right]", "response": "Return a list of score pairs between start and end."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef watch(self, *keys):\n        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue a WATCH after a MULTI\")\n        self.watching = True\n        for key in keys:\n            self._watched_keys[key] = deepcopy(self.mock_redis.redis.get(self.mock_redis._encode(key)))", "response": "Watches the specified keys and returns the new keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multi(self):\n        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue nested calls to MULTI\")\n        if self.commands:\n            raise RedisError(\"Commands without an initial WATCH have already been issued\")\n        self.explicit_transaction = True", "response": "This method is used to issue multiple commands at once."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute all of the saved commands and return results.", "response": "def execute(self):\n        \"\"\"\n        Execute all of the saved commands and return results.\n        \"\"\"\n        try:\n            for key, value in self._watched_keys.items():\n                if self.mock_redis.redis.get(self.mock_redis._encode(key)) != value:\n                    raise WatchError(\"Watched variable changed.\")\n            return [command() for command in self.commands]\n        finally:\n            self._reset()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreset the internal state of the object.", "response": "def _reset(self):\n        \"\"\"\n        Reset instance variables.\n        \"\"\"\n        self.commands = []\n        self.watching = False\n        self._watched_keys = {}\n        self.explicit_transaction = False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_to_duckling_language_id(cls, lang):\n\n        if lang is not None and cls.is_supported(lang):\n            return lang\n        elif lang is not None and cls.is_supported(lang + \"$core\"):   # Support ISO 639-1 Language Codes (e.g. \"en\")\n            return lang + \"$core\"\n        else:\n            raise ValueError(\"Unsupported language '{}'. Supported languages: {}\".format(\n                lang, \", \".join(cls.SUPPORTED_LANGUAGES)))", "response": "Ensure a language identifier has the correct duckling format and is supported."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self, languages=[]):\n        duckling_load = self.clojure.var(\"duckling.core\", \"load!\")\n        clojure_hashmap = self.clojure.var(\"clojure.core\", \"hash-map\")\n        clojure_list = self.clojure.var(\"clojure.core\", \"list\")\n\n        if languages:\n            # Duckling's load function expects ISO 639-1 Language Codes (e.g. \"en\")\n            iso_languages = [Language.convert_to_iso(lang) for lang in languages]\n\n            duckling_load.invoke(\n                clojure_hashmap.invoke(\n                    self.clojure.read(':languages'),\n                    clojure_list.invoke(*iso_languages)\n                )\n            )\n        else:\n            duckling_load.invoke()\n\n        self._is_loaded = True", "response": "Loads the Duckling corpus."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse datetime information out of string input_str and returns a list of dicts with the result from the Duckling. parse method.", "response": "def parse(self, input_str, language=Language.ENGLISH, dim_filter=None, reference_time=''):\n        \"\"\"Parses datetime information out of string input.\n\n        It invokes the Duckling.parse() function in Clojure.\n        A language can be specified, default is English.\n\n        Args:\n            input_str: The input as string that has to be parsed.\n            language: Optional parameter to specify language,\n                e.g. Duckling.ENGLISH or supported ISO 639-1 Code (e.g. \"en\")\n            dim_filter: Optional parameter to specify a single filter or\n                list of filters for dimensions in Duckling.\n            reference_time: Optional reference time for Duckling.\n\n        Returns:\n            A list of dicts with the result from the Duckling.parse() call.\n\n        Raises:\n            RuntimeError: An error occurres when Duckling model is not loaded\n                via load().\n        \"\"\"\n        if self._is_loaded is False:\n            raise RuntimeError(\n                'Please load the model first by calling load()')\n        if threading.activeCount() > 1:\n            if not jpype.isThreadAttachedToJVM():\n                jpype.attachThreadToJVM()\n        language = Language.convert_to_duckling_language_id(language)\n        duckling_parse = self.clojure.var(\"duckling.core\", \"parse\")\n        duckling_time = self.clojure.var(\"duckling.time.obj\", \"t\")\n        clojure_hashmap = self.clojure.var(\"clojure.core\", \"hash-map\")\n\n        filter_str = '[]'\n        if isinstance(dim_filter, string_types):\n            filter_str = '[:{filter}]'.format(filter=dim_filter)\n        elif isinstance(dim_filter, list):\n            filter_str = '[{filter}]'.format(filter=' :'.join(dim_filter))\n        if reference_time:\n            duckling_result = duckling_parse.invoke(\n                language,\n                input_str,\n                self.clojure.read(filter_str),\n                clojure_hashmap.invoke(\n                    self.clojure.read(':reference-time'),\n                    duckling_time.invoke(\n                        *self._parse_reference_time(reference_time))\n                )\n            )\n        else:\n            duckling_result = duckling_parse.invoke(\n                language, input_str, self.clojure.read(filter_str))\n        return self._parse_result(duckling_result)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_time(self, input_str, reference_time=''):\n        return self._parse(input_str, dim=Dim.TIME,\n                           reference_time=reference_time)", "response": "Parses a string containing Duckling for occurences of times."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries the list of commands offered by the peer.", "response": "def get_commands(self, peer_jid):\n        \"\"\"\n        Return the list of commands offered by the peer.\n\n        :param peer_jid: JID of the peer to query\n        :type peer_jid: :class:`~aioxmpp.JID`\n        :rtype: :class:`list` of :class:`~.disco.xso.Item`\n        :return: List of command items\n\n        In the returned list, each :class:`~.disco.xso.Item` represents one\n        command supported by the peer. The :attr:`~.disco.xso.Item.node`\n        attribute is the identifier of the command which can be used with\n        :meth:`get_command_info` and :meth:`execute`.\n        \"\"\"\n\n        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = yield from disco.query_items(\n            peer_jid,\n            node=namespaces.xep0050_commands,\n        )\n        return response.items"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying the service discovery node of a command.", "response": "def get_command_info(self, peer_jid, command_name):\n        \"\"\"\n        Obtain information about a command.\n\n        :param peer_jid: JID of the peer to query\n        :type peer_jid: :class:`~aioxmpp.JID`\n        :param command_name: Node name of the command\n        :type command_name: :class:`str`\n        :rtype: :class:`~.disco.xso.InfoQuery`\n        :return: Service discovery information about the command\n\n        Sends a service discovery query to the service discovery node of the\n        command. The returned object contains information about the command,\n        such as the namespaces used by its implementation (generally the\n        :xep:`4` data forms namespace) and possibly localisations of the\n        commands name.\n\n        The `command_name` can be obtained by inspecting the listing from\n        :meth:`get_commands` or from well-known command names as defined for\n        example in :xep:`133`.\n        \"\"\"\n\n        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = yield from disco.query_info(\n            peer_jid,\n            node=command_name,\n        )\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef supports_commands(self, peer_jid):\n\n        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = yield from disco.query_info(\n            peer_jid,\n        )\n\n        return namespaces.xep0050_commands in response.features", "response": "Returns true if the peer supports Ad - Hoc commands protocol False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart a command execution session.", "response": "def execute(self, peer_jid, command_name):\n        \"\"\"\n        Start execution of a command with a peer.\n\n        :param peer_jid: JID of the peer to start the command at.\n        :type peer_jid: :class:`~aioxmpp.JID`\n        :param command_name: Node name of the command to execute.\n        :type command_name: :class:`str`\n        :rtype: :class:`~.adhoc.service.ClientSession`\n        :return: A started command execution session.\n\n        Initialises a client session and starts execution of the command. The\n        session is returned.\n\n        This may raise any exception which may be raised by\n        :meth:`~.adhoc.service.ClientSession.start`.\n        \"\"\"\n\n        session = ClientSession(\n            self.client.stream,\n            peer_jid,\n            command_name,\n        )\n        yield from session.start()\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a handler for a stateless command.", "response": "def register_stateless_command(self, node, name, handler, *,\n                                   is_allowed=None,\n                                   features={namespaces.xep0004_data}):\n        \"\"\"\n        Register a handler for a stateless command.\n\n        :param node: Name of the command (``node`` in the service discovery\n                     list).\n        :type node: :class:`str`\n        :param name: Human-readable name of the command\n        :type name: :class:`str` or :class:`~.LanguageMap`\n        :param handler: Coroutine function to run to get the response for a\n                        request.\n        :param is_allowed: A predicate which determines whether the command is\n                           shown and allowed for a given peer.\n        :type is_allowed: function or :data:`None`\n        :param features: Set of features to announce for the command\n        :type features: :class:`set` of :class:`str`\n\n        When a request for the command is received, `handler` is invoked. The\n        semantics of `handler` are the same as for\n        :meth:`~.StanzaStream.register_iq_request_handler`. It must produce a\n        valid :class:`~.adhoc.xso.Command` response payload.\n\n        If `is_allowed` is not :data:`None`, it is invoked whenever a command\n        listing is generated and whenever a command request is received. The\n        :class:`aioxmpp.JID` of the requester is passed as positional argument\n        to `is_allowed`. If `is_allowed` returns false, the command is not\n        included in the list and attempts to execute it are rejected with\n        ``<forbidden/>`` without calling `handler`.\n\n        If `is_allowed` is :data:`None`, the command is always visible and\n        allowed.\n\n        The `features` are returned on a service discovery info request for the\n        command node. By default, the :xep:`4` (Data Forms) namespace is\n        included, but this can be overridden by passing a different set without\n        that feature to `features`.\n        \"\"\"\n\n        info = CommandEntry(\n            name,\n            handler,\n            is_allowed=is_allowed,\n            features=features,\n        )\n        self._commands[node] = info\n        self._disco.mount_node(\n            node,\n            info,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef allowed_actions(self):\n\n        if self._response is not None and self._response.actions is not None:\n            return self._response.actions.allowed_actions\n        return {adhoc_xso.ActionType.EXECUTE,\n                adhoc_xso.ActionType.CANCEL}", "response": "Returns a list of allowed actions for the resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitiates the session by starting to execute the command with the peer.", "response": "def start(self):\n        \"\"\"\n        Initiate the session by starting to execute the command with the peer.\n\n        :return: The :attr:`~.xso.Command.first_payload` of the response\n\n        This sends an empty command IQ request with the\n        :attr:`~.ActionType.EXECUTE` action.\n\n        The :attr:`status`, :attr:`response` and related attributes get updated\n        with the newly received values.\n        \"\"\"\n\n        if self._response is not None:\n            raise RuntimeError(\"command execution already started\")\n\n        request = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=self._peer_jid,\n            payload=adhoc_xso.Command(self._command_name),\n        )\n\n        self._response = yield from self._stream.send_iq_and_wait_for_reply(\n            request,\n        )\n\n        return self._response.first_payload"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef proceed(self, *,\n                action=adhoc_xso.ActionType.EXECUTE,\n                payload=None):\n        \"\"\"\n        Proceed command execution to the next stage.\n\n        :param action: Action type for proceeding\n        :type action: :class:`~.ActionTyp`\n        :param payload: Payload for the request, or :data:`None`\n        :return: The :attr:`~.xso.Command.first_payload` of the response\n\n        `action` must be one of the actions returned by\n        :attr:`allowed_actions`. It defaults to :attr:`~.ActionType.EXECUTE`,\n        which is (alongside with :attr:`~.ActionType.CANCEL`) always allowed.\n\n        `payload` may be a sequence of XSOs, a single XSO or :data:`None`. If\n        it is :data:`None`, the XSOs from the request are re-used. This is\n        useful if you modify the payload in-place (e.g. via\n        :attr:`first_payload`). Otherwise, the payload on the request is set to\n        the `payload` argument; if it is a single XSO, it is wrapped in a\n        sequence.\n\n        The :attr:`status`, :attr:`response` and related attributes get updated\n        with the newly received values.\n        \"\"\"\n\n        if self._response is None:\n            raise RuntimeError(\"command execution not started yet\")\n\n        if action not in self.allowed_actions:\n            raise ValueError(\"action {} not allowed in this stage\".format(\n                action\n            ))\n\n        cmd = adhoc_xso.Command(\n            self._command_name,\n            action=action,\n            payload=self._response.payload if payload is None else payload,\n            sessionid=self.sessionid,\n        )\n\n        request = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=self._peer_jid,\n            payload=cmd,\n        )\n\n        try:\n            self._response = \\\n                yield from self._stream.send_iq_and_wait_for_reply(\n                    request,\n                )\n        except (aioxmpp.errors.XMPPModifyError,\n                aioxmpp.errors.XMPPCancelError) as exc:\n            if isinstance(exc.application_defined_condition,\n                          (adhoc_xso.BadSessionID,\n                           adhoc_xso.SessionExpired)):\n                yield from self.close()\n                raise SessionError(exc.text)\n            if isinstance(exc, aioxmpp.errors.XMPPCancelError):\n                yield from self.close()\n            raise\n\n        return self._response.first_payload", "response": "Proceed command execution to the next stage."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend data over the IBB.", "response": "def write(self, data):\n        \"\"\"\n        Send `data` over the IBB. If `data` is larger than the block size\n        is is chunked and sent in chunks.\n\n        Chunks from one call of :meth:`write` will always be sent in\n        series.\n        \"\"\"\n\n        if self.is_closing():\n            return\n\n        self._write_buffer += data\n\n        if len(self._write_buffer) >= self._output_buffer_limit_high:\n            self._protocol.pause_writing()\n\n        if self._write_buffer:\n            self._can_write.set()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expect_session(self, protocol_factory, peer_jid, sid):\n        def on_done(fut):\n            del self._expected_sessions[sid, peer_jid]\n\n        _, fut = self._expected_sessions[sid, peer_jid] = (\n            protocol_factory, asyncio.Future()\n        )\n        fut.add_done_callback(on_done)\n        return fut", "response": "When a session with peer_jid and sid is established return it when it is established."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open_session(self, protocol_factory, peer_jid, *,\n                     stanza_type=ibb_xso.IBBStanzaType.IQ,\n                     block_size=4096, sid=None):\n        \"\"\"\n        Establish an in-band bytestream session with `peer_jid` and\n        return the transport and protocol.\n\n        :param protocol_factory: the protocol factory\n        :type protocol_factory: a nullary callable returning an\n                                :class:`asyncio.Protocol` instance\n        :param peer_jid: the JID with which to establish the byte-stream.\n        :type peer_jid: :class:`aioxmpp.JID`\n        :param stanza_type: the stanza type to use\n        :type stanza_type: class:`~aioxmpp.ibb.IBBStanzaType`\n        :param block_size: the maximal size of blocks to transfer\n        :type block_size: :class:`int`\n        :param sid: the session id to use\n        :type sid: :class:`str` (must be a valid NMTOKEN)\n\n        :returns: the transport and protocol\n        :rtype: a tuple of :class:`aioxmpp.ibb.service.IBBTransport`\n                and :class:`asyncio.Protocol`\n        \"\"\"\n        if block_size > MAX_BLOCK_SIZE:\n            raise ValueError(\"block_size too large\")\n\n        if sid is None:\n            sid = utils.to_nmtoken(random.getrandbits(8*8))\n        open_ = ibb_xso.Open()\n        open_.stanza = stanza_type\n        open_.sid = sid\n        open_.block_size = block_size\n\n        # XXX: retry on XMPPModifyError with RESOURCE_CONSTRAINT\n        yield from self.client.send(\n            aioxmpp.IQ(\n                aioxmpp.IQType.SET,\n                to=peer_jid,\n                payload=open_,\n            )\n        )\n\n        handle = self._sessions[sid, peer_jid] = IBBTransport(\n            self,\n            peer_jid,\n            sid,\n            stanza_type,\n            block_size,\n        )\n\n        protocol = protocol_factory()\n        handle.set_protocol(protocol)\n        return handle, protocol", "response": "Establish an in - band bytestream session with peer_jid and return the transport and protocol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and return a : class:`EnumCDataType or : class:`EnumElementType` depending on the type of enum_class.", "response": "def EnumType(enum_class, nested_type=_Undefined, **kwargs):\n    \"\"\"\n    Create and return a :class:`EnumCDataType` or :class:`EnumElementType`,\n    depending on the type of `nested_type`.\n\n    If `nested_type` is a :class:`AbstractCDataType` or omitted, a\n    :class:`EnumCDataType` is constructed. Otherwise, :class:`EnumElementType`\n    is used.\n\n    The arguments are forwarded to the respective class\u2019 constructor.\n\n    .. versionadded:: 0.10\n\n    .. deprecated:: 0.10\n\n        This function was introduced to ease the transition in 0.10 from\n        a unified :class:`EnumType` to split :class:`EnumCDataType` and\n        :class:`EnumElementType`.\n\n        It will be removed in 1.0.\n    \"\"\"\n\n    if nested_type is _Undefined:\n        return EnumCDataType(enum_class, **kwargs)\n    if isinstance(nested_type, AbstractCDataType):\n        return EnumCDataType(enum_class, nested_type, **kwargs)\n    else:\n        return EnumElementType(enum_class, nested_type, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract one language - text pair from a dict.", "response": "def _extract_one_pair(body):\n    \"\"\"\n    Extract one language-text pair from a :class:`~.LanguageMap`.\n\n    This is used for tracking.\n    \"\"\"\n    if not body:\n        return None, None\n\n    try:\n        return None, body[None]\n    except KeyError:\n        return min(body.items(), key=lambda x: x[0])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef members(self):\n\n        if self._this_occupant is not None:\n            items = [self._this_occupant]\n        else:\n            items = []\n        items += list(self._occupant_info.values())\n        return items", "response": "A list of all occupants and their local user attributes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef features(self):\n\n        return {\n            aioxmpp.im.conversation.ConversationFeature.BAN,\n            aioxmpp.im.conversation.ConversationFeature.BAN_WITH_KICK,\n            aioxmpp.im.conversation.ConversationFeature.KICK,\n            aioxmpp.im.conversation.ConversationFeature.SEND_MESSAGE,\n            aioxmpp.im.conversation.ConversationFeature.SEND_MESSAGE_TRACKED,\n            aioxmpp.im.conversation.ConversationFeature.SET_TOPIC,\n            aioxmpp.im.conversation.ConversationFeature.SET_NICK,\n            aioxmpp.im.conversation.ConversationFeature.INVITE,\n            aioxmpp.im.conversation.ConversationFeature.INVITE_DIRECT,\n        }", "response": "Return a list of features supported by this MUC."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_message(self, msg):\n        msg.type_ = aioxmpp.MessageType.GROUPCHAT\n        msg.to = self._mucjid\n        # see https://mail.jabber.org/pipermail/standards/2017-January/032048.html  # NOQA\n        # for a full discussion on the rationale for this.\n        # TL;DR: we want to help entities to discover that a message is related\n        # to a MUC.\n        msg.xep0045_muc_user = muc_xso.UserExt()\n        result = self.service.client.enqueue(msg)\n        return result", "response": "Send a message to the MUC."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a message to the MUC with tracking.", "response": "def send_message_tracked(self, msg):\n        \"\"\"\n        Send a message to the MUC with tracking.\n\n        :param msg: The message to send.\n        :type msg: :class:`aioxmpp.Message`\n\n        .. warning::\n\n            Please read :ref:`api-tracking-memory`. This is especially relevant\n            for MUCs because tracking is not guaranteed to work due to how\n            :xep:`45` is written. It will work in many cases, probably in all\n            cases you test during development, but it may fail to work for some\n            individual messages and it may fail to work consistently for some\n            services. See the implementation details below for reasons.\n\n        The message is tracked and is considered\n        :attr:`~.MessageState.DELIVERED_TO_RECIPIENT` when it is reflected back\n        to us by the MUC service. The reflected message is then available in\n        the :attr:`~.MessageTracker.response` attribute.\n\n        .. note::\n\n            Two things:\n\n            1. The MUC service may change the contents of the message. An\n               example of this is the Prosody developer MUC which replaces\n               messages with more than a few lines with a pastebin link.\n            2. Reflected messages which are caught by tracking are not emitted\n               through :meth:`on_message`.\n\n        There is no need to set the address attributes or the type of the\n        message correctly; those will be overridden by this method to conform\n        to the requirements of a message to the MUC. Other attributes are left\n        untouched (except that :meth:`~.StanzaBase.autoset_id` is called) and\n        can be used as desired for the message.\n\n        .. warning::\n\n            Using :meth:`send_message_tracked` before :meth:`on_join` has\n            emitted will cause the `member` object in the resulting\n            :meth:`on_message` event to be :data:`None` (the message will be\n            delivered just fine).\n\n            Using :meth:`send_message_tracked` before history replay is over\n            will cause the :meth:`on_message` event to be emitted during\n            history replay, even though everyone else in the MUC will -- of\n            course -- only see the message after the history.\n\n            :meth:`send_message` is not affected by these quirks.\n\n        .. seealso::\n\n            :meth:`.AbstractConversation.send_message_tracked` for the full\n            interface specification.\n\n        **Implementation details:** Currently, we try to detect reflected\n        messages using two different criteria. First, if we see a message with\n        the same message ID (note that message IDs contain 120 bits of entropy)\n        as the message we sent, we consider it as the reflection. As some MUC\n        services re-write the message ID in the reflection, as a fallback, we\n        also consider messages which originate from the correct sender and have\n        the correct body a reflection.\n\n        Obviously, this fails consistently in MUCs which re-write the body and\n        re-write the ID and randomly if the MUC always re-writes the ID but\n        only sometimes the body.\n        \"\"\"\n        msg.type_ = aioxmpp.MessageType.GROUPCHAT\n        msg.to = self._mucjid\n        # see https://mail.jabber.org/pipermail/standards/2017-January/032048.html  # NOQA\n        # for a full discussion on the rationale for this.\n        # TL;DR: we want to help entities to discover that a message is related\n        # to a MUC.\n        msg.xep0045_muc_user = muc_xso.UserExt()\n        msg.autoset_id()\n        tracking_svc = self.service.dependencies[\n            aioxmpp.tracking.BasicTrackingService\n        ]\n        tracker = aioxmpp.tracking.MessageTracker()\n        id_key = msg.id_\n        body_key = _extract_one_pair(msg.body)\n        self._tracking_by_id[id_key] = tracker\n        self._tracking_metadata[tracker] = (\n            id_key,\n            body_key,\n        )\n        self._tracking_by_body.setdefault(\n            body_key,\n            []\n        ).append(tracker)\n        tracker.on_closed.connect(functools.partial(\n            self._tracker_closed,\n            tracker,\n        ))\n        token = tracking_svc.send_tracked(msg, tracker)\n        self.on_message(\n            msg,\n            self._this_occupant,\n            aioxmpp.im.dispatcher.MessageSource.STREAM,\n            tracker=tracker,\n        )\n        return token, tracker"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_nick(self, new_nick):\n\n        stanza = aioxmpp.Presence(\n            type_=aioxmpp.PresenceType.AVAILABLE,\n            to=self._mucjid.replace(resource=new_nick),\n        )\n        yield from self._service.client.send(\n            stanza\n        )", "response": "Change the nickname of the occupant."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nkicking an occupant from the MUC.", "response": "def kick(self, member, reason=None):\n        \"\"\"\n        Kick an occupant from the MUC.\n\n        :param member: The member to kick.\n        :type member: :class:`Occupant`\n        :param reason: A reason to show to the members of the conversation\n            including the kicked member.\n        :type reason: :class:`str`\n        :raises aioxmpp.errors.XMPPError: if the server returned an error for\n                                          the kick command.\n\n        .. seealso::\n\n            :meth:`.AbstractConversation.kick` for the full interface\n            specification.\n        \"\"\"\n        yield from self.muc_set_role(\n            member.nick,\n            \"none\",\n            reason=reason\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchanges the role of an occupant.", "response": "def muc_set_role(self, nick, role, *, reason=None):\n        \"\"\"\n        Change the role of an occupant.\n\n        :param nick: The nickname of the occupant whose role shall be changed.\n        :type nick: :class:`str`\n        :param role: The new role for the occupant.\n        :type role: :class:`str`\n        :param reason: An optional reason to show to the occupant (and all\n            others).\n\n        Change the role of an occupant, identified by their `nick`, to the\n        given new `role`. Optionally, a `reason` for the role change can be\n        provided.\n\n        Setting the different roles require different privilegues of the local\n        user. The details can be checked in :xep:`0045` and are enforced solely\n        by the server, not local code.\n\n        The coroutine returns when the role change has been acknowledged by the\n        server. If the server returns an error, an appropriate\n        :class:`aioxmpp.errors.XMPPError` subclass is raised.\n        \"\"\"\n\n        if nick is None:\n            raise ValueError(\"nick must not be None\")\n\n        if role is None:\n            raise ValueError(\"role must not be None\")\n\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.SET,\n            to=self._mucjid\n        )\n\n        iq.payload = muc_xso.AdminQuery(\n            items=[\n                muc_xso.AdminItem(nick=nick,\n                                  reason=reason,\n                                  role=role)\n            ]\n        )\n\n        yield from self.service.client.send(iq)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbans an occupant from re - joining the MUC.", "response": "def ban(self, member, reason=None, *, request_kick=True):\n        \"\"\"\n        Ban an occupant from re-joining the MUC.\n\n        :param member: The occupant to ban.\n        :type member: :class:`Occupant`\n        :param reason: A reason to show to the members of the conversation\n            including the banned member.\n        :type reason: :class:`str`\n        :param request_kick: A flag indicating that the member should be\n            removed from the conversation immediately, too.\n        :type request_kick: :class:`bool`\n\n        `request_kick` is supported by MUC, but setting it to false has no\n        effect: banned members are always immediately kicked.\n\n        .. seealso::\n\n            :meth:`.AbstractConversation.ban` for the full interface\n            specification.\n        \"\"\"\n        if member.direct_jid is None:\n            raise ValueError(\n                \"cannot ban members whose direct JID is not \"\n                \"known\")\n\n        yield from self.muc_set_affiliation(\n            member.direct_jid,\n            \"outcast\",\n            reason=reason\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the affiliation of a user in the MUC.", "response": "def muc_set_affiliation(self, jid, affiliation, *, reason=None):\n        \"\"\"\n        Convenience wrapper around :meth:`.MUCClient.set_affiliation`. See\n        there for details, and consider its `mucjid` argument to be set to\n        :attr:`mucjid`.\n        \"\"\"\n        return (yield from self.service.set_affiliation(\n            self._mucjid,\n            jid, affiliation,\n            reason=reason))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_topic(self, new_topic):\n\n        msg = aioxmpp.stanza.Message(\n            type_=aioxmpp.structs.MessageType.GROUPCHAT,\n            to=self._mucjid\n        )\n        msg.subject.update(new_topic)\n\n        yield from self.service.client.send(msg)", "response": "Change the topic of the conversation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef muc_request_voice(self):\n\n        msg = aioxmpp.Message(\n            to=self._mucjid,\n            type_=aioxmpp.MessageType.NORMAL\n        )\n\n        data = aioxmpp.forms.Data(\n            aioxmpp.forms.DataType.SUBMIT,\n        )\n\n        data.fields.append(\n            aioxmpp.forms.Field(\n                type_=aioxmpp.forms.FieldType.HIDDEN,\n                var=\"FORM_TYPE\",\n                values=[\"http://jabber.org/protocol/muc#request\"],\n            ),\n        )\n        data.fields.append(\n            aioxmpp.forms.Field(\n                type_=aioxmpp.forms.FieldType.LIST_SINGLE,\n                var=\"muc#role\",\n                values=[\"participant\"],\n            )\n        )\n\n        msg.xep0004_data.append(data)\n\n        yield from self.service.client.send(msg)", "response": "Request voice in the room and wait for the request to be sent."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\njoining a multi - user chat and create a conversation for it.", "response": "def join(self, mucjid, nick, *,\n             password=None, history=None, autorejoin=True):\n        \"\"\"\n        Join a multi-user chat and create a conversation for it.\n\n        :param mucjid: The bare JID of the room to join.\n        :type mucjid: :class:`~aioxmpp.JID`.\n        :param nick: The nickname to use in the room.\n        :type nick: :class:`str`\n        :param password: The password to join the room, if required.\n        :type password: :class:`str`\n        :param history: Specification for how much and which history to fetch.\n        :type history: :class:`.xso.History`\n        :param autorejoin: Flag to indicate that the MUC should be\n            automatically rejoined after a disconnect.\n        :type autorejoin: :class:`bool`\n        :raises ValueError: if the MUC JID is invalid.\n        :return: The :term:`Conversation` and a future on the join.\n        :rtype: tuple of :class:`~.Room` and :class:`asyncio.Future`.\n\n        Join a multi-user chat at `mucjid` with `nick`. Return a :class:`Room`\n        instance which is used to track the MUC locally and a\n        :class:`aioxmpp.Future` which becomes done when the join succeeded\n        (with a :data:`None` value) or failed (with an exception).\n\n        In addition, the :meth:`~.ConversationService.on_conversation_added`\n        signal is emitted immediately with the new :class:`Room`.\n\n        It is recommended to attach the desired signals to the :class:`Room`\n        before yielding next (e.g. in a non-deferred event handler to the\n        :meth:`~.ConversationService.on_conversation_added` signal), to avoid\n        races with the server. It is guaranteed that no signals are emitted\n        before the next yield, and thus, it is safe to attach the signals right\n        after :meth:`join` returned. (This is also the reason why :meth:`join`\n        is not a coroutine, but instead returns the room and a future to wait\n        for.)\n\n        Any other interaction with the room must go through the :class:`Room`\n        instance.\n\n        If the multi-user chat at `mucjid` is already or currently being\n        joined, the existing :class:`Room` and future is returned. The `nick`\n        and other options for the new join are ignored.\n\n        If the `mucjid` is not a bare JID, :class:`ValueError` is raised.\n\n        `password` may be a string used as password for the MUC. It will be\n        remembered and stored at the returned :class:`Room` instance.\n\n        `history` may be a :class:`History` instance to request a specific\n        amount of history; otherwise, the server will return a default amount\n        of history.\n\n        If `autorejoin` is true, the MUC will be re-joined after the stream has\n        been destroyed and re-established. In that case, the service will\n        request history since the stream destruction and ignore the `history`\n        object passed here.\n\n        If the stream is currently not established, the join is deferred until\n        the stream is established.\n        \"\"\"\n        if history is not None and not isinstance(history, muc_xso.History):\n            raise TypeError(\"history must be {!s}, got {!r}\".format(\n                muc_xso.History.__name__,\n                history))\n\n        if not mucjid.is_bare:\n            raise ValueError(\"MUC JID must be bare\")\n\n        try:\n            room, fut, *_ = self._pending_mucs[mucjid]\n        except KeyError:\n            pass\n        else:\n            return room, fut\n\n        try:\n            room = self._joined_mucs[mucjid]\n        except KeyError:\n            pass\n        else:\n            fut = asyncio.Future()\n            fut.set_result(None)\n            return room, fut\n\n        room = Room(self, mucjid)\n        room.muc_autorejoin = autorejoin\n        room.muc_password = password\n        room.on_exit.connect(\n            functools.partial(\n                self._muc_exited,\n                room\n            )\n        )\n        room.on_muc_enter.connect(\n            self._pending_on_enter,\n        )\n\n        fut = asyncio.Future()\n        fut.add_done_callback(functools.partial(\n            self._pending_join_done,\n            mucjid,\n            room,\n        ))\n        self._pending_mucs[mucjid] = room, fut, nick, history\n\n        if self.client.established:\n            self._send_join_presence(mucjid, history, nick, password)\n\n        self.on_conversation_new(room)\n        self.dependencies[\n            aioxmpp.im.service.ConversationService\n        ]._add_conversation(room)\n\n        return room, fut"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges the affiliation of an entity with a MUC identified by the given MUCjid.", "response": "def set_affiliation(self, mucjid, jid, affiliation, *, reason=None):\n        \"\"\"\n        Change the affiliation of an entity with a MUC.\n\n        :param mucjid: The bare JID identifying the MUC.\n        :type mucjid: :class:`~aioxmpp.JID`\n        :param jid: The bare JID of the entity whose affiliation shall be\n            changed.\n        :type jid: :class:`~aioxmpp.JID`\n        :param affiliation: The new affiliation for the entity.\n        :type affiliation: :class:`str`\n        :param reason: Optional reason for the affiliation change.\n        :type reason: :class:`str` or :data:`None`\n\n        Change the affiliation of the given `jid` with the MUC identified by\n        the bare `mucjid` to the given new `affiliation`. Optionally, a\n        `reason` can be given.\n\n        If you are joined in the MUC, :meth:`Room.muc_set_affiliation` may be\n        more convenient, but it is possible to modify the affiliations of a MUC\n        without being joined, given sufficient privilegues.\n\n        Setting the different affiliations require different privilegues of the\n        local user. The details can be checked in :xep:`0045` and are enforced\n        solely by the server, not local code.\n\n        The coroutine returns when the change in affiliation has been\n        acknowledged by the server. If the server returns an error, an\n        appropriate :class:`aioxmpp.errors.XMPPError` subclass is raised.\n        \"\"\"\n\n        if mucjid is None or not mucjid.is_bare:\n            raise ValueError(\"mucjid must be bare JID\")\n\n        if jid is None:\n            raise ValueError(\"jid must not be None\")\n\n        if affiliation is None:\n            raise ValueError(\"affiliation must not be None\")\n\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.SET,\n            to=mucjid\n        )\n\n        iq.payload = muc_xso.AdminQuery(\n            items=[\n                muc_xso.AdminItem(jid=jid,\n                                  reason=reason,\n                                  affiliation=affiliation)\n            ]\n        )\n\n        yield from self.client.send(iq)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_room_config(self, mucjid):\n\n        if mucjid is None or not mucjid.is_bare:\n            raise ValueError(\"mucjid must be bare JID\")\n\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.GET,\n            to=mucjid,\n            payload=muc_xso.OwnerQuery(),\n        )\n\n        return (yield from self.client.send(iq)).form", "response": "Query and return the room configuration form for the given MUC."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_room_config(self, mucjid, data):\n\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.SET,\n            to=mucjid,\n            payload=muc_xso.OwnerQuery(form=data),\n        )\n\n        yield from self.client.send(iq)", "response": "Set the room configuration using a data form."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request_slot(client,\n                 service: JID,\n                 filename: str,\n                 size: int,\n                 content_type: str):\n    \"\"\"\n    Request an HTTP upload slot.\n\n    :param client: The client to request the slot with.\n    :type client: :class:`aioxmpp.Client`\n    :param service: Address of the HTTP upload service.\n    :type service: :class:`~aioxmpp.JID`\n    :param filename: Name of the file (without path), may be used by the server\n        to generate the URL.\n    :type filename: :class:`str`\n    :param size: Size of the file in bytes\n    :type size: :class:`int`\n    :param content_type: The MIME type of the file\n    :type content_type: :class:`str`\n    :return: The assigned upload slot.\n    :rtype: :class:`.xso.Slot`\n\n    Sends a :xep:`363` slot request to the XMPP service to obtain HTTP\n    PUT and GET URLs for a file upload.\n\n    The upload slot is returned as a :class:`~.xso.Slot` object.\n    \"\"\"\n\n    payload = Request(filename, size, content_type)\n    return (yield from client.send(IQ(\n        type_=IQType.GET,\n        to=service,\n        payload=payload\n    )))", "response": "Request an HTTP upload slot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_version(stream: aioxmpp.stream.StanzaStream,\n                  target: aioxmpp.JID) -> version_xso.Query:\n    \"\"\"\n    Query the software version of an entity.\n\n    :param stream: A stanza stream to send the query on.\n    :type stream: :class:`aioxmpp.stream.StanzaStream`\n    :param target: The address of the entity to query.\n    :type target: :class:`aioxmpp.JID`\n    :raises OSError: if a connection issue occured before a reply was received\n    :raises aioxmpp.errors.XMPPError: if an XMPP error was returned instead\n        of a reply.\n    :rtype: :class:`aioxmpp.version.xso.Query`\n    :return: The response from the peer.\n\n    The response is returned as :class:`~aioxmpp.version.xso.Query` object. The\n    attributes hold the data returned by the peer. Each attribute may be\n    :data:`None` if the peer chose to omit that information. In an extreme\n    case, all attributes are :data:`None`.\n    \"\"\"\n\n    return (yield from stream.send(\n        aioxmpp.IQ(\n            type_=aioxmpp.IQType.GET,\n            to=target,\n            payload=version_xso.Query(),\n        )\n    ))", "response": "Query the software version of an entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef as_bookmark_class(xso_class):\n\n    if not issubclass(xso_class, Bookmark):\n        raise TypeError(\n            \"Classes registered as bookmark types must be Bookmark subclasses\"\n        )\n\n    Storage.register_child(\n        Storage.bookmarks,\n        xso_class\n    )\n\n    return xso_class", "response": "Decorator to register xso_class as a custom bookmark class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an iterator that yields all the languages in the given list of languages.", "response": "def basic_filter_languages(languages, ranges):\n    \"\"\"\n    Filter languages using the string-based basic filter algorithm described in\n    RFC4647.\n\n    `languages` must be a sequence of :class:`LanguageTag` instances which are\n    to be filtered.\n\n    `ranges` must be an iterable which represent the basic language ranges to\n    filter with, in priority order. The language ranges must be given as\n    :class:`LanguageRange` objects.\n\n    Return an iterator of languages which matched any of the `ranges`. The\n    sequence produced by the iterator is in match order and duplicate-free. The\n    first range to match a language yields the language into the iterator, no\n    other range can yield that language afterwards.\n    \"\"\"\n\n    if LanguageRange.WILDCARD in ranges:\n        yield from languages\n        return\n\n    found = set()\n\n    for language_range in ranges:\n        range_str = language_range.match_str\n        for language in languages:\n            if language in found:\n                continue\n\n            match_str = language.match_str\n            if match_str == range_str:\n                yield language\n                found.add(language)\n                continue\n\n            if len(range_str) < len(match_str):\n                if     (match_str[:len(range_str)] == range_str and\n                        match_str[len(range_str)] == \"-\"):\n                    yield language\n                    found.add(language)\n                    continue"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lookup_language(languages, ranges):\n\n    for language_range in ranges:\n        while True:\n            try:\n                return next(iter(basic_filter_languages(\n                    languages,\n                    [language_range])))\n            except StopIteration:\n                pass\n\n            try:\n                language_range = language_range.strip_rightmost()\n            except ValueError:\n                break", "response": "Look up a single language in the sequence languages using the lookup\n    mechansim described in RFC4647."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replace(self, **kwargs):\n\n        new_kwargs = {}\n\n        strict = kwargs.pop(\"strict\", True)\n\n        try:\n            localpart = kwargs.pop(\"localpart\")\n        except KeyError:\n            pass\n        else:\n            if localpart:\n                localpart = nodeprep(\n                    localpart,\n                    allow_unassigned=not strict\n                )\n            new_kwargs[\"localpart\"] = localpart\n\n        try:\n            domain = kwargs.pop(\"domain\")\n        except KeyError:\n            pass\n        else:\n            if not domain:\n                raise ValueError(\"domain must not be empty or None\")\n            new_kwargs[\"domain\"] = nameprep(\n                domain,\n                allow_unassigned=not strict\n            )\n\n        try:\n            resource = kwargs.pop(\"resource\")\n        except KeyError:\n            pass\n        else:\n            if resource:\n                resource = resourceprep(\n                    resource,\n                    allow_unassigned=not strict\n                )\n            new_kwargs[\"resource\"] = resource\n\n        if kwargs:\n            raise TypeError(\"replace() got an unexpected keyword argument\"\n                            \" {!r}\".format(\n                                next(iter(kwargs))))\n\n        return super()._replace(**new_kwargs)", "response": "Returns a new object with the corresponding JID substitutions performed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs a JID from a string containing it.", "response": "def fromstr(cls, s, *, strict=True):\n        \"\"\"\n        Construct a JID out of a string containing it.\n\n        :param s: The string to parse.\n        :type s: :class:`str`\n        :param strict: Whether to enable strict parsing.\n        :type strict: :class:`bool`\n        :raises: See :class:`JID`\n        :return: The parsed JID\n        :rtype: :class:`JID`\n\n        See the :class:`JID` class level documentation for the semantics of\n        `strict`.\n        \"\"\"\n        nodedomain, sep, resource = s.partition(\"/\")\n        if not sep:\n            resource = None\n\n        localpart, sep, domain = nodedomain.partition(\"@\")\n        if not sep:\n            domain = localpart\n            localpart = None\n        return cls(localpart, domain, resource, strict=strict)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef strip_rightmost(self):\n\n        parts = self.print_str.split(\"-\")\n        parts.pop()\n        if parts and len(parts[-1]) == 1:\n            parts.pop()\n        return type(self).fromstr(\"-\".join(parts))", "response": "Returns a new instance of LanguageRange with the new rightmost part removed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lookup(self, language_ranges):\n        keys = list(self.keys())\n        try:\n            keys.remove(None)\n        except ValueError:\n            pass\n        keys.sort()\n        key = lookup_language(keys, language_ranges)\n        return self[key]", "response": "Perform an RFC 4647 language range lookup on the keys in the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_conversation(self, peer_jid, *, current_jid=None):\n\n        try:\n            return self._conversationmap[peer_jid]\n        except KeyError:\n            pass\n        return self._make_conversation(peer_jid, False)", "response": "Get or create a one - to - one conversation with a peer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reconfigure_resolver():\n\n    global _state\n    _state.resolver = dns.resolver.Resolver()\n    _state.overridden_resolver = False", "response": "Reset the resolver for this thread to a fresh instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lookup_srv(\n        domain: bytes,\n        service: str,\n        transport: str = \"tcp\",\n        **kwargs):\n    \"\"\"\n    Query the DNS for SRV records describing how the given `service` over the\n    given `transport` is implemented for the given `domain`. `domain` must be\n    an IDNA-encoded :class:`bytes` object; `service` must be a normal\n    :class:`str`.\n\n    Keyword arguments are passed to :func:`repeated_query`.\n\n    Return a list of tuples ``(prio, weight, (hostname, port))``, where\n    `hostname` is a IDNA-encoded :class:`bytes` object containing the hostname\n    obtained from the SRV record. The other fields are also as obtained from\n    the SRV records. The trailing dot is stripped from the `hostname`.\n\n    If the DNS query returns an empty result, :data:`None` is returned. If any\n    of the found SRV records has the root zone (``.``) as `hostname`, this\n    indicates that the service is not available at the given `domain` and\n    :class:`ValueError` is raised.\n    \"\"\"\n\n    record = b\".\".join([\n        b\"_\" + service.encode(\"ascii\"),\n        b\"_\" + transport.encode(\"ascii\"),\n        domain])\n\n    answer = yield from repeated_query(\n        record,\n        dns.rdatatype.SRV,\n        **kwargs)\n\n    if answer is None:\n        return None\n\n    items = [\n        (rec.priority, rec.weight, (str(rec.target), rec.port))\n        for rec in answer\n    ]\n\n    for i, (prio, weight, (host, port)) in enumerate(items):\n        if host == \".\":\n            raise ValueError(\n                \"protocol {!r} over {!r} not supported at {!r}\".format(\n                    service,\n                    transport,\n                    domain\n                )\n            )\n\n        items[i] = (prio, weight, (\n            host.rstrip(\".\").encode(\"ascii\"),\n            port))\n\n    return items", "response": "Query the DNS for SRV records describing how the given service over the given domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lookup_tlsa(hostname, port, transport=\"tcp\", require_ad=True, **kwargs):\n    record = b\".\".join([\n        b\"_\" + str(port).encode(\"ascii\"),\n        b\"_\" + transport.encode(\"ascii\"),\n        hostname\n    ])\n\n    answer = yield from repeated_query(\n        record,\n        dns.rdatatype.TLSA,\n        require_ad=require_ad,\n        **kwargs)\n\n    if answer is None:\n        return None\n\n    items = [\n        (rec.usage, rec.selector, rec.mtype, rec.cert)\n        for rec in answer\n    ]\n\n    return items", "response": "Query the DNS for TLSA records describing the certificates and or keys and return the information from the records."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngroup a list of SRV record information into a list of tuples.", "response": "def group_and_order_srv_records(all_records, rng=None):\n    \"\"\"\n    Order a list of SRV record information (as returned by :func:`lookup_srv`)\n    and group and order them as specified by the RFC.\n\n    Return an iterable, yielding each ``(hostname, port)`` tuple inside the\n    SRV records in the order specified by the RFC. For hosts with the same\n    priority, the given `rng` implementation is used (if none is given, the\n    :mod:`random` module is used).\n    \"\"\"\n    rng = rng or random\n\n    all_records.sort(key=lambda x: x[:2])\n\n    for priority, records in itertools.groupby(\n            all_records,\n            lambda x: x[0]):\n\n        records = list(records)\n        total_weight = sum(\n            weight\n            for _, weight, _ in records)\n\n        while records:\n            if len(records) == 1:\n                yield records[0][-1]\n                break\n\n            value = rng.randint(0, total_weight)\n            running_weight_sum = 0\n            for i, (_, weight, addr) in enumerate(records):\n                running_weight_sum += weight\n                if running_weight_sum >= value:\n                    yield addr\n                    del records[i]\n                    total_weight -= weight\n                    break"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_handler_spec(f, handler_spec, *, kwargs=None):\n    handler_dict = automake_magic_attr(f)\n    if kwargs is None:\n        kwargs = {}\n    if kwargs != handler_dict.setdefault(handler_spec, kwargs):\n        raise ValueError(\n            \"The additional keyword arguments to the handler are incompatible\")", "response": "Adds a handler specification to a function."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters the decorated function or coroutine function as IQ request handler. :param type_: IQ type to listen for :type type_: :class:`~.IQType` :param payload_cls: Payload XSO class to listen for :type payload_cls: :class:`~.XSO` subclass :param with_send_reply: Whether to pass a function to send a reply to the decorated callable as second argument. :type with_send_reply: :class:`bool` :raises ValueError: if `payload_cls` is not a registered IQ payload If the decorated function is not a coroutine function, it must return an awaitable instead. .. seealso:: :meth:`~.StanzaStream.register_iq_request_handler` for more details on the `type_`, `payload_cls` and `with_send_reply` arguments, as well as behaviour expected from the decorated function. :meth:`aioxmpp.IQ.as_payload_class` for a way to register a XSO as IQ payload .. versionadded:: 0.11 The `with_send_reply` argument. .. versionchanged:: 0.10 The decorator now checks if `payload_cls` is a valid, registered IQ payload and raises :class:`ValueError` if not.", "response": "def iq_handler(type_, payload_cls, *, with_send_reply=False):\n    \"\"\"\n    Register the decorated function or coroutine function as IQ request\n    handler.\n\n    :param type_: IQ type to listen for\n    :type type_: :class:`~.IQType`\n    :param payload_cls: Payload XSO class to listen for\n    :type payload_cls: :class:`~.XSO` subclass\n    :param with_send_reply: Whether to pass a function to send a reply\n       to the decorated callable as second argument.\n    :type with_send_reply: :class:`bool`\n\n    :raises ValueError: if `payload_cls` is not a registered IQ payload\n\n    If the decorated function is not a coroutine function, it must return an\n    awaitable instead.\n\n    .. seealso::\n\n        :meth:`~.StanzaStream.register_iq_request_handler` for more\n            details on the `type_`, `payload_cls` and\n            `with_send_reply` arguments, as well as behaviour expected\n            from the decorated function.\n\n        :meth:`aioxmpp.IQ.as_payload_class`\n            for a way to register a XSO as IQ payload\n\n    .. versionadded:: 0.11\n\n       The `with_send_reply` argument.\n\n    .. versionchanged:: 0.10\n\n        The decorator now checks if `payload_cls` is a valid, registered IQ\n        payload and raises :class:`ValueError` if not.\n    \"\"\"\n\n    if (not hasattr(payload_cls, \"TAG\") or\n            (aioxmpp.IQ.CHILD_MAP.get(payload_cls.TAG) is not\n             aioxmpp.IQ.payload.xq_descriptor) or\n            payload_cls not in aioxmpp.IQ.payload._classes):\n        raise ValueError(\n            \"{!r} is not a valid IQ payload \"\n            \"(use IQ.as_payload_class decorator)\".format(\n                payload_cls,\n            )\n        )\n\n    def decorator(f):\n        add_handler_spec(\n            f,\n            HandlerSpec(\n                (_apply_iq_handler, (type_, payload_cls)),\n                require_deps=(),\n            ),\n            kwargs=dict(with_send_reply=with_send_reply),\n        )\n        return f\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef message_handler(type_, from_):\n    import aioxmpp.dispatcher\n    return aioxmpp.dispatcher.message_handler(type_, from_)", "response": "Deprecated alias of :func:`.dispatcher.message_handler`.\n\n    .. deprecated:: 0.9"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a presence handler for the given type.", "response": "def presence_handler(type_, from_):\n    \"\"\"\n    Deprecated alias of :func:`.dispatcher.presence_handler`.\n\n    .. deprecated:: 0.9\n    \"\"\"\n    import aioxmpp.dispatcher\n    return aioxmpp.dispatcher.presence_handler(type_, from_)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister the decorated function as a service-level inbound message filter. :raise TypeError: if the decorated object is a coroutine function .. seealso:: :class:`StanzaStream` for important remarks regarding the use of stanza filters.", "response": "def inbound_message_filter(f):\n    \"\"\"\n    Register the decorated function as a service-level inbound message filter.\n\n    :raise TypeError: if the decorated object is a coroutine function\n\n    .. seealso::\n\n       :class:`StanzaStream`\n          for important remarks regarding the use of stanza filters.\n\n    \"\"\"\n\n    if asyncio.iscoroutinefunction(f):\n        raise TypeError(\n            \"inbound_message_filter must not be a coroutine function\"\n        )\n\n    add_handler_spec(\n        f,\n        HandlerSpec(\n            (_apply_inbound_message_filter, ())\n        ),\n    )\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister the decorated function as a service-level inbound presence filter. :raise TypeError: if the decorated object is a coroutine function .. seealso:: :class:`StanzaStream` for important remarks regarding the use of stanza filters.", "response": "def inbound_presence_filter(f):\n    \"\"\"\n    Register the decorated function as a service-level inbound presence filter.\n\n    :raise TypeError: if the decorated object is a coroutine function\n\n    .. seealso::\n\n       :class:`StanzaStream`\n          for important remarks regarding the use of stanza filters.\n\n    \"\"\"\n\n    if asyncio.iscoroutinefunction(f):\n        raise TypeError(\n            \"inbound_presence_filter must not be a coroutine function\"\n        )\n\n    add_handler_spec(\n        f,\n        HandlerSpec(\n            (_apply_inbound_presence_filter, ())\n        ),\n    )\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef outbound_message_filter(f):\n\n    if asyncio.iscoroutinefunction(f):\n        raise TypeError(\n            \"outbound_message_filter must not be a coroutine function\"\n        )\n\n    add_handler_spec(\n        f,\n        HandlerSpec(\n            (_apply_outbound_message_filter, ())\n        ),\n    )\n    return f", "response": "Decorator to add a function as a service - level outbound message filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters the decorated function as a service-level outbound presence filter. :raise TypeError: if the decorated object is a coroutine function .. seealso:: :class:`StanzaStream` for important remarks regarding the use of stanza filters.", "response": "def outbound_presence_filter(f):\n    \"\"\"\n    Register the decorated function as a service-level outbound presence\n    filter.\n\n    :raise TypeError: if the decorated object is a coroutine function\n\n    .. seealso::\n\n       :class:`StanzaStream`\n          for important remarks regarding the use of stanza filters.\n\n    \"\"\"\n\n    if asyncio.iscoroutinefunction(f):\n        raise TypeError(\n            \"outbound_presence_filter must not be a coroutine function\"\n        )\n\n    add_handler_spec(\n        f,\n        HandlerSpec(\n            (_apply_outbound_presence_filter, ())\n        ),\n    )\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef depsignal(class_, signal_name, *, defer=False):\n\n    def decorator(f):\n        add_handler_spec(\n            f,\n            _depsignal_spec(class_, signal_name, f, defer)\n        )\n        return f\n    return decorator", "response": "Connect the decorated method or coroutine method to the addressed signal on the given class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attrsignal(descriptor, signal_name, *, defer=False):\n    def decorator(f):\n        add_handler_spec(\n            f,\n            _attrsignal_spec(descriptor, signal_name, f, defer)\n        )\n        return f\n    return decorator", "response": "Decorator for adding a signal to the specified descriptor or coroutine method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_iq_handler(type_, payload_cls, coro, *, with_send_reply=False):\n\n    try:\n        handlers = get_magic_attr(coro)\n    except AttributeError:\n        return False\n\n    hs = HandlerSpec(\n        (_apply_iq_handler, (type_, payload_cls)),\n    )\n\n    try:\n        return handlers[hs] == dict(with_send_reply=with_send_reply)\n    except KeyError:\n        return False", "response": "Return True if coro has been decorated with iq_handler for the\n    given type_ and payload_cls and keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a message handler is registered for a given type.", "response": "def is_message_handler(type_, from_, cb):\n    \"\"\"\n    Deprecated alias of :func:`.dispatcher.is_message_handler`.\n\n    .. deprecated:: 0.9\n    \"\"\"\n    import aioxmpp.dispatcher\n    return aioxmpp.dispatcher.is_message_handler(type_, from_, cb)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a stanza is presence handler.", "response": "def is_presence_handler(type_, from_, cb):\n    \"\"\"\n    Deprecated alias of :func:`.dispatcher.is_presence_handler`.\n\n    .. deprecated:: 0.9\n    \"\"\"\n    import aioxmpp.dispatcher\n    return aioxmpp.dispatcher.is_presence_handler(type_, from_, cb)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_inbound_message_filter(cb):\n\n    try:\n        handlers = get_magic_attr(cb)\n    except AttributeError:\n        return False\n\n    hs = HandlerSpec(\n        (_apply_inbound_message_filter, ())\n    )\n\n    return hs in handlers", "response": "Return True if cb has been decorated with inbound_message_filter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if cb has been decorated with .", "response": "def is_inbound_presence_filter(cb):\n    \"\"\"\n    Return true if `cb` has been decorated with\n    :func:`inbound_presence_filter`.\n    \"\"\"\n\n    try:\n        handlers = get_magic_attr(cb)\n    except AttributeError:\n        return False\n\n    hs = HandlerSpec(\n        (_apply_inbound_presence_filter, ())\n    )\n\n    return hs in handlers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if cb has been decorated with .", "response": "def is_outbound_message_filter(cb):\n    \"\"\"\n    Return true if `cb` has been decorated with\n    :func:`outbound_message_filter`.\n    \"\"\"\n\n    try:\n        handlers = get_magic_attr(cb)\n    except AttributeError:\n        return False\n\n    hs = HandlerSpec(\n        (_apply_outbound_message_filter, ())\n    )\n\n    return hs in handlers"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_outbound_presence_filter(cb):\n\n    try:\n        handlers = get_magic_attr(cb)\n    except AttributeError:\n        return False\n\n    hs = HandlerSpec(\n        (_apply_outbound_presence_filter, ())\n    )\n\n    return hs in handlers", "response": "Return True if cb has been decorated with\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if cb has been decorated with depsignal for the given class and connection mode.", "response": "def is_depsignal_handler(class_, signal_name, cb, *, defer=False):\n    \"\"\"\n    Return true if `cb` has been decorated with :func:`depsignal` for the given\n    signal, class and connection mode.\n    \"\"\"\n    try:\n        handlers = get_magic_attr(cb)\n    except AttributeError:\n        return False\n\n    return _depsignal_spec(class_, signal_name, cb, defer) in handlers"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if filter_ has been decorated with the depfilter for the class_ and filter_name.", "response": "def is_depfilter_handler(class_, filter_name, filter_):\n    \"\"\"\n    Return true if `filter_` has been decorated with :func:`depfilter` for the\n    given filter and class.\n    \"\"\"\n    try:\n        handlers = get_magic_attr(filter_)\n    except AttributeError:\n        return False\n\n    return _depfilter_spec(class_, filter_name) in handlers"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if cb has been decorated with attrsignal for the given signal descriptor and connection mode.", "response": "def is_attrsignal_handler(descriptor, signal_name, cb, *, defer=False):\n    \"\"\"\n    Return true if `cb` has been decorated with :func:`attrsignal` for the\n    given signal, descriptor and connection mode.\n    \"\"\"\n    try:\n        handlers = get_magic_attr(cb)\n    except AttributeError:\n        return False\n\n    return _attrsignal_spec(descriptor, signal_name, cb, defer) in handlers"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_to_stack(self, instance, stack):\n\n        cm = self.init_cm(instance)\n        obj = stack.enter_context(cm)\n        self._data[instance] = cm, obj\n        return obj", "response": "Adds the given instance to the given context manager stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef orders_after(self, other, *, visited=None):\n        return self.orders_after_any(frozenset([other]), visited=visited)", "response": "Returns whether self depends on other and will be instanciated\n        later."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn whether this service orders after any of the services in the set other.", "response": "def orders_after_any(self, other, *, visited=None):\n        \"\"\"\n        Return whether `self` orders after any of the services in the set\n        `other`.\n\n        :param other: Another service.\n        :type other: A :class:`set` of\n          :class:`aioxmpp.service.Service` instances\n\n        .. versionadded:: 0.11\n        \"\"\"\n        if not other:\n            return False\n        if visited is None:\n            visited = set()\n        elif self in visited:\n            return False\n        visited.add(self)\n        for item in self.PATCHED_ORDER_AFTER:\n            if item in visited:\n                continue\n            if item in other:\n                return True\n            if item.orders_after_any(other, visited=visited):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef independent_from(self, other):\n        if self is other:\n            return False\n        return not self.orders_after(other) and not other.orders_after(self)", "response": "Return whether the services are independent from another."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef derive_logger(self, logger):\n        parts = type(self).__module__.split(\".\")[1:]\n        if parts[-1] == \"service\" and len(parts) > 1:\n            del parts[-1]\n\n        return logger.getChild(\".\".join(\n            parts+[type(self).__qualname__]\n        ))", "response": "Derive a child name from the given logger."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_limit(self, group, new_limit):\n        if new_limit is None:\n            self._group_limits.pop(group, None)\n            return\n\n        self._group_limits[group] = new_limit", "response": "Sets the number of tasks running in a group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spawn(self, __groups, __coro_fun, *args, **kwargs):\n        # ensure the implicit group is included\n        __groups = set(__groups) | {()}\n\n        return asyncio.ensure_future(__coro_fun(*args, **kwargs))", "response": "Spawn a new coroutine and add it to the pool atomically."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend an enable message carbons.", "response": "def enable(self):\n        \"\"\"\n        Enable message carbons.\n\n        :raises RuntimeError: if the server does not support message carbons.\n        :raises aioxmpp.XMPPError: if the server responded with an error to the\n                                   request.\n        :raises: as specified in :meth:`aioxmpp.Client.send`\n        \"\"\"\n        yield from self._check_for_feature()\n\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            payload=carbons_xso.Enable()\n        )\n\n        yield from self.client.send(iq)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a disable message carbons.", "response": "def disable(self):\n        \"\"\"\n        Disable message carbons.\n\n        :raises RuntimeError: if the server does not support message carbons.\n        :raises aioxmpp.XMPPError: if the server responded with an error to the\n                                   request.\n        :raises: as specified in :meth:`aioxmpp.Client.send`\n        \"\"\"\n        yield from self._check_for_feature()\n\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            payload=carbons_xso.Disable()\n        )\n\n        yield from self.client.send(iq)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iq_handler(stream, type_, payload_cls, coro, *, with_send_reply=False):\n\n    stream.register_iq_request_handler(\n        type_,\n        payload_cls,\n        coro,\n        with_send_reply=with_send_reply,\n    )\n    try:\n        yield\n    finally:\n        stream.unregister_iq_request_handler(type_, payload_cls)", "response": "Context manager to handle IQ requests on a given IQ stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef message_handler(stream, type_, from_, cb):\n\n    stream.register_message_callback(\n        type_,\n        from_,\n        cb,\n    )\n    try:\n        yield\n    finally:\n        stream.unregister_message_callback(\n            type_,\n            from_,\n        )", "response": "Context manager for handling messages on a specific element of a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stanza_filter(filter_, func, order=_Undefined):\n    if order is not _Undefined:\n        return filter_.context_register(func, order)\n    else:\n        return filter_.context_register(func)", "response": "A filter that registers a function as a context for the given filter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef abort(self):\n        if (self._state != StanzaState.ACTIVE and\n                self._state != StanzaState.ABORTED):\n            raise RuntimeError(\"cannot abort stanza (already sent)\")\n        self._set_state(StanzaState.ABORTED)", "response": "Abort the current item in any available items."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls when the main task returns.", "response": "def _done_handler(self, task):\n        \"\"\"\n        Called when the main task (:meth:`_run`, :attr:`_task`) returns.\n        \"\"\"\n        try:\n            task.result()\n        except asyncio.CancelledError:\n            # normal termination\n            pass\n        except Exception as err:\n            try:\n                if self._sm_enabled:\n                    self._xmlstream.abort()\n                else:\n                    self._xmlstream.close()\n            except Exception:\n                pass\n            self.on_failure(err)\n            self._logger.exception(\"broker task failed\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndestroying all state which does not make sense to keep after a disconnect .", "response": "def _destroy_stream_state(self, exc):\n        \"\"\"\n        Destroy all state which does not make sense to keep after a disconnect\n        (without stream management).\n        \"\"\"\n        self._logger.debug(\"destroying stream state (exc=%r)\", exc)\n        self._iq_response_map.close_all(exc)\n        for task in self._iq_request_tasks:\n            # we don\u2019t need to remove, that\u2019s handled by their\n            # add_done_callback\n            task.cancel()\n        while not self._active_queue.empty():\n            token = self._active_queue.get_nowait()\n            token._set_state(StanzaState.DISCONNECTED)\n\n        if self._established:\n            self.on_stream_destroyed(exc)\n            self._established = False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _iq_request_coro_done_send_reply(self, request, task):\n        try:\n            payload = task.result()\n        except errors.XMPPError as err:\n            self._send_iq_reply(request, err)\n        except Exception:\n            response = self._compose_undefined_condition(request)\n            self._enqueue(response)\n            self._logger.exception(\"IQ request coroutine failed\")\n        else:\n            self._send_iq_reply(request, payload)", "response": "Called when a request handler coroutine returns send a reply to the IQ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_incoming_iq(self, stanza_obj):\n        self._logger.debug(\"incoming iq: %r\", stanza_obj)\n        if stanza_obj.type_.is_response:\n            # iq response\n            self._logger.debug(\"iq is response\")\n            keys = [(stanza_obj.from_, stanza_obj.id_)]\n            if self._local_jid is not None:\n                # needed for some servers\n                if keys[0][0] == self._local_jid:\n                    keys.append((None, keys[0][1]))\n                elif keys[0][0] is None:\n                    keys.append((self._local_jid, keys[0][1]))\n            for key in keys:\n                try:\n                    self._iq_response_map.unicast(key, stanza_obj)\n                    self._logger.debug(\"iq response delivered to key %r\", key)\n                    break\n                except KeyError:\n                    pass\n            else:\n                self._logger.warning(\n                    \"unexpected IQ response: from=%r, id=%r\",\n                    *key)\n        else:\n            # iq request\n            self._logger.debug(\"iq is request\")\n            key = (stanza_obj.type_, type(stanza_obj.payload))\n            try:\n                coro, with_send_reply = self._iq_request_map[key]\n            except KeyError:\n                self._logger.warning(\n                    \"unhandleable IQ request: from=%r, type_=%r, payload=%r\",\n                    stanza_obj.from_,\n                    stanza_obj.type_,\n                    stanza_obj.payload\n                )\n                response = stanza_obj.make_reply(type_=structs.IQType.ERROR)\n                response.error = stanza.Error(\n                    condition=errors.ErrorCondition.SERVICE_UNAVAILABLE,\n                )\n                self._enqueue(response)\n                return\n\n            args = [stanza_obj]\n            if with_send_reply:\n\n                def send_reply(result=None):\n                    nonlocal task, stanza_obj, send_reply_callback\n                    if task.done():\n                        raise RuntimeError(\n                            \"send_reply called after the handler is done\")\n                    if task.remove_done_callback(send_reply_callback) == 0:\n                        raise RuntimeError(\n                            \"send_reply called more than once\")\n                    task.add_done_callback(self._iq_request_coro_done_check)\n                    self._send_iq_reply(stanza_obj, result)\n\n                args.append(send_reply)\n\n            try:\n                awaitable = coro(*args)\n            except Exception as exc:\n                awaitable = asyncio.Future()\n                awaitable.set_exception(exc)\n\n            task = asyncio.ensure_future(awaitable)\n            send_reply_callback = functools.partial(\n                self._iq_request_coro_done_send_reply,\n                stanza_obj)\n            task.add_done_callback(self._iq_request_coro_done_remove_task)\n            task.add_done_callback(send_reply_callback)\n            self._iq_request_tasks.append(task)\n            self._logger.debug(\"started task to handle request: %r\", task)", "response": "Processes an incoming IQ stanza stanza_obj. Calls the response handler coroutine or drops the stanza while logging a warning if no handler can be found."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _process_incoming_message(self, stanza_obj):\n        self._logger.debug(\"incoming message: %r\", stanza_obj)\n\n        stanza_obj = self.service_inbound_message_filter.filter(stanza_obj)\n        if stanza_obj is None:\n            self._logger.debug(\"incoming message dropped by service \"\n                               \"filter chain\")\n            return\n\n        stanza_obj = self.app_inbound_message_filter.filter(stanza_obj)\n        if stanza_obj is None:\n            self._logger.debug(\"incoming message dropped by application \"\n                               \"filter chain\")\n            return\n\n        self.on_message_received(stanza_obj)", "response": "Process an incoming message stanza stanza_obj."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_incoming_presence(self, stanza_obj):\n        self._logger.debug(\"incoming presence: %r\", stanza_obj)\n\n        stanza_obj = self.service_inbound_presence_filter.filter(stanza_obj)\n        if stanza_obj is None:\n            self._logger.debug(\"incoming presence dropped by service filter\"\n                               \" chain\")\n            return\n\n        stanza_obj = self.app_inbound_presence_filter.filter(stanza_obj)\n        if stanza_obj is None:\n            self._logger.debug(\"incoming presence dropped by application \"\n                               \"filter chain\")\n            return\n\n        self.on_presence_received(stanza_obj)", "response": "Process an incoming presence stanza stanza_obj."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _process_incoming(self, xmlstream, queue_entry):\n\n        stanza_obj, exc = queue_entry\n\n        # first, handle SM stream objects\n        if isinstance(stanza_obj, nonza.SMAcknowledgement):\n            self._logger.debug(\"received SM ack: %r\", stanza_obj)\n            if not self._sm_enabled:\n                self._logger.warning(\"received SM ack, but SM not enabled\")\n                return\n            self.sm_ack(stanza_obj.counter)\n            return\n        elif isinstance(stanza_obj, nonza.SMRequest):\n            self._logger.debug(\"received SM request: %r\", stanza_obj)\n            if not self._sm_enabled:\n                self._logger.warning(\"received SM request, but SM not enabled\")\n                return\n            response = nonza.SMAcknowledgement()\n            response.counter = self._sm_inbound_ctr\n            self._logger.debug(\"sending SM ack: %r\", response)\n            xmlstream.send_xso(response)\n            return\n\n        # raise if it is not a stanza\n        if not isinstance(stanza_obj, stanza.StanzaBase):\n            raise RuntimeError(\n                \"unexpected stanza class: {}\".format(stanza_obj))\n\n        # now handle stanzas, these always increment the SM counter\n        if self._sm_enabled:\n            self._sm_inbound_ctr += 1\n            self._sm_inbound_ctr &= 0xffffffff\n\n        # check if the stanza has errors\n        if exc is not None:\n            self._process_incoming_erroneous_stanza(stanza_obj, exc)\n            return\n\n        if isinstance(stanza_obj, stanza.IQ):\n            self._process_incoming_iq(stanza_obj)\n        elif isinstance(stanza_obj, stanza.Message):\n            self._process_incoming_message(stanza_obj)\n        elif isinstance(stanza_obj, stanza.Presence):\n            self._process_incoming_presence(stanza_obj)", "response": "Process incoming stanza objects and return the unique ID of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flush_incoming(self):\n        while True:\n            try:\n                stanza_obj = self._incoming_queue.get_nowait()\n            except asyncio.QueueEmpty:\n                break\n            self._process_incoming(None, stanza_obj)", "response": "Flushes all incoming queues to the respective processing methods."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a stanza token over the given xmlstream.", "response": "def _send_stanza(self, xmlstream, token):\n        \"\"\"\n        Send a stanza token `token` over the given `xmlstream`.\n\n        Only sends if the `token` has not been aborted (see\n        :meth:`StanzaToken.abort`). Sends the state of the token acoording to\n        :attr:`sm_enabled`.\n        \"\"\"\n        if token.state == StanzaState.ABORTED:\n            return\n\n        stanza_obj = token.stanza\n\n        if isinstance(stanza_obj, stanza.Presence):\n            stanza_obj = self.app_outbound_presence_filter.filter(\n                stanza_obj\n            )\n            if stanza_obj is not None:\n                stanza_obj = self.service_outbound_presence_filter.filter(\n                    stanza_obj\n                )\n        elif isinstance(stanza_obj, stanza.Message):\n            stanza_obj = self.app_outbound_message_filter.filter(\n                stanza_obj\n            )\n            if stanza_obj is not None:\n                stanza_obj = self.service_outbound_message_filter.filter(\n                    stanza_obj\n                )\n\n        if stanza_obj is None:\n            token._set_state(StanzaState.DROPPED)\n            self._logger.debug(\"outgoing stanza %r dropped by filter chain\",\n                               token.stanza)\n            return\n\n        self._logger.debug(\"forwarding stanza to xmlstream: %r\",\n                           stanza_obj)\n\n        try:\n            xmlstream.send_xso(stanza_obj)\n        except Exception as exc:\n            self._logger.warning(\"failed to send stanza\", exc_info=True)\n            token._set_state(StanzaState.FAILED, exc)\n            return\n\n        if self._sm_enabled:\n            token._set_state(StanzaState.SENT)\n            self._sm_unacked_list.append(token)\n        else:\n            token._set_state(StanzaState.SENT_WITHOUT_SM)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses the current outgoing stanza token and also any other outgoing stanza which is currently in the active queue.", "response": "def _process_outgoing(self, xmlstream, token):\n        \"\"\"\n        Process the current outgoing stanza `token` and also any other outgoing\n        stanza which is currently in the active queue. After all stanzas have\n        been processed, use :meth:`_send_ping` to allow an opportunistic ping\n        to be sent.\n        \"\"\"\n\n        self._send_stanza(xmlstream, token)\n        # try to send a bulk\n        while True:\n            try:\n                token = self._active_queue.get_nowait()\n            except asyncio.QueueEmpty:\n                break\n            self._send_stanza(xmlstream, token)\n\n        if self._sm_enabled:\n            self._logger.debug(\"sending SM req\")\n            xmlstream.send_xso(nonza.SMRequest())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_iq_response_callback(self, from_, id_, cb):\n\n        self._iq_response_map.add_listener(\n            (from_, id_),\n            callbacks.OneshotAsyncTagListener(cb, loop=self._loop)\n        )\n        self._logger.debug(\"iq response callback registered: from=%r, id=%r\",\n                           from_, id_)", "response": "Register a callback function to be called when a IQ stanza with the given id is recieved from the given from."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_iq_response_future(self, from_, id_, fut):\n\n        self._iq_response_map.add_listener(\n            (from_, id_),\n            StanzaErrorAwareListener(\n                callbacks.FutureListener(fut)\n            )\n        )\n        self._logger.debug(\"iq response future registered: from=%r, id=%r\",\n                           from_, id_)", "response": "Register a future for an IQ stanza with type result or error from the given id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unregister_iq_response(self, from_, id_):\n        self._iq_response_map.remove_listener((from_, id_))\n        self._logger.debug(\"iq response unregistered: from=%r, id=%r\",\n                           from_, id_)", "response": "Unregister a registered callback or future for the IQ response identified by from_ and id_."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_iq_request_coro(self, type_, payload_cls, coro):\n        warnings.warn(\n            \"register_iq_request_coro is a deprecated alias to \"\n            \"register_iq_request_handler and will be removed in aioxmpp 1.0\",\n            DeprecationWarning,\n            stacklevel=2)\n        return self.register_iq_request_handler(type_, payload_cls, coro)", "response": "Register a new request handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_iq_request_handler(self, type_, payload_cls, cb, *,\n                                    with_send_reply=False):\n        \"\"\"\n        Register a coroutine function or a function returning an awaitable to\n        run when an IQ request is received.\n\n        :param type_: IQ type to react to (must be a request type).\n        :type type_: :class:`~aioxmpp.IQType`\n        :param payload_cls: Payload class to react to (subclass of\n            :class:`~xso.XSO`)\n        :type payload_cls: :class:`~.XMLStreamClass`\n        :param cb: Function or coroutine function to invoke\n        :param with_send_reply: Whether to pass a function to send a reply\n             to `cb` as second argument.\n        :type with_send_reply: :class:`bool`\n        :raises ValueError: if there is already a coroutine registered for this\n                            target\n        :raises ValueError: if `type_` is not a request IQ type\n        :raises ValueError: if `type_` is not a valid\n                            :class:`~.IQType` (and cannot be cast to a\n                            :class:`~.IQType`)\n\n        The callback `cb` will be called whenever an IQ stanza with the given\n        `type_` and payload being an instance of the `payload_cls` is received.\n\n        The callback must either be a coroutine function or otherwise return an\n        awaitable. The awaitable must evaluate to a valid value for the\n        :attr:`.IQ.payload` attribute. That value will be set as the payload\n        attribute value of an IQ response (with type :attr:`~.IQType.RESULT`)\n        which is generated and sent by the stream.\n\n        If the awaitable or the function raises an exception, it will be\n        converted to a :class:`~.stanza.Error` object. That error object is\n        then used as payload for an IQ response (with type\n        :attr:`~.IQType.ERROR`) which is generated and sent by the stream.\n\n        If the exception is a subclass of :class:`aioxmpp.errors.XMPPError`, it\n        is converted to an :class:`~.stanza.Error` instance directly.\n        Otherwise, it is wrapped in a :class:`aioxmpp.XMPPCancelError`\n        with ``undefined-condition``.\n\n        For this to work, `payload_cls` *must* be registered using\n        :meth:`~.IQ.as_payload_class`. Otherwise, the payload will\n        not be recognised by the stream parser and the IQ is automatically\n        responded to with a ``feature-not-implemented`` error.\n\n        .. warning::\n\n            When using a coroutine function for `cb`, there is no guarantee\n            that concurrent IQ handlers and other coroutines will execute in\n            any defined order. This implies that the strong ordering guarantees\n            normally provided by XMPP XML Streams are lost when using coroutine\n            functions for `cb`. For this reason, the use of non-coroutine\n            functions is allowed.\n\n        .. note::\n\n            Using a non-coroutine function for `cb` will generally lead to\n            less readable code. For the sake of readability, it is recommended\n            to prefer coroutine functions when strong ordering guarantees are\n            not needed.\n\n        .. versionadded:: 0.11\n\n            When the argument `with_send_reply` is true `cb` will be\n            called with two arguments: the IQ stanza to handle and a\n            unary function `send_reply(result=None)` that sends a\n            response to the IQ request and prevents that an automatic\n            response is sent. If `result` is an instance of\n            :class:`~aioxmpp.XMPPError` an error result is generated.\n\n            This is useful when the handler function needs to execute\n            actions which happen after the IQ result has been sent,\n            for example, sending other stanzas.\n\n        .. versionchanged:: 0.10\n\n            Accepts an awaitable as last argument in addition to coroutine\n            functions.\n\n            Renamed from :meth:`register_iq_request_coro`.\n\n        .. versionadded:: 0.6\n\n           If the stream is :meth:`stop`\\\\ -ped (only if SM is not enabled) or\n           :meth:`close`\\\\ ed, running IQ response coroutines are\n           :meth:`asyncio.Task.cancel`\\\\ -led.\n\n           To protect against that, fork from your coroutine using\n           :func:`asyncio.ensure_future`.\n\n        .. versionchanged:: 0.7\n\n           The `type_` argument is now supposed to be a :class:`~.IQType`\n           member.\n\n        .. deprecated:: 0.7\n\n           Passing a :class:`str` as `type_` argument is deprecated and will\n           raise a :class:`TypeError` as of the 1.0 release. See the Changelog\n           for :ref:`api-changelog-0.7` for further details on how to upgrade\n           your code efficiently.\n        \"\"\"\n        type_ = self._coerce_enum(type_, structs.IQType)\n        if not type_.is_request:\n            raise ValueError(\n                \"{!r} is not a request IQType\".format(type_)\n            )\n\n        key = type_, payload_cls\n\n        if key in self._iq_request_map:\n            raise ValueError(\"only one listener is allowed per tag\")\n\n        self._iq_request_map[key] = cb, with_send_reply\n        self._logger.debug(\n            \"iq request coroutine registered: type=%r, payload=%r\",\n            type_, payload_cls)", "response": "Register a callback function or a coroutine function that will be called when an IQ request is received."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unregister_iq_request_handler(self, type_, payload_cls):\n        type_ = self._coerce_enum(type_, structs.IQType)\n        del self._iq_request_map[type_, payload_cls]\n        self._logger.debug(\n            \"iq request coroutine unregistered: type=%r, payload=%r\",\n            type_, payload_cls)", "response": "Unregisters a coroutine previously registered with register_iq_request_handler."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a callback function to be called when a message is received.", "response": "def register_message_callback(self, type_, from_, cb):\n        \"\"\"\n        Register a callback to be called when a message is received.\n\n        :param type_: Message type to listen for, or :data:`None` for a\n                      wildcard match.\n        :type type_: :class:`~.MessageType` or :data:`None`\n        :param from_: Sender JID to listen for, or :data:`None` for a wildcard\n                      match.\n        :type from_: :class:`~aioxmpp.JID` or :data:`None`\n        :param cb: Callback function to call\n        :raises ValueError: if another function is already registered for the\n                            same ``(type_, from_)`` pair.\n        :raises ValueError: if `type_` is not a valid\n                            :class:`~.MessageType` (and cannot be cast\n                            to a :class:`~.MessageType`)\n\n        `cb` will be called whenever a message stanza matching the `type_` and\n        `from_` is received, according to the wildcarding rules below. More\n        specific callbacks win over less specific callbacks, and the match on\n        the `from_` address takes precedence over the match on the `type_`.\n\n        See :meth:`.SimpleStanzaDispatcher.register_callback` for the exact\n        wildcarding rules.\n\n        .. versionchanged:: 0.7\n\n           The `type_` argument is now supposed to be a\n           :class:`~.MessageType` member.\n\n        .. deprecated:: 0.7\n\n           Passing a :class:`str` as `type_` argument is deprecated and will\n           raise a :class:`TypeError` as of the 1.0 release. See the Changelog\n           for :ref:`api-changelog-0.7` for further details on how to upgrade\n           your code efficiently.\n\n        .. deprecated:: 0.9\n\n           This method has been deprecated in favour of and is now implemented\n           in terms of the :class:`aioxmpp.dispatcher.SimpleMessageDispatcher`\n           service.\n\n           It is equivalent to call\n           :meth:`~.SimpleStanzaDispatcher.register_callback`, except that the\n           latter is not deprecated.\n        \"\"\"\n        if type_ is not None:\n            type_ = self._coerce_enum(type_, structs.MessageType)\n        warnings.warn(\n            \"register_message_callback is deprecated; use \"\n            \"aioxmpp.dispatcher.SimpleMessageDispatcher instead\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        self._xxx_message_dispatcher.register_callback(\n            type_,\n            from_,\n            cb,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a callback function to be called when a presence stanza is received from the specified sender.", "response": "def register_presence_callback(self, type_, from_, cb):\n        \"\"\"\n        Register a callback to be called when a presence stanza is received.\n\n        :param type_: Presence type to listen for.\n        :type type_: :class:`~.PresenceType`\n        :param from_: Sender JID to listen for, or :data:`None` for a wildcard\n                      match.\n        :type from_: :class:`~aioxmpp.JID` or :data:`None`.\n        :param cb: Callback function\n        :raises ValueError: if another listener with the same ``(type_,\n                            from_)`` pair is already registered\n        :raises ValueError: if `type_` is not a valid\n                            :class:`~.PresenceType` (and cannot be cast\n                            to a :class:`~.PresenceType`)\n\n        `cb` will be called whenever a presence stanza matching the `type_` is\n        received from the specified sender. `from_` may be :data:`None` to\n        indicate a wildcard. Like with :meth:`register_message_callback`, more\n        specific callbacks win over less specific callbacks. The fallback order\n        is identical, except that the ``type_=None`` entries described there do\n        not apply for presence stanzas and are thus omitted.\n\n        See :meth:`.SimpleStanzaDispatcher.register_callback` for the exact\n        wildcarding rules.\n\n        .. versionchanged:: 0.7\n\n           The `type_` argument is now supposed to be a\n           :class:`~.PresenceType` member.\n\n        .. deprecated:: 0.7\n\n           Passing a :class:`str` as `type_` argument is deprecated and will\n           raise a :class:`TypeError` as of the 1.0 release. See the Changelog\n           for :ref:`api-changelog-0.7` for further details on how to upgrade\n           your code efficiently.\n\n        .. deprecated:: 0.9\n\n           This method has been deprecated. It is recommended to use\n           :class:`aioxmpp.PresenceClient` instead.\n\n        \"\"\"\n        type_ = self._coerce_enum(type_, structs.PresenceType)\n        warnings.warn(\n            \"register_presence_callback is deprecated; use \"\n            \"aioxmpp.dispatcher.SimplePresenceDispatcher or \"\n            \"aioxmpp.PresenceClient instead\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        self._xxx_presence_dispatcher.register_callback(\n            type_,\n            from_,\n            cb,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart or resume the given XMLStream.", "response": "def start(self, xmlstream):\n        \"\"\"\n        Start or resume the stanza stream on the given\n        :class:`aioxmpp.protocol.XMLStream` `xmlstream`.\n\n        This starts the main broker task, registers stanza classes at the\n        `xmlstream` .\n        \"\"\"\n\n        if self.running:\n            raise RuntimeError(\"already started\")\n\n        self._start_prepare(xmlstream, self.recv_stanza)\n        self._closed = False\n        self._start_commit(xmlstream)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstop the broker task.", "response": "def stop(self):\n        \"\"\"\n        Send a signal to the main broker task to terminate. You have to check\n        :attr:`running` and possibly wait for it to become :data:`False` ---\n        the task takes at least one loop through the event loop to terminate.\n\n        It is guarenteed that the task will not attempt to send stanzas over\n        the existing `xmlstream` after a call to :meth:`stop` has been made.\n\n        It is legal to call :meth:`stop` even if the task is already\n        stopped. It is a no-op in that case.\n        \"\"\"\n        if not self.running:\n            return\n        self._logger.debug(\"sending stop signal to task\")\n        self._task.cancel()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstopping the stream and wait for it to stop.", "response": "def wait_stop(self):\n        \"\"\"\n        Stop the stream and wait for it to stop.\n\n        See :meth:`stop` for the general stopping conditions. You can assume\n        that :meth:`stop` is the first thing this coroutine calls.\n        \"\"\"\n        if not self.running:\n            return\n        self.stop()\n        try:\n            yield from self._task\n        except asyncio.CancelledError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        exc = DestructionRequested(\"close() called\")\n\n        if self.running:\n            if self.sm_enabled:\n                self._xmlstream.send_xso(nonza.SMAcknowledgement(\n                    counter=self._sm_inbound_ctr\n                ))\n\n            yield from self._xmlstream.close_and_wait()  # does not raise\n            yield from self.wait_stop()  # may raise\n\n        self._closed = True\n        self._xmlstream_exception = exc\n        self._destroy_stream_state(self._xmlstream_exception)\n        if self.sm_enabled:\n            self.stop_sm()", "response": "Closes the underlying XML stream and the underlying XML stream itself."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the stream management on the specified node.", "response": "def start_sm(self, request_resumption=True, resumption_timeout=None):\n        \"\"\"\n        Start stream management (version 3).\n\n        :param request_resumption: Request that the stream shall be resumable.\n        :type request_resumption: :class:`bool`\n        :param resumption_timeout: Maximum time in seconds for a stream to be\n            resumable.\n        :type resumption_timeout: :class:`int`\n        :raises aioxmpp.errors.StreamNegotiationFailure: if the server rejects\n            the attempt to enable stream management.\n\n        This method attempts to starts stream management on the stream.\n\n        `resumption_timeout` is the ``max`` attribute on\n        :class:`.nonza.SMEnabled`; it can be used to set a maximum time for\n        which the server shall consider the stream to still be alive after the\n        underlying transport (TCP) has failed. The server may impose its own\n        maximum or ignore the request, so there are no guarentees that the\n        session will stay alive for at most or at least `resumption_timeout`\n        seconds. Passing a `resumption_timeout` of 0 is equivalent to passing\n        false to `request_resumption` and takes precedence over\n        `request_resumption`.\n\n        .. note::\n\n            In addition to server implementation details, it is very well\n            possible that the server does not even detect that the underlying\n            transport has failed for quite some time for various reasons\n            (including high TCP timeouts).\n\n        If the server rejects the attempt to enable stream management, a\n        :class:`.errors.StreamNegotiationFailure` is raised. The stream is\n        still running in that case.\n\n        .. warning::\n\n           This method cannot and does not check whether the server advertised\n           support for stream management. Attempting to negotiate stream\n           management without server support might lead to termination of the\n           stream.\n\n        If an XML stream error occurs during the negotiation, the result\n        depends on a few factors. In any case, the stream is not running\n        afterwards. If the :class:`.nonza.SMEnabled` response was not received\n        before the XML stream died, SM is also disabled and the exception which\n        caused the stream to die is re-raised (this is due to the\n        implementation of :func:`~.protocol.send_and_wait_for`). If the\n        :class:`.nonza.SMEnabled` response was received and annonuced support\n        for resumption, SM is enabled. Otherwise, it is disabled. No exception\n        is raised if :class:`.nonza.SMEnabled` was received, as this method has\n        no way to determine that the stream failed.\n\n        If negotiation succeeds, this coroutine initializes a new stream\n        management session. The stream management state attributes become\n        available and :attr:`sm_enabled` becomes :data:`True`.\n        \"\"\"\n        if not self.running:\n            raise RuntimeError(\"cannot start Stream Management while\"\n                               \" StanzaStream is not running\")\n        if self.sm_enabled:\n            raise RuntimeError(\"Stream Management already enabled\")\n\n        if resumption_timeout == 0:\n            request_resumption = False\n            resumption_timeout = None\n\n        # sorry for the callback spaghetti code\n        # we have to handle the response synchronously, so we have to use a\n        # callback.\n        # otherwise, it is possible that an SM related nonza (e.g. <r/>) is\n        # received (and attempted to be deserialized) before the handlers are\n        # registered\n        # see tests/test_highlevel.py:TestProtocoltest_sm_bootstrap_race\n        def handle_response(response):\n            if isinstance(response, nonza.SMFailed):\n                # we handle the error down below\n                return\n\n            self._sm_outbound_base = 0\n            self._sm_inbound_ctr = 0\n            self._sm_unacked_list = []\n            self._sm_enabled = True\n            self._sm_id = response.id_\n            self._sm_resumable = response.resume\n            self._sm_max = response.max_\n            self._sm_location = response.location\n\n            self._logger.info(\"SM started: resumable=%s, stream id=%r\",\n                              self._sm_resumable,\n                              self._sm_id)\n\n            # if not self._xmlstream:\n            #     # stream died in the meantime...\n            #     if self._xmlstream_exception:\n            #         raise self._xmlstream_exception\n\n            self._xmlstream.stanza_parser.add_class(\n                nonza.SMRequest,\n                self.recv_stanza)\n            self._xmlstream.stanza_parser.add_class(\n                nonza.SMAcknowledgement,\n                self.recv_stanza)\n\n        with (yield from self._broker_lock):\n            response = yield from protocol.send_and_wait_for(\n                self._xmlstream,\n                [\n                    nonza.SMEnable(resume=bool(request_resumption),\n                                   max_=resumption_timeout),\n                ],\n                [\n                    nonza.SMEnabled,\n                    nonza.SMFailed\n                ],\n                cb=handle_response,\n            )\n\n            if isinstance(response, nonza.SMFailed):\n                raise errors.StreamNegotiationFailure(\n                    \"Server rejected SM request\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresumes a previously paused SM stream.", "response": "def _resume_sm(self, remote_ctr):\n        \"\"\"\n        Version of :meth:`resume_sm` which can be used during slow start.\n        \"\"\"\n        self._logger.info(\"resuming SM stream with remote_ctr=%d\", remote_ctr)\n        # remove any acked stanzas\n        self.sm_ack(remote_ctr)\n        # reinsert the remaining stanzas\n        for token in self._sm_unacked_list:\n            self._active_queue.putleft_nowait(token)\n        self._sm_unacked_list.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresuming an SM - enabled stream using the given XML stream.", "response": "def resume_sm(self, xmlstream):\n        \"\"\"\n        Resume an SM-enabled stream using the given `xmlstream`.\n\n        If the server rejects the attempt to resume stream management, a\n        :class:`.errors.StreamNegotiationFailure` is raised. The stream is then\n        in stopped state and stream management has been stopped.\n\n        .. warning::\n\n           This method cannot and does not check whether the server advertised\n           support for stream management. Attempting to negotiate stream\n           management without server support might lead to termination of the\n           stream.\n\n        If the XML stream dies at any point during the negotiation, the SM\n        state is left unchanged. If no response has been received yet, the\n        exception which caused the stream to die is re-raised. The state of the\n        stream depends on whether the main task already noticed the dead\n        stream.\n\n        If negotiation succeeds, this coroutine resumes the stream management\n        session and initiates the retransmission of any unacked stanzas. The\n        stream is then in running state.\n\n        .. versionchanged:: 0.11\n\n            Support for using the counter value provided some servers on a\n            failed resumption was added. Stanzas which are covered by the\n            counter will be marked as :attr:`~StanzaState.ACKED`; other stanzas\n            will be marked as :attr:`~StanzaState.DISCONNECTED`.\n\n            This is in contrast to the behaviour when resumption fails\n            *without* a counter given. In that case, stanzas which have not\n            been acked are marked as :attr:`~StanzaState.SENT_WITHOUT_SM`.\n        \"\"\"\n\n        if self.running:\n            raise RuntimeError(\"Cannot resume Stream Management while\"\n                               \" StanzaStream is running\")\n\n        self._start_prepare(xmlstream, self.recv_stanza)\n        try:\n            response = yield from protocol.send_and_wait_for(\n                xmlstream,\n                [\n                    nonza.SMResume(previd=self.sm_id,\n                                   counter=self._sm_inbound_ctr)\n                ],\n                [\n                    nonza.SMResumed,\n                    nonza.SMFailed\n                ]\n            )\n\n            if isinstance(response, nonza.SMFailed):\n                exc = errors.StreamNegotiationFailure(\n                    \"Server rejected SM resumption\"\n                )\n\n                if response.counter is not None:\n                    self.sm_ack(response.counter)\n                    self._clear_unacked(StanzaState.DISCONNECTED)\n\n                xmlstream.stanza_parser.remove_class(\n                    nonza.SMRequest)\n                xmlstream.stanza_parser.remove_class(\n                    nonza.SMAcknowledgement)\n                self.stop_sm()\n                raise exc\n\n            self._resume_sm(response.counter)\n        except:  # NOQA\n            self._start_rollback(xmlstream)\n            raise\n        self._start_commit(xmlstream)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _stop_sm(self):\n        if not self.sm_enabled:\n            raise RuntimeError(\"Stream Management is not enabled\")\n\n        self._logger.info(\"stopping SM stream\")\n        self._sm_enabled = False\n        del self._sm_outbound_base\n        del self._sm_inbound_ctr\n        self._clear_unacked(StanzaState.SENT_WITHOUT_SM)\n        del self._sm_unacked_list\n\n        self._destroy_stream_state(ConnectionError(\n            \"stream management disabled\"\n        ))", "response": "Stop SM stream management."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sm_ack(self, remote_ctr):\n\n        if not self._sm_enabled:\n            raise RuntimeError(\"Stream Management is not enabled\")\n\n        self._logger.debug(\"sm_ack(%d)\", remote_ctr)\n        to_drop = (remote_ctr - self._sm_outbound_base) & 0xffffffff\n        self._logger.debug(\"sm_ack: to drop %d, unacked: %d\",\n                           to_drop, len(self._sm_unacked_list))\n        if to_drop > len(self._sm_unacked_list):\n            raise errors.StreamNegotiationFailure(\n                \"acked more stanzas than have been sent \"\n                \"(outbound_base={}, remote_ctr={})\".format(\n                    self._sm_outbound_base,\n                    remote_ctr\n                )\n            )\n\n        acked = self._sm_unacked_list[:to_drop]\n        del self._sm_unacked_list[:to_drop]\n        self._sm_outbound_base = remote_ctr\n\n        if acked:\n            self._logger.debug(\"%d stanzas acked by remote\", len(acked))\n        for token in acked:\n            token._set_state(StanzaState.ACKED)", "response": "Process the acked stanzas from the local list and put them into the ACKED state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend an IQ stanza and wait for the response.", "response": "def send_iq_and_wait_for_reply(self, iq, *,\n                                   timeout=None):\n        \"\"\"\n        Send an IQ stanza `iq` and wait for the response. If `timeout` is not\n        :data:`None`, it must be the time in seconds for which to wait for a\n        response.\n\n        If the response is a ``\"result\"`` IQ, the value of the\n        :attr:`~aioxmpp.IQ.payload` attribute is returned. Otherwise,\n        the exception generated from the :attr:`~aioxmpp.IQ.error`\n        attribute is raised.\n\n        .. seealso::\n\n           :meth:`register_iq_response_future` and\n           :meth:`send_and_wait_for_sent` for other cases raising exceptions.\n\n        .. deprecated:: 0.8\n\n           This method will be removed in 1.0. Use :meth:`send` instead.\n\n        .. versionchanged:: 0.8\n\n           On a timeout, :class:`TimeoutError` is now raised instead of\n           :class:`asyncio.TimeoutError`.\n\n        \"\"\"\n        warnings.warn(\n            r\"send_iq_and_wait_for_reply is deprecated and will be removed in\"\n            r\" 1.0\",\n            DeprecationWarning,\n            stacklevel=1,\n        )\n        return (yield from self.send(iq, timeout=timeout))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends the given stanza over the given stanza stream.", "response": "def send_and_wait_for_sent(self, stanza):\n        \"\"\"\n        Send the given `stanza` over the given :class:`StanzaStream` `stream`.\n\n        .. deprecated:: 0.8\n\n           This method will be removed in 1.0. Use :meth:`send` instead.\n        \"\"\"\n        warnings.warn(\n            r\"send_and_wait_for_sent is deprecated and will be removed in 1.0\",\n            DeprecationWarning,\n            stacklevel=1,\n        )\n        yield from self._enqueue(stanza)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _send_immediately(self, stanza, *, timeout=None, cb=None):\n        stanza.autoset_id()\n        self._logger.debug(\"sending %r and waiting for it to be sent\",\n                           stanza)\n\n        if not isinstance(stanza, stanza_.IQ) or stanza.type_.is_response:\n            if cb is not None:\n                raise ValueError(\n                    \"cb not supported with non-IQ non-request stanzas\"\n                )\n            yield from self._enqueue(stanza)\n            return\n\n        # we use the long way with a custom listener instead of a future here\n        # to ensure that the callback is called synchronously from within the\n        # queue handling loop.\n        # we need that to ensure that the strong ordering guarantees reach the\n        # `cb` function.\n\n        fut = asyncio.Future()\n\n        def nested_cb(task):\n            \"\"\"\n            This callback is used to handle awaitables returned by the `cb`.\n            \"\"\"\n            nonlocal fut\n            if task.exception() is None:\n                fut.set_result(task.result())\n            else:\n                fut.set_exception(task.exception())\n\n        def handler_ok(stanza):\n            \"\"\"\n            This handler is invoked synchronously by\n            :meth:`_process_incoming_iq` (via\n            :class:`aioxmpp.callbacks.TagDispatcher`) for response stanzas\n            (including error stanzas).\n            \"\"\"\n            nonlocal fut\n            if fut.cancelled():\n                return\n\n            if cb is not None:\n                try:\n                    nested_fut = cb(stanza)\n                except Exception as exc:\n                    fut.set_exception(exc)\n                else:\n                    if nested_fut is not None:\n                        nested_fut.add_done_callback(nested_cb)\n                        return\n\n            # we can\u2019t even use StanzaErrorAwareListener because we want to\n            # forward error stanzas to the cb too...\n            if stanza.type_.is_error:\n                fut.set_exception(stanza.error.to_exception())\n            else:\n                fut.set_result(stanza.payload)\n\n        def handler_error(exc):\n            \"\"\"\n            This handler is invoked synchronously by\n            :meth:`_process_incoming_iq` (via\n            :class:`aioxmpp.callbacks.TagDispatcher`) for response errors (\n            such as parsing errors, connection errors, etc.).\n            \"\"\"\n            nonlocal fut\n            if fut.cancelled():\n                return\n            fut.set_exception(exc)\n\n        listener = callbacks.OneshotTagListener(\n            handler_ok,\n            handler_error,\n        )\n        listener_tag = (stanza.to, stanza.id_)\n\n        self._iq_response_map.add_listener(\n            listener_tag,\n            listener,\n        )\n\n        try:\n            yield from self._enqueue(stanza)\n        except Exception:\n            listener.cancel()\n            raise\n\n        try:\n            if not timeout:\n                reply = yield from fut\n            else:\n                try:\n                    reply = yield from asyncio.wait_for(\n                        fut,\n                        timeout=timeout\n                    )\n                except asyncio.TimeoutError:\n                    raise TimeoutError\n        finally:\n            try:\n                self._iq_response_map.remove_listener(listener_tag)\n            except KeyError:\n                pass\n\n        return reply", "response": "Send a stanza and wait for it to be ready to send."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the features string from an iterable of features.", "response": "def _process_features(features):\n    \"\"\"\n    Generate the `Features String` from an iterable of features.\n\n    :param features: The features to generate the features string from.\n    :type features: :class:`~collections.abc.Iterable` of :class:`str`\n    :return: The `Features String`\n    :rtype: :class:`bytes`\n\n    Generate the `Features String` from the given `features` as specified in\n    :xep:`390`.\n    \"\"\"\n    parts = [\n        feature.encode(\"utf-8\")+b\"\\x1f\"\n        for feature in features\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the Identity String from an iterable of identities.", "response": "def _process_identities(identities):\n    \"\"\"\n    Generate the `Identities String` from an iterable of identities.\n\n    :param identities: The identities to generate the features string from.\n    :type identities: :class:`~collections.abc.Iterable` of\n        :class:`~.disco.xso.Identity`\n    :return: The `Identities String`\n    :rtype: :class:`bytes`\n\n    Generate the `Identities String` from the given `identities` as specified\n    in :xep:`390`.\n    \"\"\"\n    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\""}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_extensions(exts):\n    parts = [\n        _process_form(form)\n        for form in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "response": "Generates the Extensions String from an iterable of data forms."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_toplevel_object(self, instance, class_=None):\n        if class_ is None:\n            class_ = type(instance)\n        self._toplevels[class_] = instance", "response": "Set the toplevel object to return from get_toplevel_object when class_ is not None."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nevaluating the expression expr and return the truthness of its result.", "response": "def eval_bool(self, expr):\n        \"\"\"\n        Evaluate the expression `expr` and return the truthness of its result.\n        A result of an expression is said to be true if it contains at least\n        one value. It has the same semantics as :func:`bool` on sequences.s\n        \"\"\"\n        result = expr.eval(self)\n        iterator = iter(result)\n        try:\n            next(iterator)\n        except StopIteration:\n            return False\n        else:\n            return True\n        finally:\n            if hasattr(iterator, \"close\"):\n                iterator.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rewind(self, new_state):\n        if new_state > self._state:\n            raise ValueError(\"cannot forward using rewind \"\n                             \"({} > {})\".format(new_state, self._state))\n        self._state = new_state", "response": "Rewind the state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state of the internal state."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits for the state new_state to be reached by the state machine.", "response": "def wait_for(self, new_state):\n        \"\"\"\n        Wait for an exact state `new_state` to be reached by the state\n        machine.\n\n        If the state is skipped, that is, if a state which is greater than\n        `new_state` is written to :attr:`state`, the coroutine raises\n        :class:`OrderedStateSkipped` exception as it is not possible anymore\n        that it can return successfully (see :attr:`state`).\n        \"\"\"\n        if self._state == new_state:\n            return\n\n        if self._state > new_state:\n            raise OrderedStateSkipped(new_state)\n\n        fut = asyncio.Future(loop=self.loop)\n        self._exact_waiters.append((new_state, fut))\n        yield from fut"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait_for_at_least(self, new_state):\n        if not (self._state < new_state):\n            return\n\n        fut = asyncio.Future(loop=self.loop)\n        self._least_waiters.append((new_state, fut))\n        yield from fut", "response": "Wait for a state to be entered which is greater than or equal to new_state and return."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering the given class `cls` as Publish-Subscribe payload on both :class:`Item` and :class:`EventItem`. Return the class, to allow this to be used as decorator.", "response": "def as_payload_class(cls):\n    \"\"\"\n    Register the given class `cls` as Publish-Subscribe payload on both\n    :class:`Item` and :class:`EventItem`.\n\n    Return the class, to allow this to be used as decorator.\n    \"\"\"\n\n    Item.register_child(\n        Item.registered_payload,\n        cls,\n    )\n\n    EventItem.register_child(\n        EventItem.registered_payload,\n        cls,\n    )\n\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the presence stanza describing the most available presence of the peer_jid.", "response": "def get_most_available_stanza(self, peer_jid):\n        \"\"\"\n        Obtain the stanza describing the most-available presence of the\n        contact.\n\n        :param peer_jid: Bare JID of the contact.\n        :type peer_jid: :class:`aioxmpp.JID`\n        :rtype: :class:`aioxmpp.Presence` or :data:`None`\n        :return: The presence stanza of the most available resource or\n                 :data:`None` if there is no available resource.\n\n        The \"most available\" resource is the one whose presence state orderest\n        highest according to :class:`~aioxmpp.PresenceState`.\n\n        If there is no available resource for a given `peer_jid`, :data:`None`\n        is returned.\n        \"\"\"\n        presences = sorted(\n            self.get_peer_resources(peer_jid).items(),\n            key=lambda item: aioxmpp.structs.PresenceState.from_stanza(item[1])\n        )\n        if not presences:\n            return None\n        return presences[-1][1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_peer_resources(self, peer_jid):\n        try:\n            d = dict(self._presences[peer_jid])\n            d.pop(None, None)\n            return d\n        except KeyError:\n            return {}", "response": "Returns a dict mapping resource IDs to the presence states of the given peer_jid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the last presence recieved for the given bare or full peer_jid.", "response": "def get_stanza(self, peer_jid):\n        \"\"\"\n        Return the last presence recieved for the given bare or full\n        `peer_jid`. If the last presence was unavailable, the return value is\n        :data:`None`, as if no presence was ever received.\n\n        If no presence was ever received for the given bare JID, :data:`None`\n        is returned.\n        \"\"\"\n        try:\n            return self._presences[peer_jid.bare()][peer_jid.resource]\n        except KeyError:\n            pass\n        try:\n            return self._presences[peer_jid.bare()][None]\n        except KeyError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate and return a presence stanza with the current settings.", "response": "def make_stanza(self):\n        \"\"\"\n        Create and return a presence stanza with the current settings.\n\n        :return: Presence stanza\n        :rtype: :class:`aioxmpp.Presence`\n        \"\"\"\n        stanza = aioxmpp.Presence()\n        self._state.apply_to_stanza(stanza)\n        stanza.status.update(self._status)\n        return stanza"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_presence(self, state, status={}, priority=0):\n\n        if not isinstance(priority, numbers.Integral):\n            raise TypeError(\n                \"invalid priority: got {}, expected integer\".format(\n                    type(priority)\n                )\n            )\n\n        if not isinstance(state, aioxmpp.PresenceState):\n            raise TypeError(\n                \"invalid state: got {}, expected aioxmpp.PresenceState\".format(\n                    type(state),\n                )\n            )\n\n        if isinstance(status, str):\n            new_status = {None: status}\n        else:\n            new_status = dict(status)\n        new_priority = int(priority)\n\n        emit_state_event = self._state != state\n        emit_overall_event = (\n            emit_state_event or\n            self._priority != new_priority or\n            self._status != new_status\n        )\n\n        self._state = state\n        self._status = new_status\n        self._priority = new_priority\n\n        if emit_state_event:\n            self.on_presence_state_changed()\n        if emit_overall_event:\n            self.on_presence_changed()\n            return self.resend_presence()", "response": "Change the presence of the resource at the given priority."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resend_presence(self):\n\n        if self.client.established:\n            return self.client.enqueue(self.make_stanza())", "response": "Re - send the currently configured presence stanza."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_python_dict_from_x509(x509):\n    result = {\n        \"subject\": (\n            ((\"commonName\", x509.get_subject().commonName),),\n        )\n    }\n\n    for ext_idx in range(x509.get_extension_count()):\n        ext = x509.get_extension(ext_idx)\n        sn = ext.get_short_name()\n        if sn != b\"subjectAltName\":\n            continue\n\n        data = pyasn1.codec.der.decoder.decode(\n            ext.get_data(),\n            asn1Spec=pyasn1_modules.rfc2459.SubjectAltName())[0]\n        for name in data:\n            dNSName = name.getComponentByPosition(2)\n            if dNSName is None:\n                continue\n\n            if hasattr(dNSName, \"isValue\") and not dNSName.isValue:\n                continue\n\n            result.setdefault(\"subjectAltName\", []).append(\n                (\"DNS\", str(dNSName))\n            )\n\n    return result", "response": "Extract a python dictionary similar to the return value of ssl. SSLSocket. getpeercert from the given x509 object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting an ASN. 1 encoded certificate blob to a pyasn1 structure.", "response": "def blob_to_pyasn1(blob):\n    \"\"\"\n    Convert an ASN.1 encoded certificate (such as obtained from\n    :func:`extract_blob`) to a :mod:`pyasn1` structure and return the result.\n    \"\"\"\n\n    return pyasn1.codec.der.decoder.decode(\n        blob,\n        asn1Spec=pyasn1_modules.rfc2459.Certificate()\n    )[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_pk_blob_from_pyasn1(pyasn1_struct):\n\n    pk = pyasn1_struct.getComponentByName(\n        \"tbsCertificate\"\n    ).getComponentByName(\n        \"subjectPublicKeyInfo\"\n    )\n\n    return pyasn1.codec.der.encoder.encode(pk)", "response": "Extract an ASN. 1 encoded public key blob from the given pyasn1 structure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_x509_hostname(x509, hostname):\n\n    cert_structure = extract_python_dict_from_x509(x509)\n    try:\n        ssl.match_hostname(cert_structure, hostname)\n    except ssl.CertificateError:\n        return False\n    return True", "response": "Checks whether the given certificate x509 matches the given hostname."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef default_ssl_context():\n\n    ctx = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n    ctx.set_options(OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3)\n    ctx.set_verify(OpenSSL.SSL.VERIFY_PEER, default_verify_callback)\n    return ctx", "response": "Return a sensibly configured OpenSSL. SSL. Context object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnegotiates the given jid with the given SASL authentication mechanism.", "response": "def negotiate_sasl(transport, xmlstream,\n                   sasl_providers,\n                   negotiation_timeout,\n                   jid, features):\n    \"\"\"\n    Perform SASL authentication on the given :class:`.protocol.XMLStream`\n    `stream`. `transport` must be the :class:`asyncio.Transport` over which the\n    `stream` runs. It is used to detect whether TLS is used and may be required\n    by some SASL mechanisms.\n\n    `sasl_providers` must be an iterable of :class:`SASLProvider` objects. They\n    will be tried in iteration order to authenticate against the server. If one\n    of the `sasl_providers` fails with a :class:`aiosasl.AuthenticationFailure`\n    exception, the other providers are still tried; only if all providers fail,\n    the last :class:`aiosasl.AuthenticationFailure` exception is re-raised.\n\n    If no mechanism was able to authenticate but not due to authentication\n    failures (other failures include no matching mechanism on the server side),\n    :class:`aiosasl.SASLUnavailable` is raised.\n\n    Return the :class:`.nonza.StreamFeatures` obtained after resetting the\n    stream after successful SASL authentication.\n\n    .. versionadded:: 0.6\n\n    .. deprecated:: 0.10\n\n        The `negotiation_timeout` argument is ignored. The timeout is\n        controlled using the :attr:`~.XMLStream.deadtime_hard_limit` timeout\n        of the stream.\n\n        The argument will be removed in version 1.0. To prepare for this,\n        please pass `jid` and `features` as keyword arguments.\n    \"\"\"\n\n    if not transport.get_extra_info(\"sslcontext\"):\n        transport = None\n\n    last_auth_error = None\n    for sasl_provider in sasl_providers:\n        try:\n            result = yield from sasl_provider.execute(\n                jid, features, xmlstream, transport)\n        except ValueError as err:\n            raise errors.StreamNegotiationFailure(\n                \"invalid credentials: {}\".format(err)\n            ) from err\n        except aiosasl.AuthenticationFailure as err:\n            last_auth_error = err\n            continue\n\n        if result:\n            features = yield from protocol.reset_stream_and_get_features(\n                xmlstream\n            )\n            break\n    else:\n        if last_auth_error:\n            raise last_auth_error\n        else:\n            raise errors.SASLUnavailable(\"No common mechanisms\")\n\n    return features"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new security layer for the given TLS provider and sasl_providers.", "response": "def security_layer(tls_provider, sasl_providers):\n    \"\"\"\n    .. deprecated:: 0.6\n\n       Replaced by :class:`SecurityLayer`.\n\n    Return a configured :class:`SecurityLayer`. `tls_provider` must be a\n    :class:`STARTTLSProvider`.\n\n    The return value can be passed to the constructor of\n    :class:`~.node.Client`.\n\n    Some very basic checking on the input is also performed.\n    \"\"\"\n\n    sasl_providers = tuple(sasl_providers)\n\n    if not sasl_providers:\n        raise ValueError(\"At least one SASL provider must be given.\")\n    for sasl_provider in sasl_providers:\n        sasl_provider.execute  # check that sasl_provider has execute method\n\n    result = SecurityLayer(\n        tls_provider.ssl_context_factory,\n        tls_provider.certificate_verifier_factory,\n        tls_provider.tls_required,\n        sasl_providers\n    )\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a TLS and a password - based SASL authentication.", "response": "def tls_with_password_based_authentication(\n        password_provider,\n        ssl_context_factory=default_ssl_context,\n        max_auth_attempts=3,\n        certificate_verifier_factory=PKIXCertificateVerifier):\n    \"\"\"\n    Produce a commonly used :class:`SecurityLayer`, which uses TLS and\n    password-based SASL authentication. If `ssl_context_factory` is not\n    provided, an SSL context with TLSv1+ is used.\n\n    `password_provider` must be a coroutine which is called with the jid\n    as first and the number of attempt as second argument. It must return the\n    password to us, or :data:`None` to abort.\n\n    Return a :class:`SecurityLayer` instance.\n\n    .. deprecated:: 0.7\n\n       Use :func:`make` instead.\n    \"\"\"\n\n    tls_kwargs = {}\n    if certificate_verifier_factory is not None:\n        tls_kwargs[\"certificate_verifier_factory\"] = \\\n            certificate_verifier_factory\n\n    return SecurityLayer(\n        ssl_context_factory,\n        certificate_verifier_factory,\n        True,\n        (\n            PasswordSASLProvider(\n                password_provider,\n                max_auth_attempts=max_auth_attempts),\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make(\n        password_provider,\n        *,\n        pin_store=None,\n        pin_type=PinType.PUBLIC_KEY,\n        post_handshake_deferred_failure=None,\n        anonymous=False,\n        ssl_context_factory=default_ssl_context,\n        no_verify=False):\n    \"\"\"\n    Construct a :class:`SecurityLayer`. Depending on the arguments passed,\n    different features are enabled or disabled.\n\n    .. warning::\n\n        When using any argument except `password_provider`, be sure to read\n        its documentation below the following overview **carefully**. Many\n        arguments can be used to shoot yourself in the foot easily, while\n        violating all security expectations.\n\n    Args:\n\n        password_provider (:class:`str` or coroutine function):\n            Password source to authenticate with.\n\n    Keyword Args:\n        pin_store (:class:`dict` or :class:`AbstractPinStore`):\n            Enable use of certificate/public key pinning. `pin_type` controls\n            the type of store used when a dict is passed instead of a pin store\n            object.\n        pin_type (:class:`~aioxmpp.security_layer.PinType`):\n            Type of pin store to create when `pin_store` is a dict.\n        post_handshake_deferred_failure (coroutine function):\n            Coroutine callback to invoke when using certificate pinning and the\n            verification of the certificate was not possible using either PKIX\n            or the pin store.\n        anonymous (:class:`str`, :data:`None` or :data:`False`):\n            trace token for SASL ANONYMOUS (:rfc:`4505`); passing a\n            non-:data:`False` value enables ANONYMOUS authentication.\n        ssl_context_factory (function): Factory function to create the SSL\n            context used to establish transport layer security. Defaults to\n            :func:`aioxmpp.security_layer.default_ssl_context`.\n        no_verify (:class:`bool`): *Disable* all certificate verification.\n            Usage is **strongly discouraged** outside controlled test\n            environments. See below for alternatives.\n\n    Raises:\n\n        RuntimeError: if `anonymous` is not :data:`False` and the version of\n            :mod:`aiosasl` does not support ANONYMOUS authentication.\n\n    Returns:\n        :class:`SecurityLayer`: object holding the entire security layer\n            configuration\n\n    `password_provider` must either be a coroutine function or a :class:`str`.\n    As a coroutine function, it is called during authentication with the JID we\n    are trying to authenticate against as the first, and the sequence number of\n    the authentication attempt as second argument. The sequence number starts\n    at 0. The coroutine is expected to return :data:`None` or a password. See\n    :class:`PasswordSASLProvider` for details. If `password_provider` is a\n    :class:`str`, a coroutine which returns the string on the first and\n    :data:`None` on subsequent attempts is created and used.\n\n    If `pin_store` is not :data:`None`, :class:`PinningPKIXCertificateVerifier`\n    is used instead of the default :class:`PKIXCertificateVerifier`. The\n    `pin_store` argument determines the pinned certificates: if it is a\n    dictionary, a :class:`AbstractPinStore` according to the :class:`PinType`\n    passed as `pin_type` argument is created and initialised with the data from\n    the dictionary using its :meth:`~AbstractPinStore.import_from_json` method.\n    Otherwise, `pin_store` must be a :class:`AbstractPinStore` instance which\n    is passed to the verifier.\n\n    `post_handshake_deferred_callback` is used only if `pin_store` is not\n    :data:`None`. It is passed to the equally-named argument of\n    :class:`PinningPKIXCertificateVerifier`, see the documentation there for\n    details on the semantics. If `post_handshake_deferred_callback` is\n    :data:`None` while `pin_store` is not, a coroutine which returns\n    :data:`False` is substituted.\n\n    `ssl_context_factory` can be a callable taking no arguments and returning\n    a :class:`OpenSSL.SSL.Context` object. If given, the factory will be used\n    to obtain an SSL context when the stream negotiates transport layer\n    security via TLS. By default,\n    :func:`aioxmpp.security_layer.default_ssl_context` is used, which should be\n    fine for most applications.\n\n    .. warning::\n\n        The :func:`~.default_ssl_context` implementation sets important\n        defaults. It is **strongly recommended** to use the context returned\n        by :func:`~.default_ssl_context` and modify it, instead of creating\n        a new context from scratch when implementing your own factory.\n\n    If `no_verify` is true, none of the above regarding certificate verifiers\n    matters. The internal null verifier is used, which **disables certificate\n    verification completely**.\n\n    .. warning::\n\n        Disabling certificate verification makes your application vulnerable to\n        trivial Man-in-the-Middle attacks. Do **not** use this outside\n        controlled test environments or when you know **exactly** what you\u2019re\n        doing!\n\n        If you need to handle certificates which cannot be verified using the\n        public key infrastructure, consider making use of the `pin_store`\n        argument instead.\n\n    `anonymous` may be a string or :data:`False`. If it is not :data:`False`,\n    :class:`AnonymousSASLProvider` is used before password based authentication\n    is attempted. In addition, it is allowed to set `password_provider` to\n    :data:`None`. `anonymous` is the trace token to use, and SHOULD be the\n    empty string (as specified by :xep:`175`). This requires :mod:`aiosasl` 0.3\n    or newer.\n\n    .. note::\n\n        :data:`False` and ``\"\"`` are treated differently for the `anonymous`\n        argument, despite both being false-y values!\n\n    .. note::\n\n        If `anonymous` is not :data:`False` and `password_provider` is not\n        :data:`None`, both authentication types are attempted. Anonymous\n        authentication is, in that case, preferred over password-based\n        authentication.\n\n        If you need to reverse the order, you have to construct your own\n        :class:`SecurityLayer` object.\n\n    .. warning::\n\n        Take the security and privacy considerations from :rfc:`4505` (which\n        specifies the ANONYMOUS SASL mechanism) and :xep:`175` (which discusses\n        the ANONYMOUS SASL mechanism in the XMPP context) into account before\n        using `anonymous`.\n\n    The versatility and simplicity of use of this function make (pun intended)\n    it the preferred way to construct :class:`SecurityLayer` instances.\n\n    .. versionadded:: 0.8\n\n        Support for SASL ANONYMOUS was added.\n\n    .. versionadded:: 0.11\n\n        Support for `ssl_context_factory`.\n    \"\"\"\n\n    if isinstance(password_provider, str):\n        static_password = password_provider\n\n        @asyncio.coroutine\n        def password_provider(jid, nattempt):\n            if nattempt == 0:\n                return static_password\n            return None\n\n    if pin_store is not None:\n        if post_handshake_deferred_failure is None:\n            @asyncio.coroutine\n            def post_handshake_deferred_failure(verifier):\n                return False\n\n        if not isinstance(pin_store, AbstractPinStore):\n            pin_data = pin_store\n            if pin_type == PinType.PUBLIC_KEY:\n                logger.debug(\"using PublicKeyPinStore\")\n                pin_store = PublicKeyPinStore()\n            else:\n                logger.debug(\"using CertificatePinStore\")\n                pin_store = CertificatePinStore()\n            pin_store.import_from_json(pin_data)\n\n        def certificate_verifier_factory():\n            return PinningPKIXCertificateVerifier(\n                pin_store.query,\n                post_handshake_deferred_failure,\n            )\n    elif no_verify:\n        certificate_verifier_factory = _NullVerifier\n    else:\n        certificate_verifier_factory = PKIXCertificateVerifier\n\n    sasl_providers = []\n    if anonymous is not False:\n        if AnonymousSASLProvider is None:\n            raise RuntimeError(\n                \"aiosasl does not support ANONYMOUS, please upgrade\"\n            )\n        sasl_providers.append(\n            AnonymousSASLProvider(anonymous)\n        )\n\n    if password_provider is not None:\n        sasl_providers.append(\n            PasswordSASLProvider(\n                password_provider,\n            ),\n        )\n\n    return SecurityLayer(\n        ssl_context_factory,\n        certificate_verifier_factory,\n        True,\n        tuple(sasl_providers),\n    )", "response": "Constructs a new security layer for the given set of keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pin(self, hostname, x509):\n\n        key = self._x509_key(x509)\n        self._storage.setdefault(hostname, set()).add(key)", "response": "Pin an X509 object for use with the\n        given hostname."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries the cache for a given hostname and X509 object.", "response": "def query(self, hostname, x509):\n        \"\"\"\n        Return true if the given :class:`OpenSSL.crypto.X509` object `x509` has\n        previously been pinned for use with the given `hostname` and\n        :data:`None` otherwise.\n\n        Returning :data:`None` allows this method to be used with\n        :class:`PinningPKIXCertificateVerifier`.\n        \"\"\"\n\n        key = self._x509_key(x509)\n        try:\n            pins = self._storage[hostname]\n        except KeyError:\n            return None\n\n        if key in pins:\n            return True\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_to_json(self):\n\n        return {\n            hostname: sorted(self._encode_key(key) for key in pins)\n            for hostname, pins in self._storage.items()\n        }", "response": "Export all the pins stored in this\n        store to a JSON dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_from_json(self, data, *, override=False):\n\n        if override:\n            self._storage = {\n                hostname: set(self._decode_key(key) for key in pins)\n                for hostname, pins in data.items()\n            }\n            return\n\n        for hostname, pins in data.items():\n            existing_pins = self._storage.setdefault(hostname, set())\n            existing_pins.update(self._decode_key(key) for key in pins)", "response": "Import a JSON dictionary which must have the same format as exported by the export method."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the first SASL mechanism announced in and return the corresponding SASL mechanism class and token.", "response": "def _find_supported(self, features, mechanism_classes):\n        \"\"\"\n        Find the first mechansim class which supports a mechanism announced in\n        the given stream features.\n\n        :param features: Current XMPP stream features\n        :type features: :class:`~.nonza.StreamFeatures`\n        :param mechanism_classes: SASL mechanism classes to use\n        :type mechanism_classes: iterable of :class:`SASLMechanism`\n                                 sub\\\\ *classes*\n        :raises aioxmpp.errors.SASLUnavailable: if the peer does not announce\n                                                SASL support\n        :return: the :class:`SASLMechanism` subclass to use and a token\n        :rtype: pair\n\n        Return a supported SASL mechanism class, by looking the given\n        stream features `features`.\n\n        If no matching mechanism is found, ``(None, None)`` is\n        returned. Otherwise, a pair consisting of the mechanism class and the\n        value returned by the respective\n        :meth:`~.sasl.SASLMechanism.any_supported` method is returned. The\n        latter is an opaque token which must be passed to the `token` argument\n        of :meth:`_execute` or :meth:`aiosasl.SASLMechanism.authenticate`.\n        \"\"\"\n\n        try:\n            mechanisms = features[SASLMechanisms]\n        except KeyError:\n            logger.error(\"No sasl mechanisms: %r\", list(features))\n            raise errors.SASLUnavailable(\n                \"Remote side does not support SASL\") from None\n\n        remote_mechanism_list = mechanisms.get_mechanism_list()\n\n        for our_mechanism in mechanism_classes:\n            token = our_mechanism.any_supported(remote_mechanism_list)\n            if token is not None:\n                return our_mechanism, token\n\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _execute(self, intf, mechanism, token):\n        sm = aiosasl.SASLStateMachine(intf)\n        try:\n            yield from mechanism.authenticate(sm, token)\n            return True\n        except aiosasl.SASLFailure as err:\n            if err.opaque_error in self.AUTHENTICATION_FAILURES:\n                raise aiosasl.AuthenticationFailure(\n                    opaque_error=err.opaque_error,\n                    text=err.text)\n            elif err.opaque_error in self.MECHANISM_REJECTED_FAILURES:\n                return False\n            raise", "response": "Execute the SASL authentication process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect(self, loop, metadata, domain: str, host, port,\n                negotiation_timeout, base_logger=None):\n        \"\"\"\n        .. seealso::\n\n           :meth:`BaseConnector.connect`\n              For general information on the :meth:`connect` method.\n\n        Connect to `host` at TCP port number `port`. The\n        :class:`aioxmpp.security_layer.SecurityLayer` object `metadata` is used\n        to determine the parameters of the TLS connection.\n\n        First, a normal TCP connection is opened and the stream header is sent.\n        The stream features are waited for, and then STARTTLS is negotiated if\n        possible.\n\n        :attr:`~.security_layer.SecurityLayer.tls_required` is honoured: if it\n        is true and TLS negotiation fails, :class:`~.errors.TLSUnavailable` is\n        raised. TLS negotiation is always attempted if\n        :attr:`~.security_layer.SecurityLayer.tls_required` is true, even if\n        the server does not advertise a STARTTLS stream feature. This might\n        help to prevent trivial downgrade attacks, and we don\u2019t have anything\n        to lose at this point anymore anyways.\n\n        :attr:`~.security_layer.SecurityLayer.ssl_context_factory` and\n        :attr:`~.security_layer.SecurityLayer.certificate_verifier_factory` are\n        used to configure the TLS connection.\n\n        .. versionchanged:: 0.10\n\n            The `negotiation_timeout` is set as\n            :attr:`~.XMLStream.deadtime_hard_limit` on the returned XML stream.\n        \"\"\"\n\n        features_future = asyncio.Future(loop=loop)\n\n        stream = protocol.XMLStream(\n            to=domain,\n            features_future=features_future,\n            base_logger=base_logger,\n        )\n        if base_logger is not None:\n            logger = base_logger.getChild(type(self).__name__)\n        else:\n            logger = logging.getLogger(\".\".join([\n                __name__, type(self).__qualname__,\n            ]))\n\n        try:\n            transport, _ = yield from ssl_transport.create_starttls_connection(\n                loop,\n                lambda: stream,\n                host=host,\n                port=port,\n                peer_hostname=host,\n                server_hostname=to_ascii(domain),\n                use_starttls=True,\n            )\n        except:  # NOQA\n            stream.abort()\n            raise\n\n        stream.deadtime_hard_limit = timedelta(seconds=negotiation_timeout)\n\n        features = yield from features_future\n\n        try:\n            features[nonza.StartTLSFeature]\n        except KeyError:\n            if not metadata.tls_required:\n                return transport, stream, (yield from features_future)\n            logger.debug(\n                \"attempting STARTTLS despite not announced since it is\"\n                \" required\")\n\n        try:\n            response = yield from protocol.send_and_wait_for(\n                stream,\n                [\n                    nonza.StartTLS(),\n                ],\n                [\n                    nonza.StartTLSFailure,\n                    nonza.StartTLSProceed,\n                ]\n            )\n        except errors.StreamError:\n            raise errors.TLSUnavailable(\n                \"STARTTLS not supported by server, but required by client\"\n            )\n\n        if not isinstance(response, nonza.StartTLSProceed):\n            if metadata.tls_required:\n                message = (\n                    \"server failed to STARTTLS\"\n                )\n\n                protocol.send_stream_error_and_close(\n                    stream,\n                    condition=errors.StreamErrorCondition.POLICY_VIOLATION,\n                    text=message,\n                )\n\n                raise errors.TLSUnavailable(message)\n            return transport, stream, (yield from features_future)\n\n        verifier = metadata.certificate_verifier_factory()\n        yield from verifier.pre_handshake(\n            domain,\n            host,\n            port,\n            metadata,\n        )\n\n        ssl_context = metadata.ssl_context_factory()\n        verifier.setup_context(ssl_context, transport)\n\n        yield from stream.starttls(\n            ssl_context=ssl_context,\n            post_handshake_callback=verifier.post_handshake,\n        )\n\n        features_future = yield from protocol.reset_stream_and_get_features(\n            stream,\n            timeout=negotiation_timeout,\n        )\n\n        return transport, stream, features_future", "response": "Connect to the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect(self, loop, metadata, domain, host, port,\n                negotiation_timeout, base_logger=None):\n        \"\"\"\n        .. seealso::\n\n           :meth:`BaseConnector.connect`\n              For general information on the :meth:`connect` method.\n\n        Connect to `host` at TCP port number `port`. The\n        :class:`aioxmpp.security_layer.SecurityLayer` object `metadata` is used\n        to determine the parameters of the TLS connection.\n\n        The connector connects to the server by directly establishing TLS; no\n        XML stream is started before TLS negotiation, in accordance to\n        :xep:`368` and how legacy SSL was handled in the past.\n\n        :attr:`~.security_layer.SecurityLayer.ssl_context_factory` and\n        :attr:`~.security_layer.SecurityLayer.certificate_verifier_factory` are\n        used to configure the TLS connection.\n\n        .. versionchanged:: 0.10\n\n            The `negotiation_timeout` is set as\n            :attr:`~.XMLStream.deadtime_hard_limit` on the returned XML stream.\n        \"\"\"\n\n        features_future = asyncio.Future(loop=loop)\n\n        stream = protocol.XMLStream(\n            to=domain,\n            features_future=features_future,\n            base_logger=base_logger,\n        )\n\n        if base_logger is not None:\n            logger = base_logger.getChild(type(self).__name__)\n        else:\n            logger = logging.getLogger(\".\".join([\n                __name__, type(self).__qualname__,\n            ]))\n\n        verifier = metadata.certificate_verifier_factory()\n        yield from verifier.pre_handshake(\n            domain,\n            host,\n            port,\n            metadata,\n        )\n\n        context_factory = self._context_factory_factory(logger, metadata,\n                                                        verifier)\n\n        try:\n            transport, _ = yield from ssl_transport.create_starttls_connection(\n                loop,\n                lambda: stream,\n                host=host,\n                port=port,\n                peer_hostname=host,\n                server_hostname=to_ascii(domain),\n                post_handshake_callback=verifier.post_handshake,\n                ssl_context_factory=context_factory,\n                use_starttls=False,\n            )\n        except:  # NOQA\n            stream.abort()\n            raise\n\n        stream.deadtime_hard_limit = timedelta(seconds=negotiation_timeout)\n\n        return transport, stream, (yield from features_future)", "response": "Connect to the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nnormalize an XML element tree tag into the tuple format.", "response": "def normalize_tag(tag):\n    \"\"\"\n    Normalize an XML element tree `tag` into the tuple format. The following\n    input formats are accepted:\n\n    * ElementTree namespaced string, e.g. ``{uri:bar}foo``\n    * Unnamespaced tags, e.g. ``foo``\n    * Two-tuples consisting of `namespace_uri` and `localpart`; `namespace_uri`\n      may be :data:`None` if the tag is supposed to be namespaceless. Otherwise\n      it must be, like `localpart`, a :class:`str`.\n\n    Return a two-tuple consisting the ``(namespace_uri, localpart)`` format.\n    \"\"\"\n    if isinstance(tag, str):\n        namespace_uri, sep, localname = tag.partition(\"}\")\n        if sep:\n            if not namespace_uri.startswith(\"{\"):\n                raise ValueError(\"not a valid etree-format tag\")\n            namespace_uri = namespace_uri[1:]\n        else:\n            localname = namespace_uri\n            namespace_uri = None\n        return (namespace_uri, localname)\n    elif len(tag) != 2:\n        raise ValueError(\"not a valid tuple-format tag\")\n    else:\n        if any(part is not None and not isinstance(part, str) for part in tag):\n            raise TypeError(\"tuple-format tags must only contain str and None\")\n        if tag[1] is None:\n            raise ValueError(\"tuple-format localname must not be None\")\n    return tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_message(self, body):\n        token, tracker = self.send_message_tracked(body)\n        tracker.cancel()\n        return token", "response": "Send a message to the conversation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef invite(self, address, text=None, *,\n               mode=InviteMode.DIRECT,\n               allow_upgrade=False):\n        \"\"\"\n        Invite another entity to the conversation.\n\n        :param address: The address of the entity to invite.\n        :type address: :class:`aioxmpp.JID`\n        :param text: A reason/accompanying text for the invitation.\n        :param mode: The invitation mode to use.\n        :type mode: :class:`~.im.InviteMode`\n        :param allow_upgrade: Whether to allow creating a new conversation to\n            satisfy the invitation.\n        :type allow_upgrade: :class:`bool`\n        :raises NotImplementedError: if the requested `mode` is not supported\n        :raises ValueError: if `allow_upgrade` is false, but a new conversation\n            is required.\n        :return: The stanza token for the invitation and the possibly new\n            conversation object\n        :rtype: tuple of :class:`~.StanzaToken` and\n            :class:`~.AbstractConversation`\n\n        .. note::\n\n            Even though this is a coroutine, it returns a stanza token. The\n            coroutine-ness may be needed to generate the invitation in the\n            first place. Sending the actual invitation is done non-blockingly\n            and the stanza token for that is returned. To wait until the\n            invitation has been sent, unpack the stanza token from the result\n            and await it.\n\n        Return the new conversation object to use. In many cases, this will\n        simply be the current conversation object, but in some cases (e.g. when\n        someone is invited to a one-on-one conversation), a new conversation\n        must be created and used.\n\n        If `allow_upgrade` is false and a new conversation would be needed to\n        invite an entity, :class:`ValueError` is raised.\n\n        Additional features:\n\n        :attr:`~.ConversationFeature.INVITE_DIRECT`\n           Support for :attr:`~.im.InviteMode.DIRECT` mode.\n\n        :attr:`~.ConversationFeature.INVITE_DIRECT_CONFIGURE`\n           If a direct invitation is used, the conversation will be configured\n           to allow the invitee to join before the invitation is sent. This may\n           fail with a :class:`aioxmpp.errors.XMPPError`, in which case the\n           error is re-raised and the invitation not sent.\n\n        :attr:`~.ConversationFeature.INVITE_MEDIATED`\n           Support for :attr:`~.im.InviteMode.MEDIATED` mode.\n\n        :attr:`~.ConversationFeature.INVITE_UPGRADE`\n           If `allow_upgrade` is :data:`True`, an upgrade will be performed and\n           a new conversation is returned. If `allow_upgrade` is :data:`False`,\n           the invite will fail.\n\n        .. seealso::\n\n           The corresponding feature for this method is\n           :attr:`.ConversationFeature.INVITE`. See :attr:`features` for\n           details on the semantics of features.\n        \"\"\"\n        raise self._not_implemented_error(\"inviting entities\")", "response": "Invites another entity to a conversation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hash_from_algo(algo):\n\n    try:\n        enabled, (fun_name, fun_args, fun_kwargs) = _HASH_ALGO_MAP[algo]\n    except KeyError:\n        raise NotImplementedError(\n            \"hash algorithm {!r} unknown\".format(algo)\n        ) from None\n\n    if not enabled:\n        raise ValueError(\n            \"support of {} in XMPP is forbidden\".format(algo)\n        )\n\n    try:\n        fun = getattr(hashlib, fun_name)\n    except AttributeError as exc:\n        raise NotImplementedError(\n            \"{} not supported by hashlib\".format(algo)\n        ) from exc\n\n    return fun(*fun_args, **fun_kwargs)", "response": "Returns a hash object given the hash algorithm identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string value for the given hash.", "response": "def algo_of_hash(h):\n    \"\"\"\n    Return a :xep:`300` `algo` from a given :mod:`hashlib` hash.\n\n    :param h: Hash object from :mod:`hashlib`.\n    :raises ValueError: if `h` does not have a defined `algo` value.\n    :raises ValueError: if the hash function MUST NOT be supported.\n    :return: The `algo` value for the given hash.\n    :rtype: :class:`str`\n\n    .. warning::\n\n        Use with caution for :func:`hashlib.blake2b` hashes.\n        :func:`algo_of_hash` cannot safely determine whether blake2b was\n        initialised with a salt, personality, key or other non-default\n        :xep:`300` mode.\n\n        In such a case, the return value will be the matching ``blake2b-*``\n        `algo`, but the digest will not be compatible with the results of other\n        implementations.\n\n    \"\"\"\n    try:\n        enabled, algo = _HASH_ALGO_REVERSE_MAP[h.name]\n    except KeyError:\n        pass\n    else:\n        if not enabled:\n            raise ValueError(\"support of {} in XMPP is forbidden\".format(\n                algo\n            ))\n        return algo\n\n    if h.name == \"blake2b\":\n        return \"blake2b-{}\".format(h.digest_size * 8)\n\n    raise ValueError(\n        \"unknown hash implementation: {!r}\".format(h)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select_common_hashes(self, other_entity):\n        disco_info = yield from self._disco_client.query_info(other_entity)\n        intersection = disco_info.features & SUPPORTED_HASH_FEATURES\n        if (not intersection and\n                namespaces.xep0300_hashes2 not in disco_info.features):\n            raise RuntimeError(\n                \"Remote does not support the urn:xmpp:hashes:2 feature.\")\n        return intersection", "response": "Select the common hashes for the given other_entity."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to multiple signals and wait for the first to emit.", "response": "def first_signal(*signals):\n    \"\"\"\n    Connect to multiple signals and wait for the first to emit.\n\n    :param signals: Signals to connect to.\n    :type signals: :class:`AdHocSignal`\n    :return: An awaitable for the first signal to emit.\n\n    The awaitable returns the first argument passed to the signal. If the first\n    argument is an exception, the exception is re-raised from the awaitable.\n\n    A common use-case is a situation where a class exposes a \"on_finished\" type\n    signal and an \"on_failure\" type signal. :func:`first_signal` can be used\n    to combine those nicely::\n\n        # e.g. a aioxmpp.im.conversation.AbstractConversation\n        conversation = ...\n        await first_signal(\n            # emits without arguments when the conversation is successfully\n            # entered\n            conversation.on_enter,\n            # emits with an exception when entering the conversation fails\n            conversation.on_failure,\n        )\n        # await first_signal(...) will either raise an exception (failed) or\n        # return None (success)\n\n    .. warning::\n\n        Only works with signals which emit with zero or one argument. Signals\n        which emit with more than one argument or with keyword arguments are\n        silently ignored! (Thus, if only such signals are connected, the\n        future will never complete.)\n\n        (This is a side-effect of the implementation of\n        :meth:`AdHocSignal.AUTO_FUTURE`).\n\n    .. note::\n\n        Does not work with coroutine signals (:class:`SyncAdHocSignal`).\n    \"\"\"\n\n    fut = asyncio.Future()\n    for signal in signals:\n        signal.connect(fut, signal.AUTO_FUTURE)\n    return fut"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect(self, f, mode=None):\n\n        mode = mode or self.STRONG\n        self.logger.debug(\"connecting %r with mode %r\", f, mode)\n        return self._connect(mode(f))", "response": "Connect an object f to the signal."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nemitting the signal and remove all other connected objects from the signal.", "response": "def fire(self, *args, **kwargs):\n        \"\"\"\n        Emit the signal, calling all connected objects in-line with the given\n        arguments and in the order they were registered.\n\n        :class:`AdHocSignal` provides full isolation with respect to\n        exceptions. If a connected listener raises an exception, the other\n        listeners are executed as normal, but the raising listener is removed\n        from the signal. The exception is logged to :attr:`logger` and *not*\n        re-raised, so that the caller of the signal is also not affected.\n\n        Instead of calling :meth:`fire` explicitly, the ad-hoc signal object\n        itself can be called, too.\n        \"\"\"\n        for token, wrapper in list(self._connections.items()):\n            try:\n                keep = wrapper(args, kwargs)\n            except Exception:\n                self.logger.exception(\"listener attached to signal raised\")\n                keep = False\n            if not keep:\n                del self._connections[token]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef future(self):\n        fut = asyncio.Future()\n        self.connect(fut, self.AUTO_FUTURE)\n        return fut", "response": "Return a new asyncio. Future which has been created."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects a coroutine to the signal.", "response": "def connect(self, coro):\n        \"\"\"\n        The coroutine `coro` is connected to the signal. The coroutine must\n        return a true value, unless it wants to be disconnected from the\n        signal.\n\n        .. note::\n\n           This is different from the return value convention with\n           :attr:`AdHocSignal.STRONG` and :attr:`AdHocSignal.WEAK`.\n\n        :meth:`connect` returns a token which can be used with\n        :meth:`disconnect` to disconnect the coroutine.\n        \"\"\"\n        self.logger.debug(\"connecting %r\", coro)\n        return self._connect(coro)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nemit the signal with the given arguments and kwargs.", "response": "def fire(self, *args, **kwargs):\n        \"\"\"\n        Emit the signal, calling all coroutines in-line with the given\n        arguments and in the order they were registered.\n\n        This is obviously a coroutine.\n\n        Instead of calling :meth:`fire` explicitly, the ad-hoc signal object\n        itself can be called, too.\n        \"\"\"\n        for token, coro in list(self._connections.items()):\n            keep = yield from coro(*args, **kwargs)\n            if not keep:\n                del self._connections[token]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a function as a filter in the filter chain.", "response": "def register(self, func, order):\n        \"\"\"\n        Add a function to the filter chain.\n\n        :param func: A callable which is to be added to the filter chain.\n        :param order: An object indicating the ordering of the function\n                      relative to the others.\n        :return: Token representing the registration.\n\n        Register the function `func` as a filter into the chain. `order` must\n        be a value which is used as a sorting key to order the functions\n        registered in the chain.\n\n        The type of `order` depends on the use of the filter, as does the\n        number of arguments and keyword arguments which `func` must accept.\n        This will generally be documented at the place where the\n        :class:`Filter` is used.\n\n        Functions with the same order are sorted in the order of their\n        addition, with the function which was added earliest first.\n\n        Remember that all values passed to `order` which are registered at the\n        same time in the same :class:`Filter` need to be totally orderable with\n        respect to each other.\n\n        The returned token can be used to :meth:`unregister` a filter.\n        \"\"\"\n        token = self.Token()\n        self._filter_order.append((order, token, func))\n        self._filter_order.sort(key=lambda x: x[0])\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter(self, obj, *args, **kwargs):\n        for _, _, func in self._filter_order:\n            obj = func(obj, *args, **kwargs)\n            if obj is None:\n                return None\n        return obj", "response": "Filter the given object through the filter chain."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_timeout(self, timeout):\n        loop = asyncio.get_event_loop()\n\n        if isinstance(timeout, timedelta):\n            timeout = timeout.total_seconds()\n\n        loop.call_later(timeout, self.close)", "response": "Set the timeout for the current entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_state(self, new_state, response=None):\n        if self._closed:\n            raise RuntimeError(\"message tracker is closed\")\n\n        # reject some transitions as documented\n        if (self._state == MessageState.ABORTED or\n                new_state == MessageState.IN_TRANSIT or\n                (self._state == MessageState.ERROR and\n                 new_state == MessageState.DELIVERED_TO_SERVER) or\n                (self._state == MessageState.ERROR and\n                 new_state == MessageState.ABORTED) or\n                (self._state == MessageState.DELIVERED_TO_RECIPIENT and\n                 new_state == MessageState.DELIVERED_TO_SERVER) or\n                (self._state == MessageState.SEEN_BY_RECIPIENT and\n                 new_state == MessageState.DELIVERED_TO_SERVER) or\n                (self._state == MessageState.SEEN_BY_RECIPIENT and\n                 new_state == MessageState.DELIVERED_TO_RECIPIENT)):\n            raise ValueError(\n                \"message tracker transition from {} to {} not allowed\".format(\n                    self._state,\n                    new_state\n                )\n            )\n\n        self._state = new_state\n        self._response = response\n        self.on_state_changed(self._state, self._response)", "response": "Sets the state of the message tracker."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_tracked(self, stanza, tracker):\n        token = self.client.enqueue(stanza)\n        self.attach_tracker(stanza, tracker, token)\n        return token", "response": "Send a message stanza with tracking."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nattaches a tracker to a message stanza.", "response": "def attach_tracker(self, stanza, tracker=None, token=None):\n        \"\"\"\n        Configure tracking for a stanza without sending it.\n\n        :param stanza: Message stanza to send.\n        :type stanza: :class:`aioxmpp.Message`\n        :param tracker: Message tracker to use.\n        :type tracker: :class:`~.MessageTracker` or :data:`None`\n        :param token: Optional stanza token for more fine-grained tracking.\n        :type token: :class:`~.StanzaToken`\n        :rtype: :class:`~.MessageTracker`\n        :return: The message tracker.\n\n        If `tracker` is :data:`None`, a new :class:`~.MessageTracker` is\n        created.\n\n        If `token` is not :data:`None`, updates to the stanza `token` are\n        reflected in the `tracker`.\n\n        If an error reply is received, the tracker will enter\n        :class:`~.MessageState.ERROR` and the error will be set as\n        :attr:`~.MessageTracker.response`.\n\n        You should use :meth:`send_tracked` if possible. This method however is\n        very useful if you need to track carbon copies of sent messages, as a\n        stanza token is not available here and re-sending the message to obtain\n        one is generally not desirable \u263a.\n        \"\"\"\n        if tracker is None:\n            tracker = MessageTracker()\n        stanza.autoset_id()\n        key = stanza.to.bare(), stanza.id_\n        self._trackers[key] = tracker\n        tracker.on_closed.connect(\n            functools.partial(self._tracker_closed, key)\n        )\n        if token is not None:\n            token.future.add_done_callback(\n                functools.partial(\n                    self._stanza_sent,\n                    tracker,\n                    token,\n                )\n            )\n        return tracker"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(self):\n        self.stop()\n        self._task = asyncio.ensure_future(self._pinger(), loop=self._loop)", "response": "Start the pinging coroutine using the client and event loop which was passed to the constructor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninterprets the result of a ping.", "response": "def _interpret_result(self, task):\n        \"\"\"\n        Interpret the result of a ping.\n\n        :param task: The pinger task.\n\n        The result or exception of the `task` is interpreted as follows:\n\n        * :data:`None` result: *positive*\n        * :class:`aioxmpp.errors.XMPPError`, ``service-unavailable``:\n          *positive*\n        * :class:`aioxmpp.errors.XMPPError`, ``feature-not-implemented``:\n          *positive*\n        * :class:`aioxmpp.errors.XMPPError`, ``item-not-found``: *inconclusive*\n        * :class:`aioxmpp.errors.XMPPError`: *negative*\n        * :class:`asyncio.TimeoutError`: *inconclusive*\n        * Any other exception: *inconclusive*\n        \"\"\"\n        if task.exception() is None:\n            self._on_fresh()\n            return\n\n        exc = task.exception()\n        if isinstance(exc, aioxmpp.errors.XMPPError):\n            if exc.condition in [\n                    aioxmpp.errors.ErrorCondition.SERVICE_UNAVAILABLE,\n                    aioxmpp.errors.ErrorCondition.FEATURE_NOT_IMPLEMENTED]:\n                self._on_fresh()\n                return\n\n            if exc.condition == aioxmpp.errors.ErrorCondition.ITEM_NOT_FOUND:\n                return\n\n            self._on_exited()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset(self):\n        self._monitor.notify_received()\n        self._pinger.stop()\n        self._mark_fresh()", "response": "Reset the internal state of the internal state of the internal state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomposing a message receipt for a given message.", "response": "def compose_receipt(message):\n    \"\"\"\n    Compose a :xep:`184` delivery receipt for a :class:`~aioxmpp.Message`.\n\n    :param message: The message to compose the receipt for.\n    :type message: :class:`~aioxmpp.Message`\n    :raises ValueError: if the input message is of type\n        :attr:`~aioxmpp.MessageType.ERROR`\n    :raises ValueError: if the input message is a message receipt itself\n    :return: A message which serves as a receipt for the input message.\n    :rtype: :class:`~aioxmpp.Message`\n    \"\"\"\n\n    if message.type_ == aioxmpp.MessageType.ERROR:\n        raise ValueError(\"receipts cannot be generated for error messages\")\n    if message.xep0184_received:\n        raise ValueError(\"receipts cannot be generated for receipts\")\n    if message.id_ is None:\n        raise ValueError(\"receipts cannot be generated for id-less messages\")\n\n    reply = message.make_reply()\n    reply.to = reply.to.bare()\n    reply.xep0184_received = xso.Received(message.id_)\n    return reply"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattaching a tracker to a message stanza.", "response": "def attach_tracker(self, stanza, tracker=None):\n        \"\"\"\n        Return a new tracker or modify one to track the stanza.\n\n        :param stanza: Stanza to track.\n        :type stanza: :class:`aioxmpp.Message`\n        :param tracker: Existing tracker to attach to.\n        :type tracker: :class:`.tracking.MessageTracker`\n        :raises ValueError: if the stanza is of type\n            :attr:`~aioxmpp.MessageType.ERROR`\n        :raises ValueError: if the stanza contains a delivery receipt\n        :return: The message tracker for the stanza.\n        :rtype: :class:`.tracking.MessageTracker`\n\n        The `stanza` gets a :xep:`184` reciept request attached and internal\n        handlers are set up to update the `tracker` state once a confirmation\n        is received.\n\n        .. warning::\n\n           See the :ref:`api-tracking-memory`.\n\n        \"\"\"\n        if stanza.xep0184_received is not None:\n            raise ValueError(\n                \"requesting delivery receipts for delivery receipts is not \"\n                \"allowed\"\n            )\n        if stanza.type_ == aioxmpp.MessageType.ERROR:\n            raise ValueError(\n                \"requesting delivery receipts for errors is not supported\"\n            )\n\n        if tracker is None:\n            tracker = aioxmpp.tracking.MessageTracker()\n\n        stanza.xep0184_request_receipt = True\n        stanza.autoset_id()\n        self._bare_jid_maps[stanza.to, stanza.id_] = tracker\n        return tracker"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_private_xml(self, query_xso):\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.GET,\n            payload=private_xml_xso.Query(query_xso)\n        )\n        return (yield from self.client.send(iq))", "response": "Get the private XML data for the specified element query_xso from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the serialization of xso as private XML data for the namespace of xso.", "response": "def set_private_xml(self, xso):\n        \"\"\"\n        Store the serialization of `xso` on the server as the private XML\n        data for the namespace of `xso`.\n\n        :param xso: the XSO whose serialization is send as private XML data.\n        \"\"\"\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            payload=private_xml_xso.Query(xso)\n        )\n        yield from self.client.send(iq)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping all the arguments `fut_or_coros` in futures with :func:`asyncio.ensure_future` and wait until all of them are finish or fail. :param fut_or_coros: the futures or coroutines to wait for :type fut_or_coros: future or coroutine :param message: the message included with the raised :class:`aioxmpp.errrors.GatherError` in the case of failure. :type message: :class:`str` :returns: the list of the results of the arguments. :raises aioxmpp.errors.GatherError: if any of the futures or coroutines fail. If an exception was raised, reraise all exceptions wrapped in a :class:`aioxmpp.errors.GatherError` with the message set to `message`. .. note:: This is similar to the standard function :func:`asyncio.gather`, but avoids the in-band signalling of raised exceptions as return values, by raising exceptions bundled as a :class:`aioxmpp.errors.GatherError`. .. note:: Use this function only if you are either a) not interested in the return values, or b) only interested in the return values if all futures are successful.", "response": "def gather_reraise_multi(*fut_or_coros, message=\"gather_reraise_multi\"):\n    \"\"\"\n    Wrap all the arguments `fut_or_coros` in futures with\n    :func:`asyncio.ensure_future` and wait until all of them are finish or\n    fail.\n\n    :param fut_or_coros: the futures or coroutines to wait for\n    :type fut_or_coros: future or coroutine\n    :param message: the message included with the raised\n        :class:`aioxmpp.errrors.GatherError` in the case of failure.\n    :type message: :class:`str`\n    :returns: the list of the results of the arguments.\n    :raises aioxmpp.errors.GatherError: if any of the futures or\n        coroutines fail.\n\n    If an exception was raised, reraise all exceptions wrapped in a\n    :class:`aioxmpp.errors.GatherError` with the message set to\n    `message`.\n\n    .. note::\n\n       This is similar to the standard function\n       :func:`asyncio.gather`, but avoids the in-band signalling of\n       raised exceptions as return values, by raising exceptions bundled\n       as a :class:`aioxmpp.errors.GatherError`.\n\n    .. note::\n\n       Use this function only if you are either\n\n       a) not interested in the return values, or\n\n       b) only interested in the return values if all futures are\n          successful.\n    \"\"\"\n    todo = [asyncio.ensure_future(fut_or_coro) for fut_or_coro in fut_or_coros]\n    if not todo:\n        return []\n\n    yield from asyncio.wait(todo)\n    results = []\n    exceptions = []\n    for fut in todo:\n        if fut.exception() is not None:\n            exceptions.append(fut.exception())\n        else:\n            results.append(fut.result())\n    if exceptions:\n        raise aioxmpp.errors.GatherError(message, exceptions)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_nmtoken(rand_token):\n\n    if isinstance(rand_token, int):\n        rand_token = rand_token.to_bytes(\n            (rand_token.bit_length() + 7) // 8,\n            \"little\"\n        )\n        e = base64.urlsafe_b64encode(rand_token).rstrip(b\"=\").decode(\"ascii\")\n        return \":\" + e\n\n    if isinstance(rand_token, bytes):\n        e = base64.urlsafe_b64encode(rand_token).rstrip(b\"=\").decode(\"ascii\")\n        if not e:\n            e = \".\"\n        return  e\n\n    raise TypeError(\"rand_token musst be a bytes or int instance\")", "response": "Convert a random token given as raw bytes or\n    to a valid NMTOKEN."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a localized version of the localizable_string passed to the the consturctor.", "response": "def localize(self, formatter, translator):\n        \"\"\"\n        Return a localized version of the `localizable_string` passed to the\n        consturctor. It is formatted using the `formatter` with the `args` and\n        `kwargs` passed to the constructor of :class:`UserError`.\n        \"\"\"\n        return self.localizable_string.localize(\n            formatter,\n            translator,\n            *self.args,\n            **self.kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the stored bookmarks from the server.", "response": "def _get_bookmarks(self):\n        \"\"\"\n        Get the stored bookmarks from the server.\n\n        :returns: a list of bookmarks\n        \"\"\"\n        res = yield from self._private_xml.get_private_xml(\n            bookmark_xso.Storage()\n        )\n\n        return res.registered_payload.bookmarks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_bookmarks(self, bookmarks):\n        storage = bookmark_xso.Storage()\n        storage.bookmarks[:] = bookmarks\n        yield from self._private_xml.set_private_xml(storage)", "response": "Set the bookmarks stored on the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nemitting the update signal for the new bookmark cache and the new bookmark state.", "response": "def _diff_emit_update(self, new_bookmarks):\n        \"\"\"\n        Diff the bookmark cache and the new bookmark state, emit signals as\n        needed and set the bookmark cache to the new data.\n        \"\"\"\n\n        self.logger.debug(\"diffing %s, %s\", self._bookmark_cache,\n                          new_bookmarks)\n\n        def subdivide(level, old, new):\n            \"\"\"\n            Subdivide the bookmarks according to the data item\n            ``bookmark.secondary[level]`` and emit the appropriate\n            events.\n            \"\"\"\n            if len(old) == len(new) == 1:\n                old_entry = old.pop()\n                new_entry = new.pop()\n                if old_entry == new_entry:\n                    pass\n                else:\n                    self.on_bookmark_changed(old_entry, new_entry)\n                return ([], [])\n\n            elif len(old) == 0:\n                return ([], new)\n\n            elif len(new) == 0:\n                return (old, [])\n\n            else:\n                try:\n                    groups = {}\n                    for entry in old:\n                        group = groups.setdefault(\n                            entry.secondary[level],\n                            ([], [])\n                        )\n                        group[0].append(entry)\n\n                    for entry in new:\n                        group = groups.setdefault(\n                            entry.secondary[level],\n                            ([], [])\n                        )\n                        group[1].append(entry)\n                except IndexError:\n                    # the classification is exhausted, this means\n                    # all entries in this bin are equal by the\n                    # defininition of bookmark equivalence!\n                    common = min(len(old), len(new))\n                    assert old[:common] == new[:common]\n                    return (old[common:], new[common:])\n\n                old_unhandled, new_unhandled = [], []\n                for old, new in groups.values():\n                    unhandled = subdivide(level+1, old, new)\n                    old_unhandled += unhandled[0]\n                    new_unhandled += unhandled[1]\n\n                # match up unhandleds as changes as early as possible\n                i = -1\n                for i, (old_entry, new_entry) in enumerate(\n                        zip(old_unhandled, new_unhandled)):\n                    self.logger.debug(\"changed %s -> %s\", old_entry, new_entry)\n                    self.on_bookmark_changed(old_entry, new_entry)\n                i += 1\n                return old_unhandled[i:], new_unhandled[i:]\n\n        # group the bookmarks into groups whose elements may transform\n        # among one another by on_bookmark_changed events. This information\n        # is given by the type of the bookmark and the .primary property\n        changable_groups = {}\n\n        for item in self._bookmark_cache:\n            group = changable_groups.setdefault(\n                (type(item), item.primary),\n                ([], [])\n            )\n            group[0].append(item)\n\n        for item in new_bookmarks:\n            group = changable_groups.setdefault(\n                (type(item), item.primary),\n                ([], [])\n            )\n            group[1].append(item)\n\n        for old, new in changable_groups.values():\n\n            # the first branches are fast paths which should catch\n            # most cases \u2013 especially all cases where each bare jid of\n            # a conference bookmark or each url of an url bookmark is\n            # only used in one bookmark\n            if len(old) == len(new) == 1:\n                old_entry = old.pop()\n                new_entry = new.pop()\n                if old_entry == new_entry:\n                    # the bookmark is unchanged, do not emit an event\n                    pass\n                else:\n                    self.logger.debug(\"changed %s -> %s\", old_entry, new_entry)\n                    self.on_bookmark_changed(old_entry, new_entry)\n            elif len(new) == 0:\n                for removed in old:\n                    self.logger.debug(\"removed %s\", removed)\n                    self.on_bookmark_removed(removed)\n            elif len(old) == 0:\n                for added in new:\n                    self.logger.debug(\"added %s\", added)\n                    self.on_bookmark_added(added)\n            else:\n                old, new = subdivide(0, old, new)\n\n                assert len(old) == 0 or len(new) == 0\n\n                for removed in old:\n                    self.logger.debug(\"removed %s\", removed)\n                    self.on_bookmark_removed(removed)\n\n                for added in new:\n                    self.logger.debug(\"added %s\", added)\n                    self.on_bookmark_added(added)\n\n        self._bookmark_cache = new_bookmarks"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the stored bookmarks from the server.", "response": "def get_bookmarks(self):\n        \"\"\"\n        Get the stored bookmarks from the server. Causes signals to be\n        fired to reflect the changes.\n\n        :returns: a list of bookmarks\n        \"\"\"\n        with (yield from self._lock):\n            bookmarks = yield from self._get_bookmarks()\n            self._diff_emit_update(bookmarks)\n            return bookmarks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_bookmarks(self, bookmarks):\n        with (yield from self._lock):\n            yield from self._set_bookmarks(bookmarks)\n            self._diff_emit_update(bookmarks)", "response": "Sets the sequence of bookmarks."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_bookmark(self, new_bookmark, *, max_retries=3):\n        with (yield from self._lock):\n            bookmarks = yield from self._get_bookmarks()\n\n            try:\n                modified_bookmarks = list(bookmarks)\n                if new_bookmark not in bookmarks:\n                    modified_bookmarks.append(new_bookmark)\n                yield from self._set_bookmarks(modified_bookmarks)\n\n                retries = 0\n                bookmarks = yield from self._get_bookmarks()\n                while retries < max_retries:\n                    if new_bookmark in bookmarks:\n                        break\n                    modified_bookmarks = list(bookmarks)\n                    modified_bookmarks.append(new_bookmark)\n                    yield from self._set_bookmarks(modified_bookmarks)\n                    bookmarks = yield from self._get_bookmarks()\n                    retries += 1\n\n                if new_bookmark not in bookmarks:\n                    raise RuntimeError(\"Could not add bookmark\")\n\n            finally:\n                self._diff_emit_update(bookmarks)", "response": "Add a new bookmark and check whether it was successfully added to the bookmark list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef discard_bookmark(self, bookmark_to_remove, *, max_retries=3):\n        with (yield from self._lock):\n            bookmarks = yield from self._get_bookmarks()\n            occurences = bookmarks.count(bookmark_to_remove)\n\n            try:\n                if not occurences:\n                    return\n\n                modified_bookmarks = list(bookmarks)\n                modified_bookmarks.remove(bookmark_to_remove)\n                yield from self._set_bookmarks(modified_bookmarks)\n\n                retries = 0\n                bookmarks = yield from self._get_bookmarks()\n                new_occurences = bookmarks.count(bookmark_to_remove)\n                while retries < max_retries:\n                    if new_occurences < occurences:\n                        break\n                    modified_bookmarks = list(bookmarks)\n                    modified_bookmarks.remove(bookmark_to_remove)\n                    yield from self._set_bookmarks(modified_bookmarks)\n                    bookmarks = yield from self._get_bookmarks()\n                    new_occurences = bookmarks.count(bookmark_to_remove)\n                    retries += 1\n\n                if new_occurences >= occurences:\n                    raise RuntimeError(\"Could not remove bookmark\")\n            finally:\n                self._diff_emit_update(bookmarks)", "response": "Removes a bookmark from the online storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates a bookmark and check it was successful.", "response": "def update_bookmark(self, old, new, *, max_retries=3):\n        \"\"\"\n        Update a bookmark and check it was successful.\n\n        The bookmark matches an existing bookmark `old` according to\n        bookmark equalitiy and replaces it by `new`. The bookmark\n        `new` is added if no bookmark matching `old` exists.\n\n        :param old: the bookmark to replace\n        :type bookmark_to_remove: a :class:`~bookmark_xso.Bookmark` subclass.\n        :param new: the replacement bookmark\n        :type bookmark_to_remove: a :class:`~bookmark_xso.Bookmark` subclass.\n        :param max_retries: the number of retries of removing the bookmark\n                            fails.\n        :type max_retries: :class:`int`\n\n        :raises RuntimeError: if the bookmark is not in the bookmark list\n                              after `max_retries` retries.\n\n        After replacing the bookmark it is checked, whether the\n        bookmark `new` is in the online storage, if it is not it is\n        tried again at most `max_retries` times to replace the\n        bookmark. A :class:`RuntimeError` is raised if the bookmark\n        could not be replaced successfully after `max_retries`.\n\n        .. note:: Do not modify a bookmark retrieved from the signals\n                  or from :meth:`get_bookmarks` to obtain the bookmark\n                  `new`, this will lead to data corruption as they are\n                  passed by reference.  Instead use :func:`copy.copy`\n                  and modify the copy.\n\n        \"\"\"\n        def replace_bookmark(bookmarks, old, new):\n            modified_bookmarks = list(bookmarks)\n            try:\n                i = bookmarks.index(old)\n                modified_bookmarks[i] = new\n            except ValueError:\n                modified_bookmarks.append(new)\n            return modified_bookmarks\n\n        with (yield from self._lock):\n            bookmarks = yield from self._get_bookmarks()\n\n            try:\n                yield from self._set_bookmarks(\n                    replace_bookmark(bookmarks, old, new)\n                )\n\n                retries = 0\n                bookmarks = yield from self._get_bookmarks()\n                while retries < max_retries:\n                    if new in bookmarks:\n                        break\n                    yield from self._set_bookmarks(\n                        replace_bookmark(bookmarks, old, new)\n                    )\n                    bookmarks = yield from self._get_bookmarks()\n                    retries += 1\n\n                if new not in bookmarks:\n                    raise RuntimeError(\"Cold not update bookmark\")\n            finally:\n                self._diff_emit_update(bookmarks)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an iterator of tuples describing the identities of the node.", "response": "def iter_identities(self, stanza=None):\n        \"\"\"\n        Return an iterator of tuples describing the identities of the node.\n\n        :param stanza: The IQ request stanza\n        :type stanza: :class:`~aioxmpp.IQ` or :data:`None`\n        :rtype: iterable of (:class:`str`, :class:`str`, :class:`str` or\n            :data:`None`, :class:`str` or :data:`None`) tuples\n        :return: :xep:`30` identities of this node\n\n        `stanza` can be the :class:`aioxmpp.IQ` stanza of the request. This can\n        be used to hide a node depending on who is asking. If the returned\n        iterable is empty, the :class:`~.DiscoServer` returns an\n        ``<item-not-found/>`` error.\n\n        `stanza` may be :data:`None` if the identities are queried without\n        a specific request context. In that case, implementors should assume\n        that the result is visible to everybody.\n\n        .. note::\n\n           Subclasses must allow :data:`None` for `stanza` and default it to\n           :data:`None`.\n\n        Return an iterator which yields tuples consisting of the category, the\n        type, the language code and the name of each identity declared in this\n        :class:`Node`.\n\n        Both the language code and the name may be :data:`None`, if no names or\n        a name without language code have been declared.\n        \"\"\"\n        for (category, type_), names in self._identities.items():\n            for lang, name in names.items():\n                yield category, type_, lang, name\n            if not names:\n                yield category, type_, None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an iterator which yields the features of the node.", "response": "def iter_features(self, stanza=None):\n        \"\"\"\n        Return an iterator which yields the features of the node.\n\n        :param stanza: The IQ request stanza\n        :type stanza: :class:`~aioxmpp.IQ`\n        :rtype: iterable of :class:`str`\n        :return: :xep:`30` features of this node\n\n        `stanza` is the :class:`aioxmpp.IQ` stanza of the request. This can be\n        used to filter the list according to who is asking (not recommended).\n\n        `stanza` may be :data:`None` if the features are queried without\n        a specific request context. In that case, implementors should assume\n        that the result is visible to everybody.\n\n        .. note::\n\n           Subclasses must allow :data:`None` for `stanza` and default it to\n           :data:`None`.\n\n        The features are returned as strings. The features demanded by\n        :xep:`30` are always returned.\n\n        \"\"\"\n        return itertools.chain(\n            iter(self.STATIC_FEATURES),\n            iter(self._features)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a feature with the namespace variable var.", "response": "def register_feature(self, var):\n        \"\"\"\n        Register a feature with the namespace variable `var`.\n\n        If the feature is already registered or part of the default :xep:`30`\n        features, a :class:`ValueError` is raised.\n        \"\"\"\n        if var in self._features or var in self.STATIC_FEATURES:\n            raise ValueError(\"feature already claimed: {!r}\".format(var))\n        self._features.add(var)\n        self.on_info_changed()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_identity(self, category, type_, *, names={}):\n        key = category, type_\n        if key in self._identities:\n            raise ValueError(\"identity already claimed: {!r}\".format(key))\n        self._identities[key] = names\n        self.on_info_changed()", "response": "Register an identity with the given category and type_."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_info_xso(self, stanza=None):\n\n        result = disco_xso.InfoQuery()\n        result.features.update(self.iter_features(stanza))\n        result.identities[:] = (\n            disco_xso.Identity(\n                category=category,\n                type_=type_,\n                lang=lang,\n                name=name,\n            )\n            for category, type_, lang, name in self.iter_identities(stanza)\n        )\n        return result", "response": "Constructs a disco. xso. InfoQuery object for this node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clone(cls, other_node):\n\n        result = cls()\n        result._features = {\n            feature for feature in other_node.iter_features()\n            if feature not in cls.STATIC_FEATURES\n        }\n        for category, type_, lang, name in other_node.iter_identities():\n            names = result._identities.setdefault(\n                (category, type_),\n                aioxmpp.structs.LanguageMap()\n            )\n            names[lang] = name\n        result.items = list(other_node.iter_items())\n        return result", "response": "Clone another node and return as a new node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying the features and identities of the specified entity.", "response": "def query_info(self, jid, *,\n                   node=None, require_fresh=False, timeout=None,\n                   no_cache=False):\n        \"\"\"\n        Query the features and identities of the specified entity.\n\n        :param jid: The entity to query.\n        :type jid: :class:`aioxmpp.JID`\n        :param node: The node to query.\n        :type node: :class:`str` or :data:`None`\n        :param require_fresh: Boolean flag to discard previous caches.\n        :type require_fresh: :class:`bool`\n        :param timeout: Optional timeout for the response.\n        :type timeout: :class:`float`\n        :param no_cache: Boolean flag to forbid caching of the request.\n        :type no_cache: :class:`bool`\n        :rtype: :class:`.xso.InfoQuery`\n        :return: Service discovery information of the `node` at `jid`.\n\n        The requests are cached. This means that only one request is ever fired\n        for a given target (identified by the `jid` and the `node`). The\n        request is re-used for all subsequent requests to that identity.\n\n        If `require_fresh` is set to true, the above does not hold and a fresh\n        request is always created. The new request is the request which will be\n        used as alias for subsequent requests to the same identity.\n\n        The visible effects of this are twofold:\n\n        * Caching: Results of requests are implicitly cached\n        * Aliasing: Two concurrent requests will be aliased to one request to\n          save computing resources\n\n        Both can be turned off by using `require_fresh`. In general, you should\n        not need to use `require_fresh`, as all requests are implicitly\n        cancelled whenever the underlying session gets destroyed.\n\n        `no_cache` can be set to true to prevent future requests to be aliased\n        to this request, i.e. the request is not stored in the internal request\n        cache. This does not affect `require_fresh`, i.e. if a cached result is\n        available, it is used.\n\n        The `timeout` can be used to restrict the time to wait for a\n        response. If the timeout triggers, :class:`TimeoutError` is raised.\n\n        If :meth:`~.Client.send` raises an\n        exception, all queries which were running simultanously for the same\n        target re-raise that exception. The result is not cached though. If a\n        new query is sent at a later point for the same target, a new query is\n        actually sent, independent of the value chosen for `require_fresh`.\n\n        .. versionchanged:: 0.9\n\n            The `no_cache` argument was added.\n        \"\"\"\n        key = jid, node\n\n        if not require_fresh:\n            try:\n                request = self._info_pending[key]\n            except KeyError:\n                pass\n            else:\n                try:\n                    return (yield from request)\n                except asyncio.CancelledError:\n                    pass\n\n        request = asyncio.ensure_future(\n            self.send_and_decode_info_query(jid, node)\n        )\n        request.add_done_callback(\n            functools.partial(\n                self._handle_info_received,\n                jid,\n                node\n            )\n        )\n\n        if not no_cache:\n            self._info_pending[key] = request\n        try:\n            if timeout is not None:\n                try:\n                    result = yield from asyncio.wait_for(\n                        request,\n                        timeout=timeout)\n                except asyncio.TimeoutError:\n                    raise TimeoutError()\n            else:\n                result = yield from request\n        except:  # NOQA\n            if request.done():\n                try:\n                    pending = self._info_pending[key]\n                except KeyError:\n                    pass\n                else:\n                    if pending is request:\n                        del self._info_pending[key]\n            raise\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nquery the items of the specified entity at the specified node.", "response": "def query_items(self, jid, *,\n                    node=None, require_fresh=False, timeout=None):\n        \"\"\"\n        Query the items of the specified entity.\n\n        :param jid: The entity to query.\n        :type jid: :class:`aioxmpp.JID`\n        :param node: The node to query.\n        :type node: :class:`str` or :data:`None`\n        :param require_fresh: Boolean flag to discard previous caches.\n        :type require_fresh: :class:`bool`\n        :param timeout: Optional timeout for the response.\n        :type timeout: :class:`float`\n        :rtype: :class:`.xso.ItemsQuery`\n        :return: Service discovery items of the `node` at `jid`.\n\n        The arguments have the same semantics as with :meth:`query_info`, as\n        does the caching and error handling.\n        \"\"\"\n        key = jid, node\n\n        if not require_fresh:\n            try:\n                request = self._items_pending[key]\n            except KeyError:\n                pass\n            else:\n                try:\n                    return (yield from request)\n                except asyncio.CancelledError:\n                    pass\n\n        request_iq = stanza.IQ(to=jid, type_=structs.IQType.GET)\n        request_iq.payload = disco_xso.ItemsQuery(node=node)\n\n        request = asyncio.ensure_future(\n            self.client.send(request_iq)\n        )\n\n        self._items_pending[key] = request\n        try:\n            if timeout is not None:\n                try:\n                    result = yield from asyncio.wait_for(\n                        request,\n                        timeout=timeout)\n                except asyncio.TimeoutError:\n                    raise TimeoutError()\n            else:\n                result = yield from request\n        except:  # NOQA\n            if request.done():\n                try:\n                    pending = self._items_pending[key]\n                except KeyError:\n                    pass\n                else:\n                    if pending is request:\n                        del self._items_pending[key]\n            raise\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_info_cache(self, jid, node, info):\n        fut = asyncio.Future()\n        fut.set_result(info)\n        self.set_info_future(jid, node, fut)", "response": "Set the result of a node as the result of a future."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_info_future(self, jid, node, fut):\n        self._info_pending[jid, node] = fut", "response": "Override the cache entry for the given jid and node combination with the given asyncio. Future fut."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlocalizing and format the string using the given formatter and translator.", "response": "def localize(self, formatter, translator, *args, **kwargs):\n        \"\"\"\n        Localize and format the string using the given `formatter` and\n        `translator`. The remaining args are passed to the\n        :meth:`~LocalizingFormatter.format` method of the `formatter`.\n\n        The `translator` must be an object supporting the\n        :class:`gettext.NullTranslations` interface.\n\n        If :attr:`plural` is not :data:`None`, the number which will be passed\n        to the `ngettext` method of `translator` is first extracted from the\n        `args` or `kwargs`, depending on :attr:`number_index`. The whole\n        semantics of all three are described in\n        :meth:`string.Formatter.get_field`, which is used by this method\n        (:attr:`number_index` is passed as `field_name`).\n\n        The value returned by :meth:`~string.Formatter.get_field` is then used\n        as third argument to `ngettext`, while the others are sourced from\n        :attr:`singular` and :attr:`plural`.\n\n        If :attr:`plural` is :data:`None`, the `gettext` method of `translator`\n        is used with :attr:`singular` as its only argument.\n\n        After the translation step, the `formatter` is used with the translated\n        string and `args` and `kwargs` to obtain a formatted version of the\n        string which is then returned.\n\n        All of this works best when using a :class:`LocalizingFormatter`.\n        \"\"\"\n        if self.plural is not None:\n            n, _ = formatter.get_field(self.number_index, args, kwargs)\n            translated = translator.ngettext(self.singular,\n                                             self.plural,\n                                             n)\n        else:\n            translated = translator.gettext(self.singular)\n        return formatter.vformat(translated, args, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_from_xso_item(self, xso_item):\n        self.subscription = xso_item.subscription\n        self.approved = xso_item.approved\n        self.ask = xso_item.ask\n        self.name = xso_item.name\n        self.groups = {group.name for group in xso_item.groups}", "response": "Update the attributes of this object from the values obtained from the given xso_item."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_xso_item(cls, xso_item):\n        item = cls(xso_item.jid)\n        item.update_from_xso_item(xso_item)\n        return item", "response": "Create a new item from an xso. Item object obtained from the given xso. Item object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_as_json(self):\n        result = {\n            \"subscription\": self.subscription,\n        }\n\n        if self.name:\n            result[\"name\"] = self.name\n\n        if self.ask is not None:\n            result[\"ask\"] = self.ask\n\n        if self.approved:\n            result[\"approved\"] = self.approved\n\n        if self.groups:\n            result[\"groups\"] = sorted(self.groups)\n\n        return result", "response": "Export this item as a JSON - compatible dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_from_json(self, data):\n        self.subscription = data.get(\"subscription\", \"none\")\n        self.approved = bool(data.get(\"approved\", False))\n        self.ask = data.get(\"ask\", None)\n        self.name = data.get(\"name\", None)\n        self.groups = set(data.get(\"groups\", []))", "response": "Update the attributes of this object from the dictionary data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexporting the whole roster as currently stored on the client side into a JSON - compatible dictionary and return that dictionary.", "response": "def export_as_json(self):\n        \"\"\"\n        Export the whole roster as currently stored on the client side into a\n        JSON-compatible dictionary and return that dictionary.\n        \"\"\"\n        return {\n            \"items\": {\n                str(jid): item.export_as_json()\n                for jid, item in self.items.items()\n            },\n            \"ver\": self.version\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport the roster from a JSON dict.", "response": "def import_from_json(self, data):\n        \"\"\"\n        Replace the current roster with the :meth:`export_as_json`-compatible\n        dictionary in `data`.\n\n        No events are fired during this activity. After this method completes,\n        the whole roster contents are exchanged with the contents from `data`.\n\n        Also, no data is transferred to the server; this method is intended to\n        be used for roster versioning. See below (in the docs of\n        :class:`Service`).\n        \"\"\"\n        self.version = data.get(\"ver\", None)\n\n        self.items.clear()\n        self.groups.clear()\n        for jid, data in data.get(\"items\", {}).items():\n            jid = structs.JID.fromstr(jid)\n            item = Item(jid)\n            item.update_from_json(data)\n            self.items[jid] = item\n            for group in item.groups:\n                self.groups.setdefault(group, set()).add(item)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_entry(self, jid, *,\n                  name=_Sentinel,\n                  add_to_groups=frozenset(),\n                  remove_from_groups=frozenset(),\n                  timeout=None):\n        \"\"\"\n        Set properties of a roster entry or add a new roster entry. The roster\n        entry is identified by its bare `jid`.\n\n        If an entry already exists, all values default to those stored in the\n        existing entry. For example, if no `name` is given, the current name of\n        the entry is re-used, if any.\n\n        If the entry does not exist, it will be created on the server side.\n\n        The `remove_from_groups` and `add_to_groups` arguments have to be based\n        on the locally cached state, as XMPP does not support sending\n        diffs. `remove_from_groups` takes precedence over `add_to_groups`.\n\n        `timeout` is the time in seconds to wait for a confirmation by the\n        server.\n\n        Note that the changes may not be visible immediately after his\n        coroutine returns in the :attr:`items` and :attr:`groups`\n        attributes. The :class:`Service` waits for the \"official\" roster push\n        from the server for updating the data structures and firing events, to\n        ensure that consistent state with other clients is achieved.\n\n        This may raise arbitrary :class:`.errors.XMPPError` exceptions if the\n        server replies with an error and also any kind of connection error if\n        the connection gets fatally terminated while waiting for a response.\n        \"\"\"\n\n        existing = self.items.get(jid, Item(jid))\n\n        post_groups = (existing.groups | add_to_groups) - remove_from_groups\n        post_name = existing.name\n        if name is not _Sentinel:\n            post_name = name\n\n        item = roster_xso.Item(\n            jid=jid,\n            name=post_name,\n            groups=[\n                roster_xso.Group(name=group_name)\n                for group_name in post_groups\n            ])\n\n        yield from self.client.send(\n            stanza.IQ(\n                structs.IQType.SET,\n                payload=roster_xso.Query(items=[\n                    item\n                ])\n            ),\n            timeout=timeout\n        )", "response": "Set properties of a roster entry or add a new roster entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_entry(self, jid, *, timeout=None):\n        yield from self.client.send(\n            stanza.IQ(\n                structs.IQType.SET,\n                payload=roster_xso.Query(items=[\n                    roster_xso.Item(\n                        jid=jid,\n                        subscription=\"remove\"\n                    )\n                ])\n            ),\n            timeout=timeout\n        )", "response": "Request removal of the roster entry identified by the given bare\n        jid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef approve(self, peer_jid):\n        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBED,\n                            to=peer_jid)\n        )", "response": "This method is used to approve a subscription request from a peer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting presence subscription with the given peer_jid.", "response": "def subscribe(self, peer_jid):\n        \"\"\"\n        Request presence subscription with the given `peer_jid`.\n\n        This is deliberately not a coroutine; we don\u2019t know whether the peer is\n        online (usually) and they may defer the confirmation very long, if they\n        confirm at all. Use :meth:`on_subscribed` to get notified when a peer\n        accepted a subscription request.\n        \"\"\"\n        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unsubscribe(self, peer_jid):\n        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "response": "Unsubscribe from the presence of the given peer_jid."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters the decorated function as message handler. :param type_: Message type to listen for :type type_: :class:`~.MessageType` :param from_: Sender JIDs to listen for :type from_: :class:`aioxmpp.JID` or :data:`None` :raise TypeError: if the decorated object is a coroutine function .. seealso:: :meth:`~.StanzaStream.register_message_callback` for more details on the `type_` and `from_` arguments .. versionchanged:: 0.9 This is now based on :class:`aioxmpp.dispatcher.SimpleMessageDispatcher`.", "response": "def message_handler(type_, from_):\n    \"\"\"\n    Register the decorated function as message handler.\n\n    :param type_: Message type to listen for\n    :type type_: :class:`~.MessageType`\n    :param from_: Sender JIDs to listen for\n    :type from_: :class:`aioxmpp.JID` or :data:`None`\n    :raise TypeError: if the decorated object is a coroutine function\n\n    .. seealso::\n\n       :meth:`~.StanzaStream.register_message_callback`\n          for more details on the `type_` and `from_` arguments\n\n    .. versionchanged:: 0.9\n\n       This is now based on\n       :class:`aioxmpp.dispatcher.SimpleMessageDispatcher`.\n    \"\"\"\n\n    def decorator(f):\n        if asyncio.iscoroutinefunction(f):\n            raise TypeError(\"message_handler must not be a coroutine function\")\n\n        aioxmpp.service.add_handler_spec(\n            f,\n            aioxmpp.service.HandlerSpec(\n                (_apply_message_handler, (type_, from_)),\n                require_deps=(\n                    SimpleMessageDispatcher,\n                )\n            )\n        )\n        return f\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef presence_handler(type_, from_):\n\n    def decorator(f):\n        if asyncio.iscoroutinefunction(f):\n            raise TypeError(\n                \"presence_handler must not be a coroutine function\"\n            )\n\n        aioxmpp.service.add_handler_spec(\n            f,\n            aioxmpp.service.HandlerSpec(\n                (_apply_presence_handler, (type_, from_)),\n                require_deps=(\n                    SimplePresenceDispatcher,\n                )\n            )\n        )\n        return f\n    return decorator", "response": "Decorator to register a function as presence stanza handler."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_message_handler(type_, from_, cb):\n\n    try:\n        handlers = aioxmpp.service.get_magic_attr(cb)\n    except AttributeError:\n        return False\n\n    return aioxmpp.service.HandlerSpec(\n        (_apply_message_handler, (type_, from_)),\n        require_deps=(\n            SimpleMessageDispatcher,\n        )\n    ) in handlers", "response": "Return True if cb is decorated with message_handler for the\n    given type_ and from_."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_presence_handler(type_, from_, cb):\n\n    try:\n        handlers = aioxmpp.service.get_magic_attr(cb)\n    except AttributeError:\n        return False\n\n    return aioxmpp.service.HandlerSpec(\n        (_apply_presence_handler, (type_, from_)),\n        require_deps=(\n            SimplePresenceDispatcher,\n        )\n    ) in handlers", "response": "Return True if cb has been decorated with presence_handler for\n    the given type_ and from_."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _feed(self, stanza):\n        from_ = stanza.from_\n        if from_ is None:\n            from_ = self.local_jid\n\n        keys = [\n            (stanza.type_, from_, False),\n            (stanza.type_, from_.bare(), True),\n            (None, from_, False),\n            (None, from_.bare(), True),\n            (stanza.type_, None, False),\n            (None, from_, False),\n            (None, None, False),\n        ]\n\n        for key in keys:\n            try:\n                cb = self._map[key]\n            except KeyError:\n                continue\n            cb(stanza)\n            return", "response": "Dispatch the given stanza to the appropriate handler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_callback(self, type_, from_, cb, *,\n                          wildcard_resource=True):\n        \"\"\"\n        Register a callback function.\n\n        :param type_: Stanza type to listen for, or :data:`None` for a\n                      wildcard match.\n        :param from_: Sender to listen for, or :data:`None` for a full wildcard\n                      match.\n        :type from_: :class:`aioxmpp.JID` or :data:`None`\n        :param cb: Callback function to register\n        :param wildcard_resource: Whether to wildcard the resourcepart of the\n                                  JID.\n        :type wildcard_resource: :class:`bool`\n        :raises ValueError: if another function is already registered for the\n                            callback slot.\n\n        `cb` will be called whenever a stanza with the matching `type_` and\n        `from_` is processed. The following wildcarding rules apply:\n\n        1. If the :attr:`~aioxmpp.stanza.StanzaBase.from_` attribute of the\n           stanza has a resourcepart, the following lookup order for callbacks is used:\n\n           +---------------------------+----------------------------------+----------------------+\n           |``type_``                  |``from_``                         |``wildcard_resource`` |\n           +===========================+==================================+======================+\n           |:attr:`~.StanzaBase.type_` |:attr:`~.StanzaBase.from_`        |*any*                 |\n           +---------------------------+----------------------------------+----------------------+\n           |:attr:`~.StanzaBase.type_` |*bare* :attr:`~.StanzaBase.from_` |:data:`True`          |\n           +---------------------------+----------------------------------+----------------------+\n           |:data:`None`               |:attr:`~.StanzaBase.from_`        |*any*                 |\n           +---------------------------+----------------------------------+----------------------+\n           |:data:`None`               |*bare* :attr:`~.StanzaBase.from_` |:data:`True`          |\n           +---------------------------+----------------------------------+----------------------+\n           |:attr:`~.StanzaBase.type_` |:data:`None`                      |*any*                 |\n           +---------------------------+----------------------------------+----------------------+\n           |:data:`None`               |:data:`None`                      |*any*                 |\n           +---------------------------+----------------------------------+----------------------+\n\n        2. If the :attr:`~aioxmpp.stanza.StanzaBase.from_` attribute of the\n           stanza does *not* have a resourcepart, the following lookup order\n           for callbacks is used:\n\n           +---------------------------+---------------------------+----------------------+\n           |``type_``                  |``from_``                  |``wildcard_resource`` |\n           +===========================+===========================+======================+\n           |:attr:`~.StanzaBase.type_` |:attr:`~.StanzaBase.from_` |:data:`False`         |\n           +---------------------------+---------------------------+----------------------+\n           |:data:`None`               |:attr:`~.StanzaBase.from_` |:data:`False`         |\n           +---------------------------+---------------------------+----------------------+\n           |:attr:`~.StanzaBase.type_` |:data:`None`               |*any*                 |\n           +---------------------------+---------------------------+----------------------+\n           |:data:`None`               |:data:`None`               |*any*                 |\n           +---------------------------+---------------------------+----------------------+\n\n        Only the first callback which matches is called. `wildcard_resource` is\n        ignored if `from_` is a full JID or :data:`None`.\n\n        .. note::\n\n           When the server sends a stanza without from attribute, it is\n           replaced with the bare :attr:`local_jid`, as per :rfc:`6120`.\n\n        \"\"\"  # NOQA: E501\n        if from_ is None or not from_.is_bare:\n            wildcard_resource = False\n\n        key = (type_, from_, wildcard_resource)\n        if key in self._map:\n            raise ValueError(\n                \"only one listener allowed per matcher\"\n            )\n\n        self._map[type_, from_, wildcard_resource] = cb", "response": "Register a callback function for the given type and from_."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unregister_callback(self, type_, from_, *,\n                            wildcard_resource=True):\n        \"\"\"\n        Unregister a callback function.\n\n        :param type_: Stanza type to listen for, or :data:`None` for a\n                      wildcard match.\n        :param from_: Sender to listen for, or :data:`None` for a full wildcard\n                      match.\n        :type from_: :class:`aioxmpp.JID` or :data:`None`\n        :param wildcard_resource: Whether to wildcard the resourcepart of the\n                                  JID.\n        :type wildcard_resource: :class:`bool`\n\n        The callback must be disconnected with the same arguments as were used\n        to connect it.\n        \"\"\"\n        if from_ is None or not from_.is_bare:\n            wildcard_resource = False\n\n        self._map.pop((type_, from_, wildcard_resource))", "response": "Unregister a callback function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clone_for(self, other_instance, memo=None):\n\n        if memo is None:\n            result = copy.deepcopy(self)\n        else:\n            result = copy.deepcopy(self, memo)\n        result.instance = other_instance\n        return result", "response": "Clone this bound field for another instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(self, field_xso):\n        if field_xso.desc:\n            self._desc = field_xso.desc\n\n        if field_xso.label:\n            self._label = field_xso.label\n\n        self._required = field_xso.required", "response": "Loads the current value description label and optional options from a data field."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render(self, *, use_local_metadata=True):\n\n        result = forms_xso.Field(\n            var=self.field.var,\n            type_=self.field.FIELD_TYPE,\n        )\n\n        if use_local_metadata:\n            result.desc = self.desc\n            result.label = self.label\n            result.required = self.required\n        else:\n            try:\n                result.desc = self._desc\n            except AttributeError:\n                pass\n\n            try:\n                result.label = self._label\n            except AttributeError:\n                pass\n\n            try:\n                result.required = self._required\n            except AttributeError:\n                pass\n\n        return result", "response": "Render the field in the related object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef value(self):\n        try:\n            return self._value\n        except AttributeError:\n            self.value = self._field.default()\n            return self._value", "response": "A tuple of values. This attribute can be set with any iterable and stored at the bound field."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef value(self):\n        try:\n            return self._value\n        except AttributeError:\n            self._value = self.field.default()\n            return self._value", "response": "The current value of the attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_bound(self, for_instance):\n\n        try:\n            return for_instance._descriptor_data[self]\n        except KeyError:\n            bound = self.create_bound(for_instance)\n            for_instance._descriptor_data[self] = bound\n            return bound", "response": "Create a new bound field class for the given form object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_parser():\n    p = xml.sax.make_parser()\n    p.setFeature(xml.sax.handler.feature_namespaces, True)\n    p.setFeature(xml.sax.handler.feature_external_ges, False)\n    p.setProperty(xml.sax.handler.property_lexical_handler,\n                  XMPPLexicalHandler)\n    return p", "response": "Create a parser which is suitably configured for parsing an XMPP XML\n    stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nserialize a single XSO x to a string.", "response": "def serialize_single_xso(x):\n    \"\"\"\n    Serialize a single XSO `x` to a string. This is potentially very slow and\n    should only be used for debugging purposes. It is generally more efficient\n    to use a :class:`XMPPXMLGenerator` to stream elements.\n    \"\"\"\n    buf = io.BytesIO()\n    gen = XMPPXMLGenerator(buf,\n                           short_empty_elements=True,\n                           sorted_attributes=True)\n    x.unparse_to_sax(gen)\n    return buf.getvalue().decode(\"utf8\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_single_xso(x, dest):\n    gen = XMPPXMLGenerator(dest,\n                           short_empty_elements=True,\n                           sorted_attributes=True)\n    x.unparse_to_sax(gen)", "response": "Write a single XSO x to a binary file - like object dest."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_xso(src, xsomap):\n\n    xso_parser = xso.XSOParser()\n\n    for class_, cb in xsomap.items():\n        xso_parser.add_class(class_, cb)\n\n    driver = xso.SAXDriver(xso_parser)\n\n    parser = xml.sax.make_parser()\n    parser.setFeature(\n        xml.sax.handler.feature_namespaces,\n        True)\n    parser.setFeature(\n        xml.sax.handler.feature_external_ges,\n        False)\n    parser.setContentHandler(driver)\n\n    parser.parse(src)", "response": "Read a single XSO from a binary file - like input src containing an XML\n    document."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_single_xso(src, type_):\n\n    result = None\n\n    def cb(instance):\n        nonlocal result\n        result = instance\n\n    read_xso(src, {type_: cb})\n\n    return result", "response": "Read a single ~. XSO from the binary file - like\n    input src and return the instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef startPrefixMapping(self, prefix, uri, *, auto=False):\n        if     (prefix is not None and\n                (prefix == \"xml\" or\n                 prefix == \"xmlns\" or\n                 not xmlValidateNameValue_str(prefix) or\n                 \":\" in prefix)):\n            raise ValueError(\"not a valid prefix: {!r}\".format(prefix))\n\n        if prefix in self._ns_prefixes_floating_in:\n            raise ValueError(\"prefix already declared for next element\")\n        if auto:\n            self._ns_auto_prefixes_floating_in.add(prefix)\n        self._ns_prefixes_floating_in[prefix] = uri\n        self._ns_decls_floating_in[uri] = prefix", "response": "Start a prefix mapping which maps the given prefix to the given uri."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef startElementNS(self, name, qname, attributes=None):\n        self._finish_pending_start_element()\n        old_counter = self._ns_counter\n\n        qname = self._qname(name)\n        if attributes:\n            attrib = [\n                (self._qname(attrname, attr=True), value)\n                for attrname, value in attributes.items()\n            ]\n            for attrqname, _ in attrib:\n                if attrqname == \"xmlns\":\n                    raise ValueError(\"xmlns not allowed as attribute name\")\n        else:\n            attrib = []\n\n        pending_prefixes = self._pin_floating_ns_decls(old_counter)\n\n        self._write(b\"<\")\n        self._write(qname.encode(\"utf-8\"))\n\n        if None in pending_prefixes:\n            uri = pending_prefixes.pop(None)\n            self._write(b\" xmlns=\")\n            self._write(xml.sax.saxutils.quoteattr(uri).encode(\"utf-8\"))\n\n        for prefix, uri in sorted(pending_prefixes.items()):\n            self._write(b\" xmlns\")\n            if prefix:\n                self._write(b\":\")\n                self._write(prefix.encode(\"utf-8\"))\n            self._write(b\"=\")\n            self._write(\n                xml.sax.saxutils.quoteattr(uri).encode(\"utf-8\")\n            )\n\n        if self._sorted_attributes:\n            attrib.sort()\n\n        for attrname, value in attrib:\n            self._write(b\" \")\n            self._write(attrname.encode(\"utf-8\"))\n            self._write(b\"=\")\n            self._write(\n                xml.sax.saxutils.quoteattr(\n                    value,\n                    self._additional_escapes,\n                ).encode(\"utf-8\")\n            )\n\n        if self._short_empty_elements:\n            self._pending_start_element = name\n        else:\n            self._write(b\">\")", "response": "Starts a sub - element with the specified name and namespace URI."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef endElementNS(self, name, qname):\n        if self._ns_prefixes_floating_out:\n            raise RuntimeError(\"namespace prefix has not been closed\")\n\n        if self._pending_start_element == name:\n            self._pending_start_element = False\n            self._write(b\"/>\")\n        else:\n            self._write(b\"</\")\n            self._write(self._qname(name).encode(\"utf-8\"))\n            self._write(b\">\")\n\n        self._curr_ns_map, self._ns_prefixes_floating_out, self._ns_counter = \\\n            self._ns_map_stack.pop()", "response": "End an element with the specified name and qname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nput characters in the currently open element.", "response": "def characters(self, chars):\n        \"\"\"\n        Put character data in the currently open element. Special characters\n        (such as ``<``, ``>`` and ``&``) are escaped.\n\n        If `chars` contains any ASCII control character, :class:`ValueError` is\n        raised.\n        \"\"\"\n        self._finish_pending_start_element()\n        if not is_valid_cdata_str(chars):\n            raise ValueError(\"control characters are not allowed in \"\n                             \"well-formed XML\")\n        self._write(xml.sax.saxutils.escape(\n            chars,\n            self._additional_escapes,\n        ).encode(\"utf-8\"))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef buffer(self):\n        if self._buf_in_use:\n            raise RuntimeError(\"nested use of buffer() is not supported\")\n        self._buf_in_use = True\n        old_write = self._write\n        old_flush = self._flush\n\n        if self._buf is None:\n            self._buf = io.BytesIO()\n        else:\n            try:\n                self._buf.seek(0)\n                self._buf.truncate()\n            except BufferError:\n                # we need a fresh buffer for this, the other is still in use.\n                self._buf = io.BytesIO()\n\n        self._write = self._buf.write\n        self._flush = None\n        try:\n            with self._save_state():\n                yield\n            old_write(self._buf.getbuffer())\n            if old_flush:\n                old_flush()\n        finally:\n            self._buf_in_use = False\n            self._write = old_write\n            self._flush = old_flush", "response": "A context manager that temporarily buffers the output of the current context manager."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend the stream header as described above.", "response": "def start(self):\n        \"\"\"\n        Send the stream header as described above.\n        \"\"\"\n        attrs = {\n            (None, \"to\"): str(self._to),\n            (None, \"version\"): \".\".join(map(str, self._version))\n        }\n        if self._from:\n            attrs[None, \"from\"] = str(self._from)\n\n        self._writer.startDocument()\n        for prefix, uri in self._nsmap_to_use.items():\n            self._writer.startPrefixMapping(prefix, uri)\n        self._writer.startElementNS(\n            (namespaces.xmlstream, \"stream\"),\n            None,\n            attrs)\n        self._writer.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserialises and sends a single XML stream object.", "response": "def send(self, xso):\n        \"\"\"\n        Send a single XML stream object.\n\n        :param xso: Object to serialise and send.\n        :type xso: :class:`aioxmpp.xso.XSO`\n        :raises Exception: from any serialisation errors, usually\n                           :class:`ValueError`.\n\n        Serialise the `xso` and send it over the stream. If any serialisation\n        error occurs, no data is sent over the stream and the exception is\n        re-raised; the :meth:`send` method thus provides strong exception\n        safety.\n\n        .. warning::\n\n           The behaviour of :meth:`send` after :meth:`abort` or :meth:`close`\n           and before :meth:`start` is undefined.\n\n        \"\"\"\n        with self._writer.buffer():\n            xso.unparse_to_sax(self._writer)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef abort(self):\n        if self._closed:\n            return\n        self._closed = True\n        self._writer.flush()\n        del self._writer", "response": "Abort the stream.\n\n        The stream is flushed and the internal data structures are cleaned up.\n        No stream footer is sent. The stream is :attr:`closed` afterwards.\n\n        If the stream is already :attr:`closed`, this method does nothing."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncloses the internal structures and cleans up the internal structures.", "response": "def close(self):\n        \"\"\"\n        Close the stream.\n\n        The stream footer is sent and the internal structures are cleaned up.\n\n        If the stream is already :attr:`closed`, this method does nothing.\n        \"\"\"\n        if self._closed:\n            return\n        self._closed = True\n        self._writer.endElementNS((namespaces.xmlstream, \"stream\"), None)\n        for prefix in self._nsmap_to_use:\n            self._writer.endPrefixMapping(prefix)\n        self._writer.endDocument()\n        del self._writer"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new query set which requests a specific page.", "response": "def fetch_page(cls, index, max_=None):\n        \"\"\"\n        Return a query set which requests a specific page.\n\n        :param index: Index of the first element of the page to fetch.\n        :type index: :class:`int`\n        :param max_: Maximum number of elements to fetch\n        :type max_: :class:`int` or :data:`None`\n        :rtype: :class:`ResultSetMetadata`\n        :return: A new request set up to request a page starting with the\n                 element indexed by `index`.\n\n        .. note::\n\n           This way of retrieving items may be approximate. See :xep:`59` and\n           the embedding protocol for which RSM is used for specifics.\n        \"\"\"\n\n        result = cls()\n        result.index = index\n        result.max_ = max_\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef limit(self, max_):\n\n        if isinstance(self, type):\n            result = self()\n        else:\n            result = copy.deepcopy(self)\n        result.max_ = max_\n        return result", "response": "Limit the result set to a given number of items."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new query set which requests the next page after this response.", "response": "def next_page(self, max_=None):\n        \"\"\"\n        Return a query set which requests the page after this response.\n\n        :param max_: Maximum number of items to return.\n        :type max_: :class:`int` or :data:`None`\n        :rtype: :class:`ResultSetMetadata`\n        :return: A new request set up to request the next page.\n\n        Must be called on a result set which has :attr:`last` set.\n        \"\"\"\n\n        result = type(self)()\n        result.after = After(self.last.value)\n        result.max_ = max_\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new QuerySet which requests the previous page before this response.", "response": "def previous_page(self, max_=None):\n        \"\"\"\n        Return a query set which requests the page before this response.\n\n        :param max_: Maximum number of items to return.\n        :type max_: :class:`int` or :data:`None`\n        :rtype: :class:`ResultSetMetadata`\n        :return: A new request set up to request the previous page.\n\n        Must be called on a result set which has :attr:`first` set.\n        \"\"\"\n\n        result = type(self)()\n        result.before = Before(self.first.value)\n        result.max_ = max_\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new query set which requests the last page.", "response": "def last_page(self_or_cls, max_=None):\n        \"\"\"\n        Return a query set which requests the last page.\n\n        :param max_: Maximum number of items to return.\n        :type max_: :class:`int` or :data:`None`\n        :rtype: :class:`ResultSetMetadata`\n        :return: A new request set up to request the last page.\n        \"\"\"\n        result = self_or_cls()\n        result.before = Before()\n        result.max_ = max_\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef allow_upcast(self, to):\n\n        if self == to:\n            return True\n        if self == FieldType.TEXT_SINGLE and to == FieldType.TEXT_PRIVATE:\n            return True\n        return False", "response": "Return true if the field type may be upcast to the other field type to."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_form_type(self):\n\n        for field in self.fields:\n            if field.var == \"FORM_TYPE\" and field.type_ == FieldType.HIDDEN:\n                if len(field.values) != 1:\n                    return None\n                return field.values[0]", "response": "Extract the FORM_TYPE value from the fields."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery the server to obtain the fields that need to be filled with the account.", "response": "def get_registration_fields(xmlstream,\n                            timeout=60,\n                            ):\n    \"\"\"\n    A query is sent to the server to obtain the fields that need to be\n    filled to register with the server.\n\n    :param xmlstream: Specifies the stream connected to the server where\n                      the account will be created.\n    :type xmlstream: :class:`aioxmpp.protocol.XMLStream`\n\n    :param timeout: Maximum time in seconds to wait for an IQ response, or\n                    :data:`None` to disable the timeout.\n    :type timeout: :class:`~numbers.Real` or :data:`None`\n\n    :return: :attr:`list`\n    \"\"\"\n\n    iq = aioxmpp.IQ(\n        to=aioxmpp.JID.fromstr(xmlstream._to),\n        type_=aioxmpp.IQType.GET,\n        payload=xso.Query()\n    )\n    iq.autoset_id()\n\n    reply = yield from aioxmpp.protocol.send_and_wait_for(xmlstream,\n                                                          [iq],\n                                                          [aioxmpp.IQ],\n                                                          timeout=timeout)\n    return reply.payload"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register(xmlstream,\n             query_xso,\n             timeout=60,\n             ):\n    \"\"\"\n    Create a new account on the server.\n\n    :param query_xso: XSO with the information needed for the registration.\n    :type query_xso: :class:`~aioxmpp.ibr.Query`\n\n    :param xmlstream: Specifies the stream connected to the server where\n                      the account will be created.\n    :type xmlstream: :class:`aioxmpp.protocol.XMLStream`\n\n    :param timeout: Maximum time in seconds to wait for an IQ response, or\n                    :data:`None` to disable the timeout.\n    :type timeout: :class:`~numbers.Real` or :data:`None`\n    \"\"\"\n    iq = aioxmpp.IQ(\n        to=aioxmpp.JID.fromstr(xmlstream._to),\n        type_=aioxmpp.IQType.SET,\n        payload=query_xso\n    )\n    iq.autoset_id()\n\n    yield from aioxmpp.protocol.send_and_wait_for(xmlstream,\n                                                  [iq],\n                                                  [aioxmpp.IQ],\n                                                  timeout=timeout)", "response": "Register a new account on the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list containing the names of the fields that are used in the xso. Query.", "response": "def get_used_fields(payload):\n    \"\"\"\n    Get a list containing the names of the fields that are used in the\n    xso.Query.\n\n    :param payload: Query object o be\n    :type payload: :class:`~aioxmpp.ibr.Query`\n    :return: :attr:`list`\n    \"\"\"\n    return [\n        tag\n        for tag, descriptor in payload.CHILD_MAP.items()\n        if descriptor.__get__(payload, type(payload)) is not None\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_client_info(self):\n        iq = aioxmpp.IQ(\n            to=self.client.local_jid.bare().replace(localpart=None),\n            type_=aioxmpp.IQType.GET,\n            payload=xso.Query()\n        )\n\n        reply = (yield from self.client.send(iq))\n        return reply", "response": "A query is sent to the server to obtain the client s data stored at the\n        server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef change_pass(self,\n                    new_pass):\n        \"\"\"\n        Change the client password for 'new_pass'.\n\n        :param new_pass: New password of the client.\n        :type new_pass: :class:`str`\n\n        :param old_pass: Old password of the client.\n        :type old_pass: :class:`str`\n\n        \"\"\"\n        iq = aioxmpp.IQ(\n            to=self.client.local_jid.bare().replace(localpart=None),\n            type_=aioxmpp.IQType.SET,\n            payload=xso.Query(self.client.local_jid.localpart, new_pass)\n        )\n\n        yield from self.client.send(iq)", "response": "Change the client password for new_pass."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cancel_registration(self):\n        iq = aioxmpp.IQ(\n            to=self.client.local_jid.bare().replace(localpart=None),\n            type_=aioxmpp.IQType.SET,\n            payload=xso.Query()\n        )\n\n        iq.payload.remove = True\n        yield from self.client.send(iq)", "response": "Cancel the currents client s account with the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndiscovers all connection options for a given domain.", "response": "def discover_connectors(\n        domain: str,\n        loop=None,\n        logger=logger):\n    \"\"\"\n    Discover all connection options for a domain, in descending order of\n    preference.\n\n    This coroutine returns options discovered from SRV records, or if none are\n    found, the generic option using the domain name and the default XMPP client\n    port.\n\n    Each option is represented by a triple ``(host, port, connector)``.\n    `connector` is a :class:`aioxmpp.connector.BaseConnector` instance which is\n    suitable to connect to the given host and port.\n\n    `logger` is the logger used by the function.\n\n    The following sources are supported:\n\n    * :rfc:`6120` SRV records. One option is returned per SRV record.\n\n      If one of the SRV records points to the root name (``.``),\n      :class:`ValueError` is raised (the domain specifically said that XMPP is\n      not supported here).\n\n    * :xep:`368` SRV records. One option is returned per SRV record.\n\n    * :rfc:`6120` fallback process (only if no SRV records are found). One\n      option is returned for the host name with the default XMPP client port.\n\n    The options discovered from SRV records are mixed together, ordered by\n    priority and then within priorities are shuffled according to their weight.\n    Thus, if there are multiple records of equal priority, the result of the\n    function is not deterministic.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    domain_encoded = domain.encode(\"idna\") + b\".\"\n    starttls_srv_failed = False\n    tls_srv_failed = False\n\n    try:\n        starttls_srv_records = yield from network.lookup_srv(\n            domain_encoded,\n            \"xmpp-client\",\n        )\n        starttls_srv_disabled = False\n    except dns.resolver.NoNameservers as exc:\n        starttls_srv_records = []\n        starttls_srv_disabled = False\n        starttls_srv_failed = True\n        starttls_srv_exc = exc\n        logger.debug(\"xmpp-client SRV lookup for domain %s failed \"\n                     \"(may not be fatal)\",\n                     domain_encoded,\n                     exc_info=True)\n    except ValueError:\n        starttls_srv_records = []\n        starttls_srv_disabled = True\n\n    try:\n        tls_srv_records = yield from network.lookup_srv(\n            domain_encoded,\n            \"xmpps-client\",\n        )\n        tls_srv_disabled = False\n    except dns.resolver.NoNameservers:\n        tls_srv_records = []\n        tls_srv_disabled = False\n        tls_srv_failed = True\n        logger.debug(\"xmpps-client SRV lookup for domain %s failed \"\n                     \"(may not be fatal)\",\n                     domain_encoded,\n                     exc_info=True)\n    except ValueError:\n        tls_srv_records = []\n        tls_srv_disabled = True\n\n    if starttls_srv_failed and (tls_srv_failed or tls_srv_records is None):\n        # the failure is probably more useful as a diagnostic\n        # if we find a good reason to allow this scenario, we might change it\n        # later.\n        raise starttls_srv_exc\n\n    if starttls_srv_disabled and (tls_srv_disabled or tls_srv_records is None):\n        raise ValueError(\n            \"XMPP not enabled on domain {!r}\".format(domain),\n        )\n\n    if starttls_srv_records is None and tls_srv_records is None:\n        # no SRV records published, fall back\n        logger.debug(\n            \"no SRV records found for %s, falling back\",\n            domain,\n        )\n        return [\n            (domain, 5222, connector.STARTTLSConnector()),\n        ]\n\n    starttls_srv_records = starttls_srv_records or []\n    tls_srv_records = tls_srv_records or []\n\n    srv_records = [\n        (prio, weight, (host.decode(\"ascii\"), port,\n                        connector.STARTTLSConnector()))\n        for prio, weight, (host, port) in starttls_srv_records\n    ]\n\n    srv_records.extend(\n        (prio, weight, (host.decode(\"ascii\"), port,\n                        connector.XMPPOverTLSConnector()))\n        for prio, weight, (host, port) in tls_srv_records\n    )\n\n    options = list(\n        network.group_and_order_srv_records(srv_records)\n    )\n\n    logger.debug(\n        \"options for %s: %r\",\n        domain,\n        options,\n    )\n\n    return options"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to connect to the specified options and return the transport xmlstream and features.", "response": "def _try_options(options, exceptions,\n                 jid, metadata, negotiation_timeout, loop, logger):\n    \"\"\"\n    Helper function for :func:`connect_xmlstream`.\n    \"\"\"\n    for host, port, conn in options:\n        logger.debug(\n            \"domain %s: trying to connect to %r:%s using %r\",\n            jid.domain, host, port, conn\n        )\n        try:\n            transport, xmlstream, features = yield from conn.connect(\n                loop,\n                metadata,\n                jid.domain,\n                host,\n                port,\n                negotiation_timeout,\n                base_logger=logger,\n            )\n        except OSError as exc:\n            logger.warning(\n                \"connection failed: %s\", exc\n            )\n            exceptions.append(exc)\n            continue\n\n        logger.debug(\n            \"domain %s: connection succeeded using %r\",\n            jid.domain,\n            conn,\n        )\n\n        if not metadata.sasl_providers:\n            return transport, xmlstream, features\n\n        try:\n            features = yield from security_layer.negotiate_sasl(\n                transport,\n                xmlstream,\n                metadata.sasl_providers,\n                negotiation_timeout=None,\n                jid=jid,\n                features=features,\n            )\n        except errors.SASLUnavailable as exc:\n            protocol.send_stream_error_and_close(\n                xmlstream,\n                condition=errors.StreamErrorCondition.POLICY_VIOLATION,\n                text=str(exc),\n            )\n            exceptions.append(exc)\n            continue\n        except Exception as exc:\n            protocol.send_stream_error_and_close(\n                xmlstream,\n                condition=errors.StreamErrorCondition.UNDEFINED_CONDITION,\n                text=str(exc),\n            )\n            raise\n\n        return transport, xmlstream, features\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare and connect an XMLStream to a server using the given metadata.", "response": "def connect_xmlstream(\n        jid,\n        metadata,\n        negotiation_timeout=60.,\n        override_peer=[],\n        loop=None,\n        logger=logger):\n    \"\"\"\n    Prepare and connect a :class:`aioxmpp.protocol.XMLStream` to a server\n    responsible for the given `jid` and authenticate against that server using\n    the SASL mechansims described in `metadata`.\n\n    :param jid: Address of the user for which the connection is made.\n    :type jid: :class:`aioxmpp.JID`\n    :param metadata: Connection metadata for configuring the TLS usage.\n    :type metadata: :class:`~.security_layer.SecurityLayer`\n    :param negotiation_timeout: Timeout for each individual negotiation step.\n    :type negotiation_timeout: :class:`float` in seconds\n    :param override_peer: Sequence of connection options which take precedence\n                          over normal discovery methods.\n    :type override_peer: sequence of (:class:`str`, :class:`int`,\n                         :class:`~.BaseConnector`) triples\n    :param loop: asyncio event loop to use (defaults to current)\n    :type loop: :class:`asyncio.BaseEventLoop`\n    :param logger: Logger to use (defaults to module-wide logger)\n    :type logger: :class:`logging.Logger`\n    :raises ValueError: if the domain from the `jid` announces that XMPP is not\n                        supported at all.\n    :raises aioxmpp.errors.TLSFailure: if all connection attempts fail and one\n                                       of them is a :class:`~.TLSFailure`.\n    :raises aioxmpp.errors.MultiOSError: if all connection attempts fail.\n    :return: Transport, XML stream and the current stream features\n    :rtype: tuple of (:class:`asyncio.BaseTransport`, :class:`~.XMLStream`,\n            :class:`~.nonza.StreamFeatures`)\n\n    The part of the `metadata` specifying the use of TLS is applied. If the\n    security layer does not mandate TLS, the resulting XML stream may not be\n    using TLS. TLS is used whenever possible.\n\n    The connection options in `override_peer` are tried before any standardised\n    discovery of connection options is made. Only if all of them fail,\n    automatic discovery of connection options is performed.\n\n    `loop` may be a :class:`asyncio.BaseEventLoop` to use. Defaults to the\n    current event loop.\n\n    If the domain from the `jid` announces that XMPP is not supported at all,\n    :class:`ValueError` is raised. If no options are returned from\n    :func:`discover_connectors` and `override_peer` is empty,\n    :class:`ValueError` is raised, too.\n\n    If all connection attempts fail, :class:`aioxmpp.errors.MultiOSError` is\n    raised. The error contains one exception for each of the options discovered\n    as well as the elements from `override_peer` in the order they were tried.\n\n    A TLS problem is treated like any other connection problem and the other\n    connection options are considered. However, if *all* connection options\n    fail and the set of encountered errors includes a TLS error, the TLS error\n    is re-raised instead of raising a :class:`aioxmpp.errors.MultiOSError`.\n\n    Return a triple ``(transport, xmlstream, features)``. `transport`\n    the underlying :class:`asyncio.Transport` which is used for the `xmlstream`\n    :class:`~.protocol.XMLStream` instance. `features` is the\n    :class:`aioxmpp.nonza.StreamFeatures` instance describing the features of\n    the stream.\n\n    .. versionadded:: 0.6\n\n    .. versionchanged:: 0.8\n\n       The explicit raising of TLS errors has been introduced. Before, TLS\n       errors were treated like any other connection error, possibly masking\n       configuration problems.\n    \"\"\"\n    loop = asyncio.get_event_loop() if loop is None else loop\n\n    options = list(override_peer)\n\n    exceptions = []\n\n    result = yield from _try_options(\n        options,\n        exceptions,\n        jid, metadata, negotiation_timeout, loop, logger,\n    )\n    if result is not None:\n        return result\n\n    options = list((yield from discover_connectors(\n        jid.domain,\n        loop=loop,\n        logger=logger,\n    )))\n\n    result = yield from _try_options(\n        options,\n        exceptions,\n        jid, metadata, negotiation_timeout, loop, logger,\n    )\n    if result is not None:\n        return result\n\n    if not options and not override_peer:\n        raise ValueError(\"no options to connect to XMPP domain {!r}\".format(\n            jid.domain\n        ))\n\n    for exc in exceptions:\n        if isinstance(exc, errors.TLSFailure):\n            raise exc\n\n    raise errors.MultiOSError(\n        \"failed to connect to XMPP domain {!r}\".format(jid.domain),\n        exceptions\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart the client. If it is already :attr:`running`, :class:`RuntimeError` is raised. While the client is running, it will try to keep an XMPP connection open to the server associated with :attr:`local_jid`.", "response": "def start(self):\n        \"\"\"\n        Start the client. If it is already :attr:`running`,\n        :class:`RuntimeError` is raised.\n\n        While the client is running, it will try to keep an XMPP connection\n        open to the server associated with :attr:`local_jid`.\n        \"\"\"\n        if self.running:\n            raise RuntimeError(\"client already running\")\n\n        self._main_task = asyncio.ensure_future(\n            self._main(),\n            loop=self._loop\n        )\n        self._main_task.add_done_callback(self._on_main_done)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop(self):\n        if not self.running:\n            return\n\n        self.logger.debug(\"stopping main task of %r\", self, stack_info=True)\n        self._main_task.cancel()", "response": "Stop the client. This sends a signal to the clients main task which\n        makes it terminate.\n\n        It may take some cycles through the event loop to stop the client\n        task. To check whether the task has actually stopped, query\n        :attr:`running`."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connected(self, *, presence=structs.PresenceState(False), **kwargs):\n        return UseConnected(self, presence=presence, **kwargs)", "response": "Return a context manager which does not have a presence setting."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef enqueue(self, stanza, **kwargs):\n        if not self.established_event.is_set():\n            raise ConnectionError(\"stream is not ready\")\n\n        return self.stream._enqueue(stanza, **kwargs)", "response": "Enqueue a stanza into the internal transmission queue and return a fresh token to track it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self, stanza, *, timeout=None, cb=None):\n        if not self.running:\n            raise ConnectionError(\"client is not running\")\n\n        if not self.established:\n            self.logger.debug(\"send(%s): stream not established, waiting\",\n                              stanza)\n            # wait for the stream to be established\n            stopped_fut = self.on_stopped.future()\n            failure_fut = self.on_failure.future()\n            established_fut = asyncio.ensure_future(\n                self.established_event.wait()\n            )\n            done, pending = yield from asyncio.wait(\n                [\n                    established_fut,\n                    failure_fut,\n                    stopped_fut,\n                ],\n                return_when=asyncio.FIRST_COMPLETED,\n            )\n            if not established_fut.done():\n                established_fut.cancel()\n            if failure_fut.done():\n                if not stopped_fut.done():\n                    stopped_fut.cancel()\n                failure_fut.exception()\n                raise ConnectionError(\"client failed to connect\")\n            if stopped_fut.done():\n                raise ConnectionError(\"client shut down by user request\")\n\n            self.logger.debug(\"send(%s): stream established, sending\")\n\n        return (yield from self.stream._send_immediately(stanza,\n                                                         timeout=timeout,\n                                                         cb=cb))", "response": "Send a stanza to the XMPP server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the presence state and status on the client.", "response": "def set_presence(self, state, status):\n        \"\"\"\n        Set the presence `state` and `status` on the client. This has the same\n        effects as writing `state` to :attr:`presence`, but the status of the\n        presence is also set at the same time.\n\n        `status` must be either a string or something which can be passed to\n        :class:`dict`. If it is a string, the string is wrapped in a ``{None:\n        status}`` dictionary. Otherwise, the dictionary is set as the\n        :attr:`~.Presence.status` attribute of the presence stanza. It\n        must map :class:`aioxmpp.structs.LanguageTag` instances to strings.\n\n        The `status` is the text shown alongside the `state` (indicating\n        availability such as *away*, *do not disturb* and *free to chat*).\n        \"\"\"\n        self._presence_server.set_presence(state, status=status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds the given conversation and fires the relevant event.", "response": "def _add_conversation(self, conversation):\n        \"\"\"\n        Add the conversation and fire the :meth:`on_conversation_added` event.\n\n        :param conversation: The conversation object to add.\n        :type conversation: :class:`~.AbstractConversation`\n\n        The conversation is added to the internal list of conversations which\n        can be queried at :attr:`conversations`. The\n        :meth:`on_conversation_added` event is fired.\n\n        In addition, the :class:`ConversationService` subscribes to the\n        :meth:`~.AbstractConversation.on_exit` event to remove the conversation\n        from the list automatically. There is no need to remove a conversation\n        from the list explicitly.\n        \"\"\"\n        handler = functools.partial(\n            self._handle_conversation_exit,\n            conversation\n        )\n        tokens = []\n\n        def linked_token(signal, handler):\n            return signal, signal.connect(handler)\n\n        tokens.append(linked_token(conversation.on_exit, handler))\n        tokens.append(linked_token(conversation.on_failure, handler))\n        tokens.append(linked_token(conversation.on_message, functools.partial(\n            self.on_message,\n            conversation,\n        )))\n\n        self._conversation_meta[conversation] = (\n            tokens,\n        )\n        self._conversation_map[conversation.jid] = conversation\n        self.on_conversation_added(conversation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ping(client, peer):\n\n    iq = aioxmpp.IQ(\n        to=peer,\n        type_=aioxmpp.IQType.GET,\n        payload=ping_xso.Ping()\n    )\n\n    yield from client.send(iq)", "response": "Ping a peer.\n\n    :param peer: The peer to ping.\n    :type peer: :class:`aioxmpp.JID`\n    :raises aioxmpp.errors.XMPPError: as received\n\n    Send a :xep:`199` ping IQ to `peer` and wait for the reply.\n\n    This is a low-level version of :meth:`aioxmpp.PingService.ping`.\n\n    **When to use this function vs. the service method:** See\n    :meth:`aioxmpp.PingService.ping`.\n\n    .. note::\n\n        If the peer does not support :xep:`199`, they will respond with\n        a ``cancel`` ``service-unavailable`` error. However, some\n        implementations return a ``cancel`` ``feature-not-implemented``\n        error instead. Callers should be prepared for the\n        :class:`aioxmpp.XMPPCancelError` exceptions in those cases.\n\n    .. versionchanged:: 0.11\n\n        Extracted this helper from :class:`aioxmpp.PingService`."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_header(self, name):\n\n        self._node.register_feature(\n            \"#\".join([namespaces.xep0131_shim, name])\n        )", "response": "Register support for the SHIM header with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unregister_header(self, name):\n\n        self._node.unregister_feature(\n            \"#\".join([namespaces.xep0131_shim, name])\n        )", "response": "Unregister support for the SHIM header with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the vCard stored for the jid. If jid is None get the vCard of the connected entity. If jid is None get the vCard of the connected entity.", "response": "def get_vcard(self, jid=None):\n        \"\"\"\n        Get the vCard stored for the jid `jid`. If `jid` is\n        :data:`None` get the vCard of the connected entity.\n\n        :param jid: the object to retrieve.\n        :returns: the stored vCard.\n\n        We mask a :class:`XMPPCancelError` in case it is\n        ``feature-not-implemented`` or ``item-not-found`` and return\n        an empty vCard, since this can be understood to be semantically\n        equivalent.\n        \"\"\"\n\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.GET,\n            to=jid,\n            payload=vcard_xso.VCard(),\n        )\n\n        try:\n            return (yield from self.client.send(iq))\n        except aioxmpp.XMPPCancelError as e:\n            if e.condition in (\n                    aioxmpp.ErrorCondition.FEATURE_NOT_IMPLEMENTED,\n                    aioxmpp.ErrorCondition.ITEM_NOT_FOUND):\n                return vcard_xso.VCard()\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_vcard(self, vcard):\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            payload=vcard,\n        )\n        yield from self.client.send(iq)", "response": "Store the vCard for the connected entity."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle(self, state, message=False):\n        if message:\n            if state != chatstates_xso.ChatState.ACTIVE:\n                raise ValueError(\n                    \"Only the state ACTIVE can be sent with messages.\"\n                )\n        elif self._state == state:\n            return False\n\n        self._state = state\n        return self._strategy.sending", "response": "Handle a state update."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_application_error(name, tag):\n    cls = type(xso.XSO)(name, (xso.XSO,), {\n        \"TAG\": tag,\n    })\n    Error.as_application_condition(cls)\n    return cls", "response": "Create and return a new application error class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a new error payload from an exception.", "response": "def from_exception(cls, exc):\n        \"\"\"\n        Construct a new :class:`Error` payload from the attributes of the\n        exception.\n\n        :param exc: The exception to convert\n        :type exc: :class:`aioxmpp.errors.XMPPError`\n        :result: Newly constructed error payload\n        :rtype: :class:`Error`\n\n        .. versionchanged:: 0.10\n\n            The :attr:`aioxmpp.XMPPError.application_defined_condition` is now\n            taken over into the result.\n        \"\"\"\n        result = cls(\n            condition=exc.condition,\n            type_=exc.TYPE,\n            text=exc.text\n        )\n        result.application_condition = exc.application_defined_condition\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the error payload to an exception.", "response": "def to_exception(self):\n        \"\"\"\n        Convert the error payload to a :class:`~aioxmpp.errors.XMPPError`\n        subclass.\n\n        :result: Newly constructed exception\n        :rtype: :class:`aioxmpp.errors.XMPPError`\n\n        The exact type of the result depends on the :attr:`type_` (see\n        :class:`~aioxmpp.errors.XMPPError` about the existing subclasses).\n\n        The :attr:`condition_obj`, :attr:`text` and\n        :attr:`application_condition` are transferred to the respective\n        attributes of the :class:`~aioxmpp.errors.XMPPError`.\n        \"\"\"\n        if hasattr(self.application_condition, \"to_exception\"):\n            result = self.application_condition.to_exception(self.type_)\n            if isinstance(result, Exception):\n                return result\n\n        return self.EXCEPTION_CLS_MAP[self.type_](\n            condition=self.condition_obj,\n            text=self.text,\n            application_defined_condition=self.application_condition,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef autoset_id(self):\n        try:\n            self.id_\n        except AttributeError:\n            pass\n        else:\n            if self.id_:\n                return\n\n        self.id_ = to_nmtoken(random.getrandbits(8*RANDOM_ID_BYTES))", "response": "Set the ID of the current XMPP object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new instance of type self with the given error value.", "response": "def make_error(self, error):\n        \"\"\"\n        Create a new instance of this stanza (this directly uses\n        ``type(self)``, so also works for subclasses without extra care) which\n        has the given `error` value set as :attr:`error`.\n\n        In addition, the :attr:`id_`, :attr:`from_` and :attr:`to` values are\n        transferred from the original (with from and to being swapped). Also,\n        the :attr:`type_` is set to ``\"error\"``.\n        \"\"\"\n        obj = type(self)(\n            from_=self.to,\n            to=self.from_,\n            # because flat is better than nested (sarcasm)\n            type_=type(self).type_.type_.enum_class.ERROR,\n        )\n        obj.id_ = self.id_\n        obj.error = error\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a reply for the message.", "response": "def make_reply(self):\n        \"\"\"\n        Create a reply for the message. The :attr:`id_` attribute is cleared in\n        the reply. The :attr:`from_` and :attr:`to` are swapped and the\n        :attr:`type_` attribute is the same as the one of the original\n        message.\n\n        The new :class:`Message` object is returned.\n        \"\"\"\n        obj = super()._make_reply(self.type_)\n        obj.id_ = None\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        if (self._smachine.state == State.CLOSING or\n                self._smachine.state == State.CLOSED):\n            return\n        self._writer.close()\n        if self._transport.can_write_eof():\n            self._transport.write_eof()\n        if self._smachine.state == State.STREAM_HEADER_SENT:\n            # at this point, we cannot wait for the peer to send\n            # </stream:stream>\n            self._close_transport()\n        self._smachine.state = State.CLOSING", "response": "Closes the underlying XML stream and the underlying transport."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef abort(self):\n        if self._smachine.state == State.CLOSED:\n            return\n        if self._smachine.state == State.READY:\n            self._smachine.state = State.CLOSED\n            return\n        if (self._smachine.state != State.CLOSING and\n                self._transport.can_write_eof()):\n            self._transport.write_eof()\n        self._close_transport()", "response": "Abort the stream by writing an EOF if possible and closing the transport."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef starttls(self, ssl_context, post_handshake_callback=None):\n        self._require_connection()\n        if not self.can_starttls():\n            raise RuntimeError(\"starttls not available on transport\")\n\n        yield from self._transport.starttls(ssl_context,\n                                            post_handshake_callback)\n        self._reset_state()", "response": "Start TLS on the transport and wait for it to complete."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a future which will receive the next XML stream error as exception.", "response": "def error_future(self):\n        \"\"\"\n        Return a future which will receive the next XML stream error as\n        exception.\n\n        It is safe to cancel the future at any time.\n        \"\"\"\n        fut = asyncio.Future(loop=self._loop)\n        self._error_futures.append(fut)\n        return fut"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_features(self, jid):\n\n        response = yield from self._disco.query_info(jid)\n        result = set()\n        for feature in response.features:\n            try:\n                result.add(pubsub_xso.Feature(feature))\n            except ValueError:\n                continue\n        return result", "response": "Get the features supported by a service."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsubscribe to a node.", "response": "def subscribe(self, jid, node=None, *,\n                  subscription_jid=None,\n                  config=None):\n        \"\"\"\n        Subscribe to a node.\n\n        :param jid: Address of the PubSub service.\n        :type jid: :class:`aioxmpp.JID`\n        :param node: Name of the PubSub node to subscribe to.\n        :type node: :class:`str`\n        :param subscription_jid: The address to subscribe to the service.\n        :type subscription_jid: :class:`aioxmpp.JID`\n        :param config: Optional configuration of the subscription\n        :type config: :class:`~.forms.Data`\n        :raises aioxmpp.errors.XMPPError: as returned by the service\n        :return: The response from the server.\n        :rtype: :class:`.xso.Request`\n\n        By default, the subscription request will be for the bare JID of the\n        client. It can be specified explicitly using the `subscription_jid`\n        argument.\n\n        If the service requires it or if it makes sense for other reasons, the\n        subscription configuration :class:`~.forms.Data` form can be passed\n        using the `config` argument.\n\n        On success, the whole :class:`.xso.Request` object returned by the\n        server is returned. It contains a :class:`.xso.Subscription`\n        :attr:`~.xso.Request.payload` which has information on the nature of\n        the subscription (it may be ``\"pending\"`` or ``\"unconfigured\"``) and\n        the :attr:`~.xso.Subscription.subid` which may be required for other\n        operations.\n\n        On failure, the corresponding :class:`~.errors.XMPPError` is raised.\n        \"\"\"\n\n        subscription_jid = subscription_jid or self.client.local_jid.bare()\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.SET)\n        iq.payload = pubsub_xso.Request(\n            pubsub_xso.Subscribe(subscription_jid, node=node)\n        )\n\n        if config is not None:\n            iq.payload.options = pubsub_xso.Options(\n                subscription_jid,\n                node=node\n            )\n            iq.payload.options.data = config\n\n        response = yield from self.client.send(iq)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unsubscribe(self, jid, node=None, *,\n                    subscription_jid=None,\n                    subid=None):\n        \"\"\"\n        Unsubscribe from a node.\n\n        :param jid: Address of the PubSub service.\n        :type jid: :class:`aioxmpp.JID`\n        :param node: Name of the PubSub node to unsubscribe from.\n        :type node: :class:`str`\n        :param subscription_jid: The address to subscribe from the service.\n        :type subscription_jid: :class:`aioxmpp.JID`\n        :param subid: Unique ID of the subscription to remove.\n        :type subid: :class:`str`\n        :raises aioxmpp.errors.XMPPError: as returned by the service\n\n        By default, the unsubscribe request will be for the bare JID of the\n        client. It can be specified explicitly using the `subscription_jid`\n        argument.\n\n        If available, the `subid` should also be specified.\n\n        If an error occurs, the corresponding :class:`~.errors.XMPPError` is\n        raised.\n        \"\"\"\n\n        subscription_jid = subscription_jid or self.client.local_jid.bare()\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.SET)\n        iq.payload = pubsub_xso.Request(\n            pubsub_xso.Unsubscribe(subscription_jid, node=node, subid=subid)\n        )\n\n        yield from self.client.send(iq)", "response": "Unsubscribe from a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrequests the current configuration of a subscription.", "response": "def get_subscription_config(self, jid, node=None, *,\n                                subscription_jid=None,\n                                subid=None):\n        \"\"\"\n        Request the current configuration of a subscription.\n\n        :param jid: Address of the PubSub service.\n        :type jid: :class:`aioxmpp.JID`\n        :param node: Name of the PubSub node to query.\n        :type node: :class:`str`\n        :param subscription_jid: The address to query the configuration for.\n        :type subscription_jid: :class:`aioxmpp.JID`\n        :param subid: Unique ID of the subscription to query.\n        :type subid: :class:`str`\n        :raises aioxmpp.errors.XMPPError: as returned by the service\n        :return: The current configuration of the subscription.\n        :rtype: :class:`~.forms.Data`\n\n        By default, the request will be on behalf of the bare JID of the\n        client. It can be overriden using the `subscription_jid` argument.\n\n        If available, the `subid` should also be specified.\n\n        On success, the :class:`~.forms.Data` form is returned.\n\n        If an error occurs, the corresponding :class:`~.errors.XMPPError` is\n        raised.\n        \"\"\"\n\n        subscription_jid = subscription_jid or self.client.local_jid.bare()\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.GET)\n        iq.payload = pubsub_xso.Request()\n        iq.payload.options = pubsub_xso.Options(\n            subscription_jid,\n            node=node,\n            subid=subid,\n        )\n\n        response = yield from self.client.send(iq)\n        return response.options.data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_config(self, jid, node=None):\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.GET)\n        iq.payload = pubsub_xso.Request(\n            pubsub_xso.Default(node=node)\n        )\n\n        response = yield from self.client.send(iq)\n        return response.payload.data", "response": "Request the default configuration of a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrequests the configuration of a node.", "response": "def get_node_config(self, jid, node=None):\n        \"\"\"\n        Request the configuration of a node.\n\n        :param jid: Address of the PubSub service.\n        :type jid: :class:`aioxmpp.JID`\n        :param node: Name of the PubSub node to query.\n        :type node: :class:`str`\n        :raises aioxmpp.errors.XMPPError: as returned by the service\n        :return: The configuration of the node.\n        :rtype: :class:`~.forms.Data`\n\n        On success, the :class:`~.forms.Data` form is returned.\n\n        If an error occurs, the corresponding :class:`~.errors.XMPPError` is\n        raised.\n        \"\"\"\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.GET)\n        iq.payload = pubsub_xso.OwnerRequest(\n            pubsub_xso.OwnerConfigure(node=node)\n        )\n\n        response = yield from self.client.send(iq)\n        return response.payload.data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the configuration of a node.", "response": "def set_node_config(self, jid, config, node=None):\n        \"\"\"\n        Update the configuration of a node.\n\n        :param jid: Address of the PubSub service.\n        :type jid: :class:`aioxmpp.JID`\n        :param config: Configuration form\n        :type config: :class:`aioxmpp.forms.Data`\n        :param node: Name of the PubSub node to query.\n        :type node: :class:`str`\n        :raises aioxmpp.errors.XMPPError: as returned by the service\n        :return: The configuration of the node.\n        :rtype: :class:`~.forms.Data`\n\n        .. seealso::\n\n            :class:`aioxmpp.pubsub.NodeConfigForm`\n        \"\"\"\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.SET)\n        iq.payload = pubsub_xso.OwnerRequest(\n            pubsub_xso.OwnerConfigure(node=node)\n        )\n        iq.payload.payload.data = config\n\n        yield from self.client.send(iq)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_items(self, jid, node, *, max_items=None):\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.GET)\n        iq.payload = pubsub_xso.Request(\n            pubsub_xso.Items(node, max_items=max_items)\n        )\n\n        return (yield from self.client.send(iq))", "response": "Request the most recent items from a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_items_by_id(self, jid, node, ids):\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.GET)\n        iq.payload = pubsub_xso.Request(\n            pubsub_xso.Items(node)\n        )\n\n        iq.payload.payload.items = [\n            pubsub_xso.Item(id_)\n            for id_ in ids\n        ]\n\n        if not iq.payload.payload.items:\n            raise ValueError(\"ids must not be empty\")\n\n        return (yield from self.client.send(iq))", "response": "Request specific items by their IDs from a node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_subscriptions(self, jid, node=None):\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.GET)\n        iq.payload = pubsub_xso.Request(\n            pubsub_xso.Subscriptions(node=node)\n        )\n\n        response = yield from self.client.send(iq)\n        return response.payload", "response": "Get all subscriptions of the local entity to a node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef publish(self, jid, node, payload, *,\n                id_=None,\n                publish_options=None):\n        \"\"\"\n        Publish an item to a node.\n\n        :param jid: Address of the PubSub service.\n        :type jid: :class:`aioxmpp.JID`\n        :param node: Name of the PubSub node to publish to.\n        :type node: :class:`str`\n        :param payload: Registered payload to publish.\n        :type payload: :class:`aioxmpp.xso.XSO`\n        :param id_: Item ID to use for the item.\n        :type id_: :class:`str` or :data:`None`.\n        :param publish_options: A data form with the options for the publish\n            request\n        :type publish_options: :class:`aioxmpp.forms.Data`\n        :raises aioxmpp.errors.XMPPError: as returned by the service\n        :raises RuntimeError: if `publish_options` is not :data:`None` but\n            the service does not support `publish_options`\n        :return: The Item ID which was used to publish the item.\n        :rtype: :class:`str` or :data:`None`\n\n        Publish the given `payload` (which must be a :class:`aioxmpp.xso.XSO`\n        registered with :attr:`.xso.Item.registered_payload`).\n\n        The item is published to `node` at `jid`. If `id_` is given, it is used\n        as the ID for the item. If an item with the same ID already exists at\n        the node, it is replaced. If no ID is given, a ID is generated by the\n        server.\n\n        If `publish_options` is given, it is passed as ``<publish-options/>``\n        element to the server. This needs to be a data form which allows to\n        define e.g. node configuration as a pre-condition to publishing. If\n        the publish-options cannot be satisfied, the server will raise a\n        :attr:`aioxmpp.ErrorCondition.CONFLICT` error.\n\n        If `publish_options` is given and the server does not announce the\n        :attr:`aioxmpp.pubsub.xso.Feature.PUBLISH_OPTIONS` feature,\n        :class:`RuntimeError` is raised to prevent security issues (e.g. if\n        the publish options attempt to assert a restrictive access model).\n\n        Return the ID of the item as published (or :data:`None` if the server\n        does not inform us; this is unfortunately common).\n        \"\"\"\n\n        publish = pubsub_xso.Publish()\n        publish.node = node\n\n        if payload is not None:\n            item = pubsub_xso.Item()\n            item.id_ = id_\n            item.registered_payload = payload\n            publish.item = item\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.SET)\n        iq.payload = pubsub_xso.Request(\n            publish\n        )\n\n        if publish_options is not None:\n            features = yield from self.get_features(jid)\n            if pubsub_xso.Feature.PUBLISH_OPTIONS not in features:\n                raise RuntimeError(\n                    \"publish-options given, but not supported by server\"\n                )\n\n            iq.payload.publish_options = pubsub_xso.PublishOptions()\n            iq.payload.publish_options.data = publish_options\n\n        response = yield from self.client.send(iq)\n\n        if response is not None and response.payload.item is not None:\n            return response.payload.item.id_ or id_\n        return id_", "response": "Publish an item to a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretracts an item from a node at a given ID.", "response": "def retract(self, jid, node, id_, *, notify=False):\n        \"\"\"\n        Retract a previously published item from a node.\n\n        :param jid: Address of the PubSub service.\n        :type jid: :class:`aioxmpp.JID`\n        :param node: Name of the PubSub node to send a notify from.\n        :type node: :class:`str`\n        :param id_: The ID of the item to retract.\n        :type id_: :class:`str`\n        :param notify: Flag indicating whether subscribers shall be notified\n            about the retraction.\n        :type notify: :class:`bool`\n        :raises aioxmpp.errors.XMPPError: as returned by the service\n\n        Retract an item previously published to `node` at `jid`. `id_` must be\n        the ItemID of the item to retract.\n\n        If `notify` is set to true, notifications will be generated (by setting\n        the `notify` attribute on the retraction request).\n        \"\"\"\n        retract = pubsub_xso.Retract()\n        retract.node = node\n        item = pubsub_xso.Item()\n        item.id_ = id_\n        retract.item = item\n        retract.notify = notify\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.SET)\n        iq.payload = pubsub_xso.Request(\n            retract\n        )\n\n        yield from self.client.send(iq)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, jid, node=None):\n\n        create = pubsub_xso.Create()\n        create.node = node\n\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.SET,\n            to=jid,\n            payload=pubsub_xso.Request(create)\n        )\n\n        response = yield from self.client.send(iq)\n\n        if response is not None and response.payload.node is not None:\n            return response.payload.node\n\n        return node", "response": "Create a new node at a service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, jid, node, *, redirect_uri=None):\n\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.SET,\n            to=jid,\n            payload=pubsub_xso.OwnerRequest(\n                pubsub_xso.OwnerDelete(\n                    node,\n                    redirect_uri=redirect_uri\n                )\n            )\n        )\n\n        yield from self.client.send(iq)", "response": "Delete an existing node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_nodes(self, jid, node=None):\n\n        response = yield from self._disco.query_items(\n            jid,\n            node=node,\n        )\n\n        result = []\n        for item in response.items:\n            if item.jid != jid:\n                continue\n            result.append((\n                item.node,\n                item.name,\n            ))\n\n        return result", "response": "Get all nodes available at a service or collection node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_node_affiliations(self, jid, node):\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.GET,\n            to=jid,\n            payload=pubsub_xso.OwnerRequest(\n                pubsub_xso.OwnerAffiliations(node),\n            )\n        )\n\n        return (yield from self.client.send(iq))", "response": "Query the affiliations of other jids at a node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef change_node_affiliations(self, jid, node, affiliations_to_set):\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.SET,\n            to=jid,\n            payload=pubsub_xso.OwnerRequest(\n                pubsub_xso.OwnerAffiliations(\n                    node,\n                    affiliations=[\n                        pubsub_xso.OwnerAffiliation(\n                            jid,\n                            affiliation\n                        )\n                        for jid, affiliation in affiliations_to_set\n                    ]\n                )\n            )\n        )\n\n        yield from self.client.send(iq)", "response": "Modify the affiliations at a node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef change_node_subscriptions(self, jid, node, subscriptions_to_set):\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.SET,\n            to=jid,\n            payload=pubsub_xso.OwnerRequest(\n                pubsub_xso.OwnerSubscriptions(\n                    node,\n                    subscriptions=[\n                        pubsub_xso.OwnerSubscription(\n                            jid,\n                            subscription\n                        )\n                        for jid, subscription in subscriptions_to_set\n                    ]\n                )\n            )\n        )\n\n        yield from self.client.send(iq)", "response": "Change the subscriptions at a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes all items from a node.", "response": "def purge(self, jid, node):\n        \"\"\"\n        Delete all items from a node.\n\n        :param jid: JID of the PubSub service\n        :param node: Name of the PubSub node\n        :type node: :class:`str`\n\n        Requires :attr:`.xso.Feature.PURGE`.\n        \"\"\"\n\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.SET,\n            to=jid,\n            payload=pubsub_xso.OwnerRequest(\n                pubsub_xso.OwnerPurge(\n                    node\n                )\n            )\n        )\n\n        yield from self.client.send(iq)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef capture_events(receiver, dest):\n    # the following code is a copy of the formal definition of `yield from`\n    # in PEP 380, with modifications to capture the value sent during yield\n    _i = iter(receiver)\n    try:\n        _y = next(_i)\n    except StopIteration as _e:\n        return _e.value\n\n    try:\n        while True:\n            try:\n                _s = yield _y\n            except GeneratorExit as _e:\n                try:\n                    _m = _i.close\n                except AttributeError:\n                    pass\n                else:\n                    _m()\n                    raise _e\n            except BaseException as _e:\n                _x = sys.exc_info()\n                try:\n                    _m = _i.throw\n                except AttributeError:\n                    raise _e\n                else:\n                    try:\n                        _y = _m(*_x)\n                    except StopIteration as _e:\n                        _r = _e.value\n                        break\n            else:\n                dest.append(_s)\n                try:\n                    if _s is None:\n                        _y = next(_i)\n                    else:\n                        _y = _i.send(_s)\n                except StopIteration as _e:\n                    _r = _e.value\n                    break\n    except:  # NOQA\n        dest.clear()\n        raise\n    return _r", "response": "Capture all events sent to receiver in the sequence dest."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an iterable of events of XSO events to SAX events by calling the matching SAX methods on dest.", "response": "def events_to_sax(events, dest):\n    \"\"\"\n    Convert an iterable `events` of XSO events to SAX events by calling the\n    matching SAX methods on `dest`\n    \"\"\"\n    name_stack = []\n\n    for ev_type, *ev_args in events:\n        if ev_type == \"start\":\n            name = (ev_args[0], ev_args[1])\n            dest.startElementNS(name, None, ev_args[2])\n            name_stack.append(name)\n        elif ev_type == \"end\":\n            name = name_stack.pop()\n            dest.endElementNS(name, None)\n        elif ev_type == \"text\":\n            dest.characters(ev_args[0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an iterable which produces a sequence of elements from this list that satisfy the given criteria.", "response": "def filter(self, *, type_=None, lang=None, attrs={}):\n        \"\"\"\n        Return an iterable which produces a sequence of the elements inside\n        this :class:`XSOList`, filtered by the criteria given as arguments. The\n        function starts with a working sequence consisting of the whole list.\n\n        If `type_` is not :data:`None`, elements which are not an instance of\n        the given type are excluded from the working sequence.\n\n        If `lang` is not :data:`None`, it must be either a\n        :class:`~.structs.LanguageRange` or an iterable of language ranges. The\n        set of languages present among the working sequence is determined and\n        used for a call to\n        :class:`~.structs.lookup_language`. If the lookup returns a language,\n        all elements whose :attr:`lang` is different from that value are\n        excluded from the working sequence.\n\n        .. note::\n\n           If an iterable of language ranges is given, it is evaluated into a\n           list. This may be of concern if a huge iterable is about to be used\n           for language ranges, but it is an requirement of the\n           :class:`~.structs.lookup_language` function which is used under the\n           hood.\n\n        .. note::\n\n           Filtering by language assumes that the elements have a\n           :class:`~aioxmpp.xso.LangAttr` descriptor named ``lang``.\n\n        If `attrs` is not empty, the filter iterates over each `key`-`value`\n        pair. For each iteration, all elements which do not have an attribute\n        of the name in `key` or where that attribute has a value not equal to\n        `value` are excluded from the working sequence.\n\n        In general, the iterable returned from :meth:`filter` can only be used\n        once. It is dynamic in the sense that changes to elements which are in\n        the list *behind* the last element returned from the iterator will\n        still be picked up when the iterator is resumed.\n        \"\"\"\n        result = self\n        if type_ is not None:\n            result = self._filter_type(result, type_)\n        if lang is not None:\n            result = self._filter_lang(result, lang)\n        if attrs:\n            result = self._filter_attrs(result, attrs)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the given value into the given instance attribute.", "response": "def from_value(self, instance, value):\n        \"\"\"\n        Convert the given value using the set `type_` and store it into\n        `instance`\u2019 attribute.\n        \"\"\"\n        try:\n            parsed = self.type_.parse(value)\n        except (TypeError, ValueError):\n            if self.erroneous_as_absent:\n                return False\n            raise\n        self._set_from_recv(instance, parsed)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nassigning the formatted value stored at instance attribute to the text dest.", "response": "def to_sax(self, instance, dest):\n        \"\"\"\n        Assign the formatted value stored at `instance`\u2019 attribute to the text\n        of `el`.\n\n        If the `value` is :data:`None`, no text is generated.\n        \"\"\"\n        value = self.__get__(instance, type(instance))\n        if value is None:\n            return\n        dest.characters(self.type_.format(value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetects the object to instanciate from the arguments ev_args of the start event and store it in the corresponding descriptor attribute.", "response": "def from_events(self, instance, ev_args, ctx):\n        \"\"\"\n        Detect the object to instanciate from the arguments `ev_args` of the\n        ``\"start\"`` event. The new object is stored at the corresponding\n        descriptor attribute on `instance`.\n\n        This method is suspendable.\n        \"\"\"\n        obj = yield from self._process(instance, ev_args, ctx)\n        self.__set__(instance, obj)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the object associated with this descriptor on instance into a Sax string.", "response": "def to_sax(self, instance, dest):\n        \"\"\"\n        Take the object associated with this descriptor on `instance` and\n        serialize it as child into the given :class:`lxml.etree.Element`\n        `parent`.\n\n        If the object is :data:`None`, no content is generated.\n        \"\"\"\n        obj = self.__get__(instance, type(instance))\n        if obj is None:\n            return\n        obj.unparse_to_sax(dest)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlike :meth:`.Child.from_events`, but instead of replacing the attribute value, the new object is appended to the list.", "response": "def from_events(self, instance, ev_args, ctx):\n        \"\"\"\n        Like :meth:`.Child.from_events`, but instead of replacing the attribute\n        value, the new object is appended to the list.\n        \"\"\"\n\n        obj = yield from self._process(instance, ev_args, ctx)\n        self.__get__(instance, type(instance)).append(obj)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new entry in the list at instance.", "response": "def from_events(self, instance, ev_args, ctx):\n        \"\"\"\n        Collect the events and convert them to a single XML subtree, which then\n        gets appended to the list at `instance`. `ev_args` must be the\n        arguments of the ``\"start\"`` event of the new child.\n\n        This method is suspendable.\n        \"\"\"\n\n        # goal: collect all elements starting with the element for which we got\n        # the start-ev_args in a lxml.etree.Element.\n\n        def make_from_args(ev_args, parent):\n            el = etree.SubElement(parent,\n                                  tag_to_str((ev_args[0], ev_args[1])))\n            for key, value in ev_args[2].items():\n                el.set(tag_to_str(key), value)\n            return el\n\n        root_el = make_from_args(ev_args,\n                                 self.__get__(instance, type(instance)))\n        # create an element stack\n        stack = [root_el]\n        while stack:\n            # we get send all sax-ish events until we return. we return when\n            # the stack is empty, i.e. when our top element ended.\n            ev_type, *ev_args = yield\n            if ev_type == \"start\":\n                # new element started, create and push to stack\n                stack.append(make_from_args(ev_args, stack[-1]))\n            elif ev_type == \"text\":\n                # text for current element\n                curr = stack[-1]\n                if curr.text is not None:\n                    curr.text += ev_args[0]\n                else:\n                    curr.text = ev_args[0]\n            elif ev_type == \"end\":\n                # element ended, remove from stack (it is already appended to\n                # the current element)\n                stack.pop()\n            else:\n                # not in coverage -- this is more like an assertion\n                raise ValueError(ev_type)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling a missing attribute on `instance`. This is called whenever no value for the attribute is found during parsing. The call to :meth:`missing` is independent of the value of `required`. If the `missing` callback is not :data:`None`, it is called with the `instance` and the `ctx` as arguments. If the returned value is not :data:`None`, it is used as the value of the attribute (validation takes place as if the value had been set from the code, not as if the value had been received from XML) and the handler returns. If the `missing` callback is :data:`None` or returns :data:`None`, the handling continues as normal: if `required` is true, a :class:`ValueError` is raised.", "response": "def handle_missing(self, instance, ctx):\n        \"\"\"\n        Handle a missing attribute on `instance`. This is called whenever no\n        value for the attribute is found during parsing. The call to\n        :meth:`missing` is independent of the value of `required`.\n\n        If the `missing` callback is not :data:`None`, it is called with the\n        `instance` and the `ctx` as arguments. If the returned value is not\n        :data:`None`, it is used as the value of the attribute (validation\n        takes place as if the value had been set from the code, not as if the\n        value had been received from XML) and the handler returns.\n\n        If the `missing` callback is :data:`None` or returns :data:`None`, the\n        handling continues as normal: if `required` is true, a\n        :class:`ValueError` is raised.\n        \"\"\"\n        if self.missing is not None:\n            value = self.missing(instance, ctx)\n            if value is not None:\n                self._set_from_code(instance, value)\n                return\n\n        if self.default is _PropBase.NO_DEFAULT:\n            raise ValueError(\"missing attribute {} on {}\".format(\n                tag_to_str(self.tag),\n                tag_to_str(instance.TAG),\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides the implementation from Text by storing the formatted value in the XML attribute instead of the character data.", "response": "def to_dict(self, instance, d):\n        \"\"\"\n        Override the implementation from :class:`Text` by storing the formatted\n        value in the XML attribute instead of the character data.\n\n        If the value is :data:`None`, no element is generated.\n        \"\"\"\n\n        value = self.__get__(instance, type(instance))\n        if value == self.default:\n            return\n\n        d[self.tag] = self.type_.format(value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_events(self, instance, ev_args, ctx):\n\n        # goal: take all text inside the child element and collect it as\n        # attribute value\n\n        attrs = ev_args[2]\n        if attrs and self.attr_policy == UnknownAttrPolicy.FAIL:\n            raise ValueError(\"unexpected attribute (at text only node)\")\n        parts = []\n        while True:\n            ev_type, *ev_args = yield\n            if ev_type == \"text\":\n                # collect ALL TEH TEXT!\n                parts.append(ev_args[0])\n            elif ev_type == \"start\":\n                # ok, a child inside the child was found, we look at our policy\n                # to see what to do\n                yield from enforce_unknown_child_policy(\n                    self.child_policy,\n                    ev_args)\n            elif ev_type == \"end\":\n                # end of our element, return\n                break\n\n        joined = \"\".join(parts)\n        try:\n            parsed = self.type_.parse(joined)\n        except (ValueError, TypeError):\n            if self.erroneous_as_absent:\n                return\n            raise\n        self._set_from_recv(instance, parsed)", "response": "Parse the text data from the event queue and store the result in the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_sax(self, instance, dest):\n\n        value = self.__get__(instance, type(instance))\n        if value == self.default:\n            return\n\n        if self.declare_prefix is not False and self.tag[0]:\n            dest.startPrefixMapping(self.declare_prefix, self.tag[0])\n        dest.startElementNS(self.tag, None, {})\n        try:\n            dest.characters(self.type_.format(value))\n        finally:\n            dest.endElementNS(self.tag, None)\n            if self.declare_prefix is not False and self.tag[0]:\n                dest.endPrefixMapping(self.declare_prefix)", "response": "Convert this attribute into a XML string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fill_into_dict(self, items, dest):\n        for item in items:\n            dest[self.key(item)].append(item)", "response": "Takes an iterable of items and adds them into the given dict dest."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_events(self, instance, ev_args, ctx):\n\n        tag = ev_args[0], ev_args[1]\n        cls = self._tag_map[tag]\n        obj = yield from cls.parse_events(ev_args, ctx)\n        mapping = self.__get__(instance, type(instance))\n        mapping[self.key(obj)].append(obj)", "response": "A generator that yields objects from the given events."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_events(cls, ev_args, parent_ctx):\n        with parent_ctx as ctx:\n            obj = cls.__new__(cls)\n            attrs = ev_args[2]\n            attr_map = cls.ATTR_MAP.copy()\n            for key, value in attrs.items():\n                try:\n                    prop = attr_map.pop(key)\n                except KeyError:\n                    if cls.UNKNOWN_ATTR_POLICY == UnknownAttrPolicy.DROP:\n                        continue\n                    else:\n                        raise ValueError(\n                            \"unexpected attribute {!r} on {}\".format(\n                                key,\n                                tag_to_str((ev_args[0], ev_args[1]))\n                            )) from None\n                try:\n                    if not prop.from_value(obj, value):\n                        # assignment failed due to recoverable error, treat as\n                        # absent\n                        attr_map[key] = prop\n                except Exception:\n                    prop.mark_incomplete(obj)\n                    _mark_attributes_incomplete(attr_map.values(), obj)\n                    logger.debug(\"while parsing XSO %s (%r)\", cls,\n                                 value,\n                                 exc_info=True)\n                    # true means suppress\n                    if not obj.xso_error_handler(\n                            prop,\n                            value,\n                            sys.exc_info()):\n                        raise\n\n            for key, prop in attr_map.items():\n                try:\n                    prop.handle_missing(obj, ctx)\n                except Exception:\n                    logger.debug(\"while parsing XSO %s\", cls,\n                                 exc_info=True)\n                    # true means suppress\n                    if not obj.xso_error_handler(\n                            prop,\n                            None,\n                            sys.exc_info()):\n                        raise\n\n            try:\n                prop = cls.ATTR_MAP[namespaces.xml, \"lang\"]\n            except KeyError:\n                pass\n            else:\n                lang = prop.__get__(obj, cls)\n                if lang is not None:\n                    ctx.lang = lang\n\n            collected_text = []\n            while True:\n                ev_type, *ev_args = yield\n                if ev_type == \"end\":\n                    break\n                elif ev_type == \"text\":\n                    if not cls.TEXT_PROPERTY:\n                        if ev_args[0].strip():\n                            # true means suppress\n                            if not obj.xso_error_handler(\n                                    None,\n                                    ev_args[0],\n                                    None):\n                                raise ValueError(\"unexpected text\")\n                    else:\n                        collected_text.append(ev_args[0])\n                elif ev_type == \"start\":\n                    try:\n                        handler = cls.CHILD_MAP[ev_args[0], ev_args[1]]\n                    except KeyError:\n                        if cls.COLLECTOR_PROPERTY:\n                            handler = cls.COLLECTOR_PROPERTY.xq_descriptor\n                        else:\n                            yield from enforce_unknown_child_policy(\n                                cls.UNKNOWN_CHILD_POLICY,\n                                ev_args,\n                                obj.xso_error_handler)\n                            continue\n                    try:\n                        yield from guard(\n                            handler.from_events(obj, ev_args, ctx),\n                            ev_args\n                        )\n                    except Exception:\n                        logger.debug(\"while parsing XSO %s\", type(obj),\n                                     exc_info=True)\n                        # true means suppress\n                        if not obj.xso_error_handler(\n                                handler,\n                                ev_args,\n                                sys.exc_info()):\n                            raise\n\n            if collected_text:\n                collected_text = \"\".join(collected_text)\n                try:\n                    cls.TEXT_PROPERTY.xq_descriptor.from_value(\n                        obj,\n                        collected_text\n                    )\n                except Exception:\n                    logger.debug(\"while parsing XSO\", exc_info=True)\n                    # true means suppress\n                    if not obj.xso_error_handler(\n                            cls.TEXT_PROPERTY.xq_descriptor,\n                            collected_text,\n                            sys.exc_info()):\n                        raise\n\n        obj.validate()\n\n        obj.xso_after_load()\n\n        return obj", "response": "Parse the events sent into this base class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register_child(cls, prop, child_cls):\n        if cls.__subclasses__():\n            raise TypeError(\n                \"register_child is forbidden on classes with subclasses\"\n                \" (subclasses: {})\".format(\n                    \", \".join(map(str, cls.__subclasses__()))\n                ))\n\n        if child_cls.TAG in cls.CHILD_MAP:\n            raise ValueError(\"ambiguous Child\")\n\n        prop.xq_descriptor._register(child_cls)\n        cls.CHILD_MAP[child_cls.TAG] = prop.xq_descriptor", "response": "Registers a new child class for a given XMLStreamClass instance property prop."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncapture the events sent to :meth:`.XSO.parse_events`, including the initial `ev_args` to a list and call :meth:`_set_captured_events` on the result of :meth:`.XSO.parse_events`. Like the method it overrides, :meth:`parse_events` is suspendable.", "response": "def parse_events(cls, ev_args, parent_ctx):\n        \"\"\"\n        Capture the events sent to :meth:`.XSO.parse_events`,\n        including the initial `ev_args` to a list and call\n        :meth:`_set_captured_events` on the result of\n        :meth:`.XSO.parse_events`.\n\n        Like the method it overrides, :meth:`parse_events` is suspendable.\n        \"\"\"\n\n        dest = [(\"start\", )+tuple(ev_args)]\n        result = yield from capture_events(\n            super().parse_events(ev_args, parent_ctx),\n            dest\n        )\n        result._set_captured_events(dest)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_class(self, cls, callback):\n        if cls.TAG in self._tag_map:\n            raise ValueError(\n                \"duplicate tag: {!r} is already handled by {}\".format(\n                    cls.TAG,\n                    self._tag_map[cls.TAG]))\n        self._class_map[cls] = callback\n        self._tag_map[cls.TAG] = (cls, callback)", "response": "Add a class to parsing as root level element."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a class from parsing.", "response": "def remove_class(self, cls):\n        \"\"\"\n        Remove a XSO class `cls` from parsing. This method raises\n        :class:`KeyError` with the classes :attr:`TAG` attribute as argument if\n        removing fails because the class is not registered.\n        \"\"\"\n        del self._tag_map[cls.TAG]\n        del self._class_map[cls]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a check against the character set in chars and tables. Returns the first character occuring in any of the tables or None if no such character occurs.", "response": "def check_against_tables(chars, tables):\n    \"\"\"\n    Perform a check against the table predicates in `tables`. `tables` must be\n    a reusable iterable containing characteristic functions of character sets,\n    that is, functions which return :data:`True` if the character is in the\n    table.\n\n    The function returns the first character occuring in any of the tables or\n    :data:`None` if no character matches.\n    \"\"\"\n\n    for c in chars:\n        if any(in_table(c) for in_table in tables):\n            return c\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that the given unicode string is valid for proper bidirectionality as per stringprep. Operates on a list of unicode characters provided in chars.", "response": "def check_bidi(chars):\n    \"\"\"\n    Check proper bidirectionality as per stringprep. Operates on a list of\n    unicode characters provided in `chars`.\n    \"\"\"\n\n    # the empty string is valid, as it cannot violate the RandALCat constraints\n    if not chars:\n        return\n\n    # first_is_RorAL = unicodedata.bidirectional(chars[0]) in {\"R\", \"AL\"}\n    # if first_is_RorAL:\n\n    has_RandALCat = any(is_RandALCat(c) for c in chars)\n    if not has_RandALCat:\n        return\n\n    has_LCat = any(is_LCat(c) for c in chars)\n    if has_LCat:\n        raise ValueError(\"L and R/AL characters must not occur in the same\"\n                         \" string\")\n\n    if not is_RandALCat(chars[0]) or not is_RandALCat(chars[-1]):\n        raise ValueError(\"R/AL string must start and end with R/AL character.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether the input contains a prohibited output.", "response": "def check_prohibited_output(chars, bad_tables):\n    \"\"\"\n    Check against prohibited output, by checking whether any of the characters\n    from `chars` are in any of the `bad_tables`.\n\n    Operates in-place on a list of code points from `chars`.\n    \"\"\"\n    violator = check_against_tables(chars, bad_tables)\n    if violator is not None:\n        raise ValueError(\"Input contains invalid unicode codepoint: \"\n                         \"U+{:04x}\".format(ord(violator)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_unassigned(chars, bad_tables):\n    bad_tables = (\n        stringprep.in_table_a1,)\n\n    violator = check_against_tables(chars, bad_tables)\n    if violator is not None:\n        raise ValueError(\"Input contains unassigned code point: \"\n                         \"U+{:04x}\".format(ord(violator)))", "response": "Checks that chars does not contain any unassigned code points as per\n    the given list of bad_tables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess the given string using the Nodeprep profile.", "response": "def nodeprep(string, allow_unassigned=False):\n    \"\"\"\n    Process the given `string` using the Nodeprep (`RFC 6122`_) profile. In the\n    error cases defined in `RFC 3454`_ (stringprep), a :class:`ValueError` is\n    raised.\n    \"\"\"\n\n    chars = list(string)\n    _nodeprep_do_mapping(chars)\n    do_normalization(chars)\n    check_prohibited_output(\n        chars,\n        (\n            stringprep.in_table_c11,\n            stringprep.in_table_c12,\n            stringprep.in_table_c21,\n            stringprep.in_table_c22,\n            stringprep.in_table_c3,\n            stringprep.in_table_c4,\n            stringprep.in_table_c5,\n            stringprep.in_table_c6,\n            stringprep.in_table_c7,\n            stringprep.in_table_c8,\n            stringprep.in_table_c9,\n            lambda x: x in _nodeprep_prohibited\n        ))\n    check_bidi(chars)\n\n    if not allow_unassigned:\n        check_unassigned(\n            chars,\n            (\n                stringprep.in_table_a1,\n            )\n        )\n\n    return \"\".join(chars)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resourceprep(string, allow_unassigned=False):\n\n    chars = list(string)\n    _resourceprep_do_mapping(chars)\n    do_normalization(chars)\n    check_prohibited_output(\n        chars,\n        (\n            stringprep.in_table_c12,\n            stringprep.in_table_c21,\n            stringprep.in_table_c22,\n            stringprep.in_table_c3,\n            stringprep.in_table_c4,\n            stringprep.in_table_c5,\n            stringprep.in_table_c6,\n            stringprep.in_table_c7,\n            stringprep.in_table_c8,\n            stringprep.in_table_c9,\n        ))\n    check_bidi(chars)\n\n    if not allow_unassigned:\n        check_unassigned(\n            chars,\n            (\n                stringprep.in_table_a1,\n            )\n        )\n\n    return \"\".join(chars)", "response": "Process the given string using the Resourceprep profile."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an avatar image to the avatar set.", "response": "def add_avatar_image(self, mime_type, *, id_=None,\n                         image_bytes=None, width=None, height=None,\n                         url=None, nbytes=None):\n        \"\"\"\n        Add a source of the avatar image.\n\n        All sources of an avatar image added to an avatar set must be\n        *the same image*, in different formats and sizes.\n\n        :param mime_type: The MIME type of the avatar image.\n        :param id_: The SHA1 of the image data.\n        :param nbytes: The size of the image data in bytes.\n        :param image_bytes: The image data, this must be supplied only\n                            in one call.\n        :param url: The URL of the avatar image.\n        :param height: The height of the image in pixels (optional).\n        :param width: The width of the image in pixels (optional).\n\n        `id_` and `nbytes` may be omitted if and only if `image_data`\n        is given and `mime_type` is ``image/png``. If they are\n        supplied *and* image data is given, they are checked to match\n        the image data.\n\n        It is the caller's responsibility to assure that the provided\n        links exist and the files have the correct SHA1 sums.\n        \"\"\"\n\n        if mime_type == \"image/png\":\n            if image_bytes is not None:\n                if self._image_bytes is not None:\n                    raise RuntimeError(\n                        \"Only one avatar image may be published directly.\"\n                    )\n\n                sha1 = hashlib.sha1()\n                sha1.update(image_bytes)\n                id_computed = normalize_id(sha1.hexdigest())\n                if id_ is not None:\n                    id_ = normalize_id(id_)\n                    if id_ != id_computed:\n                        raise RuntimeError(\n                            \"The given id does not match the SHA1 of \"\n                            \"the image data.\"\n                        )\n                else:\n                    id_ = id_computed\n\n                nbytes_computed = len(image_bytes)\n                if nbytes is not None:\n                    if nbytes != nbytes_computed:\n                        raise RuntimeError(\n                            \"The given length does not match the length \"\n                            \"of the image data.\"\n                        )\n                else:\n                    nbytes = nbytes_computed\n\n                self._image_bytes = image_bytes\n                self._png_id = id_\n\n        if image_bytes is None and url is None:\n            raise RuntimeError(\n                \"Either the image bytes or an url to retrieve the avatar \"\n                \"image must be given.\"\n            )\n\n        if nbytes is None:\n            raise RuntimeError(\n                \"Image data length is not given an not inferable \"\n                \"from the other arguments.\"\n            )\n\n        if id_ is None:\n            raise RuntimeError(\n                \"The SHA1 of the image data is not given an not inferable \"\n                \"from the other arguments.\"\n            )\n\n        if image_bytes is not None and mime_type != \"image/png\":\n            raise RuntimeError(\n                \"The image bytes can only be given for image/png data.\"\n            )\n\n        self._metadata.info[mime_type].append(\n            avatar_xso.Info(\n                id_=id_, mime_type=mime_type, nbytes=nbytes,\n                width=width, height=height, url=url\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the avatar metadata for a given JID.", "response": "def get_avatar_metadata(self, jid, *, require_fresh=False,\n                            disable_pep=False):\n        \"\"\"\n        Retrieve a list of avatar descriptors.\n\n        :param jid: the JID for which to retrieve the avatar metadata.\n        :type jid: :class:`aioxmpp.JID`\n        :param require_fresh: if true, do not return results from the\n            avatar metadata chache, but retrieve them again from the server.\n        :type require_fresh: :class:`bool`\n        :param disable_pep: if true, do not try to retrieve the avatar\n            via pep, only try the vCard fallback. This usually only\n            useful when querying avatars via MUC, where the PEP request\n            would be invalid (since it would be for a full jid).\n        :type disable_pep: :class:`bool`\n\n        :returns: an iterable of avatar descriptors.\n        :rtype: a :class:`list` of\n            :class:`~aioxmpp.avatar.service.AbstractAvatarDescriptor`\n            instances\n\n        Returning an empty list means that the avatar not set.\n\n        We mask a :class:`XMPPCancelError` in the case that it is\n        ``feature-not-implemented`` or ``item-not-found`` and return\n        an empty list of avatar descriptors, since this is\n        semantically equivalent to not having an avatar.\n\n        .. note::\n\n           It is usually an error to get the avatar for a full jid,\n           normally, the avatar is set for the bare jid of a user. The\n           exception are vCard avatars over MUC, where the IQ requests\n           for the vCard may be translated by the MUC server. It is\n           recommended to use the `disable_pep` option in that case.\n        \"\"\"\n\n        if require_fresh:\n            self._metadata_cache.pop(jid, None)\n        else:\n            try:\n                return self._metadata_cache[jid]\n            except KeyError:\n                pass\n\n        if disable_pep:\n            metadata = []\n        else:\n            metadata = yield from self._get_avatar_metadata_pep(jid)\n\n        # try the vcard fallback, note: we don't try this\n        # if the PEP avatar is disabled!\n        if not metadata and jid not in self._has_pep_avatar:\n            metadata = yield from self._get_avatar_metadata_vcard(jid)\n\n        # if a notify was fired while we waited for the results, then\n        # use the version in the cache, this will mitigate the race\n        # condition because if our version is actually newer we will\n        # soon get another notify for this version change!\n        if jid not in self._metadata_cache:\n            self._update_metadata(jid, metadata)\n        return self._metadata_cache[jid]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npublishes avatar_set to the broker.", "response": "def publish_avatar_set(self, avatar_set):\n        \"\"\"\n        Make `avatar_set` the current avatar of the jid associated with this\n        connection.\n\n        If :attr:`synchronize_vcard` is true and PEP is available the\n        vCard is only synchronized if the PEP update is successful.\n\n        This means publishing the ``image/png`` avatar data and the\n        avatar metadata set in pubsub. The `avatar_set` must be an\n        instance of :class:`AvatarSet`. If :attr:`synchronize_vcard` is\n        true the avatar is additionally published in the user vCard.\n        \"\"\"\n        id_ = avatar_set.png_id\n\n        done = False\n        with (yield from self._publish_lock):\n            if (yield from self._pep.available()):\n                yield from self._pep.publish(\n                    namespaces.xep0084_data,\n                    avatar_xso.Data(avatar_set.image_bytes),\n                    id_=id_\n                )\n\n                yield from self._pep.publish(\n                    namespaces.xep0084_metadata,\n                    avatar_set.metadata,\n                    id_=id_\n                )\n                done = True\n\n            if self._synchronize_vcard:\n                my_vcard = yield from self._vcard.get_vcard()\n                my_vcard.set_photo_data(\"image/png\",\n                                        avatar_set.image_bytes)\n                self._vcard_id = avatar_set.png_id\n                yield from self._vcard.set_vcard(my_vcard)\n                self._presence_server.resend_presence()\n                done = True\n\n        if not done:\n            raise RuntimeError(\n                \"failed to publish avatar: no protocol available\"\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wipe_avatar(self):\n\n        @asyncio.coroutine\n        def _wipe_pep_avatar():\n            yield from self._pep.publish(\n                namespaces.xep0084_metadata,\n                avatar_xso.Metadata()\n            )\n            yield from self._pep.publish(\n                namespaces.xep0084_data,\n                avatar_xso.Data(b'')\n            )\n\n        with (yield from self._publish_lock):\n            todo = []\n            if self._synchronize_vcard:\n                todo.append(self._disable_vcard_avatar())\n\n            if (yield from self._pep.available()):\n                todo.append(_wipe_pep_avatar())\n\n            yield from gather_reraise_multi(*todo, message=\"wipe_avatar\")", "response": "Remove all avatar data stored on the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef block_jids(self, jids_to_block):\n        yield from self._check_for_blocking()\n\n        if not jids_to_block:\n            return\n\n        cmd = blocking_xso.BlockCommand(jids_to_block)\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            payload=cmd,\n        )\n        yield from self.client.send(iq)", "response": "Block the specified set of JIDs in the client s cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nunblocking the specified set of JIDs from the client s blocklist.", "response": "def unblock_jids(self, jids_to_unblock):\n        \"\"\"\n        Remove the JIDs in the sequence `jids_to_block` from the\n        client's blocklist.\n        \"\"\"\n        yield from self._check_for_blocking()\n\n        if not jids_to_unblock:\n            return\n\n        cmd = blocking_xso.UnblockCommand(jids_to_unblock)\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            payload=cmd,\n        )\n        yield from self.client.send(iq)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _register_descriptor_keys(self, descriptor, keys):\n\n        if descriptor.root_class is not self or self.__subclasses__():\n            raise TypeError(\n                \"descriptors cannot be modified on classes with subclasses\"\n            )\n\n        meta = type(self)\n        descriptor_info = meta._upcast_descriptor_map(\n            self.DESCRIPTOR_MAP,\n            \"{}.{}\".format(self.__module__, self.__qualname__),\n        )\n\n        # this would raise on conflict\n        meta._merge_descriptors(\n            descriptor_info,\n            [\n                (key, (descriptor, \"<added via _register_descriptor_keys>\"))\n                for key in keys\n            ]\n        )\n\n        for key in keys:\n            self.DESCRIPTOR_MAP[key] = descriptor", "response": "Registers the given descriptor keys for the given class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_xso(self, xso):\n\n        my_form_type = getattr(self, \"FORM_TYPE\", None)\n\n        f = self()\n        for field in xso.fields:\n            if field.var == \"FORM_TYPE\":\n                if (my_form_type is not None and\n                        field.type_ == forms_xso.FieldType.HIDDEN and\n                        field.values):\n                    if my_form_type != field.values[0]:\n                        raise ValueError(\n                            \"mismatching FORM_TYPE ({!r} != {!r})\".format(\n                                field.values[0],\n                                my_form_type,\n                            )\n                        )\n                continue\n            if field.var is None:\n                continue\n\n            key = fields.descriptor_ns, field.var\n            try:\n                descriptor = self.DESCRIPTOR_MAP[key]\n            except KeyError:\n                continue\n\n            if (field.type_ is not None and not\n                    field.type_.allow_upcast(descriptor.FIELD_TYPE)):\n                raise ValueError(\n                    \"mismatching type ({!r} != {!r}) on field var={!r}\".format(\n                        field.type_,\n                        descriptor.FIELD_TYPE,\n                        field.var,\n                    )\n                )\n\n            data = descriptor.__get__(f, self)\n            data.load(field)\n\n        f._recv_xso = xso\n\n        return f", "response": "Construct and return an instance of this class from the given xso."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_reply(self):\n\n        data = copy.copy(self._recv_xso)\n        data.type_ = forms_xso.DataType.SUBMIT\n        data.fields = list(self._recv_xso.fields)\n\n        for i, field_xso in enumerate(data.fields):\n            if field_xso.var is None:\n                continue\n            if field_xso.var == \"FORM_TYPE\":\n                continue\n            key = fields.descriptor_ns, field_xso.var\n            try:\n                descriptor = self.DESCRIPTOR_MAP[key]\n            except KeyError:\n                continue\n\n            bound_field = descriptor.__get__(self, type(self))\n            data.fields[i] = bound_field.render(\n                use_local_metadata=False\n            )\n\n        return data", "response": "Render the reply to the form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_request(self):\n\n        data = forms_xso.Data(type_=forms_xso.DataType.FORM)\n\n        try:\n            layout = self.LAYOUT\n        except AttributeError:\n            layout = list(self.DESCRIPTORS)\n\n        my_form_type = getattr(self, \"FORM_TYPE\", None)\n        if my_form_type is not None:\n            field_xso = forms_xso.Field()\n            field_xso.var = \"FORM_TYPE\"\n            field_xso.type_ = forms_xso.FieldType.HIDDEN\n            field_xso.values[:] = [my_form_type]\n            data.fields.append(field_xso)\n\n        for item in layout:\n            if isinstance(item, str):\n                field_xso = forms_xso.Field()\n                field_xso.type_ = forms_xso.FieldType.FIXED\n                field_xso.values[:] = [item]\n            else:\n                field_xso = item.__get__(\n                    self, type(self)\n                ).render()\n            data.fields.append(field_xso)\n\n        return data", "response": "Render the request for the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlooks up the given `node` URL using the given `hash_` first in the database and then by waiting on the futures created with :meth:`create_query_future` for that node URL and hash. If the hash is not in the database, :meth:`lookup` iterates as long as there are pending futures for the given `hash_` and `node`. If there are no pending futures, :class:`KeyError` is raised. If a future raises a :class:`ValueError`, it is ignored. If the future returns a value, it is used as the result.", "response": "def lookup(self, key):\n        \"\"\"\n        Look up the given `node` URL using the given `hash_` first in the\n        database and then by waiting on the futures created with\n        :meth:`create_query_future` for that node URL and hash.\n\n        If the hash is not in the database, :meth:`lookup` iterates as long as\n        there are pending futures for the given `hash_` and `node`. If there\n        are no pending futures, :class:`KeyError` is raised. If a future raises\n        a :class:`ValueError`, it is ignored. If the future returns a value, it\n        is used as the result.\n        \"\"\"\n        try:\n            result = self.lookup_in_database(key)\n        except KeyError:\n            pass\n        else:\n            return result\n\n        while True:\n            fut = self._lookup_cache[key]\n            try:\n                result = yield from fut\n            except ValueError:\n                continue\n            else:\n                return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate and return a future for the given hash_node_url.", "response": "def create_query_future(self, key):\n        \"\"\"\n        Create and return a :class:`asyncio.Future` for the given `hash_`\n        function and `node` URL. The future is referenced internally and used\n        by any calls to :meth:`lookup` which are made while the future is\n        pending. The future is removed from the internal storage automatically\n        when a result or exception is set for it.\n\n        This allows for deduplication of queries for the same hash.\n        \"\"\"\n        fut = asyncio.Future()\n        fut.add_done_callback(\n            functools.partial(self._erase_future, key)\n        )\n        self._lookup_cache[key] = fut\n        return fut"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the given entry to the user - level database keyed with the given key.", "response": "def add_cache_entry(self, key, entry):\n        \"\"\"\n        Add the given `entry` (which must be a :class:`~.disco.xso.InfoQuery`\n        instance) to the user-level database keyed with the hash function type\n        `hash_` and the `node` URL. The `entry` is **not** validated to\n        actually map to `node` with the given `hash_` function, it is expected\n        that the caller perfoms the validation.\n        \"\"\"\n        copied_entry = copy.copy(entry)\n        self._memory_overlay[key] = copied_entry\n        if self._user_db_path is not None:\n            asyncio.ensure_future(asyncio.get_event_loop().run_in_executor(\n                None,\n                writeback,\n                self._user_db_path / key.path,\n                entry.captured_events))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef claim_pep_node(self, node_namespace, *,\n                       register_feature=True, notify=False):\n        \"\"\"\n        Claim node `node_namespace`.\n\n        :param node_namespace: the pubsub node whose events shall be\n            handled.\n        :param register_feature: Whether to publish the `node_namespace`\n            as feature.\n        :param notify: Whether to register the ``+notify`` feature to\n            receive notification without explicit subscription.\n        :raises RuntimeError: if a handler for `node_namespace` is already\n            set.\n        :returns: a :class:`~aioxmpp.pep.service.RegisteredPEPNode` instance\n            representing the claim.\n\n        .. seealso::\n\n            :class:`aioxmpp.pep.register_pep_node`\n              a descriptor which can be used with\n              :class:`~aioxmpp.service.Service` subclasses to claim a PEP node\n              automatically.\n\n        This registers `node_namespace` as feature for service discovery\n        unless ``register_feature=False`` is passed.\n\n        .. note::\n\n            For `notify` to work, it is required that\n            :class:`aioxmpp.EntityCapsService` is loaded and that presence is\n            re-sent soon after\n            :meth:`~aioxmpp.EntityCapsService.on_ver_changed` fires. See the\n            documentation of the class and the signal for details.\n\n        \"\"\"\n        if node_namespace in self._pep_node_claims:\n            raise RuntimeError(\n                \"claiming already claimed node\"\n            )\n        registered_node = RegisteredPEPNode(\n            self,\n            node_namespace,\n            register_feature=register_feature,\n            notify=notify,\n        )\n\n        finalizer = weakref.finalize(\n            registered_node,\n            weakref.WeakMethod(registered_node._unregister)\n        )\n        # we cannot guarantee that disco is not cleared up already,\n        # so we do not unclaim the feature on exit\n        finalizer.atexit = False\n\n        self._pep_node_claims[node_namespace] = registered_node\n\n        return registered_node", "response": "This method claims a node with the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck whether we have a PEP identity associated with our account.", "response": "def available(self):\n        \"\"\"\n        Check whether we have a PEP identity associated with our account.\n        \"\"\"\n        disco_info = yield from self._disco_client.query_info(\n            self.client.local_jid.bare()\n        )\n\n        for item in disco_info.identities.filter(attrs={\"category\": \"pubsub\"}):\n            if item.type_ == \"pep\":\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef publish(self, node, data, *, id_=None, access_model=None):\n        publish_options = None\n\n        def autocreate_publish_options():\n            nonlocal publish_options\n            if publish_options is None:\n                publish_options = aioxmpp.forms.Data(\n                    aioxmpp.forms.DataType.SUBMIT\n                )\n                publish_options.fields.append(\n                    aioxmpp.forms.Field(\n                        type_=aioxmpp.forms.FieldType.HIDDEN,\n                        var=\"FORM_TYPE\",\n                        values=[\n                            \"http://jabber.org/protocol/pubsub#publish-options\"\n                        ]\n                    )\n                )\n            return publish_options\n\n        if access_model is not None:\n            autocreate_publish_options()\n            publish_options.fields.append(aioxmpp.forms.Field(\n                var=\"pubsub#access_model\",\n                values=[access_model],\n            ))\n\n        yield from self._check_for_pep()\n        return (yield from self._pubsub.publish(\n            None, node, data, id_=id_,\n            publish_options=publish_options\n        ))", "response": "Publish an item in the PubSub service associated with the user s JID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self):\n        if self._closed:\n            return\n\n        self._closed = True\n        self._pep_service._unclaim(self.node_namespace)\n        self._unregister()", "response": "Closes the PEP node and unregisters the registered features."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef full_clean(self, exclude, validate_unique=False):\n\n        # validate against neomodel\n        try:\n            self.deflate(self.__properties__, self)\n        except DeflateError as e:\n            raise ValidationError({e.property_name: e.msg})\n        except RequiredProperty as e:\n            raise ValidationError({e.property_name: 'is required'})", "response": "Validate node against neomodel and validate that it is not conflicting"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef email_user(self, subject, message, from_email=None):\n        send_mail(subject, message, from_email, [self.email])", "response": "Send an email to this User."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nactivating the selected users if they are not already activated.", "response": "def activate_users(self, request, queryset):\n        \"\"\"\n        Activates the selected users, if they are not already\n        activated.\n\n        \"\"\"\n        n = 0\n        for user in queryset:\n            if not user.is_active:\n                user.activate()\n                n += 1\n        self.message_user(\n            request,\n            _('Successfully activated %(count)d %(items)s.') %\n            {'count': n, 'items': model_ngettext(self.opts, n)},  messages.SUCCESS)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending activation emails for the selected users if they are not already activated.", "response": "def send_activation_email(self, request, queryset):\n        \"\"\"\n        Send activation emails for the selected users, if they are not already\n        activated.\n        \"\"\"\n        n = 0\n        for user in queryset:\n            if not user.is_active and settings.USERS_VERIFY_EMAIL:\n                send_activation_email(user=user, request=request)\n                n += 1\n\n        self.message_user(\n            request, _('Activation emails sent to %(count)d %(items)s.') %\n            {'count': n, 'items': model_ngettext(self.opts, n)},  messages.SUCCESS)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfixing get_query_set vs get_queryset for Django < 1. 6", "response": "def get_queryset(self):\n        \"\"\"\n        Fixes get_query_set vs get_queryset for Django <1.6\n        \"\"\"\n        try:\n            qs = super(UserManager, self).get_queryset()\n        except AttributeError:  # pragma: no cover\n            qs = super(UserManager, self).get_query_set()\n        return qs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nam Socket Ready. :rtype: tuple", "response": "def is_ready(self):\n        \"\"\"Is Socket Ready.\n\n        :rtype: tuple\n        \"\"\"\n        try:\n            ready, _, _ = self.select.select([self.fileno], [], [],\n                                             POLL_TIMEOUT)\n            return bool(ready)\n        except self.select.error as why:\n            if why.args[0] != EINTR:\n                self._exceptions.append(AMQPConnectionError(why))\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open(self):\n        self._wr_lock.acquire()\n        self._rd_lock.acquire()\n        try:\n            self.data_in = EMPTY_BUFFER\n            self._running.set()\n            sock_addresses = self._get_socket_addresses()\n            self.socket = self._find_address_and_connect(sock_addresses)\n            self.poller = Poller(self.socket.fileno(), self._exceptions,\n                                 timeout=self._parameters['timeout'])\n            self._inbound_thread = self._create_inbound_thread()\n        finally:\n            self._wr_lock.release()\n            self._rd_lock.release()", "response": "Open Socket and establish a connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites data to the socket.", "response": "def write_to_socket(self, frame_data):\n        \"\"\"Write data to the socket.\n\n        :param str frame_data:\n        :return:\n        \"\"\"\n        self._wr_lock.acquire()\n        try:\n            total_bytes_written = 0\n            bytes_to_send = len(frame_data)\n            while total_bytes_written < bytes_to_send:\n                try:\n                    if not self.socket:\n                        raise socket.error('connection/socket error')\n                    bytes_written = (\n                        self.socket.send(frame_data[total_bytes_written:])\n                    )\n                    if bytes_written == 0:\n                        raise socket.error('connection/socket error')\n                    total_bytes_written += bytes_written\n                except socket.timeout:\n                    pass\n                except socket.error as why:\n                    if why.args[0] in (EWOULDBLOCK, EAGAIN):\n                        continue\n                    self._exceptions.append(AMQPConnectionError(why))\n                    return\n        finally:\n            self._wr_lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the socket addresses.", "response": "def _get_socket_addresses(self):\n        \"\"\"Get Socket address information.\n\n        :rtype: list\n        \"\"\"\n        family = socket.AF_UNSPEC\n        if not socket.has_ipv6:\n            family = socket.AF_INET\n        try:\n            addresses = socket.getaddrinfo(self._parameters['hostname'],\n                                           self._parameters['port'], family,\n                                           socket.SOCK_STREAM)\n        except socket.gaierror as why:\n            raise AMQPConnectionError(why)\n        return addresses"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_address_and_connect(self, addresses):\n        error_message = None\n        for address in addresses:\n            sock = self._create_socket(socket_family=address[0])\n            try:\n                sock.connect(address[4])\n            except (IOError, OSError) as why:\n                error_message = why.strerror\n                continue\n            return sock\n        raise AMQPConnectionError(\n            'Could not connect to %s:%d error: %s' % (\n                self._parameters['hostname'], self._parameters['port'],\n                error_message\n            )\n        )", "response": "Find and connect to the appropriate address."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_socket(self, socket_family):\n        sock = socket.socket(socket_family, socket.SOCK_STREAM, 0)\n        sock.settimeout(self._parameters['timeout'] or None)\n        if self.use_ssl:\n            if not compatibility.SSL_SUPPORTED:\n                raise AMQPConnectionError(\n                    'Python not compiled with support for TLSv1 or higher'\n                )\n            sock = self._ssl_wrap_socket(sock)\n        return sock", "response": "Create a socket with the specified socket family and return it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap the given socket around the SSLSocket.", "response": "def _ssl_wrap_socket(self, sock):\n        \"\"\"Wrap SSLSocket around the Socket.\n\n        :param socket.socket sock:\n        :rtype: SSLSocket\n        \"\"\"\n        context = self._parameters['ssl_options'].get('context')\n        if context is not None:\n            hostname = self._parameters['ssl_options'].get('server_hostname')\n            return context.wrap_socket(\n                sock, do_handshake_on_connect=True,\n                server_hostname=hostname\n            )\n        if 'ssl_version' not in self._parameters['ssl_options']:\n            self._parameters['ssl_options']['ssl_version'] = (\n                compatibility.DEFAULT_SSL_VERSION\n            )\n        return ssl.wrap_socket(\n            sock, do_handshake_on_connect=True,\n            **self._parameters['ssl_options']\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves and process any incoming data.", "response": "def _process_incoming_data(self):\n        \"\"\"Retrieve and process any incoming data.\n\n        :return:\n        \"\"\"\n        while self._running.is_set():\n            if self.poller.is_ready:\n                self.data_in += self._receive()\n                self.data_in = self._on_read_impl(self.data_in)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _receive(self):\n        data_in = EMPTY_BUFFER\n        try:\n            data_in = self._read_from_socket()\n        except socket.timeout:\n            pass\n        except (IOError, OSError) as why:\n            if why.args[0] not in (EWOULDBLOCK, EAGAIN):\n                self._exceptions.append(AMQPConnectionError(why))\n                self._running.clear()\n        return data_in", "response": "Receive any incoming socket data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading data from the socket.", "response": "def _read_from_socket(self):\n        \"\"\"Read data from the socket.\n\n        :rtype: bytes\n        \"\"\"\n        if not self.use_ssl:\n            if not self.socket:\n                raise socket.error('connection/socket error')\n            return self.socket.recv(MAX_FRAME_SIZE)\n\n        with self._rd_lock:\n            if not self.socket:\n                raise socket.error('connection/socket error')\n            return self.socket.read(MAX_FRAME_SIZE)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start(self, exceptions):\n        if not self._interval:\n            return False\n        self._running.set()\n        with self._lock:\n            self._threshold = 0\n            self._reads_since_check = 0\n            self._writes_since_check = 0\n        self._exceptions = exceptions\n        LOGGER.debug('Heartbeat Checker Started')\n        return self._start_new_timer()", "response": "Start the Heartbeat Checker."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstopping the Heartbeat Checker.", "response": "def stop(self):\n        \"\"\"Stop the Heartbeat Checker.\n\n        :return:\n        \"\"\"\n        self._running.clear()\n        with self._lock:\n            if self._timer:\n                self._timer.cancel()\n            self._timer = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking Connection for life signs.", "response": "def _check_for_life_signs(self):\n        \"\"\"Check Connection for life signs.\n\n            First check if any data has been sent, if not send a heartbeat\n            to the remote server.\n\n            If we have not received any data what so ever within two\n            intervals, we need to raise an exception so that we can\n            close the connection.\n\n        :rtype: bool\n        \"\"\"\n        if not self._running.is_set():\n            return False\n        if self._writes_since_check == 0:\n            self.send_heartbeat_impl()\n        self._lock.acquire()\n        try:\n            if self._reads_since_check == 0:\n                self._threshold += 1\n                if self._threshold >= 2:\n                    self._running.clear()\n                    self._raise_or_append_exception()\n                    return False\n            else:\n                self._threshold = 0\n        finally:\n            self._reads_since_check = 0\n            self._writes_since_check = 0\n            self._lock.release()\n\n        return self._start_new_timer()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nraise or appends an exception to the internal list of exceptions.", "response": "def _raise_or_append_exception(self):\n        \"\"\"The connection is presumably dead and we need to raise or\n        append an exception.\n\n            If we have a list for exceptions, append the exception and let\n            the connection handle it, if not raise the exception here.\n\n        :return:\n        \"\"\"\n        message = (\n            'Connection dead, no heartbeat or data received in >= '\n            '%ds' % (\n                self._interval * 2\n            )\n        )\n        why = AMQPConnectionError(message)\n        if self._exceptions is None:\n            raise why\n        self._exceptions.append(why)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _start_new_timer(self):\n        if not self._running.is_set():\n            return False\n        self._timer = self.timer_impl(\n            interval=self._interval,\n            function=self._check_for_life_signs\n        )\n        self._timer.daemon = True\n        self._timer.start()\n        return True", "response": "Create a timer that will be used to check the availability of the available resources."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef declare(self, exchange='', exchange_type='direct', passive=False,\n                durable=False, auto_delete=False, arguments=None):\n        \"\"\"Declare an Exchange.\n\n        :param str exchange: Exchange name\n        :param str exchange_type: Exchange type\n        :param bool passive: Do not create\n        :param bool durable: Durable exchange\n        :param bool auto_delete: Automatically delete when not in use\n        :param dict arguments: Exchange key/value arguments\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict\n        \"\"\"\n        if not compatibility.is_string(exchange):\n            raise AMQPInvalidArgument('exchange should be a string')\n        elif not compatibility.is_string(exchange_type):\n            raise AMQPInvalidArgument('exchange_type should be a string')\n        elif not isinstance(passive, bool):\n            raise AMQPInvalidArgument('passive should be a boolean')\n        elif not isinstance(durable, bool):\n            raise AMQPInvalidArgument('durable should be a boolean')\n        elif not isinstance(auto_delete, bool):\n            raise AMQPInvalidArgument('auto_delete should be a boolean')\n        elif arguments is not None and not isinstance(arguments, dict):\n            raise AMQPInvalidArgument('arguments should be a dict or None')\n\n        declare_frame = pamqp_exchange.Declare(exchange=exchange,\n                                               exchange_type=exchange_type,\n                                               passive=passive,\n                                               durable=durable,\n                                               auto_delete=auto_delete,\n                                               arguments=arguments)\n        return self._channel.rpc_request(declare_frame)", "response": "Declare an Exchange.\n\n        :param str exchange: Exchange name\n        :param str exchange_type: Exchange type\n        :param bool passive: Do not create\n        :param bool durable: Durable exchange\n        :param bool auto_delete: Automatically delete when not in use\n        :param dict arguments: Exchange key/value arguments\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting an Exchange. :param str exchange: Exchange name :param bool if_unused: Delete only if unused :raises AMQPInvalidArgument: Invalid Parameters :raises AMQPChannelError: Raises if the channel encountered an error. :raises AMQPConnectionError: Raises if the connection encountered an error. :rtype: dict", "response": "def delete(self, exchange='', if_unused=False):\n        \"\"\"Delete an Exchange.\n\n        :param str exchange: Exchange name\n        :param bool if_unused: Delete only if unused\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict\n        \"\"\"\n        if not compatibility.is_string(exchange):\n            raise AMQPInvalidArgument('exchange should be a string')\n\n        delete_frame = pamqp_exchange.Delete(exchange=exchange,\n                                             if_unused=if_unused)\n        return self._channel.rpc_request(delete_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbind an exchange to a specific routing key.", "response": "def bind(self, destination='', source='', routing_key='',\n             arguments=None):\n        \"\"\"Bind an Exchange.\n\n        :param str destination: Exchange name\n        :param str source: Exchange to bind to\n        :param str routing_key: The routing key to use\n        :param dict arguments: Bind key/value arguments\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict\n        \"\"\"\n        if not compatibility.is_string(destination):\n            raise AMQPInvalidArgument('destination should be a string')\n        elif not compatibility.is_string(source):\n            raise AMQPInvalidArgument('source should be a string')\n        elif not compatibility.is_string(routing_key):\n            raise AMQPInvalidArgument('routing_key should be a string')\n        elif arguments is not None and not isinstance(arguments, dict):\n            raise AMQPInvalidArgument('arguments should be a dict or None')\n\n        bind_frame = pamqp_exchange.Bind(destination=destination,\n                                         source=source,\n                                         routing_key=routing_key,\n                                         arguments=arguments)\n        return self._channel.rpc_request(bind_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unbind(self, destination='', source='', routing_key='',\n               arguments=None):\n        \"\"\"Unbind an Exchange.\n\n        :param str destination: Exchange name\n        :param str source: Exchange to unbind from\n        :param str routing_key: The routing key used\n        :param dict arguments: Unbind key/value arguments\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict\n        \"\"\"\n        if not compatibility.is_string(destination):\n            raise AMQPInvalidArgument('destination should be a string')\n        elif not compatibility.is_string(source):\n            raise AMQPInvalidArgument('source should be a string')\n        elif not compatibility.is_string(routing_key):\n            raise AMQPInvalidArgument('routing_key should be a string')\n        elif arguments is not None and not isinstance(arguments, dict):\n            raise AMQPInvalidArgument('arguments should be a dict or None')\n\n        unbind_frame = pamqp_exchange.Unbind(destination=destination,\n                                             source=source,\n                                             routing_key=routing_key,\n                                             arguments=arguments)\n        return self._channel.rpc_request(unbind_frame)", "response": "Unbind an exchange from one of the sources."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets Queue details. :param queue: Queue name :param str virtual_host: Virtual host name :raises ApiError: Raises if the remote server encountered an error. :raises ApiConnectionError: Raises if there was a connectivity issue. :rtype: dict", "response": "def get(self, queue, virtual_host='/'):\n        \"\"\"Get Queue details.\n\n        :param queue: Queue name\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(\n            API_QUEUE % (\n                virtual_host,\n                queue\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list(self, virtual_host='/', show_all=False):\n        if show_all:\n            return self.http_client.get(API_QUEUES)\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(\n            API_QUEUES_VIRTUAL_HOST % virtual_host\n        )", "response": "List Queues.\n\n        :param str virtual_host: Virtual host name\n        :param bool show_all: List all Queues\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef declare(self, queue='', virtual_host='/', passive=False, durable=False,\n                auto_delete=False, arguments=None):\n        \"\"\"Declare a Queue.\n\n        :param str queue: Queue name\n        :param str virtual_host: Virtual host name\n        :param bool passive: Do not create\n        :param bool durable: Durable queue\n        :param bool auto_delete: Automatically delete when not in use\n        :param dict|None arguments: Queue key/value arguments\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        if passive:\n            return self.get(queue, virtual_host=virtual_host)\n\n        queue_payload = json.dumps(\n            {\n                'durable': durable,\n                'auto_delete': auto_delete,\n                'arguments': arguments or {},\n                'vhost': virtual_host\n            }\n        )\n        return self.http_client.put(\n            API_QUEUE % (\n                quote(virtual_host, ''),\n                queue\n            ),\n            payload=queue_payload)", "response": "Declare a Queue.\n\n        :param str queue: Queue name\n        :param str virtual_host: Virtual host name\n        :param bool passive: Do not create\n        :param bool durable: Durable queue\n        :param bool auto_delete: Automatically delete when not in use\n        :param dict|None arguments: Queue key/value arguments\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, queue, virtual_host='/'):\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.delete(API_QUEUE %\n                                       (\n                                           virtual_host,\n                                           queue\n                                       ))", "response": "Delete a Queue.\n\n        :param str queue: Queue name\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npurge a Queue. :param str queue: Queue name :param str virtual_host: Virtual host name :raises ApiError: Raises if the remote server encountered an error. :raises ApiConnectionError: Raises if there was a connectivity issue. :rtype: None", "response": "def purge(self, queue, virtual_host='/'):\n        \"\"\"Purge a Queue.\n\n        :param str queue: Queue name\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: None\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.delete(API_QUEUE_PURGE %\n                                       (\n                                           virtual_host,\n                                           queue\n                                       ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget Queue bindings. :param str queue: Queue name :param str virtual_host: Virtual host name :raises ApiError: Raises if the remote server encountered an error. :raises ApiConnectionError: Raises if there was a connectivity issue. :rtype: list", "response": "def bindings(self, queue, virtual_host='/'):\n        \"\"\"Get Queue bindings.\n\n        :param str queue: Queue name\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: list\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(API_QUEUE_BINDINGS %\n                                    (\n                                        virtual_host,\n                                        queue\n                                    ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bind(self, queue='', exchange='', routing_key='', virtual_host='/',\n             arguments=None):\n        \"\"\"Bind a Queue.\n\n        :param str queue: Queue name\n        :param str exchange: Exchange name\n        :param str routing_key: The routing key to use\n        :param str virtual_host: Virtual host name\n        :param dict|None arguments: Bind key/value arguments\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: None\n        \"\"\"\n        bind_payload = json.dumps({\n            'destination': queue,\n            'destination_type': 'q',\n            'routing_key': routing_key,\n            'source': exchange,\n            'arguments': arguments or {},\n            'vhost': virtual_host\n        })\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.post(API_QUEUE_BIND %\n                                     (\n                                         virtual_host,\n                                         exchange,\n                                         queue\n                                     ),\n                                     payload=bind_payload)", "response": "Bind a Queue to an exchange."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nunbinds a Queue. :param str queue: Queue name :param str exchange: Exchange name :param str routing_key: The routing key to use :param str virtual_host: Virtual host name :param str properties_key: :raises ApiError: Raises if the remote server encountered an error. :raises ApiConnectionError: Raises if there was a connectivity issue. :rtype: None", "response": "def unbind(self, queue='', exchange='', routing_key='', virtual_host='/',\n               properties_key=None):\n        \"\"\"Unbind a Queue.\n\n        :param str queue: Queue name\n        :param str exchange: Exchange name\n        :param str routing_key: The routing key to use\n        :param str virtual_host: Virtual host name\n        :param str properties_key:\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: None\n        \"\"\"\n        unbind_payload = json.dumps({\n            'destination': queue,\n            'destination_type': 'q',\n            'properties_key': properties_key or routing_key,\n            'source': exchange,\n            'vhost': virtual_host\n        })\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.delete(API_QUEUE_UNBIND %\n                                       (\n                                           virtual_host,\n                                           exchange,\n                                           queue,\n                                           properties_key or routing_key\n                                       ),\n                                       payload=unbind_payload)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, exchange, virtual_host='/'):\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(\n            API_EXCHANGE\n            % (\n                virtual_host,\n                exchange)\n        )", "response": "Get Exchange details.\n\n        :param str exchange: Exchange name\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting all the properties of the user s exchanges.", "response": "def list(self, virtual_host='/', show_all=False):\n        \"\"\"List Exchanges.\n\n        :param str virtual_host: Virtual host name\n        :param bool show_all: List all Exchanges\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: list\n        \"\"\"\n        if show_all:\n            return self.http_client.get(API_EXCHANGES)\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(\n            API_EXCHANGES_VIRTUAL_HOST % virtual_host\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeclares an Exchange. :param str exchange: Exchange name :param str exchange_type: Exchange type :param str virtual_host: Virtual host name :param bool passive: Do not create :param bool durable: Durable exchange :param bool auto_delete: Automatically delete when not in use :param bool internal: Is the exchange for use by the broker only. :param dict|None arguments: Exchange key/value arguments :raises ApiError: Raises if the remote server encountered an error. :raises ApiConnectionError: Raises if there was a connectivity issue. :rtype: None", "response": "def declare(self, exchange='', exchange_type='direct', virtual_host='/',\n                passive=False, durable=False, auto_delete=False,\n                internal=False, arguments=None):\n        \"\"\"Declare an Exchange.\n\n        :param str exchange: Exchange name\n        :param str exchange_type: Exchange type\n        :param str virtual_host: Virtual host name\n        :param bool passive: Do not create\n        :param bool durable: Durable exchange\n        :param bool auto_delete: Automatically delete when not in use\n        :param bool internal: Is the exchange for use by the broker only.\n        :param dict|None arguments: Exchange key/value arguments\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: None\n        \"\"\"\n        if passive:\n            return self.get(exchange, virtual_host=virtual_host)\n        exchange_payload = json.dumps(\n            {\n                'durable': durable,\n                'auto_delete': auto_delete,\n                'internal': internal,\n                'type': exchange_type,\n                'arguments': arguments or {},\n                'vhost': virtual_host\n            }\n        )\n        return self.http_client.put(API_EXCHANGE %\n                                    (\n                                        quote(virtual_host, ''),\n                                        exchange\n                                    ),\n                                    payload=exchange_payload)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self, exchange, virtual_host='/'):\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.delete(API_EXCHANGE %\n                                       (\n                                           virtual_host,\n                                           exchange\n                                       ))", "response": "Delete an Exchange.\n\n        :param str exchange: Exchange name\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets Exchange bindings. :param str exchange: Exchange name :param str virtual_host: Virtual host name :raises ApiError: Raises if the remote server encountered an error. :raises ApiConnectionError: Raises if there was a connectivity issue. :rtype: list", "response": "def bindings(self, exchange, virtual_host='/'):\n        \"\"\"Get Exchange bindings.\n\n        :param str exchange: Exchange name\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: list\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(API_EXCHANGE_BINDINGS %\n                                    (\n                                        virtual_host,\n                                        exchange\n                                    ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbinds an Exchange. :param str source: Source Exchange name :param str destination: Destination Exchange name :param str routing_key: The routing key to use :param str virtual_host: Virtual host name :param dict|None arguments: Bind key/value arguments :raises ApiError: Raises if the remote server encountered an error. :raises ApiConnectionError: Raises if there was a connectivity issue. :rtype: None", "response": "def bind(self, destination='', source='', routing_key='', virtual_host='/',\n             arguments=None):\n        \"\"\"Bind an Exchange.\n\n        :param str source: Source Exchange name\n        :param str destination: Destination Exchange name\n        :param str routing_key: The routing key to use\n        :param str virtual_host: Virtual host name\n        :param dict|None arguments: Bind key/value arguments\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: None\n        \"\"\"\n        bind_payload = json.dumps({\n            'destination': destination,\n            'destination_type': 'e',\n            'routing_key': routing_key,\n            'source': source,\n            'arguments': arguments or {},\n            'vhost': virtual_host\n        })\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.post(API_EXCHANGE_BIND %\n                                     (\n                                         virtual_host,\n                                         source,\n                                         destination\n                                     ),\n                                     payload=bind_payload)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unbind(self, destination='', source='', routing_key='',\n               virtual_host='/', properties_key=None):\n        \"\"\"Unbind an Exchange.\n\n        :param str source: Source Exchange name\n        :param str destination: Destination Exchange name\n        :param str routing_key: The routing key to use\n        :param str virtual_host: Virtual host name\n        :param str properties_key:\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: None\n        \"\"\"\n        unbind_payload = json.dumps({\n            'destination': destination,\n            'destination_type': 'e',\n            'properties_key': properties_key or routing_key,\n            'source': source,\n            'vhost': virtual_host\n        })\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.delete(API_EXCHANGE_UNBIND %\n                                       (\n                                           virtual_host,\n                                           source,\n                                           destination,\n                                           properties_key or routing_key\n                                       ),\n                                       payload=unbind_payload)", "response": "Unbind an Exchange.\n\n        :param str source: Source Exchange name\n        :param str destination: Destination Exchange name\n        :param str routing_key: The routing key to use\n        :param str virtual_host: Virtual host name\n        :param str properties_key:\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen a new channel.", "response": "def channel(self, rpc_timeout=60, lazy=False):\n        \"\"\"Open Channel.\n\n        :param int rpc_timeout: Timeout before we give up waiting for an RPC\n                                response from the server.\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n        \"\"\"\n        LOGGER.debug('Opening a new Channel')\n        if not compatibility.is_integer(rpc_timeout):\n            raise AMQPInvalidArgument('rpc_timeout should be an integer')\n        elif self.is_closed:\n            raise AMQPConnectionError('socket/connection closed')\n\n        with self.lock:\n            channel_id = self._get_next_available_channel_id()\n            channel = Channel(channel_id, self, rpc_timeout,\n                              on_close_impl=self._cleanup_channel)\n            self._channels[channel_id] = channel\n            if not lazy:\n                channel.open()\n        LOGGER.debug('Channel #%d Opened', channel_id)\n        return self._channels[channel_id]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_for_errors(self):\n        if not self.exceptions:\n            if not self.is_closed:\n                return\n            why = AMQPConnectionError('connection was closed')\n            self.exceptions.append(why)\n        self.set_state(self.CLOSED)\n        self.close()\n        raise self.exceptions[0]", "response": "Check Connection for errors. Raises AMQPConnectionError if the connection was closed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose connection. :raises AMQPConnectionError: Raises if the connection encountered an error. :return:", "response": "def close(self):\n        \"\"\"Close connection.\n\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n        :return:\n        \"\"\"\n        LOGGER.debug('Connection Closing')\n        if not self.is_closed:\n            self.set_state(self.CLOSING)\n        self.heartbeat.stop()\n        try:\n            if not self.is_closed and self.socket:\n                self._channel0.send_close_connection()\n                self._wait_for_connection_state(state=Stateful.CLOSED)\n        except AMQPConnectionError:\n            pass\n        finally:\n            self._close_remaining_channels()\n            self._io.close()\n            self.set_state(self.CLOSED)\n        LOGGER.debug('Connection Closed')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens Connection. :raises AMQPConnectionError: Raises if the connection encountered an error.", "response": "def open(self):\n        \"\"\"Open Connection.\n\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n        \"\"\"\n        LOGGER.debug('Connection Opening')\n        self.set_state(self.OPENING)\n        self._exceptions = []\n        self._channels = {}\n        self._last_channel_id = None\n        self._io.open()\n        self._send_handshake()\n        self._wait_for_connection_state(state=Stateful.OPEN)\n        self.heartbeat.start(self._exceptions)\n        LOGGER.debug('Connection Opened')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_frame(self, channel_id, frame_out):\n        frame_data = pamqp_frame.marshal(frame_out, channel_id)\n        self.heartbeat.register_write()\n        self._io.write_to_socket(frame_data)", "response": "Marshal and write an outgoing pamqp frame to the Socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_frames(self, channel_id, frames_out):\n        data_out = EMPTY_BUFFER\n        for single_frame in frames_out:\n            data_out += pamqp_frame.marshal(single_frame, channel_id)\n        self.heartbeat.register_write()\n        self._io.write_to_socket(data_out)", "response": "Marshal and write multiple outgoing pamqp frames to the Socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _close_remaining_channels(self):\n        for channel_id in list(self._channels):\n            self._channels[channel_id].set_state(Channel.CLOSED)\n            self._channels[channel_id].close()\n            self._cleanup_channel(channel_id)", "response": "Forcefully close all open channels."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_next_available_channel_id(self):\n        for index in compatibility.RANGE(self._last_channel_id or 1,\n                                         self.max_allowed_channels + 1):\n            if index in self._channels:\n                continue\n            self._last_channel_id = index\n            return index\n\n        if self._last_channel_id:\n            self._last_channel_id = None\n            return self._get_next_available_channel_id()\n\n        raise AMQPConnectionError(\n            'reached the maximum number of channels %d' %\n            self.max_allowed_channels)", "response": "Returns the next available channel id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_amqp_frame(self, data_in):\n        if not data_in:\n            return data_in, None, None\n        try:\n            byte_count, channel_id, frame_in = pamqp_frame.unmarshal(data_in)\n            return data_in[byte_count:], channel_id, frame_in\n        except pamqp_exception.UnmarshalingException:\n            pass\n        except specification.AMQPFrameError as why:\n            LOGGER.error('AMQPFrameError: %r', why, exc_info=True)\n        except ValueError as why:\n            LOGGER.error(why, exc_info=True)\n            self.exceptions.append(AMQPConnectionError(why))\n        return data_in, None, None", "response": "Handle a single AMQP frame and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the socket buffer and direct the data to the appropriate channel.", "response": "def _read_buffer(self, data_in):\n        \"\"\"Process the socket buffer, and direct the data to the appropriate\n        channel.\n\n        :rtype: bytes\n        \"\"\"\n        while data_in:\n            data_in, channel_id, frame_in = self._handle_amqp_frame(data_in)\n\n            if frame_in is None:\n                break\n\n            self.heartbeat.register_read()\n            if channel_id == 0:\n                self._channel0.on_frame(frame_in)\n            elif channel_id in self._channels:\n                self._channels[channel_id].on_frame(frame_in)\n\n        return data_in"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _cleanup_channel(self, channel_id):\n        with self.lock:\n            if channel_id not in self._channels:\n                return\n            del self._channels[channel_id]", "response": "Remove the channel from the list of available channels."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates Connection Parameters. :return:", "response": "def _validate_parameters(self):\n        \"\"\"Validate Connection Parameters.\n\n        :return:\n        \"\"\"\n        if not compatibility.is_string(self.parameters['hostname']):\n            raise AMQPInvalidArgument('hostname should be a string')\n        elif not compatibility.is_integer(self.parameters['port']):\n            raise AMQPInvalidArgument('port should be an integer')\n        elif not compatibility.is_string(self.parameters['username']):\n            raise AMQPInvalidArgument('username should be a string')\n        elif not compatibility.is_string(self.parameters['password']):\n            raise AMQPInvalidArgument('password should be a string')\n        elif not compatibility.is_string(self.parameters['virtual_host']):\n            raise AMQPInvalidArgument('virtual_host should be a string')\n        elif not isinstance(self.parameters['timeout'], (int, float)):\n            raise AMQPInvalidArgument('timeout should be an integer or float')\n        elif not compatibility.is_integer(self.parameters['heartbeat']):\n            raise AMQPInvalidArgument('heartbeat should be an integer')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _wait_for_connection_state(self, state=Stateful.OPEN, rpc_timeout=30):\n        start_time = time.time()\n        while self.current_state != state:\n            self.check_for_errors()\n            if time.time() - start_time > rpc_timeout:\n                raise AMQPConnectionError('Connection timed out')\n            sleep(IDLE_WAIT)", "response": "Wait for a Connection state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_connection(self):\n        attempts = 0\n        while True:\n            attempts += 1\n            try:\n                self.connection = Connection('127.0.0.1', 'guest', 'guest')\n                break\n            except amqpstorm.AMQPError as why:\n                LOGGER.exception(why)\n                if self.max_retries and attempts > self.max_retries:\n                    break\n                time.sleep(min(attempts * 2, 30))\n            except KeyboardInterrupt:\n                break", "response": "Create a connection.\n\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting the Consumers. :return:", "response": "def start(self):\n        \"\"\"Start the Consumers.\n\n        :return:\n        \"\"\"\n        if not self.connection:\n            self.create_connection()\n        while True:\n            try:\n                channel = self.connection.channel()\n                channel.queue.declare('simple_queue')\n                channel.basic.consume(self, 'simple_queue', no_ack=False)\n                channel.start_consuming()\n                if not channel.consumer_tags:\n                    channel.close()\n            except amqpstorm.AMQPError as why:\n                LOGGER.exception(why)\n                self.create_connection()\n            except KeyboardInterrupt:\n                self.connection.close()\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop(self):\n        while self._consumers:\n            consumer = self._consumers.pop()\n            consumer.stop()\n        self._stopped.set()\n        self._connection.close()", "response": "Stop all consumers.\n\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_connection(self):\n        attempts = 0\n        while True:\n            attempts += 1\n            if self._stopped.is_set():\n                break\n            try:\n                self._connection = Connection(self.hostname,\n                                              self.username,\n                                              self.password)\n                break\n            except amqpstorm.AMQPError as why:\n                LOGGER.warning(why)\n                if self.max_retries and attempts > self.max_retries:\n                    raise Exception('max number of retries reached')\n                time.sleep(min(attempts * 2, 30))\n            except KeyboardInterrupt:\n                break", "response": "Create a connection.\n\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _stop_consumers(self, number_of_consumers=0):\n        while len(self._consumers) > number_of_consumers:\n            consumer = self._consumers.pop()\n            consumer.stop()", "response": "Stop a specific number of consumers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart a consumer as a new Thread.", "response": "def _start_consumer(self, consumer):\n        \"\"\"Start a consumer as a new Thread.\n\n        :param Consumer consumer:\n        :return:\n        \"\"\"\n        thread = threading.Thread(target=consumer.start,\n                                  args=(self._connection,))\n        thread.daemon = True\n        thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select(self):\n        self._tx_active = True\n        return self._channel.rpc_request(specification.Tx.Select())", "response": "Enable standard transaction mode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef commit(self):\n        self._tx_active = False\n        return self._channel.rpc_request(specification.Tx.Commit())", "response": "Commit the current transaction."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rollback(self):\n        self._tx_active = False\n        return self._channel.rpc_request(specification.Tx.Rollback())", "response": "Abandon the current transaction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rpc_call(payload):\n\n    # Send the request and store the requests Unique ID.\n    corr_id = RPC_CLIENT.send_request(payload)\n\n    # Wait until we have received a response.\n    while RPC_CLIENT.queue[corr_id] is None:\n        sleep(0.1)\n\n    # Return the response to the user.\n    return RPC_CLIENT.queue[corr_id]", "response": "Simple Flask implementation for making asynchronous Rpc calls."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_process_thread(self):\n        thread = threading.Thread(target=self._process_data_events)\n        thread.setDaemon(True)\n        thread.start()", "response": "Create a thread responsible for consuming messages in response\n         to RPC requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(channel, body, properties=None):\n        properties = properties or {}\n        if 'correlation_id' not in properties:\n            properties['correlation_id'] = str(uuid.uuid4())\n        if 'message_id' not in properties:\n            properties['message_id'] = str(uuid.uuid4())\n        if 'timestamp' not in properties:\n            properties['timestamp'] = datetime.utcnow()\n\n        return Message(channel, auto_decode=False,\n                       body=body, properties=properties)", "response": "Create a new Message object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the Message Body.", "response": "def body(self):\n        \"\"\"Return the Message Body.\n\n            If auto_decode is enabled, the body will automatically be\n            decoded using decode('utf-8') if possible.\n\n        :rtype: bytes|str|unicode\n        \"\"\"\n        if not self._auto_decode:\n            return self._body\n        if 'body' in self._decode_cache:\n            return self._decode_cache['body']\n        body = try_utf8_decode(self._body)\n        self._decode_cache['body'] = body\n        return body"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nacknowledging Message. :raises AMQPInvalidArgument: Invalid Parameters :raises AMQPChannelError: Raises if the channel encountered an error. :raises AMQPConnectionError: Raises if the connection encountered an error. :return:", "response": "def ack(self):\n        \"\"\"Acknowledge Message.\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :return:\n        \"\"\"\n        if not self._method:\n            raise AMQPMessageError(\n                'Message.ack only available on incoming messages'\n            )\n        self._channel.basic.ack(delivery_tag=self.delivery_tag)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nack(self, requeue=True):\n        if not self._method:\n            raise AMQPMessageError(\n                'Message.nack only available on incoming messages'\n            )\n        self._channel.basic.nack(delivery_tag=self.delivery_tag,\n                                 requeue=requeue)", "response": "Negative Acknowledgement.\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :param bool requeue: Re-queue the message"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef publish(self, routing_key, exchange='', mandatory=False,\n                immediate=False):\n        \"\"\"Publish Message.\n\n        :param str routing_key: Message routing key\n        :param str exchange: The exchange to publish the message to\n        :param bool mandatory: Requires the message is published\n        :param bool immediate: Request immediate delivery\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: bool|None\n        \"\"\"\n        return self._channel.basic.publish(body=self._body,\n                                           routing_key=routing_key,\n                                           exchange=exchange,\n                                           properties=self._properties,\n                                           mandatory=mandatory,\n                                           immediate=immediate)", "response": "Publish a message to a specific exchange."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_properties(self, name, value):\n        if self._auto_decode and 'properties' in self._decode_cache:\n            self._decode_cache['properties'][name] = value\n        self._properties[name] = value", "response": "Update properties and keep cache up - to - date."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _try_decode_utf8_content(self, content, content_type):\n        if not self._auto_decode or not content:\n            return content\n        if content_type in self._decode_cache:\n            return self._decode_cache[content_type]\n        if isinstance(content, dict):\n            content = self._try_decode_dict(content)\n        else:\n            content = try_utf8_decode(content)\n        self._decode_cache[content_type] = content\n        return content", "response": "Generic function to decode content."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding content of a dictionary. :param dict content: :return:", "response": "def _try_decode_dict(self, content):\n        \"\"\"Decode content of a dictionary.\n\n        :param dict content:\n        :return:\n        \"\"\"\n        result = dict()\n        for key, value in content.items():\n            key = try_utf8_decode(key)\n            if isinstance(value, dict):\n                result[key] = self._try_decode_dict(value)\n            elif isinstance(value, list):\n                result[key] = self._try_decode_list(value)\n            elif isinstance(value, tuple):\n                result[key] = self._try_decode_tuple(value)\n            else:\n                result[key] = try_utf8_decode(value)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _try_decode_list(content):\n        result = list()\n        for value in content:\n            result.append(try_utf8_decode(value))\n        return result", "response": "Decode content of a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef publish(self, body, routing_key, exchange='amq.default',\n                virtual_host='/', properties=None, payload_encoding='string'):\n        \"\"\"Publish a Message.\n\n        :param bytes|str|unicode body: Message payload\n        :param str routing_key: Message routing key\n        :param str exchange: The exchange to publish the message to\n        :param str virtual_host: Virtual host name\n        :param dict properties: Message properties\n        :param str payload_encoding: Payload encoding.\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        exchange = quote(exchange, '')\n        properties = properties or {}\n        body = json.dumps(\n            {\n                'routing_key': routing_key,\n                'payload': body,\n                'payload_encoding': payload_encoding,\n                'properties': properties,\n                'vhost': virtual_host\n            }\n        )\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.post(API_BASIC_PUBLISH %\n                                     (\n                                         virtual_host,\n                                         exchange),\n                                     payload=body)", "response": "Publish a message to a specific exchange."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, queue, virtual_host='/', requeue=False, to_dict=False,\n            count=1, truncate=50000, encoding='auto'):\n        \"\"\"Get Messages.\n\n        :param str queue: Queue name\n        :param str virtual_host: Virtual host name\n        :param bool requeue: Re-queue message\n        :param bool to_dict: Should incoming messages be converted to a\n                             dictionary before delivery.\n        :param int count: How many messages should we try to fetch.\n        :param int truncate: The maximum length in bytes, beyond that the\n                             server will truncate the message.\n        :param str encoding: Message encoding.\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: list\n        \"\"\"\n        ackmode = 'ack_requeue_false'\n        if requeue:\n            ackmode = 'ack_requeue_true'\n\n        get_messages = json.dumps(\n            {\n                'count': count,\n                'requeue': requeue,\n                'ackmode': ackmode,\n                'encoding': encoding,\n                'truncate': truncate,\n                'vhost': virtual_host\n            }\n        )\n        virtual_host = quote(virtual_host, '')\n        response = self.http_client.post(API_BASIC_GET_MESSAGE %\n                                         (\n                                             virtual_host,\n                                             queue\n                                         ),\n                                         payload=get_messages)\n        if to_dict:\n            return response\n        messages = []\n        for message in response:\n            if 'payload' in message:\n                message['body'] = message.pop('payload')\n            messages.append(Message(channel=None, auto_decode=True, **message))\n        return messages", "response": "Get the messages from a queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets Virtual Host details.", "response": "def get(self, virtual_host):\n        \"\"\"Get Virtual Host details.\n\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(API_VIRTUAL_HOST % virtual_host)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, virtual_host):\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.put(API_VIRTUAL_HOST % virtual_host)", "response": "Create a Virtual Host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a Virtual Host.", "response": "def delete(self, virtual_host):\n        \"\"\"Delete a Virtual Host.\n\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.delete(API_VIRTUAL_HOST % virtual_host)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all Virtual hosts permissions.", "response": "def get_permissions(self, virtual_host):\n        \"\"\"Get all Virtual hosts permissions.\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(API_VIRTUAL_HOSTS_PERMISSION %\n                                    (\n                                        virtual_host\n                                    ))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_consumer_tag(self, tag):\n        if not is_string(tag):\n            raise AMQPChannelError('consumer tag needs to be a string')\n        if tag not in self._consumer_tags:\n            self._consumer_tags.append(tag)", "response": "Add a Consumer tag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_consumer_tag(self, tag=None):\n        if tag is not None:\n            if tag in self._consumer_tags:\n                self._consumer_tags.remove(tag)\n        else:\n            self._consumer_tags = []", "response": "Removes a Consumer tag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_dict(self):\n        return {\n            'body': self._body,\n            'method': self._method,\n            'properties': self._properties,\n            'channel': self._channel\n        }", "response": "Converts the object to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_tuple(self):\n        return self._body, self._channel, self._method, self._properties", "response": "Converts this message to a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self, connection, reason='Closed via management api'):\n        close_payload = json.dumps({\n            'name': connection,\n            'reason': reason\n        })\n        connection = quote(connection, '')\n        return self.http_client.delete(API_CONNECTION % connection,\n                                       payload=close_payload,\n                                       headers={\n                                           'X-Reason': reason\n                                       })", "response": "Close Connection.\n\n        :param str connection: Connection name\n        :param str reason: Reason for closing connection.\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_request(self, valid_responses):\n        uuid = str(uuid4())\n        self._response[uuid] = []\n        for action in valid_responses:\n            self._request[action] = uuid\n        return uuid", "response": "Register a RPC request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove any RPC request using this uuid.", "response": "def remove_request(self, uuid):\n        \"\"\"Remove any RPC request(s) using this uuid.\n\n        :param str uuid: Rpc Identifier.\n        :return:\n        \"\"\"\n        for key in list(self._request):\n            if self._request[key] == uuid:\n                del self._request[key]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_request(self, uuid, raw=False, multiple=False,\n                    connection_adapter=None):\n        \"\"\"Get a RPC request.\n\n        :param str uuid: Rpc Identifier\n        :param bool raw: If enabled return the frame as is, else return\n                         result as a dictionary.\n        :param bool multiple: Are we expecting multiple frames.\n        :param obj connection_adapter: Provide custom connection adapter.\n        :return:\n        \"\"\"\n        if uuid not in self._response:\n            return\n        self._wait_for_request(\n            uuid, connection_adapter or self._default_connection_adapter\n        )\n        frame = self._get_response_frame(uuid)\n        if not multiple:\n            self.remove(uuid)\n        result = None\n        if raw:\n            result = frame\n        elif frame is not None:\n            result = dict(frame)\n        return result", "response": "Get a request from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a response frame.", "response": "def _get_response_frame(self, uuid):\n        \"\"\"Get a response frame.\n\n        :param str uuid: Rpc Identifier\n        :return:\n        \"\"\"\n        frame = None\n        frames = self._response.get(uuid, None)\n        if frames:\n            frame = frames.pop(0)\n        return frame"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _wait_for_request(self, uuid, connection_adapter=None):\n        start_time = time.time()\n        while not self._response[uuid]:\n            connection_adapter.check_for_errors()\n            if time.time() - start_time > self._timeout:\n                self._raise_rpc_timeout_error(uuid)\n            time.sleep(IDLE_WAIT)", "response": "Wait for a request to arrive."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngathers information and raise an Rpc exception.", "response": "def _raise_rpc_timeout_error(self, uuid):\n        \"\"\"Gather information and raise an Rpc exception.\n\n        :param str uuid: Rpc Identifier.\n        :return:\n        \"\"\"\n        requests = []\n        for key, value in self._request.items():\n            if value == uuid:\n                requests.append(key)\n        self.remove(uuid)\n        message = (\n            'rpc requests %s (%s) took too long' %\n            (\n                uuid,\n                ', '.join(requests)\n            )\n        )\n        raise AMQPChannelError(message)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_ssl_version():\n    if hasattr(ssl, 'PROTOCOL_TLSv1_2'):\n        return ssl.PROTOCOL_TLSv1_2\n    elif hasattr(ssl, 'PROTOCOL_TLSv1_1'):\n        return ssl.PROTOCOL_TLSv1_1\n    elif hasattr(ssl, 'PROTOCOL_TLSv1'):\n        return ssl.PROTOCOL_TLSv1\n    return None", "response": "Returns the highest support TLS version available in the SSL document."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_string(obj):\n    if PYTHON3:\n        str_type = (bytes, str)\n    else:\n        str_type = (bytes, str, unicode)\n    return isinstance(obj, str_type)", "response": "Is this a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nis this an integer?", "response": "def is_integer(obj):\n    \"\"\"Is this an integer.\n\n    :param object obj:\n    :return:\n    \"\"\"\n    if PYTHON3:\n        return isinstance(obj, int)\n    return isinstance(obj, (int, long))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to decode an object.", "response": "def try_utf8_decode(value):\n    \"\"\"Try to decode an object.\n\n    :param value:\n    :return:\n    \"\"\"\n    if not value or not is_string(value):\n        return value\n    elif PYTHON3 and not isinstance(value, bytes):\n        return value\n    elif not PYTHON3 and not isinstance(value, unicode):\n        return value\n\n    try:\n        return value.decode('utf-8')\n    except UnicodeDecodeError:\n        pass\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch_uri(uri):\n    index = uri.find(':')\n    if uri[:index] == 'amqps':\n        uri = uri.replace('amqps', 'https', 1)\n    elif uri[:index] == 'amqp':\n        uri = uri.replace('amqp', 'http', 1)\n    return uri", "response": "Patch the uri schema to use http and https."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_uri_options(self, parsed_uri, use_ssl=False, ssl_options=None):\n        ssl_options = ssl_options or {}\n        kwargs = urlparse.parse_qs(parsed_uri.query)\n        vhost = urlparse.unquote(parsed_uri.path[1:]) or DEFAULT_VIRTUAL_HOST\n        options = {\n            'ssl': use_ssl,\n            'virtual_host': vhost,\n            'heartbeat': int(kwargs.pop('heartbeat',\n                                        [DEFAULT_HEARTBEAT_INTERVAL])[0]),\n            'timeout': int(kwargs.pop('timeout',\n                                      [DEFAULT_SOCKET_TIMEOUT])[0])\n        }\n        if use_ssl:\n            if not compatibility.SSL_SUPPORTED:\n                raise AMQPConnectionError(\n                    'Python not compiled with support '\n                    'for TLSv1 or higher'\n                )\n            ssl_options.update(self._parse_ssl_options(kwargs))\n            options['ssl_options'] = ssl_options\n        return options", "response": "Parse the uri options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_ssl_options(self, ssl_kwargs):\n        ssl_options = {}\n        for key in ssl_kwargs:\n            if key not in compatibility.SSL_OPTIONS:\n                LOGGER.warning('invalid option: %s', key)\n                continue\n            if 'ssl_version' in key:\n                value = self._get_ssl_version(ssl_kwargs[key][0])\n            elif 'cert_reqs' in key:\n                value = self._get_ssl_validation(ssl_kwargs[key][0])\n            else:\n                value = ssl_kwargs[key][0]\n            ssl_options[key] = value\n        return ssl_options", "response": "Parse TLS Options.\n\n        :param ssl_kwargs:\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the TLS Version.", "response": "def _get_ssl_version(self, value):\n        \"\"\"Get the TLS Version.\n\n        :param str value:\n        :return: TLS Version\n        \"\"\"\n        return self._get_ssl_attribute(value, compatibility.SSL_VERSIONS,\n                                       ssl.PROTOCOL_TLSv1,\n                                       'ssl_options: ssl_version \\'%s\\' not '\n                                       'found falling back to PROTOCOL_TLSv1.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the TLS Validation option.", "response": "def _get_ssl_validation(self, value):\n        \"\"\"Get the TLS Validation option.\n\n        :param str value:\n        :return: TLS Certificate Options\n        \"\"\"\n        return self._get_ssl_attribute(value, compatibility.SSL_CERT_MAP,\n                                       ssl.CERT_NONE,\n                                       'ssl_options: cert_reqs \\'%s\\' not '\n                                       'found falling back to CERT_NONE.')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_ssl_attribute(value, mapping, default_value, warning_message):\n        for key in mapping:\n            if not key.endswith(value.lower()):\n                continue\n            return mapping[key]\n        LOGGER.warning(warning_message, value)\n        return default_value", "response": "Get the TLS attribute based on the compatibility mapping."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all the top process nodes.", "response": "def top(self):\n        \"\"\"Top Processes.\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: list\n        \"\"\"\n        nodes = []\n        for node in self.nodes():\n            nodes.append(self.http_client.get(API_TOP % node['name']))\n        return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nspecifying quality of service.", "response": "def qos(self, prefetch_count=0, prefetch_size=0, global_=False):\n        \"\"\"Specify quality of service.\n\n        :param int prefetch_count: Prefetch window in messages\n        :param int/long prefetch_size: Prefetch window in octets\n        :param bool global_: Apply to entire connection\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict\n        \"\"\"\n        if not compatibility.is_integer(prefetch_count):\n            raise AMQPInvalidArgument('prefetch_count should be an integer')\n        elif not compatibility.is_integer(prefetch_size):\n            raise AMQPInvalidArgument('prefetch_size should be an integer')\n        elif not isinstance(global_, bool):\n            raise AMQPInvalidArgument('global_ should be a boolean')\n        qos_frame = specification.Basic.Qos(prefetch_count=prefetch_count,\n                                            prefetch_size=prefetch_size,\n                                            global_=global_)\n        return self._channel.rpc_request(qos_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching a single message from a queue.", "response": "def get(self, queue='', no_ack=False, to_dict=False, auto_decode=True):\n        \"\"\"Fetch a single message.\n\n        :param str queue: Queue name\n        :param bool no_ack: No acknowledgement needed\n        :param bool to_dict: Should incoming messages be converted to a\n                    dictionary before delivery.\n        :param bool auto_decode: Auto-decode strings when possible.\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :returns: Returns a single message, as long as there is a message in\n                  the queue. If no message is available, returns None.\n\n        :rtype: dict|Message|None\n        \"\"\"\n        if not compatibility.is_string(queue):\n            raise AMQPInvalidArgument('queue should be a string')\n        elif not isinstance(no_ack, bool):\n            raise AMQPInvalidArgument('no_ack should be a boolean')\n        elif self._channel.consumer_tags:\n            raise AMQPChannelError(\"Cannot call 'get' when channel is \"\n                                   \"set to consume\")\n        get_frame = specification.Basic.Get(queue=queue,\n                                            no_ack=no_ack)\n        with self._channel.lock and self._channel.rpc.lock:\n            message = self._get_message(get_frame, auto_decode=auto_decode)\n            if message and to_dict:\n                return message.to_dict()\n            return message"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recover(self, requeue=False):\n        if not isinstance(requeue, bool):\n            raise AMQPInvalidArgument('requeue should be a boolean')\n        recover_frame = specification.Basic.Recover(requeue=requeue)\n        return self._channel.rpc_request(recover_frame)", "response": "Redeliver unacknowledged messages.\n\n        :param bool requeue: Re-queue the messages\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef consume(self, callback=None, queue='', consumer_tag='',\n                exclusive=False, no_ack=False, no_local=False, arguments=None):\n        \"\"\"Start a queue consumer.\n\n        :param function callback: Message callback\n        :param str queue: Queue name\n        :param str consumer_tag: Consumer tag\n        :param bool no_local: Do not deliver own messages\n        :param bool no_ack: No acknowledgement needed\n        :param bool exclusive: Request exclusive access\n        :param dict arguments: Consume key/value arguments\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :returns: Consumer tag\n        :rtype: str\n        \"\"\"\n        if not compatibility.is_string(queue):\n            raise AMQPInvalidArgument('queue should be a string')\n        elif not compatibility.is_string(consumer_tag):\n            raise AMQPInvalidArgument('consumer_tag should be a string')\n        elif not isinstance(exclusive, bool):\n            raise AMQPInvalidArgument('exclusive should be a boolean')\n        elif not isinstance(no_ack, bool):\n            raise AMQPInvalidArgument('no_ack should be a boolean')\n        elif not isinstance(no_local, bool):\n            raise AMQPInvalidArgument('no_local should be a boolean')\n        elif arguments is not None and not isinstance(arguments, dict):\n            raise AMQPInvalidArgument('arguments should be a dict or None')\n        consume_rpc_result = self._consume_rpc_request(arguments, consumer_tag,\n                                                       exclusive, no_ack,\n                                                       no_local, queue)\n        tag = self._consume_add_and_get_tag(consume_rpc_result)\n        self._channel._consumer_callbacks[tag] = callback\n        return tag", "response": "Start a queue consumer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncancel a queue consumer.", "response": "def cancel(self, consumer_tag=''):\n        \"\"\"Cancel a queue consumer.\n\n        :param str consumer_tag: Consumer tag\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict\n        \"\"\"\n        if not compatibility.is_string(consumer_tag):\n            raise AMQPInvalidArgument('consumer_tag should be a string')\n        cancel_frame = specification.Basic.Cancel(consumer_tag=consumer_tag)\n        result = self._channel.rpc_request(cancel_frame)\n        self._channel.remove_consumer_tag(consumer_tag)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npublishing a message to a specific exchange.", "response": "def publish(self, body, routing_key, exchange='', properties=None,\n                mandatory=False, immediate=False):\n        \"\"\"Publish a Message.\n\n        :param bytes|str|unicode body: Message payload\n        :param str routing_key: Message routing key\n        :param str exchange: The exchange to publish the message to\n        :param dict properties: Message properties\n        :param bool mandatory: Requires the message is published\n        :param bool immediate: Request immediate delivery\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: bool|None\n        \"\"\"\n        self._validate_publish_parameters(body, exchange, immediate, mandatory,\n                                          properties, routing_key)\n        properties = properties or {}\n        body = self._handle_utf8_payload(body, properties)\n        properties = specification.Basic.Properties(**properties)\n        method_frame = specification.Basic.Publish(exchange=exchange,\n                                                   routing_key=routing_key,\n                                                   mandatory=mandatory,\n                                                   immediate=immediate)\n        header_frame = pamqp_header.ContentHeader(body_size=len(body),\n                                                  properties=properties)\n\n        frames_out = [method_frame, header_frame]\n        for body_frame in self._create_content_body(body):\n            frames_out.append(body_frame)\n\n        if self._channel.confirming_deliveries:\n            with self._channel.rpc.lock:\n                return self._publish_confirm(frames_out)\n        self._channel.write_frames(frames_out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nacknowledges a message from the broker.", "response": "def ack(self, delivery_tag=0, multiple=False):\n        \"\"\"Acknowledge Message.\n\n        :param int/long delivery_tag: Server-assigned delivery tag\n        :param bool multiple: Acknowledge multiple messages\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :return:\n        \"\"\"\n        if not compatibility.is_integer(delivery_tag):\n            raise AMQPInvalidArgument('delivery_tag should be an integer')\n        elif not isinstance(multiple, bool):\n            raise AMQPInvalidArgument('multiple should be a boolean')\n        ack_frame = specification.Basic.Ack(delivery_tag=delivery_tag,\n                                            multiple=multiple)\n        self._channel.write_frame(ack_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nack(self, delivery_tag=0, multiple=False, requeue=True):\n        if not compatibility.is_integer(delivery_tag):\n            raise AMQPInvalidArgument('delivery_tag should be an integer')\n        elif not isinstance(multiple, bool):\n            raise AMQPInvalidArgument('multiple should be a boolean')\n        elif not isinstance(requeue, bool):\n            raise AMQPInvalidArgument('requeue should be a boolean')\n        nack_frame = specification.Basic.Nack(delivery_tag=delivery_tag,\n                                              multiple=multiple,\n                                              requeue=requeue)\n        self._channel.write_frame(nack_frame)", "response": "Negative Acknowledgement.\n\n        :param int/long delivery_tag: Server-assigned delivery tag\n        :param bool multiple: Negative acknowledge multiple messages\n        :param bool requeue: Re-queue the message\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrejecting a previously - accepted message.", "response": "def reject(self, delivery_tag=0, requeue=True):\n        \"\"\"Reject Message.\n\n        :param int/long delivery_tag: Server-assigned delivery tag\n        :param bool requeue: Re-queue the message\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :return:\n        \"\"\"\n        if not compatibility.is_integer(delivery_tag):\n            raise AMQPInvalidArgument('delivery_tag should be an integer')\n        elif not isinstance(requeue, bool):\n            raise AMQPInvalidArgument('requeue should be a boolean')\n        reject_frame = specification.Basic.Reject(delivery_tag=delivery_tag,\n                                                  requeue=requeue)\n        self._channel.write_frame(reject_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the tag to the channel and return it.", "response": "def _consume_add_and_get_tag(self, consume_rpc_result):\n        \"\"\"Add the tag to the channel and return it.\n\n        :param dict consume_rpc_result:\n\n        :rtype: str\n        \"\"\"\n        consumer_tag = consume_rpc_result['consumer_tag']\n        self._channel.add_consumer_tag(consumer_tag)\n        return consumer_tag"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Consume Frame and execute a RPC request.", "response": "def _consume_rpc_request(self, arguments, consumer_tag, exclusive, no_ack,\n                             no_local, queue):\n        \"\"\"Create a Consume Frame and execute a RPC request.\n\n        :param str queue: Queue name\n        :param str consumer_tag: Consumer tag\n        :param bool no_local: Do not deliver own messages\n        :param bool no_ack: No acknowledgement needed\n        :param bool exclusive: Request exclusive access\n        :param dict arguments: Consume key/value arguments\n\n        :rtype: dict\n        \"\"\"\n        consume_frame = specification.Basic.Consume(queue=queue,\n                                                    consumer_tag=consumer_tag,\n                                                    exclusive=exclusive,\n                                                    no_local=no_local,\n                                                    no_ack=no_ack,\n                                                    arguments=arguments)\n        return self._channel.rpc_request(consume_frame)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate Publish Parameters. :param bytes|str|unicode body: Message payload :param str routing_key: Message routing key :param str exchange: The exchange to publish the message to :param dict properties: Message properties :param bool mandatory: Requires the message is published :param bool immediate: Request immediate delivery :raises AMQPInvalidArgument: Invalid Parameters :return:", "response": "def _validate_publish_parameters(body, exchange, immediate, mandatory,\n                                     properties, routing_key):\n        \"\"\"Validate Publish Parameters.\n\n        :param bytes|str|unicode body: Message payload\n        :param str routing_key: Message routing key\n        :param str exchange: The exchange to publish the message to\n        :param dict properties: Message properties\n        :param bool mandatory: Requires the message is published\n        :param bool immediate: Request immediate delivery\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n\n        :return:\n        \"\"\"\n        if not compatibility.is_string(body):\n            raise AMQPInvalidArgument('body should be a string')\n        elif not compatibility.is_string(routing_key):\n            raise AMQPInvalidArgument('routing_key should be a string')\n        elif not compatibility.is_string(exchange):\n            raise AMQPInvalidArgument('exchange should be a string')\n        elif properties is not None and not isinstance(properties, dict):\n            raise AMQPInvalidArgument('properties should be a dict or None')\n        elif not isinstance(mandatory, bool):\n            raise AMQPInvalidArgument('mandatory should be a boolean')\n        elif not isinstance(immediate, bool):\n            raise AMQPInvalidArgument('immediate should be a boolean')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the Body and Properties to the appropriate encoding.", "response": "def _handle_utf8_payload(body, properties):\n        \"\"\"Update the Body and Properties to the appropriate encoding.\n\n        :param bytes|str|unicode body: Message payload\n        :param dict properties: Message properties\n\n        :return:\n        \"\"\"\n        if 'content_encoding' not in properties:\n            properties['content_encoding'] = 'utf-8'\n        encoding = properties['content_encoding']\n        if compatibility.is_unicode(body):\n            body = body.encode(encoding)\n        elif compatibility.PYTHON3 and isinstance(body, str):\n            body = bytes(body, encoding=encoding)\n        return body"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_message(self, get_frame, auto_decode):\n        message_uuid = self._channel.rpc.register_request(\n            get_frame.valid_responses + ['ContentHeader', 'ContentBody']\n        )\n        try:\n            self._channel.write_frame(get_frame)\n            get_ok_frame = self._channel.rpc.get_request(message_uuid,\n                                                         raw=True,\n                                                         multiple=True)\n            if isinstance(get_ok_frame, specification.Basic.GetEmpty):\n                return None\n            content_header = self._channel.rpc.get_request(message_uuid,\n                                                           raw=True,\n                                                           multiple=True)\n            body = self._get_content_body(message_uuid,\n                                          content_header.body_size)\n        finally:\n            self._channel.rpc.remove(message_uuid)\n        return Message(channel=self._channel,\n                       body=body,\n                       method=dict(get_ok_frame),\n                       properties=dict(content_header.properties),\n                       auto_decode=auto_decode)", "response": "Get and return a message using a Basic. Get frame."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfirms that a message was published successfully.", "response": "def _publish_confirm(self, frames_out):\n        \"\"\"Confirm that message was published successfully.\n\n        :param list frames_out:\n\n        :rtype: bool\n        \"\"\"\n        confirm_uuid = self._channel.rpc.register_request(['Basic.Ack',\n                                                           'Basic.Nack'])\n        self._channel.write_frames(frames_out)\n        result = self._channel.rpc.get_request(confirm_uuid, raw=True)\n        self._channel.check_for_errors()\n        if isinstance(result, specification.Basic.Ack):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits the body based on the maximum frame size.", "response": "def _create_content_body(self, body):\n        \"\"\"Split body based on the maximum frame size.\n\n            This function is based on code from Rabbitpy.\n            https://github.com/gmr/rabbitpy\n\n        :param bytes|str|unicode body: Message payload\n\n        :rtype: collections.Iterable\n        \"\"\"\n        frames = int(math.ceil(len(body) / float(self._max_frame_size)))\n        for offset in compatibility.RANGE(0, frames):\n            start_frame = self._max_frame_size * offset\n            end_frame = start_frame + self._max_frame_size\n            body_len = len(body)\n            if end_frame > body_len:\n                end_frame = body_len\n            yield pamqp_body.ContentBody(body[start_frame:end_frame])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the content body of a specific message.", "response": "def _get_content_body(self, message_uuid, body_size):\n        \"\"\"Get Content Body using RPC requests.\n\n        :param str uuid_body: Rpc Identifier.\n        :param int body_size: Content Size.\n\n        :rtype: str\n        \"\"\"\n        body = bytes()\n        while len(body) < body_size:\n            body_piece = self._channel.rpc.get_request(message_uuid, raw=True,\n                                                       multiple=True)\n            if not body_piece.value:\n                break\n            body += body_piece.value\n        return body"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeclares a Queue. :param str queue: Queue name :param bool passive: Do not create :param bool durable: Durable queue :param bool exclusive: Request exclusive access :param bool auto_delete: Automatically delete when not in use :param dict arguments: Queue key/value arguments :raises AMQPInvalidArgument: Invalid Parameters :raises AMQPChannelError: Raises if the channel encountered an error. :raises AMQPConnectionError: Raises if the connection encountered an error. :rtype: dict", "response": "def declare(self, queue='', passive=False, durable=False,\n                exclusive=False, auto_delete=False, arguments=None):\n        \"\"\"Declare a Queue.\n\n        :param str queue: Queue name\n        :param bool passive: Do not create\n        :param bool durable: Durable queue\n        :param bool exclusive: Request exclusive access\n        :param bool auto_delete: Automatically delete when not in use\n        :param dict arguments: Queue key/value arguments\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict\n        \"\"\"\n        if not compatibility.is_string(queue):\n            raise AMQPInvalidArgument('queue should be a string')\n        elif not isinstance(passive, bool):\n            raise AMQPInvalidArgument('passive should be a boolean')\n        elif not isinstance(durable, bool):\n            raise AMQPInvalidArgument('durable should be a boolean')\n        elif not isinstance(exclusive, bool):\n            raise AMQPInvalidArgument('exclusive should be a boolean')\n        elif not isinstance(auto_delete, bool):\n            raise AMQPInvalidArgument('auto_delete should be a boolean')\n        elif arguments is not None and not isinstance(arguments, dict):\n            raise AMQPInvalidArgument('arguments should be a dict or None')\n\n        declare_frame = pamqp_queue.Declare(queue=queue,\n                                            passive=passive,\n                                            durable=durable,\n                                            exclusive=exclusive,\n                                            auto_delete=auto_delete,\n                                            arguments=arguments)\n        return self._channel.rpc_request(declare_frame)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a Queue. :param str queue: Queue name :param bool if_unused: Delete only if unused :param bool if_empty: Delete only if empty :raises AMQPInvalidArgument: Invalid Parameters :raises AMQPChannelError: Raises if the channel encountered an error. :raises AMQPConnectionError: Raises if the connection encountered an error. :rtype: dict", "response": "def delete(self, queue='', if_unused=False, if_empty=False):\n        \"\"\"Delete a Queue.\n\n        :param str queue: Queue name\n        :param bool if_unused: Delete only if unused\n        :param bool if_empty: Delete only if empty\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict\n        \"\"\"\n        if not compatibility.is_string(queue):\n            raise AMQPInvalidArgument('queue should be a string')\n        elif not isinstance(if_unused, bool):\n            raise AMQPInvalidArgument('if_unused should be a boolean')\n        elif not isinstance(if_empty, bool):\n            raise AMQPInvalidArgument('if_empty should be a boolean')\n\n        delete_frame = pamqp_queue.Delete(queue=queue, if_unused=if_unused,\n                                          if_empty=if_empty)\n        return self._channel.rpc_request(delete_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef purge(self, queue):\n        if not compatibility.is_string(queue):\n            raise AMQPInvalidArgument('queue should be a string')\n\n        purge_frame = pamqp_queue.Purge(queue=queue)\n\n        return self._channel.rpc_request(purge_frame)", "response": "Purge a Queue.\n\n        :param str queue: Queue name\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bind(self, queue='', exchange='', routing_key='', arguments=None):\n        if not compatibility.is_string(queue):\n            raise AMQPInvalidArgument('queue should be a string')\n        elif not compatibility.is_string(exchange):\n            raise AMQPInvalidArgument('exchange should be a string')\n        elif not compatibility.is_string(routing_key):\n            raise AMQPInvalidArgument('routing_key should be a string')\n        elif arguments is not None and not isinstance(arguments, dict):\n            raise AMQPInvalidArgument('arguments should be a dict or None')\n\n        bind_frame = pamqp_queue.Bind(queue=queue,\n                                      exchange=exchange,\n                                      routing_key=routing_key,\n                                      arguments=arguments)\n        return self._channel.rpc_request(bind_frame)", "response": "Bind a Queue.\n\n        :param str queue: Queue name\n        :param str exchange: Exchange name\n        :param str routing_key: The routing key to use\n        :param dict arguments: Bind key/value arguments\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nunbind a Queue. :param str queue: Queue name :param str exchange: Exchange name :param str routing_key: The routing key used :param dict arguments: Unbind key/value arguments :raises AMQPInvalidArgument: Invalid Parameters :raises AMQPChannelError: Raises if the channel encountered an error. :raises AMQPConnectionError: Raises if the connection encountered an error. :rtype: dict", "response": "def unbind(self, queue='', exchange='', routing_key='', arguments=None):\n        \"\"\"Unbind a Queue.\n\n        :param str queue: Queue name\n        :param str exchange: Exchange name\n        :param str routing_key: The routing key used\n        :param dict arguments: Unbind key/value arguments\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: dict\n        \"\"\"\n        if not compatibility.is_string(queue):\n            raise AMQPInvalidArgument('queue should be a string')\n        elif not compatibility.is_string(exchange):\n            raise AMQPInvalidArgument('exchange should be a string')\n        elif not compatibility.is_string(routing_key):\n            raise AMQPInvalidArgument('routing_key should be a string')\n        elif arguments is not None and not isinstance(arguments, dict):\n            raise AMQPInvalidArgument('arguments should be a dict or None')\n\n        unbind_frame = pamqp_queue.Unbind(queue=queue,\n                                          exchange=exchange,\n                                          routing_key=routing_key,\n                                          arguments=arguments)\n        return self._channel.rpc_request(unbind_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_frame(self, frame_in):\n        LOGGER.debug('Frame Received: %s', frame_in.name)\n        if frame_in.name == 'Heartbeat':\n            return\n        elif frame_in.name == 'Connection.Close':\n            self._close_connection(frame_in)\n        elif frame_in.name == 'Connection.CloseOk':\n            self._close_connection_ok()\n        elif frame_in.name == 'Connection.Blocked':\n            self._blocked_connection(frame_in)\n        elif frame_in.name == 'Connection.Unblocked':\n            self._unblocked_connection()\n        elif frame_in.name == 'Connection.OpenOk':\n            self._set_connection_state(Stateful.OPEN)\n        elif frame_in.name == 'Connection.Start':\n            self.server_properties = frame_in.server_properties\n            self._send_start_ok(frame_in)\n        elif frame_in.name == 'Connection.Tune':\n            self._send_tune_ok(frame_in)\n            self._send_open_connection()\n        else:\n            LOGGER.error('[Channel0] Unhandled Frame: %s', frame_in.name)", "response": "Handle incoming Amqp frames."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a Start OK frame.", "response": "def _send_start_ok(self, frame_in):\n        \"\"\"Send Start OK frame.\n\n        :param specification.Connection.Start frame_in: Amqp frame.\n        :return:\n        \"\"\"\n        mechanisms = try_utf8_decode(frame_in.mechanisms)\n        if 'EXTERNAL' in mechanisms:\n            mechanism = 'EXTERNAL'\n            credentials = '\\0\\0'\n        elif 'PLAIN' in mechanisms:\n            mechanism = 'PLAIN'\n            credentials = self._plain_credentials()\n        else:\n            exception = AMQPConnectionError(\n                'Unsupported Security Mechanism(s): %s' %\n                frame_in.mechanisms\n            )\n            self._connection.exceptions.append(exception)\n            return\n        start_ok_frame = specification.Connection.StartOk(\n            mechanism=mechanism,\n            client_properties=self._client_properties(),\n            response=credentials,\n            locale=LOCALE\n        )\n        self._write_frame(start_ok_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a Tune OK frame.", "response": "def _send_tune_ok(self, frame_in):\n        \"\"\"Send Tune OK frame.\n\n        :param specification.Connection.Tune frame_in: Tune frame.\n\n        :return:\n        \"\"\"\n        self.max_allowed_channels = self._negotiate(frame_in.channel_max,\n                                                    MAX_CHANNELS)\n        self.max_frame_size = self._negotiate(frame_in.frame_max,\n                                              MAX_FRAME_SIZE)\n        LOGGER.debug(\n            'Negotiated max frame size %d, max channels %d',\n            self.max_frame_size, self.max_allowed_channels\n        )\n\n        tune_ok_frame = specification.Connection.TuneOk(\n            channel_max=self.max_allowed_channels,\n            frame_max=self.max_frame_size,\n            heartbeat=self._heartbeat)\n        self._write_frame(tune_ok_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _send_open_connection(self):\n        open_frame = specification.Connection.Open(\n            virtual_host=self._parameters['virtual_host']\n        )\n        self._write_frame(open_frame)", "response": "Send Open Connection frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites a pamqp frame from Channel0.", "response": "def _write_frame(self, frame_out):\n        \"\"\"Write a pamqp frame from Channel0.\n\n        :param frame_out: Amqp frame.\n        :return:\n        \"\"\"\n        self._connection.write_frame(0, frame_out)\n        LOGGER.debug('Frame Sent: %s', frame_out.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _client_properties():\n        return {\n            'product': 'AMQPStorm',\n            'platform': 'Python %s (%s)' % (platform.python_version(),\n                                            platform.python_implementation()),\n            'capabilities': {\n                'basic.nack': True,\n                'connection.blocked': True,\n                'publisher_confirms': True,\n                'consumer_cancel_notify': True,\n                'authentication_failure_close': True,\n            },\n            'information': 'See https://github.com/eandersson/amqpstorm',\n            'version': __version__\n        }", "response": "A dictionary containing the current client properties."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_inbound_messages(self, break_on_empty=False, to_tuple=False,\n                               auto_decode=True):\n        \"\"\"Build messages in the inbound queue.\n\n        :param bool break_on_empty: Should we break the loop when there are\n                                    no more messages in our inbound queue.\n\n                                    This does not guarantee that the upstream\n                                    queue is empty, as it's possible that if\n                                    messages are consumed faster than\n                                    delivered, the inbound queue will then be\n                                    emptied and the consumption will be broken.\n        :param bool to_tuple: Should incoming messages be converted to a\n                              tuple before delivery.\n        :param bool auto_decode: Auto-decode strings when possible.\n\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: :py:class:`generator`\n        \"\"\"\n        self.check_for_errors()\n        while not self.is_closed:\n            message = self._build_message(auto_decode=auto_decode)\n            if not message:\n                self.check_for_errors()\n                sleep(IDLE_WAIT)\n                if break_on_empty and not self._inbound:\n                    break\n                continue\n            if to_tuple:\n                yield message.to_tuple()\n                continue\n            yield message", "response": "Build messages in the inbound queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing the current channel.", "response": "def close(self, reply_code=200, reply_text=''):\n        \"\"\"Close Channel.\n\n        :param int reply_code: Close reply code (e.g. 200)\n        :param str reply_text: Close reply text\n\n        :raises AMQPInvalidArgument: Invalid Parameters\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :return:\n        \"\"\"\n        if not compatibility.is_integer(reply_code):\n            raise AMQPInvalidArgument('reply_code should be an integer')\n        elif not compatibility.is_string(reply_text):\n            raise AMQPInvalidArgument('reply_text should be a string')\n        try:\n            if self._connection.is_closed or self.is_closed:\n                self.stop_consuming()\n                LOGGER.debug('Channel #%d forcefully Closed', self.channel_id)\n                return\n            self.set_state(self.CLOSING)\n            LOGGER.debug('Channel #%d Closing', self.channel_id)\n            try:\n                self.stop_consuming()\n            except AMQPChannelError:\n                self.remove_consumer_tag()\n            self.rpc_request(specification.Channel.Close(\n                reply_code=reply_code,\n                reply_text=reply_text),\n                connection_adapter=self._connection\n            )\n        finally:\n            if self._inbound:\n                del self._inbound[:]\n            self.set_state(self.CLOSED)\n            if self._on_close_impl:\n                self._on_close_impl(self.channel_id)\n        LOGGER.debug('Channel #%d Closed', self.channel_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking connection and channel for errors.", "response": "def check_for_errors(self):\n        \"\"\"Check connection and channel for errors.\n\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n        :return:\n        \"\"\"\n        try:\n            self._connection.check_for_errors()\n        except AMQPConnectionError:\n            self.set_state(self.CLOSED)\n            raise\n\n        if self.exceptions:\n            exception = self.exceptions[0]\n            if self.is_open:\n                self.exceptions.pop(0)\n            raise exception\n\n        if self.is_closed:\n            raise AMQPChannelError('channel was closed')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the channel to confirm that each message has been delivered.", "response": "def confirm_deliveries(self):\n        \"\"\"Set the channel to confirm that each message has been\n        successfully delivered.\n\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :return:\n        \"\"\"\n        self._confirming_deliveries = True\n        confirm_frame = specification.Confirm.Select()\n        return self.rpc_request(confirm_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle a single frame sent to this specific channel.", "response": "def on_frame(self, frame_in):\n        \"\"\"Handle frame sent to this specific channel.\n\n        :param pamqp.Frame frame_in: Amqp frame.\n        :return:\n        \"\"\"\n        if self.rpc.on_frame(frame_in):\n            return\n\n        if frame_in.name in CONTENT_FRAME:\n            self._inbound.append(frame_in)\n        elif frame_in.name == 'Basic.Cancel':\n            self._basic_cancel(frame_in)\n        elif frame_in.name == 'Basic.CancelOk':\n            self.remove_consumer_tag(frame_in.consumer_tag)\n        elif frame_in.name == 'Basic.ConsumeOk':\n            self.add_consumer_tag(frame_in['consumer_tag'])\n        elif frame_in.name == 'Basic.Return':\n            self._basic_return(frame_in)\n        elif frame_in.name == 'Channel.Close':\n            self._close_channel(frame_in)\n        elif frame_in.name == 'Channel.Flow':\n            self.write_frame(specification.Channel.FlowOk(frame_in.active))\n        else:\n            LOGGER.error(\n                '[Channel%d] Unhandled Frame: %s -- %s',\n                self.channel_id, frame_in.name, dict(frame_in)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconsuming inbound messages. :param bool to_tuple: Should incoming messages be converted to a tuple before delivery. :param bool auto_decode: Auto-decode strings when possible. :raises AMQPChannelError: Raises if the channel encountered an error. :raises AMQPConnectionError: Raises if the connection encountered an error. :return:", "response": "def process_data_events(self, to_tuple=False, auto_decode=True):\n        \"\"\"Consume inbound messages.\n\n        :param bool to_tuple: Should incoming messages be converted to a\n                              tuple before delivery.\n        :param bool auto_decode: Auto-decode strings when possible.\n\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :return:\n        \"\"\"\n        if not self._consumer_callbacks:\n            raise AMQPChannelError('no consumer callback defined')\n        for message in self.build_inbound_messages(break_on_empty=True,\n                                                   auto_decode=auto_decode):\n            consumer_tag = message._method.get('consumer_tag')\n            if to_tuple:\n                # noinspection PyCallingNonCallable\n                self._consumer_callbacks[consumer_tag](*message.to_tuple())\n                continue\n            # noinspection PyCallingNonCallable\n            self._consumer_callbacks[consumer_tag](message)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a RPC Request.", "response": "def rpc_request(self, frame_out, connection_adapter=None):\n        \"\"\"Perform a RPC Request.\n\n        :param specification.Frame frame_out: Amqp frame.\n        :rtype: dict\n        \"\"\"\n        with self.rpc.lock:\n            uuid = self.rpc.register_request(frame_out.valid_responses)\n            self._connection.write_frame(self.channel_id, frame_out)\n            return self.rpc.get_request(\n                uuid, connection_adapter=connection_adapter\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_consuming(self, to_tuple=False, auto_decode=True):\n        while not self.is_closed:\n            self.process_data_events(\n                to_tuple=to_tuple,\n                auto_decode=auto_decode\n            )\n            if self.consumer_tags:\n                sleep(IDLE_WAIT)\n                continue\n            break", "response": "Start consuming messages.\n\n        :param bool to_tuple: Should incoming messages be converted to a\n                              tuple before delivery.\n        :param bool auto_decode: Auto-decode strings when possible.\n\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop_consuming(self):\n        if not self.consumer_tags:\n            return\n        if not self.is_closed:\n            for tag in self.consumer_tags:\n                self.basic.cancel(tag)\n        self.remove_consumer_tag()", "response": "Stop consuming messages.\n\n        :raises AMQPChannelError: Raises if the channel encountered an error.\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_frame(self, frame_out):\n        self.check_for_errors()\n        self._connection.write_frame(self.channel_id, frame_out)", "response": "Write a pamqp frame from the current channel."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_frames(self, frames_out):\n        self.check_for_errors()\n        self._connection.write_frames(self.channel_id, frames_out)", "response": "Write multiple pamqp frames from the current channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles a Basic. Cancel frame.", "response": "def _basic_cancel(self, frame_in):\n        \"\"\"Handle a Basic Cancel frame.\n\n        :param specification.Basic.Cancel frame_in: Amqp frame.\n\n        :return:\n        \"\"\"\n        LOGGER.warning(\n            'Received Basic.Cancel on consumer_tag: %s',\n            try_utf8_decode(frame_in.consumer_tag)\n        )\n        self.remove_consumer_tag(frame_in.consumer_tag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _basic_return(self, frame_in):\n        reply_text = try_utf8_decode(frame_in.reply_text)\n        message = (\n            \"Message not delivered: %s (%s) to queue '%s' from exchange '%s'\" %\n            (\n                reply_text,\n                frame_in.reply_code,\n                frame_in.routing_key,\n                frame_in.exchange\n            )\n        )\n        exception = AMQPMessageError(message,\n                                     reply_code=frame_in.reply_code)\n        self.exceptions.append(exception)", "response": "Handle a Basic Return Frame and treat it as an error."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_message(self, auto_decode):\n        with self.lock:\n            if len(self._inbound) < 2:\n                return None\n            headers = self._build_message_headers()\n            if not headers:\n                return None\n            basic_deliver, content_header = headers\n            body = self._build_message_body(content_header.body_size)\n\n        message = Message(channel=self,\n                          body=body,\n                          method=dict(basic_deliver),\n                          properties=dict(content_header.properties),\n                          auto_decode=auto_decode)\n        return message", "response": "Fetch and build a complete Message from the inbound queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_message_headers(self):\n        basic_deliver = self._inbound.pop(0)\n        if not isinstance(basic_deliver, specification.Basic.Deliver):\n            LOGGER.warning(\n                'Received an out-of-order frame: %s was '\n                'expecting a Basic.Deliver frame',\n                type(basic_deliver)\n            )\n            return None\n        content_header = self._inbound.pop(0)\n        if not isinstance(content_header, ContentHeader):\n            LOGGER.warning(\n                'Received an out-of-order frame: %s was '\n                'expecting a ContentHeader frame',\n                type(content_header)\n            )\n            return None\n\n        return basic_deliver, content_header", "response": "Fetch Message Headers (Deliver & Header Frames).\n\n        :rtype: tuple|None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_message_body(self, body_size):\n        body = bytes()\n        while len(body) < body_size:\n            if not self._inbound:\n                self.check_for_errors()\n                sleep(IDLE_WAIT)\n                continue\n            body_piece = self._inbound.pop(0)\n            if not body_piece.value:\n                break\n            body += body_piece.value\n        return body", "response": "Build the message body from the inbound queue."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _close_channel(self, frame_in):\n        if frame_in.reply_code != 200:\n            reply_text = try_utf8_decode(frame_in.reply_text)\n            message = (\n                'Channel %d was closed by remote server: %s' %\n                (\n                    self._channel_id,\n                    reply_text\n                )\n            )\n            exception = AMQPChannelError(message,\n                                         reply_code=frame_in.reply_code)\n            self.exceptions.append(exception)\n        self.set_state(self.CLOSED)\n        if self._connection.is_open:\n\n            try:\n                self._connection.write_frame(\n                    self.channel_id, specification.Channel.CloseOk()\n                )\n            except AMQPConnectionError:\n                pass\n        self.close()", "response": "This method closes the channel."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(self, username, password, tags=''):\n        user_payload = json.dumps({\n            'password': password,\n            'tags': tags\n        })\n        return self.http_client.put(API_USER % username,\n                                    payload=user_payload)", "response": "Create User.\n\n        :param str username: Username\n        :param str password: Password\n        :param str tags: Comma-separate list of tags (e.g. monitoring)\n\n        :rtype: None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting User permissions for the configured virtual host.", "response": "def get_permission(self, username, virtual_host):\n        \"\"\"Get User permissions for the configured virtual host.\n\n        :param str username: Username\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(API_USER_VIRTUAL_HOST_PERMISSIONS %\n                                    (\n                                        virtual_host,\n                                        username\n                                    ))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting permissions for the user in the configured virtual host.", "response": "def set_permission(self, username, virtual_host, configure_regex='.*',\n                       write_regex='.*', read_regex='.*'):\n        \"\"\"Set User permissions for the configured virtual host.\n\n        :param str username: Username\n        :param str virtual_host: Virtual host name\n        :param str configure_regex: Permission pattern for configuration\n                                    operations for this user.\n        :param str write_regex: Permission pattern for write operations\n                                for this user.\n        :param str read_regex: Permission pattern for read operations\n                               for this user.\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        permission_payload = json.dumps({\n            \"configure\": configure_regex,\n            \"read\": read_regex,\n            \"write\": write_regex\n        })\n        return self.http_client.put(API_USER_VIRTUAL_HOST_PERMISSIONS %\n                                    (\n                                        virtual_host,\n                                        username\n                                    ),\n                                    payload=permission_payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_permission(self, username, virtual_host):\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.delete(\n            API_USER_VIRTUAL_HOST_PERMISSIONS %\n            (\n                virtual_host,\n                username\n            ))", "response": "Delete permissions for the user in the virtual host."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts the RPC Server.", "response": "def start_server(self):\n        \"\"\"Start the RPC Server.\n\n        :return:\n        \"\"\"\n        self._stopped.clear()\n        if not self._connection or self._connection.is_closed:\n            self._create_connection()\n        while not self._stopped.is_set():\n            try:\n                # Check our connection for errors.\n                self._connection.check_for_errors()\n                self._update_consumers()\n            except amqpstorm.AMQPError as why:\n                # If an error occurs, re-connect and let update_consumers\n                # re-open the channels.\n                LOGGER.warning(why)\n                self._stop_consumers()\n                self._create_connection()\n            time.sleep(1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the list of consumers.", "response": "def _update_consumers(self):\n        \"\"\"Update Consumers.\n\n            - Add more if requested.\n            - Make sure the consumers are healthy.\n            - Remove excess consumers.\n\n        :return:\n        \"\"\"\n        # Do we need to start more consumers.\n        consumer_to_start = \\\n            min(max(self.number_of_consumers - len(self._consumers), 0), 2)\n        for _ in range(consumer_to_start):\n            consumer = Consumer(self.rpc_queue)\n            self._start_consumer(consumer)\n            self._consumers.append(consumer)\n\n        # Check that all our consumers are active.\n        for consumer in self._consumers:\n            if consumer.active:\n                continue\n            self._start_consumer(consumer)\n            break\n\n        # Do we have any overflow of consumers.\n        self._stop_consumers(self.number_of_consumers)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, path, payload=None, headers=None):\n        return self._request('get', path, payload, headers)", "response": "HTTP GET operation.\n\n        :param path: URI Path\n        :param payload: HTTP Body\n        :param headers: HTTP Headers\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :return: Response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put(self, path, payload=None, headers=None):\n        return self._request('put', path, payload, headers)", "response": "HTTP PUT operation.\n\n        :param path: URI Path\n        :param payload: HTTP Body\n        :param headers: HTTP Headers\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :return: Response"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _request(self, method, path, payload=None, headers=None):\n        url = urlparse.urljoin(self._base_url, 'api/%s' % path)\n        headers = headers or {}\n        headers['content-type'] = 'application/json'\n        try:\n            response = requests.request(\n                method, url,\n                auth=self._auth,\n                data=payload,\n                headers=headers,\n                cert=self._cert,\n                verify=self._verify,\n                timeout=self._timeout\n            )\n        except requests.RequestException as why:\n            raise ApiConnectionError(str(why))\n\n        json_response = self._get_json_output(response)\n        self._check_for_errors(response, json_response)\n        return json_response", "response": "HTTP operation.\n\n        :param method: Operation type (e.g. post)\n        :param path: URI Path\n        :param payload: HTTP Body\n        :param headers: HTTP Headers\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :return: Response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_for_errors(response, json_response):\n        status_code = response.status_code\n        try:\n            response.raise_for_status()\n        except requests.HTTPError as why:\n            raise ApiError(str(why), reply_code=status_code)\n        if isinstance(json_response, dict) and 'error' in json_response:\n            raise ApiError(json_response['error'], reply_code=status_code)", "response": "Check payload for errors.\n\n        :param response: HTTP response\n        :param json_response: Json response\n\n        :raises ApiError: Raises if the remote server encountered an error.\n\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning basic healthchecks against the current node or a given node.", "response": "def get(self, node=None):\n        \"\"\"Run basic healthchecks against the current node, or against a given\n        node.\n\n            Example response:\n                > {\"status\":\"ok\"}\n                > {\"status\":\"failed\",\"reason\":\"string\"}\n\n        :param node: Node name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        if not node:\n            return self.http_client.get(HEALTHCHECKS)\n        return self.http_client.get(HEALTHCHECKS_NODE % node)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating for each variant the mean number of pairwise differences between chromosomes sampled from within a single population.", "response": "def mean_pairwise_difference(ac, an=None, fill=np.nan):\n    \"\"\"Calculate for each variant the mean number of pairwise differences\n    between chromosomes sampled from within a single population.\n\n    Parameters\n    ----------\n\n    ac : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array.\n    an : array_like, int, shape (n_variants,), optional\n        Allele numbers. If not provided, will be calculated from `ac`.\n    fill : float\n        Use this value where there are no pairs to compare (e.g.,\n        all allele calls are missing).\n\n    Returns\n    -------\n\n    mpd : ndarray, float, shape (n_variants,)\n\n    Notes\n    -----\n\n    The values returned by this function can be summed over a genome\n    region and divided by the number of accessible bases to estimate\n    nucleotide diversity, a.k.a. *pi*.\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> h = allel.HaplotypeArray([[0, 0, 0, 0],\n    ...                           [0, 0, 0, 1],\n    ...                           [0, 0, 1, 1],\n    ...                           [0, 1, 1, 1],\n    ...                           [1, 1, 1, 1],\n    ...                           [0, 0, 1, 2],\n    ...                           [0, 1, 1, 2],\n    ...                           [0, 1, -1, -1]])\n    >>> ac = h.count_alleles()\n    >>> allel.mean_pairwise_difference(ac)\n    array([0.        , 0.5       , 0.66666667, 0.5       , 0.        ,\n           0.83333333, 0.83333333, 1.        ])\n\n    See Also\n    --------\n\n    sequence_diversity, windowed_diversity\n\n    \"\"\"\n\n    # This function calculates the mean number of pairwise differences\n    # between haplotypes within a single population, generalising to any number\n    # of alleles.\n\n    # check inputs\n    ac = asarray_ndim(ac, 2)\n\n    # total number of haplotypes\n    if an is None:\n        an = np.sum(ac, axis=1)\n    else:\n        an = asarray_ndim(an, 1)\n        check_dim0_aligned(ac, an)\n\n    # total number of pairwise comparisons for each variant:\n    # (an choose 2)\n    n_pairs = an * (an - 1) / 2\n\n    # number of pairwise comparisons where there is no difference:\n    # sum of (ac choose 2) for each allele (i.e., number of ways to\n    # choose the same allele twice)\n    n_same = np.sum(ac * (ac - 1) / 2, axis=1)\n\n    # number of pairwise differences\n    n_diff = n_pairs - n_same\n\n    # mean number of pairwise differences, accounting for cases where\n    # there are no pairs\n    with ignore_invalid():\n        mpd = np.where(n_pairs > 0, n_diff / n_pairs, fill)\n\n    return mpd"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating for each variant the mean number of pairwise differences between chromosomes sampled from two different populations.", "response": "def mean_pairwise_difference_between(ac1, ac2, an1=None, an2=None,\n                                     fill=np.nan):\n    \"\"\"Calculate for each variant the mean number of pairwise differences\n    between chromosomes sampled from two different populations.\n\n    Parameters\n    ----------\n\n    ac1 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array from the first population.\n    ac2 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array from the second population.\n    an1 : array_like, int, shape (n_variants,), optional\n        Allele numbers for the first population. If not provided, will be\n        calculated from `ac1`.\n    an2 : array_like, int, shape (n_variants,), optional\n        Allele numbers for the second population. If not provided, will be\n        calculated from `ac2`.\n    fill : float\n        Use this value where there are no pairs to compare (e.g.,\n        all allele calls are missing).\n\n    Returns\n    -------\n\n    mpd : ndarray, float, shape (n_variants,)\n\n    Notes\n    -----\n\n    The values returned by this function can be summed over a genome\n    region and divided by the number of accessible bases to estimate\n    nucleotide divergence between two populations, a.k.a. *Dxy*.\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> h = allel.HaplotypeArray([[0, 0, 0, 0],\n    ...                           [0, 0, 0, 1],\n    ...                           [0, 0, 1, 1],\n    ...                           [0, 1, 1, 1],\n    ...                           [1, 1, 1, 1],\n    ...                           [0, 0, 1, 2],\n    ...                           [0, 1, 1, 2],\n    ...                           [0, 1, -1, -1]])\n    >>> ac1 = h.count_alleles(subpop=[0, 1])\n    >>> ac2 = h.count_alleles(subpop=[2, 3])\n    >>> allel.mean_pairwise_difference_between(ac1, ac2)\n    array([0.  , 0.5 , 1.  , 0.5 , 0.  , 1.  , 0.75,  nan])\n\n    See Also\n    --------\n\n    sequence_divergence, windowed_divergence\n\n    \"\"\"\n\n    # This function calculates the mean number of pairwise differences\n    # between haplotypes from two different populations, generalising to any\n    # number of alleles.\n\n    # check inputs\n    ac1 = asarray_ndim(ac1, 2)\n    ac2 = asarray_ndim(ac2, 2)\n    check_dim0_aligned(ac1, ac2)\n    ac1, ac2 = ensure_dim1_aligned(ac1, ac2)\n\n    # total number of haplotypes sampled from each population\n    if an1 is None:\n        an1 = np.sum(ac1, axis=1)\n    else:\n        an1 = asarray_ndim(an1, 1)\n        check_dim0_aligned(ac1, an1)\n    if an2 is None:\n        an2 = np.sum(ac2, axis=1)\n    else:\n        an2 = asarray_ndim(an2, 1)\n        check_dim0_aligned(ac2, an2)\n\n    # total number of pairwise comparisons for each variant\n    n_pairs = an1 * an2\n\n    # number of pairwise comparisons where there is no difference:\n    # sum of (ac1 * ac2) for each allele (i.e., number of ways to\n    # choose the same allele twice)\n    n_same = np.sum(ac1 * ac2, axis=1)\n\n    # number of pairwise differences\n    n_diff = n_pairs - n_same\n\n    # mean number of pairwise differences, accounting for cases where\n    # there are no pairs\n    with ignore_invalid():\n        mpd = np.where(n_pairs > 0, n_diff / n_pairs, fill)\n\n    return mpd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nestimate nucleotide diversity within a given region.", "response": "def sequence_diversity(pos, ac, start=None, stop=None,\n                       is_accessible=None):\n    \"\"\"Estimate nucleotide diversity within a given region, which is the\n    average proportion of sites (including monomorphic sites not present in the\n    data) that differ between randomly chosen pairs of chromosomes.\n\n    Parameters\n    ----------\n\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    ac : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array.\n    start : int, optional\n        The position at which to start (1-based). Defaults to the first position.\n    stop : int, optional\n        The position at which to stop (1-based). Defaults to the last position.\n    is_accessible : array_like, bool, shape (len(contig),), optional\n        Boolean array indicating accessibility status for all positions in the\n        chromosome/contig.\n\n    Returns\n    -------\n\n    pi : ndarray, float, shape (n_windows,)\n        Nucleotide diversity.\n\n    Notes\n    -----\n\n    If start and/or stop are not provided, uses the difference between the last\n    and the first position as a proxy for the total number of sites, which can\n    overestimate the sequence diversity.\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> g = allel.GenotypeArray([[[0, 0], [0, 0]],\n    ...                          [[0, 0], [0, 1]],\n    ...                          [[0, 0], [1, 1]],\n    ...                          [[0, 1], [1, 1]],\n    ...                          [[1, 1], [1, 1]],\n    ...                          [[0, 0], [1, 2]],\n    ...                          [[0, 1], [1, 2]],\n    ...                          [[0, 1], [-1, -1]],\n    ...                          [[-1, -1], [-1, -1]]])\n    >>> ac = g.count_alleles()\n    >>> pos = [2, 4, 7, 14, 15, 18, 19, 25, 27]\n    >>> pi = allel.sequence_diversity(pos, ac, start=1, stop=31)\n    >>> pi\n    0.13978494623655915\n\n    \"\"\"\n\n    # check inputs\n    if not isinstance(pos, SortedIndex):\n        pos = SortedIndex(pos, copy=False)\n    ac = asarray_ndim(ac, 2)\n    is_accessible = asarray_ndim(is_accessible, 1, allow_none=True)\n\n    # deal with subregion\n    if start is not None or stop is not None:\n        loc = pos.locate_range(start, stop)\n        pos = pos[loc]\n        ac = ac[loc]\n    if start is None:\n        start = pos[0]\n    if stop is None:\n        stop = pos[-1]\n\n    # calculate mean pairwise difference\n    mpd = mean_pairwise_difference(ac, fill=0)\n\n    # sum differences over variants\n    mpd_sum = np.sum(mpd)\n\n    # calculate value per base\n    if is_accessible is None:\n        n_bases = stop - start + 1\n    else:\n        n_bases = np.count_nonzero(is_accessible[start-1:stop])\n\n    pi = mpd_sum / n_bases\n    return pi"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nestimate nucleotide divergence between two populations within a random cluster.", "response": "def sequence_divergence(pos, ac1, ac2, an1=None, an2=None, start=None,\n                        stop=None, is_accessible=None):\n    \"\"\"Estimate nucleotide divergence between two populations within a\n    given region, which is the average proportion of sites (including\n    monomorphic sites not present in the data) that differ between randomly\n    chosen pairs of chromosomes, one from each population.\n\n    Parameters\n    ----------\n\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    ac1 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array for the first population.\n    ac2 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array for the second population.\n    an1 : array_like, int, shape (n_variants,), optional\n        Allele numbers for the first population. If not provided, will be\n        calculated from `ac1`.\n    an2 : array_like, int, shape (n_variants,), optional\n        Allele numbers for the second population. If not provided, will be\n        calculated from `ac2`.\n    start : int, optional\n        The position at which to start (1-based). Defaults to the first position.\n    stop : int, optional\n        The position at which to stop (1-based). Defaults to the last position.\n    is_accessible : array_like, bool, shape (len(contig),), optional\n        Boolean array indicating accessibility status for all positions in the\n        chromosome/contig.\n\n    Returns\n    -------\n\n    Dxy : ndarray, float, shape (n_windows,)\n        Nucleotide divergence.\n\n    Examples\n    --------\n\n    Simplest case, two haplotypes in each population::\n\n        >>> import allel\n        >>> h = allel.HaplotypeArray([[0, 0, 0, 0],\n        ...                           [0, 0, 0, 1],\n        ...                           [0, 0, 1, 1],\n        ...                           [0, 1, 1, 1],\n        ...                           [1, 1, 1, 1],\n        ...                           [0, 0, 1, 2],\n        ...                           [0, 1, 1, 2],\n        ...                           [0, 1, -1, -1],\n        ...                           [-1, -1, -1, -1]])\n        >>> ac1 = h.count_alleles(subpop=[0, 1])\n        >>> ac2 = h.count_alleles(subpop=[2, 3])\n        >>> pos = [2, 4, 7, 14, 15, 18, 19, 25, 27]\n        >>> dxy = sequence_divergence(pos, ac1, ac2, start=1, stop=31)\n        >>> dxy\n        0.12096774193548387\n\n    \"\"\"\n\n    # check inputs\n    if not isinstance(pos, SortedIndex):\n        pos = SortedIndex(pos, copy=False)\n    ac1 = asarray_ndim(ac1, 2)\n    ac2 = asarray_ndim(ac2, 2)\n    if an1 is not None:\n        an1 = asarray_ndim(an1, 1)\n    if an2 is not None:\n        an2 = asarray_ndim(an2, 1)\n    is_accessible = asarray_ndim(is_accessible, 1, allow_none=True)\n\n    # handle start/stop\n    if start is not None or stop is not None:\n        loc = pos.locate_range(start, stop)\n        pos = pos[loc]\n        ac1 = ac1[loc]\n        ac2 = ac2[loc]\n        if an1 is not None:\n            an1 = an1[loc]\n        if an2 is not None:\n            an2 = an2[loc]\n    if start is None:\n        start = pos[0]\n    if stop is None:\n        stop = pos[-1]\n\n    # calculate mean pairwise difference between the two populations\n    mpd = mean_pairwise_difference_between(ac1, ac2, an1=an1, an2=an2, fill=0)\n\n    # sum differences over variants\n    mpd_sum = np.sum(mpd)\n\n    # calculate value per base, N.B., expect pos is 1-based\n    if is_accessible is None:\n        n_bases = stop - start + 1\n    else:\n        n_bases = np.count_nonzero(is_accessible[start-1:stop])\n\n    dxy = mpd_sum / n_bases\n\n    return dxy"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef windowed_diversity(pos, ac, size=None, start=None, stop=None, step=None,\n                       windows=None, is_accessible=None, fill=np.nan):\n    \"\"\"Estimate nucleotide diversity in windows over a single\n    chromosome/contig.\n\n    Parameters\n    ----------\n\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    ac : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array.\n    size : int, optional\n        The window size (number of bases).\n    start : int, optional\n        The position at which to start (1-based).\n    stop : int, optional\n        The position at which to stop (1-based).\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    windows : array_like, int, shape (n_windows, 2), optional\n        Manually specify the windows to use as a sequence of (window_start,\n        window_stop) positions, using 1-based coordinates. Overrides the\n        size/start/stop/step parameters.\n    is_accessible : array_like, bool, shape (len(contig),), optional\n        Boolean array indicating accessibility status for all positions in the\n        chromosome/contig.\n    fill : object, optional\n        The value to use where a window is completely inaccessible.\n\n    Returns\n    -------\n\n    pi : ndarray, float, shape (n_windows,)\n        Nucleotide diversity in each window.\n    windows : ndarray, int, shape (n_windows, 2)\n        The windows used, as an array of (window_start, window_stop) positions,\n        using 1-based coordinates.\n    n_bases : ndarray, int, shape (n_windows,)\n        Number of (accessible) bases in each window.\n    counts : ndarray, int, shape (n_windows,)\n        Number of variants in each window.\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> g = allel.GenotypeArray([[[0, 0], [0, 0]],\n    ...                          [[0, 0], [0, 1]],\n    ...                          [[0, 0], [1, 1]],\n    ...                          [[0, 1], [1, 1]],\n    ...                          [[1, 1], [1, 1]],\n    ...                          [[0, 0], [1, 2]],\n    ...                          [[0, 1], [1, 2]],\n    ...                          [[0, 1], [-1, -1]],\n    ...                          [[-1, -1], [-1, -1]]])\n    >>> ac = g.count_alleles()\n    >>> pos = [2, 4, 7, 14, 15, 18, 19, 25, 27]\n    >>> pi, windows, n_bases, counts = allel.windowed_diversity(\n    ...     pos, ac, size=10, start=1, stop=31\n    ... )\n    >>> pi\n    array([0.11666667, 0.21666667, 0.09090909])\n    >>> windows\n    array([[ 1, 10],\n           [11, 20],\n           [21, 31]])\n    >>> n_bases\n    array([10, 10, 11])\n    >>> counts\n    array([3, 4, 2])\n\n    \"\"\"\n\n    # check inputs\n    if not isinstance(pos, SortedIndex):\n        pos = SortedIndex(pos, copy=False)\n    is_accessible = asarray_ndim(is_accessible, 1, allow_none=True)\n\n    # calculate mean pairwise difference\n    mpd = mean_pairwise_difference(ac, fill=0)\n\n    # sum differences in windows\n    mpd_sum, windows, counts = windowed_statistic(\n        pos, values=mpd, statistic=np.sum, size=size, start=start, stop=stop,\n        step=step, windows=windows, fill=0\n    )\n\n    # calculate value per base\n    pi, n_bases = per_base(mpd_sum, windows, is_accessible=is_accessible,\n                           fill=fill)\n\n    return pi, windows, n_bases, counts", "response": "Estimate nucleotide diversity in windows over a single base."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nestimate nucleotide divergence between two populations in a single window.", "response": "def windowed_divergence(pos, ac1, ac2, size=None, start=None, stop=None,\n                        step=None, windows=None, is_accessible=None,\n                        fill=np.nan):\n    \"\"\"Estimate nucleotide divergence between two populations in windows\n    over a single chromosome/contig.\n\n    Parameters\n    ----------\n\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    ac1 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array for the first population.\n    ac2 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array for the second population.\n    size : int, optional\n        The window size (number of bases).\n    start : int, optional\n        The position at which to start (1-based).\n    stop : int, optional\n        The position at which to stop (1-based).\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    windows : array_like, int, shape (n_windows, 2), optional\n        Manually specify the windows to use as a sequence of (window_start,\n        window_stop) positions, using 1-based coordinates. Overrides the\n        size/start/stop/step parameters.\n    is_accessible : array_like, bool, shape (len(contig),), optional\n        Boolean array indicating accessibility status for all positions in the\n        chromosome/contig.\n    fill : object, optional\n        The value to use where a window is completely inaccessible.\n\n    Returns\n    -------\n\n    Dxy : ndarray, float, shape (n_windows,)\n        Nucleotide divergence in each window.\n    windows : ndarray, int, shape (n_windows, 2)\n        The windows used, as an array of (window_start, window_stop) positions,\n        using 1-based coordinates.\n    n_bases : ndarray, int, shape (n_windows,)\n        Number of (accessible) bases in each window.\n    counts : ndarray, int, shape (n_windows,)\n        Number of variants in each window.\n\n    Examples\n    --------\n\n    Simplest case, two haplotypes in each population::\n\n        >>> import allel\n        >>> h = allel.HaplotypeArray([[0, 0, 0, 0],\n        ...                           [0, 0, 0, 1],\n        ...                           [0, 0, 1, 1],\n        ...                           [0, 1, 1, 1],\n        ...                           [1, 1, 1, 1],\n        ...                           [0, 0, 1, 2],\n        ...                           [0, 1, 1, 2],\n        ...                           [0, 1, -1, -1],\n        ...                           [-1, -1, -1, -1]])\n        >>> ac1 = h.count_alleles(subpop=[0, 1])\n        >>> ac2 = h.count_alleles(subpop=[2, 3])\n        >>> pos = [2, 4, 7, 14, 15, 18, 19, 25, 27]\n        >>> dxy, windows, n_bases, counts = windowed_divergence(\n        ...     pos, ac1, ac2, size=10, start=1, stop=31\n        ... )\n        >>> dxy\n        array([0.15 , 0.225, 0.   ])\n        >>> windows\n        array([[ 1, 10],\n               [11, 20],\n               [21, 31]])\n        >>> n_bases\n        array([10, 10, 11])\n        >>> counts\n        array([3, 4, 2])\n\n    \"\"\"\n\n    # check inputs\n    pos = SortedIndex(pos, copy=False)\n    is_accessible = asarray_ndim(is_accessible, 1, allow_none=True)\n\n    # calculate mean pairwise divergence\n    mpd = mean_pairwise_difference_between(ac1, ac2, fill=0)\n\n    # sum in windows\n    mpd_sum, windows, counts = windowed_statistic(\n        pos, values=mpd, statistic=np.sum, size=size, start=start,\n        stop=stop, step=step, windows=windows, fill=0\n    )\n\n    # calculate value per base\n    dxy, n_bases = per_base(mpd_sum, windows, is_accessible=is_accessible,\n                            fill=fill)\n\n    return dxy, windows, n_bases, counts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef windowed_df(pos, ac1, ac2, size=None, start=None, stop=None, step=None,\n                windows=None, is_accessible=None, fill=np.nan):\n    \"\"\"Calculate the density of fixed differences between two populations in\n    windows over a single chromosome/contig.\n\n    Parameters\n    ----------\n\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    ac1 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array for the first population.\n    ac2 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array for the second population.\n    size : int, optional\n        The window size (number of bases).\n    start : int, optional\n        The position at which to start (1-based).\n    stop : int, optional\n        The position at which to stop (1-based).\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    windows : array_like, int, shape (n_windows, 2), optional\n        Manually specify the windows to use as a sequence of (window_start,\n        window_stop) positions, using 1-based coordinates. Overrides the\n        size/start/stop/step parameters.\n    is_accessible : array_like, bool, shape (len(contig),), optional\n        Boolean array indicating accessibility status for all positions in the\n        chromosome/contig.\n    fill : object, optional\n        The value to use where a window is completely inaccessible.\n\n    Returns\n    -------\n\n    df : ndarray, float, shape (n_windows,)\n        Per-base density of fixed differences in each window.\n    windows : ndarray, int, shape (n_windows, 2)\n        The windows used, as an array of (window_start, window_stop) positions,\n        using 1-based coordinates.\n    n_bases : ndarray, int, shape (n_windows,)\n        Number of (accessible) bases in each window.\n    counts : ndarray, int, shape (n_windows,)\n        Number of variants in each window.\n\n    See Also\n    --------\n\n    allel.model.locate_fixed_differences\n\n    \"\"\"\n\n    # check inputs\n    pos = SortedIndex(pos, copy=False)\n    is_accessible = asarray_ndim(is_accessible, 1, allow_none=True)\n\n    # locate fixed differences\n    loc_df = locate_fixed_differences(ac1, ac2)\n\n    # count number of fixed differences in windows\n    n_df, windows, counts = windowed_statistic(\n        pos, values=loc_df, statistic=np.count_nonzero, size=size, start=start,\n        stop=stop, step=step, windows=windows, fill=0\n    )\n\n    # calculate value per base\n    df, n_bases = per_base(n_df, windows, is_accessible=is_accessible,\n                           fill=fill)\n\n    return df, windows, n_bases, counts", "response": "Calculate the density of fixed differences between two populations in a single key - value basis."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the value of Watterson s estimator over a given region.", "response": "def watterson_theta(pos, ac, start=None, stop=None,\n                    is_accessible=None):\n    \"\"\"Calculate the value of Watterson's estimator over a given region.\n\n    Parameters\n    ----------\n\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    ac : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array.\n    start : int, optional\n        The position at which to start (1-based). Defaults to the first position.\n    stop : int, optional\n        The position at which to stop (1-based). Defaults to the last position.\n    is_accessible : array_like, bool, shape (len(contig),), optional\n        Boolean array indicating accessibility status for all positions in the\n        chromosome/contig.\n\n    Returns\n    -------\n\n    theta_hat_w : float\n        Watterson's estimator (theta hat per base).\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> g = allel.GenotypeArray([[[0, 0], [0, 0]],\n    ...                          [[0, 0], [0, 1]],\n    ...                          [[0, 0], [1, 1]],\n    ...                          [[0, 1], [1, 1]],\n    ...                          [[1, 1], [1, 1]],\n    ...                          [[0, 0], [1, 2]],\n    ...                          [[0, 1], [1, 2]],\n    ...                          [[0, 1], [-1, -1]],\n    ...                          [[-1, -1], [-1, -1]]])\n    >>> ac = g.count_alleles()\n    >>> pos = [2, 4, 7, 14, 15, 18, 19, 25, 27]\n    >>> theta_hat_w = allel.watterson_theta(pos, ac, start=1, stop=31)\n    >>> theta_hat_w\n    0.10557184750733138\n\n    \"\"\"\n\n    # check inputs\n    if not isinstance(pos, SortedIndex):\n        pos = SortedIndex(pos, copy=False)\n    is_accessible = asarray_ndim(is_accessible, 1, allow_none=True)\n    if not hasattr(ac, 'count_segregating'):\n        ac = AlleleCountsArray(ac, copy=False)\n\n    # deal with subregion\n    if start is not None or stop is not None:\n        loc = pos.locate_range(start, stop)\n        pos = pos[loc]\n        ac = ac[loc]\n    if start is None:\n        start = pos[0]\n    if stop is None:\n        stop = pos[-1]\n\n    # count segregating variants\n    S = ac.count_segregating()\n\n    # assume number of chromosomes sampled is constant for all variants\n    n = ac.sum(axis=1).max()\n\n    # (n-1)th harmonic number\n    a1 = np.sum(1 / np.arange(1, n))\n\n    # calculate absolute value\n    theta_hat_w_abs = S / a1\n\n    # calculate value per base\n    if is_accessible is None:\n        n_bases = stop - start + 1\n    else:\n        n_bases = np.count_nonzero(is_accessible[start-1:stop])\n    theta_hat_w = theta_hat_w_abs / n_bases\n\n    return theta_hat_w"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the value of Watterson s estimator in windows over a single species.", "response": "def windowed_watterson_theta(pos, ac, size=None, start=None, stop=None,\n                             step=None, windows=None, is_accessible=None,\n                             fill=np.nan):\n    \"\"\"Calculate the value of Watterson's estimator in windows over a single\n    chromosome/contig.\n\n    Parameters\n    ----------\n\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    ac : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array.\n    size : int, optional\n        The window size (number of bases).\n    start : int, optional\n        The position at which to start (1-based).\n    stop : int, optional\n        The position at which to stop (1-based).\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    windows : array_like, int, shape (n_windows, 2), optional\n        Manually specify the windows to use as a sequence of (window_start,\n        window_stop) positions, using 1-based coordinates. Overrides the\n        size/start/stop/step parameters.\n    is_accessible : array_like, bool, shape (len(contig),), optional\n        Boolean array indicating accessibility status for all positions in the\n        chromosome/contig.\n    fill : object, optional\n        The value to use where a window is completely inaccessible.\n\n    Returns\n    -------\n\n    theta_hat_w : ndarray, float, shape (n_windows,)\n        Watterson's estimator (theta hat per base).\n    windows : ndarray, int, shape (n_windows, 2)\n        The windows used, as an array of (window_start, window_stop) positions,\n        using 1-based coordinates.\n    n_bases : ndarray, int, shape (n_windows,)\n        Number of (accessible) bases in each window.\n    counts : ndarray, int, shape (n_windows,)\n        Number of variants in each window.\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> g = allel.GenotypeArray([[[0, 0], [0, 0]],\n    ...                          [[0, 0], [0, 1]],\n    ...                          [[0, 0], [1, 1]],\n    ...                          [[0, 1], [1, 1]],\n    ...                          [[1, 1], [1, 1]],\n    ...                          [[0, 0], [1, 2]],\n    ...                          [[0, 1], [1, 2]],\n    ...                          [[0, 1], [-1, -1]],\n    ...                          [[-1, -1], [-1, -1]]])\n    >>> ac = g.count_alleles()\n    >>> pos = [2, 4, 7, 14, 15, 18, 19, 25, 27]\n    >>> theta_hat_w, windows, n_bases, counts = allel.windowed_watterson_theta(\n    ...     pos, ac, size=10, start=1, stop=31\n    ... )\n    >>> theta_hat_w\n    array([0.10909091, 0.16363636, 0.04958678])\n    >>> windows\n    array([[ 1, 10],\n           [11, 20],\n           [21, 31]])\n    >>> n_bases\n    array([10, 10, 11])\n    >>> counts\n    array([3, 4, 2])\n\n    \"\"\"  # flake8: noqa\n\n    # check inputs\n    if not isinstance(pos, SortedIndex):\n        pos = SortedIndex(pos, copy=False)\n    is_accessible = asarray_ndim(is_accessible, 1, allow_none=True)\n    if not hasattr(ac, 'count_segregating'):\n        ac = AlleleCountsArray(ac, copy=False)\n\n    # locate segregating variants\n    is_seg = ac.is_segregating()\n\n    # count segregating variants in windows\n    S, windows, counts = windowed_statistic(pos, is_seg,\n                                            statistic=np.count_nonzero,\n                                            size=size, start=start,\n                                            stop=stop, step=step,\n                                            windows=windows, fill=0)\n\n    # assume number of chromosomes sampled is constant for all variants\n    n = ac.sum(axis=1).max()\n\n    # (n-1)th harmonic number\n    a1 = np.sum(1 / np.arange(1, n))\n\n    # absolute value of Watterson's theta\n    theta_hat_w_abs = S / a1\n\n    # theta per base\n    theta_hat_w, n_bases = per_base(theta_hat_w_abs, windows=windows,\n                                    is_accessible=is_accessible, fill=fill)\n\n    return theta_hat_w, windows, n_bases, counts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the value of Tajima s D over a given region.", "response": "def tajima_d(ac, pos=None, start=None, stop=None, min_sites=3):\n    \"\"\"Calculate the value of Tajima's D over a given region.\n\n    Parameters\n    ----------\n    ac : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array.\n    pos : array_like, int, shape (n_items,), optional\n        Variant positions, using 1-based coordinates, in ascending order.\n    start : int, optional\n        The position at which to start (1-based). Defaults to the first position.\n    stop : int, optional\n        The position at which to stop (1-based). Defaults to the last position.\n    min_sites : int, optional\n        Minimum number of segregating sites for which to calculate a value. If\n        there are fewer, np.nan is returned. Defaults to 3.\n\n    Returns\n    -------\n    D : float\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> g = allel.GenotypeArray([[[0, 0], [0, 0]],\n    ...                    [[0, 0], [0, 1]],\n    ...                          [[0, 0], [1, 1]],\n    ...                          [[0, 1], [1, 1]],\n    ...                          [[1, 1], [1, 1]],\n    ...                          [[0, 0], [1, 2]],\n    ...                          [[0, 1], [1, 2]],\n    ...                          [[0, 1], [-1, -1]],\n    ...                          [[-1, -1], [-1, -1]]])\n    >>> ac = g.count_alleles()\n    >>> allel.tajima_d(ac)\n    3.1445848780213814\n    >>> pos = [2, 4, 7, 14, 15, 18, 19, 25, 27]\n    >>> allel.tajima_d(ac, pos=pos, start=7, stop=25)\n    3.8779735196179366\n\n    \"\"\"\n\n    # check inputs\n    if not hasattr(ac, 'count_segregating'):\n        ac = AlleleCountsArray(ac, copy=False)\n\n    # deal with subregion\n    if pos is not None and (start is not None or stop is not None):\n        if not isinstance(pos, SortedIndex):\n            pos = SortedIndex(pos, copy=False)\n        loc = pos.locate_range(start, stop)\n        ac = ac[loc]\n\n    # count segregating variants\n    S = ac.count_segregating()\n    if S < min_sites:\n        return np.nan\n\n    # assume number of chromosomes sampled is constant for all variants\n    n = ac.sum(axis=1).max()\n\n    # (n-1)th harmonic number\n    a1 = np.sum(1 / np.arange(1, n))\n\n    # calculate Watterson's theta (absolute value)\n    theta_hat_w_abs = S / a1\n\n    # calculate mean pairwise difference\n    mpd = mean_pairwise_difference(ac, fill=0)\n\n    # calculate theta_hat pi (sum differences over variants)\n    theta_hat_pi_abs = np.sum(mpd)\n\n    # N.B., both theta estimates are usually divided by the number of\n    # (accessible) bases but here we want the absolute difference\n    d = theta_hat_pi_abs - theta_hat_w_abs\n\n    # calculate the denominator (standard deviation)\n    a2 = np.sum(1 / (np.arange(1, n)**2))\n    b1 = (n + 1) / (3 * (n - 1))\n    b2 = 2 * (n**2 + n + 3) / (9 * n * (n - 1))\n    c1 = b1 - (1 / a1)\n    c2 = b2 - ((n + 2) / (a1 * n)) + (a2 / (a1**2))\n    e1 = c1 / a1\n    e2 = c2 / (a1**2 + a2)\n    d_stdev = np.sqrt((e1 * S) + (e2 * S * (S - 1)))\n\n    # finally calculate Tajima's D\n    D = d / d_stdev\n\n    return D"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef windowed_tajima_d(pos, ac, size=None, start=None, stop=None,\n                      step=None, windows=None, min_sites=3):\n    \"\"\"Calculate the value of Tajima's D in windows over a single\n    chromosome/contig.\n\n    Parameters\n    ----------\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    ac : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array.\n    size : int, optional\n        The window size (number of bases).\n    start : int, optional\n        The position at which to start (1-based).\n    stop : int, optional\n        The position at which to stop (1-based).\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    windows : array_like, int, shape (n_windows, 2), optional\n        Manually specify the windows to use as a sequence of (window_start,\n        window_stop) positions, using 1-based coordinates. Overrides the\n        size/start/stop/step parameters.\n    min_sites : int, optional\n        Minimum number of segregating sites for which to calculate a value. If\n        there are fewer, np.nan is returned. Defaults to 3.\n\n    Returns\n    -------\n    D : ndarray, float, shape (n_windows,)\n        Tajima's D.\n    windows : ndarray, int, shape (n_windows, 2)\n        The windows used, as an array of (window_start, window_stop) positions,\n        using 1-based coordinates.\n    counts : ndarray, int, shape (n_windows,)\n        Number of variants in each window.\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> g = allel.GenotypeArray([[[0, 0], [0, 0]],\n    ...                          [[0, 0], [0, 1]],\n    ...                          [[0, 0], [1, 1]],\n    ...                          [[0, 1], [1, 1]],\n    ...                          [[1, 1], [1, 1]],\n    ...                          [[0, 0], [1, 2]],\n    ...                          [[0, 1], [1, 2]],\n    ...                          [[0, 1], [-1, -1]],\n    ...                          [[-1, -1], [-1, -1]]])\n    >>> ac = g.count_alleles()\n    >>> pos = [2, 4, 7, 14, 15, 20, 22, 25, 27]\n    >>> D, windows, counts = allel.windowed_tajima_d(pos, ac, size=20, step=10, start=1, stop=31)\n    >>> D\n    array([1.36521524, 4.22566622])\n    >>> windows\n    array([[ 1, 20],\n           [11, 31]])\n    >>> counts\n    array([6, 6])\n\n    \"\"\"\n\n    # check inputs\n    if not isinstance(pos, SortedIndex):\n        pos = SortedIndex(pos, copy=False)\n    if not hasattr(ac, 'count_segregating'):\n        ac = AlleleCountsArray(ac, copy=False)\n\n    # assume number of chromosomes sampled is constant for all variants\n    n = ac.sum(axis=1).max()\n\n    # calculate constants\n    a1 = np.sum(1 / np.arange(1, n))\n    a2 = np.sum(1 / (np.arange(1, n)**2))\n    b1 = (n + 1) / (3 * (n - 1))\n    b2 = 2 * (n**2 + n + 3) / (9 * n * (n - 1))\n    c1 = b1 - (1 / a1)\n    c2 = b2 - ((n + 2) / (a1 * n)) + (a2 / (a1**2))\n    e1 = c1 / a1\n    e2 = c2 / (a1**2 + a2)\n\n    # locate segregating variants\n    is_seg = ac.is_segregating()\n\n    # calculate mean pairwise difference\n    mpd = mean_pairwise_difference(ac, fill=0)\n\n    # define statistic to compute for each window\n    # noinspection PyPep8Naming\n    def statistic(w_is_seg, w_mpd):\n        S = np.count_nonzero(w_is_seg)\n        if S < min_sites:\n            return np.nan\n        pi = np.sum(w_mpd)\n        d = pi - (S / a1)\n        d_stdev = np.sqrt((e1 * S) + (e2 * S * (S - 1)))\n        wD = d / d_stdev\n        return wD\n\n    D, windows, counts = windowed_statistic(pos, values=(is_seg, mpd),\n                                            statistic=statistic, size=size,\n                                            start=start, stop=stop, step=step,\n                                            windows=windows, fill=np.nan)\n\n    return D, windows, counts", "response": "Calculate the value of Tajima s D in windows over a single Tajima s chromosome."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the value of Tajima s D in moving windows of size variants.", "response": "def moving_tajima_d(ac, size, start=0, stop=None, step=None, min_sites=3):\n    \"\"\"Calculate the value of Tajima's D in moving windows of `size` variants.\n\n\n    Parameters\n    ----------\n    ac : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array.\n    size : int\n        The window size (number of variants).\n    start : int, optional\n        The index at which to start.\n    stop : int, optional\n        The index at which to stop.\n    step : int, optional\n        The number of variants between start positions of windows. If not\n        given, defaults to the window size, i.e., non-overlapping windows.\n    min_sites : int, optional\n        Minimum number of segregating sites for which to calculate a value. If\n        there are fewer, np.nan is returned. Defaults to 3.\n\n    Returns\n    -------\n    d : ndarray, float, shape (n_windows,)\n        Tajima's D.\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> g = allel.GenotypeArray([[[0, 0], [0, 0]],\n    ...                          [[0, 0], [0, 1]],\n    ...                          [[0, 0], [1, 1]],\n    ...                          [[0, 1], [1, 1]],\n    ...                          [[1, 1], [1, 1]],\n    ...                          [[0, 0], [1, 2]],\n    ...                          [[0, 1], [1, 2]],\n    ...                          [[0, 1], [-1, -1]],\n    ...                          [[-1, -1], [-1, -1]]])\n    >>> ac = g.count_alleles()\n    >>> D = allel.moving_tajima_d(ac, size=4, step=2)\n    >>> D\n    array([0.1676558 , 2.01186954, 5.70029703])\n\n    \"\"\"\n\n    d = moving_statistic(values=ac, statistic=tajima_d, size=size, start=start, stop=stop,\n                         step=step, min_sites=min_sites)\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the likely ROH for a single individual given a genotype vector and position.", "response": "def roh_mhmm(gv, pos, phet_roh=0.001, phet_nonroh=(0.0025, 0.01), transition=1e-6,\n             min_roh=0, is_accessible=None, contig_size=None):\n\n    \"\"\"Call ROH (runs of homozygosity) in a single individual given a genotype vector.\n\n    This function computes the likely ROH using a Multinomial HMM model. There are 3\n    observable states at each position in a chromosome/contig: 0 = Hom, 1 = Het,\n    2 = inaccessible (i.e., unobserved).\n\n    The model is provided with a probability of observing a het in a ROH (`phet_roh`) and one\n    or more probabilities of observing a het in a non-ROH, as this probability may not be\n    constant across the genome (`phet_nonroh`).\n\n    Parameters\n    ----------\n    gv : array_like, int, shape (n_variants, ploidy)\n        Genotype vector.\n    pos: array_like, int, shape (n_variants,)\n        Positions of variants, same 0th dimension as `gv`.\n    phet_roh: float, optional\n        Probability of observing a heterozygote in a ROH. Appropriate values\n        will depend on de novo mutation rate and genotype error rate.\n    phet_nonroh: tuple of floats, optional\n        One or more probabilites of observing a heterozygote outside of ROH.\n        Appropriate values will depend primarily on nucleotide diversity within\n        the population, but also on mutation rate and genotype error rate.\n    transition: float, optional\n        Probability of moving between states.\n    min_roh: integer, optional\n        Minimum size (bp) to condsider as a ROH. Will depend on contig size\n        and recombination rate.\n    is_accessible: array_like, bool, shape (`contig_size`,), optional\n        Boolean array for each position in contig describing whether accessible\n        or not.\n    contig_size: int, optional\n        If is_accessible not known/not provided, allows specification of\n        total length of contig.\n\n    Returns\n    -------\n    df_roh: DataFrame\n        Data frame where each row describes a run of homozygosity. Columns are 'start',\n        'stop', 'length' and 'is_marginal'. Start and stop are 1-based, stop-inclusive.\n    froh: float\n        Proportion of genome in a ROH.\n\n    Notes\n    -----\n    This function requires `hmmlearn <http://hmmlearn.readthedocs.io/en/latest/>`_ to be\n    installed.\n\n    This function currently requires around 4GB memory for a contig size of ~50Mbp.\n\n    \"\"\"\n\n    from hmmlearn import hmm\n\n    # setup inputs\n    if isinstance(phet_nonroh, float):\n        phet_nonroh = phet_nonroh,\n    gv = GenotypeVector(gv)\n    pos = asarray_ndim(pos, 1)\n    check_dim0_aligned(gv, pos)\n    is_accessible = asarray_ndim(is_accessible, 1, dtype=bool)\n\n    # heterozygote probabilities\n    het_px = np.concatenate([(phet_roh,), phet_nonroh])\n\n    # start probabilities (all equal)\n    start_prob = np.repeat(1/het_px.size, het_px.size)\n\n    # transition between underlying states\n    transition_mx = _hmm_derive_transition_matrix(transition, het_px.size)\n\n    # probability of inaccessible\n    if is_accessible is None:\n        if contig_size is None:\n            raise ValueError(\n                \"If is_accessibile argument is not provided, you must provide contig_size\")\n        p_accessible = 1.0\n    else:\n        p_accessible = is_accessible.mean()\n        contig_size = is_accessible.size\n\n    emission_mx = _mhmm_derive_emission_matrix(het_px, p_accessible)\n\n    # initialize HMM\n    roh_hmm = hmm.MultinomialHMM(n_components=het_px.size)\n    roh_hmm.n_symbols_ = 3\n    roh_hmm.startprob_ = start_prob\n    roh_hmm.transmat_ = transition_mx\n    roh_hmm.emissionprob_ = emission_mx\n\n    # locate heterozygous calls\n    is_het = gv.is_het()\n\n    # predict ROH state\n    pred, obs = _mhmm_predict_roh_state(roh_hmm, is_het, pos, is_accessible, contig_size)\n\n    # find ROH windows\n    df_blocks = tabulate_state_blocks(pred, states=list(range(len(het_px))))\n    df_roh = df_blocks[(df_blocks.state == 0)].reset_index(drop=True)\n    # adapt the dataframe for ROH\n    for col in 'state', 'support', 'start_lidx', 'stop_ridx', 'size_max':\n        del df_roh[col]\n    df_roh.rename(columns={'start_ridx': 'start',\n                           'stop_lidx': 'stop',\n                           'size_min': 'length'},\n                  inplace=True)\n    # make coordinates 1-based\n    df_roh['start'] = df_roh['start'] + 1\n    df_roh['stop'] = df_roh['stop'] + 1\n\n    # filter by ROH size\n    if min_roh > 0:\n        df_roh = df_roh[df_roh.length >= min_roh]\n\n    # compute FROH\n    froh = df_roh.length.sum() / contig_size\n\n    return df_roh, froh"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef roh_poissonhmm(gv, pos, phet_roh=0.001, phet_nonroh=(0.0025, 0.01), transition=1e-3,\n                   window_size=1000, min_roh=0, is_accessible=None, contig_size=None):\n\n    \"\"\"Call ROH (runs of homozygosity) in a single individual given a genotype vector.\n\n    This function computes the likely ROH using a Poisson HMM model. The chromosome is divided into\n    equally accessible windows of specified size, then the number of hets observed in each is used\n    to fit a Poisson HMM. Note this is much faster than `roh_mhmm`, but at the cost of some\n    resolution.\n\n    The model is provided with a probability of observing a het in a ROH (`phet_roh`) and one\n    or more probabilities of observing a het in a non-ROH, as this probability may not be\n    constant across the genome (`phet_nonroh`).\n\n    Parameters\n    ----------\n    gv : array_like, int, shape (n_variants, ploidy)\n        Genotype vector.\n    pos: array_like, int, shape (n_variants,)\n        Positions of variants, same 0th dimension as `gv`.\n    phet_roh: float, optional\n        Probability of observing a heterozygote in a ROH. Appropriate values\n        will depend on de novo mutation rate and genotype error rate.\n    phet_nonroh: tuple of floats, optional\n        One or more probabilites of observing a heterozygote outside of ROH.\n        Appropriate values will depend primarily on nucleotide diversity within\n        the population, but also on mutation rate and genotype error rate.\n    transition: float, optional\n        Probability of moving between states. This is based on windows, so a larger window size may\n        call for a larger transitional probability\n    window_size: integer, optional\n        Window size (equally accessible bases) to consider as a potential ROH. Setting this window\n        too small may result in spurious ROH calls, while too large will result in a lack of\n        resolution.\n    min_roh: integer, optional\n        Minimum size (bp) to condsider as a ROH. Will depend on contig size and recombination rate.\n    is_accessible: array_like, bool, shape (`contig_size`,), optional\n        Boolean array for each position in contig describing whether accessible\n        or not. Although optional, highly recommended so invariant sites are distinguishable from\n        sites where variation is inaccessible\n    contig_size: integer, optional\n        If is_accessible is not available, use this to specify the size of the contig, and assume\n        all sites are accessible.\n\n\n    Returns\n    -------\n    df_roh: DataFrame\n        Data frame where each row describes a run of homozygosity. Columns are 'start',\n        'stop', 'length' and 'is_marginal'. Start and stop are 1-based, stop-inclusive.\n    froh: float\n        Proportion of genome in a ROH.\n\n    Notes\n    -----\n    This function requires `pomegranate` (>= 0.9.0) to be installed.\n\n    \"\"\"\n\n    from pomegranate import HiddenMarkovModel, PoissonDistribution\n\n    # equally accessbile windows\n    if is_accessible is None:\n        if contig_size is None:\n            raise ValueError(\n                \"If is_accessibile argument is not provided, you must provide contig_size\")\n        is_accessible = np.ones((contig_size,), dtype=\"bool\")\n    else:\n        contig_size = is_accessible.size\n\n    eqw = equally_accessible_windows(is_accessible, window_size)\n\n    ishet = GenotypeVector(gv).is_het()\n    counts, wins, records = windowed_statistic(pos, ishet, np.sum, windows=eqw)\n\n    # heterozygote probabilities\n    het_px = np.concatenate([(phet_roh,), phet_nonroh])\n\n    # start probabilities (all equal)\n    start_prob = np.repeat(1/het_px.size, het_px.size)\n\n    # transition between underlying states\n    transition_mx = _hmm_derive_transition_matrix(transition, het_px.size)\n\n    dists = [PoissonDistribution(x * window_size) for x in het_px]\n\n    model = HiddenMarkovModel.from_matrix(transition_probabilities=transition_mx,\n                                          distributions=dists,\n                                          starts=start_prob)\n\n    prediction = np.array(model.predict(counts[:, None]))\n\n    df_blocks = tabulate_state_blocks(prediction, states=list(range(len(het_px))))\n    df_roh = df_blocks[(df_blocks.state == 0)].reset_index(drop=True)\n\n    # adapt the dataframe for ROH\n    df_roh[\"start\"] = df_roh.start_ridx.apply(lambda y: eqw[y, 0])\n    df_roh[\"stop\"] = df_roh.stop_lidx.apply(lambda y: eqw[y, 1])\n    df_roh[\"length\"] = df_roh.stop - df_roh.start\n\n    # filter by ROH size\n    if min_roh > 0:\n        df_roh = df_roh[df_roh.length >= min_roh]\n\n    # compute FROH\n    froh = df_roh.length.sum() / contig_size\n\n    return df_roh[[\"start\", \"stop\", \"length\", \"is_marginal\"]], froh", "response": "This function computes the likely ROH for a single individual given a genotype vector and pos."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the site frequency spectrum given derived allele counts at a set of biallelic variants.", "response": "def sfs(dac, n=None):\n    \"\"\"Compute the site frequency spectrum given derived allele counts at\n    a set of biallelic variants.\n\n    Parameters\n    ----------\n    dac : array_like, int, shape (n_variants,)\n        Array of derived allele counts.\n    n : int, optional\n        The total number of chromosomes called.\n\n    Returns\n    -------\n    sfs : ndarray, int, shape (n_chromosomes,)\n        Array where the kth element is the number of variant sites with k\n        derived alleles.\n\n    \"\"\"\n\n    # check input\n    dac, n = _check_dac_n(dac, n)\n\n    # need platform integer for bincount\n    dac = dac.astype(int, copy=False)\n\n    # compute site frequency spectrum\n    x = n + 1\n    s = np.bincount(dac, minlength=x)\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sfs_folded(ac, n=None):\n\n    # check input\n    ac, n = _check_ac_n(ac, n)\n\n    # compute minor allele counts\n    mac = np.amin(ac, axis=1)\n\n    # need platform integer for bincount\n    mac = mac.astype(int, copy=False)\n\n    # compute folded site frequency spectrum\n    x = n//2 + 1\n    s = np.bincount(mac, minlength=x)\n\n    return s", "response": "Compute the folded site frequency spectrum given a reference and an alternate allele count array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the site frequency spectrum scaled such that a constant value isis expected across the spectrum for neutral variation and constant population size.", "response": "def sfs_scaled(dac, n=None):\n    \"\"\"Compute the site frequency spectrum scaled such that a constant value is\n    expected across the spectrum for neutral variation and constant\n    population size.\n\n    Parameters\n    ----------\n    dac : array_like, int, shape (n_variants,)\n        Array of derived allele counts.\n    n : int, optional\n        The total number of chromosomes called.\n\n    Returns\n    -------\n    sfs_scaled : ndarray, int, shape (n_chromosomes,)\n        An array where the value of the kth element is the number of variants\n        with k derived alleles, multiplied by k.\n\n    \"\"\"\n\n    # compute site frequency spectrum\n    s = sfs(dac, n=n)\n\n    # apply scaling\n    s = scale_sfs(s)\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscaling a site frequency spectrum.", "response": "def scale_sfs(s):\n    \"\"\"Scale a site frequency spectrum.\n\n    Parameters\n    ----------\n    s : array_like, int, shape (n_chromosomes,)\n        Site frequency spectrum.\n\n    Returns\n    -------\n    sfs_scaled : ndarray, int, shape (n_chromosomes,)\n        Scaled site frequency spectrum.\n\n    \"\"\"\n    k = np.arange(s.size)\n    out = s * k\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sfs_folded_scaled(ac, n=None):\n\n    # check input\n    ac, n = _check_ac_n(ac, n)\n\n    # compute the site frequency spectrum\n    s = sfs_folded(ac, n=n)\n\n    # apply scaling\n    s = scale_sfs_folded(s, n)\n\n    return s", "response": "Compute the folded site frequency spectrum scaled by a constant\n    value is expected across the spectrum for neutral variation and constant\n    population size."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scale_sfs_folded(s, n):\n    k = np.arange(s.shape[0])\n    out = s * k * (n - k) / n\n    return out", "response": "Scale a folded site frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef joint_sfs(dac1, dac2, n1=None, n2=None):\n\n    # check inputs\n    dac1, n1 = _check_dac_n(dac1, n1)\n    dac2, n2 = _check_dac_n(dac2, n2)\n\n    # compute site frequency spectrum\n    x = n1 + 1\n    y = n2 + 1\n    # need platform integer for bincount\n    tmp = (dac1 * y + dac2).astype(int, copy=False)\n    s = np.bincount(tmp)\n    s.resize(x, y)\n    return s", "response": "Compute the joint site frequency spectrum between two populations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef joint_sfs_folded(ac1, ac2, n1=None, n2=None):\n\n    # check inputs\n    ac1, n1 = _check_ac_n(ac1, n1)\n    ac2, n2 = _check_ac_n(ac2, n2)\n\n    # compute minor allele counts\n    mac1 = np.amin(ac1, axis=1)\n    mac2 = np.amin(ac2, axis=1)\n\n    # compute site frequency spectrum\n    x = n1//2 + 1\n    y = n2//2 + 1\n    tmp = (mac1 * y + mac2).astype(int, copy=False)\n    s = np.bincount(tmp)\n    s.resize(x, y)\n    return s", "response": "Compute the joint folded site frequency spectrum between two populations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef joint_sfs_scaled(dac1, dac2, n1=None, n2=None):\n\n    # compute site frequency spectrum\n    s = joint_sfs(dac1, dac2, n1=n1, n2=n2)\n\n    # apply scaling\n    s = scale_joint_sfs(s)\n\n    return s", "response": "Compute the joint site frequency spectrum between two populations and return the scaled frequency of the joint site frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scale_joint_sfs(s):\n\n    i = np.arange(s.shape[0])[:, None]\n    j = np.arange(s.shape[1])[None, :]\n    out = (s * i) * j\n    return out", "response": "Scale a joint site frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the joint folded site frequency spectrum between two populations scaled by a constant value is expected across the unrelated spectrum for neutral variation constant population size and unrelated spectrum.", "response": "def joint_sfs_folded_scaled(ac1, ac2, n1=None, n2=None):\n    \"\"\"Compute the joint folded site frequency spectrum between two\n    populations, scaled such that a constant value is expected across the\n    spectrum for neutral variation, constant population size and unrelated\n    populations.\n\n    Parameters\n    ----------\n    ac1 : array_like, int, shape (n_variants, 2)\n        Allele counts for the first population.\n    ac2 : array_like, int, shape (n_variants, 2)\n        Allele counts for the second population.\n    n1, n2 : int, optional\n        The total number of chromosomes called in each population.\n\n    Returns\n    -------\n    joint_sfs_folded_scaled : ndarray, int, shape (n1//2 + 1, n2//2 + 1)\n        Array where the (i, j)th element is the scaled frequency of variant\n        sites with a minor allele count of i in the first population and j\n        in the second population.\n\n    \"\"\"  # noqa\n\n    # check inputs\n    ac1, n1 = _check_ac_n(ac1, n1)\n    ac2, n2 = _check_ac_n(ac2, n2)\n\n    # compute site frequency spectrum\n    s = joint_sfs_folded(ac1, ac2, n1=n1, n2=n2)\n\n    # apply scaling\n    s = scale_joint_sfs_folded(s, n1, n2)\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scale_joint_sfs_folded(s, n1, n2):\n    out = np.empty_like(s)\n    for i in range(s.shape[0]):\n        for j in range(s.shape[1]):\n            out[i, j] = s[i, j] * i * j * (n1 - i) * (n2 - j)\n    return out", "response": "Scale a folded joint site frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fold_sfs(s, n):\n\n    # check inputs\n    s = asarray_ndim(s, 1)\n    assert s.shape[0] <= n + 1, 'invalid number of chromosomes'\n\n    # need to check s has all entries up to n\n    if s.shape[0] < n + 1:\n        sn = np.zeros(n + 1, dtype=s.dtype)\n        sn[:s.shape[0]] = s\n        s = sn\n\n    # fold\n    nf = (n + 1) // 2\n    n = nf * 2\n    o = s[:nf] + s[nf:n][::-1]\n\n    return o", "response": "Fold a site frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfolds a joint site frequency spectrum.", "response": "def fold_joint_sfs(s, n1, n2):\n    \"\"\"Fold a joint site frequency spectrum.\n\n    Parameters\n    ----------\n    s : array_like, int, shape (m_chromosomes, n_chromosomes)\n        Joint site frequency spectrum.\n    n1, n2 : int, optional\n        The total number of chromosomes called in each population.\n\n    Returns\n    -------\n    joint_sfs_folded : ndarray, int\n        Folded joint site frequency spectrum.\n\n    \"\"\"\n\n    # check inputs\n    s = asarray_ndim(s, 2)\n    assert s.shape[0] <= n1 + 1, 'invalid number of chromosomes'\n    assert s.shape[1] <= n2 + 1, 'invalid number of chromosomes'\n\n    # need to check s has all entries up to m\n    if s.shape[0] < n1 + 1:\n        sm = np.zeros((n1 + 1, s.shape[1]), dtype=s.dtype)\n        sm[:s.shape[0]] = s\n        s = sm\n\n    # need to check s has all entries up to n\n    if s.shape[1] < n2 + 1:\n        sn = np.zeros((s.shape[0], n2 + 1), dtype=s.dtype)\n        sn[:, :s.shape[1]] = s\n        s = sn\n\n    # fold\n    mf = (n1 + 1) // 2\n    nf = (n2 + 1) // 2\n    n1 = mf * 2\n    n2 = nf * 2\n    o = (s[:mf, :nf] +  # top left\n         s[mf:n1, :nf][::-1] +  # top right\n         s[:mf, nf:n2][:, ::-1] +  # bottom left\n         s[mf:n1, nf:n2][::-1, ::-1])  # bottom right\n\n    return o"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_sfs(s, yscale='log', bins=None, n=None,\n             clip_endpoints=True, label=None, plot_kwargs=None,\n             ax=None):\n    \"\"\"Plot a site frequency spectrum.\n\n    Parameters\n    ----------\n    s : array_like, int, shape (n_chromosomes,)\n        Site frequency spectrum.\n    yscale : string, optional\n        Y axis scale.\n    bins : int or array_like, int, optional\n        Allele count bins.\n    n : int, optional\n        Number of chromosomes sampled. If provided, X axis will be plotted\n        as allele frequency, otherwise as allele count.\n    clip_endpoints : bool, optional\n        If True, do not plot first and last values from frequency spectrum.\n    label : string, optional\n        Label for data series in plot.\n    plot_kwargs : dict-like\n        Additional keyword arguments, passed through to ax.plot().\n    ax : axes, optional\n        Axes on which to draw. If not provided, a new figure will be created.\n\n    Returns\n    -------\n    ax : axes\n        The axes on which the plot was drawn.\n\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n    import scipy\n\n    # check inputs\n    s = asarray_ndim(s, 1)\n\n    # setup axes\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # setup data\n    if bins is None:\n        if clip_endpoints:\n            x = np.arange(1, s.shape[0]-1)\n            y = s[1:-1]\n        else:\n            x = np.arange(s.shape[0])\n            y = s\n    else:\n        if clip_endpoints:\n            y, b, _ = scipy.stats.binned_statistic(\n                np.arange(1, s.shape[0]-1),\n                values=s[1:-1],\n                bins=bins,\n                statistic='sum')\n        else:\n            y, b, _ = scipy.stats.binned_statistic(\n                np.arange(s.shape[0]),\n                values=s,\n                bins=bins,\n                statistic='sum')\n        # use bin midpoints for plotting\n        x = (b[:-1] + b[1:]) / 2\n\n    if n:\n        # convert allele counts to allele frequencies\n        x = x / n\n        ax.set_xlabel('derived allele frequency')\n    else:\n        ax.set_xlabel('derived allele count')\n\n    # do plotting\n    if plot_kwargs is None:\n        plot_kwargs = dict()\n    ax.plot(x, y, label=label, **plot_kwargs)\n\n    # tidy\n    ax.set_yscale(yscale)\n    ax.set_ylabel('site frequency')\n    ax.autoscale(axis='x', tight=True)\n\n    return ax", "response": "Plots a site frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_sfs_folded(*args, **kwargs):\n\n    ax = plot_sfs(*args, **kwargs)\n    n = kwargs.get('n', None)\n    if n:\n        ax.set_xlabel('minor allele frequency')\n    else:\n        ax.set_xlabel('minor allele count')\n    return ax", "response": "Plot a folded site frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_sfs_scaled(*args, **kwargs):\n    kwargs.setdefault('yscale', 'linear')\n    ax = plot_sfs(*args, **kwargs)\n    ax.set_ylabel('scaled site frequency')\n    return ax", "response": "Plot a scaled site frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_sfs_folded_scaled(*args, **kwargs):\n    kwargs.setdefault('yscale', 'linear')\n    ax = plot_sfs_folded(*args, **kwargs)\n    ax.set_ylabel('scaled site frequency')\n    n = kwargs.get('n', None)\n    if n:\n        ax.set_xlabel('minor allele frequency')\n    else:\n        ax.set_xlabel('minor allele count')\n    return ax", "response": "Plots a folded scaled site frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots a joint site frequency spectrum.", "response": "def plot_joint_sfs(s, ax=None, imshow_kwargs=None):\n    \"\"\"Plot a joint site frequency spectrum.\n\n    Parameters\n    ----------\n    s : array_like, int, shape (n_chromosomes_pop1, n_chromosomes_pop2)\n        Joint site frequency spectrum.\n    ax : axes, optional\n        Axes on which to draw. If not provided, a new figure will be created.\n    imshow_kwargs : dict-like\n        Additional keyword arguments, passed through to ax.imshow().\n\n    Returns\n    -------\n    ax : axes\n        The axes on which the plot was drawn.\n\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n    from matplotlib.colors import LogNorm\n\n    # check inputs\n    s = asarray_ndim(s, 2)\n\n    # setup axes\n    if ax is None:\n        w = plt.rcParams['figure.figsize'][0]\n        fig, ax = plt.subplots(figsize=(w, w))\n\n    # set plotting defaults\n    if imshow_kwargs is None:\n        imshow_kwargs = dict()\n    imshow_kwargs.setdefault('cmap', 'jet')\n    imshow_kwargs.setdefault('interpolation', 'none')\n    imshow_kwargs.setdefault('aspect', 'auto')\n    imshow_kwargs.setdefault('norm', LogNorm())\n\n    # plot data\n    ax.imshow(s.T, **imshow_kwargs)\n\n    # tidy\n    ax.invert_yaxis()\n    ax.set_xlabel('derived allele count (population 1)')\n    ax.set_ylabel('derived allele count (population 2)')\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots a joint site frequency spectrum folded in the order of the population 1 and 2.", "response": "def plot_joint_sfs_folded(*args, **kwargs):\n    \"\"\"Plot a joint site frequency spectrum.\n\n    Parameters\n    ----------\n    s : array_like, int, shape (n_chromosomes_pop1/2, n_chromosomes_pop2/2)\n        Joint site frequency spectrum.\n    ax : axes, optional\n        Axes on which to draw. If not provided, a new figure will be created.\n    imshow_kwargs : dict-like\n        Additional keyword arguments, passed through to ax.imshow().\n\n    Returns\n    -------\n    ax : axes\n        The axes on which the plot was drawn.\n\n    \"\"\"\n    ax = plot_joint_sfs(*args, **kwargs)\n    ax.set_xlabel('minor allele count (population 1)')\n    ax.set_ylabel('minor allele count (population 2)')\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_joint_sfs_scaled(*args, **kwargs):\n    imshow_kwargs = kwargs.get('imshow_kwargs', dict())\n    imshow_kwargs.setdefault('norm', None)\n    kwargs['imshow_kwargs'] = imshow_kwargs\n    ax = plot_joint_sfs(*args, **kwargs)\n    return ax", "response": "Plot a scaled joint site frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a scaled folded joint site frequency spectrum.", "response": "def plot_joint_sfs_folded_scaled(*args, **kwargs):\n    \"\"\"Plot a scaled folded joint site frequency spectrum.\n\n    Parameters\n    ----------\n    s : array_like, int, shape (n_chromosomes_pop1/2, n_chromosomes_pop2/2)\n        Joint site frequency spectrum.\n    ax : axes, optional\n        Axes on which to draw. If not provided, a new figure will be created.\n    imshow_kwargs : dict-like\n        Additional keyword arguments, passed through to ax.imshow().\n\n    Returns\n    -------\n    ax : axes\n        The axes on which the plot was drawn.\n\n    \"\"\"\n    imshow_kwargs = kwargs.get('imshow_kwargs', dict())\n    imshow_kwargs.setdefault('norm', None)\n    kwargs['imshow_kwargs'] = imshow_kwargs\n    ax = plot_joint_sfs_folded(*args, **kwargs)\n    ax.set_xlabel('minor allele count (population 1)')\n    ax.set_ylabel('minor allele count (population 2)')\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef memoryview_safe(x):\n    if not x.flags.writeable:\n        if not x.flags.owndata:\n            x = x.copy(order='A')\n        x.setflags(write=True)\n    return x", "response": "Make array safe to run in a Cython memoryview - based kernel."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prep_fields_param(fields):\n\n    store_samples = False\n\n    if fields is None:\n        # add samples by default\n        return True, None\n\n    if isinstance(fields, str):\n        fields = [fields]\n    else:\n        fields = list(fields)\n\n    if 'samples' in fields:\n        fields.remove('samples')\n        store_samples = True\n    elif '*' in fields:\n        store_samples = True\n\n    return store_samples, fields", "response": "Prepare the fields parameter and determine whether or not to store samples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping a chunk iterator for progress logging.", "response": "def _chunk_iter_progress(it, log, prefix):\n    \"\"\"Wrap a chunk iterator for progress logging.\"\"\"\n    n_variants = 0\n    before_all = time.time()\n    before_chunk = before_all\n    for chunk, chunk_length, chrom, pos in it:\n        after_chunk = time.time()\n        elapsed_chunk = after_chunk - before_chunk\n        elapsed = after_chunk - before_all\n        n_variants += chunk_length\n        chrom = text_type(chrom, 'utf8')\n        message = (\n            '%s %s rows in %.2fs; chunk in %.2fs (%s rows/s)' %\n            (prefix, n_variants, elapsed, elapsed_chunk,\n             int(chunk_length // elapsed_chunk))\n        )\n        if chrom:\n            message += '; %s:%s' % (chrom, pos)\n        print(message, file=log)\n        log.flush()\n        yield chunk, chunk_length, chrom, pos\n        before_chunk = after_chunk\n    after_all = time.time()\n    elapsed = after_all - before_all\n    print('%s all done (%s rows/s)' %\n          (prefix, int(n_variants // elapsed)), file=log)\n    log.flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_vcf(input,\n             fields=None,\n             exclude_fields=None,\n             rename_fields=None,\n             types=None,\n             numbers=None,\n             alt_number=DEFAULT_ALT_NUMBER,\n             fills=None,\n             region=None,\n             tabix='tabix',\n             samples=None,\n             transformers=None,\n             buffer_size=DEFAULT_BUFFER_SIZE,\n             chunk_length=DEFAULT_CHUNK_LENGTH,\n             log=None):\n    \"\"\"Read data from a VCF file into NumPy arrays.\n\n    .. versionchanged:: 1.12.0\n        Now returns None if no variants are found in the VCF file or matching the\n        requested region.\n\n    Parameters\n    ----------\n    input : string or file-like\n        {input}\n    fields : list of strings, optional\n        {fields}\n    exclude_fields : list of strings, optional\n        {exclude_fields}\n    rename_fields : dict[str -> str], optional\n        {rename_fields}\n    types : dict, optional\n        {types}\n    numbers : dict, optional\n        {numbers}\n    alt_number : int, optional\n        {alt_number}\n    fills : dict, optional\n        {fills}\n    region : string, optional\n        {region}\n    tabix : string, optional\n        {tabix}\n    samples : list of strings\n        {samples}\n    transformers : list of transformer objects, optional\n        {transformers}\n    buffer_size : int, optional\n        {buffer_size}\n    chunk_length : int, optional\n        {chunk_length}\n    log : file-like, optional\n        {log}\n\n    Returns\n    -------\n    data : dict[str, ndarray]\n        A dictionary holding arrays, or None if no variants were found.\n\n    \"\"\"\n\n    # samples requested?\n    # noinspection PyTypeChecker\n    store_samples, fields = _prep_fields_param(fields)\n\n    # setup\n    fields, samples, headers, it = iter_vcf_chunks(\n        input=input, fields=fields, exclude_fields=exclude_fields, types=types,\n        numbers=numbers, alt_number=alt_number, buffer_size=buffer_size,\n        chunk_length=chunk_length, fills=fills, region=region, tabix=tabix,\n        samples=samples, transformers=transformers\n    )\n\n    # handle field renaming\n    if rename_fields:\n        rename_fields, it = _do_rename(it, fields=fields,\n                                       rename_fields=rename_fields,\n                                       headers=headers)\n\n    # setup progress logging\n    if log is not None:\n        it = _chunk_iter_progress(it, log, prefix='[read_vcf]')\n\n    # read all chunks into a list\n    chunks = [d[0] for d in it]\n\n    if chunks:\n\n        # setup output\n        output = dict()\n\n        if len(samples) > 0 and store_samples:\n            output['samples'] = samples\n\n        # find array keys\n        keys = sorted(chunks[0].keys())\n\n        # concatenate chunks\n        for k in keys:\n            output[k] = np.concatenate([chunk[k] for chunk in chunks], axis=0)\n\n    else:\n\n        output = None\n\n    return output", "response": "Read data from a VCF file into NumPy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads data from a VCF file into NumPy arrays and save as a. npz file.", "response": "def vcf_to_npz(input, output,\n               compressed=True,\n               overwrite=False,\n               fields=None,\n               exclude_fields=None,\n               rename_fields=None,\n               types=None,\n               numbers=None,\n               alt_number=DEFAULT_ALT_NUMBER,\n               fills=None,\n               region=None,\n               tabix=True,\n               samples=None,\n               transformers=None,\n               buffer_size=DEFAULT_BUFFER_SIZE,\n               chunk_length=DEFAULT_CHUNK_LENGTH,\n               log=None):\n    \"\"\"Read data from a VCF file into NumPy arrays and save as a .npz file.\n\n    .. versionchanged:: 1.12.0\n        Now will not create any output file if no variants are found in the VCF file or\n        matching the requested region.\n\n    Parameters\n    ----------\n    input : string\n        {input}\n    output : string\n        {output}\n    compressed : bool, optional\n        If True (default), save with compression.\n    overwrite : bool, optional\n        {overwrite}\n    fields : list of strings, optional\n        {fields}\n    exclude_fields : list of strings, optional\n        {exclude_fields}\n    rename_fields : dict[str -> str], optional\n        {rename_fields}\n    types : dict, optional\n        {types}\n    numbers : dict, optional\n        {numbers}\n    alt_number : int, optional\n        {alt_number}\n    fills : dict, optional\n        {fills}\n    region : string, optional\n        {region}\n    tabix : string, optional\n        {tabix}\n    samples : list of strings\n        {samples}\n    transformers : list of transformer objects, optional\n        {transformers}\n    buffer_size : int, optional\n        {buffer_size}\n    chunk_length : int, optional\n        {chunk_length}\n    log : file-like, optional\n        {log}\n\n    \"\"\"\n\n    # guard condition\n    if not overwrite and os.path.exists(output):\n        raise ValueError('file exists at path %r; use overwrite=True to replace' % output)\n\n    # read all data into memory\n    data = read_vcf(\n        input=input, fields=fields, exclude_fields=exclude_fields,\n        rename_fields=rename_fields, types=types, numbers=numbers,\n        alt_number=alt_number, buffer_size=buffer_size, chunk_length=chunk_length,\n        log=log, fills=fills, region=region, tabix=tabix, samples=samples,\n        transformers=transformers\n    )\n\n    if data is None:\n        # no data, bail out\n        return\n\n    # setup save function\n    if compressed:\n        savez = np.savez_compressed\n    else:\n        savez = np.savez\n\n    # save as npz\n    savez(output, **data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads data from a VCF file and store it into an HDF5 file.", "response": "def vcf_to_hdf5(input, output,\n                group='/',\n                compression='gzip',\n                compression_opts=1,\n                shuffle=False,\n                overwrite=False,\n                vlen=True,\n                fields=None,\n                exclude_fields=None,\n                rename_fields=None,\n                types=None,\n                numbers=None,\n                alt_number=DEFAULT_ALT_NUMBER,\n                fills=None,\n                region=None,\n                tabix='tabix',\n                samples=None,\n                transformers=None,\n                buffer_size=DEFAULT_BUFFER_SIZE,\n                chunk_length=DEFAULT_CHUNK_LENGTH,\n                chunk_width=DEFAULT_CHUNK_WIDTH,\n                log=None):\n    \"\"\"Read data from a VCF file and load into an HDF5 file.\n\n    .. versionchanged:: 1.12.0\n        Now will not create any output file if no variants are found in the VCF file or\n        matching the requested region.\n\n    Parameters\n    ----------\n    input : string\n        {input}\n    output : string\n        {output}\n    group : string\n        Group within destination HDF5 file to store data in.\n    compression : string\n        Compression algorithm, e.g., 'gzip' (default).\n    compression_opts : int\n        Compression level, e.g., 1 (default).\n    shuffle : bool\n        Use byte shuffling, which may improve compression (default is False).\n    overwrite : bool\n        {overwrite}\n    vlen : bool\n        If True, store variable length strings. Note that there is considerable storage\n        overhead for variable length strings in HDF5, and leaving this option as True (\n        default) may lead to large file sizes. If False, all strings will be stored in\n        the HDF5 file as fixed length strings, even if they are specified as 'object'\n        type. In this case, the string length for any field with 'object' type will be\n        determined based on the maximum length of strings found in the first chunk,\n        and this may cause values to be truncated if longer values are found in later\n        chunks. To avoid truncation and large file sizes, manually set the type for all\n        string fields to an explicit fixed length string type, e.g., 'S10' for a field\n        where you know at most 10 characters are required.\n    fields : list of strings, optional\n        {fields}\n    exclude_fields : list of strings, optional\n        {exclude_fields}\n    rename_fields : dict[str -> str], optional\n        {rename_fields}\n    types : dict, optional\n        {types}\n    numbers : dict, optional\n        {numbers}\n    alt_number : int, optional\n        {alt_number}\n    fills : dict, optional\n        {fills}\n    region : string, optional\n        {region}\n    tabix : string, optional\n        {tabix}\n    samples : list of strings\n        {samples}\n    transformers : list of transformer objects, optional\n        {transformers}\n    buffer_size : int, optional\n        {buffer_size}\n    chunk_length : int, optional\n        {chunk_length}\n    chunk_width : int, optional\n        {chunk_width}\n    log : file-like, optional\n        {log}\n\n    \"\"\"\n\n    import h5py\n\n    # samples requested?\n    # noinspection PyTypeChecker\n    store_samples, fields = _prep_fields_param(fields)\n\n    # setup chunk iterator\n    fields, samples, headers, it = iter_vcf_chunks(\n        input, fields=fields, exclude_fields=exclude_fields, types=types,\n        numbers=numbers, alt_number=alt_number, buffer_size=buffer_size,\n        chunk_length=chunk_length, fills=fills, region=region, tabix=tabix,\n        samples=samples, transformers=transformers\n    )\n\n    # handle field renaming\n    if rename_fields:\n        rename_fields, it = _do_rename(it, fields=fields,\n                                       rename_fields=rename_fields,\n                                       headers=headers)\n\n    # setup progress logging\n    if log is not None:\n        it = _chunk_iter_progress(it, log, prefix='[vcf_to_hdf5]')\n\n    # read first chunk\n    try:\n        chunk, _, _, _ = next(it)\n    except StopIteration:\n        # no data, bail out\n        return\n\n    with h5py.File(output, mode='a') as h5f:\n\n        # obtain root group that data will be stored into\n        root = h5f.require_group(group)\n\n        if len(samples) > 0 and store_samples:\n            # store samples\n            name = 'samples'\n            if name in root:\n                if overwrite:\n                    del root[name]\n                else:\n                    raise ValueError(\n                        'dataset exists at path %r; use overwrite=True to replace' % name)\n            if samples.dtype.kind == 'O':\n                if vlen:\n                    t = h5py.special_dtype(vlen=str)\n                else:\n                    samples = samples.astype('S')\n                    t = samples.dtype\n            else:\n                t = samples.dtype\n            root.create_dataset(name, data=samples, chunks=None, dtype=t)\n\n        # setup datasets\n        # noinspection PyTypeChecker\n        keys = _hdf5_setup_datasets(\n            chunk=chunk, root=root, chunk_length=chunk_length, chunk_width=chunk_width,\n            compression=compression, compression_opts=compression_opts, shuffle=shuffle,\n            overwrite=overwrite, headers=headers, vlen=vlen\n        )\n\n        # store first chunk\n        _hdf5_store_chunk(root, keys, chunk, vlen)\n\n        # store remaining chunks\n        for chunk, _, _, _ in it:\n\n            _hdf5_store_chunk(root, keys, chunk, vlen)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vcf_to_zarr(input, output,\n                group='/',\n                compressor='default',\n                overwrite=False,\n                fields=None,\n                exclude_fields=None,\n                rename_fields=None,\n                types=None,\n                numbers=None,\n                alt_number=DEFAULT_ALT_NUMBER,\n                fills=None,\n                region=None,\n                tabix='tabix',\n                samples=None,\n                transformers=None,\n                buffer_size=DEFAULT_BUFFER_SIZE,\n                chunk_length=DEFAULT_CHUNK_LENGTH,\n                chunk_width=DEFAULT_CHUNK_WIDTH,\n                log=None):\n    \"\"\"Read data from a VCF file and load into a Zarr on-disk store.\n\n    .. versionchanged:: 1.12.0\n        Now will not create any output files if no variants are found in the VCF file or\n        matching the requested region.\n\n    Parameters\n    ----------\n    input : string\n        {input}\n    output : string\n        {output}\n    group : string\n        Group within destination Zarr hierarchy to store data in.\n    compressor : compressor\n        Compression algorithm, e.g., zarr.Blosc(cname='zstd', clevel=1, shuffle=1).\n    overwrite : bool\n        {overwrite}\n    fields : list of strings, optional\n        {fields}\n    exclude_fields : list of strings, optional\n        {exclude_fields}\n    rename_fields : dict[str -> str], optional\n        {rename_fields}\n    types : dict, optional\n        {types}\n    numbers : dict, optional\n        {numbers}\n    alt_number : int, optional\n        {alt_number}\n    fills : dict, optional\n        {fills}\n    region : string, optional\n        {region}\n    tabix : string, optional\n        {tabix}\n    samples : list of strings\n        {samples}\n    transformers : list of transformer objects, optional\n        {transformers}\n    buffer_size : int, optional\n        {buffer_size}\n    chunk_length : int, optional\n        {chunk_length}\n    chunk_width : int, optional\n        {chunk_width}\n    log : file-like, optional\n        {log}\n\n    \"\"\"\n\n    import zarr\n\n    # samples requested?\n    # noinspection PyTypeChecker\n    store_samples, fields = _prep_fields_param(fields)\n\n    # setup chunk iterator\n    fields, samples, headers, it = iter_vcf_chunks(\n        input, fields=fields, exclude_fields=exclude_fields, types=types,\n        numbers=numbers, alt_number=alt_number, buffer_size=buffer_size,\n        chunk_length=chunk_length, fills=fills, region=region, tabix=tabix,\n        samples=samples, transformers=transformers\n    )\n\n    # handle field renaming\n    if rename_fields:\n        rename_fields, it = _do_rename(it, fields=fields,\n                                       rename_fields=rename_fields,\n                                       headers=headers)\n\n    # check for any case-insensitive duplicate fields\n    # https://github.com/cggh/scikit-allel/issues/215\n    ci_field_index = defaultdict(list)\n    for f in fields:\n        if rename_fields:\n            f = rename_fields.get(f, f)\n        ci_field_index[f.lower()].append(f)\n    for k, v in ci_field_index.items():\n        if len(v) > 1:\n            msg = textwrap.fill(\n                'Found two or more fields with the same name when compared '\n                'case-insensitive: {!r}; this is not supported because it causes '\n                'problems on platforms with a case-insensitive file system, which is '\n                'usually the default on Windows and Mac OS. Please rename fields so they '\n                'are distinct under a case-insensitive comparison via the '\n                'rename_fields argument.'.format(v), width=80)\n            raise ValueError(msg)\n\n    # setup progress logging\n    if log is not None:\n        it = _chunk_iter_progress(it, log, prefix='[vcf_to_zarr]')\n\n    # read first chunk\n    try:\n        chunk, _, _, _ = next(it)\n    except StopIteration:\n        # no data, bail out\n        return\n\n    # open root group\n    root = zarr.open_group(output, mode='a', path=group)\n\n    if len(samples) > 0 and store_samples:\n        # store samples\n        if samples.dtype.kind == 'O':\n            if PY2:\n                dtype = 'unicode'\n            else:\n                dtype = 'str'\n        else:\n            dtype = samples.dtype\n        root.create_dataset('samples', data=samples, compressor=None, overwrite=overwrite,\n                            dtype=dtype)\n\n    # setup datasets\n    # noinspection PyTypeChecker\n    keys = _zarr_setup_datasets(\n        chunk, root=root, chunk_length=chunk_length, chunk_width=chunk_width,\n        compressor=compressor, overwrite=overwrite, headers=headers\n    )\n\n    # store first chunk\n    _zarr_store_chunk(root, keys, chunk)\n\n    # store remaining chunks\n    for chunk, _, _, _ in it:\n\n        _zarr_store_chunk(root, keys, chunk)", "response": "Read data from a VCF file and load into a Zarr on - disk store."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_vcf_chunks(input,\n                    fields=None,\n                    exclude_fields=None,\n                    types=None,\n                    numbers=None,\n                    alt_number=DEFAULT_ALT_NUMBER,\n                    fills=None,\n                    region=None,\n                    tabix='tabix',\n                    samples=None,\n                    transformers=None,\n                    buffer_size=DEFAULT_BUFFER_SIZE,\n                    chunk_length=DEFAULT_CHUNK_LENGTH):\n    \"\"\"Iterate over chunks of data from a VCF file as NumPy arrays.\n\n    Parameters\n    ----------\n    input : string\n        {input}\n    fields : list of strings, optional\n        {fields}\n    exclude_fields : list of strings, optional\n        {exclude_fields}\n    types : dict, optional\n        {types}\n    numbers : dict, optional\n        {numbers}\n    alt_number : int, optional\n        {alt_number}\n    fills : dict, optional\n        {fills}\n    region : string, optional\n        {region}\n    tabix : string, optional\n        {tabix}\n    samples : list of strings\n        {samples}\n    transformers : list of transformer objects, optional\n        {transformers}\n    buffer_size : int, optional\n        {buffer_size}\n    chunk_length : int, optional\n        {chunk_length}\n\n    Returns\n    -------\n    fields : list of strings\n        Normalised names of fields that will be extracted.\n    samples : ndarray\n        Samples for which data will be extracted.\n    headers : VCFHeaders\n        Tuple of metadata extracted from VCF headers.\n    it : iterator\n        Chunk iterator.\n\n    \"\"\"\n\n    # setup commmon keyword args\n    kwds = dict(fields=fields, exclude_fields=exclude_fields, types=types,\n                numbers=numbers, alt_number=alt_number, chunk_length=chunk_length,\n                fills=fills, samples=samples, region=region)\n\n    # setup input stream\n    stream = _setup_input_stream(input=input, region=region, tabix=tabix,\n                                 buffer_size=buffer_size)\n\n    # setup iterator\n    fields, samples, headers, it = _iter_vcf_stream(stream, **kwds)\n\n    # setup transformers\n    if transformers is not None:\n        # API flexibility\n        if not isinstance(transformers, (list, tuple)):\n            transformers = [transformers]\n        for trans in transformers:\n            fields = trans.transform_fields(fields)\n        it = _chunk_iter_transform(it, transformers)\n\n    return fields, samples, headers, it", "response": "Iterate over chunks of data from a VCF file as NumPy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread data from a VCF file into a pandas DataFrame.", "response": "def vcf_to_dataframe(input,\n                     fields=None,\n                     exclude_fields=None,\n                     types=None,\n                     numbers=None,\n                     alt_number=DEFAULT_ALT_NUMBER,\n                     fills=None,\n                     region=None,\n                     tabix='tabix',\n                     transformers=None,\n                     buffer_size=DEFAULT_BUFFER_SIZE,\n                     chunk_length=DEFAULT_CHUNK_LENGTH,\n                     log=None):\n    \"\"\"Read data from a VCF file into a pandas DataFrame.\n\n    Parameters\n    ----------\n    input : string\n        {input}\n    fields : list of strings, optional\n        {fields}\n    exclude_fields : list of strings, optional\n        {exclude_fields}\n    types : dict, optional\n        {types}\n    numbers : dict, optional\n        {numbers}\n    alt_number : int, optional\n        {alt_number}\n    fills : dict, optional\n        {fills}\n    region : string, optional\n        {region}\n    tabix : string, optional\n        {tabix}\n    transformers : list of transformer objects, optional\n        {transformers}\n    buffer_size : int, optional\n        {buffer_size}\n    chunk_length : int, optional\n        {chunk_length}\n    log : file-like, optional\n        {log}\n\n    Returns\n    -------\n    df : pandas.DataFrame\n\n    \"\"\"\n\n    import pandas\n\n    # samples requested?\n    # noinspection PyTypeChecker\n    _, fields = _prep_fields_param(fields)\n\n    # setup\n    fields, _, _, it = iter_vcf_chunks(\n        input=input, fields=fields, exclude_fields=exclude_fields, types=types,\n        numbers=numbers, alt_number=alt_number, buffer_size=buffer_size,\n        chunk_length=chunk_length, fills=fills, region=region, tabix=tabix, samples=[],\n        transformers=transformers\n    )\n\n    # setup progress logging\n    if log is not None:\n        it = _chunk_iter_progress(it, log, prefix='[vcf_to_dataframe]')\n\n    # read all chunks into a list\n    chunks = [d[0] for d in it]\n\n    # setup output\n    output = None\n\n    if chunks:\n\n        # concatenate chunks\n        output = pandas.concat([_chunk_to_dataframe(fields, chunk)\n                                for chunk in chunks])\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vcf_to_recarray(input,\n                    fields=None,\n                    exclude_fields=None,\n                    types=None,\n                    numbers=None,\n                    alt_number=DEFAULT_ALT_NUMBER,\n                    fills=None,\n                    region=None,\n                    tabix='tabix',\n                    transformers=None,\n                    buffer_size=DEFAULT_BUFFER_SIZE,\n                    chunk_length=DEFAULT_CHUNK_LENGTH,\n                    log=None):\n    \"\"\"Read data from a VCF file into a NumPy recarray.\n\n    Parameters\n    ----------\n    input : string\n        {input}\n    fields : list of strings, optional\n        {fields}\n    exclude_fields : list of strings, optional\n        {exclude_fields}\n    types : dict, optional\n        {types}\n    numbers : dict, optional\n        {numbers}\n    alt_number : int, optional\n        {alt_number}\n    fills : dict, optional\n        {fills}\n    region : string, optional\n        {region}\n    tabix : string, optional\n        {tabix}\n    transformers : list of transformer objects, optional\n        {transformers}\n    buffer_size : int, optional\n        {buffer_size}\n    chunk_length : int, optional\n        {chunk_length}\n    log : file-like, optional\n        {log}\n\n    Returns\n    -------\n    ra : np.rec.array\n\n    \"\"\"\n\n    # samples requested?\n    # noinspection PyTypeChecker\n    _, fields = _prep_fields_param(fields)\n\n    # setup chunk iterator\n    # N.B., set samples to empty list so we don't get any calldata fields\n    fields, _, _, it = iter_vcf_chunks(\n        input=input, fields=fields, exclude_fields=exclude_fields, types=types,\n        numbers=numbers, alt_number=alt_number, buffer_size=buffer_size,\n        chunk_length=chunk_length, fills=fills, region=region, tabix=tabix, samples=[],\n        transformers=transformers\n    )\n\n    # setup progress logging\n    if log is not None:\n        it = _chunk_iter_progress(it, log, prefix='[vcf_to_recarray]')\n\n    # read all chunks into a list\n    chunks = [d[0] for d in it]\n\n    # setup output\n    output = None\n\n    if chunks:\n\n        # concatenate chunks\n        output = np.concatenate([_chunk_to_recarray(fields, chunk) for chunk in chunks])\n\n    return output", "response": "Read data from a VCF file into a NumPy recarray."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_fasta(path, sequences, names, mode='w', width=80):\n\n    # check inputs\n    if isinstance(sequences, np.ndarray):\n        # single sequence\n        sequences = [sequences]\n        names = [names]\n    if len(sequences) != len(names):\n        raise ValueError('must provide the same number of sequences and names')\n    for sequence in sequences:\n        if sequence.dtype != np.dtype('S1'):\n            raise ValueError('expected S1 dtype, found %r' % sequence.dtype)\n\n    # force binary mode\n    mode = 'ab' if 'a' in mode else 'wb'\n\n    # write to file\n    with open(path, mode=mode) as fasta:\n        for name, sequence in zip(names, sequences):\n            # force bytes\n            if isinstance(name, text_type):\n                name = name.encode('ascii')\n            header = b'>' + name + b'\\n'\n            fasta.write(header)\n            for i in range(0, sequence.size, width):\n                line = sequence[i:i+width].tostring() + b'\\n'\n                fasta.write(line)", "response": "Write nucleotide sequences stored as numpy arrays to a FASTA file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the rate of observed heterozygosity for each variant.", "response": "def heterozygosity_observed(g, fill=np.nan):\n    \"\"\"Calculate the rate of observed heterozygosity for each variant.\n\n    Parameters\n    ----------\n\n    g : array_like, int, shape (n_variants, n_samples, ploidy)\n        Genotype array.\n    fill : float, optional\n        Use this value for variants where all calls are missing.\n\n    Returns\n    -------\n\n    ho : ndarray, float, shape (n_variants,)\n        Observed heterozygosity\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> g = allel.GenotypeArray([[[0, 0], [0, 0], [0, 0]],\n    ...                          [[0, 0], [0, 1], [1, 1]],\n    ...                          [[0, 0], [1, 1], [2, 2]],\n    ...                          [[1, 1], [1, 2], [-1, -1]]])\n    >>> allel.heterozygosity_observed(g)\n    array([0.        , 0.33333333, 0.        , 0.5       ])\n\n    \"\"\"\n\n    # check inputs\n    if not hasattr(g, 'count_het') or not hasattr(g, 'count_called'):\n        g = GenotypeArray(g, copy=False)\n\n    # count hets\n    n_het = np.asarray(g.count_het(axis=1))\n    n_called = np.asarray(g.count_called(axis=1))\n\n    # calculate rate of observed heterozygosity, accounting for variants\n    # where all calls are missing\n    with ignore_invalid():\n        ho = np.where(n_called > 0, n_het / n_called, fill)\n\n    return ho"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef heterozygosity_expected(af, ploidy, fill=np.nan):\n\n    # check inputs\n    af = asarray_ndim(af, 2)\n\n    # calculate expected heterozygosity\n    out = 1 - np.sum(np.power(af, ploidy), axis=1)\n\n    # fill values where allele frequencies could not be calculated\n    af_sum = np.sum(af, axis=1)\n    with ignore_invalid():\n        out[(af_sum < 1) | np.isnan(af_sum)] = fill\n\n    return out", "response": "Calculate the expected heterozygosity for each variant\n    under Hardy - Weinberg equilibrium."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inbreeding_coefficient(g, fill=np.nan):\n\n    # check inputs\n    if not hasattr(g, 'count_het') or not hasattr(g, 'count_called'):\n        g = GenotypeArray(g, copy=False)\n\n    # calculate observed and expected heterozygosity\n    ho = heterozygosity_observed(g)\n    af = g.count_alleles().to_frequencies()\n    he = heterozygosity_expected(af, ploidy=g.shape[-1], fill=0)\n\n    # calculate inbreeding coefficient, accounting for variants with no\n    # expected heterozygosity\n    with ignore_invalid():\n        f = np.where(he > 0, 1 - (ho / he), fill)\n\n    return f", "response": "Calculate the inbreeding coefficient for each variant in the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlocating genotype calls not consistent with Mendelian transmission of the base genotypes.", "response": "def mendel_errors(parent_genotypes, progeny_genotypes):\n    \"\"\"Locate genotype calls not consistent with Mendelian transmission of\n    alleles.\n\n    Parameters\n    ----------\n    parent_genotypes : array_like, int, shape (n_variants, 2, 2)\n        Genotype calls for the two parents.\n    progeny_genotypes : array_like, int, shape (n_variants, n_progeny, 2)\n        Genotype calls for the progeny.\n\n    Returns\n    -------\n    me : ndarray, int, shape (n_variants, n_progeny)\n        Count of Mendel errors for each progeny genotype call.\n\n    Examples\n    --------\n    The following are all consistent with Mendelian transmission. Note that a\n    value of 0 is returned for missing calls::\n\n        >>> import allel\n        >>> import numpy as np\n        >>> genotypes = np.array([\n        ...     # aa x aa -> aa\n        ...     [[0, 0], [0, 0], [0, 0], [-1, -1], [-1, -1], [-1, -1]],\n        ...     [[1, 1], [1, 1], [1, 1], [-1, -1], [-1, -1], [-1, -1]],\n        ...     [[2, 2], [2, 2], [2, 2], [-1, -1], [-1, -1], [-1, -1]],\n        ...     # aa x ab -> aa or ab\n        ...     [[0, 0], [0, 1], [0, 0], [0, 1], [-1, -1], [-1, -1]],\n        ...     [[0, 0], [0, 2], [0, 0], [0, 2], [-1, -1], [-1, -1]],\n        ...     [[1, 1], [0, 1], [1, 1], [0, 1], [-1, -1], [-1, -1]],\n        ...     # aa x bb -> ab\n        ...     [[0, 0], [1, 1], [0, 1], [-1, -1], [-1, -1], [-1, -1]],\n        ...     [[0, 0], [2, 2], [0, 2], [-1, -1], [-1, -1], [-1, -1]],\n        ...     [[1, 1], [2, 2], [1, 2], [-1, -1], [-1, -1], [-1, -1]],\n        ...     # aa x bc -> ab or ac\n        ...     [[0, 0], [1, 2], [0, 1], [0, 2], [-1, -1], [-1, -1]],\n        ...     [[1, 1], [0, 2], [0, 1], [1, 2], [-1, -1], [-1, -1]],\n        ...     # ab x ab -> aa or ab or bb\n        ...     [[0, 1], [0, 1], [0, 0], [0, 1], [1, 1], [-1, -1]],\n        ...     [[1, 2], [1, 2], [1, 1], [1, 2], [2, 2], [-1, -1]],\n        ...     [[0, 2], [0, 2], [0, 0], [0, 2], [2, 2], [-1, -1]],\n        ...     # ab x bc -> ab or ac or bb or bc\n        ...     [[0, 1], [1, 2], [0, 1], [0, 2], [1, 1], [1, 2]],\n        ...     [[0, 1], [0, 2], [0, 0], [0, 1], [0, 1], [1, 2]],\n        ...     # ab x cd -> ac or ad or bc or bd\n        ...     [[0, 1], [2, 3], [0, 2], [0, 3], [1, 2], [1, 3]],\n        ... ])\n        >>> me = allel.mendel_errors(genotypes[:, :2], genotypes[:, 2:])\n        >>> me\n        array([[0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0],\n               [0, 0, 0, 0]])\n\n    The following are cases of 'non-parental' inheritance where one or two\n    alleles are found in the progeny that are not present in either parent.\n    Note that the number of errors may be 1 or 2 depending on the number of\n    non-parental alleles::\n\n        >>> genotypes = np.array([\n        ...     # aa x aa -> ab or ac or bb or cc\n        ...     [[0, 0], [0, 0], [0, 1], [0, 2], [1, 1], [2, 2]],\n        ...     [[1, 1], [1, 1], [0, 1], [1, 2], [0, 0], [2, 2]],\n        ...     [[2, 2], [2, 2], [0, 2], [1, 2], [0, 0], [1, 1]],\n        ...     # aa x ab -> ac or bc or cc\n        ...     [[0, 0], [0, 1], [0, 2], [1, 2], [2, 2], [2, 2]],\n        ...     [[0, 0], [0, 2], [0, 1], [1, 2], [1, 1], [1, 1]],\n        ...     [[1, 1], [0, 1], [1, 2], [0, 2], [2, 2], [2, 2]],\n        ...     # aa x bb -> ac or bc or cc\n        ...     [[0, 0], [1, 1], [0, 2], [1, 2], [2, 2], [2, 2]],\n        ...     [[0, 0], [2, 2], [0, 1], [1, 2], [1, 1], [1, 1]],\n        ...     [[1, 1], [2, 2], [0, 1], [0, 2], [0, 0], [0, 0]],\n        ...     # ab x ab -> ac or bc or cc\n        ...     [[0, 1], [0, 1], [0, 2], [1, 2], [2, 2], [2, 2]],\n        ...     [[0, 2], [0, 2], [0, 1], [1, 2], [1, 1], [1, 1]],\n        ...     [[1, 2], [1, 2], [0, 1], [0, 2], [0, 0], [0, 0]],\n        ...     # ab x bc -> ad or bd or cd or dd\n        ...     [[0, 1], [1, 2], [0, 3], [1, 3], [2, 3], [3, 3]],\n        ...     [[0, 1], [0, 2], [0, 3], [1, 3], [2, 3], [3, 3]],\n        ...     [[0, 2], [1, 2], [0, 3], [1, 3], [2, 3], [3, 3]],\n        ...     # ab x cd -> ae or be or ce or de\n        ...     [[0, 1], [2, 3], [0, 4], [1, 4], [2, 4], [3, 4]],\n        ... ])\n        >>> me = allel.mendel_errors(genotypes[:, :2], genotypes[:, 2:])\n        >>> me\n        array([[1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 2, 2],\n               [1, 1, 1, 2],\n               [1, 1, 1, 2],\n               [1, 1, 1, 2],\n               [1, 1, 1, 1]])\n\n    The following are cases of 'hemi-parental' inheritance, where progeny\n    appear to have inherited two copies of an allele found only once in one of\n    the parents::\n\n        >>> genotypes = np.array([\n        ...     # aa x ab -> bb\n        ...     [[0, 0], [0, 1], [1, 1], [-1, -1]],\n        ...     [[0, 0], [0, 2], [2, 2], [-1, -1]],\n        ...     [[1, 1], [0, 1], [0, 0], [-1, -1]],\n        ...     # ab x bc -> aa or cc\n        ...     [[0, 1], [1, 2], [0, 0], [2, 2]],\n        ...     [[0, 1], [0, 2], [1, 1], [2, 2]],\n        ...     [[0, 2], [1, 2], [0, 0], [1, 1]],\n        ...     # ab x cd -> aa or bb or cc or dd\n        ...     [[0, 1], [2, 3], [0, 0], [1, 1]],\n        ...     [[0, 1], [2, 3], [2, 2], [3, 3]],\n        ... ])\n        >>> me = allel.mendel_errors(genotypes[:, :2], genotypes[:, 2:])\n        >>> me\n        array([[1, 0],\n               [1, 0],\n               [1, 0],\n               [1, 1],\n               [1, 1],\n               [1, 1],\n               [1, 1],\n               [1, 1]])\n\n    The following are cases of 'uni-parental' inheritance, where progeny\n    appear to have inherited both alleles from a single parent::\n\n        >>> genotypes = np.array([\n        ...     # aa x bb -> aa or bb\n        ...     [[0, 0], [1, 1], [0, 0], [1, 1]],\n        ...     [[0, 0], [2, 2], [0, 0], [2, 2]],\n        ...     [[1, 1], [2, 2], [1, 1], [2, 2]],\n        ...     # aa x bc -> aa or bc\n        ...     [[0, 0], [1, 2], [0, 0], [1, 2]],\n        ...     [[1, 1], [0, 2], [1, 1], [0, 2]],\n        ...     # ab x cd -> ab or cd\n        ...     [[0, 1], [2, 3], [0, 1], [2, 3]],\n        ... ])\n        >>> me = allel.mendel_errors(genotypes[:, :2], genotypes[:, 2:])\n        >>> me\n        array([[1, 1],\n               [1, 1],\n               [1, 1],\n               [1, 1],\n               [1, 1],\n               [1, 1]])\n\n    \"\"\"\n\n    # setup\n    parent_genotypes = GenotypeArray(parent_genotypes)\n    progeny_genotypes = GenotypeArray(progeny_genotypes)\n    check_ploidy(parent_genotypes.ploidy, 2)\n    check_ploidy(progeny_genotypes.ploidy, 2)\n\n    # transform into per-call allele counts\n    max_allele = max(parent_genotypes.max(), progeny_genotypes.max())\n    parent_gc = parent_genotypes.to_allele_counts(max_allele=max_allele, dtype='i1')\n    progeny_gc = progeny_genotypes.to_allele_counts(max_allele=max_allele, dtype='i1')\n\n    # detect nonparental and hemiparental inheritance by comparing allele\n    # counts between parents and progeny\n    max_progeny_gc = parent_gc.clip(max=1).sum(axis=1)\n    max_progeny_gc = max_progeny_gc[:, np.newaxis, :]\n    me = (progeny_gc - max_progeny_gc).clip(min=0).sum(axis=2)\n\n    # detect uniparental inheritance by finding cases where no alleles are\n    # shared between parents, then comparing progeny allele counts to each\n    # parent\n    p1_gc = parent_gc[:, 0, np.newaxis, :]\n    p2_gc = parent_gc[:, 1, np.newaxis, :]\n    # find variants where parents don't share any alleles\n    is_shared_allele = (p1_gc > 0) & (p2_gc > 0)\n    no_shared_alleles = ~np.any(is_shared_allele, axis=2)\n    # find calls where progeny genotype is identical to one or the other parent\n    me[no_shared_alleles &\n       (np.all(progeny_gc == p1_gc, axis=2) |\n        np.all(progeny_gc == p2_gc, axis=2))] = 1\n\n    # retrofit where either or both parent has a missing call\n    me[np.any(parent_genotypes.is_missing(), axis=1)] = 0\n\n    return me"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npainting the given parent s haplotypes inherited from the given parent s progeny s haplotypes.", "response": "def paint_transmission(parent_haplotypes, progeny_haplotypes):\n    \"\"\"Paint haplotypes inherited from a single diploid parent according to\n    their allelic inheritance.\n\n    Parameters\n    ----------\n    parent_haplotypes : array_like, int, shape (n_variants, 2)\n        Both haplotypes from a single diploid parent.\n    progeny_haplotypes : array_like, int, shape (n_variants, n_progeny)\n        Haplotypes found in progeny of the given parent, inherited from the\n        given parent. I.e., haplotypes from gametes of the given parent.\n\n    Returns\n    -------\n    painting : ndarray, uint8, shape (n_variants, n_progeny)\n        An array of integers coded as follows: 1 = allele inherited from\n        first parental haplotype; 2 = allele inherited from second parental\n        haplotype; 3 = reference allele, also carried by both parental\n        haplotypes; 4 = non-reference allele, also carried by both parental\n        haplotypes; 5 = non-parental allele; 6 = either or both parental\n        alleles missing; 7 = missing allele; 0 = undetermined.\n\n    Examples\n    --------\n    >>> import allel\n    >>> haplotypes = allel.HaplotypeArray([\n    ...     [0, 0, 0, 1, 2, -1],\n    ...     [0, 1, 0, 1, 2, -1],\n    ...     [1, 0, 0, 1, 2, -1],\n    ...     [1, 1, 0, 1, 2, -1],\n    ...     [0, 2, 0, 1, 2, -1],\n    ...     [0, -1, 0, 1, 2, -1],\n    ...     [-1, 1, 0, 1, 2, -1],\n    ...     [-1, -1, 0, 1, 2, -1],\n    ... ], dtype='i1')\n    >>> painting = allel.paint_transmission(haplotypes[:, :2],\n    ...                                           haplotypes[:, 2:])\n    >>> painting\n    array([[3, 5, 5, 7],\n           [1, 2, 5, 7],\n           [2, 1, 5, 7],\n           [5, 4, 5, 7],\n           [1, 5, 2, 7],\n           [6, 6, 6, 7],\n           [6, 6, 6, 7],\n           [6, 6, 6, 7]], dtype=uint8)\n\n    \"\"\"\n\n    # check inputs\n    parent_haplotypes = HaplotypeArray(parent_haplotypes)\n    progeny_haplotypes = HaplotypeArray(progeny_haplotypes)\n    if parent_haplotypes.n_haplotypes != 2:\n        raise ValueError('exactly two parental haplotypes should be provided')\n\n    # convenience variables\n    parent1 = parent_haplotypes[:, 0, np.newaxis]\n    parent2 = parent_haplotypes[:, 1, np.newaxis]\n    progeny_is_missing = progeny_haplotypes < 0\n    parent_is_missing = np.any(parent_haplotypes < 0, axis=1)\n    # need this for broadcasting, but also need to retain original for later\n    parent_is_missing_bc = parent_is_missing[:, np.newaxis]\n    parent_diplotype = GenotypeArray(parent_haplotypes[:, np.newaxis, :])\n    parent_is_hom_ref = parent_diplotype.is_hom_ref()\n    parent_is_het = parent_diplotype.is_het()\n    parent_is_hom_alt = parent_diplotype.is_hom_alt()\n\n    # identify allele calls where inheritance can be determined\n    is_callable = ~progeny_is_missing & ~parent_is_missing_bc\n    is_callable_seg = is_callable & parent_is_het\n\n    # main inheritance states\n    inherit_parent1 = is_callable_seg & (progeny_haplotypes == parent1)\n    inherit_parent2 = is_callable_seg & (progeny_haplotypes == parent2)\n    nonseg_ref = (is_callable & parent_is_hom_ref & (progeny_haplotypes == parent1))\n    nonseg_alt = (is_callable & parent_is_hom_alt & (progeny_haplotypes == parent1))\n    nonparental = (\n        is_callable & (progeny_haplotypes != parent1) & (progeny_haplotypes != parent2)\n    )\n\n    # record inheritance states\n    # N.B., order in which these are set matters\n    painting = np.zeros(progeny_haplotypes.shape, dtype='u1')\n    painting[inherit_parent1] = INHERIT_PARENT1\n    painting[inherit_parent2] = INHERIT_PARENT2\n    painting[nonseg_ref] = INHERIT_NONSEG_REF\n    painting[nonseg_alt] = INHERIT_NONSEG_ALT\n    painting[nonparental] = INHERIT_NONPARENTAL\n    painting[parent_is_missing] = INHERIT_PARENT_MISSING\n    painting[progeny_is_missing] = INHERIT_MISSING\n\n    return painting"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef phase_progeny_by_transmission(g):\n\n    # setup\n    g = GenotypeArray(g, dtype='i1', copy=True)\n    check_ploidy(g.ploidy, 2)\n    check_min_samples(g.n_samples, 3)\n\n    # run the phasing\n    # N.B., a copy has already been made, so no need to make memoryview safe\n    is_phased = _opt_phase_progeny_by_transmission(g.values)\n    g.is_phased = np.asarray(is_phased).view(bool)\n\n    # outputs\n    return g", "response": "Phase progeny genotypes from a trio or cross using Mendelian\n    transmission."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nphasing parent genotypes from a trio or cross given progeny genotypes already phased by Mendelian transmission.", "response": "def phase_parents_by_transmission(g, window_size):\n    \"\"\"Phase parent genotypes from a trio or cross, given progeny genotypes\n    already phased by Mendelian transmission.\n\n    Parameters\n    ----------\n    g : GenotypeArray\n        Genotype array, with parents as first two columns and progeny as\n        remaining columns, where progeny genotypes are already phased.\n    window_size : int\n        Number of previous heterozygous sites to include when phasing each\n        parent. A number somewhere between 10 and 100 may be appropriate,\n        depending on levels of heterozygosity and quality of data.\n\n    Returns\n    -------\n    g : GenotypeArray\n        Genotype array with parents phased where possible.\n\n    \"\"\"\n\n    # setup\n    check_type(g, GenotypeArray)\n    check_dtype(g.values, 'i1')\n    check_ploidy(g.ploidy, 2)\n    if g.is_phased is None:\n        raise ValueError('genotype array must first have progeny phased by transmission')\n    check_min_samples(g.n_samples, 3)\n\n    # run the phasing\n    g._values = memoryview_safe(g.values)\n    g._is_phased = memoryview_safe(g.is_phased)\n    _opt_phase_parents_by_transmission(g.values, g.is_phased.view('u1'), window_size)\n\n    # outputs\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nphase genotypes in a trio or cross where possible using Mendelian transmission.", "response": "def phase_by_transmission(g, window_size, copy=True):\n    \"\"\"Phase genotypes in a trio or cross where possible using Mendelian\n    transmission.\n\n    Parameters\n    ----------\n    g : array_like, int, shape (n_variants, n_samples, 2)\n        Genotype array, with parents as first two columns and progeny as\n        remaining columns.\n    window_size : int\n        Number of previous heterozygous sites to include when phasing each\n        parent. A number somewhere between 10 and 100 may be appropriate,\n        depending on levels of heterozygosity and quality of data.\n    copy : bool, optional\n        If False, attempt to phase genotypes in-place. Note that this is\n        only possible if the input array has int8 dtype, otherwise a copy is\n        always made regardless of this parameter.\n\n    Returns\n    -------\n    g : GenotypeArray\n        Genotype array with progeny phased where possible.\n\n    \"\"\"\n\n    # setup\n    g = np.asarray(g, dtype='i1')\n    g = GenotypeArray(g, copy=copy)\n    g._values = memoryview_safe(g.values)\n    check_ploidy(g.ploidy, 2)\n    check_min_samples(g.n_samples, 3)\n\n    # phase the progeny\n    is_phased = _opt_phase_progeny_by_transmission(g.values)\n    g.is_phased = np.asarray(is_phased).view(bool)\n\n    # phase the parents\n    _opt_phase_parents_by_transmission(g.values, is_phased, window_size)\n\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry to guess a reasonable block length to use for block - wise iteration over data.", "response": "def get_blen_array(data, blen=None):\n    \"\"\"Try to guess a reasonable block length to use for block-wise iteration\n    over `data`.\"\"\"\n\n    if blen is None:\n\n        if hasattr(data, 'chunklen'):\n            # bcolz carray\n            return data.chunklen\n\n        elif hasattr(data, 'chunks') and \\\n                hasattr(data, 'shape') and \\\n                hasattr(data.chunks, '__len__') and \\\n                hasattr(data.shape, '__len__') and \\\n                len(data.chunks) == len(data.shape):\n            # something like h5py dataset\n            return data.chunks[0]\n\n        else:\n            # fall back to something simple, ~1Mb chunks\n            row = np.asarray(data[0])\n            return max(1, (2**20) // row.nbytes)\n\n    else:\n        return blen"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an in - memory HDF5 file.", "response": "def h5fmem(**kwargs):\n    \"\"\"Create an in-memory HDF5 file.\"\"\"\n\n    # need a file name even tho nothing is ever written\n    fn = tempfile.mktemp()\n\n    # file creation args\n    kwargs['mode'] = 'w'\n    kwargs['driver'] = 'core'\n    kwargs['backing_store'] = False\n\n    # open HDF5 file\n    h5f = h5py.File(fn, **kwargs)\n\n    return h5f"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an HDF5 file backed by a temporary file.", "response": "def h5ftmp(**kwargs):\n    \"\"\"Create an HDF5 file backed by a temporary file.\"\"\"\n\n    # create temporary file name\n    suffix = kwargs.pop('suffix', '.h5')\n    prefix = kwargs.pop('prefix', 'scikit_allel_')\n    tempdir = kwargs.pop('dir', None)\n    fn = tempfile.mktemp(suffix=suffix, prefix=prefix, dir=tempdir)\n    atexit.register(os.remove, fn)\n\n    # file creation args\n    kwargs['mode'] = 'w'\n\n    # open HDF5 file\n    h5f = h5py.File(fn, **kwargs)\n\n    return h5f"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies data block - wise into arr.", "response": "def store(data, arr, start=0, stop=None, offset=0, blen=None):\n    \"\"\"Copy `data` block-wise into `arr`.\"\"\"\n\n    # setup\n    blen = _util.get_blen_array(data, blen)\n    if stop is None:\n        stop = len(data)\n    else:\n        stop = min(stop, len(data))\n    length = stop - start\n    if length < 0:\n        raise ValueError('invalid stop/start')\n\n    # copy block-wise\n    for bi in range(start, stop, blen):\n        bj = min(bi+blen, stop)\n        bl = bj - bi\n        arr[offset:offset+bl] = data[bi:bj]\n        offset += bl"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies data block - wise into a new array.", "response": "def copy(data, start=0, stop=None, blen=None, storage=None, create='array',\n         **kwargs):\n    \"\"\"Copy `data` block-wise into a new array.\"\"\"\n\n    # setup\n    storage = _util.get_storage(storage)\n    blen = _util.get_blen_array(data, blen)\n    if stop is None:\n        stop = len(data)\n    else:\n        stop = min(stop, len(data))\n    length = stop - start\n    if length < 0:\n        raise ValueError('invalid stop/start')\n\n    # copy block-wise\n    out = None\n    for i in range(start, stop, blen):\n        j = min(i+blen, stop)\n        block = data[i:j]\n        if out is None:\n            out = getattr(storage, create)(block, expectedlen=length, **kwargs)\n        else:\n            out.append(block)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying a table block - wise into a new table.", "response": "def copy_table(tbl, start=0, stop=None, blen=None, storage=None,\n               create='table', **kwargs):\n    \"\"\"Copy `tbl` block-wise into a new table.\"\"\"\n\n    # setup\n    names, columns = _util.check_table_like(tbl)\n    storage = _util.get_storage(storage)\n    blen = _util.get_blen_table(tbl, blen)\n    if stop is None:\n        stop = len(columns[0])\n    else:\n        stop = min(stop, len(columns[0]))\n    length = stop - start\n    if length < 0:\n        raise ValueError('invalid stop/start')\n\n    # copy block-wise\n    out = None\n    for i in range(start, stop, blen):\n        j = min(i+blen, stop)\n        res = [c[i:j] for c in columns]\n        if out is None:\n            out = getattr(storage, create)(res, names=names,\n                                           expectedlen=length, **kwargs)\n        else:\n            out.append(res)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply function f block - wise over data.", "response": "def map_blocks(data, f, blen=None, storage=None, create='array', **kwargs):\n    \"\"\"Apply function `f` block-wise over `data`.\"\"\"\n\n    # setup\n    storage = _util.get_storage(storage)\n    if isinstance(data, tuple):\n        blen = max(_util.get_blen_array(d, blen) for d in data)\n    else:\n        blen = _util.get_blen_array(data, blen)\n    if isinstance(data, tuple):\n        _util.check_equal_length(*data)\n        length = len(data[0])\n    else:\n        length = len(data)\n\n    # block-wise iteration\n    out = None\n    for i in range(0, length, blen):\n        j = min(i+blen, length)\n\n        # obtain blocks\n        if isinstance(data, tuple):\n            blocks = [d[i:j] for d in data]\n        else:\n            blocks = [data[i:j]]\n\n        # map\n        res = f(*blocks)\n\n        # store\n        if out is None:\n            out = getattr(storage, create)(res, expectedlen=length, **kwargs)\n        else:\n            out.append(res)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply an operation to data that reduces over one or more axes.", "response": "def reduce_axis(data, reducer, block_reducer, mapper=None, axis=None,\n                blen=None, storage=None, create='array', **kwargs):\n    \"\"\"Apply an operation to `data` that reduces over one or more axes.\"\"\"\n\n    # setup\n    storage = _util.get_storage(storage)\n    blen = _util.get_blen_array(data, blen)\n    length = len(data)\n    # normalise axis arg\n    if isinstance(axis, int):\n        axis = (axis,)\n\n    # deal with 'out' kwarg if supplied, can arise if a chunked array is\n    # passed as an argument to numpy.sum(), see also\n    # https://github.com/cggh/scikit-allel/issues/66\n    kwarg_out = kwargs.pop('out', None)\n    if kwarg_out is not None:\n        raise ValueError('keyword argument \"out\" is not supported')\n\n    if axis is None or 0 in axis:\n        # two-step reduction\n        out = None\n        for i in range(0, length, blen):\n            j = min(i+blen, length)\n            block = data[i:j]\n            if mapper:\n                block = mapper(block)\n            res = reducer(block, axis=axis)\n            if out is None:\n                out = res\n            else:\n                out = block_reducer(out, res)\n        if np.isscalar(out):\n            return out\n        elif len(out.shape) == 0:\n            return out[()]\n        else:\n            return getattr(storage, create)(out, **kwargs)\n\n    else:\n        # first dimension is preserved, no need to reduce blocks\n        out = None\n        for i in range(0, length, blen):\n            j = min(i+blen, length)\n            block = data[i:j]\n            if mapper:\n                block = mapper(block)\n            r = reducer(block, axis=axis)\n            if out is None:\n                out = getattr(storage, create)(r, expectedlen=length, **kwargs)\n            else:\n                out.append(r)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the maximum value.", "response": "def amax(data, axis=None, mapper=None, blen=None, storage=None,\n         create='array', **kwargs):\n    \"\"\"Compute the maximum value.\"\"\"\n    return reduce_axis(data, axis=axis, reducer=np.amax,\n                       block_reducer=np.maximum, mapper=mapper,\n                       blen=blen, storage=storage, create=create, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the minimum value.", "response": "def amin(data, axis=None, mapper=None, blen=None, storage=None,\n         create='array', **kwargs):\n    \"\"\"Compute the minimum value.\"\"\"\n    return reduce_axis(data, axis=axis, reducer=np.amin,\n                       block_reducer=np.minimum, mapper=mapper,\n                       blen=blen, storage=storage, create=create, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef count_nonzero(data, mapper=None, blen=None, storage=None,\n                  create='array', **kwargs):\n    \"\"\"Count the number of non-zero elements.\"\"\"\n    return reduce_axis(data, reducer=np.count_nonzero,\n                       block_reducer=np.add, mapper=mapper,\n                       blen=blen, storage=storage, create=create, **kwargs)", "response": "Count the number of non - zero elements."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn selected slices of an array along given axis.", "response": "def compress(condition, data, axis=0, out=None, blen=None, storage=None, create='array',\n             **kwargs):\n    \"\"\"Return selected slices of an array along given axis.\"\"\"\n\n    # setup\n    if out is not None:\n        # argument is only there for numpy API compatibility\n        raise NotImplementedError('out argument is not supported')\n    storage = _util.get_storage(storage)\n    blen = _util.get_blen_array(data, blen)\n    length = len(data)\n    nnz = count_nonzero(condition)\n\n    if axis == 0:\n        _util.check_equal_length(data, condition)\n\n        # block iteration\n        out = None\n        for i in range(0, length, blen):\n            j = min(i+blen, length)\n            bcond = np.asarray(condition[i:j])\n            # don't access any data unless we have to\n            if np.any(bcond):\n                block = np.asarray(data[i:j])\n                res = np.compress(bcond, block, axis=0)\n                if out is None:\n                    out = getattr(storage, create)(res, expectedlen=nnz, **kwargs)\n                else:\n                    out.append(res)\n        return out\n\n    elif axis == 1:\n\n        # block iteration\n        out = None\n        condition = np.asanyarray(condition)\n        for i in range(0, length, blen):\n            j = min(i+blen, length)\n            block = np.asarray(data[i:j])\n            res = np.compress(condition, block, axis=1)\n            if out is None:\n                out = getattr(storage, create)(res, expectedlen=length,\n                                               **kwargs)\n            else:\n                out.append(res)\n\n        return out\n\n    else:\n        raise NotImplementedError('axis not supported: %s' % axis)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes elements from an array along an axis.", "response": "def take(data, indices, axis=0, out=None, mode='raise', blen=None, storage=None,\n         create='array', **kwargs):\n    \"\"\"Take elements from an array along an axis.\"\"\"\n\n    # setup\n    if out is not None:\n        # argument is only there for numpy API compatibility\n        raise NotImplementedError('out argument is not supported')\n    length = len(data)\n\n    if axis == 0:\n\n        # check that indices are strictly increasing\n        indices = np.asanyarray(indices)\n        if np.any(indices[1:] <= indices[:-1]):\n            raise NotImplementedError(\n                'indices must be strictly increasing'\n            )\n\n        # implement via compress()\n        condition = np.zeros((length,), dtype=bool)\n        condition[indices] = True\n        return compress(condition, data, axis=0, blen=blen, storage=storage,\n                        create=create, **kwargs)\n\n    elif axis == 1:\n\n        # setup\n        storage = _util.get_storage(storage)\n        blen = _util.get_blen_array(data, blen)\n\n        # block iteration\n        out = None\n        for i in range(0, length, blen):\n            j = min(i+blen, length)\n            block = data[i:j]\n            res = np.take(block, indices, axis=1, mode=mode)\n            if out is None:\n                out = getattr(storage, create)(res, expectedlen=length,\n                                               **kwargs)\n            else:\n                out.append(res)\n        return out\n\n    else:\n        raise NotImplementedError('axis not supported: %s' % axis)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compress_table(condition, tbl, axis=None, out=None, blen=None, storage=None,\n                   create='table', **kwargs):\n    \"\"\"Return selected rows of a table.\"\"\"\n\n    # setup\n    if axis is not None and axis != 0:\n        raise NotImplementedError('only axis 0 is supported')\n    if out is not None:\n        # argument is only there for numpy API compatibility\n        raise NotImplementedError('out argument is not supported')\n    storage = _util.get_storage(storage)\n    names, columns = _util.check_table_like(tbl)\n    blen = _util.get_blen_table(tbl, blen)\n    _util.check_equal_length(columns[0], condition)\n    length = len(columns[0])\n    nnz = count_nonzero(condition)\n\n    # block iteration\n    out = None\n    for i in range(0, length, blen):\n        j = min(i+blen, length)\n        bcond = condition[i:j]\n        # don't access any data unless we have to\n        if np.any(bcond):\n            bcolumns = [c[i:j] for c in columns]\n            res = [np.compress(bcond, c, axis=0) for c in bcolumns]\n            if out is None:\n                out = getattr(storage, create)(res, names=names,\n                                               expectedlen=nnz, **kwargs)\n            else:\n                out.append(res)\n    return out", "response": "Return selected rows of a table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef take_table(tbl, indices, axis=None, out=None, mode='raise', blen=None, storage=None,\n               create='table', **kwargs):\n    \"\"\"Return selected rows of a table.\"\"\"\n\n    # setup\n    if axis is not None and axis != 0:\n        raise NotImplementedError('only axis 0 is supported')\n    if out is not None:\n        # argument is only there for numpy API compatibility\n        raise NotImplementedError('out argument is not supported')\n    if mode is not None and mode != 'raise':\n        raise NotImplementedError('only mode=raise is supported')\n    names, columns = _util.check_table_like(tbl)\n    length = len(columns[0])\n\n    # check that indices are strictly increasing\n    indices = np.asanyarray(indices)\n    if np.any(indices[1:] <= indices[:-1]):\n        raise NotImplementedError(\n            'indices must be strictly increasing'\n        )\n\n    # implement via compress()\n    condition = np.zeros((length,), dtype=bool)\n    condition[indices] = True\n    return compress_table(condition, tbl, blen=blen, storage=storage,\n                          create=create, **kwargs)", "response": "Take a table and return a new table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef subset(data, sel0=None, sel1=None, blen=None, storage=None, create='array',\n           **kwargs):\n    \"\"\"Return selected rows and columns of an array.\"\"\"\n\n    # TODO refactor sel0 and sel1 normalization with ndarray.subset\n\n    # setup\n    storage = _util.get_storage(storage)\n    blen = _util.get_blen_array(data, blen)\n    length = len(data)\n    if sel0 is not None:\n        sel0 = np.asanyarray(sel0)\n    if sel1 is not None:\n        sel1 = np.asanyarray(sel1)\n\n    # ensure boolean array for dim 0\n    if sel0 is not None and sel0.dtype.kind != 'b':\n        # assume indices, convert to boolean condition\n        tmp = np.zeros(length, dtype=bool)\n        tmp[sel0] = True\n        sel0 = tmp\n\n    # ensure indices for dim 1\n    if sel1 is not None and sel1.dtype.kind == 'b':\n        # assume boolean condition, convert to indices\n        sel1, = np.nonzero(sel1)\n\n    # shortcuts\n    if sel0 is None and sel1 is None:\n        return copy(data, blen=blen, storage=storage, create=create, **kwargs)\n    elif sel1 is None:\n        return compress(sel0, data, axis=0, blen=blen, storage=storage,\n                        create=create, **kwargs)\n    elif sel0 is None:\n        return take(data, sel1, axis=1, blen=blen, storage=storage,\n                    create=create, **kwargs)\n\n    # build output\n    sel0_nnz = count_nonzero(sel0)\n    out = None\n    for i in range(0, length, blen):\n        j = min(i+blen, length)\n        bsel0 = sel0[i:j]\n        # don't access data unless we have to\n        if np.any(bsel0):\n            block = data[i:j]\n            res = _numpy_subset(block, bsel0, sel1)\n            if out is None:\n                out = getattr(storage, create)(res, expectedlen=sel0_nnz,\n                                               **kwargs)\n            else:\n                out.append(res)\n\n    return out", "response": "Return selected rows and columns of an array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef concatenate_table(tup, blen=None, storage=None, create='table', **kwargs):\n\n    # setup\n    storage = _util.get_storage(storage)\n    if not isinstance(tup, (tuple, list)):\n        raise ValueError('expected tuple or list, found %r' % tup)\n    if len(tup) < 2:\n        raise ValueError('expected two or more tables to stack')\n\n    # build output\n    expectedlen = sum(len(t) for t in tup)\n    out = None\n    tnames = None\n    for tdata in tup:\n        tblen = _util.get_blen_table(tdata, blen)\n        tnames, tcolumns = _util.check_table_like(tdata, names=tnames)\n        tlen = len(tcolumns[0])\n        for i in range(0, tlen, tblen):\n            j = min(i+tblen, tlen)\n            bcolumns = [c[i:j] for c in tcolumns]\n            if out is None:\n                out = getattr(storage, create)(bcolumns, names=tnames,\n                                               expectedlen=expectedlen,\n                                               **kwargs)\n            else:\n                out.append(bcolumns)\n    return out", "response": "Stack tables in sequence vertically ( row - wise )."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef binary_op(data, op, other, blen=None, storage=None, create='array',\n              **kwargs):\n    \"\"\"Compute a binary operation block-wise over `data`.\"\"\"\n\n    # normalise scalars\n    if hasattr(other, 'shape') and len(other.shape) == 0:\n        other = other[()]\n\n    if np.isscalar(other):\n        def f(block):\n            return op(block, other)\n        return map_blocks(data, f, blen=blen, storage=storage, create=create, **kwargs)\n\n    elif len(data) == len(other):\n        def f(a, b):\n            return op(a, b)\n        return map_blocks((data, other), f, blen=blen, storage=storage, create=create,\n                          **kwargs)\n\n    else:\n        raise NotImplementedError('argument type not supported')", "response": "Compute a binary operation block - wise over data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nevaluate expression against columns of a table.", "response": "def eval_table(tbl, expression, vm='python', blen=None, storage=None,\n               create='array', vm_kwargs=None, **kwargs):\n    \"\"\"Evaluate `expression` against columns of a table.\"\"\"\n\n    # setup\n    storage = _util.get_storage(storage)\n    names, columns = _util.check_table_like(tbl)\n    length = len(columns[0])\n    if vm_kwargs is None:\n        vm_kwargs = dict()\n\n    # setup vm\n    if vm == 'numexpr':\n        import numexpr\n        evaluate = numexpr.evaluate\n    elif vm == 'python':\n        # noinspection PyUnusedLocal\n        def evaluate(expr, local_dict=None, **kw):\n            # takes no keyword arguments\n            return eval(expr, dict(), local_dict)\n    else:\n        raise ValueError('expected vm either \"numexpr\" or \"python\"')\n\n    # compile expression and get required columns\n    variables = _get_expression_variables(expression, vm)\n    required_columns = {v: columns[names.index(v)] for v in variables}\n\n    # determine block size for evaluation\n    blen = _util.get_blen_table(required_columns, blen=blen)\n\n    # build output\n    out = None\n    for i in range(0, length, blen):\n        j = min(i+blen, length)\n        blocals = {v: c[i:j] for v, c in required_columns.items()}\n        res = evaluate(expression, local_dict=blocals, **vm_kwargs)\n        if out is None:\n            out = getattr(storage, create)(res, expectedlen=length, **kwargs)\n        else:\n            out.append(res)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_allele_mapping(ref, alt, alleles, dtype='i1'):\n\n    ref = asarray_ndim(ref, 1)\n    alt = asarray_ndim(alt, 1, 2)\n    alleles = asarray_ndim(alleles, 1, 2)\n    check_dim0_aligned(ref, alt, alleles)\n\n    # reshape for convenience\n    ref = ref[:, None]\n    if alt.ndim == 1:\n        alt = alt[:, None]\n    if alleles.ndim == 1:\n        alleles = alleles[:, None]\n    source_alleles = np.append(ref, alt, axis=1)\n\n    # setup output array\n    out = np.empty(source_alleles.shape, dtype=dtype)\n    out.fill(-1)\n\n    # find matches\n    for ai in range(source_alleles.shape[1]):\n        match = source_alleles[:, ai, None] == alleles\n        match_i, match_j = match.nonzero()\n        out[match_i, ai] = match_j\n\n    return out", "response": "Create an array mapping variant alleles into a different allele index."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlocates variants with no shared alleles between two populations.", "response": "def locate_fixed_differences(ac1, ac2):\n    \"\"\"Locate variants with no shared alleles between two populations.\n\n    Parameters\n    ----------\n    ac1 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array from the first population.\n    ac2 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array from the second population.\n\n    Returns\n    -------\n    loc : ndarray, bool, shape (n_variants,)\n\n    See Also\n    --------\n    allel.stats.diversity.windowed_df\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> g = allel.GenotypeArray([[[0, 0], [0, 0], [1, 1], [1, 1]],\n    ...                          [[0, 1], [0, 1], [0, 1], [0, 1]],\n    ...                          [[0, 1], [0, 1], [1, 1], [1, 1]],\n    ...                          [[0, 0], [0, 0], [1, 1], [2, 2]],\n    ...                          [[0, 0], [-1, -1], [1, 1], [-1, -1]]])\n    >>> ac1 = g.count_alleles(subpop=[0, 1])\n    >>> ac2 = g.count_alleles(subpop=[2, 3])\n    >>> loc_df = allel.locate_fixed_differences(ac1, ac2)\n    >>> loc_df\n    array([ True, False, False,  True,  True])\n\n    \"\"\"\n\n    # check inputs\n    ac1 = asarray_ndim(ac1, 2)\n    ac2 = asarray_ndim(ac2, 2)\n    check_dim0_aligned(ac1, ac2)\n    ac1, ac2 = ensure_dim1_aligned(ac1, ac2)\n\n    # stack allele counts for convenience\n    pac = np.dstack([ac1, ac2])\n\n    # count numbers of alleles called in each population\n    pan = np.sum(pac, axis=1)\n\n    # count the numbers of populations with each allele\n    npa = np.sum(pac > 0, axis=2)\n\n    # locate variants with allele calls in both populations\n    non_missing = np.all(pan > 0, axis=1)\n\n    # locate variants where all alleles are only found in a single population\n    no_shared_alleles = np.all(npa <= 1, axis=1)\n\n    return non_missing & no_shared_alleles"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlocate all the alleles that are found only in a single population.", "response": "def locate_private_alleles(*acs):\n    \"\"\"Locate alleles that are found only in a single population.\n\n    Parameters\n    ----------\n    *acs : array_like, int, shape (n_variants, n_alleles)\n        Allele counts arrays from each population.\n\n    Returns\n    -------\n    loc : ndarray, bool, shape (n_variants, n_alleles)\n        Boolean array where elements are True if allele is private to a\n        single population.\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> g = allel.GenotypeArray([[[0, 0], [0, 0], [1, 1], [1, 1]],\n    ...                          [[0, 1], [0, 1], [0, 1], [0, 1]],\n    ...                          [[0, 1], [0, 1], [1, 1], [1, 1]],\n    ...                          [[0, 0], [0, 0], [1, 1], [2, 2]],\n    ...                          [[0, 0], [-1, -1], [1, 1], [-1, -1]]])\n    >>> ac1 = g.count_alleles(subpop=[0, 1])\n    >>> ac2 = g.count_alleles(subpop=[2])\n    >>> ac3 = g.count_alleles(subpop=[3])\n    >>> loc_private_alleles = allel.locate_private_alleles(ac1, ac2, ac3)\n    >>> loc_private_alleles\n    array([[ True, False, False],\n           [False, False, False],\n           [ True, False, False],\n           [ True,  True,  True],\n           [ True,  True, False]])\n    >>> loc_private_variants = np.any(loc_private_alleles, axis=1)\n    >>> loc_private_variants\n    array([ True, False,  True,  True,  True])\n\n    \"\"\"\n\n    # check inputs\n    acs = [asarray_ndim(ac, 2) for ac in acs]\n    check_dim0_aligned(*acs)\n    acs = ensure_dim1_aligned(*acs)\n\n    # stack allele counts for convenience\n    pac = np.dstack(acs)\n\n    # count the numbers of populations with each allele\n    npa = np.sum(pac > 0, axis=2)\n\n    # locate alleles found only in a single population\n    loc_pa = npa == 1\n\n    return loc_pa"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hudson_fst(ac1, ac2, fill=np.nan):\n\n    # check inputs\n    ac1 = asarray_ndim(ac1, 2)\n    ac2 = asarray_ndim(ac2, 2)\n    check_dim0_aligned(ac1, ac2)\n    ac1, ac2 = ensure_dim1_aligned(ac1, ac2)\n\n    # calculate these once only\n    an1 = np.sum(ac1, axis=1)\n    an2 = np.sum(ac2, axis=1)\n\n    # calculate average diversity (a.k.a. heterozygosity) within each\n    # population\n    within = (mean_pairwise_difference(ac1, an1, fill=fill) +\n              mean_pairwise_difference(ac2, an2, fill=fill)) / 2\n\n    # calculate divergence (a.k.a. heterozygosity) between each population\n    between = mean_pairwise_difference_between(ac1, ac2, an1, an2, fill=fill)\n\n    # define numerator and denominator for Fst calculations\n    num = between - within\n    den = between\n\n    return num, den", "response": "Calculates the numerator and denominator for Fst estimation using the Hudson method of Elevator et al. 2003."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates of differentiation between populations A and B based on the F2 parameter.", "response": "def patterson_fst(aca, acb):\n    \"\"\"Estimator of differentiation between populations A and B based on the\n    F2 parameter.\n\n    Parameters\n    ----------\n    aca : array_like, int, shape (n_variants, 2)\n        Allele counts for population A.\n    acb : array_like, int, shape (n_variants, 2)\n        Allele counts for population B.\n\n    Returns\n    -------\n    num : ndarray, shape (n_variants,), float\n        Numerator.\n    den : ndarray, shape (n_variants,), float\n        Denominator.\n\n    Notes\n    -----\n    See Patterson (2012), Appendix A.\n\n    TODO check if this is  numerically equivalent to Hudson's estimator.\n\n    \"\"\"\n\n    from allel.stats.admixture import patterson_f2, h_hat\n    num = patterson_f2(aca, acb)\n    den = num + h_hat(aca) + h_hat(acb)\n\n    return num, den"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates average Fst in windows over a single chromosome or contig.", "response": "def windowed_weir_cockerham_fst(pos, g, subpops, size=None, start=None,\n                                stop=None, step=None, windows=None,\n                                fill=np.nan, max_allele=None):\n    \"\"\"Estimate average Fst in windows over a single chromosome/contig,\n    following the method of Weir and Cockerham (1984).\n\n    Parameters\n    ----------\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    g : array_like, int, shape (n_variants, n_samples, ploidy)\n        Genotype array.\n    subpops : sequence of sequences of ints\n        Sample indices for each subpopulation.\n    size : int\n        The window size (number of bases).\n    start : int, optional\n        The position at which to start (1-based).\n    stop : int, optional\n        The position at which to stop (1-based).\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    windows : array_like, int, shape (n_windows, 2), optional\n        Manually specify the windows to use as a sequence of (window_start,\n        window_stop) positions, using 1-based coordinates. Overrides the\n        size/start/stop/step parameters.\n    fill : object, optional\n        The value to use where there are no variants within a window.\n    max_allele : int, optional\n        The highest allele index to consider.\n\n    Returns\n    -------\n    fst : ndarray, float, shape (n_windows,)\n        Average Fst in each window.\n    windows : ndarray, int, shape (n_windows, 2)\n        The windows used, as an array of (window_start, window_stop) positions,\n        using 1-based coordinates.\n    counts : ndarray, int, shape (n_windows,)\n        Number of variants in each window.\n\n    \"\"\"\n\n    # compute values per-variant\n    a, b, c = weir_cockerham_fst(g, subpops, max_allele=max_allele)\n\n    # define the statistic to compute within each window\n    def average_fst(wa, wb, wc):\n        return np.nansum(wa) / (np.nansum(wa) + np.nansum(wb) + np.nansum(wc))\n\n    # calculate average Fst in windows\n    fst, windows, counts = windowed_statistic(pos, values=(a, b, c),\n                                              statistic=average_fst,\n                                              size=size, start=start,\n                                              stop=stop, step=step,\n                                              windows=windows, fill=fill)\n\n    return fst, windows, counts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef windowed_hudson_fst(pos, ac1, ac2, size=None, start=None, stop=None,\n                        step=None, windows=None, fill=np.nan):\n    \"\"\"Estimate average Fst in windows over a single chromosome/contig,\n    following the method of Hudson (1992) elaborated by Bhatia et al. (2013).\n\n    Parameters\n    ----------\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    ac1 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array from the first population.\n    ac2 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array from the second population.\n    size : int, optional\n        The window size (number of bases).\n    start : int, optional\n        The position at which to start (1-based).\n    stop : int, optional\n        The position at which to stop (1-based).\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    windows : array_like, int, shape (n_windows, 2), optional\n        Manually specify the windows to use as a sequence of (window_start,\n        window_stop) positions, using 1-based coordinates. Overrides the\n        size/start/stop/step parameters.\n    fill : object, optional\n        The value to use where there are no variants within a window.\n\n    Returns\n    -------\n    fst : ndarray, float, shape (n_windows,)\n        Average Fst in each window.\n    windows : ndarray, int, shape (n_windows, 2)\n        The windows used, as an array of (window_start, window_stop) positions,\n        using 1-based coordinates.\n    counts : ndarray, int, shape (n_windows,)\n        Number of variants in each window.\n\n    \"\"\"\n\n    # compute values per-variants\n    num, den = hudson_fst(ac1, ac2)\n\n    # define the statistic to compute within each window\n    def average_fst(wn, wd):\n        return np.nansum(wn) / np.nansum(wd)\n\n    # calculate average Fst in windows\n    fst, windows, counts = windowed_statistic(pos, values=(num, den),\n                                              statistic=average_fst,\n                                              size=size, start=start,\n                                              stop=stop, step=step,\n                                              windows=windows, fill=fill)\n\n    return fst, windows, counts", "response": "Estimate average Fst in windows over a single chromosome or contig."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nestimate average Fst in moving windows over a single chromosome or contig.", "response": "def moving_weir_cockerham_fst(g, subpops, size, start=0, stop=None, step=None,\n                              max_allele=None):\n    \"\"\"Estimate average Fst in moving windows over a single chromosome/contig,\n    following the method of Weir and Cockerham (1984).\n\n    Parameters\n    ----------\n    g : array_like, int, shape (n_variants, n_samples, ploidy)\n        Genotype array.\n    subpops : sequence of sequences of ints\n        Sample indices for each subpopulation.\n    size : int\n        The window size (number of variants).\n    start : int, optional\n        The index at which to start.\n    stop : int, optional\n        The index at which to stop.\n    step : int, optional\n        The number of variants between start positions of windows. If not\n        given, defaults to the window size, i.e., non-overlapping windows.\n    max_allele : int, optional\n        The highest allele index to consider.\n\n    Returns\n    -------\n    fst : ndarray, float, shape (n_windows,)\n        Average Fst in each window.\n\n    \"\"\"\n\n    # calculate per-variant values\n    a, b, c = weir_cockerham_fst(g, subpops, max_allele=max_allele)\n\n    # compute the numerator and denominator in moving windows\n    num = moving_statistic(a, statistic=np.nansum, size=size, start=start,\n                           stop=stop, step=step)\n    den = moving_statistic(a + b + c, statistic=np.nansum, size=size,\n                           start=start, stop=stop, step=step)\n\n    #  calculate fst in each window\n    fst = num / den\n\n    return fst"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nestimates average Fst in moving windows over a single chromosome or contig.", "response": "def moving_hudson_fst(ac1, ac2, size, start=0, stop=None, step=None):\n    \"\"\"Estimate average Fst in moving windows over a single chromosome/contig,\n    following the method of Hudson (1992) elaborated by Bhatia et al. (2013).\n\n    Parameters\n    ----------\n    ac1 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array from the first population.\n    ac2 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array from the second population.\n    size : int\n        The window size (number of variants).\n    start : int, optional\n        The index at which to start.\n    stop : int, optional\n        The index at which to stop.\n    step : int, optional\n        The number of variants between start positions of windows. If not\n        given, defaults to the window size, i.e., non-overlapping windows.\n\n    Returns\n    -------\n    fst : ndarray, float, shape (n_windows,)\n        Average Fst in each window.\n\n    \"\"\"\n\n    # calculate per-variant values\n    num, den = hudson_fst(ac1, ac2, fill=np.nan)\n\n    # compute the numerator and denominator in moving windows\n    num_sum = moving_statistic(num, statistic=np.nansum, size=size,\n                               start=start, stop=stop, step=step)\n    den_sum = moving_statistic(den, statistic=np.nansum, size=size,\n                               start=start, stop=stop, step=step)\n\n    # calculate fst in each window\n    fst = num_sum / den_sum\n\n    return fst"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef moving_patterson_fst(ac1, ac2, size, start=0, stop=None, step=None):\n\n    # calculate per-variant values\n    num, den = patterson_fst(ac1, ac2)\n\n    # compute the numerator and denominator in moving windows\n    num_sum = moving_statistic(num, statistic=np.nansum, size=size,\n                               start=start, stop=stop, step=step)\n    den_sum = moving_statistic(den, statistic=np.nansum, size=size,\n                               start=start, stop=stop, step=step)\n\n    # calculate fst in each window\n    fst = num_sum / den_sum\n\n    return fst", "response": "Estimate average Fst in moving windows over a single chromosome or contig."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef average_weir_cockerham_fst(g, subpops, blen, max_allele=None):\n\n    # calculate per-variant values\n    a, b, c = weir_cockerham_fst(g, subpops, max_allele=max_allele)\n\n    # calculate overall estimate\n    a_sum = np.nansum(a)\n    b_sum = np.nansum(b)\n    c_sum = np.nansum(c)\n    fst = a_sum / (a_sum + b_sum + c_sum)\n\n    # compute the numerator and denominator within each block\n    num_bsum = moving_statistic(a, statistic=np.nansum, size=blen)\n    den_bsum = moving_statistic(a + b + c, statistic=np.nansum, size=blen)\n\n    # calculate the statistic values in each block\n    vb = num_bsum / den_bsum\n\n    # estimate standard error\n    _, se, vj = jackknife((num_bsum, den_bsum),\n                          statistic=lambda n, d: np.sum(n) / np.sum(d))\n\n    return fst, se, vb, vj", "response": "Estimate average Fst and standard error using the block - jackknife resampling."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef average_hudson_fst(ac1, ac2, blen):\n\n    # calculate per-variant values\n    num, den = hudson_fst(ac1, ac2, fill=np.nan)\n\n    # calculate overall estimate\n    fst = np.nansum(num) / np.nansum(den)\n\n    # compute the numerator and denominator within each block\n    num_bsum = moving_statistic(num, statistic=np.nansum, size=blen)\n    den_bsum = moving_statistic(den, statistic=np.nansum, size=blen)\n\n    # calculate the statistic values in each block\n    vb = num_bsum / den_bsum\n\n    # estimate standard error\n    _, se, vj = jackknife((num_bsum, den_bsum),\n                          statistic=lambda n, d: np.sum(n) / np.sum(d))\n\n    return fst, se, vb, vj", "response": "Estimate average Fst between two populations and standard error using the block - jackknife resampling algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef average_patterson_fst(ac1, ac2, blen):\n\n    # calculate per-variant values\n    num, den = patterson_fst(ac1, ac2)\n\n    # calculate overall estimate\n    fst = np.nansum(num) / np.nansum(den)\n\n    # compute the numerator and denominator within each block\n    num_bsum = moving_statistic(num, statistic=np.nansum, size=blen)\n    den_bsum = moving_statistic(den, statistic=np.nansum, size=blen)\n\n    # calculate the statistic values in each block\n    vb = num_bsum / den_bsum\n\n    # estimate standard error\n    _, se, vj = jackknife((num_bsum, den_bsum),\n                          statistic=lambda n, d: np.sum(n) / np.sum(d))\n\n    return fst, se, vb, vj", "response": "Estimate average Fst between two populations and standard error using the block - jackknife resampling algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rogers_huff_r(gn):\n\n    # check inputs\n    gn = asarray_ndim(gn, 2, dtype='i1')\n    gn = memoryview_safe(gn)\n\n    # compute correlation coefficients\n    r = gn_pairwise_corrcoef_int8(gn)\n\n    # convenience for singletons\n    if r.size == 1:\n        r = r[0]\n\n    return r", "response": "Estimate the linkage disequilibrium parameter r for each pair of genotypes at biallelic variants using the method of Rogers and Huff."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rogers_huff_r_between(gna, gnb):\n\n    # check inputs\n    gna = asarray_ndim(gna, 2, dtype='i1')\n    gnb = asarray_ndim(gnb, 2, dtype='i1')\n    gna = memoryview_safe(gna)\n    gnb = memoryview_safe(gnb)\n\n    # compute correlation coefficients\n    r = gn_pairwise2_corrcoef_int8(gna, gnb)\n\n    # convenience for singletons\n    if r.size == 1:\n        r = r[0, 0]\n\n    return r", "response": "Estimate the linkage disequilibrium parameter r for each pair of genotypes gna and gnb."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlocate unlinked genotypes in a given approximate linkage equilibrium.", "response": "def locate_unlinked(gn, size=100, step=20, threshold=.1, blen=None):\n    \"\"\"Locate variants in approximate linkage equilibrium, where r**2 is\n    below the given `threshold`.\n\n    Parameters\n    ----------\n    gn : array_like, int8, shape (n_variants, n_samples)\n        Diploid genotypes at biallelic variants, coded as the number of\n        alternate alleles per call (i.e., 0 = hom ref, 1 = het, 2 = hom alt).\n    size : int\n        Window size (number of variants).\n    step : int\n        Number of variants to advance to the next window.\n    threshold : float\n        Maximum value of r**2 to include variants.\n    blen : int, optional\n        Block length to use for chunked computation.\n\n    Returns\n    -------\n    loc : ndarray, bool, shape (n_variants)\n        Boolean array where True items locate variants in approximate\n        linkage equilibrium.\n\n    Notes\n    -----\n    The value of r**2 between each pair of variants is calculated using the\n    method of Rogers and Huff (2008).\n\n    \"\"\"\n\n    # check inputs\n    if not hasattr(gn, 'shape') or not hasattr(gn, 'dtype'):\n        gn = np.asarray(gn, dtype='i1')\n    if gn.ndim != 2:\n        raise ValueError('gn must have two dimensions')\n\n    # setup output\n    loc = np.ones(gn.shape[0], dtype='u1')\n\n    # compute in chunks to avoid loading big arrays into memory\n    blen = get_blen_array(gn, blen)\n    blen = max(blen, 10*size)  # avoid too small chunks\n    n_variants = gn.shape[0]\n    for i in range(0, n_variants, blen):\n        # N.B., ensure overlap with next window\n        j = min(n_variants, i+blen+size)\n        gnb = np.asarray(gn[i:j], dtype='i1')\n        gnb = memoryview_safe(gnb)\n        locb = loc[i:j]\n        gn_locate_unlinked_int8(gnb, locb, size, step, threshold)\n\n    return loc.astype('b1')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsummarises linkage disequilibrium in windows over a single base.", "response": "def windowed_r_squared(pos, gn, size=None, start=None, stop=None, step=None,\n                       windows=None, fill=np.nan, percentile=50):\n    \"\"\"Summarise linkage disequilibrium in windows over a single\n    chromosome/contig.\n\n    Parameters\n    ----------\n    pos : array_like, int, shape (n_items,)\n        The item positions in ascending order, using 1-based coordinates..\n    gn : array_like, int8, shape (n_variants, n_samples)\n        Diploid genotypes at biallelic variants, coded as the number of\n        alternate alleles per call (i.e., 0 = hom ref, 1 = het, 2 = hom alt).\n    size : int, optional\n        The window size (number of bases).\n    start : int, optional\n        The position at which to start (1-based).\n    stop : int, optional\n        The position at which to stop (1-based).\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    windows : array_like, int, shape (n_windows, 2), optional\n        Manually specify the windows to use as a sequence of (window_start,\n        window_stop) positions, using 1-based coordinates. Overrides the\n        size/start/stop/step parameters.\n    fill : object, optional\n        The value to use where a window is empty, i.e., contains no items.\n    percentile : int or sequence of ints, optional\n        The percentile or percentiles to calculate within each window.\n\n    Returns\n    -------\n    out : ndarray, shape (n_windows,)\n        The value of the statistic for each window.\n    windows : ndarray, int, shape (n_windows, 2)\n        The windows used, as an array of (window_start, window_stop) positions,\n        using 1-based coordinates.\n    counts : ndarray, int, shape (n_windows,)\n        The number of items in each window.\n\n    Notes\n    -----\n    Linkage disequilibrium (r**2) is calculated using the method of Rogers\n    and Huff (2008).\n\n    See Also\n    --------\n\n    allel.stats.window.windowed_statistic\n\n    \"\"\"\n\n    # define the statistic function\n    if isinstance(percentile, (list, tuple)):\n        fill = [fill for _ in percentile]\n\n        def statistic(gnw):\n            r_squared = rogers_huff_r(gnw) ** 2\n            return [np.percentile(r_squared, p) for p in percentile]\n\n    else:\n        def statistic(gnw):\n            r_squared = rogers_huff_r(gnw) ** 2\n            return np.percentile(r_squared, percentile)\n\n    return windowed_statistic(pos, gn, statistic, size, start=start,\n                              stop=stop, step=step, windows=windows, fill=fill)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_pairwise_ld(m, colorbar=True, ax=None, imshow_kwargs=None):\n\n    import matplotlib.pyplot as plt\n\n    # check inputs\n    m_square = ensure_square(m)\n\n    # blank out lower triangle and flip up/down\n    m_square = np.tril(m_square)[::-1, :]\n\n    # set up axes\n    if ax is None:\n        # make a square figure with enough pixels to represent each variant\n        x = m_square.shape[0] / plt.rcParams['figure.dpi']\n        x = max(x, plt.rcParams['figure.figsize'][0])\n        fig, ax = plt.subplots(figsize=(x, x))\n        fig.tight_layout(pad=0)\n\n    # setup imshow arguments\n    if imshow_kwargs is None:\n        imshow_kwargs = dict()\n    imshow_kwargs.setdefault('interpolation', 'none')\n    imshow_kwargs.setdefault('cmap', 'Greys')\n    imshow_kwargs.setdefault('vmin', 0)\n    imshow_kwargs.setdefault('vmax', 1)\n\n    # plot as image\n    im = ax.imshow(m_square, **imshow_kwargs)\n\n    # tidy up\n    ax.set_xticks([])\n    ax.set_yticks([])\n    for s in 'bottom', 'right':\n        ax.spines[s].set_visible(False)\n    if colorbar:\n        plt.gcf().colorbar(im, shrink=.5, pad=0)\n\n    return ax", "response": "Plot a matrix of genotype linkage disequilibrium values between all pairs of variants."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites a Numpy array to an HDF5 dataset.", "response": "def array_to_hdf5(a, parent, name, **kwargs):\n    \"\"\"Write a Numpy array to an HDF5 dataset.\n\n    Parameters\n    ----------\n    a : ndarray\n        Data to write.\n    parent : string or h5py group\n        Parent HDF5 file or group. If a string, will be treated as HDF5 file\n        name.\n    name : string\n        Name or path of dataset to write data into.\n    kwargs : keyword arguments\n        Passed through to h5py require_dataset() function.\n\n    Returns\n    -------\n    h5d : h5py dataset\n\n    \"\"\"\n\n    import h5py\n\n    h5f = None\n\n    if isinstance(parent, str):\n        h5f = h5py.File(parent, mode='a')\n        parent = h5f\n\n    try:\n\n        kwargs.setdefault('chunks', True)  # auto-chunking\n        kwargs.setdefault('dtype', a.dtype)\n        kwargs.setdefault('compression', 'gzip')\n        h5d = parent.require_dataset(name, shape=a.shape, **kwargs)\n        h5d[...] = a\n        return h5d\n\n    finally:\n        if h5f is not None:\n            h5f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a recarray from columns stored as separate datasets with an HDF5 group.", "response": "def recarray_from_hdf5_group(*args, **kwargs):\n    \"\"\"Load a recarray from columns stored as separate datasets with an\n    HDF5 group.\n\n    Either provide an h5py group as a single positional argument,\n    or provide two positional arguments giving the HDF5 file path and the\n    group node path within the file.\n\n    The following optional parameters may be given.\n\n    Parameters\n    ----------\n    start : int, optional\n        Index to start loading from.\n    stop : int, optional\n        Index to finish loading at.\n    condition : array_like, bool, optional\n        A 1-dimensional boolean array of the same length as the columns of the\n        table to load, indicating a selection of rows to load.\n\n    \"\"\"\n\n    import h5py\n\n    h5f = None\n\n    if len(args) == 1:\n        group = args[0]\n\n    elif len(args) == 2:\n        file_path, node_path = args\n        h5f = h5py.File(file_path, mode='r')\n        try:\n            group = h5f[node_path]\n        except Exception as e:\n            h5f.close()\n            raise e\n\n    else:\n        raise ValueError('bad arguments; expected group or (file_path, '\n                         'node_path), found %s' % repr(args))\n\n    try:\n\n        if not isinstance(group, h5py.Group):\n            raise ValueError('expected group, found %r' % group)\n\n        # determine dataset names to load\n        available_dataset_names = [n for n in group.keys()\n                                   if isinstance(group[n], h5py.Dataset)]\n        names = kwargs.pop('names', available_dataset_names)\n        names = [str(n) for n in names]  # needed for PY2\n        for n in names:\n            if n not in set(group.keys()):\n                raise ValueError('name not found: %s' % n)\n            if not isinstance(group[n], h5py.Dataset):\n                raise ValueError('name does not refer to a dataset: %s, %r'\n                                 % (n, group[n]))\n\n        # check datasets are aligned\n        datasets = [group[n] for n in names]\n        length = datasets[0].shape[0]\n        for d in datasets[1:]:\n            if d.shape[0] != length:\n                raise ValueError('datasets must be of equal length')\n\n        # determine start and stop parameters for load\n        start = kwargs.pop('start', 0)\n        stop = kwargs.pop('stop', length)\n\n        # check condition\n        condition = kwargs.pop('condition', None)  # type: np.ndarray\n        condition = asarray_ndim(condition, 1, allow_none=True)\n        if condition is not None and condition.size != length:\n            raise ValueError('length of condition does not match length '\n                             'of datasets')\n\n        # setup output data\n        dtype = [(n, d.dtype, d.shape[1:]) for n, d in zip(names, datasets)]\n        ra = np.empty(length, dtype=dtype)\n\n        for n, d in zip(names, datasets):\n            a = d[start:stop]\n            if condition is not None:\n                a = np.compress(condition[start:stop], a, axis=0)\n            ra[n] = a\n\n        return ra\n\n    finally:\n        if h5f is not None:\n            h5f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recarray_to_hdf5_group(ra, parent, name, **kwargs):\n\n    import h5py\n\n    h5f = None\n\n    if isinstance(parent, str):\n        h5f = h5py.File(parent, mode='a')\n        parent = h5f\n\n    try:\n\n        h5g = parent.require_group(name)\n        for n in ra.dtype.names:\n            array_to_hdf5(ra[n], h5g, n, **kwargs)\n\n        return h5g\n\n    finally:\n        if h5f is not None:\n            h5f.close()", "response": "Write each column in a recarray to an HDF5 group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subset(data, sel0, sel1):\n\n    # check inputs\n    data = np.asarray(data)\n    if data.ndim < 2:\n        raise ValueError('data must have 2 or more dimensions')\n    sel0 = asarray_ndim(sel0, 1, allow_none=True)\n    sel1 = asarray_ndim(sel1, 1, allow_none=True)\n\n    # ensure indices\n    if sel0 is not None and sel0.dtype.kind == 'b':\n        sel0, = np.nonzero(sel0)\n    if sel1 is not None and sel1.dtype.kind == 'b':\n        sel1, = np.nonzero(sel1)\n\n    # ensure leading dimension indices can be broadcast correctly\n    if sel0 is not None and sel1 is not None:\n        sel0 = sel0[:, np.newaxis]\n\n    # deal with None arguments\n    if sel0 is None:\n        sel0 = _total_slice\n    if sel1 is None:\n        sel1 = _total_slice\n\n    return data[sel0, sel1]", "response": "Apply selections on first and second axes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate an expression against the table columns.", "response": "def eval(self, expression, vm='python'):\n        \"\"\"Evaluate an expression against the table columns.\n\n        Parameters\n        ----------\n        expression : string\n            Expression to evaluate.\n        vm : {'numexpr', 'python'}\n            Virtual machine to use.\n\n        Returns\n        -------\n        result : ndarray\n\n        \"\"\"\n\n        if vm == 'numexpr':\n            import numexpr as ne\n            return ne.evaluate(expression, local_dict=self)\n        else:\n            if PY2:\n                # locals must be a mapping\n                m = {k: self[k] for k in self.dtype.names}\n            else:\n                m = self\n            return eval(expression, dict(), m)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nevaluating expression and then use it to extract rows from the table.", "response": "def query(self, expression, vm='python'):\n        \"\"\"Evaluate expression and then use it to extract rows from the table.\n\n        Parameters\n        ----------\n        expression : string\n            Expression to evaluate.\n        vm : {'numexpr', 'python'}\n            Virtual machine to use.\n\n        Returns\n        -------\n        result : structured array\n\n        \"\"\"\n\n        condition = self.eval(expression, vm=vm)\n        return self.compress(condition)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfills the masked genotype calls with a given value.", "response": "def fill_masked(self, value=-1, copy=True):\n        \"\"\"Fill masked genotype calls with a given value.\n\n        Parameters\n        ----------\n        value : int, optional\n            The fill value.\n        copy : bool, optional\n            If False, modify the array in place.\n\n        Returns\n        -------\n        g : GenotypeArray\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 1], [1, 1]],\n        ...                          [[0, 2], [-1, -1]]], dtype='i1')\n        >>> mask = [[True, False], [False, True], [False, False]]\n        >>> g.mask = mask\n        >>> g.fill_masked().values\n        array([[[-1, -1],\n                [ 0,  1]],\n               [[ 0,  1],\n                [-1, -1]],\n               [[ 0,  2],\n                [-1, -1]]], dtype=int8)\n\n        \"\"\"\n\n        if self.mask is None:\n            raise ValueError('no mask is set')\n\n        # apply the mask\n        data = np.array(self.values, copy=copy)\n        data[self.mask, ...] = value\n\n        if copy:\n            out = type(self)(data)  # wrap\n            out.is_phased = self.is_phased\n            # don't set mask because it has been filled in\n        else:\n            out = self\n            out.mask = None  # reset mask\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_called(self):\n\n        out = np.all(self.values >= 0, axis=-1)\n\n        # handle mask\n        if self.mask is not None:\n            out &= ~self.mask\n\n        return out", "response": "Find non - missing genotype calls."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds missing genotype calls.", "response": "def is_missing(self):\n        \"\"\"Find missing genotype calls.\n\n        Returns\n        -------\n        out : ndarray, bool, shape (n_variants, n_samples)\n            Array where elements are True if the genotype call matches the\n            condition.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 1], [1, 1]],\n        ...                          [[0, 2], [-1, -1]]])\n        >>> g.is_missing()\n        array([[False, False],\n               [False, False],\n               [False,  True]])\n        >>> v = g[:, 1]\n        >>> v\n        <GenotypeVector shape=(3, 2) dtype=int64>\n        0/1 1/1 ./.\n        >>> v.is_missing()\n        array([False, False,  True])\n\n        \"\"\"\n\n        out = np.any(self.values < 0, axis=-1)\n\n        # handle mask\n        if self.mask is not None:\n            out |= self.mask\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_hom(self, allele=None):\n\n        if allele is None:\n            allele1 = self.values[..., 0, np.newaxis]\n            other_alleles = self.values[..., 1:]\n            tmp = (allele1 >= 0) & (allele1 == other_alleles)\n            out = np.all(tmp, axis=-1)\n        else:\n            out = np.all(self.values == allele, axis=-1)\n\n        # handle mask\n        if self.mask is not None:\n            out &= ~self.mask\n\n        return out", "response": "Find genotype calls that are homozygous."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds genotype calls that are homozygous for any alternate allele.", "response": "def is_hom_alt(self):\n        \"\"\"Find genotype calls that are homozygous for any alternate (i.e.,\n        non-reference) allele.\n\n        Returns\n        -------\n        out : ndarray, bool, shape (n_variants, n_samples)\n            Array where elements are True if the genotype call matches the\n            condition.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 1], [1, 1]],\n        ...                          [[2, 2], [-1, -1]]])\n        >>> g.is_hom_alt()\n        array([[False, False],\n               [False,  True],\n               [ True, False]])\n        >>> v = g[:, 1]\n        >>> v\n        <GenotypeVector shape=(3, 2) dtype=int64>\n        0/1 1/1 ./.\n        >>> v.is_hom_alt()\n        array([False,  True, False])\n\n        \"\"\"\n\n        allele1 = self.values[..., 0, np.newaxis]\n        other_alleles = self.values[..., 1:]\n        tmp = (allele1 > 0) & (allele1 == other_alleles)\n        out = np.all(tmp, axis=-1)\n\n        # handle mask\n        if self.mask is not None:\n            out &= ~self.mask\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_het(self, allele=None):\n\n        allele1 = self.values[..., 0, np.newaxis]  # type: np.ndarray\n        other_alleles = self.values[..., 1:]  # type: np.ndarray\n        out = np.all(self.values >= 0, axis=-1) & np.any(allele1 != other_alleles, axis=-1)\n        if allele is not None:\n            out &= np.any(self.values == allele, axis=-1)\n\n        # handle mask\n        if self.mask is not None:\n            out &= ~self.mask\n\n        return out", "response": "Find genotype calls that are heterozygous."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the genotype with a given call is present in the local cache.", "response": "def is_call(self, call):\n        \"\"\"Locate genotypes with a given call.\n\n        Parameters\n        ----------\n        call : array_like, int, shape (ploidy,)\n            The genotype call to find.\n\n        Returns\n        -------\n        out : ndarray, bool, shape (n_variants, n_samples)\n            Array where elements are True if the genotype is `call`.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 1], [1, 1]],\n        ...                          [[0, 2], [-1, -1]]])\n        >>> g.is_call((0, 2))\n        array([[False, False],\n               [False, False],\n               [ True, False]])\n        >>> v = g[:, 0]\n        >>> v\n        <GenotypeVector shape=(3, 2) dtype=int64>\n        0/0 0/1 0/2\n        >>> v.is_call((0, 2))\n        array([False, False,  True])\n\n        \"\"\"\n\n        # guard conditions\n        if not len(call) == self.shape[-1]:\n            raise ValueError('invalid call ploidy: %s', repr(call))\n\n        if self.ndim == 2:\n            call = np.asarray(call)[np.newaxis, :]\n        else:\n            call = np.asarray(call)[np.newaxis, np.newaxis, :]\n        out = np.all(self.values == call, axis=-1)\n\n        # handle mask\n        if self.mask is not None:\n            out &= ~self.mask\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_called(self, axis=None):\n        b = self.is_called()\n        return np.sum(b, axis=axis)", "response": "Count the number of genotypes called over a given axis."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncounting missing genotypes. Parameters ---------- axis : int, optional Axis over which to count, or None to perform overall count.", "response": "def count_missing(self, axis=None):\n        \"\"\"Count missing genotypes.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis over which to count, or None to perform overall count.\n\n        \"\"\"\n        b = self.is_missing()\n        return np.sum(b, axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncount homozygous genotypes. Parameters ---------- allele : int, optional Allele index. axis : int, optional Axis over which to count, or None to perform overall count.", "response": "def count_hom(self, allele=None, axis=None):\n        \"\"\"Count homozygous genotypes.\n\n        Parameters\n        ----------\n        allele : int, optional\n            Allele index.\n        axis : int, optional\n            Axis over which to count, or None to perform overall count.\n\n        \"\"\"\n        b = self.is_hom(allele=allele)\n        return np.sum(b, axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncounting homozygous reference genotypes.", "response": "def count_hom_ref(self, axis=None):\n        \"\"\"Count homozygous reference genotypes.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis over which to count, or None to perform overall count.\n\n        \"\"\"\n        b = self.is_hom_ref()\n        return np.sum(b, axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef count_hom_alt(self, axis=None):\n        b = self.is_hom_alt()\n        return np.sum(b, axis=axis)", "response": "Count homozygous alternate genotypes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_het(self, allele=None, axis=None):\n        b = self.is_het(allele=allele)\n        return np.sum(b, axis=axis)", "response": "Count heterozygous genotypes.\n\n        Parameters\n        ----------\n        allele : int, optional\n            Allele index.\n        axis : int, optional\n            Axis over which to count, or None to perform overall count."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncounts the number of genotypes with a given call.", "response": "def count_call(self, call, axis=None):\n        \"\"\"Count genotypes with a given call.\n\n        Parameters\n        ----------\n        call : array_like, int, shape (ploidy,)\n            The genotype call to find.\n        axis : int, optional\n            Axis over which to count, or None to perform overall count.\n\n        \"\"\"\n        b = self.is_call(call=call)\n        return np.sum(b, axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform each genotype call into the number of reference alleles.", "response": "def to_n_ref(self, fill=0, dtype='i1'):\n        \"\"\"Transform each genotype call into the number of\n        reference alleles.\n\n        Parameters\n        ----------\n        fill : int, optional\n            Use this value to represent missing calls.\n        dtype : dtype, optional\n            Output dtype.\n\n        Returns\n        -------\n        out : ndarray, int8, shape (n_variants, n_samples)\n            Array of ref alleles per genotype call.\n\n        Notes\n        -----\n        By default this function returns 0 for missing genotype calls\n        **and** for homozygous non-reference genotype calls. Use the\n        `fill` argument to change how missing calls are represented.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 2], [1, 1]],\n        ...                          [[2, 2], [-1, -1]]])\n        >>> g.to_n_ref()\n        array([[2, 1],\n               [1, 0],\n               [0, 0]], dtype=int8)\n        >>> g.to_n_ref(fill=-1)\n        array([[ 2,  1],\n               [ 1,  0],\n               [ 0, -1]], dtype=int8)\n        >>> v = g[:, 0]\n        >>> v\n        <GenotypeVector shape=(3, 2) dtype=int64>\n        0/0 0/2 2/2\n        >>> v.to_n_ref()\n        array([2, 1, 0], dtype=int8)\n\n        \"\"\"\n\n        # count number of alternate alleles\n        out = np.empty(self.shape[:-1], dtype=dtype)\n        np.sum(self.values == 0, axis=-1, out=out)\n\n        # fill missing calls\n        if fill != 0:\n            m = self.is_missing()\n            out[m] = fill\n\n        # handle mask\n        if self.mask is not None:\n            out[self.mask] = fill\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransforming genotype calls into allele counts per call.", "response": "def to_allele_counts(self, max_allele=None, dtype='u1'):\n        \"\"\"Transform genotype calls into allele counts per call.\n\n        Parameters\n        ----------\n        max_allele : int, optional\n            Highest allele index. Provide this value to speed up computation.\n        dtype : dtype, optional\n            Output dtype.\n\n        Returns\n        -------\n        out : ndarray, uint8, shape (n_variants, n_samples, len(alleles))\n            Array of allele counts per call.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 2], [1, 1]],\n        ...                          [[2, 2], [-1, -1]]])\n        >>> g.to_allele_counts()\n        <GenotypeAlleleCountsArray shape=(3, 2, 3) dtype=uint8>\n        2:0:0 1:1:0\n        1:0:1 0:2:0\n        0:0:2 0:0:0\n        >>> v = g[:, 0]\n        >>> v\n        <GenotypeVector shape=(3, 2) dtype=int64>\n        0/0 0/2 2/2\n        >>> v.to_allele_counts()\n        <GenotypeAlleleCountsVector shape=(3, 3) dtype=uint8>\n        2:0:0 1:0:1 0:0:2\n\n        \"\"\"\n\n        # determine alleles to count\n        if max_allele is None:\n            max_allele = self.max()\n        alleles = list(range(max_allele + 1))\n\n        # set up output array\n        outshape = self.shape[:-1] + (len(alleles),)\n        out = np.zeros(outshape, dtype=dtype)\n\n        for allele in alleles:\n            # count alleles along ploidy dimension\n            allele_match = self.values == allele\n            if self.mask is not None:\n                allele_match &= ~self.mask[..., np.newaxis]\n            np.sum(allele_match, axis=-1, out=out[..., allele])\n\n        if self.ndim == 2:\n            out = GenotypeAlleleCountsVector(out)\n        elif self.ndim == 3:\n            out = GenotypeAlleleCountsArray(out)\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_gt(self, max_allele=None):\n\n        # how many characters needed per allele call?\n        if max_allele is None:\n            max_allele = np.max(self)\n        if max_allele <= 0:\n            max_allele = 1\n        nchar = int(np.floor(np.log10(max_allele))) + 1\n\n        # convert to string\n        a = self.astype((np.string_, nchar)).view(np.chararray)\n\n        # recode missing alleles\n        a[self < 0] = b'.'\n        if self.mask is not None:\n            a[self.mask] = b'.'\n\n        # determine allele call separator\n        if self.is_phased is None:\n            sep = b'/'\n        else:\n            sep = np.empty(self.shape[:-1], dtype='S1').view(np.chararray)\n            sep[self.is_phased] = b'|'\n            sep[~self.is_phased] = b'/'\n\n        # join via separator, coping with any ploidy\n        gt = a[..., 0]\n        for i in range(1, self.ploidy):\n            gt = gt + sep + a[..., i]\n\n        return gt", "response": "Convert genotype calls to VCF - style string representation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform alleles via a mapping.", "response": "def map_alleles(self, mapping, copy=True):\n        \"\"\"Transform alleles via a mapping.\n\n        Parameters\n        ----------\n        mapping : ndarray, int8, shape (n_variants, max_allele)\n            An array defining the allele mapping for each variant.\n        copy : bool, optional\n            If True, return a new array; if False, apply mapping in place\n            (only applies for arrays with dtype int8; all other dtypes\n            require a copy).\n\n        Returns\n        -------\n        gm : GenotypeArray\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> import numpy as np\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 2], [1, 1]],\n        ...                          [[1, 2], [2, 1]],\n        ...                          [[2, 2], [-1, -1]]], dtype='i1')\n        >>> mapping = np.array([[1, 2, 0],\n        ...                     [2, 0, 1],\n        ...                     [2, 1, 0],\n        ...                     [0, 2, 1]], dtype='i1')\n        >>> g.map_alleles(mapping)\n        <GenotypeArray shape=(4, 2, 2) dtype=int8>\n        1/1 1/2\n        2/1 0/0\n        1/0 0/1\n        1/1 ./.\n        >>> v = g[:, 0]\n        >>> v\n        <GenotypeVector shape=(4, 2) dtype=int8>\n        0/0 0/2 1/2 2/2\n        >>> v.map_alleles(mapping)\n        <GenotypeVector shape=(4, 2) dtype=int8>\n        1/1 2/1 1/0 1/1\n\n        Notes\n        -----\n        If a mask has been set, it is ignored by this function.\n\n        For arrays with dtype int8 an optimised implementation is used which is\n        faster and uses far less memory. It is recommended to convert arrays to\n        dtype int8 where possible before calling this method.\n\n        See Also\n        --------\n        create_allele_mapping\n\n        \"\"\"\n\n        h = self.to_haplotypes()\n        hm = h.map_alleles(mapping, copy=copy)\n        if self.ndim == 2:\n            gm = GenotypeVector(hm)\n        else:\n            gm = hm.to_genotypes(ploidy=self.ploidy)\n        return gm"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npack the diploid genotypes into a single byte array.", "response": "def to_packed(self, boundscheck=True):\n        \"\"\"Pack diploid genotypes into a single byte for each genotype,\n        using the left-most 4 bits for the first allele and the right-most 4\n        bits for the second allele. Allows single byte encoding of diploid\n        genotypes for variants with up to 15 alleles.\n\n        Parameters\n        ----------\n        boundscheck : bool, optional\n            If False, do not check that minimum and maximum alleles are\n            compatible with bit-packing.\n\n        Returns\n        -------\n        packed : ndarray, uint8, shape (n_variants, n_samples)\n            Bit-packed genotype array.\n\n        Notes\n        -----\n        If a mask has been set, it is ignored by this function.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 2], [1, 1]],\n        ...                          [[2, 2], [-1, -1]]], dtype='i1')\n        >>> g.to_packed()\n        array([[  0,   1],\n               [  2,  17],\n               [ 34, 239]], dtype=uint8)\n\n        \"\"\"\n\n        check_ploidy(self.ploidy, 2)\n\n        if boundscheck:\n            amx = self.max()\n            if amx > 14:\n                raise ValueError('max allele for packing is 14, found %s' % amx)\n            amn = self.min()\n            if amn < -1:\n                raise ValueError('min allele for packing is -1, found %s' % amn)\n\n        # pack data\n        values = memoryview_safe(self.values)\n        packed = genotype_array_pack_diploid(values)\n\n        return packed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nunpack diploid genotypes that have been bit - packed into single bytes.", "response": "def from_packed(cls, packed):\n        \"\"\"Unpack diploid genotypes that have been bit-packed into single\n        bytes.\n\n        Parameters\n        ----------\n        packed : ndarray, uint8, shape (n_variants, n_samples)\n            Bit-packed diploid genotype array.\n\n        Returns\n        -------\n        g : GenotypeArray, shape (n_variants, n_samples, 2)\n            Genotype array.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> import numpy as np\n        >>> packed = np.array([[0, 1],\n        ...                    [2, 17],\n        ...                    [34, 239]], dtype='u1')\n        >>> allel.GenotypeArray.from_packed(packed)\n        <GenotypeArray shape=(3, 2, 2) dtype=int8>\n        0/0 0/1\n        0/2 1/1\n        2/2 ./.\n\n        \"\"\"\n\n        # check arguments\n        packed = np.asarray(packed)\n        check_ndim(packed, 2)\n        check_dtype(packed, 'u1')\n        packed = memoryview_safe(packed)\n\n        data = genotype_array_unpack_diploid(packed)\n        return cls(data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_sparse(self, format='csr', **kwargs):\n\n        h = self.to_haplotypes()\n        m = h.to_sparse(format=format, **kwargs)\n        return m", "response": "Convert the current object into a sparse matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_sparse(m, ploidy, order=None, out=None):\n\n        h = HaplotypeArray.from_sparse(m, order=order, out=out)\n        g = h.to_genotypes(ploidy=ploidy)\n        return g", "response": "Construct a genotype array from a sparse matrix."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct a pseudo - haplotype for each sample by randomly selecting an allele from each genotype call.", "response": "def haploidify_samples(self):\n        \"\"\"Construct a pseudo-haplotype for each sample by randomly\n        selecting an allele from each genotype call.\n\n        Returns\n        -------\n        h : HaplotypeArray\n\n        Notes\n        -----\n        If a mask has been set, it is ignored by this function.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> import numpy as np\n        >>> np.random.seed(42)\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 2], [1, 1]],\n        ...                          [[1, 2], [2, 1]],\n        ...                          [[2, 2], [-1, -1]]])\n        >>> g.haploidify_samples()\n        <HaplotypeArray shape=(4, 2) dtype=int64>\n        0 1\n        0 1\n        1 1\n        2 .\n        >>> g = allel.GenotypeArray([[[0, 0, 0], [0, 0, 1]],\n        ...                          [[0, 1, 1], [1, 1, 1]],\n        ...                          [[0, 1, 2], [-1, -1, -1]]])\n        >>> g.haploidify_samples()\n        <HaplotypeArray shape=(3, 2) dtype=int64>\n        0 0\n        1 1\n        2 .\n\n        \"\"\"\n\n        # N.B., this implementation is obscure and uses more memory than\n        # necessary, TODO review\n\n        # define the range of possible indices, e.g., diploid => (0, 1)\n        index_range = np.arange(0, self.ploidy, dtype='u1')\n\n        # create a random index for each genotype call\n        indices = np.random.choice(index_range, size=self.n_calls, replace=True)\n\n        # reshape genotype data so it's suitable for passing to np.choose\n        # by merging the variants and samples dimensions\n        choices = self.reshape(-1, self.ploidy).T\n\n        # now use random indices to haploidify\n        data = np.choose(indices, choices)\n\n        # reshape the haploidified data to restore the variants and samples\n        # dimensions\n        data = data.reshape((self.n_variants, self.n_samples))\n\n        # view as haplotype array\n        h = HaplotypeArray(data, copy=False)\n\n        return h"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncounting the number of calls of each allele per variant.", "response": "def count_alleles(self, max_allele=None, subpop=None):\n        \"\"\"Count the number of calls of each allele per variant.\n\n        Parameters\n        ----------\n        max_allele : int, optional\n            The highest allele index to count. Alleles above this will be\n            ignored.\n        subpop : sequence of ints, optional\n            Indices of samples to include in count.\n\n        Returns\n        -------\n        ac : AlleleCountsArray\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 2], [1, 1]],\n        ...                          [[2, 2], [-1, -1]]])\n        >>> g.count_alleles()\n        <AlleleCountsArray shape=(3, 3) dtype=int32>\n        3 1 0\n        1 2 1\n        0 0 2\n        >>> g.count_alleles(max_allele=1)\n        <AlleleCountsArray shape=(3, 2) dtype=int32>\n        3 1\n        1 2\n        0 0\n\n        \"\"\"\n\n        # check inputs\n        subpop = _normalize_subpop_arg(subpop, self.shape[1])\n\n        # determine alleles to count\n        if max_allele is None:\n            max_allele = self.max()\n\n        # use optimisations\n        values = memoryview_safe(self.values)\n        mask = memoryview_safe(self.mask).view(dtype='u1') if self.mask is not None else None\n        if subpop is None and mask is None:\n            ac = genotype_array_count_alleles(values, max_allele)\n        elif subpop is None:\n            ac = genotype_array_count_alleles_masked(values, mask, max_allele)\n        elif mask is None:\n            ac = genotype_array_count_alleles_subpop(values, max_allele, subpop)\n        else:\n            ac = genotype_array_count_alleles_subpop_masked(values, mask, max_allele, subpop)\n\n        return AlleleCountsArray(ac, copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncounts alleles for multiple subpopulations simultaneously.", "response": "def count_alleles_subpops(self, subpops, max_allele=None):\n        \"\"\"Count alleles for multiple subpopulations simultaneously.\n\n        Parameters\n        ----------\n        subpops : dict (string -> sequence of ints)\n            Mapping of subpopulation names to sample indices.\n        max_allele : int, optional\n            The highest allele index to count. Alleles above this will be\n            ignored.\n\n        Returns\n        -------\n        out : dict (string -> AlleleCountsArray)\n            A mapping of subpopulation names to allele counts arrays.\n\n        \"\"\"\n\n        if max_allele is None:\n            max_allele = self.max()\n\n        out = {name: self.count_alleles(max_allele=max_allele, subpop=subpop)\n               for name, subpop in subpops.items()}\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compress(self, condition, axis=0, out=None):\n        return compress_haplotype_array(self, condition, axis=axis, cls=type(self),\n                                        compress=np.compress, out=out)", "response": "Return selected slices of an array along given axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes elements from an array along an axis.", "response": "def take(self, indices, axis=0, out=None, mode='raise'):\n        \"\"\"Take elements from an array along an axis.\n\n        This function does the same thing as \"fancy\" indexing (indexing arrays\n        using arrays); however, it can be easier to use if you need elements\n        along a given axis.\n\n        Parameters\n        ----------\n        indices : array_like\n            The indices of the values to extract.\n        axis : int, optional\n            The axis over which to select values.\n        out : ndarray, optional\n            If provided, the result will be placed in this array. It should\n            be of the appropriate shape and dtype.\n        mode : {'raise', 'wrap', 'clip'}, optional\n            Specifies how out-of-bounds indices will behave.\n\n            * 'raise' -- raise an error (default)\n            * 'wrap' -- wrap around\n            * 'clip' -- clip to the range\n\n            'clip' mode means that all indices that are too large are replaced\n            by the index that addresses the last element along that axis. Note\n            that this disables indexing with negative numbers.\n\n        Returns\n        -------\n        subarray : ndarray\n\n        Examples\n        --------\n        >>> import allel\n        >>> h = allel.HaplotypeArray([[0, 0, 0, 1],\n        ...                           [0, 1, 1, 1],\n        ...                           [0, 2, -1, -1]], dtype='i1')\n        >>> h.take([0, 2], axis=0)\n        <HaplotypeArray shape=(2, 4) dtype=int8>\n        0 0 0 1\n        0 2 . .\n        >>> h.take([0, 2], axis=1)\n        <HaplotypeArray shape=(3, 2) dtype=int8>\n        0 0\n        0 1\n        0 .\n\n        \"\"\"\n\n        return take_haplotype_array(self, indices, axis=axis, cls=type(self), take=np.take,\n                                    out=out, mode=mode)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subset(self, sel0=None, sel1=None):\n        return subset_haplotype_array(self, sel0, sel1, cls=type(self), subset=subset)", "response": "Make a sub - selection of variants and haplotypes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef concatenate(self, others, axis=0):\n        return concatenate_haplotype_array(self, others, axis=axis, cls=type(self),\n                                           concatenate=np.concatenate)", "response": "Concatenate a sequence of arrays along an existing axis."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_genotypes(self, ploidy, copy=False):\n\n        # check ploidy is compatible\n        if (self.shape[1] % ploidy) > 0:\n            raise ValueError('incompatible ploidy')\n\n        # reshape\n        newshape = (self.shape[0], -1, ploidy)\n        data = self.reshape(newshape)\n\n        # wrap\n        g = GenotypeArray(data, copy=copy)\n\n        return g", "response": "Reshape a haplotype array to view it as genotypes by restoring the ploidy dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the current instance into a sparse matrix.", "response": "def to_sparse(self, format='csr', **kwargs):\n        \"\"\"Convert into a sparse matrix.\n\n        Parameters\n        ----------\n        format : {'coo', 'csc', 'csr', 'dia', 'dok', 'lil'}\n            Sparse matrix format.\n        kwargs : keyword arguments\n            Passed through to sparse matrix constructor.\n\n        Returns\n        -------\n        m : scipy.sparse.spmatrix\n            Sparse matrix\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> h = allel.HaplotypeArray([[0, 0, 0, 0],\n        ...                           [0, 1, 0, 1],\n        ...                           [1, 1, 0, 0],\n        ...                           [0, 0, -1, -1]], dtype='i1')\n        >>> m = h.to_sparse(format='csr')\n        >>> m\n        <4x4 sparse matrix of type '<class 'numpy.int8'>'\n            with 6 stored elements in Compressed Sparse Row format>\n        >>> m.data\n        array([ 1,  1,  1,  1, -1, -1], dtype=int8)\n        >>> m.indices\n        array([1, 3, 0, 1, 2, 3], dtype=int32)\n        >>> m.indptr\n        array([0, 0, 2, 4, 6], dtype=int32)\n\n        \"\"\"\n\n        import scipy.sparse\n\n        # check arguments\n        f = {\n            'bsr': scipy.sparse.bsr_matrix,\n            'coo': scipy.sparse.coo_matrix,\n            'csc': scipy.sparse.csc_matrix,\n            'csr': scipy.sparse.csr_matrix,\n            'dia': scipy.sparse.dia_matrix,\n            'dok': scipy.sparse.dok_matrix,\n            'lil': scipy.sparse.lil_matrix\n        }\n        if format not in f:\n            raise ValueError('invalid format: %r' % format)\n\n        # create sparse matrix\n        m = f[format](self, **kwargs)\n\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a HaplotypeArray from a sparse matrix.", "response": "def from_sparse(m, order=None, out=None):\n        \"\"\"Construct a haplotype array from a sparse matrix.\n\n        Parameters\n        ----------\n        m : scipy.sparse.spmatrix\n            Sparse matrix\n        order : {'C', 'F'}, optional\n            Whether to store data in C (row-major) or Fortran (column-major)\n            order in memory.\n        out : ndarray, shape (n_variants, n_samples), optional\n            Use this array as the output buffer.\n\n        Returns\n        -------\n        h : HaplotypeArray, shape (n_variants, n_haplotypes)\n            Haplotype array.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> import numpy as np\n        >>> import scipy.sparse\n        >>> data = np.array([ 1,  1,  1,  1, -1, -1], dtype=np.int8)\n        >>> indices = np.array([1, 3, 0, 1, 2, 3], dtype=np.int32)\n        >>> indptr = np.array([0, 0, 2, 4, 6], dtype=np.int32)\n        >>> m = scipy.sparse.csr_matrix((data, indices, indptr))\n        >>> h = allel.HaplotypeArray.from_sparse(m)\n        >>> h\n        <HaplotypeArray shape=(4, 4) dtype=int8>\n        0 0 0 0\n        0 1 0 1\n        1 1 0 0\n        0 0 . .\n\n        \"\"\"\n\n        import scipy.sparse\n\n        # check arguments\n        if not scipy.sparse.isspmatrix(m):\n            raise ValueError('not a sparse matrix: %r' % m)\n\n        # convert to dense array\n        data = m.toarray(order=order, out=out)\n\n        # wrap\n        h = HaplotypeArray(data)\n\n        return h"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef count_alleles(self, max_allele=None, subpop=None):\n\n        # check inputs\n        subpop = _normalize_subpop_arg(subpop, self.shape[1])\n\n        # determine alleles to count\n        if max_allele is None:\n            max_allele = self.max()\n\n        # use optimisations\n        values = memoryview_safe(self.values)\n        if subpop is None:\n            ac = haplotype_array_count_alleles(values, max_allele)\n\n        else:\n            ac = haplotype_array_count_alleles_subpop(values, max_allele, subpop)\n\n        return AlleleCountsArray(ac, copy=False)", "response": "Count the number of calls of each allele per variant."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms alleles via a mapping.", "response": "def map_alleles(self, mapping, copy=True):\n        \"\"\"Transform alleles via a mapping.\n\n        Parameters\n        ----------\n        mapping : ndarray, int8, shape (n_variants, max_allele)\n            An array defining the allele mapping for each variant.\n        copy : bool, optional\n            If True, return a new array; if False, apply mapping in place\n            (only applies for arrays with dtype int8; all other dtypes\n            require a copy).\n\n        Returns\n        -------\n        hm : HaplotypeArray\n\n        Examples\n        --------\n        >>> import allel\n        >>> import numpy as np\n        >>> h = allel.HaplotypeArray([[0, 0, 0, 1],\n        ...                           [0, 1, 1, 1],\n        ...                           [0, 2, -1, -1]], dtype='i1')\n        >>> mapping = np.array([[1, 2, 0],\n        ...                     [2, 0, 1],\n        ...                     [2, 1, 0]], dtype='i1')\n        >>> h.map_alleles(mapping)\n        <HaplotypeArray shape=(3, 4) dtype=int8>\n        1 1 1 2\n        2 0 0 0\n        2 0 . .\n\n        Notes\n        -----\n        For arrays with dtype int8 an optimised implementation is used which is\n        faster and uses far less memory. It is recommended to convert arrays to\n        dtype int8 where possible before calling this method.\n\n        See Also\n        --------\n        allel.model.util.create_allele_mapping\n\n        \"\"\"\n\n        # check inputs\n        mapping = asarray_ndim(mapping, 2)\n        check_dim0_aligned(self, mapping)\n\n        # use optimisation\n        mapping = np.asarray(mapping, dtype=self.dtype)\n        mapping = memoryview_safe(mapping)\n        values = memoryview_safe(self.values)\n        data = haplotype_array_map_alleles(values, mapping, copy=copy)\n\n        return HaplotypeArray(data, copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef distinct(self):\n\n        # setup collection\n        d = collections.defaultdict(set)\n\n        # iterate over haplotypes\n        for i in range(self.shape[1]):\n\n            # hash the haplotype\n            k = hash(self.values[:, i].tobytes())\n\n            # collect\n            d[k].add(i)\n\n        # extract sets, sorted by most common\n        return sorted(d.values(), key=len, reverse=True)", "response": "Return sets of indices for each distinct haplotype."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning counts for each distinct haplotype.", "response": "def distinct_counts(self):\n        \"\"\"Return counts for each distinct haplotype.\"\"\"\n\n        # hash the haplotypes\n        k = [hash(self.values[:, i].tobytes()) for i in range(self.shape[1])]\n\n        # count and sort\n        # noinspection PyArgumentList\n        counts = sorted(collections.Counter(k).values(), reverse=True)\n\n        return np.asarray(counts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distinct_frequencies(self):\n\n        c = self.distinct_counts()\n        n = self.shape[1]\n        return c / n", "response": "Return frequencies for each distinct haplotype."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the allele frequencies.", "response": "def to_frequencies(self, fill=np.nan):\n        \"\"\"Compute allele frequencies.\n\n        Parameters\n        ----------\n        fill : float, optional\n            Value to use when number of allele calls is 0.\n\n        Returns\n        -------\n        af : ndarray, float, shape (n_variants, n_alleles)\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 2], [1, 1]],\n        ...                          [[2, 2], [-1, -1]]])\n        >>> ac = g.count_alleles()\n        >>> ac.to_frequencies()\n        array([[0.75, 0.25, 0.  ],\n               [0.25, 0.5 , 0.25],\n               [0.  , 0.  , 1.  ]])\n\n        \"\"\"\n\n        an = np.sum(self, axis=1)[:, None]\n        with ignore_invalid():\n            af = np.where(an > 0, self / an, fill)\n\n        return af"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the highest allele index for each variant.", "response": "def max_allele(self):\n        \"\"\"Return the highest allele index for each variant.\n\n        Returns\n        -------\n        n : ndarray, int, shape (n_variants,)\n            Allele index array.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 1]],\n        ...                          [[0, 2], [1, 1]],\n        ...                          [[2, 2], [-1, -1]]])\n        >>> ac = g.count_alleles()\n        >>> ac.max_allele()\n        array([1, 2, 2], dtype=int8)\n\n        \"\"\"\n\n        out = np.empty(self.shape[0], dtype='i1')\n        out.fill(-1)\n        for i in range(self.shape[1]):\n            d = self.values[:, i] > 0\n            out[d] = i\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the set of variants in the log are non - segregating.", "response": "def is_non_segregating(self, allele=None):\n        \"\"\"Find non-segregating variants (where at most one allele is\n        observed).\n\n        Parameters\n        ----------\n        allele : int, optional\n            Allele index.\n\n        Returns\n        -------\n        out : ndarray, bool, shape (n_variants,)\n            Boolean array where elements are True if variant matches the\n            condition.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 0]],\n        ...                          [[0, 0], [0, 1]],\n        ...                          [[0, 2], [1, 1]],\n        ...                          [[2, 2], [-1, -1]]])\n        >>> ac = g.count_alleles()\n        >>> ac.is_non_segregating()\n        array([ True, False, False,  True])\n        >>> ac.is_non_segregating(allele=2)\n        array([False, False, False,  True])\n\n        \"\"\"\n\n        if allele is None:\n            return self.allelism() <= 1\n        else:\n            return (self.allelism() == 1) & (self.values[:, allele] > 0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the sequence contains only biallelic 1 - allele entries.", "response": "def is_biallelic_01(self, min_mac=None):\n        \"\"\"Find variants biallelic for the reference (0) and first alternate\n        (1) allele.\n\n        Parameters\n        ----------\n        min_mac : int, optional\n            Minimum minor allele count.\n\n        Returns\n        -------\n        out : ndarray, bool, shape (n_variants,)\n            Boolean array where elements are True if variant matches the\n            condition.\n\n        \"\"\"\n        loc = self.is_biallelic() & (self.max_allele() == 1)\n        if min_mac is not None:\n            # noinspection PyAugmentAssignment\n            loc = loc & (self.values[:, :2].min(axis=1) >= min_mac)\n        return loc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransforming alleles via a mapping.", "response": "def map_alleles(self, mapping, max_allele=None):\n        \"\"\"Transform alleles via a mapping.\n\n        Parameters\n        ----------\n        mapping : ndarray, int8, shape (n_variants, max_allele)\n            An array defining the allele mapping for each variant.\n        max_allele : int, optional\n            Highest allele index expected in the output. If not provided\n            will be determined from maximum value in `mapping`.\n\n        Returns\n        -------\n        ac : AlleleCountsArray\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> g = allel.GenotypeArray([[[0, 0], [0, 0]],\n        ...                          [[0, 0], [0, 1]],\n        ...                          [[0, 2], [1, 1]],\n        ...                          [[2, 2], [-1, -1]]])\n        >>> ac = g.count_alleles()\n        >>> ac\n        <AlleleCountsArray shape=(4, 3) dtype=int32>\n        4 0 0\n        3 1 0\n        1 2 1\n        0 0 2\n        >>> mapping = [[1, 0, 2],\n        ...            [1, 0, 2],\n        ...            [2, 1, 0],\n        ...            [1, 2, 0]]\n        >>> ac.map_alleles(mapping)\n        <AlleleCountsArray shape=(4, 3) dtype=int32>\n        0 4 0\n        1 3 0\n        1 2 1\n        2 0 0\n\n        See Also\n        --------\n        create_allele_mapping\n\n        \"\"\"\n\n        # ensure correct dimensionality and matching dtype\n        mapping = asarray_ndim(mapping, 2, dtype=self.dtype)\n        check_dim0_aligned(self, mapping)\n        check_dim1_aligned(self, mapping)\n\n        # use optimisation\n        out = allele_counts_array_map_alleles(self.values, mapping, max_allele)\n\n        # wrap and return\n        return type(self)(out)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntruing if no duplicate entries.", "response": "def is_unique(self):\n        \"\"\"True if no duplicate entries.\"\"\"\n        if self._is_unique is None:\n            t = self.values[:-1] == self.values[1:]  # type: np.ndarray\n            self._is_unique = ~np.any(t)\n        return self._is_unique"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlocating the index location of the requested key.", "response": "def locate_key(self, key):\n        \"\"\"Get index location for the requested key.\n\n        Parameters\n        ----------\n        key : object\n            Value to locate.\n\n        Returns\n        -------\n        loc : int or slice\n            Location of `key` (will be slice if there are duplicate entries).\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> idx = allel.SortedIndex([3, 6, 6, 11])\n        >>> idx.locate_key(3)\n        0\n        >>> idx.locate_key(11)\n        3\n        >>> idx.locate_key(6)\n        slice(1, 3, None)\n        >>> try:\n        ...     idx.locate_key(2)\n        ... except KeyError as e:\n        ...     print(e)\n        ...\n        2\n\n        \"\"\"\n\n        left = bisect.bisect_left(self, key)\n        right = bisect.bisect_right(self, key)\n        diff = right - left\n        if diff == 0:\n            raise KeyError(key)\n        elif diff == 1:\n            return left\n        else:\n            return slice(left, right)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef locate_intersection(self, other):\n\n        # check inputs\n        other = SortedIndex(other, copy=False)\n\n        # find intersection\n        assume_unique = self.is_unique and other.is_unique\n        loc = np.in1d(self, other, assume_unique=assume_unique)\n        loc_other = np.in1d(other, self, assume_unique=assume_unique)\n\n        return loc, loc_other", "response": "Locate the intersection with another array."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef locate_keys(self, keys, strict=True):\n\n        # check inputs\n        keys = SortedIndex(keys, copy=False)\n\n        # find intersection\n        loc, found = self.locate_intersection(keys)\n\n        if strict and np.any(~found):\n            raise KeyError(keys[~found])\n\n        return loc", "response": "Locate the keys in the index."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nintersects with other sorted index.", "response": "def intersect(self, other):\n        \"\"\"Intersect with `other` sorted index.\n\n        Parameters\n        ----------\n        other : array_like, int\n            Array of values to intersect with.\n\n        Returns\n        -------\n        out : SortedIndex\n            Values in common.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> idx1 = allel.SortedIndex([3, 6, 11, 20, 35])\n        >>> idx2 = allel.SortedIndex([4, 6, 20, 39])\n        >>> idx1.intersect(idx2)\n        <SortedIndex shape=(2,) dtype=int64>\n        [6, 20]\n\n        \"\"\"\n\n        loc = self.locate_keys(other, strict=False)\n        return self.compress(loc, axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef locate_range(self, start=None, stop=None):\n\n        # locate start and stop indices\n        if start is None:\n            start_index = 0\n        else:\n            start_index = bisect.bisect_left(self, start)\n        if stop is None:\n            stop_index = len(self)\n        else:\n            stop_index = bisect.bisect_right(self, stop)\n\n        if stop_index - start_index == 0:\n            raise KeyError(start, stop)\n\n        loc = slice(start_index, stop_index)\n        return loc", "response": "Locate slice of index containing all entries within start and stop values inclusive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nintersecting with range defined by start and stop values.", "response": "def intersect_range(self, start=None, stop=None):\n        \"\"\"Intersect with range defined by `start` and `stop` values\n        **inclusive**.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start value.\n        stop : int, optional\n            Stop value.\n\n        Returns\n        -------\n        idx : SortedIndex\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> idx = allel.SortedIndex([3, 6, 11, 20, 35])\n        >>> idx.intersect_range(4, 32)\n        <SortedIndex shape=(3,) dtype=int64>\n        [6, 11, 20]\n\n        \"\"\"\n\n        try:\n            loc = self.locate_range(start=start, stop=stop)\n        except KeyError:\n            return self.values[0:0]\n        else:\n            return self[loc]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef locate_intersection_ranges(self, starts, stops):\n\n        # check inputs\n        starts = asarray_ndim(starts, 1)\n        stops = asarray_ndim(stops, 1)\n        check_dim0_aligned(starts, stops)\n\n        # find indices of start and stop values in idx\n        start_indices = np.searchsorted(self, starts)\n        stop_indices = np.searchsorted(self, stops, side='right')\n\n        # find intervals overlapping at least one value\n        loc_ranges = start_indices < stop_indices\n\n        # find values within at least one interval\n        loc = np.zeros(self.shape, dtype=np.bool)\n        for i, j in zip(start_indices[loc_ranges], stop_indices[loc_ranges]):\n            loc[i:j] = True\n\n        return loc, loc_ranges", "response": "Locate the intersection with a set of start and stop values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlocate items within the given ranges.", "response": "def locate_ranges(self, starts, stops, strict=True):\n        \"\"\"Locate items within the given ranges.\n\n        Parameters\n        ----------\n        starts : array_like, int\n            Range start values.\n        stops : array_like, int\n            Range stop values.\n        strict : bool, optional\n            If True, raise KeyError if any ranges contain no entries.\n\n        Returns\n        -------\n        loc : ndarray, bool\n            Boolean array with location of entries found.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> import numpy as np\n        >>> idx = allel.SortedIndex([3, 6, 11, 20, 35])\n        >>> ranges = np.array([[0, 2], [6, 17], [12, 15], [31, 35],\n        ...                    [100, 120]])\n        >>> starts = ranges[:, 0]\n        >>> stops = ranges[:, 1]\n        >>> loc = idx.locate_ranges(starts, stops, strict=False)\n        >>> loc\n        array([False,  True,  True, False,  True])\n        >>> idx[loc]\n        <SortedIndex shape=(3,) dtype=int64>\n        [6, 11, 35]\n\n        \"\"\"\n\n        loc, found = self.locate_intersection_ranges(starts, stops)\n\n        if strict and np.any(~found):\n            raise KeyError(starts[~found], stops[~found])\n\n        return loc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nintersect with a set of ranges.", "response": "def intersect_ranges(self, starts, stops):\n        \"\"\"Intersect with a set of ranges.\n\n        Parameters\n        ----------\n        starts : array_like, int\n            Range start values.\n        stops : array_like, int\n            Range stop values.\n\n        Returns\n        -------\n        idx : SortedIndex\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> import numpy as np\n        >>> idx = allel.SortedIndex([3, 6, 11, 20, 35])\n        >>> ranges = np.array([[0, 2], [6, 17], [12, 15], [31, 35],\n        ...                    [100, 120]])\n        >>> starts = ranges[:, 0]\n        >>> stops = ranges[:, 1]\n        >>> idx.intersect_ranges(starts, stops)\n        <SortedIndex shape=(3,) dtype=int64>\n        [6, 11, 35]\n\n        \"\"\"\n\n        loc = self.locate_ranges(starts, stops, strict=False)\n        return self.compress(loc, axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef locate_intersection(self, other):\n\n        # check inputs\n        other = UniqueIndex(other)\n\n        # find intersection\n        assume_unique = True\n        loc = np.in1d(self, other, assume_unique=assume_unique)\n        loc_other = np.in1d(other, self, assume_unique=assume_unique)\n\n        return loc, loc_other", "response": "Locate the intersection with another array."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlocates the keys in the index.", "response": "def locate_keys(self, keys, strict=True):\n        \"\"\"Get index locations for the requested keys.\n\n        Parameters\n        ----------\n        keys : array_like\n            Array of keys to locate.\n        strict : bool, optional\n            If True, raise KeyError if any keys are not found in the index.\n\n        Returns\n        -------\n        loc : ndarray, bool\n            Boolean array with location of keys.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> idx = allel.UniqueIndex(['A', 'C', 'B', 'F'])\n        >>> idx.locate_keys(['F', 'C'])\n        array([False,  True, False,  True])\n        >>> idx.locate_keys(['X', 'F', 'G', 'C', 'Z'], strict=False)\n        array([False,  True, False,  True])\n\n        \"\"\"\n\n        # check inputs\n        keys = UniqueIndex(keys)\n\n        # find intersection\n        loc, found = self.locate_intersection(keys)\n\n        if strict and np.any(~found):\n            raise KeyError(keys[~found])\n\n        return loc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlocate the index location for the requested key.", "response": "def locate_key(self, k1, k2=None):\n        \"\"\"\n        Get index location for the requested key.\n\n        Parameters\n        ----------\n        k1 : object\n            Level 1 key.\n        k2 : object, optional\n            Level 2 key.\n\n        Returns\n        -------\n        loc : int or slice\n            Location of requested key (will be slice if there are duplicate\n            entries).\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> chrom = ['chr1', 'chr1', 'chr2', 'chr2', 'chr2', 'chr3']\n        >>> pos = [1, 4, 2, 5, 5, 3]\n        >>> idx = allel.SortedMultiIndex(chrom, pos)\n        >>> idx.locate_key('chr1')\n        slice(0, 2, None)\n        >>> idx.locate_key('chr1', 4)\n        1\n        >>> idx.locate_key('chr2', 5)\n        slice(3, 5, None)\n        >>> try:\n        ...     idx.locate_key('chr3', 4)\n        ... except KeyError as e:\n        ...     print(e)\n        ...\n        ('chr3', 4)\n\n        \"\"\"\n\n        loc1 = self.l1.locate_key(k1)\n        if k2 is None:\n            return loc1\n        if isinstance(loc1, slice):\n            offset = loc1.start\n            try:\n                loc2 = SortedIndex(self.l2[loc1], copy=False).locate_key(k2)\n            except KeyError:\n                # reraise with more information\n                raise KeyError(k1, k2)\n            else:\n                if isinstance(loc2, slice):\n                    loc = slice(offset + loc2.start, offset + loc2.stop)\n                else:\n                    # assume singleton\n                    loc = offset + loc2\n        else:\n            # singleton match in l1\n            v = self.l2[loc1]\n            if v == k2:\n                loc = loc1\n            else:\n                raise KeyError(k1, k2)\n        return loc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlocating slice of index containing all entries within the range start - stop.", "response": "def locate_range(self, key, start=None, stop=None):\n        \"\"\"Locate slice of index containing all entries within the range\n        `key`:`start`-`stop` **inclusive**.\n\n        Parameters\n        ----------\n        key : object\n            Level 1 key value.\n        start : object, optional\n            Level 2 start value.\n        stop : object, optional\n            Level 2 stop value.\n\n        Returns\n        -------\n        loc : slice\n            Slice object.\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> chrom = ['chr1', 'chr1', 'chr2', 'chr2', 'chr2', 'chr3']\n        >>> pos = [1, 4, 2, 5, 5, 3]\n        >>> idx = allel.SortedMultiIndex(chrom, pos)\n        >>> idx.locate_range('chr1')\n        slice(0, 2, None)\n        >>> idx.locate_range('chr1', 1, 4)\n        slice(0, 2, None)\n        >>> idx.locate_range('chr2', 3, 7)\n        slice(3, 5, None)\n        >>> try:\n        ...     idx.locate_range('chr3', 4, 9)\n        ... except KeyError as e:\n        ...     print(e)\n        ('chr3', 4, 9)\n\n        \"\"\"\n\n        loc1 = self.l1.locate_key(key)\n        if start is None and stop is None:\n            loc = loc1\n        elif isinstance(loc1, slice):\n            offset = loc1.start\n            idx = SortedIndex(self.l2[loc1], copy=False)\n            try:\n                loc2 = idx.locate_range(start, stop)\n            except KeyError:\n                raise KeyError(key, start, stop)\n            else:\n                loc = slice(offset + loc2.start, offset + loc2.stop)\n        else:\n            # singleton match in l1\n            v = self.l2[loc1]\n            if start <= v <= stop:\n                loc = loc1\n            else:\n                raise KeyError(key, start, stop)\n        # ensure slice is always returned\n        if not isinstance(loc, slice):\n            loc = slice(loc, loc + 1)\n        return loc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlocate the key in the cache for the requested key.", "response": "def locate_key(self, chrom, pos=None):\n        \"\"\"\n        Get index location for the requested key.\n\n        Parameters\n        ----------\n        chrom : object\n            Chromosome or contig.\n        pos : int, optional\n            Position within chromosome or contig.\n\n        Returns\n        -------\n        loc : int or slice\n            Location of requested key (will be slice if there are duplicate\n            entries).\n\n        Examples\n        --------\n\n        >>> import allel\n        >>> chrom = ['chr2', 'chr2', 'chr1', 'chr1', 'chr1', 'chr3']\n        >>> pos = [1, 4, 2, 5, 5, 3]\n        >>> idx = allel.ChromPosIndex(chrom, pos)\n        >>> idx.locate_key('chr1')\n        slice(2, 5, None)\n        >>> idx.locate_key('chr2', 4)\n        1\n        >>> idx.locate_key('chr1', 5)\n        slice(3, 5, None)\n        >>> try:\n        ...     idx.locate_key('chr3', 4)\n        ... except KeyError as e:\n        ...     print(e)\n        ...\n        ('chr3', 4)\n\n        \"\"\"\n\n        if pos is None:\n            # we just want the region for a chromosome\n            if chrom in self.chrom_ranges:\n                # return previously cached result\n                return self.chrom_ranges[chrom]\n            else:\n                loc_chrom = np.nonzero(self.chrom == chrom)[0]\n                if len(loc_chrom) == 0:\n                    raise KeyError(chrom)\n                slice_chrom = slice(min(loc_chrom), max(loc_chrom) + 1)\n                # cache the result\n                self.chrom_ranges[chrom] = slice_chrom\n                return slice_chrom\n\n        else:\n            slice_chrom = self.locate_key(chrom)\n            pos_chrom = SortedIndex(self.pos[slice_chrom])\n            try:\n                idx_within_chrom = pos_chrom.locate_key(pos)\n            except KeyError:\n                raise KeyError(chrom, pos)\n            if isinstance(idx_within_chrom, slice):\n                return slice(slice_chrom.start + idx_within_chrom.start,\n                             slice_chrom.start + idx_within_chrom.stop)\n            else:\n                return slice_chrom.start + idx_within_chrom"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef locate_range(self, chrom, start=None, stop=None):\n\n        slice_chrom = self.locate_key(chrom)\n\n        if start is None and stop is None:\n            return slice_chrom\n\n        else:\n\n            pos_chrom = SortedIndex(self.pos[slice_chrom])\n            try:\n                slice_within_chrom = pos_chrom.locate_range(start, stop)\n            except KeyError:\n                raise KeyError(chrom, start, stop)\n            loc = slice(slice_chrom.start + slice_within_chrom.start,\n                        slice_chrom.start + slice_within_chrom.stop)\n            return loc", "response": "Locate slice of index containing all entries within the range start - stop."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_index(self, index):\n        if index is None:\n            pass\n        elif isinstance(index, str):\n            index = SortedIndex(self[index], copy=False)\n        elif isinstance(index, (tuple, list)) and len(index) == 2:\n            index = SortedMultiIndex(self[index[0]], self[index[1]],\n                                     copy=False)\n        else:\n            raise ValueError('invalid index argument, expected string or '\n                             'pair of strings, found %s' % repr(index))\n        self.index = index", "response": "Set or reset the index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query_position(self, chrom=None, position=None):\n\n        if self.index is None:\n            raise ValueError('no index has been set')\n        if isinstance(self.index, SortedIndex):\n            # ignore chrom\n            loc = self.index.locate_key(position)\n        else:\n            loc = self.index.locate_key(chrom, position)\n        return self[loc]", "response": "Query the table returning row or rows matching the given genomic sequence and position."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query_region(self, chrom=None, start=None, stop=None):\n        if self.index is None:\n            raise ValueError('no index has been set')\n        if isinstance(self.index, SortedIndex):\n            # ignore chrom\n            loc = self.index.locate_range(start, stop)\n        else:\n            loc = self.index.locate_range(chrom, start, stop)\n        return self[loc]", "response": "Query the table returning row or rows within the given genomic\n            region."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_vcf(self, path, rename=None, number=None, description=None,\n               fill=None, write_header=True):\n        r\"\"\"Write to a variant call format (VCF) file.\n\n        Parameters\n        ----------\n        path : string\n            File path.\n        rename : dict, optional\n            Rename these columns in the VCF.\n        number : dict, optional\n            Override the number specified in INFO headers.\n        description : dict, optional\n            Descriptions for the INFO and FILTER headers.\n        fill : dict, optional\n            Fill values used for missing data in the table.\n        write_header : bool, optional\n            If True write VCF header.\n\n        Examples\n        --------\n        Setup a variant table to write out::\n\n            >>> import allel\n            >>> chrom = [b'chr1', b'chr1', b'chr2', b'chr2', b'chr3']\n            >>> pos = [2, 6, 3, 8, 1]\n            >>> ids = ['a', 'b', 'c', 'd', 'e']\n            >>> ref = [b'A', b'C', b'T', b'G', b'N']\n            >>> alt = [(b'T', b'.'),\n            ...        (b'G', b'.'),\n            ...        (b'A', b'C'),\n            ...        (b'C', b'A'),\n            ...        (b'X', b'.')]\n            >>> qual = [1.2, 2.3, 3.4, 4.5, 5.6]\n            >>> filter_qd = [True, True, True, False, False]\n            >>> filter_dp = [True, False, True, False, False]\n            >>> dp = [12, 23, 34, 45, 56]\n            >>> qd = [12.3, 23.4, 34.5, 45.6, 56.7]\n            >>> flg = [True, False, True, False, True]\n            >>> ac = [(1, -1), (3, -1), (5, 6), (7, 8), (9, -1)]\n            >>> xx = [(1.2, 2.3), (3.4, 4.5), (5.6, 6.7), (7.8, 8.9),\n            ...       (9.0, 9.9)]\n            >>> columns = [chrom, pos, ids, ref, alt, qual, filter_dp,\n            ...            filter_qd, dp, qd, flg, ac, xx]\n            >>> records = list(zip(*columns))\n            >>> dtype = [('CHROM', 'S4'),\n            ...          ('POS', 'u4'),\n            ...          ('ID', 'S1'),\n            ...          ('REF', 'S1'),\n            ...          ('ALT', ('S1', 2)),\n            ...          ('qual', 'f4'),\n            ...          ('filter_dp', bool),\n            ...          ('filter_qd', bool),\n            ...          ('dp', int),\n            ...          ('qd', float),\n            ...          ('flg', bool),\n            ...          ('ac', (int, 2)),\n            ...          ('xx', (float, 2))]\n            >>> vt = allel.VariantTable(records, dtype=dtype)\n\n        Now write out to VCF and inspect the result::\n\n            >>> rename = {'dp': 'DP', 'qd': 'QD', 'filter_qd': 'QD'}\n            >>> fill = {'ALT': b'.', 'ac': -1}\n            >>> number = {'ac': 'A'}\n            >>> description = {'ac': 'Allele counts', 'filter_dp': 'Low depth'}\n            >>> vt.to_vcf('example.vcf', rename=rename, fill=fill,\n            ...           number=number, description=description)\n            >>> print(open('example.vcf').read())\n            ##fileformat=VCFv4.1\n            ##fileDate=...\n            ##source=...\n            ##INFO=<ID=DP,Number=1,Type=Integer,Description=\"\">\n            ##INFO=<ID=QD,Number=1,Type=Float,Description=\"\">\n            ##INFO=<ID=ac,Number=A,Type=Integer,Description=\"Allele counts\">\n            ##INFO=<ID=flg,Number=0,Type=Flag,Description=\"\">\n            ##INFO=<ID=xx,Number=2,Type=Float,Description=\"\">\n            ##FILTER=<ID=QD,Description=\"\">\n            ##FILTER=<ID=dp,Description=\"Low depth\">\n            #CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\n            chr1\t2\ta\tA\tT\t1.2\tQD;dp\tDP=12;QD=12.3;ac=1;flg;xx=...\n            chr1\t6\tb\tC\tG\t2.3\tQD\tDP=23;QD=23.4;ac=3;xx=3.4,4.5\n            chr2\t3\tc\tT\tA,C\t3.4\tQD;dp\tDP=34;QD=34.5;ac=5,6;flg;x...\n            chr2\t8\td\tG\tC,A\t4.5\tPASS\tDP=45;QD=45.6;ac=7,8;xx=7...\n            chr3\t1\te\tN\tX\t5.6\tPASS\tDP=56;QD=56.7;ac=9;flg;xx=...\n\n        \"\"\"\n\n        write_vcf(path, callset=self, rename=rename, number=number,\n                  description=description, fill=fill,\n                  write_header=write_header)", "response": "r Write the current state of the object to a VCF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs a mask array where elements are True if the fall within features in the table.", "response": "def to_mask(self, size, start_name='start', stop_name='end'):\n        \"\"\"Construct a mask array where elements are True if the fall within\n        features in the table.\n\n        Parameters\n        ----------\n\n        size : int\n            Size of chromosome/contig.\n        start_name : string, optional\n            Name of column with start coordinates.\n        stop_name : string, optional\n            Name of column with stop coordinates.\n\n        Returns\n        -------\n\n        mask : ndarray, bool\n\n        \"\"\"\n        m = np.zeros(size, dtype=bool)\n        for start, stop in self[[start_name, stop_name]]:\n            m[start-1:stop] = True\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_gff3(path, attributes=None, region=None, score_fill=-1, phase_fill=-1,\n                  attributes_fill='.', dtype=None):\n        \"\"\"Read a feature table from a GFF3 format file.\n\n        Parameters\n        ----------\n        path : string\n            File path.\n        attributes : list of strings, optional\n            List of columns to extract from the \"attributes\" field.\n        region : string, optional\n            Genome region to extract. If given, file must be position\n            sorted, bgzipped and tabix indexed. Tabix must also be installed\n            and on the system path.\n        score_fill : int, optional\n            Value to use where score field has a missing value.\n        phase_fill : int, optional\n            Value to use where phase field has a missing value.\n        attributes_fill : object or list of objects, optional\n            Value(s) to use where attribute field(s) have a missing value.\n        dtype : numpy dtype, optional\n            Manually specify a dtype.\n\n        Returns\n        -------\n        ft : FeatureTable\n\n        \"\"\"\n\n        a = gff3_to_recarray(path, attributes=attributes, region=region,\n                             score_fill=score_fill, phase_fill=phase_fill,\n                             attributes_fill=attributes_fill, dtype=dtype)\n        if a is None:\n            return None\n        else:\n            return FeatureTable(a, copy=False)", "response": "Read a feature table from a GFF3 format file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pairwise_distance(x, metric, chunked=False, blen=None):\n\n    import scipy.spatial\n\n    # check inputs\n    if not hasattr(x, 'ndim'):\n        x = np.asarray(x)\n    if x.ndim < 2:\n        raise ValueError('array with at least 2 dimensions expected')\n\n    if x.ndim == 2:\n        # use scipy to calculate distance, it's most efficient\n\n        def f(b):\n\n            # transpose as pdist expects (m, n) for m observations in an\n            # n-dimensional space\n            t = b.T\n\n            # compute the distance matrix\n            return scipy.spatial.distance.pdist(t, metric=metric)\n\n    else:\n        # use our own implementation, it handles multidimensional observations\n\n        def f(b):\n            return pdist(b, metric=metric)\n\n    if chunked:\n        # use block-wise implementation\n        blen = get_blen_array(x, blen)\n        dist = None\n        for i in range(0, x.shape[0], blen):\n            j = min(x.shape[0], i+blen)\n            block = x[i:j]\n            if dist is None:\n                dist = f(block)\n            else:\n                dist += f(block)\n\n    else:\n        # standard implementation\n        dist = f(x)\n\n    return dist", "response": "Compute pairwise distance between the given array of observations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pdist(x, metric):\n\n    if isinstance(metric, str):\n        import scipy.spatial\n        if hasattr(scipy.spatial.distance, metric):\n            metric = getattr(scipy.spatial.distance, metric)\n        else:\n            raise ValueError('metric name not found')\n\n    m = x.shape[1]\n    dist = list()\n    for i, j in itertools.combinations(range(m), 2):\n        a = x[:, i, ...]\n        b = x[:, j, ...]\n        d = metric(a, b)\n        dist.append(d)\n    return np.array(dist)", "response": "Alternative implementation of scipy. spatial. distance. pdist which uses scipy. spatial. distance. pdist to compute the distance between two arrays of n dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pcoa(dist):\n    import scipy.linalg\n\n    # This implementation is based on the skbio.math.stats.ordination.PCoA\n    # implementation, with some minor adjustments.\n\n    # check inputs\n    dist = ensure_square(dist)\n\n    # perform scaling\n    e_matrix = (dist ** 2) / -2\n    row_means = np.mean(e_matrix, axis=1, keepdims=True)\n    col_means = np.mean(e_matrix, axis=0, keepdims=True)\n    matrix_mean = np.mean(e_matrix)\n    f_matrix = e_matrix - row_means - col_means + matrix_mean\n    eigvals, eigvecs = scipy.linalg.eigh(f_matrix)\n\n    # deal with eigvals close to zero\n    close_to_zero = np.isclose(eigvals, 0)\n    eigvals[close_to_zero] = 0\n\n    # sort descending\n    idxs = eigvals.argsort()[::-1]\n    eigvals = eigvals[idxs]\n    eigvecs = eigvecs[:, idxs]\n\n    # keep only positive eigenvalues\n    keep = eigvals >= 0\n    eigvecs = eigvecs[:, keep]\n    eigvals = eigvals[keep]\n\n    # compute coordinates\n    coords = eigvecs * np.sqrt(eigvals)\n\n    # compute ratio explained\n    explained_ratio = eigvals / eigvals.sum()\n\n    return coords, explained_ratio", "response": "Perform principal coordinate analysis of a distance matrix a. k. a.\n    classical multi - dimensional scaling."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef condensed_coords(i, j, n):\n\n    # guard conditions\n    if i == j or i >= n or j >= n or i < 0 or j < 0:\n        raise ValueError('invalid coordinates: %s, %s' % (i, j))\n\n    # normalise order\n    i, j = sorted([i, j])\n\n    # calculate number of items in rows before this one (sum of arithmetic\n    # progression)\n    x = i * ((2 * n) - i - 1) / 2\n\n    # add on previous items in current row\n    ix = x + j - i - 1\n\n    return int(ix)", "response": "Transform square distance matrix coordinates to the corresponding\n    index into a condensed 1D form of the matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef condensed_coords_within(pop, n):\n\n    return [condensed_coords(i, j, n)\n            for i, j in itertools.combinations(sorted(pop), 2)]", "response": "Return indices into a condensed distance matrix for all\n    pairwise comparisons within the given population."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef condensed_coords_between(pop1, pop2, n):\n\n    return [condensed_coords(i, j, n)\n            for i, j in itertools.product(sorted(pop1), sorted(pop2))]", "response": "Return indices into a condensed distance matrix for all pairwise\n    comparisons between two populations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_pairwise_distance(dist, labels=None, colorbar=True, ax=None,\n                           imshow_kwargs=None):\n    \"\"\"Plot a pairwise distance matrix.\n\n    Parameters\n    ----------\n    dist : array_like\n        The distance matrix in condensed form.\n    labels : sequence of strings, optional\n        Sample labels for the axes.\n    colorbar : bool, optional\n        If True, add a colorbar to the current figure.\n    ax : axes, optional\n        The axes on which to draw. If not provided, a new figure will be\n        created.\n    imshow_kwargs : dict-like, optional\n        Additional keyword arguments passed through to\n        :func:`matplotlib.pyplot.imshow`.\n\n    Returns\n    -------\n    ax : axes\n        The axes on which the plot was drawn\n\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    # check inputs\n    dist_square = ensure_square(dist)\n\n    # set up axes\n    if ax is None:\n        # make a square figure\n        x = plt.rcParams['figure.figsize'][0]\n        fig, ax = plt.subplots(figsize=(x, x))\n        fig.tight_layout()\n\n    # setup imshow arguments\n    if imshow_kwargs is None:\n        imshow_kwargs = dict()\n    imshow_kwargs.setdefault('interpolation', 'none')\n    imshow_kwargs.setdefault('cmap', 'jet')\n    imshow_kwargs.setdefault('vmin', np.min(dist))\n    imshow_kwargs.setdefault('vmax', np.max(dist))\n\n    # plot as image\n    im = ax.imshow(dist_square, **imshow_kwargs)\n\n    # tidy up\n    if labels:\n        ax.set_xticks(range(len(labels)))\n        ax.set_yticks(range(len(labels)))\n        ax.set_xticklabels(labels, rotation=90)\n        ax.set_yticklabels(labels, rotation=0)\n    else:\n        ax.set_xticks([])\n        ax.set_yticks([])\n    if colorbar:\n        plt.gcf().colorbar(im, shrink=.5)\n\n    return ax", "response": "Plots a pairwise distance matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nestimate standard error for statistic computed over values using the jackknife.", "response": "def jackknife(values, statistic):\n    \"\"\"Estimate standard error for `statistic` computed over `values` using\n    the jackknife.\n\n    Parameters\n    ----------\n    values : array_like or tuple of array_like\n        Input array, or tuple of input arrays.\n    statistic : function\n        The statistic to compute.\n\n    Returns\n    -------\n    m : float\n        Mean of jackknife values.\n    se : float\n        Estimate of standard error.\n    vj : ndarray\n        Statistic values computed for each jackknife iteration.\n\n    \"\"\"\n\n    if isinstance(values, tuple):\n        # multiple input arrays\n        n = len(values[0])\n        masked_values = [np.ma.asarray(v) for v in values]\n        for m in masked_values:\n            assert m.ndim == 1, 'only 1D arrays supported'\n            assert m.shape[0] == n, 'input arrays not of equal length'\n            m.mask = np.zeros(m.shape, dtype=bool)\n\n    else:\n        n = len(values)\n        masked_values = np.ma.asarray(values)\n        assert masked_values.ndim == 1, 'only 1D arrays supported'\n        masked_values.mask = np.zeros(masked_values.shape, dtype=bool)\n\n    # values of the statistic calculated in each jackknife iteration\n    vj = list()\n\n    for i in range(n):\n\n        if isinstance(values, tuple):\n            # multiple input arrays\n            for m in masked_values:\n                m.mask[i] = True\n            x = statistic(*masked_values)\n            for m in masked_values:\n                m.mask[i] = False\n\n        else:\n            masked_values.mask[i] = True\n            x = statistic(masked_values)\n            masked_values.mask[i] = False\n\n        vj.append(x)\n\n    # convert to array for convenience\n    vj = np.array(vj)\n\n    # compute mean of jackknife values\n    m = vj.mean()\n\n    # compute standard error\n    sv = ((n - 1) / n) * np.sum((vj - m) ** 2)\n    se = np.sqrt(sv)\n\n    return m, se, vj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_variant_locator(pos, step=None, ax=None, start=None,\n                         stop=None, flip=False,\n                         line_kwargs=None):\n    \"\"\"\n    Plot lines indicating the physical genome location of variants from a\n    single chromosome/contig. By default the top x axis is in variant index\n    space, and the bottom x axis is in genome position space.\n\n    Parameters\n    ----------\n\n    pos : array_like\n        A sorted 1-dimensional array of genomic positions from a single\n        chromosome/contig.\n    step : int, optional\n        Plot a line for every `step` variants.\n    ax : axes, optional\n        The axes on which to draw. If not provided, a new figure will be\n        created.\n    start : int, optional\n        The start position for the region to draw.\n    stop : int, optional\n        The stop position for the region to draw.\n    flip : bool, optional\n        Flip the plot upside down.\n    line_kwargs : dict-like\n        Additional keyword arguments passed through to `plt.Line2D`.\n\n    Returns\n    -------\n\n    ax : axes\n        The axes on which the plot was drawn\n\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    # check inputs\n    pos = SortedIndex(pos, copy=False)\n\n    # set up axes\n    if ax is None:\n        x = plt.rcParams['figure.figsize'][0]\n        y = x / 7\n        fig, ax = plt.subplots(figsize=(x, y))\n        fig.tight_layout()\n\n    # determine x axis limits\n    if start is None:\n        start = np.min(pos)\n    if stop is None:\n        stop = np.max(pos)\n    loc = pos.locate_range(start, stop)\n    pos = pos[loc]\n    if step is None:\n        step = len(pos) // 100\n    ax.set_xlim(start, stop)\n\n    # plot the lines\n    if line_kwargs is None:\n        line_kwargs = dict()\n    # line_kwargs.setdefault('linewidth', .5)\n    n_variants = len(pos)\n    for i, p in enumerate(pos[::step]):\n        xfrom = p\n        xto = (\n            start +\n            ((i * step / n_variants) * (stop-start))\n        )\n        line = plt.Line2D([xfrom, xto], [0, 1], **line_kwargs)\n        ax.add_line(line)\n\n    # invert?\n    if flip:\n        ax.invert_yaxis()\n        ax.xaxis.tick_top()\n    else:\n        ax.xaxis.tick_bottom()\n\n    # tidy up\n    ax.set_yticks([])\n    ax.xaxis.set_tick_params(direction='out')\n    for spine in 'left', 'right':\n        ax.spines[spine].set_visible(False)\n\n    return ax", "response": "Plots lines indicating the physical genome location of variants from a single chromosome or contig."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a dataframe where each row provides information about a state transition.", "response": "def tabulate_state_transitions(x, states, pos=None):\n    \"\"\"Construct a dataframe where each row provides information about a state transition.\n\n    Parameters\n    ----------\n    x : array_like, int\n        1-dimensional array of state values.\n    states : set\n        Set of states of interest. Any state value not in this set will be ignored.\n    pos : array_like, int, optional\n        Array of positions corresponding to values in `x`.\n\n    Returns\n    -------\n    df : DataFrame\n\n    Notes\n    -----\n    The resulting dataframe includes one row at the start representing the first state\n    observation and one row at the end representing the last state observation.\n\n    Examples\n    --------\n    >>> import allel\n    >>> x = [1, 1, 0, 1, 1, 2, 2, 0, 2, 1, 1]\n    >>> df = allel.tabulate_state_transitions(x, states={1, 2})\n    >>> df\n       lstate  rstate  lidx  ridx\n    0      -1       1    -1     0\n    1       1       2     4     5\n    2       2       1     8     9\n    3       1      -1    10    -1\n    >>> pos = [2, 4, 7, 8, 10, 14, 19, 23, 28, 30, 31]\n    >>> df = allel.tabulate_state_transitions(x, states={1, 2}, pos=pos)\n    >>> df\n       lstate  rstate  lidx  ridx  lpos  rpos\n    0      -1       1    -1     0    -1     2\n    1       1       2     4     5    10    14\n    2       2       1     8     9    28    30\n    3       1      -1    10    -1    31    -1\n\n    \"\"\"\n\n    # check inputs\n    x = asarray_ndim(x, 1)\n    check_integer_dtype(x)\n    x = memoryview_safe(x)\n\n    # find state transitions\n    switch_points, transitions, _ = state_transitions(x, states)\n\n    # start to build a dataframe\n    items = [('lstate', transitions[:, 0]),\n             ('rstate', transitions[:, 1]),\n             ('lidx', switch_points[:, 0]),\n             ('ridx', switch_points[:, 1])]\n\n    # deal with optional positions\n    if pos is not None:\n        pos = asarray_ndim(pos, 1)\n        check_dim0_aligned(x, pos)\n        check_integer_dtype(pos)\n\n        # find switch positions\n        switch_positions = np.take(pos, switch_points)\n        # deal with boundary transitions\n        switch_positions[0, 0] = -1\n        switch_positions[-1, 1] = -1\n\n        # add columns into dataframe\n        items += [('lpos', switch_positions[:, 0]),\n                  ('rpos', switch_positions[:, 1])]\n\n    import pandas\n    return pandas.DataFrame.from_dict(OrderedDict(items))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tabulate_state_blocks(x, states, pos=None):\n\n    # check inputs\n    x = asarray_ndim(x, 1)\n    check_integer_dtype(x)\n    x = memoryview_safe(x)\n\n    # find state transitions\n    switch_points, transitions, observations = state_transitions(x, states)\n\n    # setup some helpers\n    t = transitions[1:, 0]\n    o = observations[1:]\n    s1 = switch_points[:-1]\n    s2 = switch_points[1:]\n    is_marginal = (s1[:, 0] < 0) | (s2[:, 1] < 0)\n    size_min = s2[:, 0] - s1[:, 1] + 1\n    size_max = s2[:, 1] - s1[:, 0] - 1\n    size_max[is_marginal] = -1\n\n    # start to build a dataframe\n    items = [\n        ('state', t),\n        ('support', o),\n        ('start_lidx', s1[:, 0]),\n        ('start_ridx', s1[:, 1]),\n        ('stop_lidx', s2[:, 0]),\n        ('stop_ridx', s2[:, 1]),\n        ('size_min', size_min),\n        ('size_max', size_max),\n        ('is_marginal', is_marginal)\n    ]\n\n    # deal with optional positions\n    if pos is not None:\n        pos = asarray_ndim(pos, 1)\n        check_dim0_aligned(x, pos)\n        check_integer_dtype(pos)\n\n        # obtain switch positions\n        switch_positions = np.take(pos, switch_points)\n        # deal with boundary transitions\n        switch_positions[0, 0] = -1\n        switch_positions[-1, 1] = -1\n\n        # setup helpers\n        p1 = switch_positions[:-1]\n        p2 = switch_positions[1:]\n        length_min = p2[:, 0] - p1[:, 1] + 1\n        length_max = p2[:, 1] - p1[:, 0] - 1\n        length_max[is_marginal] = -1\n\n        items += [\n            ('start_lpos', p1[:, 0]),\n            ('start_rpos', p1[:, 1]),\n            ('stop_lpos', p2[:, 0]),\n            ('stop_rpos', p2[:, 1]),\n            ('length_min', length_min),\n            ('length_max', length_max),\n        ]\n\n    import pandas\n    return pandas.DataFrame.from_dict(OrderedDict(items))", "response": "Construct a dataframe where each row provides information about continuous state blocks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_vcf(path, callset, rename=None, number=None, description=None,\n              fill=None, write_header=True):\n    \"\"\"Preliminary support for writing a VCF file. Currently does not support sample data.\n    Needs further work.\"\"\"\n\n    names, callset = normalize_callset(callset)\n\n    with open(path, 'w') as vcf_file:\n        if write_header:\n            write_vcf_header(vcf_file, names, callset=callset, rename=rename,\n                             number=number, description=description)\n        write_vcf_data(vcf_file, names, callset=callset, rename=rename, fill=fill)", "response": "Write a VCF file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure numpy array. asarray_ndim returns a new array_like .", "response": "def asarray_ndim(a, *ndims, **kwargs):\n    \"\"\"Ensure numpy array.\n\n    Parameters\n    ----------\n    a : array_like\n    *ndims : int, optional\n        Allowed values for number of dimensions.\n    **kwargs\n        Passed through to :func:`numpy.array`.\n\n    Returns\n    -------\n    a : numpy.ndarray\n\n    \"\"\"\n    allow_none = kwargs.pop('allow_none', False)\n    kwargs.setdefault('copy', False)\n    if a is None and allow_none:\n        return None\n    a = np.array(a, **kwargs)\n    if a.ndim not in ndims:\n        if len(ndims) > 1:\n            expect_str = 'one of %s' % str(ndims)\n        else:\n            # noinspection PyUnresolvedReferences\n            expect_str = '%s' % ndims[0]\n        raise TypeError('bad number of dimensions: expected %s; found %s' %\n                        (expect_str, a.ndim))\n    return a"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hdf5_cache(filepath=None, parent=None, group=None, names=None, typed=False,\n               hashed_key=False, **h5dcreate_kwargs):\n    \"\"\"HDF5 cache decorator.\n\n    Parameters\n    ----------\n    filepath : string, optional\n        Path to HDF5 file. If None a temporary file name will be used.\n    parent : string, optional\n        Path to group within HDF5 file to use as parent. If None the root\n        group will be used.\n    group : string, optional\n        Path to group within HDF5 file, relative to parent, to use as\n        container for cached data. If None the name of the wrapped function\n        will be used.\n    names : sequence of strings, optional\n        Name(s) of dataset(s). If None, default names will be 'f00', 'f01',\n        etc.\n    typed : bool, optional\n        If True, arguments of different types will be cached separately.\n        For example, f(3.0) and f(3) will be treated as distinct calls with\n        distinct results.\n    hashed_key : bool, optional\n        If False (default) the key will not be hashed, which makes for\n        readable cache group names. If True the key will be hashed, however\n        note that on Python >= 3.3 the hash value will not be the same between\n        sessions unless the environment variable PYTHONHASHSEED has been set\n        to the same value.\n\n    Returns\n    -------\n    decorator : function\n\n    Examples\n    --------\n\n    Without any arguments, will cache using a temporary HDF5 file::\n\n        >>> import allel\n        >>> @allel.util.hdf5_cache()\n        ... def foo(n):\n        ...     print('executing foo')\n        ...     return np.arange(n)\n        ...\n        >>> foo(3)\n        executing foo\n        array([0, 1, 2])\n        >>> foo(3)\n        array([0, 1, 2])\n        >>> foo.cache_filepath # doctest: +SKIP\n        '/tmp/tmp_jwtwgjz'\n\n    Supports multiple return values, including scalars, e.g.::\n\n        >>> @allel.util.hdf5_cache()\n        ... def bar(n):\n        ...     print('executing bar')\n        ...     a = np.arange(n)\n        ...     return a, a**2, n**2\n        ...\n        >>> bar(3)\n        executing bar\n        (array([0, 1, 2]), array([0, 1, 4]), 9)\n        >>> bar(3)\n        (array([0, 1, 2]), array([0, 1, 4]), 9)\n\n    Names can also be specified for the datasets, e.g.::\n\n        >>> @allel.util.hdf5_cache(names=['z', 'x', 'y'])\n        ... def baz(n):\n        ...     print('executing baz')\n        ...     a = np.arange(n)\n        ...     return a, a**2, n**2\n        ...\n        >>> baz(3)\n        executing baz\n        (array([0, 1, 2]), array([0, 1, 4]), 9)\n        >>> baz(3)\n        (array([0, 1, 2]), array([0, 1, 4]), 9)\n\n    \"\"\"\n\n    # initialise HDF5 file path\n    if filepath is None:\n        import tempfile\n        filepath = tempfile.mktemp(prefix='scikit_allel_', suffix='.h5')\n        atexit.register(os.remove, filepath)\n\n    # initialise defaults for dataset creation\n    h5dcreate_kwargs.setdefault('chunks', True)\n\n    def decorator(user_function):\n\n        # setup the name for the cache container group\n        if group is None:\n            container = user_function.__name__\n        else:\n            container = group\n\n        def wrapper(*args, **kwargs):\n\n            # load from cache or not\n            no_cache = kwargs.pop('no_cache', False)\n\n            # compute a key from the function arguments\n            key = _make_key(args, kwargs, typed)\n            if hashed_key:\n                key = str(hash(key))\n            else:\n                key = str(key).replace('/', '__slash__')\n\n            return _hdf5_cache_act(filepath, parent, container, key, names,\n                                   no_cache, user_function, args, kwargs,\n                                   h5dcreate_kwargs)\n\n        wrapper.cache_filepath = filepath\n        return update_wrapper(wrapper, user_function)\n\n    return decorator", "response": "HDF5 cache decorator.\n\n    Parameters\n    ----------\n    filepath : string, optional\n        Path to HDF5 file. If None a temporary file name will be used.\n    parent : string, optional\n        Path to group within HDF5 file to use as parent. If None the root\n        group will be used.\n    group : string, optional\n        Path to group within HDF5 file, relative to parent, to use as\n        container for cached data. If None the name of the wrapped function\n        will be used.\n    names : sequence of strings, optional\n        Name(s) of dataset(s). If None, default names will be 'f00', 'f01',\n        etc.\n    typed : bool, optional\n        If True, arguments of different types will be cached separately.\n        For example, f(3.0) and f(3) will be treated as distinct calls with\n        distinct results.\n    hashed_key : bool, optional\n        If False (default) the key will not be hashed, which makes for\n        readable cache group names. If True the key will be hashed, however\n        note that on Python >= 3.3 the hash value will not be the same between\n        sessions unless the environment variable PYTHONHASHSEED has been set\n        to the same value.\n\n    Returns\n    -------\n    decorator : function\n\n    Examples\n    --------\n\n    Without any arguments, will cache using a temporary HDF5 file::\n\n        >>> import allel\n        >>> @allel.util.hdf5_cache()\n        ... def foo(n):\n        ...     print('executing foo')\n        ...     return np.arange(n)\n        ...\n        >>> foo(3)\n        executing foo\n        array([0, 1, 2])\n        >>> foo(3)\n        array([0, 1, 2])\n        >>> foo.cache_filepath # doctest: +SKIP\n        '/tmp/tmp_jwtwgjz'\n\n    Supports multiple return values, including scalars, e.g.::\n\n        >>> @allel.util.hdf5_cache()\n        ... def bar(n):\n        ...     print('executing bar')\n        ...     a = np.arange(n)\n        ...     return a, a**2, n**2\n        ...\n        >>> bar(3)\n        executing bar\n        (array([0, 1, 2]), array([0, 1, 4]), 9)\n        >>> bar(3)\n        (array([0, 1, 2]), array([0, 1, 4]), 9)\n\n    Names can also be specified for the datasets, e.g.::\n\n        >>> @allel.util.hdf5_cache(names=['z', 'x', 'y'])\n        ... def baz(n):\n        ...     print('executing baz')\n        ...     a = np.arange(n)\n        ...     return a, a**2, n**2\n        ...\n        >>> baz(3)\n        executing baz\n        (array([0, 1, 2]), array([0, 1, 4]), 9)\n        >>> baz(3)\n        (array([0, 1, 2]), array([0, 1, 4]), 9)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform principal components analysis of genotypes at biallelic variants of the given species.", "response": "def pca(gn, n_components=10, copy=True, scaler='patterson', ploidy=2):\n    \"\"\"Perform principal components analysis of genotype data, via singular\n    value decomposition.\n\n    Parameters\n    ----------\n\n    gn : array_like, float, shape (n_variants, n_samples)\n        Genotypes at biallelic variants, coded as the number of alternate\n        alleles per call (i.e., 0 = hom ref, 1 = het, 2 = hom alt).\n    n_components : int, optional\n        Number of components to keep.\n    copy : bool, optional\n        If False, data passed to fit are overwritten.\n    scaler : {'patterson', 'standard', None}\n        Scaling method; 'patterson' applies the method of Patterson et al\n        2006; 'standard' scales to unit variance; None centers the data only.\n    ploidy : int, optional\n        Sample ploidy, only relevant if 'patterson' scaler is used.\n\n    Returns\n    -------\n\n    coords : ndarray, float, shape (n_samples, n_components)\n        Transformed coordinates for the samples.\n    model : GenotypePCA\n        Model instance containing the variance ratio explained and the stored\n        components (a.k.a., loadings). Can be used to project further data\n        into the same principal components space via the transform() method.\n\n    Notes\n    -----\n\n    Genotype data should be filtered prior to using this function to remove\n    variants in linkage disequilibrium.\n\n    See Also\n    --------\n\n    randomized_pca, allel.stats.ld.locate_unlinked\n\n    \"\"\"\n\n    # set up the model\n    model = GenotypePCA(n_components, copy=copy, scaler=scaler, ploidy=ploidy)\n\n    # fit the model and project the input data onto the new dimensions\n    coords = model.fit_transform(gn)\n\n    return coords, model"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef randomized_pca(gn, n_components=10, copy=True, iterated_power=3,\n                   random_state=None, scaler='patterson', ploidy=2):\n    \"\"\"Perform principal components analysis of genotype data, via an\n    approximate truncated singular value decomposition using randomization\n    to speed up the computation.\n\n    Parameters\n    ----------\n\n    gn : array_like, float, shape (n_variants, n_samples)\n        Genotypes at biallelic variants, coded as the number of alternate\n        alleles per call (i.e., 0 = hom ref, 1 = het, 2 = hom alt).\n    n_components : int, optional\n        Number of components to keep.\n    copy : bool, optional\n        If False, data passed to fit are overwritten.\n    iterated_power : int, optional\n        Number of iterations for the power method.\n    random_state : int or RandomState instance or None (default)\n        Pseudo Random Number generator seed control. If None, use the\n        numpy.random singleton.\n    scaler : {'patterson', 'standard', None}\n        Scaling method; 'patterson' applies the method of Patterson et al\n        2006; 'standard' scales to unit variance; None centers the data only.\n    ploidy : int, optional\n        Sample ploidy, only relevant if 'patterson' scaler is used.\n\n    Returns\n    -------\n\n    coords : ndarray, float, shape (n_samples, n_components)\n        Transformed coordinates for the samples.\n    model : GenotypeRandomizedPCA\n        Model instance containing the variance ratio explained and the stored\n        components (a.k.a., loadings). Can be used to project further data\n        into the same principal components space via the transform() method.\n\n    Notes\n    -----\n\n    Genotype data should be filtered prior to using this function to remove\n    variants in linkage disequilibrium.\n\n    Based on the :class:`sklearn.decomposition.RandomizedPCA` implementation.\n\n    See Also\n    --------\n\n    pca, allel.stats.ld.locate_unlinked\n\n    \"\"\"\n\n    # set up the model\n    model = GenotypeRandomizedPCA(n_components, copy=copy,\n                                  iterated_power=iterated_power,\n                                  random_state=random_state, scaler=scaler,\n                                  ploidy=ploidy)\n\n    # fit the model and project the input data onto the new dimensions\n    coords = model.fit_transform(gn)\n\n    return coords, model", "response": "Perform randomized PCA decomposition of genotypes at biallelic variants of the given species."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nestimates F3 of the moving patterson distribution for a single set of variants.", "response": "def moving_patterson_f3(acc, aca, acb, size, start=0, stop=None, step=None,\n                        normed=True):\n    \"\"\"Estimate F3(C; A, B) in moving windows.\n\n    Parameters\n    ----------\n    acc : array_like, int, shape (n_variants, 2)\n        Allele counts for the test population (C).\n    aca : array_like, int, shape (n_variants, 2)\n        Allele counts for the first source population (A).\n    acb : array_like, int, shape (n_variants, 2)\n        Allele counts for the second source population (B).\n    size : int\n        The window size (number of variants).\n    start : int, optional\n        The index at which to start.\n    stop : int, optional\n        The index at which to stop.\n    step : int, optional\n        The number of variants between start positions of windows. If not\n        given, defaults to the window size, i.e., non-overlapping windows.\n    normed : bool, optional\n        If False, use un-normalised f3 values.\n\n    Returns\n    -------\n    f3 : ndarray, float, shape (n_windows,)\n        Estimated value of the statistic in each window.\n\n    \"\"\"\n\n    # calculate per-variant values\n    T, B = patterson_f3(acc, aca, acb)\n\n    # calculate value of statistic within each block\n    if normed:\n        T_bsum = moving_statistic(T, statistic=np.nansum, size=size,\n                                  start=start, stop=stop, step=step)\n        B_bsum = moving_statistic(B, statistic=np.nansum, size=size,\n                                  start=start, stop=stop, step=step)\n        f3 = T_bsum / B_bsum\n\n    else:\n        f3 = moving_statistic(T, statistic=np.nanmean, size=size,\n                              start=start, stop=stop, step=step)\n\n    return f3"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nestimates the density of the statistic in each block of moving windows.", "response": "def moving_patterson_d(aca, acb, acc, acd, size, start=0, stop=None,\n                       step=None):\n    \"\"\"Estimate D(A, B; C, D) in moving windows.\n\n    Parameters\n    ----------\n    aca : array_like, int, shape (n_variants, 2),\n        Allele counts for population A.\n    acb : array_like, int, shape (n_variants, 2)\n        Allele counts for population B.\n    acc : array_like, int, shape (n_variants, 2)\n        Allele counts for population C.\n    acd : array_like, int, shape (n_variants, 2)\n        Allele counts for population D.\n    size : int\n        The window size (number of variants).\n    start : int, optional\n        The index at which to start.\n    stop : int, optional\n        The index at which to stop.\n    step : int, optional\n        The number of variants between start positions of windows. If not\n        given, defaults to the window size, i.e., non-overlapping windows.\n\n    Returns\n    -------\n    d : ndarray, float, shape (n_windows,)\n        Estimated value of the statistic in each window.\n\n    \"\"\"\n\n    # calculate per-variant values\n    num, den = patterson_d(aca, acb, acc, acd)\n\n    # N.B., nans can occur if any of the populations have completely missing\n    # genotype calls at a variant (i.e., allele number is zero). Here we\n    # assume that is rare enough to be negligible.\n\n    # compute the numerator and denominator within each window\n    num_sum = moving_statistic(num, statistic=np.nansum, size=size,\n                               start=start, stop=stop, step=step)\n    den_sum = moving_statistic(den, statistic=np.nansum, size=size,\n                               start=start, stop=stop, step=step)\n\n    # calculate the statistic values in each block\n    d = num_sum / den_sum\n\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef average_patterson_f3(acc, aca, acb, blen, normed=True):\n\n    # calculate per-variant values\n    T, B = patterson_f3(acc, aca, acb)\n\n    # N.B., nans can occur if any of the populations have completely missing\n    # genotype calls at a variant (i.e., allele number is zero). Here we\n    # assume that is rare enough to be negligible.\n\n    # calculate overall value of statistic\n    if normed:\n        f3 = np.nansum(T) / np.nansum(B)\n    else:\n        f3 = np.nanmean(T)\n\n    # calculate value of statistic within each block\n    if normed:\n        T_bsum = moving_statistic(T, statistic=np.nansum, size=blen)\n        B_bsum = moving_statistic(B, statistic=np.nansum, size=blen)\n        vb = T_bsum / B_bsum\n        _, se, vj = jackknife((T_bsum, B_bsum),\n                              statistic=lambda t, b: np.sum(t) / np.sum(b))\n\n    else:\n        vb = moving_statistic(T, statistic=np.nanmean, size=blen)\n        _, se, vj = jackknife(vb, statistic=np.mean)\n\n    # compute Z score\n    z = f3 / se\n\n    return f3, se, z, vb, vj", "response": "Estimate F3 for each test population A and B and returns the average of the statistic F3 for each source population A and B."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nestimating D and standard error using the Patterson method.", "response": "def average_patterson_d(aca, acb, acc, acd, blen):\n    \"\"\"Estimate D(A, B; C, D) and standard error using the block-jackknife.\n\n    Parameters\n    ----------\n    aca : array_like, int, shape (n_variants, 2),\n        Allele counts for population A.\n    acb : array_like, int, shape (n_variants, 2)\n        Allele counts for population B.\n    acc : array_like, int, shape (n_variants, 2)\n        Allele counts for population C.\n    acd : array_like, int, shape (n_variants, 2)\n        Allele counts for population D.\n    blen : int\n        Block size (number of variants).\n\n    Returns\n    -------\n    d : float\n        Estimated value of the statistic using all data.\n    se : float\n        Estimated standard error.\n    z : float\n        Z-score (number of standard errors from zero).\n    vb : ndarray, float, shape (n_blocks,)\n        Value of the statistic in each block.\n    vj : ndarray, float, shape (n_blocks,)\n        Values of the statistic from block-jackknife resampling.\n\n    Notes\n    -----\n    See Patterson (2012), main text and Appendix A.\n\n    See Also\n    --------\n    allel.stats.admixture.patterson_d\n\n    \"\"\"\n\n    # calculate per-variant values\n    num, den = patterson_d(aca, acb, acc, acd)\n\n    # N.B., nans can occur if any of the populations have completely missing\n    # genotype calls at a variant (i.e., allele number is zero). Here we\n    # assume that is rare enough to be negligible.\n\n    # calculate overall estimate\n    d_avg = np.nansum(num) / np.nansum(den)\n\n    # compute the numerator and denominator within each block\n    num_bsum = moving_statistic(num, statistic=np.nansum, size=blen)\n    den_bsum = moving_statistic(den, statistic=np.nansum, size=blen)\n\n    # calculate the statistic values in each block\n    vb = num_bsum / den_bsum\n\n    # estimate standard error\n    _, se, vj = jackknife((num_bsum, den_bsum),\n                          statistic=lambda n, d: np.sum(n) / np.sum(d))\n\n    # compute Z score\n    z = d_avg / se\n\n    return d_avg, se, z, vb, vj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_chunks(data, chunks=None):\n\n    if chunks is None:\n\n        if hasattr(data, 'chunklen') and hasattr(data, 'shape'):\n            # bcolz carray, chunk first dimension only\n            return (data.chunklen,) + data.shape[1:]\n\n        elif hasattr(data, 'chunks') and hasattr(data, 'shape') and \\\n                len(data.chunks) == len(data.shape):\n            # h5py dataset or zarr array\n            return data.chunks\n\n        else:\n            # fall back to something simple, ~4Mb chunks of first dimension\n            row = np.asarray(data[0])\n            chunklen = max(1, (2**22) // row.nbytes)\n            if row.shape:\n                chunks = (chunklen,) + row.shape\n            else:\n                chunks = (chunklen,)\n            return chunks\n\n    else:\n\n        return chunks", "response": "Try to guess a reasonable chunk shape to use for block - wise\n    algorithms operating over data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\niterates over records in a GFF3 file.", "response": "def iter_gff3(path, attributes=None, region=None, score_fill=-1,\n              phase_fill=-1, attributes_fill='.', tabix='tabix'):\n    \"\"\"Iterate over records in a GFF3 file.\n\n    Parameters\n    ----------\n    path : string\n        Path to input file.\n    attributes : list of strings, optional\n        List of columns to extract from the \"attributes\" field.\n    region : string, optional\n        Genome region to extract. If given, file must be position\n        sorted, bgzipped and tabix indexed. Tabix must also be installed\n        and on the system path.\n    score_fill : int, optional\n        Value to use where score field has a missing value.\n    phase_fill : int, optional\n        Value to use where phase field has a missing value.\n    attributes_fill : object or list of objects, optional\n        Value(s) to use where attribute field(s) have a missing value.\n    tabix : string\n        Tabix command.\n\n    Returns\n    -------\n    Iterator\n\n    \"\"\"\n\n    # prepare fill values for attributes\n    if attributes is not None:\n        attributes = list(attributes)\n        if isinstance(attributes_fill, (list, tuple)):\n            if len(attributes) != len(attributes_fill):\n                raise ValueError('number of fills does not match attributes')\n        else:\n            attributes_fill = [attributes_fill] * len(attributes)\n\n    # open input stream\n    if region is not None:\n        cmd = [tabix, path, region]\n        buffer = subprocess.Popen(cmd, stdout=subprocess.PIPE).stdout\n    elif path.endswith('.gz') or path.endswith('.bgz'):\n        buffer = gzip.open(path, mode='rb')\n    else:\n        buffer = open(path, mode='rb')\n\n    try:\n        for line in buffer:\n            if line[0] == b'>':\n                # assume begin embedded FASTA\n                return\n            if line[0] == b'#':\n                # skip comment lines\n                continue\n            vals = line.split(b'\\t')\n            if len(vals) == 9:\n                # unpack for processing\n                fseqid, fsource, ftype, fstart, fend, fscore, fstrand, fphase, fattrs = vals\n                # convert numerics\n                fstart = int(fstart)\n                fend = int(fend)\n                if fscore == b'.':\n                    fscore = score_fill\n                else:\n                    fscore = float(fscore)\n                if fphase == b'.':\n                    fphase = phase_fill\n                else:\n                    fphase = int(fphase)\n                if not PY2:\n                    fseqid = str(fseqid, 'ascii')\n                    fsource = str(fsource, 'ascii')\n                    ftype = str(ftype, 'ascii')\n                    fstrand = str(fstrand, 'ascii')\n                    fattrs = str(fattrs, 'ascii')\n                rec = (fseqid, fsource, ftype, fstart, fend, fscore, fstrand, fphase)\n                if attributes is not None:\n                    dattrs = gff3_parse_attributes(fattrs)\n                    vattrs = tuple(\n                        dattrs.get(k, f)\n                        for k, f in zip(attributes, attributes_fill)\n                    )\n                    rec += vattrs\n                yield rec\n\n    finally:\n        buffer.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gff3_to_recarray(path, attributes=None, region=None, score_fill=-1,\n                     phase_fill=-1, attributes_fill='.', tabix='tabix', dtype=None):\n    \"\"\"Load data from a GFF3 into a NumPy recarray.\n\n    Parameters\n    ----------\n    path : string\n        Path to input file.\n    attributes : list of strings, optional\n        List of columns to extract from the \"attributes\" field.\n    region : string, optional\n        Genome region to extract. If given, file must be position\n        sorted, bgzipped and tabix indexed. Tabix must also be installed\n        and on the system path.\n    score_fill : int, optional\n        Value to use where score field has a missing value.\n    phase_fill : int, optional\n        Value to use where phase field has a missing value.\n    attributes_fill : object or list of objects, optional\n        Value(s) to use where attribute field(s) have a missing value.\n    tabix : string, optional\n        Tabix command.\n    dtype : dtype, optional\n        Override dtype.\n\n    Returns\n    -------\n    np.recarray\n\n    \"\"\"\n\n    # read records\n    recs = list(iter_gff3(path, attributes=attributes, region=region,\n                          score_fill=score_fill, phase_fill=phase_fill,\n                          attributes_fill=attributes_fill, tabix=tabix))\n\n    if not recs:\n        return None\n\n    # determine dtype\n    if dtype is None:\n        dtype = [('seqid', object),\n                 ('source', object),\n                 ('type', object),\n                 ('start', int),\n                 ('end', int),\n                 ('score', float),\n                 ('strand', object),\n                 ('phase', int)]\n        if attributes:\n            for n in attributes:\n                dtype.append((n, object))\n\n    a = np.rec.fromrecords(recs, dtype=dtype)\n    return a", "response": "Load data from a GFF3 file into a NumPy recarray."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gff3_to_dataframe(path, attributes=None, region=None, score_fill=-1,\n                      phase_fill=-1, attributes_fill='.', tabix='tabix', **kwargs):\n    \"\"\"Load data from a GFF3 into a pandas DataFrame.\n\n    Parameters\n    ----------\n    path : string\n        Path to input file.\n    attributes : list of strings, optional\n        List of columns to extract from the \"attributes\" field.\n    region : string, optional\n        Genome region to extract. If given, file must be position\n        sorted, bgzipped and tabix indexed. Tabix must also be installed\n        and on the system path.\n    score_fill : int, optional\n        Value to use where score field has a missing value.\n    phase_fill : int, optional\n        Value to use where phase field has a missing value.\n    attributes_fill : object or list of objects, optional\n        Value(s) to use where attribute field(s) have a missing value.\n    tabix : string, optional\n        Tabix command.\n\n    Returns\n    -------\n    pandas.DataFrame\n\n    \"\"\"\n\n    import pandas\n\n    # read records\n    recs = list(iter_gff3(path, attributes=attributes, region=region,\n                          score_fill=score_fill, phase_fill=phase_fill,\n                          attributes_fill=attributes_fill, tabix=tabix))\n\n    # load into pandas\n    columns = ['seqid', 'source', 'type', 'start', 'end', 'score', 'strand', 'phase']\n    if attributes:\n        columns += list(attributes)\n    df = pandas.DataFrame.from_records(recs, columns=columns, **kwargs)\n\n    return df", "response": "Load data from a GFF3 file into a pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ehh_decay(h, truncate=False):\n\n    # check inputs\n    # N.B., ensure int8 so we can use cython optimisation\n    h = HaplotypeArray(np.asarray(h), copy=False)\n    if h.min() < 0:\n        raise NotImplementedError('missing calls are not supported')\n\n    # initialise\n    n_variants = h.n_variants  # number of rows, i.e., variants\n    n_haplotypes = h.n_haplotypes  # number of columns, i.e., haplotypes\n    n_pairs = (n_haplotypes * (n_haplotypes - 1)) // 2\n\n    # compute the shared prefix length between all pairs of haplotypes\n    spl = pairwise_shared_prefix_lengths(memoryview_safe(np.asarray(h)))\n\n    # compute EHH by counting the number of shared prefixes extending beyond\n    # each variant\n    minlength = None if truncate else n_variants + 1\n    b = np.bincount(spl, minlength=minlength)\n    c = np.cumsum(b[::-1])[:-1]\n    ehh = (c / n_pairs)[::-1]\n\n    return ehh", "response": "Compute the decay of extended haplotype homozygosity at successive variants."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef voight_painting(h):\n\n    # check inputs\n    # N.B., ensure int8 so we can use cython optimisation\n    h = HaplotypeArray(np.asarray(h), copy=False)\n    if h.max() > 1:\n        raise NotImplementedError('only biallelic variants are supported')\n    if h.min() < 0:\n        raise NotImplementedError('missing calls are not supported')\n\n    # sort by prefix\n    indices = h.prefix_argsort()\n    h = np.take(h, indices, axis=1)\n\n    # paint\n    painting = paint_shared_prefixes(memoryview_safe(np.asarray(h)))\n\n    return painting, indices", "response": "Paints the haplotypes in the hierarchy of shared haplotypes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot a painting of shared haplotypes.", "response": "def plot_voight_painting(painting, palette='colorblind', flank='right',\n                         ax=None, height_factor=0.01):\n    \"\"\"Plot a painting of shared haplotype prefixes.\n\n    Parameters\n    ----------\n    painting : array_like, int, shape (n_variants, n_haplotypes)\n        Painting array.\n    ax : axes, optional\n        The axes on which to draw. If not provided, a new figure will be\n        created.\n    palette : string, optional\n        A Seaborn palette name.\n    flank : {'right', 'left'}, optional\n        If left, painting will be reversed along first axis.\n    height_factor : float, optional\n        If no axes provided, determine height of figure by multiplying\n        height of painting array by this number.\n\n    Returns\n    -------\n    ax : axes\n\n    \"\"\"\n\n    import seaborn as sns\n    from matplotlib.colors import ListedColormap\n    import matplotlib.pyplot as plt\n\n    if flank == 'left':\n        painting = painting[::-1]\n\n    n_colors = painting.max()\n    palette = sns.color_palette(palette, n_colors)\n    # use white for singleton haplotypes\n    cmap = ListedColormap(['white'] + palette)\n\n    # setup axes\n    if ax is None:\n        w = plt.rcParams['figure.figsize'][0]\n        h = height_factor*painting.shape[1]\n        fig, ax = plt.subplots(figsize=(w, h))\n        sns.despine(ax=ax, bottom=True, left=True)\n\n    ax.pcolormesh(painting.T, cmap=cmap)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlim(0, painting.shape[0])\n    ax.set_ylim(0, painting.shape[1])\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a figure of shared haplotypes for both left and right flanks centred on some variant of choice.", "response": "def fig_voight_painting(h, index=None, palette='colorblind',\n                        height_factor=0.01, fig=None):\n    \"\"\"Make a figure of shared haplotype prefixes for both left and right\n    flanks, centred on some variant of choice.\n\n    Parameters\n    ----------\n    h : array_like, int, shape (n_variants, n_haplotypes)\n        Haplotype array.\n    index : int, optional\n        Index of the variant within the haplotype array to centre on. If not\n        provided, the middle variant will be used.\n    palette : string, optional\n        A Seaborn palette name.\n    height_factor : float, optional\n        If no axes provided, determine height of figure by multiplying\n        height of painting array by this number.\n    fig : figure\n        The figure on which to draw. If not provided, a new figure will be\n        created.\n\n    Returns\n    -------\n    fig : figure\n\n    Notes\n    -----\n    N.B., the ordering of haplotypes on the left and right flanks will be\n    different. This means that haplotypes on the right flank **will not**\n    correspond to haplotypes on the left flank at the same vertical position.\n\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n    from matplotlib.gridspec import GridSpec\n    import seaborn as sns\n\n    # check inputs\n    h = asarray_ndim(h, 2)\n    if index is None:\n        # use midpoint\n        index = h.shape[0] // 2\n\n    # divide data into two flanks\n    hl = h[:index+1][::-1]\n    hr = h[index:]\n\n    # paint both flanks\n    pl, il = voight_painting(hl)\n    pr, ir = voight_painting(hr)\n\n    # compute ehh decay for both flanks\n    el = ehh_decay(hl, truncate=False)\n    er = ehh_decay(hr, truncate=False)\n\n    # setup figure\n    # fixed height for EHH decay subplot\n    h_ehh = plt.rcParams['figure.figsize'][1] // 3\n    # add height for paintings\n    h_painting = height_factor*h.shape[1]\n    if fig is None:\n        w = plt.rcParams['figure.figsize'][0]\n        h = h_ehh + h_painting\n        fig = plt.figure(figsize=(w, h))\n\n    # setup gridspec\n    gs = GridSpec(2, 2,\n                  width_ratios=[hl.shape[0], hr.shape[0]],\n                  height_ratios=[h_painting, h_ehh])\n\n    # plot paintings\n    ax = fig.add_subplot(gs[0, 0])\n    sns.despine(ax=ax, left=True, bottom=True)\n    plot_voight_painting(pl, palette=palette, flank='left', ax=ax)\n    ax = fig.add_subplot(gs[0, 1])\n    sns.despine(ax=ax, left=True, bottom=True)\n    plot_voight_painting(pr, palette=palette, flank='right', ax=ax)\n\n    # plot ehh\n    ax = fig.add_subplot(gs[1, 0])\n    sns.despine(ax=ax, offset=3)\n    x = np.arange(el.shape[0])\n    y = el\n    ax.fill_between(x, 0, y)\n    ax.set_ylim(0, 1)\n    ax.set_yticks([0, 1])\n    ax.set_ylabel('EHH')\n    ax.invert_xaxis()\n    ax = fig.add_subplot(gs[1, 1])\n    sns.despine(ax=ax, left=True, right=False, offset=3)\n    ax.yaxis.tick_right()\n    ax.set_ylim(0, 1)\n    ax.set_yticks([0, 1])\n    x = np.arange(er.shape[0])\n    y = er\n    ax.fill_between(x, 0, y)\n\n    # tidy up\n    fig.tight_layout()\n\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_ihh_gaps(pos, map_pos, gap_scale, max_gap, is_accessible):\n\n    # check inputs\n    if map_pos is None:\n        # integrate over physical distance\n        map_pos = pos\n    else:\n        map_pos = asarray_ndim(map_pos, 1)\n        check_dim0_aligned(pos, map_pos)\n\n    # compute physical gaps\n    physical_gaps = np.diff(pos)\n\n    # compute genetic gaps\n    gaps = np.diff(map_pos).astype('f8')\n\n    if is_accessible is not None:\n\n        # compute accessible gaps\n        is_accessible = asarray_ndim(is_accessible, 1)\n        assert is_accessible.shape[0] > pos[-1], \\\n            'accessibility array too short'\n        accessible_gaps = np.zeros_like(physical_gaps)\n        for i in range(1, len(pos)):\n            # N.B., expect pos is 1-based\n            n_access = np.count_nonzero(is_accessible[pos[i-1]-1:pos[i]-1])\n            accessible_gaps[i-1] = n_access\n\n        # adjust using accessibility\n        scaling = accessible_gaps / physical_gaps\n        gaps = gaps * scaling\n\n    elif gap_scale is not None and gap_scale > 0:\n\n        scaling = np.ones(gaps.shape, dtype='f8')\n        loc_scale = physical_gaps > gap_scale\n        scaling[loc_scale] = gap_scale / physical_gaps[loc_scale]\n        gaps = gaps * scaling\n\n    if max_gap is not None and max_gap > 0:\n\n        # deal with very large gaps\n        gaps[physical_gaps > max_gap] = -1\n\n    return gaps", "response": "Compute the spacing between variants for integrating haplotype\n    homozygosity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ihs(h, pos, map_pos=None, min_ehh=0.05, min_maf=0.05, include_edges=False,\n        gap_scale=20000, max_gap=200000, is_accessible=None, use_threads=True):\n    \"\"\"Compute the unstandardized integrated haplotype score (IHS) for each\n    variant, comparing integrated haplotype homozygosity between the\n    reference (0) and alternate (1) alleles.\n\n    Parameters\n    ----------\n    h : array_like, int, shape (n_variants, n_haplotypes)\n        Haplotype array.\n    pos : array_like, int, shape (n_variants,)\n        Variant positions (physical distance).\n    map_pos : array_like, float, shape (n_variants,)\n        Variant positions (genetic map distance).\n    min_ehh: float, optional\n        Minimum EHH beyond which to truncate integrated haplotype\n        homozygosity calculation.\n    min_maf : float, optional\n        Do not compute integrated haplotype homozogysity for variants with\n        minor allele frequency below this value.\n    include_edges : bool, optional\n        If True, report scores even if EHH does not decay below `min_ehh`\n        before reaching the edge of the data.\n    gap_scale : int, optional\n        Rescale distance between variants if gap is larger than this value.\n    max_gap : int, optional\n        Do not report scores if EHH spans a gap larger than this number of\n        base pairs.\n    is_accessible : array_like, bool, optional\n        Genome accessibility array. If provided, distance between variants\n        will be computed as the number of accessible bases between them.\n    use_threads : bool, optional\n        If True use multiple threads to compute.\n\n    Returns\n    -------\n    score : ndarray, float, shape (n_variants,)\n        Unstandardized IHS scores.\n\n    Notes\n    -----\n\n    This function will calculate IHS for all variants. To exclude variants\n    below a given minor allele frequency, filter the input haplotype array\n    before passing to this function.\n\n    This function computes IHS comparing the reference and alternate alleles.\n    These can be polarised by switching the sign for any variant where the\n    reference allele is derived.\n\n    This function returns NaN for any IHS calculations where haplotype\n    homozygosity does not decay below `min_ehh` before reaching the first or\n    last variant. To disable this behaviour, set `include_edges` to True.\n\n    Note that the unstandardized score is returned. Usually these scores are\n    then standardized in different allele frequency bins.\n\n    See Also\n    --------\n    standardize_by_allele_count\n\n    \"\"\"\n\n    # check inputs\n    h = asarray_ndim(h, 2)\n    check_integer_dtype(h)\n    pos = asarray_ndim(pos, 1)\n    check_dim0_aligned(h, pos)\n    h = memoryview_safe(h)\n    pos = memoryview_safe(pos)\n\n    # compute gaps between variants for integration\n    gaps = compute_ihh_gaps(pos, map_pos, gap_scale, max_gap, is_accessible)\n\n    # setup kwargs\n    kwargs = dict(min_ehh=min_ehh, min_maf=min_maf, include_edges=include_edges)\n\n    if use_threads and multiprocessing.cpu_count() > 1:\n        # run with threads\n\n        # create pool\n        pool = ThreadPool(2)\n\n        # scan forward\n        result_fwd = pool.apply_async(ihh01_scan, (h, gaps), kwargs)\n\n        # scan backward\n        result_rev = pool.apply_async(ihh01_scan, (h[::-1], gaps[::-1]), kwargs)\n\n        # wait for both to finish\n        pool.close()\n        pool.join()\n\n        # obtain results\n        ihh0_fwd, ihh1_fwd = result_fwd.get()\n        ihh0_rev, ihh1_rev = result_rev.get()\n\n        # cleanup\n        pool.terminate()\n\n    else:\n        # run without threads\n\n        # scan forward\n        ihh0_fwd, ihh1_fwd = ihh01_scan(h, gaps, **kwargs)\n\n        # scan backward\n        ihh0_rev, ihh1_rev = ihh01_scan(h[::-1], gaps[::-1], **kwargs)\n\n    # handle reverse scan\n    ihh0_rev = ihh0_rev[::-1]\n    ihh1_rev = ihh1_rev[::-1]\n\n    # compute unstandardized score\n    ihh0 = ihh0_fwd + ihh0_rev\n    ihh1 = ihh1_fwd + ihh1_rev\n    score = np.log(ihh1 / ihh0)\n\n    return score", "response": "Compute the unstandardized integrated IHS for each variant in the given Haplotype array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xpehh(h1, h2, pos, map_pos=None, min_ehh=0.05, include_edges=False,\n          gap_scale=20000, max_gap=200000, is_accessible=None,\n          use_threads=True):\n    \"\"\"Compute the unstandardized cross-population extended haplotype\n    homozygosity score (XPEHH) for each variant.\n\n    Parameters\n    ----------\n    h1 : array_like, int, shape (n_variants, n_haplotypes)\n        Haplotype array for the first population.\n    h2 : array_like, int, shape (n_variants, n_haplotypes)\n        Haplotype array for the second population.\n    pos : array_like, int, shape (n_variants,)\n        Variant positions on physical or genetic map.\n    map_pos : array_like, float, shape (n_variants,)\n        Variant positions (genetic map distance).\n    min_ehh: float, optional\n        Minimum EHH beyond which to truncate integrated haplotype\n        homozygosity calculation.\n    include_edges : bool, optional\n        If True, report scores even if EHH does not decay below `min_ehh`\n        before reaching the edge of the data.\n    gap_scale : int, optional\n        Rescale distance between variants if gap is larger than this value.\n    max_gap : int, optional\n        Do not report scores if EHH spans a gap larger than this number of\n        base pairs.\n    is_accessible : array_like, bool, optional\n        Genome accessibility array. If provided, distance between variants\n        will be computed as the number of accessible bases between them.\n    use_threads : bool, optional\n        If True use multiple threads to compute.\n\n    Returns\n    -------\n    score : ndarray, float, shape (n_variants,)\n        Unstandardized XPEHH scores.\n\n    Notes\n    -----\n\n    This function will calculate XPEHH for all variants. To exclude variants\n    below a given minor allele frequency, filter the input haplotype arrays\n    before passing to this function.\n\n    This function returns NaN for any EHH calculations where haplotype\n    homozygosity does not decay below `min_ehh` before reaching the first or\n    last variant. To disable this behaviour, set `include_edges` to True.\n\n    Note that the unstandardized score is returned. Usually these scores are\n    then standardized genome-wide.\n\n    Haplotype arrays from the two populations may have different numbers of\n    haplotypes.\n\n    See Also\n    --------\n    standardize\n\n    \"\"\"\n\n    # check inputs\n    h1 = asarray_ndim(h1, 2)\n    check_integer_dtype(h1)\n    h2 = asarray_ndim(h2, 2)\n    check_integer_dtype(h2)\n    pos = asarray_ndim(pos, 1)\n    check_dim0_aligned(h1, h2, pos)\n    h1 = memoryview_safe(h1)\n    h2 = memoryview_safe(h2)\n    pos = memoryview_safe(pos)\n\n    # compute gaps between variants for integration\n    gaps = compute_ihh_gaps(pos, map_pos, gap_scale, max_gap, is_accessible)\n\n    # setup kwargs\n    kwargs = dict(min_ehh=min_ehh, include_edges=include_edges)\n\n    if use_threads and multiprocessing.cpu_count() > 1:\n        # use multiple threads\n\n        # setup threadpool\n        pool = ThreadPool(min(4, multiprocessing.cpu_count()))\n\n        # scan forward\n        res1_fwd = pool.apply_async(ihh_scan, (h1, gaps), kwargs)\n        res2_fwd = pool.apply_async(ihh_scan, (h2, gaps), kwargs)\n\n        # scan backward\n        res1_rev = pool.apply_async(ihh_scan, (h1[::-1], gaps[::-1]), kwargs)\n        res2_rev = pool.apply_async(ihh_scan, (h2[::-1], gaps[::-1]), kwargs)\n\n        # wait for both to finish\n        pool.close()\n        pool.join()\n\n        # obtain results\n        ihh1_fwd = res1_fwd.get()\n        ihh2_fwd = res2_fwd.get()\n        ihh1_rev = res1_rev.get()\n        ihh2_rev = res2_rev.get()\n\n        # cleanup\n        pool.terminate()\n\n    else:\n        # compute without threads\n\n        # scan forward\n        ihh1_fwd = ihh_scan(h1, gaps, **kwargs)\n        ihh2_fwd = ihh_scan(h2, gaps, **kwargs)\n\n        # scan backward\n        ihh1_rev = ihh_scan(h1[::-1], gaps[::-1], **kwargs)\n        ihh2_rev = ihh_scan(h2[::-1], gaps[::-1], **kwargs)\n\n    # handle reverse scans\n    ihh1_rev = ihh1_rev[::-1]\n    ihh2_rev = ihh2_rev[::-1]\n\n    # compute unstandardized score\n    ihh1 = ihh1_fwd + ihh1_rev\n    ihh2 = ihh2_fwd + ihh2_rev\n    score = np.log(ihh1 / ihh2)\n\n    return score", "response": "Compute the unstandardized cross - population extended haplotype homozygosity score for each variant."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nsl(h, use_threads=True):\n\n    # check inputs\n    h = asarray_ndim(h, 2)\n    check_integer_dtype(h)\n    h = memoryview_safe(h)\n\n    # # check there are no invariant sites\n    # ac = h.count_alleles()\n    # assert np.all(ac.is_segregating()), 'please remove non-segregating sites'\n\n    if use_threads and multiprocessing.cpu_count() > 1:\n\n        # create pool\n        pool = ThreadPool(2)\n\n        # scan forward\n        result_fwd = pool.apply_async(nsl01_scan, args=(h,))\n\n        # scan backward\n        result_rev = pool.apply_async(nsl01_scan, args=(h[::-1],))\n\n        # wait for both to finish\n        pool.close()\n        pool.join()\n\n        # obtain results\n        nsl0_fwd, nsl1_fwd = result_fwd.get()\n        nsl0_rev, nsl1_rev = result_rev.get()\n\n    else:\n\n        # scan forward\n        nsl0_fwd, nsl1_fwd = nsl01_scan(h)\n\n        # scan backward\n        nsl0_rev, nsl1_rev = nsl01_scan(h[::-1])\n\n    # handle backwards\n    nsl0_rev = nsl0_rev[::-1]\n    nsl1_rev = nsl1_rev[::-1]\n\n    # compute unstandardized score\n    nsl0 = nsl0_fwd + nsl0_rev\n    nsl1 = nsl1_fwd + nsl1_rev\n    score = np.log(nsl1 / nsl0)\n\n    return score", "response": "Compute the unstandardized number of segregating sites by length nSl for each variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xpnsl(h1, h2, use_threads=True):\n\n    # check inputs\n    h1 = asarray_ndim(h1, 2)\n    check_integer_dtype(h1)\n    h2 = asarray_ndim(h2, 2)\n    check_integer_dtype(h2)\n    check_dim0_aligned(h1, h2)\n    h1 = memoryview_safe(h1)\n    h2 = memoryview_safe(h2)\n\n    if use_threads and multiprocessing.cpu_count() > 1:\n        # use multiple threads\n\n        # setup threadpool\n        pool = ThreadPool(min(4, multiprocessing.cpu_count()))\n\n        # scan forward\n        res1_fwd = pool.apply_async(nsl_scan, args=(h1,))\n        res2_fwd = pool.apply_async(nsl_scan, args=(h2,))\n\n        # scan backward\n        res1_rev = pool.apply_async(nsl_scan, args=(h1[::-1],))\n        res2_rev = pool.apply_async(nsl_scan, args=(h2[::-1],))\n\n        # wait for both to finish\n        pool.close()\n        pool.join()\n\n        # obtain results\n        nsl1_fwd = res1_fwd.get()\n        nsl2_fwd = res2_fwd.get()\n        nsl1_rev = res1_rev.get()\n        nsl2_rev = res2_rev.get()\n\n        # cleanup\n        pool.terminate()\n\n    else:\n        # compute without threads\n\n        # scan forward\n        nsl1_fwd = nsl_scan(h1)\n        nsl2_fwd = nsl_scan(h2)\n\n        # scan backward\n        nsl1_rev = nsl_scan(h1[::-1])\n        nsl2_rev = nsl_scan(h2[::-1])\n\n    # handle reverse scans\n    nsl1_rev = nsl1_rev[::-1]\n    nsl2_rev = nsl2_rev[::-1]\n\n    # compute unstandardized score\n    nsl1 = nsl1_fwd + nsl1_rev\n    nsl2 = nsl2_fwd + nsl2_rev\n    score = np.log(nsl1 / nsl2)\n\n    return score", "response": "Cross - population version of the NSL statistic."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates the haplotype diversity of a single class or class.", "response": "def haplotype_diversity(h):\n    \"\"\"Estimate haplotype diversity.\n\n    Parameters\n    ----------\n    h : array_like, int, shape (n_variants, n_haplotypes)\n        Haplotype array.\n\n    Returns\n    -------\n    hd : float\n        Haplotype diversity.\n\n    \"\"\"\n\n    # check inputs\n    h = HaplotypeArray(h, copy=False)\n\n    # number of haplotypes\n    n = h.n_haplotypes\n\n    # compute haplotype frequencies\n    f = h.distinct_frequencies()\n\n    # estimate haplotype diversity\n    hd = (1 - np.sum(f**2)) * n / (n - 1)\n\n    return hd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nestimating haplotype diversity in moving windows.", "response": "def moving_haplotype_diversity(h, size, start=0, stop=None, step=None):\n    \"\"\"Estimate haplotype diversity in moving windows.\n\n    Parameters\n    ----------\n    h : array_like, int, shape (n_variants, n_haplotypes)\n        Haplotype array.\n    size : int\n        The window size (number of variants).\n    start : int, optional\n        The index at which to start.\n    stop : int, optional\n        The index at which to stop.\n    step : int, optional\n        The number of variants between start positions of windows. If not\n        given, defaults to the window size, i.e., non-overlapping windows.\n\n    Returns\n    -------\n    hd : ndarray, float, shape (n_windows,)\n        Haplotype diversity.\n\n    \"\"\"\n\n    hd = moving_statistic(values=h, statistic=haplotype_diversity, size=size,\n                          start=start, stop=stop, step=step)\n    return hd"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the H1 H12 H123 and H2 statistics for detecting signatures of soft sweeps.", "response": "def garud_h(h):\n    \"\"\"Compute the H1, H12, H123 and H2/H1 statistics for detecting signatures\n    of soft sweeps, as defined in Garud et al. (2015).\n\n    Parameters\n    ----------\n    h : array_like, int, shape (n_variants, n_haplotypes)\n        Haplotype array.\n\n    Returns\n    -------\n    h1 : float\n        H1 statistic (sum of squares of haplotype frequencies).\n    h12 : float\n        H12 statistic (sum of squares of haplotype frequencies, combining\n        the two most common haplotypes into a single frequency).\n    h123 : float\n        H123 statistic (sum of squares of haplotype frequencies, combining\n        the three most common haplotypes into a single frequency).\n    h2_h1 : float\n        H2/H1 statistic, indicating the \"softness\" of a sweep.\n\n    \"\"\"\n\n    # check inputs\n    h = HaplotypeArray(h, copy=False)\n\n    # compute haplotype frequencies\n    f = h.distinct_frequencies()\n\n    # compute H1\n    h1 = np.sum(f**2)\n\n    # compute H12\n    h12 = np.sum(f[:2])**2 + np.sum(f[2:]**2)\n\n    # compute H123\n    h123 = np.sum(f[:3])**2 + np.sum(f[3:]**2)\n\n    # compute H2/H1\n    h2 = h1 - f[0]**2\n    h2_h1 = h2 / h1\n\n    return h1, h12, h123, h2_h1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the H1 H12 H123 and H2 H1 statistics for detecting signatures of soft sweeps in moving windows.", "response": "def moving_garud_h(h, size, start=0, stop=None, step=None):\n    \"\"\"Compute the H1, H12, H123 and H2/H1 statistics for detecting signatures\n    of soft sweeps, as defined in Garud et al. (2015), in moving windows,\n\n    Parameters\n    ----------\n    h : array_like, int, shape (n_variants, n_haplotypes)\n        Haplotype array.\n    size : int\n        The window size (number of variants).\n    start : int, optional\n        The index at which to start.\n    stop : int, optional\n        The index at which to stop.\n    step : int, optional\n        The number of variants between start positions of windows. If not\n        given, defaults to the window size, i.e., non-overlapping windows.\n\n    Returns\n    -------\n    h1 : ndarray, float, shape (n_windows,)\n        H1 statistics (sum of squares of haplotype frequencies).\n    h12 : ndarray, float, shape (n_windows,)\n        H12 statistics (sum of squares of haplotype frequencies, combining\n        the two most common haplotypes into a single frequency).\n    h123 : ndarray, float, shape (n_windows,)\n        H123 statistics (sum of squares of haplotype frequencies, combining\n        the three most common haplotypes into a single frequency).\n    h2_h1 : ndarray, float, shape (n_windows,)\n        H2/H1 statistics, indicating the \"softness\" of a sweep.\n\n    \"\"\"\n\n    gh = moving_statistic(values=h, statistic=garud_h, size=size, start=start,\n                          stop=stop, step=step)\n\n    h1 = gh[:, 0]\n    h12 = gh[:, 1]\n    h123 = gh[:, 2]\n    h2_h1 = gh[:, 3]\n\n    return h1, h12, h123, h2_h1"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the haplotype frequencies.", "response": "def plot_haplotype_frequencies(h, palette='Paired', singleton_color='w',\n                               ax=None):\n    \"\"\"Plot haplotype frequencies.\n\n    Parameters\n    ----------\n    h : array_like, int, shape (n_variants, n_haplotypes)\n        Haplotype array.\n    palette : string, optional\n        A Seaborn palette name.\n    singleton_color : string, optional\n        Color to paint singleton haplotypes.\n    ax : axes, optional\n        The axes on which to draw. If not provided, a new figure will be\n        created.\n\n    Returns\n    -------\n    ax : axes\n\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    # check inputs\n    h = HaplotypeArray(h, copy=False)\n\n    # setup figure\n    if ax is None:\n        width = plt.rcParams['figure.figsize'][0]\n        height = width / 10\n        fig, ax = plt.subplots(figsize=(width, height))\n        sns.despine(ax=ax, left=True)\n\n    # count distinct haplotypes\n    hc = h.distinct_counts()\n\n    # setup palette\n    n_colors = np.count_nonzero(hc > 1)\n    palette = sns.color_palette(palette, n_colors)\n\n    # paint frequencies\n    x1 = 0\n    for i, c in enumerate(hc):\n        x2 = x1 + c\n        if c > 1:\n            color = palette[i]\n        else:\n            color = singleton_color\n        ax.axvspan(x1, x2, color=color)\n        x1 = x2\n\n    # tidy up\n    ax.set_xlim(0, h.shape[1])\n    ax.set_yticks([])\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the haplotype frequencies in moving windows over the genome.", "response": "def plot_moving_haplotype_frequencies(pos, h, size, start=0, stop=None, n=None,\n                                      palette='Paired', singleton_color='w',\n                                      ax=None):\n    \"\"\"Plot haplotype frequencies in moving windows over the genome.\n\n    Parameters\n    ----------\n    pos : array_like, int, shape (n_items,)\n        Variant positions, using 1-based coordinates, in ascending order.\n    h : array_like, int, shape (n_variants, n_haplotypes)\n        Haplotype array.\n    size : int\n        The window size (number of variants).\n    start : int, optional\n        The index at which to start.\n    stop : int, optional\n        The index at which to stop.\n    n : int, optional\n        Color only the `n` most frequent haplotypes (by default, all\n        non-singleton haplotypes are colored).\n    palette : string, optional\n        A Seaborn palette name.\n    singleton_color : string, optional\n        Color to paint singleton haplotypes.\n    ax : axes, optional\n        The axes on which to draw. If not provided, a new figure will be\n        created.\n\n    Returns\n    -------\n    ax : axes\n\n    \"\"\"\n\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    # setup figure\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # compute haplotype frequencies\n    # N.B., here we use a haplotype rank data structure to enable the use of\n    # pcolormesh() which is a lot faster than any other type of plotting\n    # function\n    hr = moving_hfs_rank(h, size=size, start=start, stop=stop)\n\n    # truncate to n most common haplotypes\n    if n:\n        hr[hr > n] = 0\n\n    # compute window start and stop positions\n    windows = moving_statistic(pos, statistic=lambda v: (v[0], v[-1]),\n                               size=size, start=start, stop=stop)\n\n    # create color map\n    colors = [singleton_color] + sns.color_palette(palette, n_colors=hr.max())\n    cmap = mpl.colors.ListedColormap(colors)\n\n    # draw colors\n    x = np.append(windows[:, 0], windows[-1, -1])\n    y = np.arange(h.shape[1]+1)\n    ax.pcolormesh(x, y, hr.T, cmap=cmap)\n\n    # tidy up\n    ax.set_xlim(windows[0, 0], windows[-1, -1])\n    ax.set_ylim(0, h.shape[1])\n    ax.set_ylabel('haplotype count')\n    ax.set_xlabel('position (bp)')\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the difference in Tajima s D between two populations in moving windows.", "response": "def moving_delta_tajima_d(ac1, ac2, size, start=0, stop=None, step=None):\n    \"\"\"Compute the difference in Tajima's D between two populations in\n    moving windows.\n\n    Parameters\n    ----------\n    ac1 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array for the first population.\n    ac2 : array_like, int, shape (n_variants, n_alleles)\n        Allele counts array for the second population.\n    size : int\n        The window size (number of variants).\n    start : int, optional\n        The index at which to start.\n    stop : int, optional\n        The index at which to stop.\n    step : int, optional\n        The number of variants between start positions of windows. If not\n        given, defaults to the window size, i.e., non-overlapping windows.\n\n    Returns\n    -------\n    delta_d : ndarray, float, shape (n_windows,)\n        Standardized delta Tajima's D.\n\n    See Also\n    --------\n    allel.stats.diversity.moving_tajima_d\n\n    \"\"\"\n\n    d1 = moving_tajima_d(ac1, size=size, start=start, stop=stop, step=step)\n    d2 = moving_tajima_d(ac2, size=size, start=start, stop=stop, step=step)\n    delta = d1 - d2\n    delta_z = (delta - np.mean(delta)) / np.std(delta)\n    return delta_z"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef standardize(score):\n    score = asarray_ndim(score, 1)\n    return (score - np.nanmean(score)) / np.nanstd(score)", "response": "Centre and scale to unit variance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef standardize_by_allele_count(score, aac, bins=None, n_bins=None,\n                                diagnostics=True):\n    \"\"\"Standardize `score` within allele frequency bins.\n\n    Parameters\n    ----------\n    score : array_like, float\n        The score to be standardized, e.g., IHS or NSL.\n    aac : array_like, int\n        An array of alternate allele counts.\n    bins : array_like, int, optional\n        Allele count bins, overrides `n_bins`.\n    n_bins : int, optional\n        Number of allele count bins to use.\n    diagnostics : bool, optional\n        If True, plot some diagnostic information about the standardization.\n\n    Returns\n    -------\n    score_standardized : ndarray, float\n        Standardized scores.\n    bins : ndarray, int\n        Allele count bins used for standardization.\n\n    \"\"\"\n\n    from scipy.stats import binned_statistic\n\n    # check inputs\n    score = asarray_ndim(score, 1)\n    aac = asarray_ndim(aac, 1)\n    check_dim0_aligned(score, aac)\n\n    # remove nans\n    nonan = ~np.isnan(score)\n    score_nonan = score[nonan]\n    aac_nonan = aac[nonan]\n\n    if bins is None:\n        # make our own similar sized bins\n\n        # how many bins to make?\n        if n_bins is None:\n            # something vaguely reasonable\n            n_bins = np.max(aac) // 2\n\n        # make bins\n        bins = make_similar_sized_bins(aac_nonan, n_bins)\n\n    else:\n        # user-provided bins\n        bins = asarray_ndim(bins, 1)\n\n    mean_score, _, _ = binned_statistic(aac_nonan, score_nonan,\n                                        statistic=np.mean,\n                                        bins=bins)\n    std_score, _, _ = binned_statistic(aac_nonan, score_nonan,\n                                       statistic=np.std,\n                                       bins=bins)\n\n    if diagnostics:\n        import matplotlib.pyplot as plt\n        x = (bins[:-1] + bins[1:]) / 2\n        plt.figure()\n        plt.fill_between(x,\n                         mean_score - std_score,\n                         mean_score + std_score,\n                         alpha=.5,\n                         label='std')\n        plt.plot(x, mean_score, marker='o', label='mean')\n        plt.grid(axis='y')\n        plt.xlabel('Alternate allele count')\n        plt.ylabel('Unstandardized score')\n        plt.title('Standardization diagnostics')\n        plt.legend()\n\n    # apply standardization\n    score_standardized = np.empty_like(score)\n    for i in range(len(bins) - 1):\n        x1 = bins[i]\n        x2 = bins[i + 1]\n        if i == 0:\n            # first bin\n            loc = (aac < x2)\n        elif i == len(bins) - 2:\n            # last bin\n            loc = (aac >= x1)\n        else:\n            # middle bins\n            loc = (aac >= x1) & (aac < x2)\n        m = mean_score[i]\n        s = std_score[i]\n        score_standardized[loc] = (score[loc] - m) / s\n\n    return score_standardized, bins", "response": "Standardize the score within the given allele count bins."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the branching statistic of two alleles.", "response": "def pbs(ac1, ac2, ac3, window_size, window_start=0, window_stop=None,\n        window_step=None, normed=True):\n    \"\"\"Compute the population branching statistic (PBS) which performs a comparison\n    of allele frequencies between three populations to detect genome regions that are\n    unusually differentiated in one population relative to the other two populations.\n\n    Parameters\n    ----------\n    ac1 : array_like, int\n        Allele counts from the first population.\n    ac2 : array_like, int\n        Allele counts from the second population.\n    ac3 : array_like, int\n        Allele counts from the third population.\n    window_size : int\n        The window size (number of variants) within which to compute PBS values.\n    window_start : int, optional\n        The variant index at which to start windowed calculations.\n    window_stop : int, optional\n        The variant index at which to stop windowed calculations.\n    window_step : int, optional\n        The number of variants between start positions of windows. If not given, defaults\n        to the window size, i.e., non-overlapping windows.\n    normed : bool, optional\n        If True (default), use the normalised version of PBS, also known as PBSn1 [2]_.\n        Otherwise, use the PBS statistic as originally defined in [1]_.\n\n    Returns\n    -------\n    pbs : ndarray, float\n        Windowed PBS values.\n\n    Notes\n    -----\n    The F:sub:`ST` calculations use Hudson's estimator.\n\n    References\n    ----------\n    .. [1] Yi et al., \"Sequencing of Fifty Human Exomes Reveals Adaptation to High\n       Altitude\", Science, 329(5987): 75\u201378, 2 July 2010.\n    .. [2] Malaspinas et al., \"A genomic history of Aboriginal Australia\", Nature. volume\n       538, pages 207\u2013214, 13 October 2016.\n\n    \"\"\"\n\n    # normalise and check inputs\n    ac1 = AlleleCountsArray(ac1)\n    ac2 = AlleleCountsArray(ac2)\n    ac3 = AlleleCountsArray(ac3)\n    check_dim0_aligned(ac1, ac2, ac3)\n\n    # compute fst\n    fst12 = moving_hudson_fst(ac1, ac2, size=window_size, start=window_start,\n                              stop=window_stop, step=window_step)\n    fst13 = moving_hudson_fst(ac1, ac3, size=window_size, start=window_start,\n                              stop=window_stop, step=window_step)\n    fst23 = moving_hudson_fst(ac2, ac3, size=window_size, start=window_start,\n                              stop=window_stop, step=window_step)\n\n    # clip fst values to avoid infinite if fst is 1\n    for x in fst12, fst13, fst23:\n        np.clip(x, a_min=0, a_max=0.99999, out=x)\n\n    # compute fst transform\n    t12 = -np.log(1 - fst12)\n    t13 = -np.log(1 - fst13)\n    t23 = -np.log(1 - fst23)\n\n    # compute pbs\n    ret = (t12 + t13 - t23) / 2\n\n    if normed:\n        # compute pbs normalising constant\n        norm = 1 + (t12 + t13 + t23) / 2\n        ret = ret / norm\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates a statistic in a moving window over values.", "response": "def moving_statistic(values, statistic, size, start=0, stop=None, step=None, **kwargs):\n    \"\"\"Calculate a statistic in a moving window over `values`.\n\n    Parameters\n    ----------\n\n    values : array_like\n        The data to summarise.\n    statistic : function\n        The statistic to compute within each window.\n    size : int\n        The window size (number of values).\n    start : int, optional\n        The index at which to start.\n    stop : int, optional\n        The index at which to stop.\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    kwargs\n        Additional keyword arguments are passed through to the `statistic`\n        function.\n\n    Returns\n    -------\n\n    out : ndarray, shape (n_windows,)\n\n    Examples\n    --------\n\n    >>> import allel\n    >>> values = [2, 5, 8, 16]\n    >>> allel.moving_statistic(values, np.sum, size=2)\n    array([ 7, 24])\n    >>> allel.moving_statistic(values, np.sum, size=2, step=1)\n    array([ 7, 13, 24])\n\n    \"\"\"\n\n    windows = index_windows(values, size, start, stop, step)\n\n    # setup output\n    out = np.array([statistic(values[i:j], **kwargs) for i, j in windows])\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef position_windows(pos, size, start, stop, step):\n    last = False\n\n    # determine start and stop positions\n    if start is None:\n        start = pos[0]\n    if stop is None:\n        stop = pos[-1]\n    if step is None:\n        # non-overlapping\n        step = size\n\n    windows = []\n    for window_start in range(start, stop, step):\n\n        # determine window stop\n        window_stop = window_start + size\n        if window_stop >= stop:\n            # last window\n            window_stop = stop\n            last = True\n        else:\n            window_stop -= 1\n\n        windows.append([window_start, window_stop])\n\n        if last:\n            break\n\n    return np.asarray(windows)", "response": "Convenience function to construct a list of windows for a given position."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlocates indices in pos corresponding to the start and stop positions of windows.", "response": "def window_locations(pos, windows):\n    \"\"\"Locate indices in `pos` corresponding to the start and stop positions\n    of `windows`.\n\n    \"\"\"\n    start_locs = np.searchsorted(pos, windows[:, 0])\n    stop_locs = np.searchsorted(pos, windows[:, 1], side='right')\n    locs = np.column_stack((start_locs, stop_locs))\n    return locs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef windowed_count(pos, size=None, start=None, stop=None, step=None,\n                   windows=None):\n    \"\"\"Count the number of items in windows over a single chromosome/contig.\n\n    Parameters\n    ----------\n\n    pos : array_like, int, shape (n_items,)\n        The item positions in ascending order, using 1-based coordinates..\n    size : int, optional\n        The window size (number of bases).\n    start : int, optional\n        The position at which to start (1-based).\n    stop : int, optional\n        The position at which to stop (1-based).\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    windows : array_like, int, shape (n_windows, 2), optional\n        Manually specify the windows to use as a sequence of (window_start,\n        window_stop) positions, using 1-based coordinates. Overrides the\n        size/start/stop/step parameters.\n\n    Returns\n    -------\n\n    counts : ndarray, int, shape (n_windows,)\n        The number of items in each window.\n    windows : ndarray, int, shape (n_windows, 2)\n        The windows used, as an array of (window_start, window_stop) positions,\n        using 1-based coordinates.\n\n    Notes\n    -----\n\n    The window stop positions are included within a window.\n\n    The final window will be truncated to the specified stop position,\n    and so may be smaller than the other windows.\n\n    Examples\n    --------\n\n    Non-overlapping windows::\n\n        >>> import allel\n        >>> pos = [1, 7, 12, 15, 28]\n        >>> counts, windows = allel.windowed_count(pos, size=10)\n        >>> counts\n        array([2, 2, 1])\n        >>> windows\n        array([[ 1, 10],\n               [11, 20],\n               [21, 28]])\n\n    Half-overlapping windows::\n\n        >>> counts, windows = allel.windowed_count(pos, size=10, step=5)\n        >>> counts\n        array([2, 3, 2, 0, 1])\n        >>> windows\n        array([[ 1, 10],\n               [ 6, 15],\n               [11, 20],\n               [16, 25],\n               [21, 28]])\n\n    \"\"\"\n\n    # assume sorted positions\n    if not isinstance(pos, SortedIndex):\n        pos = SortedIndex(pos, copy=False)\n\n    # setup windows\n    if windows is None:\n        windows = position_windows(pos, size, start, stop, step)\n    else:\n        windows = asarray_ndim(windows, 2)\n\n    # find window locations\n    locs = window_locations(pos, windows)\n\n    # count number of items in each window\n    counts = np.diff(locs, axis=1).reshape(-1)\n\n    return counts, windows", "response": "Count the number of items in windows over a single chromosome or contig."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates a statistic over a single - item set of items in windows over a single - item set of chromosomes.", "response": "def windowed_statistic(pos, values, statistic, size=None, start=None,\n                       stop=None, step=None, windows=None, fill=np.nan):\n    \"\"\"Calculate a statistic from items in windows over a single\n    chromosome/contig.\n\n    Parameters\n    ----------\n\n    pos : array_like, int, shape (n_items,)\n        The item positions in ascending order, using 1-based coordinates..\n    values : array_like, int, shape (n_items,)\n        The values to summarise. May also be a tuple of values arrays,\n        in which case each array will be sliced and passed through to the\n        statistic function as separate arguments.\n    statistic : function\n        The statistic to compute.\n    size : int, optional\n        The window size (number of bases).\n    start : int, optional\n        The position at which to start (1-based).\n    stop : int, optional\n        The position at which to stop (1-based).\n    step : int, optional\n        The distance between start positions of windows. If not given,\n        defaults to the window size, i.e., non-overlapping windows.\n    windows : array_like, int, shape (n_windows, 2), optional\n        Manually specify the windows to use as a sequence of (window_start,\n        window_stop) positions, using 1-based coordinates. Overrides the\n        size/start/stop/step parameters.\n    fill : object, optional\n        The value to use where a window is empty, i.e., contains no items.\n\n    Returns\n    -------\n\n    out : ndarray, shape (n_windows,)\n        The value of the statistic for each window.\n    windows : ndarray, int, shape (n_windows, 2)\n        The windows used, as an array of (window_start, window_stop) positions,\n        using 1-based coordinates.\n    counts : ndarray, int, shape (n_windows,)\n        The number of items in each window.\n\n    Notes\n    -----\n\n    The window stop positions are included within a window.\n\n    The final window will be truncated to the specified stop position,\n    and so may be smaller than the other windows.\n\n    Examples\n    --------\n\n    Count non-zero (i.e., True) items in non-overlapping windows::\n\n        >>> import allel\n        >>> pos = [1, 7, 12, 15, 28]\n        >>> values = [True, False, True, False, False]\n        >>> nnz, windows, counts = allel.windowed_statistic(\n        ...     pos, values, statistic=np.count_nonzero, size=10\n        ... )\n        >>> nnz\n        array([1, 1, 0])\n        >>> windows\n        array([[ 1, 10],\n               [11, 20],\n               [21, 28]])\n        >>> counts\n        array([2, 2, 1])\n\n    Compute a sum over items in half-overlapping windows::\n\n        >>> values = [3, 4, 2, 6, 9]\n        >>> x, windows, counts = allel.windowed_statistic(\n        ...     pos, values, statistic=np.sum, size=10, step=5, fill=0\n        ... )\n        >>> x\n        array([ 7, 12,  8,  0,  9])\n        >>> windows\n        array([[ 1, 10],\n               [ 6, 15],\n               [11, 20],\n               [16, 25],\n               [21, 28]])\n        >>> counts\n        array([2, 3, 2, 0, 1])\n\n    \"\"\"\n\n    # assume sorted positions\n    if not isinstance(pos, SortedIndex):\n        pos = SortedIndex(pos, copy=False)\n\n    # check lengths are equal\n    if isinstance(values, tuple):\n        # assume multiple values arrays\n        check_equal_length(pos, *values)\n    else:\n        # assume a single values array\n        check_equal_length(pos, values)\n\n    # setup windows\n    if windows is None:\n        windows = position_windows(pos, size, start, stop, step)\n    else:\n        windows = asarray_ndim(windows, 2)\n\n    # find window locations\n    locs = window_locations(pos, windows)\n\n    # setup outputs\n    out = []\n    counts = []\n\n    # iterate over windows\n    for start_idx, stop_idx in locs:\n\n        # calculate number of values in window\n        n = stop_idx - start_idx\n\n        if n == 0:\n            # window is empty\n            s = fill\n\n        else:\n\n            if isinstance(values, tuple):\n                # assume multiple values arrays\n                wv = [v[start_idx:stop_idx] for v in values]\n                s = statistic(*wv)\n\n            else:\n                # assume a single values array\n                wv = values[start_idx:stop_idx]\n                s = statistic(wv)\n\n        # store outputs\n        out.append(s)\n        counts.append(n)\n\n    # convert to arrays for output\n    return np.asarray(out), windows, np.asarray(counts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef per_base(x, windows, is_accessible=None, fill=np.nan):\n\n    # calculate window sizes\n    if is_accessible is None:\n        # N.B., window stops are included\n        n_bases = np.diff(windows, axis=1).reshape(-1) + 1\n    else:\n        n_bases = np.array([np.count_nonzero(is_accessible[i-1:j])\n                            for i, j in windows])\n\n    # deal with multidimensional x\n    if x.ndim == 1:\n        pass\n    elif x.ndim == 2:\n        n_bases = n_bases[:, None]\n    else:\n        raise NotImplementedError('only arrays of 1 or 2 dimensions supported')\n\n    # calculate density per-base\n    with ignore_invalid():\n        y = np.where(n_bases > 0, x / n_bases, fill)\n\n    # restore to 1-dimensional\n    if n_bases.ndim > 1:\n        n_bases = n_bases.reshape(-1)\n\n    return y, n_bases", "response": "Calculate the per - base value of a windowed statistic."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating windows that are equally accessible.", "response": "def equally_accessible_windows(is_accessible, size, start=0, stop=None, step=None):\n    \"\"\"Create windows each containing the same number of accessible bases.\n\n    Parameters\n    ----------\n    is_accessible : array_like, bool, shape (n_bases,)\n        Array defining accessible status of all bases on a contig/chromosome.\n    size : int\n        Window size (number of accessible bases).\n    start : int, optional\n        The genome position at which to start.\n    stop : int, optional\n        The genome position at which to stop.\n    step : int, optional\n        The number of accessible sites between start positions\n        of windows. If not given, defaults to the window size, i.e.,\n        non-overlapping windows. Use half the window size to get\n        half-overlapping windows.\n\n    Returns\n    -------\n    windows : ndarray, int, shape (n_windows, 2)\n        Window start/stop positions (1-based).\n\n    \"\"\"\n    pos_accessible, = np.nonzero(is_accessible)\n    pos_accessible += 1  # convert to 1-based coordinates\n\n    # N.B., need some care in handling start and stop positions, these are\n    # genomic positions at which to start and stop the windows\n    if start:\n        pos_accessible = pos_accessible[pos_accessible >= start]\n    if stop:\n        pos_accessible = pos_accessible[pos_accessible <= stop]\n\n    # now construct moving windows\n    windows = moving_statistic(pos_accessible, lambda v: [v[0], v[-1]],\n                               size=size, step=step)\n    return windows"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef attachment_form(context, obj):\n    if context['user'].has_perm('attachments.add_attachment'):\n        return {\n            'form': AttachmentForm(),\n            'form_url': add_url_for_obj(obj),\n            'next': context.request.build_absolute_uri(),\n        }\n    else:\n        return {'form': None}", "response": "Renders a attachment form."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attachment_delete_link(context, attachment):\n    if context['user'].has_perm('attachments.delete_foreign_attachments') or (\n        context['user'] == attachment.creator\n        and context['user'].has_perm('attachments.delete_attachment')\n    ):\n        return {\n            'next': context.request.build_absolute_uri(),\n            'delete_url': reverse(\n                'attachments:delete', kwargs={'attachment_pk': attachment.pk}\n            ),\n        }\n    return {'delete_url': None}", "response": "Renders a html link to the delete view of the given attachment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore the attachment in a per module / appname / primary key folder", "response": "def attachment_upload(instance, filename):\n    \"\"\"Stores the attachment in a \"per module/appname/primary key\" folder\"\"\"\n    return 'attachments/{app}_{model}/{pk}/{filename}'.format(\n        app=instance.content_object._meta.app_label,\n        model=instance.content_object._meta.object_name.lower(),\n        pk=instance.content_object.pk,\n        filename=filename,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __get_line_profile_data(self):\n        if self.line_profiler is None:\n            return {}\n\n        # the [0] is because pprofile.Profile.file_dict stores the line_dict\n        # in a list so that it can be modified in a thread-safe way\n        # see https://github.com/vpelletier/pprofile/blob/da3d60a1b59a061a0e2113bf768b7cb4bf002ccb/pprofile.py#L398\n        return self.line_profiler.file_dict[self.pyfile.path][0].line_dict", "response": "Method to procure line profiles.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __fetch_heatmap_data_from_profile(self):\n        # Read lines from file.\n        with open(self.pyfile.path, \"r\") as file_to_read:\n            for line in file_to_read:\n                # Remove return char from the end of the line and add a\n                # space in the beginning for better visibility.\n                self.pyfile.lines.append(\"  \" + line.strip(\"\\n\"))\n\n        # Total number of lines in file.\n        self.pyfile.length = len(self.pyfile.lines)\n\n        # Fetch line profiles.\n        line_profiles = self.__get_line_profile_data()\n\n        # Creating an array of data points. As the profile keys are 1 indexed\n        # we should range from 1 to line_count + 1 and not 0 to line_count.\n        arr = []\n        for line_num in range(1, self.pyfile.length + 1):\n            if line_num in line_profiles:\n                # line_profiles[i] will have multiple entries if line i is\n                # invoked from multiple places in the code. Here we sum over\n                # each invocation to get the total time spent on that line.\n                line_times = [\n                    ltime for _, ltime in line_profiles[line_num].values()\n                ]\n                arr.append([sum(line_times)])\n            else:\n                arr.append([0.0])\n\n        # Create nd-array from list of data points.\n        self.pyfile.data = np.array(arr)", "response": "Method to create heatmap data from profile information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __create_heatmap_plot(self):\n        # Define the heatmap plot.\n        height = len(self.pyfile.lines) / 3\n        width = max(map(lambda x: len(x), self.pyfile.lines)) / 8\n        self.fig, self.ax = plt.subplots(figsize=(width, height))\n\n        # Set second sub plot to occupy bottom 20%\n        plt.subplots_adjust(bottom=0.20)\n\n        # Heat scale orange to red\n        heatmap = self.ax.pcolor(self.pyfile.data, cmap=\"OrRd\")\n\n        # X Axis\n        # Remove X axis.\n        self.ax.xaxis.set_visible(False)\n\n        # Y Axis\n        # Create lables for y-axis ticks\n        row_labels = range(1, self.pyfile.length + 1)\n        # Set y-tick labels.\n        self.ax.set_yticklabels(row_labels, minor=False)\n        # Put y-axis major ticks at the middle of each cell.\n        self.ax.set_yticks(np.arange(self.pyfile.data.shape[0]) + 0.5, minor=False)\n        # Inver y-axis to have top down line numbers\n        self.ax.invert_yaxis()\n\n        # Plot definitions\n        # Set plot y-axis label.\n        plt.ylabel(\"Line Number\")\n        # Annotate each cell with lines in file in order.\n        max_time_spent_on_a_line = max(self.pyfile.data)\n        for i, line in enumerate(self.pyfile.lines):\n            # In order to ensure easy readability of the code, we need to\n            # invert colour of text display for darker colours which\n            # correspond to higher amount of time spent on the line.\n            if self.pyfile.data[i] >= 0.7 * max_time_spent_on_a_line:\n                color = (1.0, 1.0, 1.0)  # White text\n            else:\n                color = (0.0, 0.0, 0.0)  # Black text\n            plt.text(\n                0.0,\n                i + 0.5,\n                line,\n                ha=\"left\",\n                va=\"center\",\n                color=color,\n                clip_on=True,\n            )\n\n        # Define legend\n        cbar = plt.colorbar(heatmap)\n        cbar.set_label(\"# of seconds\")", "response": "Method to actually create the heatmap from profile stats."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    # Create command line parser.\n    parser = argparse.ArgumentParser()\n    # Adding command line arguments.\n    parser.add_argument(\"-o\", \"--out\", help=\"Output file\", default=None)\n    parser.add_argument(\n        \"pyfile\", help=\"Python file to be profiled\", default=None\n    )\n    # Parse command line arguments.\n    arguments = parser.parse_args()\n    if arguments.pyfile is not None:\n        # Core functionality.\n        pyheat = PyHeat(arguments.pyfile)\n        pyheat.create_heatmap()\n        pyheat.show_heatmap(output_file=arguments.out, enable_scroll=True)\n        pyheat.close_heatmap()\n    else:\n        # Print command help\n        parser.print_help()", "response": "Main entry point for the program execution."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef round(self, ndigits=0):\n        if ndigits is None:\n            ndigits = 0\n        return self.__class__(\n            amount=self.amount.quantize(Decimal('1e' + str(-ndigits))),\n            currency=self.currency)", "response": "Rounds the amount using the current Decimal rounding algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(path):\n    parsed = re.sre_parse.parse(path)\n    for case, _ in parsed:\n        if case not in (re.sre_parse.LITERAL, re.sre_parse.ANY):\n            break\n    else:\n        return path\n\n    path = path.strip('^$')\n\n    def parse_(match):\n        [part] = match.groups()\n        match = DYNR_RE.match(part)\n        params = match.groupdict()\n        return '(?P<%s>%s)' % (params['var'], params['re'] or '[^{}/]+')\n\n    return re.compile('^%s$' % DYNS_RE.sub(parse_, path))", "response": "Parse URL path and convert it to regexp if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds URL for a set of subgroups and groups.", "response": "def url_for(self, *subgroups, **groups):\n        \"\"\"Build URL.\"\"\"\n        parsed = re.sre_parse.parse(self._pattern.pattern)\n        subgroups = {n:str(v) for n, v in enumerate(subgroups, 1)}\n        groups_ = dict(parsed.pattern.groupdict)\n        subgroups.update({\n            groups_[k0]: str(v0)\n            for k0, v0 in groups.items()\n            if k0 in groups_\n        })\n        path = ''.join(str(val) for val in Traverser(parsed, subgroups))\n        return URL.build(path=path, encoded=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a not literal.", "response": "def state_not_literal(self, value):\n        \"\"\"Parse not literal.\"\"\"\n        value = negate = chr(value)\n        while value == negate:\n            value = choice(self.literals)\n        yield value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate HMAC Signature from secret for value.", "response": "def create_signature(secret, value, digestmod='sha256', encoding='utf-8'):\n    \"\"\" Create HMAC Signature from secret for value. \"\"\"\n    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n\n    if isinstance(value, str):\n        value = value.encode(encoding)\n\n    if isinstance(digestmod, str):\n        digestmod = getattr(hashlib, digestmod, hashlib.sha1)\n\n    hm = hmac.new(secret, digestmod=digestmod)\n    hm.update(value)\n    return hm.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_signature(signature, *args, **kwargs):\n    return hmac.compare_digest(signature, create_signature(*args, **kwargs))", "response": "Check if the signature is correct."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a password hash.", "response": "def generate_password_hash(password, digestmod='sha256', salt_length=8):\n    \"\"\" Hash a password with given method and salt length. \"\"\"\n\n    salt = ''.join(random.sample(SALT_CHARS, salt_length))\n    signature = create_signature(salt, password, digestmod=digestmod)\n    return '$'.join((digestmod, salt, signature))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimporting all submodules by package name.", "response": "def import_submodules(package_name, *submodules):\n    \"\"\"Import all submodules by package name.\"\"\"\n    package = sys.modules[package_name]\n    return {\n        name: importlib.import_module(package_name + '.' + name)\n        for _, name, _ in pkgutil.walk_packages(package.__path__)\n        if not submodules or name in submodules\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register(*paths, methods=None, name=None, handler=None):\n    def wrapper(method):\n        \"\"\"Store route params into method.\"\"\"\n        method = to_coroutine(method)\n        setattr(method, ROUTE_PARAMS_ATTR, (paths, methods, name))\n        if handler and not hasattr(handler, method.__name__):\n            setattr(handler, method.__name__, method)\n        return method\n    return wrapper", "response": "Register a method to the handler."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_view(cls, view, *methods, name=None):\n        docs = getattr(view, '__doc__', None)\n        view = to_coroutine(view)\n        methods = methods or ['GET']\n\n        if METH_ANY in methods:\n            methods = METH_ALL\n\n        def proxy(self, *args, **kwargs):\n            return view(*args, **kwargs)\n\n        params = {m.lower(): proxy for m in methods}\n        params['methods'] = methods\n        if docs:\n            params['__doc__'] = docs\n\n        return type(name or view.__name__, (cls,), params)", "response": "Create a handler class from a function or coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bind(cls, app, *paths, methods=None, name=None, router=None, view=None):\n        cls.app = app\n        if cls.app is not None:\n            for _, m in inspect.getmembers(cls, predicate=inspect.isfunction):\n                if not hasattr(m, ROUTE_PARAMS_ATTR):\n                    continue\n                paths_, methods_, name_ = getattr(m, ROUTE_PARAMS_ATTR)\n                name_ = name_ or (\"%s.%s\" % (cls.name, m.__name__))\n                delattr(m, ROUTE_PARAMS_ATTR)\n                cls.app.register(*paths_, methods=methods_, name=name_, handler=cls)(m)\n\n        @coroutine\n        @functools.wraps(cls)\n        def handler(request):\n            return cls().dispatch(request, view=view)\n\n        if not paths:\n            paths = [\"/%s\" % cls.__name__]\n\n        return routes_register(\n            app, handler, *paths, methods=methods, router=router, name=name or cls.name)", "response": "Bind to the given application."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering view to handler.", "response": "def register(cls, *args, **kwargs):\n        \"\"\"Register view to handler.\"\"\"\n        if cls.app is None:\n            return register(*args, handler=cls, **kwargs)\n        return cls.app.register(*args, handler=cls, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def make_response(self, request, response):\n        while iscoroutine(response):\n            response = await response\n\n        if isinstance(response, StreamResponse):\n            return response\n\n        if isinstance(response, str):\n            return Response(text=response, content_type='text/html')\n\n        if isinstance(response, bytes):\n            return Response(body=response, content_type='text/html')\n\n        return Response(text=json.dumps(response), content_type='application/json')", "response": "Convert a handler result to web response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a coroutine which parses data from request depends on content - type.", "response": "async def parse(self, request):\n        \"\"\"Return a coroutine which parses data from request depends on content-type.\n\n        Usage: ::\n\n            def post(self, request):\n                data = await self.parse(request)\n                # ...\n        \"\"\"\n        if request.content_type in {'application/x-www-form-urlencoded', 'multipart/form-data'}:\n            return await request.post()\n\n        if request.content_type == 'application/json':\n            return await request.json()\n\n        return await request.text()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the plugin. Fill the plugin's options from application.", "response": "def setup(self, app):\n        \"\"\"Initialize the plugin.\n\n        Fill the plugin's options from application.\n        \"\"\"\n        self.app = app\n        for name, ptype in self.dependencies.items():\n            if name not in app.ps or not isinstance(app.ps[name], ptype):\n                raise PluginException(\n                    'Plugin `%s` requires for plugin `%s` to be installed to the application.' % (\n                        self.name, ptype))\n\n        for oname, dvalue in self.defaults.items():\n            aname = ('%s_%s' % (self.name, oname)).upper()\n            self.cfg.setdefault(oname, app.cfg.get(aname, dvalue))\n            app.cfg.setdefault(aname, self.cfg[oname])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register(self, *paths, methods=None, name=None, handler=None):\n        if isinstance(methods, str):\n            methods = [methods]\n\n        def wrapper(view):\n            if handler is None:\n\n                handler_ = view\n                methods_ = methods or [METH_ANY]\n\n                if isfunction(handler_) or ismethod(handler_):\n                    handler_ = Handler.from_view(view, *methods_, name=name)\n\n                handler_.bind(self, *paths, methods=methods_, name=name)\n\n            else:\n\n                view_name = view.__name__\n                if not hasattr(handler, view_name):\n                    setattr(handler, view_name, to_coroutine(view))\n                name_ = name or view_name\n                handler.bind(self, *paths, methods=methods, name=name_, view=view_name)\n\n            return view\n\n        # Support for @app.register(func)\n        if len(paths) == 1 and callable(paths[0]):\n            view = paths[0]\n\n            if isclass(view) and issubclass(view, BaseException):\n                return wrapper\n\n            paths = []\n            return wrapper(view)\n\n        return wrapper", "response": "Register function with the application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cfg(self):\n        config = LStruct(self.defaults)\n        module = config['CONFIG'] = os.environ.get(\n            CONFIGURATION_ENVIRON_VARIABLE, config['CONFIG'])\n\n        if module:\n            try:\n                module = import_module(module)\n                config.update({\n                    name: getattr(module, name) for name in dir(module)\n                    if name == name.upper() and not name.startswith('_')\n                })\n\n            except ImportError as exc:\n                config.CONFIG = None\n                self.logger.error(\"Error importing %s: %s\", module, exc)\n\n        # Patch configuration from ENV\n        for name in config:\n            if name.startswith('_') or name != name.upper() or name not in os.environ:\n                continue\n            try:\n                config[name] = json.loads(os.environ[name])\n            except ValueError:\n                pass\n\n        return config", "response": "Load the application configuration. This method loads the application configuration from python module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninstalls a plugin to the application.", "response": "def install(self, plugin, name=None, **opts):\n        \"\"\"Install plugin to the application.\"\"\"\n        source = plugin\n\n        if isinstance(plugin, str):\n            module, _, attr = plugin.partition(':')\n            module = import_module(module)\n            plugin = getattr(module, attr or 'Plugin', None)\n\n        if isinstance(plugin, types.ModuleType):\n            plugin = getattr(module, 'Plugin', None)\n\n        if plugin is None:\n            raise MuffinException('Plugin is not found %r' % source)\n\n        name = name or plugin.name\n        if name in self.ps:\n            raise MuffinException('Plugin with name `%s` is already intalled.' % name)\n\n        if isinstance(plugin, type):\n            plugin = plugin(**opts)\n\n        if hasattr(plugin, 'setup'):\n            plugin.setup(self)\n\n        if hasattr(plugin, 'middleware') and plugin.middleware not in self.middlewares:\n            self.middlewares.append(plugin.middleware)\n\n        if hasattr(plugin, 'startup'):\n            self.on_startup.append(plugin.startup)\n\n        if hasattr(plugin, 'cleanup'):\n            self.on_cleanup.append(plugin.cleanup)\n\n        # Save plugin links\n        self.ps[name] = plugin\n\n        return plugin"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts the application. Support for start-callbacks and lock the application's configuration and plugins.", "response": "async def startup(self):\n        \"\"\"Start the application.\n\n        Support for start-callbacks and lock the application's configuration and plugins.\n        \"\"\"\n        if self.frozen:\n            return False\n\n        if self._error_handlers:\n            self.middlewares.append(_exc_middleware_factory(self))\n\n        # Register static paths\n        for path in self.cfg.STATIC_FOLDERS:\n            self.router.register_resource(SafeStaticResource(self.cfg.STATIC_PREFIX, path))\n\n        await super(Application, self).startup()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef middleware(self, func):\n        self.middlewares.append(web.middleware(to_coroutine(func)))", "response": "Register given middleware ( v1 )."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(\n    html,\n    transport_encoding=None,\n    namespace_elements=False,\n    treebuilder='lxml',\n    fallback_encoding=None,\n    keep_doctype=True,\n    maybe_xhtml=False,\n    return_root=True,\n    line_number_attr=None,\n    sanitize_names=True,\n    stack_size=16 * 1024\n):\n    '''\n    Parse the specified :attr:`html` and return the parsed representation.\n\n    :param html: The HTML to be parsed. Can be either bytes or a unicode string.\n\n    :param transport_encoding: If specified, assume the passed in bytes are in this encoding.\n        Ignored if :attr:`html` is unicode.\n\n    :param namespace_elements:\n        Add XML namespaces when parsing so that the resulting tree is XHTML.\n\n    :param treebuilder:\n        The type of tree to return. Note that only the lxml treebuilder is fast, as all\n        other treebuilders are implemented in python, not C. Supported values are:\n          * `lxml <http://lxml.de>`_  -- the default, and fastest\n          * etree (the python stdlib :mod:`xml.etree.ElementTree`)\n          * dom (the python stdlib :mod:`xml.dom.minidom`)\n          * `soup <https://www.crummy.com/software/BeautifulSoup>`_ -- BeautifulSoup,\n            which must be installed or it will raise an :class:`ImportError`\n\n    :param fallback_encoding: If no encoding could be detected, then use this encoding.\n        Defaults to an encoding based on system locale.\n\n    :param keep_doctype: Keep the <DOCTYPE> (if any).\n\n    :param maybe_xhtml: Useful when it is unknown if the HTML to be parsed is\n        actually XHTML. Changes the HTML 5 parsing algorithm to be more\n        suitable for XHTML. In particular handles self-closed CDATA elements.\n        So a ``<title/>`` or ``<style/>`` in the HTML will not completely break\n        parsing. Also preserves namespaced tags and attributes even for namespaces\n        not supported by HTML 5 (this works only with the ``lxml`` treebuilder).\n        Note that setting this also implicitly sets ``namespace_elements``.\n\n    :param return_root: If True, return the root node of the document, otherwise\n        return the tree object for the document.\n\n    :param line_number_attr: The optional name of an attribute used to store the line number\n        of every element. If set, this attribute will be added to each element with the\n        element's line number.\n\n    :param sanitize_names: Ensure tag and attributes contain only ASCII alphanumeric\n        charactes, underscores, hyphens and periods. This ensures that the resulting\n        tree is also valid XML. Any characters outside this set are replaced by\n        underscores. Note that this is not strictly HTML 5 spec compliant, so turn it\n        off if you need strict spec compliance.\n\n    :param stack_size: The initial size (number of items) in the stack. The\n        default is sufficient to avoid memory allocations for all but the\n        largest documents.\n\n    '''\n    data = as_utf8(html or b'', transport_encoding, fallback_encoding)\n    treebuilder = normalize_treebuilder(treebuilder)\n    if treebuilder == 'soup':\n        from .soup import parse\n        return parse(\n            data, return_root=return_root, keep_doctype=keep_doctype, stack_size=stack_size)\n    if treebuilder not in NAMESPACE_SUPPORTING_BUILDERS:\n        namespace_elements = False\n\n    capsule = html_parser.parse(\n        data,\n        namespace_elements=namespace_elements or maybe_xhtml,\n        keep_doctype=keep_doctype,\n        maybe_xhtml=maybe_xhtml,\n        line_number_attr=line_number_attr,\n        sanitize_names=sanitize_names,\n        stack_size=stack_size)\n\n    ans = etree.adopt_external_document(capsule)\n    if treebuilder == 'lxml':\n        return ans.getroot() if return_root else ans\n    m = importlib.import_module('html5_parser.' + treebuilder)\n    return m.adapt(ans, return_root=return_root)", "response": "Parses the given HTML and returns the resulting tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the value passed to this function is equal to the value passed to the honeypot_equals function.", "response": "def honeypot_equals(val):\n    \"\"\"\n        Default verifier used if HONEYPOT_VERIFIER is not specified.\n        Ensures val == HONEYPOT_VALUE or HONEYPOT_VALUE() if it's a callable.\n    \"\"\"\n    expected = getattr(settings, 'HONEYPOT_VALUE', '')\n    if callable(expected):\n        expected = expected()\n    return val == expected"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies that the value of the given field_name is a valid honeypot.", "response": "def verify_honeypot_value(request, field_name):\n    \"\"\"\n        Verify that request.POST[field_name] is a valid honeypot.\n\n        Ensures that the field exists and passes verification according to\n        HONEYPOT_VERIFIER.\n    \"\"\"\n    verifier = getattr(settings, 'HONEYPOT_VERIFIER', honeypot_equals)\n    if request.method == 'POST':\n        field = field_name or settings.HONEYPOT_FIELD_NAME\n        if field not in request.POST or not verifier(request.POST[field]):\n            resp = render_to_string('honeypot/honeypot_error.html',\n                                    {'fieldname': field})\n            return HttpResponseBadRequest(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks request.POST for valid honeypot field. Takes an optional field_name that defaults to HONEYPOT_FIELD_NAME if not specified.", "response": "def check_honeypot(func=None, field_name=None):\n    \"\"\"\n        Check request.POST for valid honeypot field.\n\n        Takes an optional field_name that defaults to HONEYPOT_FIELD_NAME if\n        not specified.\n    \"\"\"\n    # hack to reverse arguments if called with str param\n    if isinstance(func, six.string_types):\n        func, field_name = field_name, func\n\n    def decorated(func):\n        def inner(request, *args, **kwargs):\n            response = verify_honeypot_value(request, field_name)\n            if response:\n                return response\n            else:\n                return func(request, *args, **kwargs)\n        return wraps(func, assigned=available_attrs(func))(inner)\n\n    if func is None:\n        def decorator(func):\n            return decorated(func)\n        return decorator\n    return decorated(func)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmarks view as exempt from honeypot validation", "response": "def honeypot_exempt(view_func):\n    \"\"\"\n        Mark view as exempt from honeypot validation\n    \"\"\"\n    # borrowing liberally from django's csrf_exempt\n    def wrapped(*args, **kwargs):\n        return view_func(*args, **kwargs)\n    wrapped.honeypot_exempt = True\n    return wraps(view_func, assigned=available_attrs(view_func))(wrapped)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_honeypot_field(field_name=None):\n    if not field_name:\n        field_name = settings.HONEYPOT_FIELD_NAME\n    value = getattr(settings, 'HONEYPOT_VALUE', '')\n    if callable(value):\n        value = value()\n    return {'fieldname': field_name, 'value': value}", "response": "Renders a honeypot field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the command line arguments and return a new object with the values generated.", "response": "def parse_args():\n    \"\"\"Parse the command line arguments.\"\"\"\n    import argparse\n    import textwrap\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter,\n                                     description=textwrap.dedent('''\\\nCommand line utility to generate .svg badges.\n\nThis utility can be used to generate .svg badge images, using configurable\nthresholds for coloring.  Values can be passed as string, integer or floating\npoint.  The type will be detected automatically.\n\nRunning the utility with a --file option will result in the .svg image being\nwritten to file.  Without the --file option the .svg file content will be\nwritten to stdout, so can be redirected to a file.\n\nSome thresholds have been built in to save time.  To use these thresholds you\ncan simply specify the template name instead of threshold value/color pairs.\n\nexamples:\n\n    Here are some usage specific examples that may save time on defining\n    thresholds.\n\n    Pylint\n        anybadge.py --value=2.22 --file=pylint.svg pylint\n\n        anybadge.py --label=pylint --value=2.22 --file=pylint.svg \\\\\n          2=red 4=orange 8=yellow 10=green\n\n    Coverage\n        anybadge.py --value=65 --file=coverage.svg coverage\n\n        anybadge.py --label=coverage --value=65 --suffix='%%' --file=coverage.svg \\\\\n          50=red 60=orange 80=yellow 100=green\n\n    CI Pipeline\n        anybadge.py --label=pipeline --value=passing --file=pipeline.svg \\\\\n          passing=green failing=red\n\n'''))\n    parser.add_argument('-l', '--label', type=str, help='The badge label.')\n    parser.add_argument('-v', '--value', type=str, help='The badge value.', required=True)\n    parser.add_argument('-m', '--value-format', type=str, default=None,\n                        help='Formatting string for value (e.g. \"%%.2f\" for 2dp floats)')\n    parser.add_argument('-c', '--color', type=str, help='For fixed color badges use --color'\n                                                        'to specify the badge color.',\n                        default=DEFAULT_COLOR)\n    parser.add_argument('-p', '--prefix', type=str, help='Optional prefix for value.',\n                        default='')\n    parser.add_argument('-s', '--suffix', type=str, help='Optional suffix for value.',\n                        default='')\n    parser.add_argument('-d', '--padding', type=int, help='Number of characters to pad on '\n                                                          'either side of the badge text.',\n                        default=NUM_PADDING_CHARS)\n    parser.add_argument('-n', '--font', type=str,\n                        help='Font name.  Supported fonts: '\n                             ','.join(['\"%s\"' % x for x in FONT_WIDTHS.keys()]),\n                        default=DEFAULT_FONT)\n    parser.add_argument('-z', '--font-size', type=int, help='Font size.',\n                        default=DEFAULT_FONT_SIZE)\n    parser.add_argument('-t', '--template', type=str, help='Location of alternative '\n                                                           'template .svg file.',\n                        default=TEMPLATE_SVG)\n    parser.add_argument('-u', '--use-max', action='store_true',\n                        help='Use the maximum threshold color when the value exceeds the '\n                             'maximum threshold.')\n    parser.add_argument('-f', '--file', type=str, help='Output file location.')\n    parser.add_argument('-o', '--overwrite', action='store_true',\n                        help='Overwrite output file if it already exists.')\n    parser.add_argument('-r', '--text-color', type=str, help='Text color. Single value affects both label'\n                                                             'and value colors.  A comma separated pair '\n                                                             'affects label and value text respectively.',\n                        default=DEFAULT_TEXT_COLOR)\n    parser.add_argument('args', nargs=argparse.REMAINDER, help='Pairs of <upper>=<color>. '\n                        'For example 2=red 4=orange 6=yellow 8=good. '\n                        'Read this as \"Less than 2 = red, less than 4 = orange...\".')\n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    # Parse command line arguments\n    args = parse_args()\n\n    label = args.label\n    threshold_text = args.args\n    suffix = args.suffix\n\n    # Check whether thresholds were sent as one word, and is in the\n    # list of templates.  If so, swap in the template.\n    if len(args.args) == 1 and args.args[0] in BADGE_TEMPLATES:\n        template_name = args.args[0]\n        template_dict = BADGE_TEMPLATES[template_name]\n        threshold_text = template_dict['threshold'].split(' ')\n        if not args.label:\n            label = template_dict['label']\n        if not args.suffix and 'suffix' in template_dict:\n            suffix = template_dict['suffix']\n\n    if not label:\n        raise ValueError('Label has not been set.  Please use --label argument.')\n\n    # Create threshold list from args\n    threshold_list = [x.split('=') for x in threshold_text]\n    threshold_dict = {x[0]: x[1] for x in threshold_list}\n\n    # Create badge object\n    badge = Badge(label, args.value, value_prefix=args.prefix, value_suffix=suffix,\n                  default_color=args.color, num_padding_chars=args.padding, font_name=args.font,\n                  font_size=args.font_size, template=args.template,\n                  use_max_when_value_exceeds=args.use_max, thresholds=threshold_dict,\n                  value_format=args.value_format, text_color=args.text_color)\n\n    if args.file:\n        # Write badge SVG to file\n        badge.write_badge(args.file, overwrite=args.overwrite)\n    else:\n        print(badge.badge_svg_text)", "response": "Generate a new object based on command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nidentifying whether the value text is an int.", "response": "def value_is_int(self):\n        \"\"\"Identify whether the value text is an int.\"\"\"\n        try:\n            a = float(self.value)\n            b = int(a)\n        except ValueError:\n            return False\n        else:\n            return a == b"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef font_width(self):\n        return self.get_font_width(font_name=self.font_name, font_size=self.font_size)", "response": "Return the badge font width."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef color_split_position(self):\n        return self.get_text_width(' ') + self.label_width + \\\n            int(float(self.font_width) * float(self.num_padding_chars))", "response": "The SVG x position where the color split should occur."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef badge_svg_text(self):\n\n        # Identify whether template is a file or the actual template text\n        if len(self.template.split('\\n')) == 1:\n            with open(self.template, mode='r') as file_handle:\n                badge_text = file_handle.read()\n        else:\n            badge_text = self.template\n\n        return badge_text.replace('{{ badge width }}', str(self.badge_width)) \\\n            .replace('{{ font name }}', self.font_name) \\\n            .replace('{{ font size }}', str(self.font_size)) \\\n            .replace('{{ label }}', self.label) \\\n            .replace('{{ value }}', self.value_text) \\\n            .replace('{{ label anchor }}', str(self.label_anchor)) \\\n            .replace('{{ label anchor shadow }}', str(self.label_anchor_shadow)) \\\n            .replace('{{ value anchor }}', str(self.value_anchor)) \\\n            .replace('{{ value anchor shadow }}', str(self.value_anchor_shadow)) \\\n            .replace('{{ color }}', self.badge_color_code) \\\n            .replace('{{ label text color }}', self.label_text_color) \\\n            .replace('{{ value text color }}', self.value_text_color) \\\n            .replace('{{ color split x }}', str(self.color_split_position)) \\\n            .replace('{{ value width }}', str(self.badge_width - self.color_split_position))", "response": "The badge SVG text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the width of the text.", "response": "def get_text_width(self, text):\n        \"\"\"Return the width of text.\n\n        This implementation assumes a fixed font of:\n\n        font-family=\"DejaVu Sans,Verdana,Geneva,sans-serif\" font-size=\"11\"\n        >>> badge = Badge('x', 1, font_name='DejaVu Sans,Verdana,Geneva,sans-serif', font_size=11)\n        >>> badge.get_text_width('pylint')\n        42\n        \"\"\"\n        return len(text) * self.get_font_width(self.font_name, self.font_size)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the badge color based on the thresholds.", "response": "def badge_color(self):\n        \"\"\"Find the badge color based on the thresholds.\"\"\"\n        # If no thresholds were passed then return the default color\n        if not self.thresholds:\n            return self.default_color\n\n        if self.value_type == str:\n            if self.value in self.thresholds:\n                return self.thresholds[self.value]\n            else:\n                return self.default_color\n\n        # Convert the threshold dictionary into a sorted list of lists\n        threshold_list = [[self.value_type(i[0]), i[1]] for i in self.thresholds.items()]\n        threshold_list.sort(key=lambda x: x[0])\n\n        color = None\n\n        for threshold, color in threshold_list:\n            if float(self.value) < float(threshold):\n                return color\n\n        # If we drop out the top of the range then return the last max color\n        if color and self.use_max_when_value_exceeds:\n            return color\n        else:\n            return self.default_color"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite badge to file.", "response": "def write_badge(self, file_path, overwrite=False):\n        \"\"\"Write badge to file.\"\"\"\n\n        # Validate path (part 1)\n        if file_path.endswith('/'):\n            raise Exception('File location may not be a directory.')\n\n        # Get absolute filepath\n        path = os.path.abspath(file_path)\n        if not path.lower().endswith('.svg'):\n            path += '.svg'\n\n        # Validate path (part 2)\n        if not overwrite and os.path.exists(path):\n            raise Exception('File \"{}\" already exists.'.format(path))\n\n        with open(path, mode='w') as file_handle:\n            file_handle.write(self.badge_svg_text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the name of a given object", "response": "def get_object_name(obj):\n    \"\"\"\n    Return the name of a given object\n    \"\"\"\n    name_dispatch = {\n        ast.Name: \"id\",\n        ast.Attribute: \"attr\",\n        ast.Call: \"func\",\n        ast.FunctionDef: \"name\",\n        ast.ClassDef: \"name\",\n        ast.Subscript: \"value\",\n    }\n\n    # This is a new ast type in Python 3\n    if hasattr(ast, \"arg\"):\n        name_dispatch[ast.arg] = \"arg\"\n\n    while not isinstance(obj, str):\n        assert type(obj) in name_dispatch\n        obj = getattr(obj, name_dispatch[type(obj)])\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the attribute name identifier.", "response": "def get_attribute_name_id(attr):\n    \"\"\"\n    Return the attribute name identifier\n    \"\"\"\n    return attr.value.id if isinstance(attr.value, ast.Name) else None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_class_method_bound(method, arg_name=BOUND_METHOD_ARGUMENT_NAME):\n    if not method.args.args:\n        return False\n\n    first_arg = method.args.args[0]\n\n    first_arg_name = get_object_name(first_arg)\n\n    return first_arg_name == arg_name", "response": "Returns True if a class method is bound to the class\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_class_methods(cls):\n    return [\n        node\n        for node in cls.body\n        if isinstance(node, ast.FunctionDef)\n    ]", "response": "Returns a list of methods associated with a given class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the class variables associated with a given class", "response": "def get_class_variables(cls):\n    \"\"\"\n    Return class variables associated with a given class\n    \"\"\"\n    return [\n        target\n        for node in cls.body\n        if isinstance(node, ast.Assign)\n        for target in node.targets\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn instance variables used in an AST node", "response": "def get_instance_variables(node, bound_name_classifier=BOUND_METHOD_ARGUMENT_NAME):\n    \"\"\"\n    Return instance variables used in an AST node\n    \"\"\"\n    node_attributes = [\n        child\n        for child in ast.walk(node)\n        if isinstance(child, ast.Attribute) and\n        get_attribute_name_id(child) == bound_name_classifier\n    ]\n    node_function_call_names = [\n        get_object_name(child)\n        for child in ast.walk(node)\n        if isinstance(child, ast.Call)\n    ]\n    node_instance_variables = [\n        attribute\n        for attribute in node_attributes\n        if get_object_name(attribute) not in node_function_call_names\n    ]\n    return node_instance_variables"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all classes associated with a given module", "response": "def get_module_classes(node):\n    \"\"\"\n    Return classes associated with a given module\n    \"\"\"\n    return [\n        child\n        for child in ast.walk(node)\n        if isinstance(child, ast.ClassDef)\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recursively_get_files_from_directory(directory):\n    return [\n        os.path.join(root, filename)\n        for root, directories, filenames in os.walk(directory)\n        for filename in filenames\n    ]", "response": "Recursively returns all files under a directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return PriorityLevel(key)\n        if key not in PriorityLevel._member_map_:\n            extend_enum(PriorityLevel, key, default)\n        return PriorityLevel[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return TOS_THR(key)\n        if key not in TOS_THR._member_map_:\n            extend_enum(TOS_THR, key, default)\n        return TOS_THR[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _missing_(cls, value):\n        if not (isinstance(value, int) and 0 <= value <= 1):\n            raise ValueError('%r is not a valid %s' % (value, cls.__name__))\n        extend_enum(cls, 'Unassigned [%d]' % value, value)\n        return cls(value)\n        super()._missing_(value)", "response": "Lookup function used when value is not found."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn first index of value.", "response": "def index(self, value, start=None, stop=None):\n        \"\"\"Return first index of value.\"\"\"\n        return self.__alias__.index(value, start, stop)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return NAT_Traversal(key)\n        if key not in NAT_Traversal._member_map_:\n            extend_enum(NAT_Traversal, key, default)\n        return NAT_Traversal[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_ospf(self, length):\n        if length is None:\n            length = len(self)\n\n        _vers = self._read_unpack(1)\n        _type = self._read_unpack(1)\n        _tlen = self._read_unpack(2)\n        _rtid = self._read_id_numbers()\n        _area = self._read_id_numbers()\n        _csum = self._read_fileng(2)\n        _autp = self._read_unpack(2)\n\n        ospf = dict(\n            version=_vers,\n            type=TYPE.get(_type),\n            len=_tlen,\n            router_id=_rtid,\n            area_id=_area,\n            chksum=_csum,\n            autype=AUTH.get(_autp) or 'Reserved',\n        )\n\n        if _autp == 2:\n            ospf['auth'] = self._read_encrypt_auth()\n        else:\n            ospf['auth'] = self._read_fileng(8)\n\n        length = ospf['len'] - 24\n        ospf['packet'] = self._read_packet(header=24, payload=length)\n\n        return self._decode_next_layer(ospf, length)", "response": "Read a single OSPF entry from the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads router and area IDs.", "response": "def _read_id_numbers(self):\n        \"\"\"Read router and area IDs.\"\"\"\n        _byte = self._read_fileng(4)\n        _addr = '.'.join([str(_) for _ in _byte])\n        return _addr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_encrypt_auth(self):\n        _resv = self._read_fileng(2)\n        _keys = self._read_unpack(1)\n        _alen = self._read_unpack(1)\n        _seqn = self._read_unpack(4)\n\n        auth = dict(\n            key_id=_keys,\n            len=_alen,\n            seq=_seqn,\n        )\n\n        return auth", "response": "Read the Authentication field when Cryptographic Authentication is employed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef int_check(*args, func=None):\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, numbers.Integral):\n            name = type(var).__name__\n            raise ComplexError(\n                f'Function {func} expected integral number, {name} got instead.')", "response": "Check if arguments are integrals."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if arguments are real numbers.", "response": "def real_check(*args, func=None):\n    \"\"\"Check if arguments are real numbers.\"\"\"\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, numbers.Real):\n            name = type(var).__name__\n            raise ComplexError(\n                f'Function {func} expected real number, {name} got instead.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if arguments are complex numbers.", "response": "def complex_check(*args, func=None):\n    \"\"\"Check if arguments are complex numbers.\"\"\"\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, numbers.Complex):\n            name = type(var).__name__\n            raise ComplexError(\n                f'Function {func} expected complex number, {name} got instead.')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef number_check(*args, func=None):\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, numbers.Number):\n            name = type(var).__name__\n            raise DigitError(\n                f'Function {func} expected number, {name} got instead.')", "response": "Check if arguments are numbers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bytes_check(*args, func=None):\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, (bytes, collections.abc.ByteString)):\n            name = type(var).__name__\n            raise BytesError(\n                f'Function {func} expected bytes, {name} got instead.')", "response": "Check if arguments are bytes type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if arguments are bytearray type.", "response": "def bytearray_check(*args, func=None):\n    \"\"\"Check if arguments are bytearray type.\"\"\"\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, (bytearray, collections.abc.ByteString, collections.abc.MutableSequence)):\n            name = type(var).__name__\n            raise BytearrayError(\n                f'Function {func} expected bytearray, {name} got instead.')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef str_check(*args, func=None):\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, (str, collections.UserString, collections.abc.Sequence)):\n            name = type(var).__name__\n            raise StringError(\n                f'Function {func} expected str, {name} got instead.')", "response": "Check if arguments are str type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if arguments are bytes type.", "response": "def bool_check(*args, func=None):\n    \"\"\"Check if arguments are bytes type.\"\"\"\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, bool):\n            name = type(var).__name__\n            raise BoolError(\n                f'Function {func} expected bool, {name} got instead.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_check(*args, func=None):\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, (list, collections.UserList, collections.abc.MutableSequence)):\n            name = type(var).__name__\n            raise ListError(\n                f'Function {func} expected list, {name} got instead.')", "response": "Check if arguments are list type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dict_check(*args, func=None):\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, (dict, collections.UserDict, collections.abc.MutableMapping)):\n            name = type(var).__name__\n            raise DictError(\n                f'Function {func} expected dict, {name} got instead.')", "response": "Check if arguments are dict type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if arguments are tuple type.", "response": "def tuple_check(*args, func=None):\n    \"\"\"Check if arguments are tuple type.\"\"\"\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, (tuple, collections.abc.Sequence)):\n            name = type(var).__name__\n            raise TupleError(\n                f'Function {func} expected tuple, {name} got instead.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if arguments are file - like object.", "response": "def io_check(*args, func=None):\n    \"\"\"Check if arguments are file-like object.\"\"\"\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, io.IOBase):\n            name = type(var).__name__\n            raise IOObjError(\n                f'Function {func} expected file-like object, {name} got instead.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if arguments are Info instance.", "response": "def info_check(*args, func=None):\n    \"\"\"Check if arguments are Info instance.\"\"\"\n    from pcapkit.corekit.infoclass import Info\n\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, Info):\n            name = type(var).__name__\n            raise InfoError(\n                f'Function {func} expected Info instance, {name} got instead.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if arguments are IP addresses.", "response": "def ip_check(*args, func=None):\n    \"\"\"Check if arguments are IP addresses.\"\"\"\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, ipaddress._IPAddressBase):\n            name = type(var).__name__\n            raise IPError(\n                f'Function {func} expected IP address, {name} got instead.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if arguments are of protocol type.", "response": "def enum_check(*args, func=None):\n    \"\"\"Check if arguments are of protocol type.\"\"\"\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, (enum.EnumMeta, aenum.EnumMeta)):\n            name = type(var).__name__\n            raise EnumError(\n                f'Function {func} expected enumeration, {name} got instead.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if arguments are valid fragments.", "response": "def frag_check(*args, protocol, func=None):\n    \"\"\"Check if arguments are valid fragments.\"\"\"\n    func = func or inspect.stack()[2][3]\n    if 'IP' in protocol:\n        _ip_frag_check(*args, func=func)\n    elif 'TCP' in protocol:\n        _tcp_frag_check(*args, func=func)\n    else:\n        raise FragmentError(f'Unknown fragmented protocol {protocol}.')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ip_frag_check(*args, func=None):\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        dict_check(var, func=func)\n        bufid = var.get('bufid')\n        str_check(bufid[3], func=func)\n        bool_check(var.get('mf'), func=func)\n        ip_check(bufid[0], bufid[1], func=func)\n        bytearray_check(var.get('header'), var.get('payload'), func=func)\n        int_check(bufid[2], var.get('num'), var.get('fo'),\n                  var.get('ihl'), var.get('tl'), func=func)", "response": "Check if arguments are valid IP fragments."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pkt_check(*args, func=None):\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        dict_check(var, func=func)\n        dict_check(var.get('frame'), func=func)\n        enum_check(var.get('protocol'), func=func)\n        real_check(var.get('timestamp'), func=func)\n        ip_check(var.get('src'), var.get('dst'), func=func)\n        bool_check(var.get('syn'), var.get('fin'), func=func)\n        int_check(var.get('srcport'), var.get('dstport'), var.get('index'), func=func)", "response": "Check if arguments are valid packet."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the index of a datagram.", "response": "def index(self, pkt_num):\n        \"\"\"Return datagram index.\"\"\"\n        int_check(pkt_num)\n        for counter, datagram in enumerate(self.datagram):\n            if pkt_num in datagram.index:\n                return counter\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning automatically. Positional arguments: * packets -- list<dict>, list of packet dicts to be reassembled", "response": "def run(self, packets):\n        \"\"\"Run automatically.\n\n        Positional arguments:\n            * packets -- list<dict>, list of packet dicts to be reassembled\n\n        \"\"\"\n        for packet in packets:\n            frag_check(packet, protocol=self.protocol)\n            info = Info(packet)\n            self.reassembly(info)\n        self._newflg = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_mh(self, length, extension):\n        if length is None:\n            length = len(self)\n\n        _next = self._read_protos(1)\n        _hlen = self._read_unpack(1)\n        _type = self._read_unpack(1)\n        _temp = self._read_fileng(1)\n        _csum = self._read_fileng(2)\n        # _data = self._read_fileng((_hlen+1)*8)\n\n        mh = dict(\n            next=_next,\n            length=(_hlen + 1) * 8,\n            type=_MOBILITY_TYPE.get(_type, 'Unassigned'),\n            chksum=_csum,\n        )\n\n        length -= mh['length']\n        mh['packet'] = self._read_packet(header=mh['length'], payload=length)\n\n        if extension:\n            self._protos = None\n            return mh\n        return self._decode_next_layer(mh, _next, length)", "response": "Read Mobility Header.\n\n        Structure of MH header [RFC 6275]:\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            | Payload Proto |  Header Len   |   MH Type     |   Reserved    |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |           Checksum            |                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+                               |\n            |                                                               |\n            .                                                               .\n            .                       Message Data                            .\n            .                                                               .\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     mh.next                 Next Header\n              1           8     mh.length               Header Length\n              2          16     mh.type                 Mobility Header Type\n              3          24     -                       Reserved\n              4          32     mh.chksum               Checksum\n              6          48     mh.data                 Message Data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return DI(key)\n        if key not in DI._member_map_:\n            extend_enum(DI, key, default)\n        return DI[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return Certificate(key)\n        if key not in Certificate._member_map_:\n            extend_enum(Certificate, key, default)\n        return Certificate[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_udp(self, length):\n        if length is None:\n            length = len(self)\n\n        _srcp = self._read_unpack(2)\n        _dstp = self._read_unpack(2)\n        _tlen = self._read_unpack(2)\n        _csum = self._read_fileng(2)\n\n        udp = dict(\n            srcport=_srcp,\n            dstport=_dstp,\n            len=_tlen,\n            checksum=_csum,\n        )\n\n        length = udp['len'] - 8\n        udp['packet'] = self._read_packet(header=8, payload=length)\n\n        return self._decode_next_layer(udp, None, length)", "response": "Read User Datagram Protocol."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread Internetwork Packet Exchange.", "response": "def read_ipx(self, length):\n        \"\"\"Read Internetwork Packet Exchange.\n\n        Structure of IPX header [RFC 1132]:\n            Octets      Bits        Name                    Description\n              0           0     ipx.cksum               Checksum\n              2          16     ipx.len                 Packet Length (header includes)\n              4          32     ipx.count               Transport Control (hop count)\n              5          40     ipx.type                Packet Type\n              6          48     ipx.dst                 Destination Address\n              18        144     ipx.src                 Source Address\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        _csum = self._read_fileng(2)\n        _tlen = self._read_unpack(2)\n        _ctrl = self._read_unpack(1)\n        _type = self._read_unpack(1)\n        _dsta = self._read_ipx_address()\n        _srca = self._read_ipx_address()\n\n        ipx = dict(\n            chksum=_csum,\n            len=_tlen,\n            count=_ctrl,\n            type=TYPE.get(_type),\n            dst=_dsta,\n            src=_srca,\n        )\n\n        proto = ipx['type']\n        length = ipx['len'] - 30\n        ipx['packet'] = self._read_packet(header=30, payload=length)\n\n        return self._decode_next_layer(ipx, proto, length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the IPX address field.", "response": "def _read_ipx_address(self):\n        \"\"\"Read IPX address field.\n\n        Structure of IPX address:\n            Octets      Bits        Name                    Description\n              0           0     ipx.addr.network        Network Number\n              4          32     ipx.addr.node           Node Number\n              10         80     ipx.addr.socket         Socket Number\n\n        \"\"\"\n        # Address Number\n        _byte = self._read_fileng(4)\n        _ntwk = ':'.join(textwrap.wrap(_byte.hex(), 2))\n\n        # Node Number (MAC)\n        _byte = self._read_fileng(6)\n        _node = ':'.join(textwrap.wrap(_byte.hex(), 2))\n        _maca = '-'.join(textwrap.wrap(_byte.hex(), 2))\n\n        # Socket Number\n        _sock = self._read_fileng(2)\n\n        # Whole Address\n        _list = [_ntwk, _node, _sock.hex()]\n        _addr = ':'.join(_list)\n\n        addr = dict(\n            network=_ntwk,\n            node=_maca,\n            socket=SOCK.get(int(_sock.hex(), base=16)) or _sock,\n            addr=_addr,\n        )\n\n        return addr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return Group(key)\n        if key not in Group._member_map_:\n            extend_enum(Group, key, default)\n        return Group[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_arp(self, length):\n        if length is None:\n            length = len(self)\n\n        _hwty = self._read_unpack(2)\n        _ptty = self._read_unpack(2)\n        _hlen = self._read_unpack(1)\n        _plen = self._read_unpack(1)\n        _oper = self._read_unpack(2)\n        _shwa = self._read_addr_resolve(_hlen, _hwty)\n        _spta = self._read_proto_resolve(_plen, _ptty)\n        _thwa = self._read_addr_resolve(_hlen, _hwty)\n        _tpta = self._read_proto_resolve(_plen, _ptty)\n\n        if _oper in (5, 6, 7):\n            self._acnm = 'DRARP'\n            self._name = 'Dynamic Reverse Address Resolution Protocol'\n        elif _oper in (8, 9):\n            self._acnm = 'InARP'\n            self._name = 'Inverse Address Resolution Protocol'\n        elif _oper in (3, 4):\n            self._acnm = 'RARP'\n            self._name = 'Reverse Address Resolution Protocol'\n        else:\n            self._acnm = 'ARP'\n            self._name = 'Address Resolution Protocol'\n\n        _htype = HRD.get(_hwty)\n        if re.match(r'.*Ethernet.*', _htype, re.IGNORECASE):\n            _ptype = ETHERTYPE.get(_ptty)\n        else:\n            _ptype = f'Unknown [{_ptty}]'\n\n        arp = dict(\n            htype=_htype,\n            ptype=_ptype,\n            hlen=_hlen,\n            plen=_plen,\n            oper=OPER.get(_oper),\n            sha=_shwa,\n            spa=_spta,\n            tha=_thwa,\n            tpa=_tpta,\n            len=8 + _hlen * 2 + _plen * 2,\n        )\n\n        length -= arp['len']\n        arp['packet'] = self._read_packet(header=arp['len'], payload=length)\n\n        return self._decode_next_layer(arp, None, length)", "response": "Read ARP from the file and return a new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_addr_resolve(self, length, htype):\n        if htype == 1:  # Ethernet\n            _byte = self._read_fileng(6)\n            _addr = '-'.join(textwrap.wrap(_byte.hex(), 2))\n        else:\n            _addr = self._read_fileng(length)\n        return _addr", "response": "Read MAC address according to protocol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the next IP address according to protocol.", "response": "def _read_proto_resolve(self, length, ptype):\n        \"\"\"Resolve IP address according to protocol.\n\n        Positional arguments:\n            * length -- int, protocol address length\n            * ptype -- int, protocol type\n\n        Returns:\n            * str -- IP address\n\n        \"\"\"\n        # if ptype == '0800':     # IPv4\n        #     _byte = self._read_fileng(4)\n        #     _addr = '.'.join([str(_) for _ in _byte])\n        # elif ptype == '86dd':   # IPv6\n        #     adlt = []       # list of IPv6 hexadecimal address\n        #     ctr_ = collections.defaultdict(int)\n        #                     # counter for consecutive groups of zero value\n        #     ptr_ = 0        # start pointer of consecutive groups of zero value\n        #     last = False    # if last hextet/group is zero value\n        #     omit = False    # omitted flag, since IPv6 address can omit to `::` only once\n\n        #     for index in range(8):\n        #         hex_ = self._read_fileng(2).hex().lstrip('0')\n\n        #         if hex_:    # if hextet is not '', directly append\n        #             adlt.append(hex_)\n        #             last = False\n        #         else:       # if hextet is '', append '0'\n        #             adlt.append('0')\n        #             if last:    # if last hextet is '', ascend counter\n        #                 ctr_[ptr_] += 1\n        #             else:       # if last hextet is not '', record pointer\n        #                 ptr_ = index\n        #                 last = True\n        #                 ctr_[ptr_] = 1\n\n        #     ptr_ = max(ctr_, key=ctr_.get) if ctr_ else 0   # fetch start pointer with longest zero values\n        #     end_ = ptr_ + ctr_[ptr_]                        # calculate end pointer\n\n        #     if ctr_[ptr_] > 1:      # only omit if zero values are in a consecutive group\n        #         del adlt[ptr_:end_] # remove zero values\n\n        #         if ptr_ == 0 and end_ == 8:     # insert `::` if IPv6 unspecified address (::)\n        #             adlt.insert(ptr_, '::')\n        #         elif ptr_ == 0 or end_ == 8:    # insert `:` if zero values are from start or at end\n        #             adlt.insert(ptr_, ':')\n        #         else:                           # insert '' otherwise\n        #             adlt.insert(ptr_, '')\n\n        #     _addr = ':'.join(adlt)\n        # else:\n        #     _addr = self._read_fileng(length)\n        # return _addr\n        if ptype == '0800':     # IPv4\n            return ipaddress.ip_address(self._read_fileng(4))\n        elif ptype == '86dd':   # IPv6\n            return ipaddress.ip_address(self._read_fileng(16))\n        else:\n            return self._read_fileng(length)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return TOS_ECN(key)\n        if key not in TOS_ECN._member_map_:\n            extend_enum(TOS_ECN, key, default)\n        return TOS_ECN[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return Frame(key)\n        if key not in Frame._member_map_:\n            extend_enum(Frame, key, default)\n        return Frame[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ipv4_reassembly(frame):\n    if 'IPv4' in frame:\n        ipv4 = frame['IPv4'].info\n        if ipv4.flags.df:       # dismiss not fragmented frame\n            return False, None\n        data = dict(\n            bufid=(\n                ipv4.src,                                   # source IP address\n                ipv4.dst,                                   # destination IP address\n                ipv4.id,                                    # identification\n                ipv4.proto.name,                            # payload protocol type\n            ),\n            num=frame.info.number,                          # original packet range number\n            fo=ipv4.frag_offset,                            # fragment offset\n            ihl=ipv4.hdr_len,                               # internet header length\n            mf=ipv4.flags.mf,                               # more fragment flag\n            tl=ipv4.len,                                    # total length, header includes\n            header=bytearray(ipv4.packet.header),           # raw bytearray type header\n            payload=bytearray(ipv4.packet.payload or b''),  # raw bytearray type payload\n        )\n        return True, data\n    return False, None", "response": "Make data for IPv4 reassembly."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking data for IPv6 reassembly.", "response": "def ipv6_reassembly(frame):\n    \"\"\"Make data for IPv6 reassembly.\"\"\"\n    if 'IPv6' in frame:\n        ipv6 = frame['IPv6'].info\n        if 'frag' not in ipv6:      # dismiss not fragmented frame\n            return False, None\n        data = dict(\n            bufid=(\n                ipv6.src,                                       # source IP address\n                ipv6.dst,                                       # destination IP address\n                ipv6.label,                                     # label\n                ipv6.ipv6_frag.next.name,                       # next header field in IPv6 Fragment Header\n            ),\n            num=frame.info.number,                              # original packet range number\n            fo=ipv6.ipv6_frag.offset,                           # fragment offset\n            ihl=ipv6.hdr_len,                                   # header length, only headers before IPv6-Frag\n            mf=ipv6.ipv6_frag.mf,                               # more fragment flag\n            tl=ipv6.hdr_len + ipv6.raw_len,                     # total length, header includes\n            header=bytearray(ipv6.fragment.header),             # raw bytearray type header before IPv6-Frag\n            payload=bytearray(ipv6.fragment.payload or b''),    # raw bytearray type payload after IPv6-Frag\n        )\n        return True, data\n    return False, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tcp_reassembly(frame):\n    if 'TCP' in frame:\n        ip = (frame['IPv4'] if 'IPv4' in frame else frame['IPv6']).info\n        tcp = frame['TCP'].info\n        data = dict(\n            bufid=(\n                ip.src,                                     # source IP address\n                ip.dst,                                     # destination IP address\n                tcp.srcport,                                # source port\n                tcp.dstport,                                # destination port\n            ),\n            num=frame.info.number,                          # original packet range number\n            ack=tcp.ack,                                    # acknowledgement\n            dsn=tcp.seq,                                    # data sequence number\n            syn=tcp.flags.syn,                              # synchronise flag\n            fin=tcp.flags.fin,                              # finish flag\n            rst=tcp.flags.rst,                              # reset connection flag\n            payload=bytearray(tcp.packet.payload or b''),   # raw bytearray type payload\n        )\n        raw_len = len(data['payload'])                      # payload length, header excludes\n        data['first'] = tcp.seq                             # this sequence number\n        data['last'] = tcp.seq + raw_len                    # next (wanted) sequence number\n        data['len'] = raw_len                               # payload length, header excludes\n        return True, data\n    return False, None", "response": "Make data for TCP reassembly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntrace packet flow for TCP.", "response": "def tcp_traceflow(frame, *, data_link):\n    \"\"\"Trace packet flow for TCP.\"\"\"\n    if 'TCP' in frame:\n        ip = (frame['IPv4'] if 'IPv4' in frame else frame['IPv6']).info\n        tcp = frame['TCP'].info\n        data = dict(\n            protocol=data_link,                     # data link type from global header\n            index=frame.info.number,                # frame number\n            frame=frame.info,                       # extracted frame info\n            syn=tcp.flags.syn,                      # TCP synchronise (SYN) flag\n            fin=tcp.flags.fin,                      # TCP finish (FIN) flag\n            src=ip.src,                             # source IP\n            dst=ip.dst,                             # destination IP\n            srcport=tcp.srcport,                    # TCP source port\n            dstport=tcp.dstport,                    # TCP destination port\n            timestamp=frame.info.time_epoch,        # frame timestamp\n        )\n        return True, data\n    return False, None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reassembly(self, info):\n        BUFID = info.bufid  # Buffer Identifier\n        FO = info.fo        # Fragment Offset\n        IHL = info.ihl      # Internet Header Length\n        MF = info.mf        # More Fragments flag\n        TL = info.tl        # Total Length\n\n        # when non-fragmented (possibly discarded) packet received\n        if not FO and not MF:\n            if BUFID in self._buffer:\n                self._dtgram += self.submit(self._buffer[BUFID])\n                del self._buffer[BUFID]\n                return\n\n        # initialise buffer with BUFID\n        if BUFID not in self._buffer:\n            self._buffer[BUFID] = dict(\n                TDL=0,                          # Total Data Length\n                RCVBT=bytearray(8191),          # Fragment Received Bit Table\n                index=list(),                   # index record\n                header=bytearray(),             # header buffer\n                datagram=bytearray(65535),      # data buffer\n            )\n\n        # append packet index\n        self._buffer[BUFID]['index'].append(info.num)\n\n        # put data into data buffer\n        start = FO\n        stop = TL - IHL + FO\n        self._buffer[BUFID]['datagram'][start:stop] = info.payload\n\n        # set RCVBT bits (in 8 octets)\n        start = FO // 8\n        stop = FO // 8 + (TL - IHL + 7) // 8\n        self._buffer[BUFID]['RCVBT'][start:stop] = b'\\x01' * (stop - start + 1)\n\n        # get total data length (header excludes)\n        if not MF:\n            TDL = TL - IHL + FO\n\n        # put header into header buffer\n        if not FO:\n            self._buffer[BUFID]['header'] = info.header\n\n        # when datagram is reassembled in whole\n        start = 0\n        stop = (TDL + 7) // 8\n        if TDL and all(self._buffer[BUFID]['RCVBT'][start:stop]):\n            self._dtgram += self.submit(self._buffer[BUFID], checked=True)\n            del self._buffer[BUFID]", "response": "Reassembly procedure.\n\n        Positional arguments:\n            * info -- Info, info dict of packets to be reassembled"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef submit(self, buf, *, checked=False):\n        TDL = buf['TDL']\n        RCVBT = buf['RCVBT']\n        index = buf['index']\n        header = buf['header']\n        datagram = buf['datagram']\n\n        start = 0\n        stop = (TDL + 7) // 8\n        flag = checked or (TDL and all(RCVBT[start:stop]))\n        # if datagram is not implemented\n        if not flag and self._strflg:\n            data = list()\n            byte = bytearray()\n            # extract received payload\n            for (bctr, bit) in enumerate(RCVBT):\n                if bit:     # received bit\n                    this = bctr * 8\n                    that = this + 8\n                    byte += datagram[this:that]\n                else:       # missing bit\n                    if byte:    # strip empty payload\n                        data.append(bytes(byte))\n                    byte = bytearray()\n            # strip empty packets\n            if data or header:\n                packet = Info(\n                    NotImplemented=True,\n                    index=tuple(index),\n                    header=header or None,\n                    payload=tuple(data) or None,\n                )\n        # if datagram is reassembled in whole\n        else:\n            payload = datagram[:TDL]\n            packet = Info(\n                NotImplemented=False,\n                index=tuple(index),\n                packet=(bytes(header) + bytes(payload)) or None,\n            )\n        return [packet]", "response": "Submit reassembled payload.\n\n        Positional arguments:\n            * buf -- dict, buffer dict of reassembled packets\n\n        Keyword arguments:\n            * bufid -- tuple, buffer identifier\n\n        Returns:\n            * list -- reassembled packets"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a single IPv6 header from the file and returns the name and description of the header.", "response": "def read_ipv6(self, length):\n        \"\"\"Read Internet Protocol version 6 (IPv6).\n\n        Structure of IPv6 header [RFC 2460]:\n\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |Version| Traffic Class |           Flow Label                  |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |         Payload Length        |  Next Header  |   Hop Limit   |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            +                                                               +\n            |                                                               |\n            +                         Source Address                        +\n            |                                                               |\n            +                                                               +\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            +                                                               +\n            |                                                               |\n            +                      Destination Address                      +\n            |                                                               |\n            +                                                               +\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     ip.version              Version (6)\n              0           4     ip.class                Traffic Class\n              1          12     ip.label                Flow Label\n              4          32     ip.payload              Payload Length (header excludes)\n              6          48     ip.next                 Next Header\n              7          56     ip.limit                Hop Limit\n              8          64     ip.src                  Source Address\n              24        192     ip.dst                  Destination Address\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        _htet = self._read_ip_hextet()\n        _plen = self._read_unpack(2)\n        _next = self._read_protos(1)\n        _hlmt = self._read_unpack(1)\n        _srca = self._read_ip_addr()\n        _dsta = self._read_ip_addr()\n\n        ipv6 = dict(\n            version=_htet[0],\n            tclass=_htet[1],\n            label=_htet[2],\n            payload=_plen,\n            next=_next,\n            limit=_hlmt,\n            src=_srca,\n            dst=_dsta,\n        )\n\n        hdr_len = 40\n        raw_len = ipv6['payload']\n        ipv6['packet'] = self._read_packet(header=hdr_len, payload=raw_len)\n\n        return self._decode_next_layer(ipv6, _next, raw_len)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_ip_hextet(self):\n        _htet = self._read_fileng(4).hex()\n        _vers = _htet[0]                    # version number (6)\n        _tcls = int(_htet[0:2], base=16)    # traffic class\n        _flow = int(_htet[2:], base=16)     # flow label\n\n        return (_vers, _tcls, _flow)", "response": "Read first four hextets of IPv6."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _decode_next_layer(self, ipv6, proto=None, length=None):\n        hdr_len = 40                # header length\n        raw_len = ipv6['payload']   # payload length\n        _protos = list()            # ProtoChain buffer\n\n        # traverse if next header is an extensive header\n        while proto in EXT_HDR:\n            # keep original data after fragment header\n            if proto.value == 44:\n                ipv6['fragment'] = self._read_packet(header=hdr_len, payload=raw_len)\n\n            # # directly break when No Next Header occurs\n            # if proto.name == 'IPv6-NoNxt':\n            #     proto = None\n            #     break\n\n            # make protocol name\n            next_ = self._import_next_layer(proto, version=6, extension=True)\n            info = next_.info\n            name = next_.alias.lstrip('IPv6-').lower()\n            ipv6[name] = info\n\n            # record protocol name\n            # self._protos = ProtoChain(name, chain, alias)\n            _protos.append(next_)\n            proto = info.next  # pylint: disable=E1101\n\n            # update header & payload length\n            hdr_len += info.length  # pylint: disable=E1101\n            raw_len -= info.length  # pylint: disable=E1101\n\n        # record real header & payload length (headers exclude)\n        ipv6['hdr_len'] = hdr_len\n        ipv6['raw_len'] = raw_len\n\n        # update next header\n        ipv6['protocol'] = proto\n        return super()._decode_next_layer(ipv6, proto, raw_len, ipv6_exthdr=_protos)", "response": "Decode next layer into a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread next layer protocol type.", "response": "def _read_protos(self, size):\n        \"\"\"Read next layer protocol type.\n\n        Positional arguments:\n            * size  -- int, buffer size\n\n        Returns:\n            * str -- next layer's protocol name\n\n        \"\"\"\n        _byte = self._read_unpack(size)\n        _prot = TP_PROTO.get(_byte)\n        return _prot"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _decode_next_layer(self, dict_, proto=None, length=None, *, version=4, ipv6_exthdr=None):\n        if self._onerror:\n            next_ = beholder(self._import_next_layer)(self, proto, length, version=version)\n        else:\n            next_ = self._import_next_layer(proto, length, version=version)\n        info, chain = next_.info, next_.protochain\n\n        # make next layer protocol name\n        layer = next_.alias.lower()\n        # proto = next_.__class__.__name__\n\n        # write info and protocol chain into dict\n        dict_[layer] = info\n        self._next = next_\n        if ipv6_exthdr is not None:\n            for proto in reversed(ipv6_exthdr):\n                chain = ProtoChain(proto.__class__, proto.alias, basis=chain)\n        self._protos = ProtoChain(self.__class__, self.alias, basis=chain)\n        return dict_", "response": "Decode next layer into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimports next layer from pcapkit.", "response": "def _import_next_layer(self, proto, length=None, *, version=4, extension=False):\n        \"\"\"Import next layer extractor.\n\n        Positional arguments:\n            * proto -- str, next layer protocol name\n            * length -- int, valid (not padding) length\n\n        Keyword Arguments:\n            * version -- int, IP version (4 in default)\n                            <keyword> 4 / 6\n            * extension -- bool, if is extension header (False in default)\n                            <keyword> True / False\n\n        Returns:\n            * bool -- flag if extraction of next layer succeeded\n            * Info -- info of next layer\n            * ProtoChain -- protocol chain of next layer\n            * str -- alias of next layer\n\n        Protocols:\n            * IPv4 -- internet layer\n            * IPv6 -- internet layer\n            * AH -- internet layer\n            * TCP -- transport layer\n            * UDP -- transport layer\n\n        \"\"\"\n        if length == 0:\n            from pcapkit.protocols.null import NoPayload as Protocol\n        elif self._sigterm or proto == 59:\n            from pcapkit.protocols.raw import Raw as Protocol\n        elif proto == 51:\n            from pcapkit.protocols.internet.ah import AH as Protocol\n        elif proto == 139:\n            from pcapkit.protocols.internet.hip import HIP as Protocol\n        elif proto == 0:\n            from pcapkit.protocols.internet.hopopt import HOPOPT as Protocol\n        elif proto == 44:\n            from pcapkit.protocols.internet.ipv6_frag import IPv6_Frag as Protocol\n        elif proto == 60:\n            from pcapkit.protocols.internet.ipv6_opts import IPv6_Opts as Protocol\n        elif proto == 43:\n            from pcapkit.protocols.internet.ipv6_route import IPv6_Route as Protocol\n        elif proto == 135:\n            from pcapkit.protocols.internet.mh import MH as Protocol\n        elif proto == 4:\n            from pcapkit.protocols.internet.ipv4 import IPv4 as Protocol\n        elif proto == 41:\n            from pcapkit.protocols.internet.ipv6 import IPv6 as Protocol\n        elif proto == 6:\n            from pcapkit.protocols.transport.tcp import TCP as Protocol\n        elif proto == 17:\n            from pcapkit.protocols.transport.udp import UDP as Protocol\n        else:\n            from pcapkit.protocols.raw import Raw as Protocol\n        next_ = Protocol(self._file, length, version=version, extension=extension,\n                         error=self._onerror, layer=self._exlayer, protocol=self._exproto)\n        return next_"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_hip(self, length, extension):\n        if length is None:\n            length = len(self)\n\n        _next = self._read_protos(1)\n        _hlen = self._read_unpack(1)\n        _type = self._read_binary(1)\n        if _type[0] != '0':\n            raise ProtocolError('HIP: invalid format')\n        _vers = self._read_binary(1)\n        if _vers[7] != '1':\n            raise ProtocolError('HIP: invalid format')\n        _csum = self._read_fileng(2)\n        _ctrl = self._read_binary(2)\n        _shit = self._read_unpack(16)\n        _rhit = self._read_unpack(16)\n\n        hip = dict(\n            next=_next,\n            length=(_hlen + 1) * 8,\n            type=_HIP_TYPES.get(int(_type[1:], base=2), 'Unassigned'),\n            version=int(_vers[:4], base=2),\n            chksum=_csum,\n            control=dict(\n                anonymous=True if int(_ctrl[15], base=2) else False,\n            ),\n            shit=_shit,\n            rhit=_rhit,\n        )\n\n        _prml = _hlen - 38\n        if _prml:\n            parameters = self._read_hip_para(_prml, version=hip['version'])\n            hip['parameters'] = parameters[0]   # tuple of parameter acronyms\n            hip.update(parameters[1])           # merge parameters info to buffer\n\n        length -= hip['length']\n        hip['packet'] = self._read_packet(header=hip['length'], payload=length)\n\n        if extension:\n            self._protos = None\n            return hip\n        return self._decode_next_layer(hip, _next, length)", "response": "Read Host Identity Protocol.\n\n        Structure of HIP header [RFC 5201][RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            | Next Header   | Header Length |0| Packet Type |Version| RES.|1|\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |          Checksum             |           Controls            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                Sender's Host Identity Tag (HIT)               |\n            |                                                               |\n            |                                                               |\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |               Receiver's Host Identity Tag (HIT)              |\n            |                                                               |\n            |                                                               |\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            /                        HIP Parameters                         /\n            /                                                               /\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     hip.next                Next Header\n              1           8     hip.length              Header Length\n              2          16     -                       Reserved (0)\n              2          17     hip.type                Packet Type\n              3          24     hip.version             Version\n              3          28     -                       Reserved\n              3          31     -                       Reserved (1)\n              4          32     hip.chksum              Checksum\n              6          48     hip.control             Controls\n              8          64     hip.shit                Sender's Host Identity Tag\n              24        192     hip.rhit                Receiver's Host Identity Tag\n              40        320     hip.parameters          HIP Parameters"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread HIP parameters. Positional arguments: * length -- int, length of parameters Keyword arguments: * version -- int, HIP version Returns: * dict -- extracted HIP parameters", "response": "def _read_hip_para(self, length, *, version):\n        \"\"\"Read HIP parameters.\n\n        Positional arguments:\n            * length -- int, length of parameters\n\n        Keyword arguments:\n            * version -- int, HIP version\n\n        Returns:\n            * dict -- extracted HIP parameters\n\n        \"\"\"\n        counter = 0         # length of read parameters\n        optkind = list()    # parameter type list\n        options = dict()    # dict of parameter data\n\n        while counter < length:\n            # break when eol triggered\n            kind = self._read_binary(2)\n            if not kind:\n                break\n\n            # get parameter type & C-bit\n            code = int(kind, base=2)\n            cbit = True if int(kind[15], base=2) else False\n\n            # get parameter length\n            clen = self._read_unpack(2)\n            plen = 11 + clen - (clen + 3) % 8\n\n            # extract parameter\n            dscp = _HIP_PARA.get(code, 'Unassigned')\n            # if 0 <= code <= 1023 or 61440 <= code <= 65535:\n            #     desc = f'{dscp} (IETF Review)'\n            # elif 1024 <= code <= 32767 or 49152 <= code <= 61439:\n            #     desc = f'{dscp} (Specification Required)'\n            # elif 32768 <= code <= 49151:\n            #     desc = f'{dscp} (Reserved for Private Use)'\n            # else:\n            #     raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid parameter')\n            data = _HIP_PROC(dscp)(self, code, cbit, clen, desc=dscp, length=plen, version=version)\n\n            # record parameter data\n            counter += plen\n            if dscp in optkind:\n                if isinstance(options[dscp], tuple):\n                    options[dscp] += (Info(data),)\n                else:\n                    options[dscp] = (Info(options[dscp]), Info(data))\n            else:\n                optkind.append(dscp)\n                options[dscp] = data\n\n        # check threshold\n        if counter != length:\n            raise ProtocolError(f'HIPv{version}: invalid format')\n\n        return tuple(optkind), options"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_para_unassigned(self, code, cbit, clen, *, desc, length, version):\n        unassigned = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            contents=self._read_fileng(clen),\n        )\n\n        plen = length - clen\n        if plen:\n            self._read_fileng(plen)\n\n        return unassigned", "response": "Read the unassigned parameters from the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_para_esp_info(self, code, cbit, clen, *, desc, length, version):\n        if clen != 12:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _resv = self._read_fileng(2)\n        _kind = self._read_unpack(2)\n        _olds = self._read_unpack(2)\n        _news = self._read_unpack(2)\n\n        esp_info = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            index=_kind,\n            old_spi=_olds,\n            new_spi=_news,\n        )\n\n        return esp_info", "response": "Read HIP ESP_INFO parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_para_r1_counter(self, code, cbit, clen, *, desc, length, version):\n        if clen != 12:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n        if code == 128 and version != 1:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid parameter')\n\n        _resv = self._read_fileng(4)\n        _genc = self._read_unpack(8)\n\n        r1_counter = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            count=_genc,\n        )\n\n        return r1_counter", "response": "Read HIP R1_COUNTER parameter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_para_locator_set(self, code, cbit, clen, *, desc, length, version):\n        def _read_locator(kind, size):\n            if kind == 0 and size == 16:\n                return ipaddress.ip_address(self._read_fileng(16))\n            elif kind == 1 and size == 20:\n                return dict(\n                    spi=self._read_unpack(4),\n                    ip=ipaddress.ip_address(self._read_fileng(16)),\n                )\n            else:\n                raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _size = 0           # length of read locators\n        _locs = list()      # list of locators\n\n        while _size < clen:\n            _traf = self._read_unpack(1)\n            _loct = self._read_unpack(1)\n            _locl = self._read_unpack(1) * 4\n            _resp = self._read_binary(1)\n            _life = self._read_unpack(4)\n            _lobj = _read_locator(_loct, _locl)\n\n            _locs.append(Info(\n                traffic=_traf,\n                type=_loct,\n                length=_locl,\n                preferred=int(_resp[7], base=2),\n                lifetime=_life,\n                object=_lobj,\n            ))\n\n        locator_set = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            locator=tuple(_locs),\n        )\n\n        return locator_set", "response": "Read HIP LOCATOR_SET parameter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_para_puzzle(self, code, cbit, clen, *, desc, length, version):\n        if version == 1 and clen != 12:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _numk = self._read_unpack(1)\n        _time = self._read_unpack(1)\n        _opak = self._read_fileng(2)\n        _rand = self._read_unpack(clen-4)\n\n        puzzle = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            number=_numk,\n            lifetime=2 ** (_time - 32),\n            opaque=_opak,\n            random=_rand,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return puzzle", "response": "Read HIP PUZZLE parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads HIP SEQ parameter.", "response": "def _read_para_seq(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP SEQ parameter.\n\n        Structure of HIP SEQ parameter [RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                            Update ID                          |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     seq.type                Parameter Type\n              1          15     seq.critical            Critical Bit\n              2          16     seq.length              Length of Contents\n              4          32     seq.id                  Update ID\n\n        \"\"\"\n        if clen != 4:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _upid = self._read_unpack(4)\n\n        seq = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=_upid,\n        )\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_para_ack(self, code, cbit, clen, *, desc, length, version):\n        if clen % 4 != 0:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _upid = list()\n        for _ in range(clen // 4):\n            _upid.append(self._read_unpack(4))\n\n        ack = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=tuple(_upid),\n        )\n\n        return ack", "response": "Read HIP ACK parameter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_para_dh_group_list(self, code, cbit, clen, *, desc, length, version):\n        _dhid = list()\n        for _ in range(clen):\n            _dhid.append(_GROUP_ID.get(self._read_unpack(1), 'Unassigned'))\n\n        dh_group_list = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=tuple(_dhid),\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return dh_group_list", "response": "Read HIP DH_GROUP_LIST parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_para_diffie_hellman(self, code, cbit, clen, *, desc, length, version):\n        _gpid = self._read_unpack(1)\n        _vlen = self._read_unpack(2)\n        _pval = self._read_fileng(_vlen)\n\n        diffie_hellman = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=_GROUP_ID.get(_gpid, 'Unassigned'),\n            pub_len=_vlen,\n            pub_val=_pval,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return diffie_hellman", "response": "Read HIP DIFFIE_HELLMAN parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads HIP HIP_TRANSFORM parameter.", "response": "def _read_para_hip_transform(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP HIP_TRANSFORM parameter.\n\n        Structure of HIP HIP_TRANSFORM parameter [RFC 5201]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |            Suite ID #1        |          Suite ID #2          |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |            Suite ID #n        |             Padding           |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     hip_transform.type      Parameter Type\n              1          15     hip_transform.critical  Critical Bit\n              2          16     hip_transform.length    Length of Contents\n              4          32     hip_transform.id        Group ID\n                                ............\n              ?           ?     -                       Padding\n\n        \"\"\"\n        if version != 1:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid parameter')\n        if clen % 2 != 0:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _stid = list()\n        for _ in range(clen // 2):\n            _stid.append(_SUITE_ID.get(self._read_unpack(2), 'Unassigned'))\n\n        hip_transform = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=_stid,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return hip_transform"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread HIP HIP_CIPHER parameter.", "response": "def _read_para_hip_cipher(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP HIP_CIPHER parameter.\n\n        Structure of HIP HIP_CIPHER parameter [RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |          Cipher ID #1         |          Cipher ID #2         |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |          Cipher ID #n         |             Padding           |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     hip_cipher.type         Parameter Type\n              1          15     hip_cipher.critical     Critical Bit\n              2          16     hip_cipher.length       Length of Contents\n              4          32     hip_cipher.id           Cipher ID\n                                ............\n              ?           ?     -                       Padding\n\n        \"\"\"\n        if clen % 2 != 0:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _cpid = list()\n        for _ in range(clen // 2):\n            _cpid.append(_CIPHER_ID.get(self._read_unpack(2), 'Unassigned'))\n\n        hip_cipher = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=_cpid,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return hip_cipher"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_para_nat_traversal_mode(self, code, cbit, clen, *, desc, length, version):\n        if clen % 2 != 0:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _resv = self._read_fileng(2)\n        _mdid = list()\n        for _ in range((clen - 2) // 2):\n            _mdid.append(_MODE_ID.get(self._read_unpack(2), 'Unassigned'))\n\n        nat_traversal_mode = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=_mdid,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return nat_traversal_mode", "response": "Read HIP NAT_TRAVERSAL_MODE parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread HIP TRANSACTION_PACING parameter.", "response": "def _read_para_transaction_pacing(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP TRANSACTION_PACING parameter.\n\n        Structure of HIP TRANSACTION_PACING parameter [RFC 5770]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                            Min Ta                             |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                        Description\n              0           0     transaction_pacing.type     Parameter Type\n              1          15     transaction_pacing.critical Critical Bit\n              2          16     transaction_pacing.length   Length of Contents\n              4          32     transaction_pacing.min_ta   Min Ta\n\n        \"\"\"\n        if clen != 4:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _data = self._read_unpack(4)\n\n        transaction_pacing = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            min_ta=_data,\n        )\n\n        return transaction_pacing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the encrypted parameter.", "response": "def _read_para_encrypted(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP ENCRYPTED parameter.\n\n        Structure of HIP ENCRYPTED parameter [RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                           Reserved                            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                              IV                               /\n            /                                                               /\n            /                               +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+                               /\n            /                        Encrypted data                         /\n            /                                                               /\n            /                               +-------------------------------+\n            /                               |            Padding            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                Description\n              0           0     encrypted.type      Parameter Type\n              1          15     encrypted.critical  Critical Bit\n              2          16     encrypted.length    Length of Contents\n              4          32     -                   Reserved\n              8          48     encrypted.iv        Initialization Vector\n              ?           ?     encrypted.data      Encrypted data\n              ?           ?     -                   Padding\n\n\n        \"\"\"\n        _resv = self._read_fileng(4)\n        _data = self._read_fileng(clen-4)\n\n        encrypted = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            raw=_data,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return encrypted"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_para_host_id(self, code, cbit, clen, *, desc, length, version):\n        def _read_host_identifier(length, code):\n            algorithm = _HI_ALGORITHM.get(code, 'Unassigned')\n            if algorithm == 'ECDSA':\n                host_id = dict(\n                    curve=_ECDSA_CURVE.get(self._read_unpack(2)),\n                    pubkey=self._read_fileng(length-2),\n                )\n            elif algorithm == 'ECDSA_LOW':\n                host_id = dict(\n                    curve=_ECDSA_LOW_CURVE.get(self._read_unpack(2)),\n                    pubkey=self._read_fileng(length-2),\n                )\n            else:\n                host_id = self._read_fileng(length)\n            return algorithm, host_id\n\n        def _read_domain_identifier(di_data):\n            di_type = _DI_TYPE.get(int(di_data[:4], base=2), 'Unassigned')\n            di_len = int(di_data[4:], base=2)\n            domain_id = self._read_fileng(di_len)\n            return di_type, di_len, domain_id\n\n        _hlen = self._read_unpack(2)\n        _didt = self._read_binary(2)\n        _algo = self._read_unpack(2)\n        _hidf = _read_host_identifier(_hlen, _algo)\n        _didf = _read_domain_identifier(_didt)\n\n        host_id = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id_len=_hlen,\n            di_type=_didf[0],\n            di_len=_didf[1],\n            algorithm=_hidf[0],\n            host_id=_hidf[1],\n            domain_id=_didf[2],\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return host_id", "response": "Read HIP HOST_ID parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread HIP HIT_SUITE_LIST parameter.", "response": "def _read_para_hit_suite_list(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP HIT_SUITE_LIST parameter.\n\n        Structure of HIP HIT_SUITE_LIST parameter [RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |     ID #1     |     ID #2     |     ID #3     |     ID #4     |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |     ID #n     |                Padding                        |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     hit_suite_list.type     Parameter Type\n              1          15     hit_suite_list.critical Critical Bit\n              2          16     hit_suite_list.length   Length of Contents\n              4          32     hit_suite_list.id       HIT Suite ID\n                                ............\n              ?           ?     -                       Padding\n\n        \"\"\"\n        _hsid = list()\n        for _ in range(clen):\n            _hsid.append(_HIT_SUITE_ID.get(self._read_unpack(1), 'Unassigned'))\n\n        hit_suite_list = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=tuple(_hsid),\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return hit_suite_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_para_cert(self, code, cbit, clen, *, desc, length, version):\n        _ctgp = self._read_unpack(1)\n        _ctct = self._read_unpack(1)\n        _ctid = self._read_unpack(1)\n        _cttp = self._read_unpack(1)\n        _ctdt = self._read_fileng(clen-4)\n\n        cert = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            group=_GROUP_ID.get(_ctgp, 'Unassigned'),\n            count=_ctct,\n            id=_ctid,\n            cert_type=_CERT_TYPE.get(_cttp, 'Unassigned'),\n            certificate=_ctdt,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return cert", "response": "Read HIP CERT parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread HIP NOTIFICATION parameter.", "response": "def _read_para_notification(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP NOTIFICATION parameter.\n\n        Structure of HIP NOTIFICATION parameter [RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |          Reserved             |      Notify Message Type      |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               /\n            /                   Notification Data                           /\n            /                                               +---------------+\n            /                                               |     Padding   |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     notification.type       Parameter Type\n              1          15     notification.critical   Critical Bit\n              2          16     notification.length     Length of Contents\n              4          32     -                       Reserved\n              6          48     notification.msg_type   Notify Message Type\n              8          64     notification.data       Notification Data\n              ?           ?     -                       Padding\n\n        \"\"\"\n        _resv = self._read_fileng(2)\n        _code = self._read_unpack(2)\n        _data = self._read_fileng(2)\n\n        _type = _NOTIFICATION_TYPE.get(_code)\n        if _type is None:\n            if 1 <= _code <= 50:\n                _type = 'Unassigned (IETF Review)'\n            elif 51 <= _code <= 8191:\n                _type = 'Unassigned (Specification Required; Error Message)'\n            elif 8192 <= _code <= 16383:\n                _type = 'Unassigned (Reserved for Private Use; Error Message)'\n            elif 16384 <= _code <= 40959:\n                _type = 'Unassigned (Specification Required; Status Message)'\n            elif 40960 <= _code <= 65535:\n                _type = 'Unassigned (Reserved for Private Use; Status Message)'\n            else:\n                raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        notification = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            msg_type=_type,\n            data=_data,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return notification"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading HIP ECHO_REQUEST_SIGNED parameter.", "response": "def _read_para_echo_request_signed(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP ECHO_REQUEST_SIGNED parameter.\n\n        Structure of HIP ECHO_REQUEST_SIGNED parameter [RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                 Opaque data (variable length)                 |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     echo_request_signed.type        Parameter Type\n              1          15     echo_request_signed.critical    Critical Bit\n              2          16     echo_request_signed.length      Length of Contents\n              4          32     echo_request_signed.data        Opaque Data\n\n        \"\"\"\n        _data = self._read_fileng(clen)\n\n        echo_request_signed = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            data=_data,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return echo_request_signed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_para_reg_failed(self, code, cbit, clen, *, desc, length, version):\n        _life = collections.namedtuple('Lifetime', ('min', 'max'))\n        _mint = self._read_unpack(1)\n        _maxt = self._read_unpack(1)\n        _type = list()\n        for _ in range(clen-2):\n            _code = self._read_unpack(1)\n            _kind = _REG_FAILURE_TYPE.get(_code)\n            if _kind is None:\n                if 0 <= _code <= 200:\n                    _kind = 'Unassigned (IETF Review)'\n                elif 201 <= _code <= 255:\n                    _kind = 'Unassigned (Reserved for Private Use)'\n                else:\n                    raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n            _type.append(_kind)\n\n        reg_failed = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            lifetime=_life(_mint, _maxt),\n            reg_type=tuple(_type),\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return reg_failed", "response": "Read the HIP REG_FAILED parameter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading HIP REG_FROM parameter.", "response": "def _read_para_reg_from(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP REG_FROM parameter.\n\n        Structure of HIP REG_FROM parameter [RFC 5770]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Port              |    Protocol   |     Reserved  |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            |                            Address                            |\n            |                                                               |\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     reg_from.type               Parameter Type\n              1          15     reg_from.critical           Critical Bit\n              2          16     reg_from.length             Length of Contents\n              4          32     reg_from.port               Port\n              6          48     reg_from.protocol           Protocol\n              7          56     -                           Reserved\n              8          64     reg_from.ip                 Address (IPv6)\n\n        \"\"\"\n        if clen != 20:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _port = self._read_unpack(2)\n        _ptcl = self._read_unpack(1)\n        _resv = self._read_fileng(1)\n        _addr = self._read_fileng(16)\n\n        reg_from = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            port=_port,\n            protocol=TP_PROTO.get(_ptcl),\n            ip=ipaddress.ip_address(_addr),\n        )\n\n        return reg_from"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_para_echo_response_signed(self, code, cbit, clen, *, desc, length, version):\n        _data = self._read_fileng(clen)\n\n        echo_response_signed = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            data=_data,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return echo_response_signed", "response": "Read HIP ECHO_RESPONSE_SIGNED parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread HIP TRANSPORT_FORMAT_LIST parameter.", "response": "def _read_para_transport_format_list(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP TRANSPORT_FORMAT_LIST parameter.\n\n        Structure of HIP TRANSPORT_FORMAT_LIST parameter [RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |          TF type #1           |           TF type #2          /\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            /          TF type #n           |             Padding           |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     transport_format_list.type      Parameter Type\n              1          15     transport_format_list.critical  Critical Bit\n              2          16     transport_format_list.length    Length of Contents\n              4          32     transport_format_list.tf_type   TF Type\n                                ............\n              ?           ?     -                               Padding\n\n        \"\"\"\n        if clen % 2 != 0:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _tfid = list()\n        for _ in range(clen // 2):\n            _tfid.append(self._read_unpack(2))\n\n        transport_format_list = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            tf_type=tuple(_tfid),\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return transport_format_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread HIP ESP_TRANSFORM parameter.", "response": "def _read_para_esp_transform(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP ESP_TRANSFORM parameter.\n\n        Structure of HIP ESP_TRANSFORM parameter [RFC 7402]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |          Reserved             |           Suite ID #1         |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |          Suite ID #2          |           Suite ID #3         |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |          Suite ID #n          |             Padding           |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     esp_transform.type              Parameter Type\n              1          15     esp_transform.critical          Critical Bit\n              2          16     esp_transform.length            Length of Contents\n              4          32     -                               Reserved\n              6          48     esp_transform.id                Suite ID\n                                ............\n              ?           ?     -                               Padding\n\n        \"\"\"\n        if clen % 2 != 0:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _resv = self._read_fileng(2)\n        _stid = list()\n        for _ in range((clen - 2) // 2):\n            _stid.append(_ESP_SUITE_ID.get(self._read_unpack(2), 'Unassigned'))\n\n        esp_transform = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=tuple(_stid),\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return esp_transform"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads HIP SEQ_DATA parameter.", "response": "def _read_para_seq_data(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP SEQ_DATA parameter.\n\n        Structure of HIP SEQ_DATA parameter [RFC 6078]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                        Sequence number                        |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     seq_data.type                   Parameter Type\n              1          15     seq_data.critical               Critical Bit\n              2          16     seq_data.length                 Length of Contents\n              4          32     seq_data.seq                    Sequence number\n\n        \"\"\"\n        if clen != 4:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _seqn = self._read_unpack(4)\n\n        seq_data = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            seq=_seqn,\n        )\n\n        return seq_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_para_ack_data(self, code, cbit, clen, *, desc, length, version):\n        if clen % 4 != 0:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _ackn = list()\n        for _ in range(clen // 4):\n            _ackn.append(self._read_unpack(4))\n\n        ack_data = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            ack=tuple(_ackn),\n        )\n\n        return ack_data", "response": "Read HIP ACK_DATA parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_para_payload_mic(self, code, cbit, clen, *, desc, length, version):\n        _next = self._read_unpack(1)\n        _resv = self._read_fileng(3)\n        _data = self._read_fileng(4)\n        _micv = self._read_fileng(clen-8)\n\n        payload_mic = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            next=TP_PROTO.get(_next),\n            data=_data,\n            value=_micv,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return payload_mic", "response": "Read HIP ACK_DATA parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_para_transaction_id(self, code, cbit, clen, *, desc, length, version):\n        _tsid = self._read_unpack(clen)\n\n        transaction_id = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=_tsid,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return transaction_id", "response": "Read HIP TRANSACTION_ID parameter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread HIP OVERLAY_ID parameter.", "response": "def _read_para_overlay_id(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP TRANSACTION_ID parameter.\n\n        Structure of HIP TRANSACTION_ID parameter [RFC 6079]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                           Identifier                          /\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            /                                               |    Padding    |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     overlay_id.type                 Parameter Type\n              1          15     overlay_id.critical             Critical Bit\n              2          16     overlay_id.length               Length of Contents\n              4          32     overlay_id.id                   Identifier\n\n        \"\"\"\n        _olid = self._read_unpack(clen)\n\n        overlay_id = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            id=_olid,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return overlay_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading HIP ROUTE_DST parameter.", "response": "def _read_para_route_dst(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP ROUTE_DST parameter.\n\n        Structure of HIP ROUTE_DST parameter [RFC 6028]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Flags             |            Reserved           |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            |                            HIT #1                             |\n            |                                                               |\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            .                               .                               .\n            .                               .                               .\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            |                            HIT #n                             |\n            |                                                               |\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     route_dst.type                  Parameter Type\n              1          15     route_dst.critical              Critical Bit\n              2          16     route_dst.length                Length of Contents\n              4          32     route_dst.flags                 Flags\n              4          32     route_dst.flags.symmetric       SYMMETRIC [RFC 6028]\n              4          33     route_dst.flags.must_follow     MUST_FOLLOW [RFC 6028]\n              6          48     -                               Reserved\n              8          64     route_dst.ip                    HIT\n                                ............\n\n        \"\"\"\n        if (clen - 4) % 16 != 0:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _flag = self._read_binary(2)\n        _resv = self._read_fileng(2)\n        _addr = list()\n        for _ in range((clen - 4) // 16):\n            _addr.append(ipaddress.ip_address(self._read_fileng(16)))\n\n        route_dst = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            flags=dict(\n                symmetric=True if int(_flag[0], base=2) else False,\n                must_follow=True if int(_flag[1], base=2) else False,\n            ),\n            ip=tuple(_addr),\n        )\n\n        return route_dst"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_para_hip_transport_mode(self, code, cbit, clen, *, desc, length, version):\n        if clen % 2 != 0:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _port = self._read_unpack(2)\n        _mdid = list()\n        for _ in range((clen - 2) // 2):\n            _mdid.append(_TP_MODE_ID.get(self._read_unpack(2), 'Unassigned'))\n\n        hip_transport_mode = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            port=_port,\n            id=tuple(_mdid),\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return hip_transport_mode", "response": "Read HIP HIP_TRANSPORT_MODE parameter."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_para_hip_mac(self, code, cbit, clen, *, desc, length, version):\n        _hmac = self._read_fileng(clen)\n\n        hip_mac = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            hmac=_hmac,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return hip_mac", "response": "Read HIP HIP_MAC parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads HIP HIP_MAC_2 parameter.", "response": "def _read_para_hip_mac_2(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP HIP_MAC_2 parameter.\n\n        Structure of HIP HIP_MAC_2 parameter [RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            |                             HMAC                              |\n            /                                                               /\n            /                               +-------------------------------+\n            |                               |            Padding            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     hip_mac_2.type                  Parameter Type\n              1          15     hip_mac_2.critical              Critical Bit\n              2          16     hip_mac_2.length                Length of Contents\n              4          32     hip_mac_2.hmac                  HMAC\n              ?           ?     -                               Padding\n\n        \"\"\"\n        _hmac = self._read_fileng(clen)\n\n        hip_mac_2 = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            hmac=_hmac,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return hip_mac_2"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading HIP HIP_SIGNATURE_2 parameter.", "response": "def _read_para_hip_signature_2(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP HIP_SIGNATURE_2 parameter.\n\n        Structure of HIP HIP_SIGNATURE_2 parameter [RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |    SIG alg                    |            Signature          /\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            /                               |             Padding           |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     hip_signature_2.type            Parameter Type\n              1          15     hip_signature_2.critical        Critical Bit\n              2          16     hip_signature_2.length          Length of Contents\n              4          32     hip_signature_2.algorithm       SIG Algorithm\n              6          48     hip_signature_2.signature       Signature\n              ?           ?     -                               Padding\n\n        \"\"\"\n        _algo = self._read_unpack(2)\n        _sign = self._read_fileng(clen-2)\n\n        hip_signature_2 = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            algorithm=_HI_ALGORITHM.get(_algo, 'Unassigned'),\n            signature=_sign,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return hip_signature_2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_para_echo_request_unsigned(self, code, cbit, clen, *, desc, length, version):\n        _data = self._read_fileng(clen)\n\n        echo_request_unsigned = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            data=_data,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return echo_request_unsigned", "response": "Read HIP ECHO_REQUEST_UNSIGNED parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread HIP ECHO_RESPONSE_UNSIGNED parameter.", "response": "def _read_para_echo_response_unsigned(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP ECHO_RESPONSE_UNSIGNED parameter.\n\n        Structure of HIP ECHO_RESPONSE_UNSIGNED parameter [RFC 7401]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                 Opaque data (variable length)                 |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     echo_response_unsigned.type     Parameter Type\n              1          15     echo_response_unsigned.critical Critical Bit\n              2          16     echo_response_unsigned.length   Length of Contents\n              4          32     echo_response_unsigned.data     Opaque Data\n\n        \"\"\"\n        _data = self._read_fileng(clen)\n\n        echo_response_unsigned = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            data=_data,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return echo_response_unsigned"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_para_overlay_ttl(self, code, cbit, clen, *, desc, length, version):\n        if clen != 4:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _ttln = self._read_unpack(2)\n\n        overlay_ttl = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            ttl=_ttln,\n        )\n\n        return overlay_ttl", "response": "Read HIP OVERLAY_TTL parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading HIP FROM parameter.", "response": "def _read_para_from(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP FROM parameter.\n\n        Structure of HIP FROM parameter [RFC 8004]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Type              |             Length            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            |                             Address                           |\n            |                                                               |\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     from.type                       Parameter Type\n              1          15     from.critical                   Critical Bit\n              2          16     from.length                     Length of Contents\n              4          32     from.ip                         Address\n\n        \"\"\"\n        if clen != 16:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _addr = self._read_fileng(16)\n\n        from_ = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            ip=ipaddress.ip_address(_addr),\n        )\n\n        return from_"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread HIP RVS_HMAC parameter.", "response": "def _read_para_rvs_hmac(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP RVS_HMAC parameter.\n\n        Structure of HIP RVS_HMAC parameter [RFC 8004]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            |                             HMAC                              |\n            /                                                               /\n            /                               +-------------------------------+\n            |                               |            Padding            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     rvs_hmac.type                   Parameter Type\n              1          15     rvs_hmac.critical               Critical Bit\n              2          16     rvs_hmac.length                 Length of Contents\n              4          32     rvs_hmac.hmac                   HMAC\n              ?           ?     -                               Padding\n\n        \"\"\"\n        _hmac = self._read_fileng(clen)\n\n        rvs_hmac = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            hmac=_hmac,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return rvs_hmac"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_para_via_rvs(self, code, cbit, clen, *, desc, length, version):\n        if clen % 16 != 0:\n            raise ProtocolError(f'HIPv{version}: [Parano {code}] invalid format')\n\n        _addr = list()\n        for _ in range(clen // 16):\n            _addr.append(ipaddress.ip_address(self._read_fileng(16)))\n\n        via_rvs = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            ip=tuple(_addr),\n        )\n\n        return via_rvs", "response": "Read HIP VIA_RVS parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads HIP RELAY_HMAC parameter.", "response": "def _read_para_relay_hmac(self, code, cbit, clen, *, desc, length, version):\n        \"\"\"Read HIP RELAY_HMAC parameter.\n\n        Structure of HIP RELAY_HMAC parameter [RFC 5770]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            |                             HMAC                              |\n            /                                                               /\n            /                               +-------------------------------+\n            |                               |            Padding            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     relay_hmac.type                 Parameter Type\n              1          15     relay_hmac.critical             Critical Bit\n              2          16     relay_hmac.length               Length of Contents\n              4          32     relay_hmac.hmac                 HMAC\n              ?           ?     -                               Padding\n\n        \"\"\"\n        _hmac = self._read_fileng(clen)\n\n        relay_hmac = dict(\n            type=desc,\n            critical=cbit,\n            length=clen,\n            hmac=_hmac,\n        )\n\n        _plen = length - clen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return relay_hmac"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stacklevel():\n    pcapkit = f'{os.path.sep}pcapkit{os.path.sep}'\n    tb = traceback.extract_stack()\n    for index, tbitem in enumerate(tb):\n        if pcapkit in tbitem[0]:\n            break\n    else:\n        index = len(tb)\n    return (index-1)", "response": "Fetch current stack level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return LinkType(key)\n        if key not in LinkType._member_map_:\n            extend_enum(LinkType, key, default)\n        return LinkType[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread file from start then set back to original.", "response": "def seekset(func):\n    \"\"\"[ClassMethod] Read file from start then set back to original.\"\"\"\n    @functools.wraps(func)\n    def seekcur(self, *args, **kw):\n        seek_cur = self._file.tell()\n        self._file.seek(self._seekset, os.SEEK_SET)\n        return_ = func(self, *args, **kw)\n        self._file.seek(seek_cur, os.SEEK_SET)\n        return return_\n    return seekcur"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads file from start then set back to original.", "response": "def seekset_ng(func):\n    \"\"\"Read file from start then set back to original.\"\"\"\n    @functools.wraps(func)\n    def seekcur(file, *args, seekset=os.SEEK_SET, **kw):\n        # seek_cur = file.tell()\n        file.seek(seekset, os.SEEK_SET)\n        return_ = func(file, *args, seekset=seekset, **kw)\n        # file.seek(seek_cur, os.SEEK_SET)\n        return return_\n    return seekcur"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef beholder(func):\n    @functools.wraps(func)\n    def behold(self, proto, length, *args, **kwargs):\n        seek_cur = self._file.tell()\n        try:\n            return func(proto, length, *args, **kwargs)\n        except Exception:\n            from pcapkit.protocols.raw import Raw\n            error = traceback.format_exc(limit=1).strip().split(os.linesep)[-1]\n            # error = traceback.format_exc()\n\n            self._file.seek(seek_cur, os.SEEK_SET)\n            next_ = Raw(io.BytesIO(self._read_fileng(length)), length, error=error)\n            return next_\n    return behold", "response": "A class method that returns a function that returns a sequence of objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a single L2TP message from the specified language.", "response": "def read_l2tp(self, length):\n        \"\"\"Read Layer Two Tunnelling Protocol.\n\n        Structure of L2TP header [RFC 2661]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |T|L|x|x|S|x|O|P|x|x|x|x|  Ver  |          Length (opt)         |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |           Tunnel ID           |           Session ID          |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Ns (opt)          |             Nr (opt)          |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |      Offset Size (opt)        |    Offset pad... (opt)\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     l2tp.flags              Flags and Version Info\n              0           0     l2tp.flags.type         Type (0/1)\n              0           1     l2tp.flags.len          Length\n              0           2     -                       Reserved (must be zero)\n              0           4     l2tp.flags.seq          Sequence\n              0           5     -                       Reserved (must be zero)\n              0           6     l2tp.flags.offset       Offset\n              0           7     l2tp.flags.prio         Priority\n              1           8     -                       Reserved (must be zero)\n              1          12     l2tp.ver                Version (2)\n              2          16     l2tp.length             Length (optional by len)\n              4          32     l2tp.tunnelid           Tunnel ID\n              6          48     l2tp.sessionid          Session ID\n              8          64     l2tp.ns                 Sequence Number (optional by seq)\n              10         80     l2tp.nr                 Next Sequence Number (optional by seq)\n              12         96     l2tp.offset             Offset Size (optional by offset)\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        _flag = self._read_binary(1)\n        _vers = self._read_fileng(1).hex()[1]\n        _hlen = self._read_unpack(2) if int(_flag[1]) else None\n        _tnnl = self._read_unpack(2)\n        _sssn = self._read_unpack(2)\n        _nseq = self._read_unpack(2) if int(_flag[4]) else None\n        _nrec = self._read_unpack(2) if int(_flag[4]) else None\n        _size = self._read_unpack(2) if int(_flag[6]) else 0\n\n        l2tp = dict(\n            flags=dict(\n                type='Control' if int(_flag[0]) else 'Data',\n                len=True if int(_flag[1]) else False,\n                seq=True if int(_flag[4]) else False,\n                offset=True if int(_flag[6]) else False,\n                prio=True if int(_flag[7]) else False,\n            ),\n            ver=int(_vers, base=16),\n            length=_hlen,\n            tunnelid=_tnnl,\n            sessionid=_sssn,\n            ns=_nseq,\n            nr=_nrec,\n            offset=8*_size or None,\n        )\n\n        hdr_len = _hlen or (6 + 2*(int(_flag[1]) + 2*int(_flag[4]) + int(_flag[6])))\n        l2tp['hdr_len'] = hdr_len + _size * 8\n        # if _size:\n        #     l2tp['padding'] = self._read_fileng(_size * 8)\n\n        length -= l2tp['hdr_len']\n        l2tp['packet'] = self._read_packet(header=l2tp['hdr_len'], payload=length)\n\n        return self._decode_next_layer(l2tp, length)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return Hardware(key)\n        if key not in Hardware._member_map_:\n            extend_enum(Hardware, key, default)\n        return Hardware[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_vlan(self, length):\n        if length is None:\n            length = len(self)\n\n        _tcif = self._read_binary(2)\n        _type = self._read_protos(2)\n\n        vlan = dict(\n            tci=dict(\n                pcp=_PCP.get(int(_tcif[:3], base=2)),\n                dei=True if _tcif[3] else False,\n                vid=int(_tcif[4:], base=2),\n            ),\n            type=_type,\n        )\n\n        length -= 4\n        vlan['packet'] = self._read_packet(header=4, payload=length)\n\n        return self._decode_next_layer(vlan, _type, length)", "response": "Read a VLAN from the packet stream."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a single RFC 793 record from the beginning of the sequence.", "response": "def read_tcp(self, length):\n        \"\"\"Read Transmission Control Protocol (TCP).\n\n        Structure of TCP header [RFC 793]:\n\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |          Source Port          |       Destination Port        |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                        Sequence Number                        |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                   Acknowledgement Number                      |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |  Data |           |U|A|P|R|S|F|                               |\n            | Offset| Reserved  |R|C|S|S|Y|I|            Window             |\n            |       |           |G|K|H|T|N|N|                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |           Checksum            |         Urgent Pointer        |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                    Options                    |    Padding    |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                             data                              |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n\n            Octets      Bits        Name                    Description\n              0           0     tcp.srcport             Source Port\n              2          16     tcp.dstport             Destination Port\n              4          32     tcp.seq                 Sequence Number\n              8          64     tcp.ack                 Acknowledgement Number (if ACK set)\n              12         96     tcp.hdr_len             Data Offset\n              12        100     -                       Reserved (must be zero)\n              12        103     tcp.flags.ns            ECN Concealment Protection (NS)\n              13        104     tcp.flags.cwr           Congestion Window Reduced (CWR)\n              13        105     tcp.flags.ece           ECN-Echo (ECE)\n              13        106     tcp.flags.urg           Urgent (URG)\n              13        107     tcp.flags.ack           Acknowledgement (ACK)\n              13        108     tcp.flags.psh           Push Function (PSH)\n              13        109     tcp.flags.rst           Reset Connection (RST)\n              13        110     tcp.flags.syn           Synchronize Sequence Numbers (SYN)\n              13        111     tcp.flags.fin           Last Packet from Sender (FIN)\n              14        112     tcp.window_size         Size of Receive Window\n              16        128     tcp.checksum            Checksum\n              18        144     tcp.urgent_pointer      Urgent Pointer (if URG set)\n              20        160     tcp.opt                 TCP Options (if data offset > 5)\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        _srcp = self._read_unpack(2)\n        _dstp = self._read_unpack(2)\n        _seqn = self._read_unpack(4)\n        _ackn = self._read_unpack(4)\n        _lenf = self._read_binary(1)\n        _flag = self._read_binary(1)\n        _wins = self._read_unpack(2)\n        _csum = self._read_fileng(2)\n        _urgp = self._read_unpack(2)\n\n        tcp = dict(\n            srcport=_srcp,\n            dstport=_dstp,\n            seq=_seqn,\n            ack=_ackn,\n            hdr_len=int(_lenf[:4], base=2) * 4,\n            flags=dict(\n                ns=True if int(_lenf[7]) else False,\n                cwr=True if int(_flag[0]) else False,\n                ece=True if int(_flag[1]) else False,\n                urg=True if int(_flag[2]) else False,\n                ack=True if int(_flag[3]) else False,\n                psh=True if int(_flag[4]) else False,\n                rst=True if int(_flag[5]) else False,\n                syn=True if int(_flag[6]) else False,\n                fin=True if int(_flag[7]) else False,\n            ),\n            window_size=_wins,\n            checksum=_csum,\n            urgent_pointer=_urgp,\n        )\n\n        # packet type flags\n        self._syn = True if int(_flag[6]) else False\n        self._ack = True if int(_flag[3]) else False\n\n        _hlen = tcp['hdr_len']\n        _optl = _hlen - 20\n        if _optl:\n            options = self._read_tcp_options(_optl)\n            tcp['opt'] = options[0]     # tuple of option acronyms\n            tcp.update(options[1])      # merge option info to buffer\n\n        length -= _hlen\n        tcp['packet'] = self._read_packet(header=_hlen, payload=length)\n\n        return self._decode_next_layer(tcp, None, length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_tcp_options(self, size):\n        counter = 0         # length of read option list\n        optkind = list()    # option kind list\n        options = dict()    # dict of option data\n\n        while counter < size:\n            # get option kind\n            kind = self._read_unpack(1)\n\n            # fetch corresponding option tuple\n            opts = TCP_OPT.get(kind)\n            enum = OPT_TYPE.get(kind)\n            if opts is None:\n                len_ = size - counter\n                counter = size\n                optkind.append(enum)\n                options[enum.name] = self._read_fileng(len_)\n                break\n\n            # extract option\n            dscp = opts[1]\n            if opts[0]:\n                len_ = self._read_unpack(1)\n                byte = opts[2](len_)\n                if byte:    # check option process mode\n                    data = process_opt[opts[3]](self, byte, kind)\n                else:       # permission options (length is 2)\n                    data = dict(\n                        kind=kind,      # option kind\n                        length=2,       # option length\n                        flag=True,      # permission flag\n                    )\n            else:           # 1-bytes options\n                len_ = 1\n                data = dict(\n                    kind=kind,      # option kind\n                    length=1,       # option length\n                )\n\n            # record option data\n            counter += len_\n            if enum in optkind:\n                if isinstance(options[dscp], tuple):\n                    options[dscp] += (Info(data),)\n                else:\n                    options[dscp] = (Info(options[dscp]), Info(data))\n            else:\n                optkind.append(enum)\n                options[dscp] = data\n\n            # break when eol triggered\n            if not kind:\n                break\n\n        # get padding\n        if counter < size:\n            len_ = size - counter\n            options['padding'] = self._read_fileng(len_)\n\n        return tuple(optkind), options", "response": "Read TCP option list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_mode_donone(self, size, kind):\n        data = dict(\n            kind=kind,\n            length=size,\n            data=self._read_fileng(size),\n        )\n        return data", "response": "Read options request no process."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread options request unpack process.", "response": "def _read_mode_unpack(self, size, kind):\n        \"\"\"Read options request unpack process.\n\n        Keyword arguments:\n            size - int, length of option\n            kind - int, option kind value\n\n        Returns:\n            * dict -- extracted option which unpacked\n\n        Structure of TCP options:\n            Octets      Bits        Name                    Description\n              0           0     tcp.opt.kind            Kind\n              1           8     tcp.opt.length          Length\n              2          16     tcp.opt.data            Kind-specific Data\n\n        \"\"\"\n        data = dict(\n            kind=kind,\n            length=size,\n            data=self._read_unpack(size),\n        )\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading Timestamps option. Positional arguments: * size - int, length of option * kind - int, 8 (Timestamps) Returns: * dict -- extracted Timestamps (TS) option Structure of TCP TSopt [RFC 7323]: +-------+-------+---------------------+---------------------+ |Kind=8 | 10 | TS Value (TSval) |TS Echo Reply (TSecr)| +-------+-------+---------------------+---------------------+ 1 1 4 4 Octets Bits Name Description 0 0 tcp.ts.kind Kind (8) 1 8 tcp.ts.length Length (10) 2 16 tcp.ts.val Timestamp Value 6 48 tcp.ts.ecr Timestamps Echo Reply", "response": "def _read_mode_tsopt(self, size, kind):\n        \"\"\"Read Timestamps option.\n\n        Positional arguments:\n            * size - int, length of option\n            * kind - int, 8 (Timestamps)\n\n        Returns:\n            * dict -- extracted Timestamps (TS) option\n\n        Structure of TCP TSopt [RFC 7323]:\n            +-------+-------+---------------------+---------------------+\n            |Kind=8 |  10   |   TS Value (TSval)  |TS Echo Reply (TSecr)|\n            +-------+-------+---------------------+---------------------+\n                1       1              4                     4\n\n            Octets      Bits        Name                    Description\n              0           0     tcp.ts.kind             Kind (8)\n              1           8     tcp.ts.length           Length (10)\n              2          16     tcp.ts.val              Timestamp Value\n              6          48     tcp.ts.ecr              Timestamps Echo Reply\n\n        \"\"\"\n        temp = struct.unpack('>II', self._read_fileng(size))\n        data = dict(\n            kind=kind,\n            length=size,\n            val=temp[0],\n            ecr=temp[1],\n        )\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_mode_pocsp(self, size, kind):\n        temp = self._read_binary(size)\n\n        data = dict(\n            kind=kind,\n            length=size,\n            start=True if int(temp[0]) else False,\n            end=True if int(temp[1]) else False,\n            filler=bytes(chr(int(temp[2:], base=2)), encoding='utf-8'),\n        )\n\n        return data", "response": "Read Partial Order Connection Service Profile option."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading Alternate Checksum Request option.", "response": "def _read_mode_acopt(self, size, kind):\n        \"\"\"Read Alternate Checksum Request option.\n\n        Positional arguments:\n            size - int, length of option\n            kind - int, 14 (Alt-Chksum Request)\n\n        Returns:\n            * dict -- extracted Alternate Checksum Request (CHKSUM-REQ) option\n\n        Structure of TCP CHKSUM-REQ [RFC 1146][RFC 6247]:\n            +----------+----------+----------+\n            |  Kind=14 | Length=3 |  chksum  |\n            +----------+----------+----------+\n\n            Octets      Bits        Name                    Description\n              0           0     tcp.chksumreq.kind      Kind (14)\n              1           8     tcp.chksumreq.length    Length (3)\n              2          16     tcp.chksumreq.ac        Checksum Algorithm\n\n        \"\"\"\n        temp = self._read_unpack(size)\n        algo = chksum_opt.get(temp)\n\n        data = dict(\n            kind=kind,\n            length=size,\n            ac=algo,\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_mode_qsopt(self, size, kind):\n        rvrr = self._read_binary(1)\n        ttld = self._read_unpack(1)\n        noun = self._read_fileng(4)\n\n        data = dict(\n            kind=kind,\n            length=size,\n            req_rate=int(rvrr[4:], base=2),\n            ttl_diff=ttld,\n            nounce=noun[:-2],\n        )\n\n        return data", "response": "Read Quick - Start Response option."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_mode_utopt(self, size, kind):\n        temp = self._read_fileng(size)\n\n        data = dict(\n            kind=kind,\n            length=size,\n            granularity='minutes' if int(temp[0]) else 'seconds',\n            timeout=bytes(chr(int(temp[0:], base=2)), encoding='utf-8'),\n        )\n\n        return data", "response": "Read TCP User Timeout option."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads TCP Authentication option.", "response": "def _read_mode_tcpao(self, size, kind):\n        \"\"\"Read Authentication option.\n\n        Positional arguments:\n            * size - int, length of option\n            * kind - int, 29 (TCP Authentication Option)\n\n        Returns:\n            * dict -- extracted Authentication (AO) option\n\n        Structure of TCP AOopt [RFC 5925]:\n            +------------+------------+------------+------------+\n            |  Kind=29   |   Length   |   KeyID    | RNextKeyID |\n            +------------+------------+------------+------------+\n            |                     MAC           ...\n            +-----------------------------------...\n\n            ...-----------------+\n            ...  MAC (con't)    |\n            ...-----------------+\n\n            Octets      Bits        Name                    Description\n              0           0     tcp.ao.kind             Kind (29)\n              1           8     tcp.ao.length           Length\n              2          16     tcp.ao.keyid            KeyID\n              3          24     tcp.ao.rnextkeyid       RNextKeyID\n              4          32     tcp.ao.mac              Message Authentication Code\n\n        \"\"\"\n        key_ = self._read_unpack(1)\n        rkey = self._read_unpack(1)\n        mac_ = self._read_fileng(size - 2)\n\n        data = dict(\n            kind=kind,\n            length=size,\n            keyid=key_,\n            rnextkeyid=rkey,\n            mac=mac_,\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading Multipath TCP option.", "response": "def _read_mode_mptcp(self, size, kind):\n        \"\"\"Read Multipath TCP option.\n\n        Positional arguments:\n            * size - int, length of option\n            * kind - int, 30 (Multipath TCP)\n\n        Returns:\n            * dict -- extracted Multipath TCP (MP-TCP) option\n\n        Structure of MP-TCP [RFC 6824]:\n                                 1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +---------------+---------------+-------+-----------------------+\n            |     Kind      |    Length     |Subtype|                       |\n            +---------------+---------------+-------+                       |\n            |                     Subtype-specific data                     |\n            |                       (variable length)                       |\n            +---------------------------------------------------------------+\n\n            Octets      Bits        Name                    Description\n              0           0     tcp.mp.kind             Kind (30)\n              1           8     tcp.mp.length           Length\n              2          16     tcp.mp.subtype          Subtype\n              2          20     tcp.mp.data             Subtype-specific Data\n\n        \"\"\"\n        bins = self._read_binary(1)\n        subt = int(bins[:4], base=2)    # subtype number\n        bits = bins[4:]                 # 4-bit data\n        dlen = size - 1                 # length of remaining data\n\n        # fetch subtype-specific data\n        func = mptcp_opt.get(subt)\n        if func is None:    # if subtype not exist, directly read all data\n            temp = self._read_fileng(dlen)\n            data = dict(\n                kind=kind,\n                length=size,\n                subtype='Unknown',\n                data=bytes(chr(int(bits[:4], base=2)), encoding='utf-8') + temp,\n            )\n        else:               # fetch corresponding subtype data dict\n            data = func(self, bits, dlen, kind)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_mptcp_capable(self, bits, size, kind):\n        vers = int(bits, base=2)\n        bins = self._read_binary(1)\n        skey = self._read_fileng(8)\n        rkey = self._read_fileng(8) if size == 17 else None\n\n        data = dict(\n            kind=kind,\n            length=size + 1,\n            subtype='MP_CAPABLE',\n            capable=dict(\n                version=vers,\n                flags=dict(\n                    req=True if int(bins[0]) else False,\n                    ext=True if int(bins[1]) else False,\n                    res=bytes(chr(int(bits[2:7], base=2)), encoding='utf-8'),\n                    hsa=True if int(bins[7]) else False,\n                ),\n                skey=skey,\n                rkey=rkey,\n            ),\n        )\n\n        return data", "response": "Read Multipath Capable option."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_mptcp_join(self, bits, size, kind):\n        if self._syn and self._ack:     # MP_JOIN-SYN/ACK\n            return self._read_join_synack(bits, size, kind)\n        elif self._syn:                 # MP_JOIN-SYN\n            return self._read_join_syn(bits, size, kind)\n        elif self._ack:                 # MP_JOIN-ACK\n            return self._read_join_ack(bits, size, kind)\n        else:                           # illegal MP_JOIN occurred\n            temp = self._read_fileng(size)\n            data = dict(\n                kind=kind,\n                length=size + 1,\n                subtype='MP_JOIN-Unknown',\n                data=bytes(chr(int(bits[:4], base=2)), encoding='utf-8') + temp,\n            )\n            return data", "response": "Read Multipath Join Connection option."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading Join Connection option for Initial SYN.", "response": "def _read_join_syn(self, bits, size, kind):\n        \"\"\"Read Join Connection option for Initial SYN.\n\n        Positional arguments:\n            * bits - str, 4-bit data\n            * size - int, length of option\n            * kind - int, 30 (Multipath TCP)\n\n        Returns:\n            * dict -- extracted Join Connection (MP_JOIN-SYN) option for Initial SYN\n\n        Structure of MP_JOIN-SYN [RFC 6824]:\n                                 1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +---------------+---------------+-------+-----+-+---------------+\n            |     Kind      |  Length = 12  |Subtype|     |B|   Address ID  |\n            +---------------+---------------+-------+-----+-+---------------+\n            |                   Receiver's Token (32 bits)                  |\n            +---------------------------------------------------------------+\n            |                Sender's Random Number (32 bits)               |\n            +---------------------------------------------------------------+\n\n            Octets      Bits        Name                        Description\n              0           0     tcp.mp.kind                 Kind (30)\n              1           8     tcp.mp.length               Length (12)\n              2          16     tcp.mp.subtype              Subtype (1|SYN)\n              2          20     -                           Reserved (must be zero)\n              2          23     tcp.mp.join.syn.backup      Backup Path (B)\n              3          24     tcp.mp.join.syn.addrid      Address ID\n              4          32     tcp.mp.join.syn.token       Receiver's Token\n              8          64     tcp.mp.join.syn.randnum     Sender's Random Number\n\n        \"\"\"\n        adid = self._read_unpack(1)\n        rtkn = self._read_fileng(4)\n        srno = self._read_unpack(4)\n\n        data = dict(\n            kind=kind,\n            length=size + 1,\n            subtype='MP_JOIN-SYN',\n            join=dict(\n                syn=dict(\n                    backup=True if int(bits[3]) else False,\n                    addrid=adid,\n                    token=rtkn,\n                    randnum=srno,\n                ),\n            ),\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_join_synack(self, bits, size, kind):\n        adid = self._read_unpack(1)\n        hmac = self._read_fileng(8)\n        srno = self._read_unpack(4)\n\n        data = dict(\n            kind=kind,\n            length=size + 1,\n            subtype='MP_JOIN-SYN/ACK',\n            join=dict(\n                synack=dict(\n                    backup=True if int(bits[3]) else False,\n                    addrid=adid,\n                    hmac=hmac,\n                    randnum=srno,\n                ),\n            ),\n        )\n\n        return data", "response": "Read Join - SYN - ACK option."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_join_ack(self, bits, size, kind):\n        temp = self._read_fileng(20)\n\n        data = dict(\n            kind=kind,\n            length=size + 1,\n            subtype='MP_JOIN-ACK',\n            join=dict(\n                ack=dict(\n                    hmac=temp,\n                ),\n            ),\n        )\n\n        return data", "response": "Read Third ACK option."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread Data Sequence Signal (Data ACK and Data Sequence Mapping) option. Positional arguments: * bits - str, 4-bit data * size - int, length of option * kind - int, 30 (Multipath TCP) Returns: * dict -- extracted Data Sequence Signal (DSS) option Structure of DSS [RFC 6824]: 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +---------------+---------------+-------+----------------------+ | Kind | Length |Subtype| (reserved) |F|m|M|a|A| +---------------+---------------+-------+----------------------+ | Data ACK (4 or 8 octets, depending on flags) | +--------------------------------------------------------------+ | Data sequence number (4 or 8 octets, depending on flags) | +--------------------------------------------------------------+ | Subflow Sequence Number (4 octets) | +-------------------------------+------------------------------+ | Data-Level Length (2 octets) | Checksum (2 octets) | +-------------------------------+------------------------------+ 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +--------------------------------------------------------------+ | | | Data Sequence Number (8 octets) | | | +--------------------------------------------------------------+ | Subflow Sequence Number (4 octets) | +-------------------------------+------------------------------+ | Data-Level Length (2 octets) | Zeros (2 octets) | +-------------------------------+------------------------------+ Octets Bits Name Description 0 0 tcp.mp.kind Kind (30) 1 8 tcp.mp.length Length 2 16 tcp.mp.subtype Subtype (2) 2 20 - Reserved (must be zero) 3 27 tcp.mp.dss.flags.fin DATA_FIN (F) 3 28 tcp.mp.dss.flags.dsn_len DSN Length (m) 3 29 tcp.mp.dss.flags.data_pre DSN, SSN, Data-Level Length, CHKSUM Present (M) 3 30 tcp.mp.dss.flags.ack_len ACK Length (a) 3 31 tcp.mp.dss.flags.ack_pre Data ACK Present (A) 4 32 tcp.mp.dss.ack Data ACK (4/8 octets) 8-12 64-96 tcp.mp.dss.dsn DSN (4/8 octets) 12-20 48-160 tcp.mp.dss.ssn Subflow Sequence Number 16-24 128-192 tcp.mp.dss.dl_len Data-Level Length 18-26 144-208 tcp.mp.dss.checksum Checksum", "response": "def _read_mptcp_dss(self, bits, size, kind):\n        \"\"\"Read Data Sequence Signal (Data ACK and Data Sequence Mapping) option.\n\n        Positional arguments:\n            * bits - str, 4-bit data\n            * size - int, length of option\n            * kind - int, 30 (Multipath TCP)\n\n        Returns:\n            * dict -- extracted Data Sequence Signal (DSS) option\n\n        Structure of DSS [RFC 6824]:\n                                 1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +---------------+---------------+-------+----------------------+\n            |     Kind      |    Length     |Subtype| (reserved) |F|m|M|a|A|\n            +---------------+---------------+-------+----------------------+\n            |           Data ACK (4 or 8 octets, depending on flags)       |\n            +--------------------------------------------------------------+\n            |   Data sequence number (4 or 8 octets, depending on flags)   |\n            +--------------------------------------------------------------+\n            |              Subflow Sequence Number (4 octets)              |\n            +-------------------------------+------------------------------+\n            |  Data-Level Length (2 octets) |      Checksum (2 octets)     |\n            +-------------------------------+------------------------------+\n\n                                1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +--------------------------------------------------------------+\n            |                                                              |\n            |                Data Sequence Number (8 octets)               |\n            |                                                              |\n            +--------------------------------------------------------------+\n            |              Subflow Sequence Number (4 octets)              |\n            +-------------------------------+------------------------------+\n            |  Data-Level Length (2 octets) |        Zeros (2 octets)      |\n            +-------------------------------+------------------------------+\n\n            Octets      Bits        Name                        Description\n              0           0     tcp.mp.kind                 Kind (30)\n              1           8     tcp.mp.length               Length\n              2          16     tcp.mp.subtype              Subtype (2)\n              2          20     -                           Reserved (must be zero)\n              3          27     tcp.mp.dss.flags.fin        DATA_FIN (F)\n              3          28     tcp.mp.dss.flags.dsn_len    DSN Length (m)\n              3          29     tcp.mp.dss.flags.data_pre   DSN, SSN, Data-Level Length, CHKSUM Present (M)\n              3          30     tcp.mp.dss.flags.ack_len    ACK Length (a)\n              3          31     tcp.mp.dss.flags.ack_pre    Data ACK Present (A)\n              4          32     tcp.mp.dss.ack              Data ACK (4/8 octets)\n              8-12    64-96     tcp.mp.dss.dsn              DSN (4/8 octets)\n              12-20  48-160     tcp.mp.dss.ssn              Subflow Sequence Number\n              16-24 128-192     tcp.mp.dss.dl_len           Data-Level Length\n              18-26 144-208     tcp.mp.dss.checksum         Checksum\n\n        \"\"\"\n        bits = self._read_binary(1)\n        mflg = 8 if int(bits[4]) else 4\n        Mflg = True if int(bits[5]) else False\n        aflg = 8 if int(bits[6]) else 4\n        Aflg = True if int(bits[7]) else False\n        ack_ = self._read_fileng(aflg) if Aflg else None\n        dsn_ = self._read_unpack(mflg) if Mflg else None\n        ssn_ = self._read_unpack(4) if Mflg else None\n        dll_ = self._read_unpack(2) if Mflg else None\n        chk_ = self._read_fileng(2) if Mflg else None\n\n        data = dict(\n            kind=kind,\n            length=size + 1,\n            subtype='DSS',\n            dss=dict(\n                flags=dict(\n                    fin=True if int(bits[3]) else False,\n                    dsn_len=mflg,\n                    data_pre=Mflg,\n                    ack_len=aflg,\n                    ack_pre=Aflg,\n                ),\n                ack=ack_,\n                dsn=dsn_,\n                ssn=ssn_,\n                dl_len=dll_,\n                checksum=chk_,\n            ),\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads Multipath TCP Add Address option.", "response": "def _read_mptcp_add(self, bits, size, kind):\n        \"\"\"Read Add Address option.\n\n        Positional arguments:\n            * bits - str, 4-bit data\n            * size - int, length of option\n            * kind - int, 30 (Multipath TCP)\n\n        Returns:\n            * dict -- extracted Add Address (ADD_ADDR) option\n\n        Structure of ADD_ADDR [RFC 6824]:\n                                 1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +---------------+---------------+-------+-------+---------------+\n            |     Kind      |     Length    |Subtype| IPVer |  Address ID   |\n            +---------------+---------------+-------+-------+---------------+\n            |          Address (IPv4 - 4 octets / IPv6 - 16 octets)         |\n            +-------------------------------+-------------------------------+\n            |   Port (2 octets, optional)   |\n            +-------------------------------+\n\n            Octets      Bits        Name                        Description\n              0           0     tcp.mp.kind                 Kind (30)\n              1           8     tcp.mp.length               Length\n              2          16     tcp.mp.subtype              Subtype (3)\n              2          20     tcp.mp.addaddr.ipver        IP Version\n              3          24     tcp.mp.addaddr.addrid       Address ID\n              4          32     tcp.mp.addaddr.addr         IP Address (4/16)\n              8-20   64-160     tcp.mp.addaddr.port         Port (optional)\n\n        \"\"\"\n        vers = int(bits, base=2)\n        adid = self._read_unpack(1)\n        ipad = self._read_fileng(4) if vers == 4 else self._read_fileng(16)\n        ip_l = 4 if vers == 4 else 16\n        pt_l = size - 1 - ip_l\n        port = self._read_unpack(2) if pt_l else None\n\n        data = dict(\n            kind=kind,\n            length=size + 1,\n            subtype='ADD_ADDR',\n            addaddr=dict(\n                ipver=vers,\n                addrid=adid,\n                addr=ipaddress.ip_address(ipad),\n                port=port,\n            ),\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_mptcp_remove(self, bits, size):\n        adid = []\n        for _ in size:\n            adid.append(self._read_unpack(1))\n\n        data = dict(\n            subtype='REMOVE_ADDR',\n            removeaddr=dict(\n                addrid=adid or None,\n            ),\n        )\n\n        return data", "response": "Read Remove Address option."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading Change Subflow Priority option.", "response": "def _read_mptcp_prio(self, bits, size):\n        \"\"\"Read Change Subflow Priority option.\n\n        Positional arguments:\n            * bits - str, 4-bit data\n            * size - int, length of option\n\n        Returns:\n            * dict -- extracted Change Subflow Priority (MP_PRIO) option\n\n        Structure of MP_PRIO [RFC 6824]:\n                                  1                   2                   3\n              0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +---------------+---------------+-------+-----+-+--------------+\n            |     Kind      |     Length    |Subtype|     |B| AddrID (opt) |\n            +---------------+---------------+-------+-----+-+--------------+\n\n            Octets      Bits        Name                    Description\n              0           0     tcp.opt.kind            Kind (30)\n              1           8     tcp.opt.length          Length (3/4)\n              2          16     tcp.opt.mp.subtype      Subtype (5)\n              2          23     tcp.opt.mp.prio.backup  Backup Path (B)\n              3          24     tcp.opt.mp.prio.addrid  Address ID (optional)\n\n        \"\"\"\n        temp = self._read_unpack(1) if size else None\n\n        data = dict(\n            subtype='MP_PRIO',\n            prio=dict(\n                res=b'\\x00' * 3,\n                backup=True if int(bits[3]) else False,\n                addrid=temp,\n            ),\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_mptcp_fail(self, bits, size):\n        ____ = self._read_fileng(1)\n        dsn_ = self._read_unpack(8)\n\n        data = dict(\n            subtype='MP_FAIL',\n            fail=dict(\n                dsn=dsn_,\n            ),\n        )\n\n        return data", "response": "Read Fallback option.\n\n        Positional arguments:\n            * bits - str, 4-bit data\n            * size - int, length of option\n\n        Returns:\n            * dict -- extracted Fallback (MP_FAIL) option\n\n        Structure of MP_FAIL [RFC 6824]:\n                                 1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +---------------+---------------+-------+----------------------+\n            |     Kind      |   Length=12   |Subtype|      (reserved)      |\n            +---------------+---------------+-------+----------------------+\n            |                                                              |\n            |                 Data Sequence Number (8 octets)              |\n            |                                                              |\n            +--------------------------------------------------------------+\n\n            Octets      Bits        Name                    Description\n              0           0     tcp.opt.kind            Kind (30)\n              1           8     tcp.opt.length          Length (12)\n              2          16     tcp.opt.mp.subtype      Subtype (6)\n              2          23     -                       Reserved (must be zero)\n              4          32     tcp.opt.mp.fail.dsn     Data Sequence Number"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_mptcp_fastclose(self, bits, size):\n        ____ = self._read_fileng(1)\n        rkey = self._read_fileng(8)\n\n        data = dict(\n            subtype='MP_FASTCLOSE',\n            fastclose=dict(\n                rkey=rkey,\n            ),\n        )\n\n        return data", "response": "Read One - Time Fast Close option."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting a PCAP file.", "response": "def extract(fin=None, fout=None, format=None,                           # basic settings\n            auto=True, extension=True, store=True,                      # internal settings\n            files=False, nofile=False, verbose=False,                   # output settings\n            engine=None, layer=None, protocol=None,                     # extraction settings\n            ip=False, ipv4=False, ipv6=False, tcp=False, strict=True,   # reassembly settings\n            trace=False, trace_fout=None, trace_format=None,            # trace settings\n            trace_byteorder=sys.byteorder, trace_nanosecond=False):     # trace settings\n    \"\"\"Extract a PCAP file.\n\n    Keyword arguments:\n        * fin  -- str, file name to be read; if file not exist, raise an error\n        * fout -- str, file name to be written\n        * format  -- str, file format of output\n                        <keyword> 'plist' / 'json' / 'tree' / 'html'\n\n        * auto -- bool, if automatically run till EOF (default is True)\n                        <keyword> True / False\n        * extension -- bool, if check and append extensions to output file (default is True)\n                        <keyword> True / False\n        * store -- bool, if store extracted packet info (default is True)\n                        <keyword> True / False\n\n        * files -- bool, if split each frame into different files (default is False)\n                        <keyword> True / False\n        * nofile -- bool, if no output file is to be dumped (default is False)\n                        <keyword> True / False\n        * verbose -- bool, if print verbose output information (default is False)\n                        <keyword> True / False\n\n        * engine -- str, extraction engine to be used\n                        <keyword> 'default | pcapkit'\n        * layer -- str, extract til which layer\n                        <keyword> 'Link' / 'Internet' / 'Transport' / 'Application'\n        * protocol -- str, extract til which protocol\n                        <keyword> available protocol name\n\n        * ip -- bool, if record data for IPv4 & IPv6 reassembly (default is False)\n                        <keyword> True / False\n        * ipv4 -- bool, if perform IPv4 reassembly (default is False)\n                        <keyword> True / False\n        * ipv6 -- bool, if perform IPv6 reassembly (default is False)\n                        <keyword> True / False\n        * tcp -- bool, if perform TCP reassembly (default is False)\n                        <keyword> True / False\n        * strict -- bool, if set strict flag for reassembly (default is True)\n                        <keyword> True / False\n\n        * trace -- bool, if trace TCP traffic flows (default is False)\n                        <keyword> True / False\n        * trace_fout -- str, path name for flow tracer if necessary\n        * trace_format -- str, output file format of flow tracer\n                        <keyword> 'plist' / 'json' / 'tree' / 'html' / 'pcap'\n        * trace_byteorder -- str, output file byte order\n                        <keyword> 'little' / 'big'\n        * trace_nanosecond -- bool, output nanosecond-resolution file flag\n                        <keyword> True / False\n\n    Returns:\n        * Extractor -- an Extractor object form `pcapkit.extractor`\n\n    \"\"\"\n    if isinstance(layer, type) and issubclass(layer, Protocol):\n        layer = layer.__layer__\n    if isinstance(protocol, type) and issubclass(protocol, Protocol):\n        protocol = protocol.__index__()\n\n    str_check(fin or '', fout or '', format or '',\n              trace_fout or '', trace_format or '',\n              engine or '', layer or '', *(protocol or ''))\n    bool_check(files, nofile, verbose, auto, extension, store,\n               ip, ipv4, ipv6, tcp, strict, trace)\n\n    return Extractor(fin=fin, fout=fout, format=format,\n                     store=store, files=files, nofile=nofile,\n                     auto=auto, verbose=verbose, extension=extension,\n                     engine=engine, layer=layer, protocol=protocol,\n                     ip=ip, ipv4=ipv4, ipv6=ipv6, tcp=tcp, strict=strict,\n                     trace=trace, trace_fout=trace_fout, trace_format=trace_format,\n                     trace_byteorder=trace_byteorder, trace_nanosecond=trace_nanosecond)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nanalyses application layer packets.", "response": "def analyse(file, length=None):\n    \"\"\"Analyse application layer packets.\n\n    Keyword arguments:\n        * file -- bytes or file-like object, packet to be analysed\n        * length -- int, length of the analysing packet\n\n    Returns:\n        * Analysis -- an Analysis object from `pcapkit.analyser`\n\n    \"\"\"\n    if isinstance(file, bytes):\n        file = io.BytesIO(file)\n\n    io_check(file)\n    int_check(length or sys.maxsize)\n\n    return analyse2(file, length)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reassemble(protocol, strict=False):\n    if isinstance(protocol, type) and issubclass(protocol, Protocol):\n        protocol = protocol.__index__()\n\n    str_check(protocol)\n    bool_check(strict)\n\n    if protocol == 'IPv4':\n        return IPv4_Reassembly(strict=strict)\n    elif protocol == 'IPv6':\n        return IPv6_Reassembly(strict=strict)\n    elif protocol == 'TCP':\n        return TCP_Reassembly(strict=strict)\n    else:\n        raise FormatError(f'Unsupported reassembly protocol: {protocol}')", "response": "Reassemble the datagrams in the specified protocol into a new set of datagrams."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef trace(fout=None, format=None, byteorder=sys.byteorder, nanosecond=False):\n    str_check(fout or '', format or '')\n    return TraceFlow(fout=fout, format=format, byteorder=byteorder, nanosecond=nanosecond)", "response": "Trace TCP flows.\n\n    Keyword arguments:\n        * fout -- str, output path\n        * format -- str, output format\n        * byteorder -- str, output file byte order\n        * nanosecond -- bool, output nanosecond-resolution file flag"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads IPv6 - Fragment header.", "response": "def read_ipv6_frag(self, length, extension):\n        \"\"\"Read Fragment Header for IPv6.\n\n        Structure of IPv6-Frag header [RFC 8200]:\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |  Next Header  |   Reserved    |      Fragment Offset    |Res|M|\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                         Identification                        |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     frag.next               Next Header\n              1           8     -                       Reserved\n              2          16     frag.offset             Fragment Offset\n              3          29     -                       Reserved\n              3          31     frag.mf                 More Flag\n              4          32     frag.id                 Identification\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        _next = self._read_protos(1)\n        _temp = self._read_fileng(1)\n        _offm = self._read_binary(2)\n        _ipid = self._read_unpack(4)\n\n        ipv6_frag = dict(\n            next=_next,\n            length=8,\n            offset=int(_offm[:13], base=2),\n            mf=True if int(_offm[15], base=2) else False,\n            id=_ipid,\n        )\n\n        length -= ipv6_frag['length']\n        ipv6_frag['packet'] = self._read_packet(header=8, payload=length)\n\n        if extension:\n            self._protos = None\n            return ipv6_frag\n        return self._decode_next_layer(ipv6_frag, _next, length)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return HI_Algorithm(key)\n        if key not in HI_Algorithm._member_map_:\n            extend_enum(HI_Algorithm, key, default)\n        return HI_Algorithm[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread an IPv4 header from the file and return it.", "response": "def read_ipv4(self, length):\n        \"\"\"Read Internet Protocol version 4 (IPv4).\n\n        Structure of IPv4 header [RFC 791]:\n\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |Version|  IHL  |Type of Service|          Total Length         |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |         Identification        |Flags|      Fragment Offset    |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |  Time to Live |    Protocol   |         Header Checksum       |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                       Source Address                          |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                    Destination Address                        |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                    Options                    |    Padding    |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     ip.version              Version (4)\n              0           4     ip.hdr_len              Internal Header Length (IHL)\n              1           8     ip.dsfield.dscp         Differentiated Services Code Point (DSCP)\n              1          14     ip.dsfield.ecn          Explicit Congestion Notification (ECN)\n              2          16     ip.len                  Total Length\n              4          32     ip.id                   Identification\n              6          48     -                       Reserved Bit (must be zero)\n              6          49     ip.flags.df             Don't Fragment (DF)\n              6          50     ip.flags.mf             More Fragments (MF)\n              6          51     ip.frag_offset          Fragment Offset\n              8          64     ip.ttl                  Time To Live (TTL)\n              9          72     ip.proto                Protocol (Transport Layer)\n              10         80     ip.checksum             Header Checksum\n              12         96     ip.src                  Source IP Address\n              16        128     ip.dst                  Destination IP Address\n              20        160     ip.options              IP Options (if IHL > 5)\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        _vihl = self._read_fileng(1).hex()\n        _dscp = self._read_binary(1)\n        _tlen = self._read_unpack(2)\n        _iden = self._read_unpack(2)\n        _frag = self._read_binary(2)\n        _ttol = self._read_unpack(1)\n        _prot = self._read_protos(1)\n        _csum = self._read_fileng(2)\n        _srca = self._read_ipv4_addr()\n        _dsta = self._read_ipv4_addr()\n\n        ipv4 = dict(\n            version=_vihl[0],\n            hdr_len=int(_vihl[1], base=16) * 4,\n            dsfield=dict(\n                dscp=(\n                    TOS_PRE.get(int(_dscp[:3], base=2)),\n                    TOS_DEL.get(int(_dscp[3], base=2)),\n                    TOS_THR.get(int(_dscp[4], base=2)),\n                    TOS_REL.get(int(_dscp[5], base=2)),\n                ),\n                ecn=TOS_ECN.get(int(_dscp[-2:], base=2)),\n            ),\n            len=_tlen,\n            id=_iden,\n            flags=dict(\n                df=True if int(_frag[1]) else False,\n                mf=True if int(_frag[2]) else False,\n            ),\n            frag_offset=int(_frag[3:], base=2) * 8,\n            ttl=_ttol,\n            proto=_prot,\n            checksum=_csum,\n            src=_srca,\n            dst=_dsta,\n        )\n\n        _optl = ipv4['hdr_len'] - 20\n        if _optl:\n            options = self._read_ipv4_options(_optl)\n            ipv4['opt'] = options[0]    # tuple of option acronyms\n            ipv4.update(options[1])     # merge option info to buffer\n            # ipv4['opt'] = self._read_fileng(_optl) or None\n\n        hdr_len = ipv4['hdr_len']\n        raw_len = ipv4['len'] - hdr_len\n        ipv4['packet'] = self._read_packet(header=hdr_len, payload=raw_len)\n\n        return self._decode_next_layer(ipv4, _prot, raw_len)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the option type field.", "response": "def _read_opt_type(self, kind):\n        \"\"\"Read option type field.\n\n        Positional arguments:\n            * kind -- int, option kind value\n\n        Returns:\n            * dict -- extracted IPv4 option\n\n        Structure of option type field [RFC 791]:\n\n            Octets      Bits        Name                    Descriptions\n              0           0     ip.opt.type.copy        Copied Flag (0/1)\n              0           1     ip.opt.type.class       Option Class (0-3)\n              0           3     ip.opt.type.number      Option Number\n\n        \"\"\"\n        bin_ = bin(kind)[2:].zfill(8)\n\n        type_ = {\n            'copy': bool(int(bin_[0], base=2)),\n            'class': opt_class.get(int(bin_[1:3], base=2)),\n            'number': int(bin_[3:], base=2),\n        }\n\n        return type_"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading IPv4 option list.", "response": "def _read_ipv4_options(self, size=None):\n        \"\"\"Read IPv4 option list.\n\n        Positional arguments:\n            * size -- int, buffer size\n\n        Returns:\n            * tuple -- IPv4 option list\n            * dict -- extracted IPv4 option\n\n        \"\"\"\n        counter = 0         # length of read option list\n        optkind = list()    # option kind list\n        options = dict()    # dict of option data\n\n        while counter < size:\n            # get option kind\n            kind = self._read_unpack(1)\n\n            # fetch corresponding option tuple\n            opts = IPv4_OPT.get(kind)\n            if opts is None:\n                len_ = size - counter\n                counter = size\n                options['Unknown'] = self._read_fileng(len_)\n                break\n\n            # extract option\n            dscp = OPT_TYPE.get(kind)\n            desc = dscp.name\n            if opts[0]:\n                byte = self._read_unpack(1)\n                if byte:    # check option process mode\n                    data = process_opt[opts[2]](self, byte, kind)\n                else:       # permission options (length is 2)\n                    data = dict(\n                        kind=kind,                          # option kind\n                        type=self._read_opt_type(kind),     # option type info\n                        length=2,                           # option length\n                        flag=True,                          # permission flag\n                    )\n            else:           # 1-bytes options\n                byte = 1\n\n                data = dict(\n                    kind=kind,                          # option kind\n                    type=self._read_opt_type(kind),     # option type info\n                    length=1,                           # option length\n                )\n\n            # record option data\n            counter += byte\n            if dscp in optkind:\n                if isinstance(options[desc], tuple):\n                    options[desc] += (Info(data),)\n                else:\n                    options[desc] = (Info(options[desc]), Info(data))\n            else:\n                optkind.append(dscp)\n                options[desc] = data\n\n            # break when eol triggered\n            if not kind:\n                break\n\n        # get padding\n        if counter < size:\n            len_ = size - counter\n            self._read_binary(len_)\n\n        return tuple(optkind), options"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread options request no process.", "response": "def _read_mode_donone(self, size, kind):\n        \"\"\"Read options request no process.\n\n        Positional arguments:\n            * size - int, length of option\n            * kind - int, option kind value\n\n        Returns:\n            * dict -- extracted option\n\n        Structure of IPv4 options:\n            Octets      Bits        Name                    Description\n              0           0     ip.opt.kind             Kind\n              0           0     ip.opt.type.copy        Copied Flag\n              0           1     ip.opt.type.class       Option Class\n              0           3     ip.opt.type.number      Option Number\n              1           8     ip.opt.length           Length\n              2          16     ip.opt.data             Kind-specific Data\n\n        \"\"\"\n        if size < 3:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        data = dict(\n            kind=kind,\n            type=self._read_opt_type(kind),\n            length=size,\n            data=self._read_fileng(size),\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread options request unpack process.", "response": "def _read_mode_unpack(self, size, kind):\n        \"\"\"Read options request unpack process.\n\n        Positional arguments:\n            * size - int, length of option\n            * kind - int, option kind value\n\n        Returns:\n            * dict -- extracted option\n\n        Structure of IPv4 options:\n            Octets      Bits        Name                    Description\n              0           0     ip.opt.kind             Kind\n              0           0     ip.opt.type.copy        Copied Flag\n              0           1     ip.opt.type.class       Option Class\n              0           3     ip.opt.type.number      Option Number\n              1           8     ip.opt.length           Length\n              2          16     ip.opt.data             Kind-specific Data\n\n        \"\"\"\n        if size < 3:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        data = dict(\n            kind=kind,\n            type=self._read_opt_type(kind),\n            length=size,\n            data=self._read_unpack(size),\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_mode_route(self, size, kind):\n        if size < 3 or (size - 3) % 4 != 0:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        _rptr = self._read_unpack(1)\n\n        if _rptr < 4:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        data = dict(\n            kind=kind,\n            type=self._read_opt_type(kind),\n            length=size,\n            pointer=_rptr,\n        )\n\n        counter = 4\n        address = list()\n        endpoint = min(_rptr, size)\n        while counter < endpoint:\n            counter += 4\n            address.append(self._read_ipv4_addr())\n        data['ip'] = address or None\n\n        return data", "response": "Read option with route data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_mode_qs(self, size, kind):\n        if size != 8:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        _type = self._read_opt_type(kind)\n        _fcrr = self._read_binary(1)\n        _func = int(_fcrr[:4], base=2)\n        _rate = int(_fcrr[4:], base=2)\n        _ttlv = self._read_unpack(1)\n        _nonr = self._read_binary(4)\n        _qsnn = int(_nonr[:30], base=2)\n\n        if _func != 0 and _func != 8:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        data = dict(\n            kind=kind,\n            type=_type,\n            length=size,\n            func=QS_FUNC.get(_func),\n            rate=40000 * (2 ** _rate) / 1000,\n            ttl=None if _func else _rate,\n            nounce=_qsnn,\n        )\n\n        return data", "response": "Read Quick - Start option."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_mode_ts(self, size, kind):\n        if size > 40 or size < 4:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        _tptr = self._read_unpack(1)\n        _oflg = self._read_binary(1)\n        _oflw = int(_oflg[:4], base=2)\n        _flag = int(_oflg[4:], base=2)\n\n        if _tptr < 5:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        data = dict(\n            kind=kind,\n            type=self._read_opt_type(kind),\n            length=size,\n            pointer=_tptr,\n            overflow=_oflw,\n            flag=_flag,\n        )\n\n        endpoint = min(_tptr, size)\n        if _flag == 0:\n            if (size - 4) % 4 != 0:\n                raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n            counter = 5\n            timestamp = list()\n            while counter < endpoint:\n                counter += 4\n                time = self._read_unpack(4, lilendian=True)\n                timestamp.append(datetime.datetime.fromtimestamp(time))\n            data['timestamp'] = timestamp or None\n        elif _flag == 1 or _flag == 3:\n            if (size - 4) % 8 != 0:\n                raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n            counter = 5\n            ipaddress = list()\n            timestamp = list()\n            while counter < endpoint:\n                counter += 8\n                ipaddress.append(self._read_ipv4_addr())\n                time = self._read_unpack(4, lilendian=True)\n                timestamp.append(datetime.datetime.fromtimestamp(time))\n            data['ip'] = ipaddress or None\n            data['timestamp'] = timestamp or None\n        else:\n            data['data'] = self._read_fileng(size - 4) or None\n\n        return data", "response": "Read Time Stamp option."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading option for Traceroute Section 3. 1. 3. 1. 1.", "response": "def _read_mode_tr(self, size, kind):\n        \"\"\"Read Traceroute option.\n\n        Positional arguments:\n            size - int, length of option\n            kind - int, 82 (TR)\n\n        Returns:\n            * dict -- extracted Traceroute (TR) option\n\n        Structure of Traceroute (TR) option [RFC 1393][RFC 6814]:\n             0               8              16              24\n            +-+-+-+-+-+-+-+-+---------------+---------------+---------------+\n            |F| C |  Number |    Length     |          ID Number            |\n            +-+-+-+-+-+-+-+-+---------------+---------------+---------------+\n            |      Outbound Hop Count       |       Return Hop Count        |\n            +---------------+---------------+---------------+---------------+\n            |                     Originator IP Address                     |\n            +---------------+---------------+---------------+---------------+\n\n            Octets      Bits        Name                    Description\n              0           0     ip.tr.kind              Kind (82)\n              0           0     ip.tr.type.copy         Copied Flag (0)\n              0           1     ip.tr.type.class        Option Class (0)\n              0           3     ip.tr.type.number       Option Number (18)\n              1           8     ip.tr.length            Length (12)\n              2          16     ip.tr.id                ID Number\n              4          32     ip.tr.ohc               Outbound Hop Count\n              6          48     ip.tr.rhc               Return Hop Count\n              8          64     ip.tr.ip                Originator IP Address\n\n        \"\"\"\n        if size != 12:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        _idnm = self._read_unpack(2)\n        _ohcn = self._read_unpack(2)\n        _rhcn = self._read_unpack(2)\n        _ipad = self._read_ipv4_addr()\n\n        data = dict(\n            kind=kind,\n            type=self._read_opt_type(kind),\n            length=size,\n            id=_idnm,\n            ohc=_ohcn,\n            rhc=_rhcn,\n            ip=_ipad,\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_mode_sec(self, size, kind):\n        if size < 3:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        _clvl = self._read_unpack(1)\n\n        data = dict(\n            kind=kind,\n            type=self._read_opt_type(kind),\n            length=size,\n            level=_CLASSIFICATION_LEVEL.get(_clvl, _clvl),\n        )\n\n        if size > 3:\n            _list = list()\n            for counter in range(3, size):\n                _flag = self._read_binary(1)\n                if (counter < size - 1 and not int(_flag[7], base=2)) \\\n                        or (counter == size - 1 and int(_flag[7], base=2)):\n                    raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n                _dict = dict()\n                for (index, bit) in enumerate(_flag[:5]):\n                    _auth = _PROTECTION_AUTHORITY.get(index)\n                    _dict[_auth] = True if int(bit, base=2) else False\n                _list.append(Info(_dict))\n            data['flags'] = tuple(_list)\n\n        return data", "response": "Read options with security info."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_mode_rsralt(self, size, kind):\n        if size != 4:\n            raise ProtocolError(f'{self.alias}: [Optno {kind}] invalid format')\n\n        _code = self._read_unpack(2)\n\n        data = dict(\n            kind=kind,\n            type=self._read_opt_type(kind),\n            length=size,\n            alert=_ROUTER_ALERT.get(_code, 'Reserved'),\n            code=_code,\n        )\n\n        return data", "response": "Read Router Alert option."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the global header of PCAP file.", "response": "def read_header(self):\n        \"\"\"Read global header of PCAP file.\n\n        Structure of global header (C):\n            typedef struct pcap_hdr_s {\n                guint32 magic_number;   /* magic number */\n                guint16 version_major;  /* major version number */\n                guint16 version_minor;  /* minor version number */\n                gint32  thiszone;       /* GMT to local correction */\n                guint32 sigfigs;        /* accuracy of timestamps */\n                guint32 snaplen;        /* max length of captured packets, in octets */\n                guint32 network;        /* data link type */\n            } pcap_hdr_t;\n\n        \"\"\"\n        _magn = self._read_fileng(4)\n        if _magn == b'\\xd4\\xc3\\xb2\\xa1':\n            lilendian = True\n            self._nsec = False\n            self._byte = 'little'\n        elif _magn == b'\\xa1\\xb2\\xc3\\xd4':\n            lilendian = False\n            self._nsec = False\n            self._byte = 'big'\n        elif _magn == b'\\x4d\\x3c\\xb2\\xa1':\n            lilendian = True\n            self._nsec = True\n            self._byte = 'little'\n        elif _magn == b'\\xa1\\xb2\\x3c\\x4d':\n            lilendian = False\n            self._nsec = True\n            self._byte = 'big'\n        else:\n            raise FileError(5, 'Unknown file format', self._file.name)\n\n        _vmaj = self._read_unpack(2, lilendian=lilendian)\n        _vmin = self._read_unpack(2, lilendian=lilendian)\n        _zone = self._read_unpack(4, lilendian=lilendian, signed=True)\n        _acts = self._read_unpack(4, lilendian=lilendian)\n        _slen = self._read_unpack(4, lilendian=lilendian)\n        _type = self._read_protos(4)\n\n        _byte = self._read_packet(24)\n        self._file = io.BytesIO(_byte) \n\n        header = dict(\n            magic_number=dict(\n                data=_magn,\n                byteorder=self._byte,\n                nanosecond=self._nsec,\n            ),\n            version_major=_vmaj,\n            version_minor=_vmin,\n            thiszone=_zone,\n            sigfigs=_acts,\n            snaplen=_slen,\n            network=_type,\n            packet=_byte,\n        )\n\n        return header"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads next layer protocol type.", "response": "def _read_protos(self, size):\n        \"\"\"Read next layer protocol type.\n\n        Positional arguments:\n            * size  -- int, buffer size\n\n        Returns:\n            * str -- link layer protocol name\n\n        \"\"\"\n        _byte = self._read_unpack(4, lilendian=True)\n        _prot = LINKTYPE.get(_byte)\n        return _prot"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_frame(self):\n        # _scur = self._file.tell()\n        _temp = self._read_unpack(4, lilendian=True, quiet=True)\n        if _temp is None:\n            raise EOFError\n\n        _tsss = _temp\n        _tsus = self._read_unpack(4, lilendian=True)\n        _ilen = self._read_unpack(4, lilendian=True)\n        _olen = self._read_unpack(4, lilendian=True)\n\n        if self._nsec:\n            _epch = _tsss + _tsus / 1000000000\n        else:\n            _epch = _tsss + _tsus / 1000000\n        _time = datetime.datetime.fromtimestamp(_epch)\n\n        frame = dict(\n            frame_info=dict(\n                ts_sec=_tsss,\n                ts_usec=_tsus,\n                incl_len=_ilen,\n                orig_len=_olen,\n            ),\n            time=_time,\n            number=self._fnum,\n            time_epoch=_epch,\n            len=_ilen,\n            cap_len=_olen,\n        )\n\n        # load packet data\n        length = frame['len']\n        bytes_ = self._file.read(length)\n\n        # record file pointer\n        if self._mpkt and self._mpfp:\n            # print(self._fnum, 'ready')\n            self._mpfp.put(self._file.tell())\n            self._mpkt.pool += 1\n\n        # make BytesIO from frame packet data\n        frame['packet'] = bytes_\n        self._file = io.BytesIO(bytes_)\n        # frame['packet'] = self._read_packet(header=0, payload=length, discard=True)\n\n        return self._decode_next_layer(frame, length)", "response": "Read each block of data from the file and return a dict of the record class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes next layer protocol into dict.", "response": "def _decode_next_layer(self, dict_, length=None):\n        \"\"\"Decode next layer protocol.\n\n        Positional arguments:\n            dict_ -- dict, info buffer\n            proto -- str, next layer protocol name\n            length -- int, valid (not padding) length\n\n        Returns:\n            * dict -- current protocol with packet extracted\n\n        \"\"\"\n        seek_cur = self._file.tell()\n        try:\n            next_ = self._import_next_layer(self._prot, length)\n        except Exception:\n            dict_['error'] = traceback.format_exc(limit=1).strip().split(os.linesep)[-1]\n            self._file.seek(seek_cur, os.SEEK_SET)\n            next_ = beholder(self._import_next_layer)(self, self._prot, length, error=True)\n        info, chain = next_.info, next_.protochain\n\n        # make next layer protocol name\n        layer = next_.alias.lower()\n        # proto = next_.__class__.__name__\n\n        # write info and protocol chain into dict\n        self._next = next_\n        self._protos = chain\n        dict_[layer] = info\n        dict_['protocols'] = self._protos.chain\n        return dict_"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _import_next_layer(self, proto, length, error=False):\n        if proto == 1:\n            from pcapkit.protocols.link import Ethernet as Protocol\n        elif proto == 228:\n            from pcapkit.protocols.internet import IPv4 as Protocol\n        elif proto == 229:\n            from pcapkit.protocols.internet import IPv6 as Protocol\n        else:\n            from pcapkit.protocols.raw import Raw as Protocol\n        next_ = Protocol(self._file, length, error=error,\n                         layer=self._exlayer, protocol=self._exproto)\n        return next_", "response": "Imports next layer from the file and returns the next layer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return RegistrationFailure(key)\n        if key not in RegistrationFailure._member_map_:\n            extend_enum(RegistrationFailure, key, default)\n        return RegistrationFailure[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return TOS_PRE(key)\n        if key not in TOS_PRE._member_map_:\n            extend_enum(TOS_PRE, key, default)\n        return TOS_PRE[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads raw packet data.", "response": "def read_raw(self, length, *, error=None):\n        \"\"\"Read raw packet data.\"\"\"\n        if length is None:\n            length = len(self)\n\n        raw = dict(\n            packet=self._read_fileng(length),\n            error=error or None,\n        )\n\n        return raw"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return NotifyMessage(key)\n        if key not in NotifyMessage._member_map_:\n            extend_enum(NotifyMessage, key, default)\n        return NotifyMessage[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate length of headers before IPv6 - Fragment", "response": "def ipv6_hdr_len(ipv6):\n    \"\"\"Calculate length of headers before IPv6-Frag\"\"\"\n    hdr_len = ipv6.__hdr_len__\n    for code in (0, 60, 43):\n        ext_hdr = ipv6.extension_hdrs.get(code)\n        if ext_hdr is not None:\n            hdr_len += ext_hdr.length\n    return hdr_len"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef packet2chain(packet):\n    chain = [type(packet).__name__]\n    payload = packet.data\n    while not isinstance(payload, bytes):\n        chain.append(type(payload).__name__)\n        payload = payload.data\n    return ':'.join(chain)", "response": "Fetch DPKT packet protocol chain."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert DPKT packet into dict.", "response": "def packet2dict(packet, timestamp, *, data_link):\n    \"\"\"Convert DPKT packet into dict.\"\"\"\n    def wrapper(packet):\n        dict_ = dict()\n        for field in packet.__hdr_fields__:\n            dict_[field] = getattr(packet, field, None)\n        payload = packet.data\n        if not isinstance(payload, bytes):\n            dict_[type(payload).__name__] = wrapper(payload)\n        return dict_\n    return {\n        'timestamp': timestamp,\n        'packet': packet.pack(),\n        data_link.name: wrapper(packet),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes data for IPv4 reassembly.", "response": "def ipv4_reassembly(packet, *, count=NotImplemented):\n    \"\"\"Make data for IPv4 reassembly.\"\"\"\n    ipv4 = getattr(packet, 'ip', None)\n    if ipv4 is not None:\n        if ipv4.df:     # dismiss not fragmented packet\n            return False, None\n        data = dict(\n            bufid=(\n                ipaddress.ip_address(ipv4.src),                 # source IP address\n                ipaddress.ip_address(ipv4.dst),                 # destination IP address\n                ipv4.id,                                        # identification\n                TP_PROTO.get(ipv4.p).name,                      # payload protocol type\n            ),\n            num=count,                                          # original packet range number\n            fo=ipv4.off,                                        # fragment offset\n            ihl=ipv4.__hdr_len__,                               # internet header length\n            mf=bool(ipv4.mf),                                   # more fragment flag\n            tl=ipv4.len,                                        # total length, header includes\n            header=bytearray(ipv4.pack()[:ipv4.__hdr_len__]),   # raw bytearray type header\n            payload=bytearray(ipv4.pack()[ipv4.__hdr_len__:]),  # raw bytearray type payload\n        )\n        return True, data\n    return False, None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ipv6_reassembly(packet, *, count=NotImplemented):\n    ipv6 = getattr(packet, 'ip6', None)\n    if ipv6 is not None:\n        ipv6_frag = ipv6.extension_hdrs.get(44)\n        if ipv6_frag is None:       # dismiss not fragmented packet\n            return False, None\n        hdr_len = ipv6_hdr_len(ipv6)\n        data = dict(\n            bufid=(\n                ipaddress.ip_address(ipv6.src),                     # source IP address\n                ipaddress.ip_address(ipv6.dst),                     # destination IP address\n                ipv6.flow,                                          # label\n                TP_PROTO.get(ipv6_frag.nh).name,                    # next header field in IPv6 Fragment Header\n            ),\n            num=count,                                              # original packet range number\n            fo=ipv6_frag.nxt,                                       # fragment offset\n            ihl=hdr_len,                                            # header length, only headers before IPv6-Frag\n            mf=bool(ipv6_frag.m_flag),                              # more fragment flag\n            tl=len(ipv6),                                           # total length, header includes\n            header=bytearray(ipv6.pack()[:hdr_len]),                # raw bytearray type header before IPv6-Frag\n            payload=bytearray(ipv6.pack()[hdr_len+ipv6_frag:]),     # raw bytearray type payload after IPv6-Frag\n        )\n        return True, data\n    return False, None", "response": "Make data for IPv6 reassembly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tcp_reassembly(packet, *, count=NotImplemented):\n    if getattr(packet, 'ip', None):\n        ip = packet['ip']\n    elif getattr(packet, 'ip6', None):\n        ip = packet['ip6']\n    else:\n        return False, None\n    tcp = getattr(ip, 'tcp', None)\n    if tcp is not None:\n        flags = bin(tcp.flags)[2:].zfill(8)\n        data = dict(\n            bufid=(\n                ipaddress.ip_address(ip.src),                   # source IP address\n                ipaddress.ip_address(ip.dst),                   # destination IP address\n                tcp.sport,                                      # source port\n                tcp.dport,                                      # destination port\n            ),\n            num=count,                                          # original packet range number\n            ack=tcp.ack,                                        # acknowledgement\n            dsn=tcp.seq,                                        # data sequence number\n            rst=bool(int(flags[5])),                            # reset connection flag\n            syn=bool(int(flags[6])),                            # synchronise flag\n            fin=bool(int(flags[7])),                            # finish flag\n            payload=bytearray(tcp.pack()[tcp.__hdr_len__:]),    # raw bytearray type payload\n        )\n        raw_len = len(tcp.data)                                 # payload length, header excludes\n        data['first'] = tcp.seq                                 # this sequence number\n        data['last'] = tcp.seq + raw_len                        # next (wanted) sequence number\n        data['len'] = raw_len                                   # payload length, header excludes\n        return True, data\n    return False, None", "response": "Make data for TCP reassembly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntrace packet flow for TCP.", "response": "def tcp_traceflow(packet, timestamp, *, data_link, count=NotImplemented):\n    \"\"\"Trace packet flow for TCP.\"\"\"\n    if getattr(packet, 'ip', None):\n        ip = packet['ip']\n    elif getattr(packet, 'ip6', None):\n        ip = packet['ip6']\n    else:\n        return False, None\n    tcp = getattr(ip, 'tcp', None)\n    if tcp is not None:\n        flags = bin(tcp.flags)[2:].zfill(8)\n        data = dict(\n            protocol=data_link,                                         # data link type from global header\n            index=count,                                                # frame number\n            frame=packet2dict(packet, timestamp, data_link=data_link),  # extracted packet\n            syn=bool(int(flags[6])),                                    # TCP synchronise (SYN) flag\n            fin=bool(int(flags[7])),                                    # TCP finish (FIN) flag\n            src=ipaddress.ip_address(ip.src),                           # source IP\n            dst=ipaddress.ip_address(ip.dst),                           # destination IP\n            srcport=tcp.sport,                                          # TCP source port\n            dstport=tcp.dport,                                          # TCP destination port\n            timestamp=timestamp,                                        # timestamp\n        )\n        return True, data\n    return False, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads Hypertext Transfer Protocol HTTP.", "response": "def read_http(self, length):\n        \"\"\"Read Hypertext Transfer Protocol (HTTP/2).\n\n        Structure of HTTP/2 packet [RFC 7540]:\n            +-----------------------------------------------+\n            |                 Length (24)                   |\n            +---------------+---------------+---------------+\n            |   Type (8)    |   Flags (8)   |\n            +-+-------------+---------------+-------------------------------+\n            |R|                 Stream Identifier (31)                      |\n            +=+=============================================================+\n            |                   Frame Payload (0...)                      ...\n            +---------------------------------------------------------------+\n\n            Octets      Bits        Name                    Description\n              0           0     http.length             Length\n              3          24     http.type               Type\n              4          32     http.flags              Flags\n              5          40     -                       Reserved\n              5          41     http.sid                Stream Identifier\n              9          72     http.payload            Frame Payload\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        if length < 9:\n            raise ProtocolError(f'HTTP/2: invalid format', quiet=True)\n\n        _tlen = self._read_unpack(3)\n        _type = self._read_unpack(1)\n        _flag = self._read_binary(1)\n        _rsid = self._read_binary(4)\n\n        if _tlen != length:\n            raise ProtocolError(f'HTTP/2: [Type {_type}] invalid format', quiet=True)\n\n        if int(_rsid[0], base=2):\n            raise ProtocolError(f'HTTP/2: [Type {_type}] invalid format', quiet=True)\n\n        http = dict(\n            length=_tlen,\n            type=_HTTP_TYPE.get(_type),\n            sid=int(_rsid[1:], base=2),\n            packet=self._read_packet(_tlen),\n        )\n\n        if http['type'] is None:\n            raise ProtocolError(f'HTTP/2: [Type {_type}] invalid format', quiet=True)\n\n        if http['type'] in ('SETTINGS', 'PING') and http['sid'] != 0:\n            raise ProtocolError(f'HTTP/2: [Type {_type}] invalid format', quiet=True)\n\n        _http = _HTTP_FUNC[_type](self, _tlen, _type, _flag)\n        http.update(_http)\n\n        return http"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread HTTP packet with unsigned type.", "response": "def _read_http_none(self, size, kind, flag):\n        \"\"\"Read HTTP packet with unsigned type.\"\"\"\n        if any((int(bit, base=2) for bit in flag)):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        data = dict(\n            flags=None,\n            payload=self._read_fileng(size - 9) or None,\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading HTTP 2 DATA frames.", "response": "def _read_http_data(self, size, kind, flag):\n        \"\"\"Read HTTP/2 DATA frames.\n\n        Structure of HTTP/2 DATA frame [RFC 7540]:\n            +-----------------------------------------------+\n            |                 Length (24)                   |\n            +---------------+---------------+---------------+\n            |   Type (8)    |   Flags (8)   |\n            +-+-------------+---------------+-------------------------------+\n            |R|                 Stream Identifier (31)                      |\n            +---------------+-----------------------------------------------+\n            |Pad Length? (8)|\n            +---------------+-----------------------------------------------+\n            |                            Data (*)                         ...\n            +---------------------------------------------------------------+\n            |                           Padding (*)                       ...\n            +---------------------------------------------------------------+\n\n            Octets      Bits        Name                    Description\n              0           0     http.length             Length\n              3          24     http.type               Type (0)\n              4          32     http.flags              Flags\n              5          40     -                       Reserved\n              5          41     http.sid                Stream Identifier\n              9          72     http.pad_len            Pad Length (Optional)\n              10         80     http.data               Data\n              ?           ?     -                       Padding (Optional)\n\n        \"\"\"\n        _plen = 0\n        _flag = dict(\n            END_STREAM=False,   # bit 0\n            PADDED=False,       # bit 3\n        )\n        for index, bit in enumerate(flag):\n            if index == 0 and bit:\n                _flag['END_STREAM'] = True\n            elif index == 3 and bit:\n                _flag['PADDED'] = True\n                _plen = self._read_unpack(1)\n            elif bit:\n                raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n            else:\n                continue\n\n        if _plen > size - 10:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        if _flag['PADDED']:\n            _dlen = size - _plen - 1\n        else:\n            _dlen = size - _plen\n        if _dlen < 0:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _data = self._read_fileng(_dlen)\n\n        padding = self._read_binary(_plen)\n        if any((int(bit, base=2) for bit in padding)):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        data = dict(\n            flags=_flag,\n            data=_data,\n        )\n        if _flag['PADDED']:\n            data['ped_len'] = _plen\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading HTTP headers from the HTTP response.", "response": "def _read_http_headers(self, size, kind, flag):\n        \"\"\"Read HTTP/2 HEADERS frames.\n\n        Structure of HTTP/2 HEADERS frame [RFC 7540]:\n            +-----------------------------------------------+\n            |                 Length (24)                   |\n            +---------------+---------------+---------------+\n            |   Type (8)    |   Flags (8)   |\n            +-+-------------+---------------+-------------------------------+\n            |R|                 Stream Identifier (31)                      |\n            +---------------+-----------------------------------------------+\n            |Pad Length? (8)|\n            +-+-------------+-----------------------------------------------+\n            |E|                 Stream Dependency? (31)                     |\n            +-+-------------+-----------------------------------------------+\n            |  Weight? (8)  |\n            +-+-------------+-----------------------------------------------+\n            |                   Header Block Fragment (*)                 ...\n            +---------------------------------------------------------------+\n            |                           Padding (*)                       ...\n            +---------------------------------------------------------------+\n\n            Octets      Bits        Name                    Description\n              0           0     http.length             Length\n              3          24     http.type               Type (1)\n              4          32     http.flags              Flags\n              5          40     -                       Reserved\n              5          41     http.sid                Stream Identifier\n              9          72     http.pad_len            Pad Length (Optional)\n              10         80     http.exclusive          Exclusive Flag\n              10         81     http.deps               Stream Dependency (Optional)\n              14        112     http.weight             Weight (Optional)\n              15        120     http.frag               Header Block Fragment\n              ?           ?     -                       Padding (Optional)\n\n        \"\"\"\n        _plen = 0\n        _elen = 0\n        _flag = dict(\n            END_STREAM=False,       # bit 0\n            END_HEADERS=False,      # bit 2\n            PADDED=False,           # bit 3\n            PRIORITY=False,         # bit 5\n        )\n        for index, bit in enumerate(flag):\n            if index == 0 and bit:\n                _flag['END_STREAM'] = True\n            elif index == 2 and bit:\n                _flag['END_HEADERS'] = True\n            elif index == 3 and bit:\n                _flag['PADDED'] = True\n                _plen = self._read_unpack(1)\n            elif index == 5 and bit:\n                _flag['PRIORITY'] = True\n                _edep = self._read_binary(4)\n                _wght = self._read_unpack(1)\n                _elen = 5\n            elif bit:\n                raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n            else:\n                continue\n\n        if _flag['PADDED']:\n            _dlen = size - _plen - _elen - 1\n        else:\n            _dlen = size - _plen - _elen\n        if _dlen < 0:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _frag = self._read_fileng(_dlen) or None\n\n        padding = self._read_binary(_plen)\n        if any((int(bit, base=2) for bit in padding)):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        data = dict(\n            flags=_flag,\n            frag=_frag,\n        )\n        if _flag['PADDED']:\n            data['ped_len'] = _plen\n        if _flag['PRIORITY']:\n            data['exclusive'] = True if int(_edep[0], base=2) else False\n            data['deps'] = int(_edep[1:], base=2)\n            data['weight'] = _wght + 1\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_http_priority(self, size, kind, flag):\n        if size != 9:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n        if any((int(bit, base=2) for bit in flag)):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _edep = self._read_binary(4)\n        _wght = self._read_unpack(1)\n\n        data = dict(\n            flags=None,\n            exclusive=True if int(_edep[0], base=2) else False,\n            deps=int(_edep[1:], base=2),\n            weight=_wght + 1,\n        )\n\n        return data", "response": "Read HTTP/2 PRIORITY frames."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_http_rst_stream(self, size, kind, flag):\n        if size != 8:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n        if any((int(bit, base=2) for bit in flag)):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _code = self._read_unpack(4)\n\n        data = dict(\n            flags=None,\n            error=_ERROR_CODE.get(_code, _code),\n        )\n\n        return data", "response": "Read HTTP 2 RST_STREAM frames."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_http_settings(self, size, kind, flag):\n        if size % 5 != 0:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _flag = dict(\n            ACK=False,      # bit 0\n        )\n        for index, bit in enumerate(flag):\n            if index == 0 and bit:\n                _flag['ACK'] = True\n            elif bit:\n                raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n            else:\n                continue\n\n        if _flag['ACK'] and size:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _para = dict()\n        counter = 0\n        while counter < size:\n            _stid = self._read_unpack(1)\n            _pval = self._read_unpack(4)\n            _pkey = _PARA_NAME.get(_stid, 'Unsigned')\n            if _pkey in _para:\n                if isinstance(_para[_pkey], tuple):\n                    _para[_pkey] += (_pval,)\n                else:\n                    _para[_pkey] = (_para[_pkey], _pval)\n            else:\n                _para[_pkey] = _pval\n\n        data = dict(\n            flags=_flag,\n        )\n        data.update(_para)\n\n        return data", "response": "Read HTTP 2 SETTINGS frames."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_http_push_promise(self, size, kind, flag):\n        if size < 4:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _plen = 0\n        _flag = dict(\n            END_HEADERS=False,      # bit 2\n            PADDED=False,           # bit 3\n        )\n        for index, bit in enumerate(flag):\n            if index == 2 and bit:\n                _flag['END_HEADERS'] = True\n            elif index == 3 and bit:\n                _flag['PADDED'] = True\n                _plen = self._read_unpack(1)\n            elif bit:\n                raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n            else:\n                continue\n\n        if _flag['PADDED']:\n            _dlen = size - _plen - 5\n        else:\n            _dlen = size - _plen - 4\n        if _dlen < 0:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _rpid = self._read_binary(4)\n        _frag = self._read_fileng(_dlen) or None\n\n        if int(_rpid[0], base=2):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        padding = self._read_binary(_plen)\n        if any((int(bit, base=2) for bit in padding)):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        data = dict(\n            flags=_flag,\n            pid=int(_rpid[1:], base=2),\n            frag=_frag,\n        )\n        if _flag['PADDED']:\n            data['ped_len'] = _plen\n\n        return data", "response": "Read HTTP - 2 PUSH_PROMISE frames."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread HTTP 2 PING frames.", "response": "def _read_http_ping(self, size, kind, flag):\n        \"\"\"Read HTTP/2 PING frames.\n\n        Structure of HTTP/2 PING frame [RFC 7540]:\n            +-----------------------------------------------+\n            |                 Length (24)                   |\n            +---------------+---------------+---------------+\n            |   Type (8)    |   Flags (8)   |\n            +-+-------------+---------------+-------------------------------+\n            |R|                 Stream Identifier (31)                      |\n            +---------------------------------------------------------------+\n            |                                                               |\n            |                      Opaque Data (64)                         |\n            |                                                               |\n            +---------------------------------------------------------------+\n\n            Octets      Bits        Name                    Description\n              0           0     http.length             Length\n              3          24     http.type               Type (2)\n              4          32     http.flags              Flags\n              5          40     -                       Reserved\n              5          41     http.sid                Stream Identifier\n              9          72     http.data               Opaque Data\n\n        \"\"\"\n        if size != 8:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _flag = dict(\n            ACK=False,      # bit 0\n        )\n        for index, bit in enumerate(flag):\n            if index == 0 and bit:\n                _flag['ACK'] = True\n            elif bit:\n                raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n            else:\n                continue\n\n        _data = self._read_fileng(8)\n\n        data = dict(\n            flags=_flag,\n            data=_data,\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading HTTP 2 GOAWAY frames.", "response": "def _read_http_goaway(self, size, kind, flag):\n        \"\"\"Read HTTP/2 GOAWAY frames.\n\n        Structure of HTTP/2 GOAWAY frame [RFC 7540]:\n            +-----------------------------------------------+\n            |                 Length (24)                   |\n            +---------------+---------------+---------------+\n            |   Type (8)    |   Flags (8)   |\n            +-+-------------+---------------+-------------------------------+\n            |R|                 Stream Identifier (31)                      |\n            +-+-------------+---------------+-------------------------------+\n            |R|                  Last-Stream-ID (31)                        |\n            +-+-------------------------------------------------------------+\n            |                      Error Code (32)                          |\n            +---------------------------------------------------------------+\n            |                  Additional Debug Data (*)                    |\n            +---------------------------------------------------------------+\n\n            Octets      Bits        Name                    Description\n              0           0     http.length             Length\n              3          24     http.type               Type (2)\n              4          32     http.flags              Flags\n              5          40     -                       Reserved\n              5          41     http.sid                Stream Identifier\n              9          72     -                       Reserved\n              9          73     http.last_sid           Last Stream ID\n              13        104     http.error              Error Code\n              17        136     http.data               Additional Debug Data (Optional)\n\n        \"\"\"\n        _dlen = size - 8\n        if _dlen < 0:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n        if any((int(bit, base=2) for bit in flag)):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _rsid = self._read_binary(4)\n        _code = self._read_unpack(4)\n        _data = self._read_fileng(_dlen) or None\n\n        if int(_rsid[0], base=2):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        data = dict(\n            flags=None,\n            last_sid=int(_rsid[1:], base=2),\n            error=_ERROR_CODE.get(_code, _code),\n            data=_data,\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_http_window_update(self, size, kind, flag):\n        if size != 4:\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n        if any((int(bit, base=2) for bit in flag)):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        _size = self._read_binary(4)\n\n        if int(_size[0], base=2):\n            raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n\n        data = dict(\n            flags=None,\n            window=int(_size[1:], base=2),\n        )\n\n        return data", "response": "Read HTTP 2 WINDOW_UPDATE frames."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_http_continuation(self, size, kind, flag):\n        _flag = dict(\n            END_HEADERS=False,      # bit 2\n        )\n        for index, bit in enumerate(flag):\n            if index == 2 and bit:\n                _flag['END_HEADERS'] = True\n            elif bit:\n                raise ProtocolError(f'HTTP/2: [Type {kind}] invalid format', quiet=True)\n            else:\n                continue\n\n        _frag = self._read_fileng(size) or None\n\n        data = dict(\n            flags=_flag,\n            frag=_frag,\n        )\n\n        return data", "response": "Read HTTP Continuation HTTP message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return OptionNumber(key)\n        if key not in OptionNumber._member_map_:\n            extend_enum(OptionNumber, key, default)\n        return OptionNumber[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting next layer from the file.", "response": "def _import_next_layer(self, proto, length):\n        \"\"\"Import next layer extractor.\n\n        Positional arguments:\n            * proto -- str, next layer protocol name\n            * length -- int, valid (not padding) length\n\n        Returns:\n            * bool -- flag if extraction of next layer succeeded\n            * Info -- info of next layer\n            * ProtoChain -- protocol chain of next layer\n            * str -- alias of next layer\n\n        \"\"\"\n        if self._exproto == 'null' and self._exlayer == 'None':\n            from pcapkit.protocols.raw import Raw as NextLayer\n        else:\n            from pcapkit.foundation.analysis import analyse as NextLayer\n        # from pcapkit.foundation.analysis import analyse as NextLayer\n        if length == 0:\n            next_ = NoPayload()\n        elif self._onerror:\n            next_ = beholder_ng(NextLayer)(self._file, length, _termination=self._sigterm)\n        else:\n            next_ = NextLayer(self._file, length, _termination=self._sigterm)\n        return next_"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return QS_Function(key)\n        if key not in QS_Function._member_map_:\n            extend_enum(QS_Function, key, default)\n        return QS_Function[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading global header and write to plist file.", "response": "def record_header(self):\n        \"\"\"Read global header.\n\n        - Extract global header.\n        - Make Info object out of header properties.\n        - Append Info.\n        - Write plist file.\n\n        \"\"\"\n        self._gbhdr = Header(self._ifile)\n        self._vinfo = self._gbhdr.version\n        self._dlink = self._gbhdr.protocol\n        self._nnsec = self._gbhdr.nanosecond\n\n        if self._trace is not NotImplemented:\n            self._trace._endian = self._gbhdr.byteorder\n            self._trace._nnsecd = self._gbhdr.nanosecond\n\n        if not self._flag_q:\n            if self._flag_f:\n                ofile = self._ofile(f'{self._ofnm}/Global Header.{self._fext}')\n                ofile(self._gbhdr.info, name='Global Header')\n                self._type = ofile.kind\n            else:\n                self._ofile(self._gbhdr.info, name='Global Header')\n                self._type = self._ofile.kind"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _cleanup(self):\n        self._expkg = None\n        self._extmp = None\n        self._flag_e = True\n        self._ifile.close()", "response": "Cleanup after extraction & analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_frame(self):\n        if self._exeng == 'scapy':\n            return self._scapy_read_frame()\n        elif self._exeng == 'dpkt':\n            return self._dpkt_read_frame()\n        elif self._exeng == 'pyshark':\n            return self._pyshark_read_frame()\n        else:\n            return self._default_read_frame()", "response": "Reads the next available message from the specified log file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _default_read_frame(self, *, frame=None, mpkit=None):\n        from pcapkit.toolkit.default import (ipv4_reassembly, ipv6_reassembly,\n                                             tcp_reassembly, tcp_traceflow)\n\n        # read frame header\n        if not self._flag_m:\n            frame = Frame(self._ifile, num=self._frnum+1, proto=self._dlink,\n                          layer=self._exlyr, protocol=self._exptl, nanosecond=self._nnsec)\n            self._frnum += 1\n\n        # verbose output\n        if self._flag_v:\n            print(f' - Frame {self._frnum:>3d}: {frame.protochain}')\n\n        # write plist\n        frnum = f'Frame {self._frnum}'\n        if not self._flag_q:\n            if self._flag_f:\n                ofile = self._ofile(f'{self._ofnm}/{frnum}.{self._fext}')\n                ofile(frame.info, name=frnum)\n            else:\n                self._ofile(frame.info, name=frnum)\n\n        # record fragments\n        if self._ipv4:\n            flag, data = ipv4_reassembly(frame)\n            if flag:\n                self._reasm[0](data)  # pylint: disable=E1102\n        if self._ipv6:\n            flag, data = ipv6_reassembly(frame)\n            if flag:\n                self._reasm[1](data)  # pylint: disable=E1102\n        if self._tcp:\n            flag, data = tcp_reassembly(frame)\n            if flag:\n                self._reasm[2](data)  # pylint: disable=E1102\n\n        # trace flows\n        if self._flag_t:\n            flag, data = tcp_traceflow(frame, data_link=self._dlink)\n            if flag:\n                self._trace(data)\n\n        # record frames\n        if self._exeng == 'pipeline':\n            if self._flag_d:\n                # frame._file = NotImplemented\n                mpkit.frames[self._frnum] = frame\n                # print(self._frnum, 'stored')\n            mpkit.current += 1\n        elif self._exeng == 'server':\n            # record frames\n            if self._flag_d:\n                # frame._file = NotImplemented\n                self._frame.append(frame)\n                # print(self._frnum, 'stored')\n            self._frnum += 1\n        else:\n            if self._flag_d:\n                self._frame.append(frame)\n            self._proto = frame.protochain.chain\n\n        # return frame record\n        return frame", "response": "Read a single frame and return the corresponding entry in the dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run_scapy(self, scapy_all):\n        # if not self._flag_a:\n        #     self._flag_a = True\n        #     warnings.warn(f\"'Extractor(engine=scapy)' object is not iterable; \"\n        #                     \"so 'auto=False' will be ignored\", AttributeWarning, stacklevel=stacklevel())\n\n        if self._exlyr != 'None' or self._exptl != 'null':\n            warnings.warn(\"'Extractor(engine=scapy)' does not support protocol and layer threshold; \"\n                          f\"'layer={self._exlyr}' and 'protocol={self._exptl}' ignored\",\n                          AttributeWarning, stacklevel=stacklevel())\n\n        # extract & analyse file\n        self._expkg = scapy_all\n        self._extmp = iter(scapy_all.sniff(offline=self._ifnm))\n\n        # start iteration\n        self.record_frames()", "response": "Run the scapy. all. sniff function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading frames with Scapy.", "response": "def _scapy_read_frame(self):\n        \"\"\"Read frames with Scapy.\"\"\"\n        from pcapkit.toolkit.scapy import (ipv4_reassembly, ipv6_reassembly,\n                                           packet2chain, packet2dict, tcp_reassembly,\n                                           tcp_traceflow)\n\n        # fetch Scapy packet\n        packet = next(self._extmp)\n\n        # verbose output\n        self._frnum += 1\n        self._proto = packet2chain(packet)\n        if self._flag_v:\n            print(f' - Frame {self._frnum:>3d}: {self._proto}')\n\n        # write plist\n        frnum = f'Frame {self._frnum}'\n        if not self._flag_q:\n            info = packet2dict(packet)\n            if self._flag_f:\n                ofile = self._ofile(f'{self._ofnm}/{frnum}.{self._fext}')\n                ofile(info, name=frnum)\n            else:\n                self._ofile(info, name=frnum)\n\n        # record frames\n        if self._flag_d:\n            # setattr(packet, 'packet2dict', packet2dict)\n            # setattr(packet, 'packet2chain', packet2chain)\n            self._frame.append(packet)\n\n        # record fragments\n        if self._ipv4:\n            flag, data = ipv4_reassembly(packet, count=self._frnum)\n            if flag:\n                self._reasm[0](data)  # pylint: disable=E1102\n        if self._ipv6:\n            flag, data = ipv6_reassembly(packet, count=self._frnum)\n            if flag:\n                self._reasm[1](data)  # pylint: disable=E1102\n        if self._tcp:\n            flag, data = tcp_reassembly(packet, count=self._frnum)\n            if flag:\n                self._reasm[2](data)  # pylint: disable=E1102\n\n        # trace flows\n        if self._flag_t:\n            flag, data = tcp_traceflow(packet, count=self._frnum)\n            if flag:\n                self._trace(data)\n\n        return packet"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall dpkt. pcap. Reader to extract PCAP files.", "response": "def _run_dpkt(self, dpkt):\n        \"\"\"Call dpkt.pcap.Reader to extract PCAP files.\"\"\"\n        # if not self._flag_a:\n        #     self._flag_a = True\n        #     warnings.warn(f\"'Extractor(engine=dpkt)' object is not iterable; \"\n        #                     \"so 'auto=False' will be ignored\", AttributeWarning, stacklevel=stacklevel())\n\n        if self._exlyr != 'None' or self._exptl != 'null':\n            warnings.warn(\"'Extractor(engine=dpkt)' does not support protocol and layer threshold; \"\n                          f\"'layer={self._exlyr}' and 'protocol={self._exptl}' ignored\",\n                          AttributeWarning, stacklevel=stacklevel())\n\n        # extract global header\n        self.record_header()\n        self._ifile.seek(0, os.SEEK_SET)\n\n        # extract & analyse file\n        self._expkg = dpkt\n        self._extmp = iter(dpkt.pcap.Reader(self._ifile))\n\n        # start iteration\n        self.record_frames()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dpkt_read_frame(self):\n        from pcapkit.toolkit.dpkt import (ipv4_reassembly, ipv6_reassembly,\n                                          packet2chain, packet2dict, tcp_reassembly,\n                                          tcp_traceflow)\n\n        # fetch DPKT packet\n        timestamp, packet = next(self._extmp)\n\n        # extract packet\n        if self._dlink.value == 1:\n            packet = self._expkg.ethernet.Ethernet(packet)\n        elif self._dlink.value == 228:\n            packet = self._expkg.ip.IP(packet)\n        elif self._dlink.value == 229:\n            packet = self._expkg.ip6.IP6(packet)\n        else:\n            warnings.warn('unrecognised link layer protocol; all analysis functions ignored',\n                          DPKTWarning, stacklevel=stacklevel())\n            self._frnum += 1\n            if self._flag_d:\n                self._frame.append(packet)\n            return packet\n\n        # verbose output\n        self._frnum += 1\n        self._proto = packet2chain(packet)\n        if self._flag_v:\n            print(f' - Frame {self._frnum:>3d}: {self._proto}')\n\n        # write plist\n        frnum = f'Frame {self._frnum}'\n        if not self._flag_q:\n            info = packet2dict(packet, timestamp, data_link=self._dlink)\n            if self._flag_f:\n                ofile = self._ofile(f'{self._ofnm}/{frnum}.{self._fext}')\n                ofile(info, name=frnum)\n            else:\n                self._ofile(info, name=frnum)\n\n        # record frames\n        if self._flag_d:\n            setattr(packet, 'packet2dict', packet2dict)\n            setattr(packet, 'packet2chain', packet2chain)\n            self._frame.append(packet)\n\n        # record fragments\n        if self._ipv4:\n            flag, data = ipv4_reassembly(packet, count=self._frnum)\n            if flag:\n                self._reasm[0](data)  # pylint: disable=E1102\n        if self._ipv6:\n            flag, data = ipv6_reassembly(packet, count=self._frnum)\n            if flag:\n                self._reasm[1](data)  # pylint: disable=E1102\n        if self._tcp:\n            flag, data = tcp_reassembly(packet, count=self._frnum)\n            if flag:\n                self._reasm[2](data)  # pylint: disable=E1102\n\n        # trace flows\n        if self._flag_t:\n            flag, data = tcp_traceflow(packet, timestamp, data_link=self._dlink, count=self._frnum)\n            if flag:\n                self._trace(data)\n\n        return packet", "response": "Read a single DPKT packet and store it in the internal list of available DPKT objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall pyshark. FileCapture to extract PCAP files.", "response": "def _run_pyshark(self, pyshark):\n        \"\"\"Call pyshark.FileCapture to extract PCAP files.\"\"\"\n        # if not self._flag_a:\n        #     self._flag_a = True\n        #     warnings.warn(f\"'Extractor(engine=pyshark)' object is not iterable; \"\n        #                     \"so 'auto=False' will be ignored\", AttributeWarning, stacklevel=stacklevel())\n\n        if self._exlyr != 'None' or self._exptl != 'null':\n            warnings.warn(\"'Extractor(engine=pyshark)' does not support protocol and layer threshold; \"\n                          f\"'layer={self._exlyr}' and 'protocol={self._exptl}' ignored\",\n                          AttributeWarning, stacklevel=stacklevel())\n\n        if (self._ipv4 or self._ipv6 or self._tcp):\n            self._ipv4 = self._ipv6 = self._tcp = False\n            self._reasm = [None] * 3\n            warnings.warn(\"'Extractor(engine=pyshark)' object dose not support reassembly; \"\n                          f\"so 'ipv4={self._ipv4}', 'ipv6={self._ipv6}' and 'tcp={self._tcp}' will be ignored\",\n                          AttributeWarning, stacklevel=stacklevel())\n\n        # extract & analyse file\n        self._expkg = pyshark\n        self._extmp = iter(pyshark.FileCapture(self._ifnm, keep_packets=False))\n\n        # start iteration\n        self.record_frames()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a single frame from the PyShark packet.", "response": "def _pyshark_read_frame(self):\n        \"\"\"Read frames.\"\"\"\n        from pcapkit.toolkit.pyshark import packet2dict, tcp_traceflow\n\n        # fetch PyShark packet\n        packet = next(self._extmp)\n\n        # def _pyshark_packet2chain(packet):\n        #     \"\"\"Fetch PyShark packet protocol chain.\"\"\"\n        #     return ':'.join(map(lambda layer: layer.layer_name.upper(), packet.layers))\n\n        # verbose output\n        self._frnum = int(packet.number)\n        self._proto = packet.frame_info.protocols\n        if self._flag_v:\n            print(f' - Frame {self._frnum:>3d}: {self._proto}')\n\n        # write plist\n        frnum = f'Frame {self._frnum}'\n        if not self._flag_q:\n            info = packet2dict(packet)\n            if self._flag_f:\n                ofile = self._ofile(f'{self._ofnm}/{frnum}.{self._fext}')\n                ofile(info, name=frnum)\n            else:\n                self._ofile(info, name=frnum)\n\n        # record frames\n        if self._flag_d:\n            setattr(packet, 'packet2dict', packet2dict)\n            self._frame.append(packet)\n\n        # trace flows\n        if self._flag_t:\n            flag, data = tcp_traceflow(packet)\n            if flag:\n                self._trace(data)\n\n        return packet"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing multiprocessing to extract PCAP files.", "response": "def _run_pipeline(self, multiprocessing):\n        \"\"\"Use pipeline multiprocessing to extract PCAP files.\"\"\"\n        if not self._flag_m:\n            raise UnsupportedCall(f\"Extractor(engine={self._exeng})' has no attribute '_run_pipline'\")\n\n        if not self._flag_q:\n            self._flag_q = True\n            warnings.warn(\"'Extractor(engine=pipeline)' does not support output; \"\n                          f\"'fout={self._ofnm}' ignored\", AttributeWarning, stacklevel=stacklevel())\n\n        self._frnum = 1                                                 # frame number (revised)\n        self._expkg = multiprocessing                                   # multiprocessing module\n        self._mpprc = list()                                            # multiprocessing process list\n        self._mpfdp = collections.defaultdict(multiprocessing.Queue)    # multiprocessing file pointer\n\n        self._mpmng = multiprocessing.Manager()                         # multiprocessing manager\n        self._mpkit = self._mpmng.Namespace()                           # multiprocessing work kit\n\n        self._mpkit.counter = 0                                         # work count (on duty)\n        self._mpkit.pool = 1                                            # work pool (ready)\n        self._mpkit.current = 1                                         # current frame number\n        self._mpkit.eof = False                                         # EOF flag\n        self._mpkit.frames = dict()                                     # frame storage\n        self._mpkit.trace = self._trace                                 # flow tracer\n        self._mpkit.reassembly = copy.deepcopy(self._reasm)             # reassembly buffers\n\n        # preparation\n        self.record_header()\n        self._mpfdp[0].put(self._gbhdr.length)\n\n        # extraction\n        while True:\n            # check EOF\n            if self._mpkit.eof:\n                self._update_eof()\n                break\n\n            # check counter\n            if self._mpkit.pool and self._mpkit.counter < CPU_CNT:\n                # update file offset\n                self._ifile.seek(self._mpfdp.pop(self._frnum-1).get(), os.SEEK_SET)\n\n                # create worker\n                # print(self._frnum, 'start')\n                proc = multiprocessing.Process(\n                    target=self._pipeline_read_frame,\n                    kwargs={'mpkit': self._mpkit, 'mpfdp': self._mpfdp[self._frnum]}\n                )\n\n                # update status\n                self._mpkit.pool -= 1\n                self._mpkit.counter += 1\n\n                # start and record\n                proc.start()\n                self._frnum += 1\n                self._mpprc.append(proc)\n\n            # check buffer\n            if len(self._mpprc) >= CPU_CNT:\n                [proc.join() for proc in self._mpprc[:-4]]\n                del self._mpprc[:-4]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _pipeline_read_frame(self, *, mpfdp, mpkit):\n        # check EOF\n        if self._flag_e:\n            raise EOFError\n\n        def _analyse_frame(*, frame, mpkit):\n            \"\"\"Analyse frame.\"\"\"\n            # wait until ready\n            while mpkit.current != self._frnum:\n                time.sleep(random.randint(0, datetime.datetime.now().second) // 600)\n\n            # analysis and storage\n            # print(self._frnum, 'get')\n            self._trace = mpkit.trace\n            self._reasm = mpkit.reassembly\n            self._default_read_frame(frame=frame, mpkit=mpkit)\n            # print(self._frnum, 'analysed')\n            mpkit.trace = copy.deepcopy(self._trace)\n            mpkit.reassembly = copy.deepcopy(self._reasm)\n            # print(self._frnum, 'put')\n\n        # extract frame\n        try:\n            # extraction\n            frame = Frame(self._ifile, num=self._frnum, proto=self._dlink, layer=self._exlyr,\n                          protocol=self._exptl, nanosecond=self._nnsec, mpkit=mpkit, mpfdp=mpfdp)\n            # analysis\n            _analyse_frame(frame=frame, mpkit=mpkit)\n        except EOFError:\n            mpkit.eof = True\n        finally:\n            mpkit.counter -= 1\n            self._ifile.close()", "response": "Read a single frame from the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run_server(self, multiprocessing):\n        if not self._flag_m:\n            raise UnsupportedCall(f\"Extractor(engine={self._exeng})' has no attribute '_run_server'\")\n\n        if not self._flag_q:\n            self._flag_q = True\n            warnings.warn(\"'Extractor(engine=pipeline)' does not support output; \"\n                          f\"'fout={self._ofnm}' ignored\", AttributeWarning, stacklevel=stacklevel())\n\n        self._frnum = 1                                                 # frame number (revised)\n        self._expkg = multiprocessing                                   # multiprocessing module\n        self._mpsvc = NotImplemented                                    # multiprocessing server process\n        self._mpprc = list()                                            # multiprocessing process list\n        self._mpfdp = collections.defaultdict(multiprocessing.Queue)    # multiprocessing file pointer\n\n        self._mpmng = multiprocessing.Manager()                         # multiprocessing manager\n        self._mpbuf = self._mpmng.dict()                                # multiprocessing frame dict\n        self._mpfrm = self._mpmng.list()                                # multiprocessing frame storage\n        self._mprsm = self._mpmng.list()                                # multiprocessing reassembly buffer\n\n        self._mpkit = self._mpmng.Namespace()                           # multiprocessing work kit\n        self._mpkit.counter = 0                                         # work count (on duty)\n        self._mpkit.pool = 1                                            # work pool (ready)\n        self._mpkit.eof = False                                         # EOF flag\n        self._mpkit.trace = None                                        # flow tracer\n\n        # preparation\n        self.record_header()\n        self._mpfdp[0].put(self._gbhdr.length)\n        self._mpsvc = multiprocessing.Process(\n            target=self._server_analyse_frame,\n            kwargs={'mpfrm': self._mpfrm, 'mprsm': self._mprsm, 'mpbuf': self._mpbuf, 'mpkit': self._mpkit}\n        )\n        self._mpsvc.start()\n\n        # extraction\n        while True:\n            # check EOF\n            if self._mpkit.eof:\n                self._update_eof()\n                break\n\n            # check counter\n            if self._mpkit.pool and self._mpkit.counter < CPU_CNT - 1:\n                # update file offset\n                self._ifile.seek(self._mpfdp.pop(self._frnum-1).get(), os.SEEK_SET)\n\n                # create worker\n                # print(self._frnum, 'start')\n                proc = multiprocessing.Process(\n                        target=self._server_extract_frame,\n                        kwargs={'mpkit': self._mpkit, 'mpbuf': self._mpbuf, 'mpfdp': self._mpfdp[self._frnum]}\n                )\n\n                # update status\n                self._mpkit.pool -= 1\n                self._mpkit.counter += 1\n\n                # start and record\n                proc.start()\n                self._frnum += 1\n                self._mpprc.append(proc)\n\n            # check buffer\n            if len(self._mpprc) >= CPU_CNT - 1:\n                [proc.join() for proc in self._mpprc[:-4]]\n                del self._mpprc[:-4]", "response": "Use multiprocessing to extract PCAP files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting a single frame from the server.", "response": "def _server_extract_frame(self, *, mpfdp, mpkit, mpbuf):\n        \"\"\"Extract frame.\"\"\"\n        # check EOF\n        if self._flag_e:\n            raise EOFError\n\n        # extract frame\n        try:\n            frame = Frame(self._ifile, num=self._frnum, proto=self._dlink, layer=self._exlyr,\n                          protocol=self._exptl, nanosecond=self._nnsec, mpkit=mpkit, mpfdp=mpfdp)\n            # frame._file = NotImplemented\n            mpbuf[self._frnum] = frame\n        except EOFError:\n            mpbuf[self._frnum] = EOFError\n            mpkit.eof = True\n        finally:\n            mpkit.counter -= 1\n            self._ifile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nanalysing the next frame from the buffer.", "response": "def _server_analyse_frame(self, *, mpkit, mpfrm, mprsm, mpbuf):\n        \"\"\"Analyse frame.\"\"\"\n        while True:\n            # fetch frame\n            # print(self._frnum, 'trying')\n            frame = mpbuf.pop(self._frnum, None)\n            if frame is EOFError:\n                break\n            if frame is None:\n                continue\n            # print(self._frnum, 'get')\n\n            self._default_read_frame(frame=frame)\n        mpfrm += self._frame\n        mprsm += self._reasm\n        mpkit.trace = copy.deepcopy(self._trace)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return Checksum(key)\n        if key not in Checksum._member_map_:\n            extend_enum(Checksum, key, default)\n        return Checksum[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return Operation(key)\n        if key not in Operation._member_map_:\n            extend_enum(Operation, key, default)\n        return Operation[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_http(self, length):\n        _plen = self._read_binary(3)\n        _type = self._read_unpack(1)\n        _flag = self._read_binary(1)\n        _stid = self._read_binary(4)", "response": "Read Hypertext Transfer Protocol version 2."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nanalyses application layer packets.", "response": "def analyse(file, length=None, *, _termination=False):\n    \"\"\"Analyse application layer packets.\"\"\"\n    seekset = file.tell()\n    if not _termination:\n        # FTP analysis\n        flag, ftp = _analyse_ftp(file, length, seekset=seekset)\n        if flag:\n            return ftp\n\n        # HTTP/1.* analysis\n        flag, http = _analyse_httpv1(file, length, seekset=seekset)\n        if flag:\n            return http\n\n        # NOTE: due to format similarity of HTTP/2 and TLS/SSL, HTTP/2 won't be analysed before TLS/SSL is implemented.\n        # NB: the NOTE above is deprecated, since validations are performed\n\n        # HTTP/2 analysis\n        flag, http = _analyse_httpv2(file, length, seekset=seekset)\n        if flag:\n            return http\n\n        # backup file offset\n        file.seek(seekset, os.SEEK_SET)\n\n    # raw packet analysis\n    return Raw(file, length)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return Routing(key)\n        if key not in Routing._member_map_:\n            extend_enum(Routing, key, default)\n        return Routing[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread next layer protocol type.", "response": "def _read_protos(self, size):\n        \"\"\"Read next layer protocol type.\n\n        Positional arguments:\n            * size  -- int, buffer size\n\n        Returns:\n            * str -- next layer's protocol name\n\n        \"\"\"\n        _byte = self._read_unpack(size)\n        _prot = ETHERTYPE.get(_byte)\n        return _prot"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _import_next_layer(self, proto, length):\n        if length == 0:\n            from pcapkit.protocols.null import NoPayload as Protocol\n        elif self._sigterm:\n            from pcapkit.protocols.raw import Raw as Protocol\n        elif proto == 0x0806:\n            from pcapkit.protocols.link.arp import ARP as Protocol\n        elif proto == 0x8035:\n            from pcapkit.protocols.link.rarp import RARP as Protocol\n        elif proto == 0x8100:\n            from pcapkit.protocols.link.vlan import VLAN as Protocol\n        elif proto == 0x0800:\n            from pcapkit.protocols.internet.ipv4 import IPv4 as Protocol\n        elif proto == 0x86DD:\n            from pcapkit.protocols.internet.ipv6 import IPv6 as Protocol\n        elif proto == 0x8137:\n            from pcapkit.protocols.internet.ipx import IPX as Protocol\n        else:\n            from pcapkit.protocols.raw import Raw as Protocol\n        next_ = Protocol(self._file, length, error=self._onerror,\n                         layer=self._exlayer, protocol=self._exproto)\n        return next_", "response": "Imports next layer from the file and returns the next layer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_fout(fout='./tmp', fmt='pcap'):\n        if fmt == 'pcap':       # output PCAP file\n            from pcapkit.dumpkit import PCAP as output\n        elif fmt == 'plist':    # output PLIST file\n            from dictdumper import PLIST as output\n        elif fmt == 'json':     # output JSON file\n            from dictdumper import JSON as output\n        elif fmt == 'tree':     # output treeview text file\n            from dictdumper import Tree as output\n            fmt = 'txt'\n        elif fmt == 'html':     # output JavaScript file\n            from dictdumper import JavaScript as output\n            fmt = 'js'\n        elif fmt == 'xml':      # output XML file\n            from dictdumper import XML as output\n        else:                   # no output file\n            from pcapkit.dumpkit import NotImplementedIO as output\n            if fmt is not None:\n                warnings.warn(f'Unsupported output format: {fmt}; disabled file output feature',\n                              FormatWarning, stacklevel=stacklevel())\n            return output, ''\n\n        try:\n            pathlib.Path(fout).mkdir(parents=True, exist_ok=True)\n        except FileExistsError as error:\n            if fmt is None:\n                warnings.warn(error.strerror, FileWarning, stacklevel=stacklevel())\n            else:\n                raise FileExists(*error.args) from None\n\n        return output, fmt", "response": "Make a temporary directory for the specified format and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dump(self, packet):\n        # fetch flow label\n        output = self.trace(packet, _check=False, _output=True)\n\n        # dump files\n        output(packet['frame'], name=f\"Frame {packet['index']}\",\n               byteorder=self._endian, nanosecond=self._nnsecd)", "response": "Dump a single frame to output files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrace a single packet and return a dictionary of the most recent version of the packet.", "response": "def trace(self, packet, *, _check=True, _output=False):\n        \"\"\"Trace packets.\n\n        Positional arguments:\n            * packet -- dict, a flow packet\n\n        Keyword arguments:\n            * _check -- bool, flag if run validations\n            * _output -- bool, flag if has formatted dumper\n\n        \"\"\"\n        self._newflg = True\n        if _check:\n            pkt_check(packet)\n        info = Info(packet)\n\n        # Buffer Identifier\n        BUFID = tuple(sorted([str(info.src), str(info.srcport),    # pylint: disable=E1101\n                              str(info.dst), str(info.dstport)]))  # pylint: disable=E1101\n        # SYN = info.syn      # Synchronise Flag (Establishment)\n        # Finish Flag (Termination)\n        FIN = info.fin        # pylint: disable=E1101\n\n        # # when SYN is set, reset buffer of this seesion\n        # if SYN and BUFID in self._buffer:\n        #     temp = self._buffer.pop(BUFID)\n        #     temp['fpout'] = (self._fproot, self._fdpext)\n        #     temp['index'] = tuple(temp['index'])\n        #     self._stream.append(Info(temp))\n\n        # initialise buffer with BUFID\n        if BUFID not in self._buffer:\n            label = f'{info.src}_{info.srcport}-{info.dst}_{info.dstport}-{info.timestamp}'  # pylint: disable=E1101\n            self._buffer[BUFID] = dict(\n                fpout=self._foutio(f'{self._fproot}/{label}.{self._fdpext}',\n                                   protocol=info.protocol),  # pylint: disable=E1101\n                index=list(),\n                label=label,\n            )\n\n        # trace frame record\n        self._buffer[BUFID]['index'].append(info.index)  # pylint: disable=E1101\n        fpout = self._buffer[BUFID]['fpout']\n        label = self._buffer[BUFID]['label']\n\n        # when FIN is set, submit buffer of this session\n        if FIN:\n            buf = self._buffer.pop(BUFID)\n            # fpout, label = buf['fpout'], buf['label']\n            if self._fdpext:\n                buf['fpout'] = f'{self._fproot}/{label}.{self._fdpext}'\n            else:\n                del buf['fpout']\n            buf['index'] = tuple(buf['index'])\n            self._stream.append(Info(buf))\n\n        # return label or output object\n        return fpout if _output else label"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubmit traced TCP flows.", "response": "def submit(self):\n        \"\"\"Submit traced TCP flows.\"\"\"\n        self._newflg = False\n        ret = list()\n        for buf in self._buffer.values():\n            buf = copy.deepcopy(buf)\n            if self._fdpext:\n                buf['fpout'] = f\"{self._fproot}/{buf['label']}.{self._fdpext}\"\n            else:\n                del buf['fpout']\n            buf['index'] = tuple(buf['index'])\n            ret.append(Info(buf))\n        ret += self._stream\n        return tuple(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_ah(self, length, version, extension):\n        if length is None:\n            length = len(self)\n\n        _next = self._read_protos(1)\n        _plen = self._read_unpack(1)\n        _resv = self._read_fileng(2)\n        _scpi = self._read_unpack(4)\n        _dsnf = self._read_unpack(4)\n\n        # ICV length & value\n        _tlen = _plen * 4 - 2\n        _vlen = _tlen - 12\n        _chkv = self._read_fileng(_vlen)\n\n        ah = dict(\n            next=_next,\n            length=_tlen,\n            spi=_scpi,\n            seq=_dsnf,\n            icv=_chkv,\n        )\n\n        if version == 6:\n            _plen = 8 - (_tlen % 8)\n        elif version == 4:\n            _plen = 4 - (_tlen % 4)\n        else:\n            raise VersionError(f'Unknown IP version {version}')\n\n        if _plen:   # explicit padding in need\n            padding = self._read_binary(_plen)\n            if any((int(bit, base=2) for bit in padding)):\n                raise ProtocolError(f'{self.alias}: invalid format')\n\n        length -= ah['length']\n        ah['packet'] = self._read_packet(header=ah['length'], payload=length)\n\n        if extension:\n            self._protos = None\n            return ah\n        return self._decode_next_layer(ah, _next, length)", "response": "Read Authentication Header.\n\n        Structure of AH header [RFC 4302]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            | Next Header   |  Payload Len  |          RESERVED             |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                 Security Parameters Index (SPI)               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                    Sequence Number Field                      |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            +                Integrity Check Value-ICV (variable)           |\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     ah.next                 Next Header\n              1           8     ah.length               Payload Length\n              2          16     -                       Reserved (must be zero)\n              4          32     ah.spi                  Security Parameters Index (SPI)\n              8          64     ah.seq                  Sequence Number Field\n              12         96     ah.icv                  Integrity Check Value (ICV)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_ethernet(self, length):\n        if length is None:\n            length = len(self)\n\n        _dstm = self._read_mac_addr()\n        _srcm = self._read_mac_addr()\n        _type = self._read_protos(2)\n\n        ethernet = dict(\n            dst=_dstm,\n            src=_srcm,\n            type=_type,\n        )\n\n        length -= 14\n        ethernet['packet'] = self._read_packet(header=14, payload=length)\n\n        return self._decode_next_layer(ethernet, _type, length)", "response": "Read an ethernet packet."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return ECDSA_Curve(key)\n        if key not in ECDSA_Curve._member_map_:\n            extend_enum(ECDSA_Curve, key, default)\n        return ECDSA_Curve[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef packet2dict(packet):\n    dict_ = dict()\n    frame = packet.frame_info\n    for field in frame.field_names:\n        dict_[field] = getattr(frame, field)\n\n    tempdict = dict_\n    for layer in packet.layers:\n        tempdict[layer.layer_name.upper()] = dict()\n        tempdict = tempdict[layer.layer_name.upper()]\n        for field in layer.field_names:\n            tempdict[field] = getattr(layer, field)\n\n    return dict_", "response": "Convert PyShark packet into dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntraces packet flow for TCP.", "response": "def tcp_traceflow(packet):\n    \"\"\"Trace packet flow for TCP.\"\"\"\n    if 'TCP' in packet:\n        ip = packet.ip if 'IP' in packet else packet.ipv6\n        tcp = packet.tcp\n        data = dict(\n            protocol=LINKTYPE.get(packet.layers[0].layer_name.upper()),     # data link type from global header\n            index=int(packet.number),                                       # frame number\n            frame=packet2dict(packet),                                      # extracted packet\n            syn=bool(int(tcp.flags_syn)),                                   # TCP synchronise (SYN) flag\n            fin=bool(int(tcp.flags_fin)),                                   # TCP finish (FIN) flag\n            src=ipaddress.ip_address(ip.src),                               # source IP\n            dst=ipaddress.ip_address(ip.dst),                               # destination IP\n            srcport=int(tcp.srcport),                                       # TCP source port\n            dstport=int(tcp.dstport),                                       # TCP destination port\n            timestamp=packet.frame_info.time_epoch,                         # timestamp\n        )\n        return True, data\n    return False, None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reassembly(self, info):\n        BUFID = info.bufid  # Buffer Identifier\n        DSN = info.dsn      # Data Sequence Number\n        ACK = info.ack      # Acknowledgement Number\n        FIN = info.fin      # Finish Flag (Termination)\n        RST = info.rst      # Reset Connection Flag (Termination)\n        SYN = info.syn      # Synchronise Flag (Establishment)\n\n        # when SYN is set, reset buffer of this session\n        if SYN and BUFID in self._buffer:\n            self._dtgram += self.submit(self._buffer[BUFID], bufid=BUFID)\n            del self._buffer[BUFID]\n\n        # initialise buffer with BUFID & ACK\n        if BUFID not in self._buffer:\n            self._buffer[BUFID] = {\n                'hdl': [Info(first=info.len, last=sys.maxsize)],\n                ACK: dict(\n                    ind=[info.num],\n                    isn=info.dsn,\n                    len=info.len,\n                    raw=info.payload,\n                ),\n            }\n        else:\n            # initialise buffer with ACK\n            if ACK not in self._buffer[BUFID]:\n                self._buffer[BUFID][ACK] = dict(\n                    ind=[info.num],\n                    isn=info.dsn,\n                    len=info.len,\n                    raw=info.payload,\n                )\n            else:\n                # append packet index\n                self._buffer[BUFID][ACK]['ind'].append(info.num)\n\n                # record fragment payload\n                ISN = self._buffer[BUFID][ACK]['isn']   # Initial Sequence Number\n                RAW = self._buffer[BUFID][ACK]['raw']   # Raw Payload Data\n                if DSN >= ISN:  # if fragment goes after existing payload\n                    LEN = self._buffer[BUFID][ACK]['len']\n                    GAP = DSN - (ISN + LEN)     # gap length between payloads\n                    if GAP >= 0:    # if fragment goes after existing payload\n                        RAW += bytearray(GAP) + info.payload\n                    else:           # if fragment partially overlaps existing payload\n                        RAW[DSN-ISN:] = info.payload\n                else:           # if fragment exceeds existing payload\n                    LEN = info.len\n                    GAP = ISN - (DSN + LEN)     # gap length between payloads\n                    self._buffer[BUFID][ACK]['isn'] = DSN\n                    if GAP >= 0:    # if fragment exceeds existing payload\n                        RAW = info.payload + bytearray(GAP) + RAW\n                    else:           # if fragment partially overlaps existing payload\n                        RAW = info.payload + RAW[ISN-GAP:]\n                self._buffer[BUFID][ACK]['raw'] = RAW       # update payload datagram\n                self._buffer[BUFID][ACK]['len'] = len(RAW)  # update payload length\n\n            # update hole descriptor list\n            HDL = self._buffer[BUFID]['hdl']\n            for (index, hole) in enumerate(HDL):                            # step one\n                if info.first > hole.last:                                  # step two\n                    continue\n                if info.last < hole.first:                                  # step three\n                    continue\n                del HDL[index]                                              # step four\n                if info.first > hole.first:                                 # step five\n                    new_hole = Info(\n                        first=hole.first,\n                        last=info.first - 1,\n                    )\n                    HDL.insert(index, new_hole)\n                    index += 1\n                if info.last < hole.last and not FIN and not RST:           # step six\n                    new_hole = Info(\n                        first=info.last + 1,\n                        last=hole.last\n                    )\n                    HDL.insert(index, new_hole)\n                break                                                       # step seven\n            self._buffer[BUFID]['hdl'] = HDL                                # update HDL\n\n        # when FIN/RST is set, submit buffer of this session\n        if FIN or RST:\n            self._dtgram += self.submit(self._buffer[BUFID], bufid=BUFID)\n            del self._buffer[BUFID]", "response": "Reassembly the sequence number of the packet in the current session."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsubmit reassembled payload. Positional arguments: * buf -- dict, buffer dict of reassembled packets Keyword arguments: * bufid -- tuple, buffer identifier Returns: * list -- reassembled packets", "response": "def submit(self, buf, *, bufid):\n        \"\"\"Submit reassembled payload.\n\n        Positional arguments:\n            * buf -- dict, buffer dict of reassembled packets\n\n        Keyword arguments:\n            * bufid -- tuple, buffer identifier\n\n        Returns:\n            * list -- reassembled packets\n\n        \"\"\"\n        datagram = []           # reassembled datagram\n        HDL = buf.pop('hdl')    # hole descriptor list (remove from dict)\n\n        # check through every buffer with ACK\n        for (ack, buffer) in buf.items():\n            # if this buffer is not implemented\n            # go through every hole and extract received payload\n            if len(HDL) > 2 and self._strflg:\n                data = []\n                start = stop = 0\n                for hole in HDL:\n                    stop = hole.first\n                    byte = buffer['raw'][start:stop]\n                    start = hole.last\n                    if byte:    # strip empty payload\n                        data.append(byte)\n                byte = buffer['raw'][start:]\n                if byte:    # strip empty payload\n                    data.append(byte)\n                if data:    # strip empty buffer\n                    packet = Info(\n                        NotImplemented=True,\n                        id=Info(\n                            src=(bufid[0], bufid[2]),\n                            dst=(bufid[1], bufid[3]),\n                            ack=ack,\n                        ),\n                        index=tuple(buffer['ind']),\n                        payload=tuple(data) or None,\n                        packets=tuple(analyse(io.BytesIO(frag), len(frag)) for frag in data),\n                    )\n                    datagram.append(packet)\n            # if this buffer is implemented\n            # export payload data & convert into bytes\n            else:\n                data = buffer['raw']\n                if data:    # strip empty buffer\n                    packet = Info(\n                        NotImplemented=False,\n                        id=Info(\n                            src=(bufid[0], bufid[2]),\n                            dst=(bufid[1], bufid[3]),\n                            ack=ack,\n                        ),\n                        index=tuple(buffer['ind']),\n                        payload=bytes(data) or None,\n                        packets=(analyse(io.BytesIO(data), len(data)),),\n                    )\n                    datagram.append(packet)\n        return datagram"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef packet2chain(packet):\n    if scapy_all is None:\n        raise ModuleNotFound(\"No module named 'scapy'\", name='scapy')\n    chain = [packet.name]\n    payload = packet.payload\n    while not isinstance(payload, scapy_all.packet.NoPayload):\n        chain.append(payload.name)\n        payload = payload.payload\n    return ':'.join(chain)", "response": "Fetch Scapy packet protocol chain."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert Scapy packet into dict.", "response": "def packet2dict(packet, *, count=NotImplemented):\n    \"\"\"Convert Scapy packet into dict.\"\"\"\n    if scapy_all is None:\n        raise ModuleNotFound(\"No module named 'scapy'\", name='scapy')\n\n    def wrapper(packet):\n        dict_ = packet.fields\n        payload = packet.payload\n        if not isinstance(payload, scapy_all.packet.NoPayload):\n            dict_[payload.name] = wrapper(payload)\n        return dict_\n\n    return {\n        'packet': bytes(packet),\n        packet.name: wrapper(packet),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ipv4_reassembly(packet, *, count=NotImplemented):\n    if 'IP' in packet:\n        ipv4 = packet['IP']\n        if ipv4.flags.DF:       # dismiss not fragmented packet\n            return False, None\n        data = dict(\n            bufid=(\n                ipaddress.ip_address(ipv4.src),         # source IP address\n                ipaddress.ip_address(ipv4.dst),         # destination IP address\n                ipv4.id,                                # identification\n                TP_PROTO.get(ipv4.proto).name,          # payload protocol type\n            ),\n            num=count,                                  # original packet range number\n            fo=ipv4.frag,                               # fragment offset\n            ihl=ipv4.ihl,                               # internet header length\n            mf=bool(ipv4.flags.MF),                     # more fragment flag\n            tl=ipv4.len,                                # total length, header includes\n            header=bytearray(ipv4.raw_packet_cache),    # raw bytearray type header\n            payload=bytearray(bytes(ipv4.payload)),     # raw bytearray type payload\n        )\n        return True, data\n    return False, None", "response": "Make data for IPv4 reassembly."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking data for IPv6 reassembly.", "response": "def ipv6_reassembly(packet, *, count=NotImplemented):\n    \"\"\"Make data for IPv6 reassembly.\"\"\"\n    if scapy_all is None:\n        raise ModuleNotFound(\"No module named 'scapy'\", name='scapy')\n    if 'IPv6' in packet:\n        ipv6 = packet['IPv6']\n        if scapy_all.IPv6ExtHdrFragment not in ipv6:  # pylint: disable=E1101\n            return False, None                      # dismiss not fragmented packet\n        ipv6_frag = ipv6['IPv6ExtHdrFragment']\n        data = dict(\n            bufid=(\n                ipaddress.ip_address(ipv6.src),                 # source IP address\n                ipaddress.ip_address(ipv6.dst),                 # destination IP address\n                ipv6.fl,                                        # label\n                TP_PROTO.get(ipv6_frag.nh).name,                # next header field in IPv6 Fragment Header\n            ),\n            num=count,                                          # original packet range number\n            fo=ipv6_frag.offset,                                # fragment offset\n            ihl=len(ipv6) - len(ipv6_frag),                     # header length, only headers before IPv6-Frag\n            mf=bool(ipv6_frag.m),                               # more fragment flag\n            tl=len(ipv6),                                       # total length, header includes\n            header=bytearray(bytes(ipv6)[:-len(ipv6_frag)]),    # raw bytearray type header before IPv6-Frag\n            payload=bytearray(bytes(ipv6_frag.payload)),        # raw bytearray type payload after IPv6-Frag\n        )\n        return True, data\n    return False, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstores data for TCP reassembly.", "response": "def tcp_reassembly(packet, *, count=NotImplemented):\n    \"\"\"Store data for TCP reassembly.\"\"\"\n    if 'TCP' in packet:\n        ip = packet['IP'] if 'IP' in packet else packet['IPv6']\n        tcp = packet['TCP']\n        data = dict(\n            bufid=(\n                ipaddress.ip_address(ip.src),       # source IP address\n                ipaddress.ip_address(ip.dst),       # destination IP address\n                tcp.sport,                          # source port\n                tcp.dport,                          # destination port\n            ),\n            num=count,                              # original packet range number\n            ack=tcp.ack,                            # acknowledgement\n            dsn=tcp.seq,                            # data sequence number\n            syn=bool(tcp.flags.S),                  # synchronise flag\n            fin=bool(tcp.flags.F),                  # finish flag\n            rst=bool(tcp.flags.R),                  # reset connection flag\n            payload=bytearray(bytes(tcp.payload)),  # raw bytearray type payload\n        )\n        raw_len = len(tcp.payload)                  # payload length, header excludes\n        data['first'] = tcp.seq                     # this sequence number\n        data['last'] = tcp.seq + raw_len            # next (wanted) sequence number\n        data['len'] = raw_len                       # payload length, header excludes\n        return True, data\n    return False, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tcp_traceflow(packet, *, count=NotImplemented):\n    if 'TCP' in packet:\n        ip = packet['IP'] if 'IP' in packet else packet['IPv6']\n        tcp = packet['TCP']\n        data = dict(\n            protocol=LINKTYPE.get(packet.name.upper()),     # data link type from global header\n            index=count,                                    # frame number\n            frame=packet2dict(packet),                      # extracted packet\n            syn=bool(tcp.flags.S),                          # TCP synchronise (SYN) flag\n            fin=bool(tcp.flags.F),                          # TCP finish (FIN) flag\n            src=ipaddress.ip_address(ip.src),               # source IP\n            dst=ipaddress.ip_address(ip.dst),               # destination IP\n            srcport=tcp.sport,                              # TCP source port\n            dstport=tcp.dport,                              # TCP destination port\n            timestamp=time.time(),                          # timestamp\n        )\n        return True, data\n    return False, None", "response": "Trace packet flow for TCP."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread IPv6 Hop - by - Hop Options.", "response": "def read_hopopt(self, length, extension):\n        \"\"\"Read IPv6 Hop-by-Hop Options.\n\n        Structure of HOPOPT header [RFC 8200]:\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |  Next Header  |  Hdr Ext Len  |                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+                               +\n            |                                                               |\n            .                                                               .\n            .                            Options                            .\n            .                                                               .\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     hopopt.next             Next Header\n              1           8     hopopt.length           Header Extensive Length\n              2          16     hopopt.options          Options\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        _next = self._read_protos(1)\n        _hlen = self._read_unpack(1)\n        # _opts = self._read_fileng(_hlen*8+6)\n\n        hopopt = dict(\n            next=_next,\n            length=(_hlen + 1) * 8,\n        )\n\n        options = self._read_hopopt_options(_hlen * 8 + 6)\n        hopopt['options'] = options[0]      # tuple of option acronyms\n        hopopt.update(options[1])           # merge option info to buffer\n\n        length -= hopopt['length']\n        hopopt['packet'] = self._read_packet(header=hopopt['length'], payload=length)\n\n        if extension:\n            self._protos = None\n            return hopopt\n        return self._decode_next_layer(hopopt, _next, length)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading HOPOPT options. Positional arguments: * length -- int, length of options Returns: * dict -- extracted HOPOPT options", "response": "def _read_hopopt_options(self, length):\n        \"\"\"Read HOPOPT options.\n\n        Positional arguments:\n            * length -- int, length of options\n\n        Returns:\n            * dict -- extracted HOPOPT options\n\n        \"\"\"\n        counter = 0         # length of read options\n        optkind = list()    # option type list\n        options = dict()    # dict of option data\n\n        while counter < length:\n            # break when eol triggered\n            code = self._read_unpack(1)\n            if not code:\n                break\n\n            # extract parameter\n            abbr, desc = _HOPOPT_OPT.get(code, ('none', 'Unassigned'))\n            data = _HOPOPT_PROC(abbr)(self, code, desc=desc)\n            enum = _OPT_TYPE.get(code)\n\n            # record parameter data\n            counter += data['length']\n            if enum in optkind:\n                if isinstance(options[abbr], tuple):\n                    options[abbr] += (Info(data),)\n                else:\n                    options[abbr] = (Info(options[abbr]), Info(data))\n            else:\n                optkind.append(enum)\n                options[abbr] = data\n\n        # check threshold\n        if counter != length:\n            raise ProtocolError(f'{self.alias}: invalid format')\n\n        return tuple(optkind), options"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_opt_pad(self, code, *, desc):\n        _type = self._read_opt_type(code)\n\n        if code == 0:\n            opt = dict(\n                desc=desc,\n                type=_type,\n                length=1,\n            )\n        elif code == 1:\n            _size = self._read_unpack(1)\n            _padn = self._read_fileng(_size)\n\n            opt = dict(\n                desc=desc,\n                type=_type,\n                length=_size + 2,\n                padding=_padn,\n            )\n        else:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n\n        return opt", "response": "Read HOPOPT padding options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_opt_tun(self, code, *, desc):\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        if _size != 1:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n        _limt = self._read_unpack(1)\n\n        opt = dict(\n            desc=desc,\n            type=_type,\n            length=_size + 2,\n            limit=_limt,\n        )\n\n        return opt", "response": "Read an option from the archive and return it as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread option from the demos.", "response": "def _read_opt_ra(self, code, *, desc):\n        \"\"\"Read HOPOPT Router Alert option.\n\n        Structure of HOPOPT Router Alert option [RFC 2711]:\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |0 0 0|0 0 1 0 1|0 0 0 0 0 0 1 0|        Value (2 octets)       |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     hopopt.ra.type          Option Type\n              0           0     hopopt.ra.type.value    Option Number\n              0           0     hopopt.ra.type.action   Action (00)\n              0           2     hopopt.ra.type.change   Change Flag (0)\n              1           8     hopopt.opt.length       Length of Option Data\n              2          16     hopopt.ra.value         Value\n\n        \"\"\"\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        if _size != 2:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n        _rval = self._read_unpack(2)\n\n        if 4 <= _rval <= 35:\n            _dscp = f'Aggregated Reservation Nesting Level {_rval-4}'   # [RFC 3175]\n        elif 36 <= _rval <= 67:\n            _dscp = f'QoS NSLP Aggregation Level {_rval-36}'            # [RFC 5974]\n        elif 65503 <= _rval <= 65534:\n            _dscp = 'Reserved for experimental use'                     # [RFC 5350]\n        else:\n            _dscp = _ROUTER_ALERT.get(_rval, 'Unassigned')\n\n        opt = dict(\n            desc=desc,\n            type=_type,\n            length=_size + 2,\n            value=_rval,\n            alert=_dscp,\n        )\n\n        return opt"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading HOPOPT CALIPSO option.", "response": "def _read_opt_calipso(self, code, *, desc):\n        \"\"\"Read HOPOPT CALIPSO option.\n\n        Structure of HOPOPT CALIPSO option [RFC 5570]:\n            ------------------------------------------------------------\n            | Next Header | Hdr Ext Len   | Option Type | Option Length|\n            +-------------+---------------+-------------+--------------+\n            |             CALIPSO Domain of Interpretation             |\n            +-------------+---------------+-------------+--------------+\n            | Cmpt Length |  Sens Level   |     Checksum (CRC-16)      |\n            +-------------+---------------+-------------+--------------+\n            |      Compartment Bitmap (Optional; variable length)      |\n            +-------------+---------------+-------------+--------------+\n\n            Octets      Bits        Name                        Description\n              0           0     hopopt.calipso.type         Option Type\n              0           0     hopopt.calipso.type.value   Option Number\n              0           0     hopopt.calipso.type.action  Action (00)\n              0           2     hopopt.calipso.type.change  Change Flag (0)\n              1           8     hopopt.calipso.length       Length of Option Data\n              2          16     hopopt.calipso.domain       CALIPSO Domain of Interpretation\n              6          48     hopopt.calipso.cmpt_len     Cmpt Length\n              7          56     hopopt.calipso.level        Sens Level\n              8          64     hopopt.calipso.chksum       Checksum (CRC-16)\n              9          72     hopopt.calipso.bitmap       Compartment Bitmap\n\n        \"\"\"\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        if _size < 8 and _size % 8 != 0:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n        _cmpt = self._read_unpack(4)\n        _clen = self._read_unpack(1)\n        if _clen % 2 != 0:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n        _sens = self._read_unpack(1)\n        _csum = self._read_fileng(2)\n\n        opt = dict(\n            desc=desc,\n            type=_type,\n            length=_size + 2,\n            domain=_cmpt,\n            cmpt_len=_clen * 4,\n            level=_sens,\n            chksum=_csum,\n        )\n\n        if _clen:\n            _bmap = list()\n            for _ in range(_clen // 2):\n                _bmap.append(self._read_binary(8))\n            opt['bitmap'] = tuple(_bmap)\n\n        _plen = _size - _clen * 4 - 8\n        if _plen:\n            self._read_fileng(_plen)\n\n        return opt"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the SMF_DPD option from the entry in the entry table.", "response": "def _read_opt_smf_dpd(self, code, *, desc):\n        \"\"\"Read HOPOPT SMF_DPD option.\n\n        Structure of HOPOPT SMF_DPD option [RFC 5570]:\n            * IPv6 SMF_DPD Option Header in I-DPD mode\n                 0                   1                   2                   3\n                 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n                               ...              |0|0|0|  01000  | Opt. Data Len |\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n                |0|TidTy| TidLen|             TaggerId (optional) ...           |\n                +-+-+-+-+-+-+-+-+               +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n                |                               |            Identifier  ...\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n                Octets      Bits        Name                        Description\n                  0           0     hopopt.smf_dpd.type         Option Type\n                  0           0     hopopt.smf_dpd.type.value   Option Number\n                  0           0     hopopt.smf_dpd.type.action  Action (00)\n                  0           2     hopopt.smf_dpd.type.change  Change Flag (0)\n                  1           8     hopopt.smf_dpd.length       Length of Option Data\n                  2          16     hopopt.smf_dpd.dpd_type     DPD Type (0)\n                  2          17     hopopt.smf_dpd.tid_type     TaggerID Type\n                  2          20     hopopt.smf_dpd.tid_len      TaggerID Length\n                  3          24     hopopt.smf_dpd.tid          TaggerID\n                  ?           ?     hopopt.smf_dpd.id           Identifier\n\n            * IPv6 SMF_DPD Option Header in H-DPD Mode\n                 0                   1                   2                   3\n                 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n                               ...              |0|0|0| OptType | Opt. Data Len |\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n                |1|    Hash Assist Value (HAV) ...\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n                Octets      Bits        Name                        Description\n                  0           0     hopopt.smf_dpd.type         Option Type\n                  0           0     hopopt.smf_dpd.type.value   Option Number\n                  0           0     hopopt.smf_dpd.type.action  Action (00)\n                  0           2     hopopt.smf_dpd.type.change  Change Flag (0)\n                  1           8     hopopt.smf_dpd.length       Length of Option Data\n                  2          16     hopopt.smf_dpd.dpd_type     DPD Type (1)\n                  2          17     hopopt.smf_dpd.hav          Hash Assist Value\n\n        \"\"\"\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        _tidd = self._read_binary(1)\n\n        if _tidd[0] == '0':\n            _mode = 'I-DPD'\n            _tidt = _TID_TYPE.get(_tidd[1:4], 'Unassigned')\n            _tidl = int(_tidd[4:], base=2)\n            if _tidt == 'NULL':\n                if _tidl != 0:\n                    raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n                _iden = self._read_fileng(_size-1)\n\n                opt = dict(\n                    desc=desc,\n                    type=_type,\n                    length=_size + 2,\n                    dpd_type=_mode,\n                    tid_type=_tidt,\n                    tid_len=_tidl,\n                    id=_iden,\n                )\n            elif _tidt == 'IPv4':\n                if _tidl != 3:\n                    raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n                _tidf = self._read_fileng(4)\n                _iden = self._read_fileng(_size-4)\n\n                opt = dict(\n                    desc=desc,\n                    type=_type,\n                    length=_size + 2,\n                    dpd_type=_mode,\n                    tid_type=_tidt,\n                    tid_len=_tidl,\n                    tid=ipaddress.ip_address(_tidf),\n                    id=_iden,\n                )\n            elif _tidt == 'IPv6':\n                if _tidl != 15:\n                    raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n                _tidf = self._read_fileng(15)\n                _iden = self._read_fileng(_size-15)\n\n                opt = dict(\n                    desc=desc,\n                    type=_type,\n                    length=_size + 2,\n                    dpd_type=_mode,\n                    tid_type=_tidt,\n                    tid_len=_tidl,\n                    tid=ipaddress.ip_address(_tidf),\n                    id=_iden,\n                )\n            else:\n                _tidf = self._read_unpack(_tidl+1)\n                _iden = self._read_fileng(_size-_tidl-2)\n\n                opt = dict(\n                    desc=desc,\n                    type=_type,\n                    length=_size + 2,\n                    dpd_type=_mode,\n                    tid_type=_tidt,\n                    tid_len=_tidl,\n                    tid=_tidf,\n                    id=_iden,\n                )\n        elif _tidd[0] == '1':\n            _data = self._read_binary(_size-1)\n\n            opt = dict(\n                desc=desc,\n                type=_type,\n                length=_size + 2,\n                dpd_type=_mode,\n                tid_type=_tidt,\n                hav=_tidd[1:] + _data,\n            )\n        else:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n\n        return opt"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_opt_pdm(self, code, *, desc):\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        if _size != 10:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n        _stlr = self._read_unpack(1)\n        _stls = self._read_unpack(1)\n        _psnt = self._read_unpack(2)\n        _psnl = self._read_unpack(2)\n        _dtlr = self._read_unpack(2)\n        _dtls = self._read_unpack(2)\n\n        opt = dict(\n            desc=desc,\n            type=_type,\n            length=_size + 2,\n            scaledtlr=datetime.timedelta(seconds=_stlr),\n            scaledtls=datetime.timedelta(seconds=_stls),\n            psntp=_psnt,\n            psnlr=_psnl,\n            deltatlr=datetime.timedelta(seconds=_dtlr),\n            deltatls=datetime.timedelta(seconds=_dtls),\n        )\n\n        return opt", "response": "Read an option from the PDM file and return the entry in the desc dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the option and return the entry in the header file.", "response": "def _read_opt_qs(self, code, *, desc):\n        \"\"\"Read HOPOPT Quick Start option.\n\n        Structure of HOPOPT Quick-Start option [RFC 4782]:\n            * A Quick-Start Request.\n                 0                   1                   2                   3\n                 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n                |   Option      |  Length=6     | Func. | Rate  |   QS TTL      |\n                |               |               | 0000  |Request|               |\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n                |                        QS Nonce                           | R |\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            * Report of Approved Rate.\n                 0                   1                   2                   3\n                 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n                |   Option      |  Length=6     | Func. | Rate  |   Not Used    |\n                |               |               | 1000  | Report|               |\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n                |                        QS Nonce                           | R |\n                +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     hopopt.qs.type          Option Type\n              0           0     hopopt.qs.type.value    Option Number\n              0           0     hopopt.qs.type.action   Action (00)\n              0           2     hopopt.qs.type.change   Change Flag (1)\n              1           8     hopopt.qs.length        Length of Option Data\n              2          16     hopopt.qs.func          Function (0/8)\n              2          20     hopopt.qs.rate          Rate Request / Report (in Kbps)\n              3          24     hopopt.qs.ttl           QS TTL / None\n              4          32     hopopt.qs.nounce        QS Nounce\n              7          62     -                       Reserved\n\n        \"\"\"\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        if _size != 6:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n\n        _fcrr = self._read_binary(1)\n        _func = int(_fcrr[:4], base=2)\n        _rate = int(_fcrr[4:], base=2)\n        _ttlv = self._read_unpack(1)\n        _nonr = self._read_binary(4)\n        _qsnn = int(_nonr[:30], base=2)\n\n        if _func != 0 and _func != 8:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n\n        data = dict(\n            type=_type,\n            length=_size + 2,\n            func=_QS_FUNC.get(_func),\n            rate=40000 * (2 ** _rate) / 1000,\n            ttl=None if _func else _rate,\n            nounce=_qsnn,\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_opt_rpl(self, code, *, desc):\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        if _size < 4:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n        _flag = self._read_binary(1)\n        _rpld = self._read_unpack(1)\n        _rank = self._read_unpack(2)\n\n        opt = dict(\n            desc=desc,\n            type=_type,\n            length=_size + 2,\n            flags=dict(\n                down=True if int(_flag[0], base=2) else False,\n                rank_error=True if int(_flag[1], base=2) else False,\n                fwd_error=True if int(_flag[2], base=2) else False,\n            ),\n            id=_rpld,\n            rank=_rank,\n        )\n\n        if _size > 4:\n            opt['data'] = self._read_fileng(_size-4)\n\n        return opt", "response": "Read an option from the ISO 1918 and return the corresponding entry in the description."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_opt_ilnp(self, code, *, desc):\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        _nval = self._read_fileng(_size)\n\n        opt = dict(\n            desc=desc,\n            type=_type,\n            length=_size + 2,\n            value=_nval,\n        )\n\n        return opt", "response": "Read an option from the archive."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_opt_lio(self, code, *, desc):\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        _llen = self._read_unpack(1)\n        _line = self._read_fileng(_llen)\n\n        opt = dict(\n            desc=desc,\n            type=_type,\n            length=_size + 2,\n            lid_len=_llen,\n            lid=_line,\n        )\n\n        _plen = _size - _llen\n        if _plen:\n            self._read_fileng(_plen)\n\n        return opt", "response": "Read an option from the file and return it as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_opt_jumbo(self, code, *, desc):\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        if _size != 4:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n        _jlen = self._read_unpack(4)\n\n        opt = dict(\n            desc=desc,\n            type=_type,\n            length=_size + 2,\n            payload_len=_jlen,\n        )\n\n        return opt", "response": "Read an option from the header and return it as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_opt_home(self, code, *, desc):\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        if _size != 16:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n        _addr = self._read_fileng(16)\n\n        opt = dict(\n            desc=desc,\n            type=_type,\n            length=_size + 2,\n            ip=ipaddress.ip_address(_addr),\n        )\n\n        return opt", "response": "Read an option from the archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads IPv6 - Options for the current locale.", "response": "def read_ipv6_opts(self, length, extension):\n        \"\"\"Read Destination Options for IPv6.\n\n        Structure of IPv6-Opts header [RFC 8200]:\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |  Next Header  |  Hdr Ext Len  |                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+                               +\n            |                                                               |\n            .                                                               .\n            .                            Options                            .\n            .                                                               .\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     opt.next                Next Header\n              1           8     opt.length              Header Extensive Length\n              2          16     opt.options             Options\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        _next = self._read_protos(1)\n        _hlen = self._read_unpack(1)\n        # _opts = self._read_fileng(_hlen*8+6)\n\n        ipv6_opts = dict(\n            next=_next,\n            length=(_hlen + 1) * 8,\n        )\n\n        options = self._read_ipv6_opts_options(_hlen * 8 + 6)\n        ipv6_opts['options'] = options[0]       # tuple of option acronyms\n        ipv6_opts.update(options[1])            # merge option info to buffer\n\n        length -= ipv6_opts['length']\n        ipv6_opts['packet'] = self._read_packet(header=ipv6_opts['length'], payload=length)\n\n        if extension:\n            self._protos = None\n            return ipv6_opts\n        return self._decode_next_layer(ipv6_opts, _next, length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads option type field.", "response": "def _read_opt_type(self, kind):\n        \"\"\"Read option type field.\n\n        Positional arguments:\n            * kind -- int, option kind value\n\n        Returns:\n            * dict -- extracted IPv6_Opts option\n\n        Structure of option type field [RFC 791]:\n\n            Octets      Bits        Name                        Descriptions\n              0           0     ipv6_opts.opt.type.value    Option Number\n              0           0     ipv6_opts.opt.type.action   Action (00-11)\n              0           2     ipv6_opts.opt.type.change   Change Flag (0/1)\n\n        \"\"\"\n        bin_ = bin(kind)[2:].zfill(8)\n        type_ = dict(\n            value=kind,\n            action=_IPv6_Opts_ACT.get(bin_[:2]),\n            change=True if int(bin_[2], base=2) else False,\n        )\n        return type_"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads IPv6_Opts options. Positional arguments: * length -- int, length of options Returns: * dict -- extracted IPv6_Opts options", "response": "def _read_ipv6_opts_options(self, length):\n        \"\"\"Read IPv6_Opts options.\n\n        Positional arguments:\n            * length -- int, length of options\n\n        Returns:\n            * dict -- extracted IPv6_Opts options\n\n        \"\"\"\n        counter = 0         # length of read options\n        optkind = list()    # option type list\n        options = dict()    # dict of option data\n\n        while counter < length:\n            # break when eol triggered\n            code = self._read_unpack(1)\n            if not code:\n                break\n\n            # extract parameter\n            abbr, desc = _IPv6_Opts_OPT.get(code, ('None', 'Unassigned'))\n            data = _IPv6_Opts_PROC(abbr)(self, code, desc=desc)\n            enum = _OPT_TYPE.get(code)\n\n            # record parameter data\n            counter += data['length']\n            if enum in optkind:\n                if isinstance(options[abbr], tuple):\n                    options[abbr] += (Info(data),)\n                else:\n                    options[abbr] = (Info(options[abbr]), Info(data))\n            else:\n                optkind.append(enum)\n                options[abbr] = data\n\n        # check threshold\n        if counter != length:\n            raise ProtocolError(f'{self.alias}: invalid format')\n\n        return tuple(optkind), options"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads IPv6_Opts option with null value.", "response": "def _read_opt_none(self, code, *, desc):\n        \"\"\"Read IPv6_Opts unassigned options.\n\n        Structure of IPv6_Opts unassigned options [RFC 8200]:\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+- - - - - - - - -\n            |  Option Type  |  Opt Data Len |  Option Data\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+- - - - - - - - -\n\n            Octets      Bits        Name                        Description\n              0           0     ipv6_opts.opt.type          Option Type\n              0           0     ipv6_opts.opt.type.value    Option Number\n              0           0     ipv6_opts.opt.type.action   Action (00-11)\n              0           2     ipv6_opts.opt.type.change   Change Flag (0/1)\n              1           8     ipv6_opts.opt.length        Length of Option Data\n              2          16     ipv6_opts.opt.data          Option Data\n\n        \"\"\"\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        _data = self._read_fileng(_size)\n\n        opt = dict(\n            desc=_IPv6_Opts_NULL.get(code, desc),\n            type=_type,\n            length=_size + 2,\n            data=_data,\n        )\n\n        return opt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread IPv6_Opts MPL option.", "response": "def _read_opt_mpl(self, code, *, desc):\n        \"\"\"Read IPv6_Opts MPL option.\n\n        Structure of IPv6_Opts MPL option [RFC 7731]:\n             0                   1                   2                   3\n             0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n                                            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n                                            |  Option Type  |  Opt Data Len |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            | S |M|V|  rsv  |   sequence    |      seed-id (optional)       |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                            Description\n              0           0     ipv6_opts.mpl.type              Option Type\n              0           0     ipv6_opts.mpl.type.value        Option Number\n              0           0     ipv6_opts.mpl.type.action       Action (01)\n              0           2     ipv6_opts.mpl.type.change       Change Flag (1)\n              1           8     ipv6_opts.mpl.length            Length of Option Data\n              2          16     ipv6_opts.mpl.seed_len          Seed-ID Length\n              2          18     ipv6_opts.mpl.flags             MPL Option Flags\n              2          18     ipv6_opts.mpl.max               Maximum SEQ Flag\n              2          19     ipv6_opts.mpl.verification      Verification Flag\n              2          20     -                               Reserved\n              3          24     ipv6_opts.mpl.seq               Sequence\n              4          32     ipv6_opts.mpl.seed_id           Seed-ID\n\n        \"\"\"\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        if _size < 2:\n            raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n\n        _type = self._read_opt_type(code)\n        _size = self._read_unpack(1)\n        _smvr = self._read_binary(1)\n        _seqn = self._read_unpack(1)\n\n        opt = dict(\n            desc=desc,\n            type=_type,\n            length=_size + 2,\n            seed_len=_IPv6_Opts_SEED.get(int(_smvr[:2], base=2)),\n            flags=dict(\n                max=True if int(_smvr[2], base=2) else False,\n                verification=True if int(_smvr[3], base=2) else False,\n            ),\n            seq=_seqn,\n        )\n\n        _kind = _smvr[:2]\n        if _kind == '00':\n            if _size != 2:\n                raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n        elif _kind == '01':\n            if _size != 4:\n                raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n            opt['seed_id'] = self._read_unpack(2)\n        elif _kind == '10':\n            if _size != 10:\n                raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n            opt['seed_id'] = self._read_unpack(8)\n        elif _kind == '11':\n            if _size != 18:\n                raise ProtocolError(f'{self.alias}: [Optno {code}] invalid format')\n            opt['seed_id'] = self._read_unpack(16)\n        else:\n            opt['seed_id'] = self._read_unpack(_size-2)\n\n        _plen = _size - opt['seed_len']\n        if _plen:\n            self._read_fileng(_plen)\n\n        return opt"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return ReturnCode(key)\n        if key not in ReturnCode._member_map_:\n            extend_enum(ReturnCode, key, default)\n        return ReturnCode[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _missing_(cls, value):\n        if not (isinstance(value, int) and 100 <= value <= 659):\n            raise ValueError('%r is not a valid %s' % (value, cls.__name__))\n        code = str(value)\n        kind = KIND.get(code[0], 'Reserved')\n        info = INFO.get(code[1], 'Reserved')\n        extend_enum(cls, '%s - %s [%s]' % (kind, info, value), value)\n        return cls(value)", "response": "Lookup function used when value is not found."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return Option(key)\n        if key not in Option._member_map_:\n            extend_enum(Option, key, default)\n        return Option[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return ExtensionHeader(key)\n        if key not in ExtensionHeader._member_map_:\n            extend_enum(ExtensionHeader, key, default)\n        return ExtensionHeader[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(key, default=-1):\n        if isinstance(key, int):\n            return OptionClass(key)\n        if key not in OptionClass._member_map_:\n            extend_enum(OptionClass, key, default)\n        return OptionClass[key]", "response": "Backport support for original codes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread IPv6 - Route header.", "response": "def read_ipv6_route(self, length, extension):\n        \"\"\"Read Routing Header for IPv6.\n\n        Structure of IPv6-Route header [RFC 8200][RFC 5095]:\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |  Next Header  |  Hdr Ext Len  |  Routing Type | Segments Left |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            .                                                               .\n            .                       type-specific data                      .\n            .                                                               .\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     route.next              Next Header\n              1           8     route.length            Header Extensive Length\n              2          16     route.type              Routing Type\n              3          24     route.seg_left          Segments Left\n              4          32     route.data              Type-Specific Data\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        _next = self._read_protos(1)\n        _hlen = self._read_unpack(1)\n        _type = self._read_unpack(1)\n        _left = self._read_unpack(1)\n\n        ipv6_route = dict(\n            next=_next,\n            length=(_hlen + 1) * 8,\n            type=_ROUTING_TYPE.get(_type, 'Unassigned'),\n            seg_left=_left,\n        )\n\n        _dlen = _hlen * 8 - 4\n        if _dlen:\n            _func = _ROUTE_PROC.get(_type, 'none')\n            _data = eval(f'self._read_data_type_{_func}')(_dlen)\n            ipv6_route.update(_data)\n\n        length -= ipv6_route['length']\n        ipv6_route['packet'] = self._read_packet(header=ipv6_route['length'], payload=length)\n\n        if extension:\n            self._protos = None\n            return ipv6_route\n        return self._decode_next_layer(ipv6_route, _next, length)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_data_type_none(self, length):\n        _data = self._read_fileng(length)\n\n        data = dict(\n            data=_data,\n        )\n\n        return data", "response": "Read unknown type data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_data_type_src(self, length):\n        _resv = self._read_fileng(4)\n        _addr = list()\n        for _ in range((length - 4) // 16):\n            _addr.append(ipaddress.ip_address(self._read_fileng(16)))\n\n        data = dict(\n            ip=tuple(_addr),\n        )\n\n        return data", "response": "Read IPv6 - Route source route data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread IPv6 - Route Type 2 data.", "response": "def _read_data_type_2(self, length):\n        \"\"\"Read IPv6-Route Type 2 data.\n\n        Structure of IPv6-Route Type 2 data [RFC 6275]:\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |  Next Header  | Hdr Ext Len=2 | Routing Type=2|Segments Left=1|\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                            Reserved                           |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |                                                               |\n            +                                                               +\n            |                                                               |\n            +                         Home Address                          +\n            |                                                               |\n            +                                                               +\n            |                                                               |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n            Octets      Bits        Name                    Description\n              0           0     route.next              Next Header\n              1           8     route.length            Header Extensive Length\n              2          16     route.type              Routing Type\n              3          24     route.seg_left          Segments Left\n              4          32     -                       Reserved\n              8          64     route.ip                Home Address\n\n        \"\"\"\n        if length != 20:\n            raise ProtocolError(f'{self.alias}: [Typeno 2] invalid format')\n\n        _resv = self._read_fileng(4)\n        _home = self._read_fileng(16)\n\n        data = dict(\n            ip=ipaddress.ip_address(_home),\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread Hypertext Transfer Protocol (HTTP/1.*). Structure of HTTP/1.* packet [RFC 7230]: HTTP-message :==: start-line *( header-field CRLF ) CRLF [ message-body ]", "response": "def read_http(self, length):\n        \"\"\"Read Hypertext Transfer Protocol (HTTP/1.*).\n\n        Structure of HTTP/1.* packet [RFC 7230]:\n            HTTP-message    :==:    start-line\n                                    *( header-field CRLF )\n                                    CRLF\n                                    [ message-body ]\n\n        \"\"\"\n        if length is None:\n            length = len(self)\n\n        packet = self._file.read(length)\n        try:\n            header, body = packet.split(b'\\r\\n\\r\\n', 1)\n        except ValueError:\n            raise ProtocolError('HTTP: invalid format', quiet=True)\n\n        header_unpacked, http_receipt = self._read_http_header(header)\n        body_unpacked = self._read_http_body(body) or None\n\n        http = dict(\n            receipt=http_receipt,\n            header=header_unpacked,\n            body=body_unpacked,\n            raw=dict(\n                header=header,\n                body=body,\n                packet=self._read_packet(length),\n            ),\n        )\n        self.__receipt__ = http_receipt\n\n        return http"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_http_header(self, header):\n        try:\n            startline, headerfield = header.split(b'\\r\\n', 1)\n            para1, para2, para3 = re.split(rb'\\s+', startline, 2)\n            fields = headerfield.split(b'\\r\\n')\n            lists = (re.split(rb'\\s*:\\s*', field, 1) for field in fields)\n        except ValueError:\n            raise ProtocolError('HTTP: invalid format', quiet=True)\n\n        match1 = re.match(_RE_METHOD, para1)\n        match2 = re.match(_RE_VERSION, para3)\n        match3 = re.match(_RE_VERSION, para1)\n        match4 = re.match(_RE_STATUS, para2)\n        if match1 and match2:\n            receipt = 'request'\n            header = dict(\n                request=dict(\n                    method=self.decode(para1),\n                    target=self.decode(para2),\n                    version=self.decode(match2.group('version')),\n                ),\n            )\n        elif match3 and match4:\n            receipt = 'response'\n            header = dict(\n                response=dict(\n                    version=self.decode(match3.group('version')),\n                    status=int(para2),\n                    phrase=self.decode(para3),\n                ),\n            )\n        else:\n            raise ProtocolError('HTTP: invalid format', quiet=True)\n\n        try:\n            for item in lists:\n                key = self.decode(item[0].strip()).replace(receipt, f'{receipt}_field')\n                value = self.decode(item[1].strip())\n                if key in header:\n                    if isinstance(header[key], tuple):\n                        header[key] += (value,)\n                    else:\n                        header[key] = (header[key], value)\n                else:\n                    header[key] = value\n        except IndexError:\n            raise ProtocolError('HTTP: invalid format', quiet=True)\n\n        return header, receipt", "response": "Read HTTP/1.* header.\n\n        Structure of HTTP/1.* header [RFC 7230]:\n            start-line      :==:    request-line / status-line\n            request-line    :==:    method SP request-target SP HTTP-version CRLF\n            status-line     :==:    HTTP-version SP status-code SP reason-phrase CRLF\n            header-field    :==:    field-name \":\" OWS field-value OWS"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef geohash_to_polygon(geo):\n    lat_centroid, lng_centroid, lat_offset, lng_offset = geohash.decode_exactly(geo)\n\n    corner_1 = (lat_centroid - lat_offset, lng_centroid - lng_offset)[::-1]\n    corner_2 = (lat_centroid - lat_offset, lng_centroid + lng_offset)[::-1]\n    corner_3 = (lat_centroid + lat_offset, lng_centroid + lng_offset)[::-1]\n    corner_4 = (lat_centroid + lat_offset, lng_centroid - lng_offset)[::-1]\n\n    return geometry.Polygon([corner_1, corner_2, corner_3, corner_4, corner_1])", "response": "Converts a geohash to a Shapely s Polygon instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert shapely polygon to set of geohashes.", "response": "def polygon_to_geohashes(polygon, precision, inner=True):\n    \"\"\"\n    :param polygon: shapely polygon.\n    :param precision: int. Geohashes' precision that form resulting polygon.\n    :param inner: bool, default 'True'. If false, geohashes that are completely outside from the polygon are ignored.\n    :return: set. Set of geohashes that form the polygon.\n    \"\"\"\n    inner_geohashes = set()\n    outer_geohashes = set()\n\n    envelope = polygon.envelope\n    centroid = polygon.centroid\n\n    testing_geohashes = queue.Queue()\n    testing_geohashes.put(geohash.encode(centroid.y, centroid.x, precision))\n\n    while not testing_geohashes.empty():\n        current_geohash = testing_geohashes.get()\n\n        if current_geohash not in inner_geohashes and current_geohash not in outer_geohashes:\n            current_polygon = geohash_to_polygon(current_geohash)\n\n            condition = envelope.contains(current_polygon) if inner else envelope.intersects(current_polygon)\n\n            if condition:\n                if inner:\n                    if polygon.contains(current_polygon):\n                        inner_geohashes.add(current_geohash)\n                    else:\n                        outer_geohashes.add(current_geohash)\n                else:\n                    if polygon.intersects(current_polygon):\n                        inner_geohashes.add(current_geohash)\n                    else:\n                        outer_geohashes.add(current_geohash)\n                for neighbor in geohash.neighbors(current_geohash):\n                    if neighbor not in inner_geohashes and neighbor not in outer_geohashes:\n                        testing_geohashes.put(neighbor)\n\n    return inner_geohashes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_oggopus_output_gain(file):\n  #\n  # Ogg header\n  #\n\n  # check fields of Ogg page header\n  chunk = file.read(OGG_FIRST_PAGE_HEADER.size)\n  first_ogg_page = bytearray()\n  first_ogg_page.extend(chunk)\n  if len(chunk) < OGG_FIRST_PAGE_HEADER.size:\n    logger().error(\"Not enough bytes in Ogg page header: %u, expected at least %u\" % (len(chunk),\n                                                                                      OGG_FIRST_PAGE_HEADER.size))\n    return\n  capture_pattern, version, header_type, granule_position, bitstream_serial_number, page_sequence_number, \\\n      crc_checksum, page_segments = OGG_FIRST_PAGE_HEADER.unpack(chunk)\n  if capture_pattern != b\"OggS\":\n    logger().error(\"Invalid OGG capture pattern: %s, expected '%s'\" % (repr(capture_pattern), \"OggS\"))\n    return\n  if version != 0:\n    logger().error(\"Invalid OGG version: %u, expected %u\" % (version, 0))\n    return\n  if header_type != 2:  # should be first page of stream\n    logger().error(\"Invalid OGG page header type: %u, expected %u\" % (header_type, 2))\n    return\n  if page_sequence_number != 0:\n    logger().error(\"Invalid OGG page sequence number: %u, expected %u\" % (page_sequence_number, 0))\n    return\n  segment_table_fmt = struct.Struct(\"<%uB\" % (page_segments))\n  chunk = file.read(segment_table_fmt.size)\n  first_ogg_page.extend(chunk)\n  if len(chunk) < segment_table_fmt.size:\n    logger().error(\"Not enough bytes for OGG segment table: %u, expected at least %u\" % (len(chunk),\n                                                                                         segment_table_fmt.size))\n    return\n  segment_table = segment_table_fmt.unpack(chunk)\n\n  # check crc of first page\n  first_ogg_page_size = OGG_FIRST_PAGE_HEADER.size + segment_table_fmt.size + sum(segment_table)\n  chunk = file.read(sum(segment_table))\n  first_ogg_page.extend(chunk)\n  if len(first_ogg_page) < first_ogg_page_size:\n    logger().error(\"Not enough bytes for first OGG page: %u, expected at least %u\" % (len(first_ogg_page),\n                                                                                      first_ogg_page_size))\n    return\n  computed_crc = _compute_ogg_page_crc(first_ogg_page)\n  if computed_crc != crc_checksum:\n    logger().error(\"Invalid OGG page CRC: 0x%08x, expected 0x%08x\" % (crc_checksum, computed_crc))\n    return\n\n  #\n  # Opus header\n  #\n  chunk = first_ogg_page[OGG_FIRST_PAGE_HEADER.size + segment_table_fmt.size:][:segment_table[0]]\n  if len(chunk) < OGG_OPUS_ID_HEADER.size:\n    logger().error(\"Not enough bytes for Opus Identification header: %u, \"\n                   \"expected at least %u\" % (len(chunk),\n                                             OGG_OPUS_ID_HEADER.size))\n    return\n  magic, version, channel_count, preskip, input_samplerate, output_gain, \\\n      mapping_family = OGG_OPUS_ID_HEADER.unpack(chunk[:OGG_OPUS_ID_HEADER.size])\n  if magic != b\"OpusHead\":\n    logger().error(\"Invalid Opus magic number: %s, expected '%s'\" % (repr(magic), \"OpusHead\"))\n    return\n  if (version >> 4) != 0:\n    logger().error(\"Invalid Opus version: 0x%x, expected 0x0-0xf\" % (version))\n    return\n\n  # seek to Opus header\n  file.seek(OGG_FIRST_PAGE_HEADER.size + segment_table_fmt.size)\n\n  return output_gain", "response": "Parse an Opus file and return its output gain."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_oggopus_output_gain(file, new_output_gain):\n  opus_header_pos = file.tell()\n\n  # write Opus header with new gain\n  file.seek(opus_header_pos + OGG_OPUS_ID_HEADER_GAIN_OFFSET)\n  file.write(OGG_OPUS_ID_HEADER_GAIN.pack(new_output_gain))\n\n  # compute page crc\n  file.seek(0)\n  page = file.read(opus_header_pos + OGG_OPUS_ID_HEADER.size)\n  computed_crc = _compute_ogg_page_crc(page)\n\n  # write CRC\n  file.seek(OGG_FIRST_PAGE_HEADER_CRC_OFFSET)\n  file.write(OGG_FIRST_PAGE_HEADER_CRC.pack(computed_crc))", "response": "Write output gain for a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_ogg_page_crc(page):\n  page_zero_crc = page[:OGG_FIRST_PAGE_HEADER_CRC_OFFSET] + \\\n      b\"\\00\" * OGG_FIRST_PAGE_HEADER_CRC.size + \\\n      page[OGG_FIRST_PAGE_HEADER_CRC_OFFSET + OGG_FIRST_PAGE_HEADER_CRC.size:]\n  return ogg_page_crc(page_zero_crc)", "response": "Compute the CRC of an Ogg page."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_ffmpeg_lib_versions(ffmpeg_path=None):\n  r = collections.OrderedDict()\n  cmd = (ffmpeg_path or \"ffmpeg\", \"-version\")\n  output = subprocess.run(cmd,\n                          check=True,\n                          stdout=subprocess.PIPE,\n                          universal_newlines=True).stdout\n  output = output.splitlines()\n  lib_version_regex = re.compile(\"^\\s*(lib[a-z]+)\\s+([0-9]+).\\s*([0-9]+).\\s*([0-9]+)\\s+\")\n  for line in output:\n    match = lib_version_regex.search(line)\n    if match:\n      lib_name, *lib_version = match.group(1, 2, 3, 4)\n      int_lib_version = 0\n      for i, d in enumerate(map(int, reversed(lib_version)), 1):\n        int_lib_version |= d << (8 * i)\n      r[lib_name] = int_lib_version\n  return r", "response": "Get the library versions as 32 bit integers with same format as sys. hexversion.\n Example : 0x3040100 for FFmpeg 3. 4. 1."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_ffmpeg_filter(name, params):\n  return \"%s=%s\" % (name,\n                    \":\".join(\"%s=%s\" % (k, v) for k, v in params.items()))", "response": "Build a string to call a FFMpeg filter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget R128 loudness level and sample peak.", "response": "def get_r128_loudness(audio_filepaths, *, calc_peak=True, enable_ffmpeg_threading=True, ffmpeg_path=None):\n  \"\"\" Get R128 loudness loudness level and sample peak. \"\"\"\n  logger().info(\"Analyzing loudness of file%s %s...\" % (\"s\" if (len(audio_filepaths) > 1) else \"\",\n                                                        \", \".join(\"'%s'\" % (audio_filepath) for audio_filepath in audio_filepaths)))\n\n  # build command line\n  cmd = [ffmpeg_path or \"ffmpeg\",\n         \"-hide_banner\", \"-nostats\"]\n  for i, audio_filepath in enumerate(audio_filepaths):\n    if not enable_ffmpeg_threading:\n      cmd.extend((\"-threads:%u\" % (i), \"1\"))  # single decoding thread\n    cmd.extend((\"-i\", audio_filepath))\n  if (get_ffmpeg_lib_versions()[\"libavfilter\"] >= 0x06526400) and (not enable_ffmpeg_threading):\n    cmd.extend((\"-filter_threads\", \"1\"))  # single filter thread\n  cmd.extend((\"-map\", \"a\"))\n  ebur128_filter_params = {\"framelog\": \"verbose\"}\n  aformat_r128_filter_params = {\"sample_fmts\": \"s16\",\n                                \"sample_rates\": \"48000\",\n                                \"channel_layouts\": \"stereo\"}\n  aformat_rg_filter_params = {\"sample_fmts\": \"s16\"}\n  filter_chain = []\n  if len(audio_filepaths) > 1:\n    cmd.append(\"-filter_complex\")\n    for i in range(len(audio_filepaths)):\n      if calc_peak:\n        filter_chain.append(\"[%u:a]asplit[a_rg_in_%u][a_r128_in_%u]\" % (i, i, i))\n        filter_chain.append(\"[a_rg_in_%u]%s,replaygain,anullsink\" % (i,\n                                                                     format_ffmpeg_filter(\"aformat\",\n                                                                                          aformat_rg_filter_params)))\n      else:\n        filter_chain.append(\"[%u:a]anul[a_r128_in_%u]\" % (i, i))\n      filter_chain.append(\"[a_r128_in_%u]%s,afifo[a_r128_in_fmt_%u]\" % (i,\n                                                                        format_ffmpeg_filter(\"aformat\",\n                                                                                             aformat_r128_filter_params),\n                                                                        i))\n    filter_chain.append(\"%sconcat=n=%u:v=0:a=1[a_r128_in_concat]\" % (\"\".join((\"[a_r128_in_fmt_%u]\" % (i)) for i in range(len(audio_filepaths))),\n                                                                     len(audio_filepaths)))\n    filter_chain.append(\"[a_r128_in_concat]%s\" % (format_ffmpeg_filter(\"ebur128\", ebur128_filter_params)))\n    cmd.append(\"; \".join(filter_chain))\n  else:\n    if calc_peak:\n      filter_chain.extend((format_ffmpeg_filter(\"aformat\", aformat_rg_filter_params),\n                           \"replaygain\"))\n    filter_chain.append(format_ffmpeg_filter(\"ebur128\", ebur128_filter_params))\n    # filter_chain.append(\"anullsink\")\n    cmd.extend((\"-filter:a\", \",\".join(filter_chain)))\n  cmd.extend((\"-f\", \"null\", os.devnull))\n\n  # run\n  logger().debug(subprocess.list2cmdline(cmd))\n  output = subprocess.run(cmd,\n                          check=True,\n                          stdin=subprocess.DEVNULL,\n                          stderr=subprocess.PIPE).stderr\n  output = output.decode(\"utf-8\", errors=\"replace\").splitlines()\n\n  if calc_peak:\n    # parse replaygain filter output\n    sample_peaks = []\n    for line in reversed(output):\n      if line.startswith(\"[Parsed_replaygain_\") and (\"] track_peak = \" in line):\n        sample_peaks.append(float(line.rsplit(\"=\", 1)[1]))\n        if len(sample_peaks) == len(audio_filepaths):\n          break\n    sample_peak = max(sample_peaks)\n  else:\n    sample_peak = None\n\n  # parse r128 filter output\n  for i in reversed(range(len(output))):\n    line = output[i]\n    if line.startswith(\"[Parsed_ebur128_\") and line.endswith(\"Summary:\"):\n      break\n  output = filter(lambda x: x and not x.startswith(\"[Parsed_replaygain_\"),\n                  map(str.strip, output[i:]))\n  r128_stats = dict(tuple(map(str.strip, line.split(\":\", 1))) for line in output if not line.endswith(\":\"))\n  r128_stats = {k: float(v.split(\" \", 1)[0]) for k, v in r128_stats.items()}\n\n  return r128_stats[\"I\"], sample_peak"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nscans the audio files and return a dictionary of filepath to loudness metadata.", "response": "def scan(audio_filepaths, *, album_gain=False, skip_tagged=False, thread_count=None, ffmpeg_path=None, executor=None):\n  \"\"\" Analyze files, and return a dictionary of filepath to loudness metadata or filepath to future if executor is not None. \"\"\"\n  r128_data = {}\n\n  with contextlib.ExitStack() as cm:\n    if executor is None:\n      if thread_count is None:\n        try:\n          thread_count = len(os.sched_getaffinity(0))\n        except AttributeError:\n          thread_count = os.cpu_count()\n      enable_ffmpeg_threading = thread_count > (len(audio_filepaths) + int(album_gain))\n      executor = cm.enter_context(concurrent.futures.ThreadPoolExecutor(max_workers=thread_count))\n      asynchronous = False\n    else:\n      enable_ffmpeg_threading = False\n      asynchronous = True\n\n    loudness_tags = tuple(map(has_loudness_tag, audio_filepaths))\n\n    # remove invalid files\n    audio_filepaths = tuple(audio_filepath for (audio_filepath,\n                                                has_tags) in zip(audio_filepaths,\n                                                                 loudness_tags) if has_tags is not None)\n    loudness_tags = tuple(filter(None, loudness_tags))\n\n    futures = {}\n    if album_gain:\n      if skip_tagged and all(map(operator.itemgetter(1), loudness_tags)):\n        logger().info(\"All files already have an album gain tag, skipping album gain scan\")\n      elif audio_filepaths:\n        calc_album_peak = any(map(lambda x: os.path.splitext(x)[-1].lower() != \".opus\",\n                                  audio_filepaths))\n        futures[ALBUM_GAIN_KEY] = executor.submit(get_r128_loudness,\n                                                  audio_filepaths,\n                                                  calc_peak=calc_album_peak,\n                                                  enable_ffmpeg_threading=enable_ffmpeg_threading,\n                                                  ffmpeg_path=ffmpeg_path)\n    for audio_filepath in audio_filepaths:\n      if skip_tagged and has_loudness_tag(audio_filepath)[0]:\n        logger().info(\"File '%s' already has a track gain tag, skipping track gain scan\" % (audio_filepath))\n        continue\n      if os.path.splitext(audio_filepath)[-1].lower() == \".opus\":\n        # http://www.rfcreader.com/#rfc7845_line1060\n        calc_peak = False\n      else:\n        calc_peak = True\n      futures[audio_filepath] = executor.submit(get_r128_loudness,\n                                                (audio_filepath,),\n                                                calc_peak=calc_peak,\n                                                enable_ffmpeg_threading=enable_ffmpeg_threading,\n                                                ffmpeg_path=ffmpeg_path)\n\n    if asynchronous:\n      return futures\n\n    for audio_filepath in audio_filepaths:\n      try:\n        r128_data[audio_filepath] = futures[audio_filepath].result()\n      except KeyError:\n        # track gain was skipped\n        pass\n      except Exception as e:\n        # raise\n        logger().warning(\"Failed to analyze file '%s': %s %s\" % (audio_filepath,\n                                                                 e.__class__.__qualname__,\n                                                                 e))\n    if album_gain and audio_filepaths:\n      try:\n        r128_data[ALBUM_GAIN_KEY] = futures[ALBUM_GAIN_KEY].result()\n      except KeyError:\n        # album gain was skipped\n        pass\n      except Exception as e:\n        # raise\n        logger().warning(\"Failed to analyze files %s: %s %s\" % (\", \".join(\"'%s'\" % (audio_filepath) for audio_filepath in audio_filepaths),\n                                                                e.__class__.__qualname__,\n                                                                e))\n  return r128_data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tag(filepath, loudness, peak, *,\n        album_loudness=None, album_peak=None, opus_output_gain=False, mtime_second_offset=None):\n  \"\"\" Tag audio file with loudness metadata. \"\"\"\n  assert((loudness is not None) or (album_loudness is not None))\n\n  if peak is not None:\n    assert(0 <= peak <= 1.0)\n    if album_peak is not None:\n      assert(0 <= album_peak <= 1.0)\n\n  logger().info(\"Tagging file '%s'\" % (filepath))\n  original_mtime = os.path.getmtime(filepath)\n  mf = mutagen.File(filepath)\n  if (mf is not None) and (mf.tags is None):\n    mf.add_tags()\n\n  if (isinstance(mf.tags, mutagen.id3.ID3) or\n          isinstance(mf, mutagen.id3.ID3FileType)):\n    # http://wiki.hydrogenaud.io/index.php?title=ReplayGain_2.0_specification#ID3v2\n    if loudness is not None:\n      mf.tags.add(mutagen.id3.TXXX(encoding=mutagen.id3.Encoding.LATIN1,\n                                   desc=\"REPLAYGAIN_TRACK_GAIN\",\n                                   text=\"%.2f dB\" % (RG2_REF_R128_LOUDNESS_DBFS - loudness)))\n      mf.tags.add(mutagen.id3.TXXX(encoding=mutagen.id3.Encoding.LATIN1,\n                                   desc=\"REPLAYGAIN_TRACK_PEAK\",\n                                   text=\"%.6f\" % (peak)))\n    if album_loudness is not None:\n      mf.tags.add(mutagen.id3.TXXX(encoding=mutagen.id3.Encoding.LATIN1,\n                                   desc=\"REPLAYGAIN_ALBUM_GAIN\",\n                                   text=\"%.2f dB\" % (RG2_REF_R128_LOUDNESS_DBFS - album_loudness)))\n      mf.tags.add(mutagen.id3.TXXX(encoding=mutagen.id3.Encoding.LATIN1,\n                                   desc=\"REPLAYGAIN_ALBUM_PEAK\",\n                                   text=\"%.6f\" % (album_peak)))\n    # other legacy formats:\n    # http://wiki.hydrogenaud.io/index.php?title=ReplayGain_legacy_metadata_formats#ID3v2_RGAD\n    # http://wiki.hydrogenaud.io/index.php?title=ReplayGain_legacy_metadata_formats#ID3v2_RVA2\n\n  elif isinstance(mf, mutagen.oggopus.OggOpus):\n    if opus_output_gain and (loudness is not None):\n      with open(filepath, \"r+b\") as file:\n        current_output_gain = opusgain.parse_oggopus_output_gain(file)\n        new_output_gain = current_output_gain + float_to_q7dot8(OPUS_REF_R128_LOUDNESS_DBFS - loudness)\n        opusgain.write_oggopus_output_gain(file, new_output_gain)\n\n      # now that the output gain header is written, we will write the R128 tag for the new loudness\n      loudness = OPUS_REF_R128_LOUDNESS_DBFS\n      if album_loudness is not None:\n        # assume the whole album will be normalized the same way\n        # TODO better behavior? rescan album? disable R128 tags?\n        album_loudness = OPUS_REF_R128_LOUDNESS_DBFS\n\n    # https://wiki.xiph.org/OggOpus#Comment_Header\n    if loudness is not None:\n      q78 = float_to_q7dot8(OPUS_REF_R128_LOUDNESS_DBFS - loudness)\n      assert(-32768 <= q78 <= 32767)\n      mf[\"R128_TRACK_GAIN\"] = str(q78)\n    if album_loudness is not None:\n      q78 = float_to_q7dot8(OPUS_REF_R128_LOUDNESS_DBFS - album_loudness)\n      assert(-32768 <= q78 <= 32767)\n      mf[\"R128_ALBUM_GAIN\"] = str(q78)\n\n  elif (isinstance(mf.tags, (mutagen._vorbis.VComment, mutagen.apev2.APEv2)) or\n          isinstance(mf, (mutagen.ogg.OggFileType, mutagen.apev2.APEv2File))):\n    # https://wiki.xiph.org/VorbisComment#Replay_Gain\n    if loudness is not None:\n      mf[\"REPLAYGAIN_TRACK_GAIN\"] = \"%.2f dB\" % (RG2_REF_R128_LOUDNESS_DBFS - loudness)\n      mf[\"REPLAYGAIN_TRACK_PEAK\"] = \"%.8f\" % (peak)\n    if album_loudness is not None:\n      mf[\"REPLAYGAIN_ALBUM_GAIN\"] = \"%.2f dB\" % (RG2_REF_R128_LOUDNESS_DBFS - album_loudness)\n      mf[\"REPLAYGAIN_ALBUM_PEAK\"] = \"%.8f\" % (album_peak)\n\n  elif (isinstance(mf.tags, mutagen.mp4.MP4Tags) or\n        isinstance(mf, mutagen.mp4.MP4)):\n    # https://github.com/xbmc/xbmc/blob/9e855967380ef3a5d25718ff2e6db5e3dd2e2829/xbmc/music/tags/TagLoaderTagLib.cpp#L806-L812\n    if loudness is not None:\n      mf[\"----:COM.APPLE.ITUNES:REPLAYGAIN_TRACK_GAIN\"] = mutagen.mp4.MP4FreeForm((\"%.2f dB\" % (RG2_REF_R128_LOUDNESS_DBFS - loudness)).encode())\n      mf[\"----:COM.APPLE.ITUNES:REPLAYGAIN_TRACK_PEAK\"] = mutagen.mp4.MP4FreeForm((\"%.6f\" % (peak)).encode())\n    if album_loudness is not None:\n      mf[\"----:COM.APPLE.ITUNES:REPLAYGAIN_ALBUM_GAIN\"] = mutagen.mp4.MP4FreeForm((\"%.2f dB\" % (RG2_REF_R128_LOUDNESS_DBFS - album_loudness)).encode())\n      mf[\"----:COM.APPLE.ITUNES:REPLAYGAIN_ALBUM_PEAK\"] = mutagen.mp4.MP4FreeForm((\"%.6f\" % (album_peak)).encode())\n\n  else:\n    logger().warning(\"Unhandled '%s' tag format for file '%s'\" % (mf.__class__.__name__,\n                                                                  filepath))\n    return\n\n  mf.save()\n\n  # preserve original modification time, possibly increasing it by some seconds\n  if mtime_second_offset is not None:\n    if mtime_second_offset == 0:\n      logger().debug(\"Restoring modification time for file '{}'\".format(filepath))\n    else:\n      logger().debug(\"Restoring modification time for file '{}' (adding {} seconds)\".format(filepath,\n                                                                                            mtime_second_offset))\n    os.utime(filepath, times=(os.stat(filepath).st_atime, original_mtime + mtime_second_offset))", "response": "Tag an audio file with loudness metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a pair of booleans indicating if filepath has a RG or R128 track or album tag.", "response": "def has_loudness_tag(filepath):\n  \"\"\" Return a pair of booleans indicating if filepath has a RG or R128 track/album tag, or None if file is invalid. \"\"\"\n  track, album = False, False\n\n  try:\n    mf = mutagen.File(filepath)\n  except mutagen.MutagenError as e:\n    logger().warning(\"File '%s' %s: %s\" % (filepath,\n                                           e.__class__.__qualname__,\n                                           e))\n    return\n\n  if (isinstance(mf.tags, mutagen.id3.ID3) or\n          isinstance(mf, mutagen.id3.ID3FileType)):\n    track = (\"TXXX:REPLAYGAIN_TRACK_GAIN\" in mf) and (\"TXXX:REPLAYGAIN_TRACK_PEAK\" in mf)\n    album = (\"TXXX:REPLAYGAIN_ALBUM_GAIN\" in mf) and (\"TXXX:REPLAYGAIN_ALBUM_PEAK\" in mf)\n\n  elif isinstance(mf, mutagen.oggopus.OggOpus):\n    track = \"R128_TRACK_GAIN\" in mf\n    album = \"R128_ALBUM_GAIN\" in mf\n\n  elif (isinstance(mf.tags, (mutagen._vorbis.VComment, mutagen.apev2.APEv2)) or\n        isinstance(mf, (mutagen.ogg.OggFileType, mutagen.apev2.APEv2File))):\n    track = (\"REPLAYGAIN_TRACK_GAIN\" in mf) and (\"REPLAYGAIN_TRACK_PEAK\" in mf)\n    album = (\"REPLAYGAIN_ALBUM_GAIN\" in mf) and (\"REPLAYGAIN_ALBUM_PEAK\" in mf)\n\n  elif (isinstance(mf.tags, mutagen.mp4.MP4Tags) or\n        isinstance(mf, mutagen.mp4.MP4)):\n    track = (\"----:COM.APPLE.ITUNES:REPLAYGAIN_TRACK_GAIN\" in mf) and (\"----:COM.APPLE.ITUNES:REPLAYGAIN_TRACK_PEAK\" in mf)\n    album = (\"----:COM.APPLE.ITUNES:REPLAYGAIN_ALBUM_GAIN\" in mf) and (\"----:COM.APPLE.ITUNES:REPLAYGAIN_ALBUM_PEAK\" in mf)\n\n  else:\n    logger().warning(\"Unhandled '%s' tag format for file '%s'\" % (mf.__class__.__name__,\n                                                                  filepath))\n    return\n\n  return track, album"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_scan_report(audio_filepaths, album_dir, r128_data):\n  # track loudness/peak\n  for audio_filepath in audio_filepaths:\n    try:\n      loudness, peak = r128_data[audio_filepath]\n    except KeyError:\n      loudness, peak = \"SKIPPED\", \"SKIPPED\"\n    else:\n      loudness = \"%.1f LUFS\" % (loudness)\n      if peak is None:\n        peak = \"-\"\n      else:\n        peak = \"%.1f dBFS\" % (scale_to_gain(peak))\n    logger().info(\"File '%s': loudness = %s, sample peak = %s\" % (audio_filepath, loudness, peak))\n\n  # album loudness/peak\n  if album_dir:\n    try:\n      album_loudness, album_peak = r128_data[ALBUM_GAIN_KEY]\n    except KeyError:\n      album_loudness, album_peak = \"SKIPPED\", \"SKIPPED\"\n    else:\n      album_loudness = \"%.1f LUFS\" % (album_loudness)\n      if album_peak is None:\n        album_peak = \"-\"\n      else:\n        album_peak = \"%.1f dBFS\" % (scale_to_gain(album_peak))\n    logger().info(\"Album '%s': loudness = %s, sample peak = %s\" % (album_dir, album_loudness, album_peak))", "response": "Display loudness scan results."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_requirements(source):\n\n    install_reqs = parse_requirements(filename=source, session=PipSession())\n\n    return [str(ir.req) for ir in install_reqs]", "response": "Get the requirements from the given source"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest if the given response contains the cloudflare s anti - bot protection", "response": "def is_cloudflare_challenge(response):\n        \"\"\"Test if the given response contains the cloudflare's anti-bot protection\"\"\"\n\n        return (\n            response.status == 503\n            and response.headers.get('Server', '').startswith(b'cloudflare')\n            and 'jschl_vc' in response.text\n            and 'jschl_answer' in response.text\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_response(self, request, response, spider):\n\n        if not self.is_cloudflare_challenge(response):\n            return response\n\n        logger = logging.getLogger('cloudflaremiddleware')\n\n        logger.debug(\n            'Cloudflare protection detected on %s, trying to bypass...',\n            response.url\n        )\n\n        cloudflare_tokens, __ = get_tokens(\n            request.url,\n            user_agent=spider.settings.get('USER_AGENT')\n        )\n\n        logger.debug(\n            'Successfully bypassed the protection for %s, re-scheduling the request',\n            response.url\n        )\n\n        request.cookies.update(cloudflare_tokens)\n        request.priority = 99999\n\n        return request", "response": "Handle a Scrapy response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride this method for processing captcha.", "response": "async def enter_captcha(self, url: str, sid: str) -> str:\n        \"\"\"\n        Override this method for processing captcha.\n\n        :param url: link to captcha image\n        :param sid: captcha id. I do not know why pass here but may be useful\n        :return captcha value\n        \"\"\"\n        raise VkCaptchaNeeded(url, sid)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a new token from server", "response": "async def authorize(self) -> None:\n        \"\"\"Getting a new token from server\"\"\"\n        html = await self._get_auth_page()\n        url = URL('/authorize?email')\n        for step in range(self.num_of_attempts):\n            if url.path == '/authorize' and 'email' in url.query:\n                # Invalid login or password  and 'email' in q.query\n                url, html = await self._process_auth_form(html)\n            if url.path == '/login' and url.query.get('act', '') == 'authcheck':\n                # Entering 2auth code\n                url, html = await self._process_2auth_form(html)\n            if url.path == '/login' and url.query.get('act', '') == 'authcheck_code':\n                # Need captcha\n                url, html = await self._process_auth_form(html)\n            if url.path == '/authorize' and '__q_hash' in url.query:\n                # Give rights for app\n                url, html = await self._process_access_form(html)\n            if url.path == '/blank.html':\n                # Success\n                self.access_token = url.query['access_token']\n                return\n        raise VkAuthError('Something went wrong', 'Exceeded the number of attempts to log in')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _get_auth_page(self) -> str:\n        # Prepare request\n        params = {\n            'client_id': self.app_id,\n            'redirect_uri': 'https://oauth.vk.com/blank.html',\n            'display': 'mobile',\n            'response_type': 'token',\n            'v': self.API_VERSION\n        }\n        if self.scope:\n            params['scope'] = self.scope\n\n        # Send request\n        status, response = await self.driver.get_text(self.AUTH_URL, params)\n\n        # Process response\n        if status != 200:\n            error_dict = json.loads(response)\n            raise VkAuthError(error_dict['error'], error_dict['error_description'], self.AUTH_URL, params)\n        return response", "response": "Get authorization mobile page without js js\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _process_auth_form(self, html: str) -> (str, str):\n        # Parse page\n        p = AuthPageParser()\n        p.feed(html)\n        p.close()\n\n        # Get data from hidden inputs\n        form_data = dict(p.inputs)\n        form_url = p.url\n        form_data['email'] = self.login\n        form_data['pass'] = self.password\n        if p.message:\n            # Show form errors\n            raise VkAuthError('invalid_data', p.message, form_url, form_data)\n        elif p.captcha_url:\n            form_data['captcha_key'] = await self.enter_captcha(\n                \"https://m.vk.com{}\".format(p.captcha_url),\n                form_data['captcha_sid']\n            )\n            form_url = \"https://m.vk.com{}\".format(form_url)\n\n        # Send request\n        url, html = await self.driver.post_text(form_url, form_data)\n        return url, html", "response": "Process the authorization page and fill the form with data from hidden inputs and submit the form"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _process_2auth_form(self, html: str) -> (str, str):\n        # Parse page\n        p = TwoFactorCodePageParser()\n        p.feed(html)\n        p.close()\n\n        # Prepare request data\n        form_url = p.url\n        form_data = dict(p.inputs)\n        form_data['remember'] = 0\n        if p.message:\n            raise VkAuthError('invalid_data', p.message, form_url, form_data)\n        form_data['code'] = await self.enter_confirmation_code()\n\n        # Send request\n        url, html = await self.driver.post_text(form_url, form_data)\n        return url, html", "response": "Process the two - factor authorization page and fill the code\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing the access form and return the url and html from redirected page with access rights.", "response": "async def _process_access_form(self, html: str) -> (str, str):\n        \"\"\"\n        Parsing page with access rights\n\n        :param html: html page\n        :return: url and  html from redirected page\n        \"\"\"\n        # Parse page\n        p = AccessPageParser()\n        p.feed(html)\n        p.close()\n\n        form_url = p.url\n        form_data = dict(p.inputs)\n\n        # Send request\n        url, html = await self.driver.post_text(form_url, form_data)\n        return url, html"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def authorize(self, code: str=None) -> None:\n        code = await self.get_code(code)\n        params = {\n            'client_id': self.app_id,\n            'client_secret': self.app_secret,\n            'redirect_uri': self.redirect_uri,\n            'code': code\n        }\n        response = await self.driver.json(self.CODE_URL, params, self.timeout)\n        if 'error' in response:\n            raise VkAuthError(response['error'], response['error_description'], self.CODE_URL, params)\n        self.access_token = response['access_token']", "response": "Get a new token from server"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def wait(self, need_pts=False) -> dict:\n        if not self.base_url:\n            await self._get_long_poll_server(need_pts)\n\n        params = {\n            'ts': self.ts,\n            'key': self.key,\n        }\n        params.update(self.base_params)\n        # invalid mimetype from server\n        code, response = await self.api._session.driver.get_text(\n            self.base_url, params,\n            timeout=2 * self.base_params['wait']\n        )\n\n        if code == 403:\n            raise VkLongPollError(403, 'smth weth wrong', self.base_url + '/', params)\n\n        response = json.loads(response)\n        failed = response.get('failed')\n\n        if not failed:\n            self.ts = response['ts']\n            return response\n\n        if failed == 1:\n            self.ts = response['ts']\n        elif failed == 4:\n            raise VkLongPollError(\n                4,\n                'An invalid version number was passed in the version parameter',\n                self.base_url + '/',\n                params\n            )\n        else:\n            self.base_url = None\n\n        return await self.wait()", "response": "Send long poll request to the VkLongPoll API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pdf(alphas):\n    '''Returns a Dirichlet PDF function'''\n    alphap = alphas - 1\n    c = np.exp(gammaln(alphas.sum()) - gammaln(alphas).sum())\n    def dirichlet(xs):\n        '''N x K array'''\n        return c * (xs**alphap).prod(axis=1)\n    return dirichlet", "response": "Returns a Dirichlet PDF function"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmean and precision of Dirichlet distribution.", "response": "def meanprecision(a):\n    '''Mean and precision of Dirichlet distribution.\n\n    Parameters\n    ----------\n    a : array\n        Parameters of Dirichlet distribution.\n\n    Returns\n    -------\n    mean : array\n        Numbers [0,1] of the means of the Dirichlet distribution.\n    precision : float\n        Precision or concentration parameter of the Dirichlet distribution.'''\n\n    s = a.sum()\n    m = a / s\n    return (m,s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loglikelihood(D, a):\n    '''Compute log likelihood of Dirichlet distribution, i.e. log p(D|a).\n\n    Parameters\n    ----------\n    D : 2D array\n        where ``N`` is the number of observations, ``K`` is the number of\n        parameters for the Dirichlet distribution.\n    a : array\n        Parameters for the Dirichlet distribution.\n\n    Returns\n    -------\n    logl : float\n        The log likelihood of the Dirichlet distribution'''\n    N, K = D.shape\n    logp = log(D).mean(axis=0)\n    return N*(gammaln(a.sum()) - gammaln(a).sum() + ((a - 1)*logp).sum())", "response": "Compute log likelihood of Dirichlet distribution i. e. log p ( D | a )."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _fixedpoint(D, tol=1e-7, maxiter=None):\n    '''Simple fixed point iteration method for MLE of Dirichlet distribution'''\n    N, K = D.shape\n    logp = log(D).mean(axis=0)\n    a0 = _init_a(D)\n\n    # Start updating\n    if maxiter is None:\n        maxiter = MAXINT\n    for i in xrange(maxiter):\n        a1 = _ipsi(psi(a0.sum()) + logp)\n        # if norm(a1-a0) < tol:\n        if abs(loglikelihood(D, a1)-loglikelihood(D, a0)) < tol: # much faster\n            return a1\n        a0 = a1\n    raise Exception('Failed to converge after {} iterations, values are {}.'\n                    .format(maxiter, a1))", "response": "Simple fixed point iteration method for MLE of Dirichlet distribution"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmeans and precision alternating method for MLE of Dirichlet distribution", "response": "def _meanprecision(D, tol=1e-7, maxiter=None):\n    '''Mean and precision alternating method for MLE of Dirichlet\n    distribution'''\n    N, K = D.shape\n    logp = log(D).mean(axis=0)\n    a0 = _init_a(D)\n    s0 = a0.sum()\n    if s0 < 0:\n        a0 = a0/s0\n        s0 = 1\n    elif s0 == 0:\n        a0 = ones(a.shape) / len(a)\n        s0 = 1\n    m0 = a0/s0\n\n    # Start updating\n    if maxiter is None:\n        maxiter = MAXINT\n    for i in xrange(maxiter):\n        a1 = _fit_s(D, a0, logp, tol=tol)\n        s1 = sum(a1)\n        a1 = _fit_m(D, a1, logp, tol=tol)\n        m = a1/s1\n        # if norm(a1-a0) < tol:\n        if abs(loglikelihood(D, a1)-loglikelihood(D, a0)) < tol: # much faster\n            return a1\n        a0 = a1\n    raise Exception('Failed to converge after {} iterations, values are {}.'\n                    .format(maxiter, a1))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassume a fixed mean for Dirichlet distribution maximize likelihood for preicision a. k. a. s", "response": "def _fit_s(D, a0, logp, tol=1e-7, maxiter=1000):\n    '''Assuming a fixed mean for Dirichlet distribution, maximize likelihood\n    for preicision a.k.a. s'''\n    N, K = D.shape\n    s1 = a0.sum()\n    m = a0 / s1\n    mlogp = (m*logp).sum()\n    for i in xrange(maxiter):\n        s0 = s1\n        g = psi(s1) - (m*psi(s1*m)).sum() + mlogp\n        h = _trigamma(s1) - ((m**2)*_trigamma(s1*m)).sum()\n\n        if g + s1 * h < 0:\n            s1 = 1/(1/s0 + g/h/(s0**2))\n        if s1 <= 0:\n            s1 = s0 * exp(-g/(s0*h + g)) # Newton on log s\n        if s1 <= 0:\n            s1 = 1/(1/s0 + g/((s0**2)*h + 2*s0*g)) # Newton on 1/s\n        if s1 <= 0:\n            s1 = s0 - g/h # Newton\n        if s1 <= 0:\n            raise Exception('Unable to update s from {}'.format(s0))\n\n        a = s1 * m\n        if abs(s1 - s0) < tol:\n            return a\n\n    raise Exception('Failed to converge after {} iterations, s is {}'\n            .format(maxiter, s1))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fit_m(D, a0, logp, tol=1e-7, maxiter=1000):\n    '''With fixed precision s, maximize mean m'''\n    N,K = D.shape\n    s = a0.sum()\n\n    for i in xrange(maxiter):\n        m = a0 / s\n        a1 = _ipsi(logp + (m*(psi(a0) - logp)).sum())\n        a1 = a1/a1.sum() * s\n\n        if norm(a1 - a0) < tol:\n            return a1\n        a0 = a1\n\n    raise Exception('Failed to converge after {} iterations, s is {}'\n            .format(maxiter, s))", "response": "With fixed precision s maximize mean m"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _piecewise(x, condlist, funclist, *args, **kw):\n    '''Fixed version of numpy.piecewise for 0-d arrays'''\n    x = asanyarray(x)\n    n2 = len(funclist)\n    if isscalar(condlist) or \\\n            (isinstance(condlist, np.ndarray) and condlist.ndim == 0) or \\\n            (x.ndim > 0 and condlist[0].ndim == 0):\n        condlist = [condlist]\n    condlist = [asarray(c, dtype=bool) for c in condlist]\n    n = len(condlist)\n\n    zerod = False\n    # This is a hack to work around problems with NumPy's\n    #  handling of 0-d arrays and boolean indexing with\n    #  numpy.bool_ scalars\n    if x.ndim == 0:\n        x = x[None]\n        zerod = True\n        newcondlist = []\n        for k in range(n):\n            if condlist[k].ndim == 0:\n                condition = condlist[k][None]\n            else:\n                condition = condlist[k]\n            newcondlist.append(condition)\n        condlist = newcondlist\n\n    if n == n2-1:  # compute the \"otherwise\" condition.\n        totlist = condlist[0]\n        for k in range(1, n):\n            totlist |= condlist[k]\n        condlist.append(~totlist)\n        n += 1\n    if (n != n2):\n        raise ValueError(\n                \"function list and condition list must be the same\")\n\n    y = zeros(x.shape, x.dtype)\n    for k in range(n):\n        item = funclist[k]\n        if not callable(item):\n            y[condlist[k]] = item\n        else:\n            vals = x[condlist[k]]\n            if vals.size > 0:\n                y[condlist[k]] = item(vals, *args, **kw)\n    if zerod:\n        y = y.squeeze()\n    return y", "response": "Fixed version of numpy. piecewise for 0 - d arrays"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _init_a(D):\n    '''Initial guess for Dirichlet alpha parameters given data D'''\n    E = D.mean(axis=0)\n    E2 = (D**2).mean(axis=0)\n    return ((E[0] - E2[0])/(E2[0]-E[0]**2)) * E", "response": "Initial guess for Dirichlet alpha parameters given data D"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninverses of psi using Newton s method.", "response": "def _ipsi(y, tol=1.48e-9, maxiter=10):\n    '''Inverse of psi (digamma) using Newton's method. For the purposes\n    of Dirichlet MLE, since the parameters a[i] must always\n    satisfy a > 0, we define ipsi :: R -> (0,inf).'''\n    y = asanyarray(y, dtype='float')\n    x0 = _piecewise(y, [y >= -2.22, y < -2.22],\n            [(lambda x: exp(x) + 0.5), (lambda x: -1/(x+euler))])\n    for i in xrange(maxiter):\n        x1 = x0 - (psi(x0) - y)/_trigamma(x0)\n        if norm(x1 - x0) < tol:\n            return x1\n        x0 = x1\n    raise Exception(\n        'Unable to converge in {} iterations, value is {}'.format(maxiter, x1))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cartesian(points):\n    '''Converts array of barycentric coordinates on a 2-simplex to an array of\n    Cartesian coordinates on a 2D triangle in the first quadrant, i.e.::\n\n        >>> cartesian((1,0,0))\n        array([0, 0])\n        >>> cartesian((0,1,0))\n        array([0, 1])\n        >>> cartesian((0,0,1))\n        array([0.5, 0.8660254037844386]) # == [0.5, sqrt(3)/2]\n\n    :param points: Points on a 2-simplex.\n    :type points: N x 3 list or ndarray.\n    :returns: Cartesian coordinate points.\n    :rtype: N x 2 ndarray.'''\n    points = np.asanyarray(points)\n    ndim = points.ndim # will use this to have similar output shape to input\n    if ndim == 1:\n        points = points.reshape((1,points.size))\n    d = points.sum(axis=1) # in case values aren't normalized\n    x = 0.5*(2*points[:,1] + points[:,2])/d\n    y = (np.sqrt(3.0)/2) * points[:,2]/d\n    out = np.vstack([x,y]).T\n    if ndim == 1:\n        return out.reshape((2,))\n    return out", "response": "Converts array of barycentric coordinates on a 2 - simplex to array of barycentric coordinates on a 2D triangle in the first quadrant."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nscatters plot of barycentric 2 - simplex points on a 2D triangle.", "response": "def scatter(points, vertexlabels=None, **kwargs):\n    '''Scatter plot of barycentric 2-simplex points on a 2D triangle.\n\n    :param points: Points on a 2-simplex.\n    :type points: N x 3 list or ndarray.\n    :param vertexlabels: Labels for corners of plot in the order\n        ``(a, b, c)`` where ``a == (1,0,0)``, ``b == (0,1,0)``,\n        ``c == (0,0,1)``.\n    :type vertexlabels: 3-tuple of strings.\n    :param **kwargs: Arguments to :func:`plt.scatter`.\n    :type **kwargs: keyword arguments.'''\n    if vertexlabels is None:\n        vertexlabels = ('1','2','3')\n\n    projected = cartesian(points)\n    plt.scatter(projected[:,0], projected[:,1], **kwargs)\n\n    _draw_axes(vertexlabels)\n    return plt.gcf()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncontouring line plot on a 2D triangle of a function evaluated at barycentric 2 - simplex points.", "response": "def contour(f, vertexlabels=None, **kwargs):\n    '''Contour line plot on a 2D triangle of a function evaluated at\n    barycentric 2-simplex points.\n\n    :param f: Function to evaluate on N x 3 ndarray of coordinates\n    :type f: ``ufunc``\n    :param vertexlabels: Labels for corners of plot in the order\n        ``(a, b, c)`` where ``a == (1,0,0)``, ``b == (0,1,0)``,\n        ``c == (0,0,1)``.\n    :type vertexlabels: 3-tuple of strings.\n    :param **kwargs: Arguments to :func:`plt.tricontour`.\n    :type **kwargs: keyword arguments.'''\n    return _contour(f, vertexlabels, contourfunc=plt.tricontour, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfilling contour plot on a 2D triangle of a function evaluated at barycentric 2 - simplex points.", "response": "def contourf(f, vertexlabels=None, **kwargs):\n    '''Filled contour plot on a 2D triangle of a function evaluated at\n    barycentric 2-simplex points.\n\n    Function signature is identical to :func:`contour` with the caveat that\n    ``**kwargs`` are passed on to :func:`plt.tricontourf`.'''\n    return _contour(f, vertexlabels, contourfunc=plt.tricontourf, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disable_on_env(func):\n    @wraps(func)\n    def func_wrapper(*args, **kwargs):\n        # pylint: disable=C0111, C0103\n        function_name = func.__name__\n        VALIDATORS_DISABLED = os.getenv('VALIDATORS_DISABLED', '')\n        disabled_functions = [x.strip() for x in VALIDATORS_DISABLED.split(',')]\n\n        force_run = kwargs.get('force_run', False)\n\n        try:\n            value = args[0]\n        except IndexError:\n            raise ValidatorUsageError('no value was supplied')\n\n        if function_name in disabled_functions and not force_run:\n            return value\n        else:\n            updated_kwargs = {key : kwargs[key]\n                              for key in kwargs\n                              if key != 'force_run'}\n            return func(*args, **updated_kwargs)\n\n    return func_wrapper", "response": "Disable the function if its name is present in VALIDATORS_DISABLED."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisable the function if its name is present in CHECKERS_DISABLED.", "response": "def disable_checker_on_env(func):\n    \"\"\"Disable the ``func`` called if its name is present in ``CHECKERS_DISABLED``.\n\n    :param func: The function/validator to be disabled.\n    :type func: callable\n\n    :returns: If disabled, ``True``. If enabled, the result of ``func``.\n\n    \"\"\"\n    @wraps(func)\n    def func_wrapper(*args, **kwargs):\n        # pylint: disable=C0111, C0103\n        function_name = func.__name__\n        CHECKERS_DISABLED = os.getenv('CHECKERS_DISABLED', '')\n        disabled_functions = [x.strip() for x in CHECKERS_DISABLED.split(',')]\n\n        force_run = kwargs.get('force_run', False)\n\n        if function_name in disabled_functions and not force_run:\n            return True\n        else:\n            return func(*args, **kwargs)\n\n    return func_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that value is a valid UUID.", "response": "def uuid(value,\n         allow_empty = False,\n         **kwargs):\n    \"\"\"Validate that ``value`` is a valid :class:`UUID <python:uuid.UUID>`.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` coerced to a :class:`UUID <python:uuid.UUID>` object /\n      :obj:`None <python:None>`\n    :rtype: :class:`UUID <python:uuid.UUID>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` cannot be coerced to a\n      :class:`UUID <python:uuid.UUID>`\n\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if isinstance(value, uuid_.UUID):\n        return value\n\n    try:\n        value = uuid_.UUID(value)\n    except ValueError:\n        raise errors.CannotCoerceError('value (%s) cannot be coerced to a valid UUID')\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef string(value,\n           allow_empty = False,\n           coerce_value = False,\n           minimum_length = None,\n           maximum_length = None,\n           whitespace_padding = False,\n           **kwargs):\n    \"\"\"Validate that ``value`` is a valid string.\n\n    :param value: The value to validate.\n    :type value: :class:`str <python:str>` / :obj:`None <python:None>`\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if ``value``\n      is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>` if\n      ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param coerce_value: If ``True``, will attempt to coerce ``value`` to a string if\n      it is not already. If ``False``, will raise a :class:`ValueError` if ``value``\n      is not a string. Defaults to ``False``.\n    :type coerce_value: :class:`bool <python:bool>`\n\n    :param minimum_length: If supplied, indicates the minimum number of characters\n      needed to be valid.\n    :type minimum_length: :class:`int <python:int>`\n\n    :param maximum_length: If supplied, indicates the minimum number of characters\n      needed to be valid.\n    :type maximum_length: :class:`int <python:int>`\n\n    :param whitespace_padding: If ``True`` and the value is below the\n      ``minimum_length``, pad the value with spaces. Defaults to ``False``.\n    :type whitespace_padding: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`str <python:str>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` is not a valid string and ``coerce_value``\n      is ``False``\n    :raises MinimumLengthError: if ``minimum_length`` is supplied and the length of\n      ``value`` is less than ``minimum_length`` and ``whitespace_padding`` is\n      ``False``\n    :raises MaximumLengthError: if ``maximum_length`` is supplied and the length of\n      ``value`` is more than the ``maximum_length``\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    minimum_length = integer(minimum_length, allow_empty = True)\n    maximum_length = integer(maximum_length, allow_empty = True)\n\n    if coerce_value:\n        value = str(value)\n    elif not isinstance(value, basestring):\n        raise errors.CannotCoerceError('value (%s) was not coerced to a string' % value)\n\n    if value and maximum_length and len(value) > maximum_length:\n        raise errors.MaximumLengthError(\n            'value (%s) exceeds maximum length %s' % (value, maximum_length)\n        )\n\n    if value and minimum_length and len(value) < minimum_length:\n        if whitespace_padding:\n            value = value.ljust(minimum_length, ' ')\n        else:\n            raise errors.MinimumLengthError(\n                'value (%s) is below the minimum length %s' % (value, minimum_length)\n            )\n\n    return value", "response": "Validate that value is a valid string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate that value is a valid iterable.", "response": "def iterable(value,\n             allow_empty = False,\n             forbid_literals = (str, bytes),\n             minimum_length = None,\n             maximum_length = None,\n             **kwargs):\n    \"\"\"Validate that ``value`` is a valid iterable.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if ``value``\n      is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>` if\n      ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param forbid_literals: A collection of literals that will be considered invalid\n      even if they are (actually) iterable. Defaults to :class:`str <python:str>` and\n      :class:`bytes <python:bytes>`.\n    :type forbid_literals: iterable\n\n    :param minimum_length: If supplied, indicates the minimum number of members\n      needed to be valid.\n    :type minimum_length: :class:`int <python:int>`\n\n    :param maximum_length: If supplied, indicates the minimum number of members\n      needed to be valid.\n    :type maximum_length: :class:`int <python:int>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: iterable / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises NotAnIterableError: if ``value`` is not a valid iterable or\n      :obj:`None <python:None>`\n    :raises MinimumLengthError: if ``minimum_length`` is supplied and the length of\n      ``value`` is less than ``minimum_length`` and ``whitespace_padding`` is\n      ``False``\n    :raises MaximumLengthError: if ``maximum_length`` is supplied and the length of\n      ``value`` is more than the ``maximum_length``\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif value is None:\n        return None\n\n    minimum_length = integer(minimum_length, allow_empty = True, force_run = True) # pylint: disable=E1123\n    maximum_length = integer(maximum_length, allow_empty = True, force_run = True) # pylint: disable=E1123\n\n    if isinstance(value, forbid_literals) or not hasattr(value, '__iter__'):\n        raise errors.NotAnIterableError('value type (%s) not iterable' % type(value))\n\n    if value and minimum_length is not None and len(value) < minimum_length:\n        raise errors.MinimumLengthError(\n            'value has fewer items than the minimum length %s' % minimum_length\n        )\n\n    if value and maximum_length is not None and len(value) > maximum_length:\n        raise errors.MaximumLengthError(\n            'value has more items than the maximum length %s' % maximum_length\n        )\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates that value is None.", "response": "def none(value,\n         allow_empty = False,\n         **kwargs):\n    \"\"\"Validate that ``value`` is :obj:`None <python:None>`.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if ``value``\n      is empty but **not** :obj:`None <python:None>`. If  ``False``, raises a\n      :class:`NotNoneError` if ``value`` is empty but **not**\n      :obj:`None <python:None>`. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: :obj:`None <python:None>`\n\n    :raises NotNoneError: if ``allow_empty`` is ``False`` and ``value`` is empty\n      but **not** :obj:`None <python:None>` and\n\n    \"\"\"\n    if value is not None and not value and allow_empty:\n        pass\n    elif (value is not None and not value) or value:\n        raise errors.NotNoneError('value was not None')\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef not_empty(value,\n              allow_empty = False,\n              **kwargs):\n    \"\"\"Validate that ``value`` is not empty.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    \"\"\"\n    if not value and allow_empty:\n        return None\n    elif not value:\n        raise errors.EmptyValueError('value was empty')\n\n    return value", "response": "Validate that value is not empty."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef variable_name(value,\n                  allow_empty = False,\n                  **kwargs):\n    \"\"\"Validate that the value is a valid Python variable name.\n\n    .. caution::\n\n      This function does **NOT** check whether the variable exists. It only\n      checks that the ``value`` would work as a Python variable (or class, or\n      function, etc.) name.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`str <python:str>` or :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value``\n      is empty\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    try:\n        parse('%s = None' % value)\n    except (SyntaxError, ValueError, TypeError):\n        raise errors.InvalidVariableNameError(\n            'value (%s) is not a valid variable name' % value\n        )\n\n    return value", "response": "Validate that the value is a valid Python variable name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dict(value,\n         allow_empty = False,\n         json_serializer = None,\n         **kwargs):\n    \"\"\"Validate that ``value`` is a :class:`dict <python:dict>`.\n\n    .. hint::\n\n      If ``value`` is a string, this validator will assume it is a JSON\n      object and try to convert it into a :class:`dict <python:dict>`\n\n      You can override the JSON serializer used by passing it to the\n      ``json_serializer`` property. By default, will utilize the Python\n      :class:`json <json>` encoder/decoder.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param json_serializer: The JSON encoder/decoder to use to deserialize a\n      string passed in ``value``. If not supplied, will default to the Python\n      :class:`json <python:json>` encoder/decoder.\n    :type json_serializer: callable\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`dict <python:dict>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` cannot be coerced to a\n      :class:`dict <python:dict>`\n    :raises NotADictError: if ``value`` is not a :class:`dict <python:dict>`\n\n    \"\"\"\n    original_value = value\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if json_serializer is None:\n        json_serializer = json_\n\n    if isinstance(value, str):\n        try:\n            value = json_serializer.loads(value)\n        except Exception:\n            raise errors.CannotCoerceError(\n                'value (%s) cannot be coerced to a dict' % original_value\n            )\n\n        value = dict(value,\n                     json_serializer = json_serializer)\n\n    if not isinstance(value, dict_):\n        raise errors.NotADictError('value (%s) is not a dict' % original_value)\n\n    return value", "response": "Validate that value is a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates that value conforms to the supplied JSON Schema.", "response": "def json(value,\n         schema = None,\n         allow_empty = False,\n         json_serializer = None,\n         **kwargs):\n    \"\"\"Validate that ``value`` conforms to the supplied JSON Schema.\n\n    .. note::\n\n      ``schema`` supports JSON Schema Drafts 3 - 7. Unless the JSON Schema indicates the\n      meta-schema using a ``$schema`` property, the schema will be assumed to conform to\n      Draft 7.\n\n    .. hint::\n\n      If either ``value`` or ``schema`` is a string, this validator will assume it is a\n      JSON object and try to convert it into a :class:`dict <python:dict>`.\n\n      You can override the JSON serializer used by passing it to the\n      ``json_serializer`` property. By default, will utilize the Python\n      :class:`json <json>` encoder/decoder.\n\n    :param value: The value to validate.\n\n    :param schema: An optional JSON Schema against which ``value`` will be validated.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param json_serializer: The JSON encoder/decoder to use to deserialize a\n      string passed in ``value``. If not supplied, will default to the Python\n      :class:`json <python:json>` encoder/decoder.\n    :type json_serializer: callable\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`dict <python:dict>` / :class:`list <python:list>` of\n      :class:`dict <python:dict>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` cannot be coerced to a\n      :class:`dict <python:dict>`\n    :raises NotJSONError: if ``value`` cannot be deserialized from JSON\n    :raises NotJSONSchemaError: if ``schema`` is not a valid JSON Schema object\n    :raises JSONValidationError: if ``value`` does not validate against the JSON Schema\n\n    \"\"\"\n    original_value = value\n    original_schema = schema\n\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if not json_serializer:\n        json_serializer = json_\n\n    if isinstance(value, str):\n        try:\n            value = json_serializer.loads(value)\n        except Exception:\n            raise errors.CannotCoerceError(\n                'value (%s) cannot be deserialized from JSON' % original_value\n            )\n    if isinstance(schema, str):\n        try:\n            schema = dict(schema,\n                          allow_empty = allow_empty,\n                          json_serializer = json_serializer,\n                          **kwargs)\n        except Exception:\n            raise errors.CannotCoerceError(\n                'schema (%s) cannot be coerced to a dict' % original_schema\n            )\n\n    if not isinstance(value, (list, dict_)):\n        raise errors.NotJSONError('value (%s) is not a JSON object' % original_value)\n\n    if original_schema and not isinstance(schema, dict_):\n        raise errors.NotJSONError('schema (%s) is not a JSON object' % original_schema)\n\n    if not schema:\n        return value\n\n    try:\n        jsonschema.validate(value, schema)\n    except jsonschema.exceptions.ValidationError as error:\n        raise errors.JSONValidationError(error.message)\n    except jsonschema.exceptions.SchemaError as error:\n        raise errors.NotJSONSchemaError(error.message)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef date(value,\n         allow_empty = False,\n         minimum = None,\n         maximum = None,\n         coerce_value = True,\n         **kwargs):\n    \"\"\"Validate that ``value`` is a valid date.\n\n    :param value: The value to validate.\n    :type value: :class:`str <python:str>` / :class:`datetime <python:datetime.datetime>`\n      / :class:`date <python:datetime.date>` / :obj:`None <python:None>`\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param minimum: If supplied, will make sure that ``value`` is on or after this value.\n    :type minimum: :class:`datetime <python:datetime.datetime>` /\n      :class:`date <python:datetime.date>` / compliant :class:`str <python:str>`\n      / :obj:`None <python:None>`\n\n    :param maximum: If supplied, will make sure that ``value`` is on or before this\n      value.\n    :type maximum: :class:`datetime <python:datetime.datetime>` /\n      :class:`date <python:datetime.date>` / compliant :class:`str <python:str>`\n      / :obj:`None <python:None>`\n\n    :param coerce_value: If ``True``, will attempt to coerce ``value`` to a\n      :class:`date <python:datetime.date>` if it is a timestamp value. If ``False``,\n      will not.\n    :type coerce_value: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`date <python:datetime.date>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` cannot be coerced to a\n      :class:`date <python:datetime.date>` and is not :obj:`None <python:None>`\n    :raises MinimumValueError: if ``minimum`` is supplied but ``value`` occurs before\n      ``minimum``\n    :raises MaximumValueError: if ``maximum`` is supplied but ``value`` occurs after\n      ``maximum``\n\n    \"\"\"\n    # pylint: disable=too-many-branches\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    minimum = date(minimum, allow_empty = True, force_run = True)               # pylint: disable=E1123\n    maximum = date(maximum, allow_empty = True, force_run = True)               # pylint: disable=E1123\n\n    if not isinstance(value, date_types):\n        raise errors.CannotCoerceError(\n            'value (%s) must be a date object, datetime object, '\n            'ISO 8601-formatted string, '\n            'or POSIX timestamp, but was %s' % (value, type(value))\n        )\n    elif isinstance(value, datetime_.datetime) and not coerce_value:\n        raise errors.CannotCoerceError(\n            'value (%s) must be a date object, or '\n            'ISO 8601-formatted string, '\n            'but was %s' % (value, type(value))\n        )\n    elif isinstance(value, datetime_.datetime) and coerce_value:\n        value = value.date()\n    elif isinstance(value, timestamp_types) and coerce_value:\n        try:\n            value = datetime_.date.fromtimestamp(value)\n        except ValueError:\n            raise errors.CannotCoerceError(\n                'value (%s) must be a date object, datetime object, '\n                'ISO 8601-formatted string, '\n                'or POSIX timestamp, but was %s' % (value, type(value))\n            )\n    elif isinstance(value, str):\n        try:\n            value = datetime_.datetime.strptime(value, '%Y-%m-%dT%H:%M:%S.%f')\n            if coerce_value:\n                value = value.date()\n            else:\n                raise errors.CannotCoerceError(\n                    'value (%s) must be a date object, '\n                    'ISO 8601-formatted string, '\n                    'or POSIX timestamp, but was %s' % (value, type(value))\n                )\n        except ValueError:\n            if len(value) > 10 and not coerce_value:\n                raise errors.CannotCoerceError(\n                    'value (%s) must be a date object, or '\n                    'ISO 8601-formatted string, '\n                    'but was %s' % (value, type(value))\n                )\n            if ' ' in value:\n                value = value.split(' ')[0]\n            if 'T' in value:\n                value = value.split('T')[0]\n\n            if len(value) != 10:\n                raise errors.CannotCoerceError(\n                    'value (%s) must be a date object, datetime object, '\n                    'ISO 8601-formatted string, '\n                    'or POSIX timestamp, but was %s' % (value, type(value))\n                )\n            try:\n                year = int(value[:4])\n                month = int(value[5:7])\n                day = int(value[-2:])\n                value = datetime_.date(year, month, day)\n            except (ValueError, TypeError):\n                raise errors.CannotCoerceError(\n                    'value (%s) must be a date object, datetime object, '\n                    'ISO 8601-formatted string, '\n                    'or POSIX timestamp, but was %s' % (value, type(value))\n                )\n    elif isinstance(value, numeric_types) and not coerce_value:\n        raise errors.CannotCoerceError(\n            'value (%s) must be a date object, or '\n            'ISO 8601-formatted string, '\n            'but was %s' % (value, type(value))\n        )\n\n\n    if minimum and value and value < minimum:\n        raise errors.MinimumValueError(\n            'value (%s) is before the minimum given (%s)' % (value.isoformat(),\n                                                             minimum.isoformat())\n        )\n    if maximum and value and value > maximum:\n        raise errors.MaximumValueError(\n            'value (%s) is after the maximum given (%s)' % (value.isoformat(),\n                                                            maximum.isoformat())\n        )\n\n    return value", "response": "Validate that value is a valid date."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef datetime(value,\n             allow_empty = False,\n             minimum = None,\n             maximum = None,\n             coerce_value = True,\n             **kwargs):\n    \"\"\"Validate that ``value`` is a valid datetime.\n\n    .. caution::\n\n      If supplying a string, the string needs to be in an ISO 8601-format to pass\n      validation. If it is not in an ISO 8601-format, validation will fail.\n\n    :param value: The value to validate.\n    :type value: :class:`str <python:str>` / :class:`datetime <python:datetime.datetime>`\n      / :class:`date <python:datetime.date>` / :obj:`None <python:None>`\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param minimum: If supplied, will make sure that ``value`` is on or after this value.\n    :type minimum: :class:`datetime <python:datetime.datetime>` /\n      :class:`date <python:datetime.date>` / compliant :class:`str <python:str>` /\n      :obj:`None <python:None>`\n\n    :param maximum: If supplied, will make sure that ``value`` is on or before this\n      value.\n    :type maximum: :class:`datetime <python:datetime.datetime>` /\n      :class:`date <python:datetime.date>` / compliant :class:`str <python:str>` /\n      :obj:`None <python:None>`\n\n    :param coerce_value: If ``True``, will coerce dates to\n      :class:`datetime <python:datetime.datetime>` objects with times of 00:00:00. If ``False``, will error\n      if ``value`` is not an unambiguous timestamp. Defaults to ``True``.\n    :type coerce_value: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`datetime <python:datetime.datetime>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` cannot be coerced to a\n      :class:`datetime <python:datetime.datetime>` value and is not\n      :obj:`None <python:None>`\n    :raises MinimumValueError: if ``minimum`` is supplied but ``value`` occurs\n      before ``minimum``\n    :raises MaximumValueError: if ``maximum`` is supplied but ``value`` occurs\n      after ``minimum``\n\n    \"\"\"\n    # pylint: disable=too-many-branches\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    minimum = datetime(minimum, allow_empty = True, force_run = True)           # pylint: disable=E1123\n    maximum = datetime(maximum, allow_empty = True, force_run = True)           # pylint: disable=E1123\n\n    if not isinstance(value, datetime_types):\n        raise errors.CannotCoerceError(\n            'value (%s) must be a date object, datetime object, '\n            'ISO 8601-formatted string, '\n            'or POSIX timestamp, but was %s' % (value,\n                                                type(value))\n        )\n    elif isinstance(value, timestamp_types) and coerce_value:\n        try:\n            value = datetime_.datetime.fromtimestamp(value)\n        except ValueError:\n            raise errors.CannotCoerceError(\n                'value (%s) must be a date object, datetime object, '\n                'ISO 8601-formatted string, '\n                'or POSIX timestamp, but was %s' % (value,\n                                                    type(value))\n            )\n    elif isinstance(value, str):\n        # pylint: disable=line-too-long\n        try:\n            if 'T' in value:\n                value = datetime_.datetime.strptime(value, '%Y-%m-%dT%H:%M:%S.%f%z')\n            else:\n                value = datetime_.datetime.strptime(value, '%Y-%m-%d %H:%M:%S.%f%z')\n        except ValueError:\n            try:\n                if 'T' in value:\n                    value = datetime_.datetime.strptime(value, '%Y/%m/%dT%H:%M:%S%z')\n                else:\n                    value = datetime_.datetime.strptime(value, '%Y/%m/%d %H:%M:%S%z')\n            except ValueError:\n                try:\n                    if 'T' in value:\n                        value = datetime_.datetime.strptime(value, '%Y-%m-%dT%H:%M:%S%z')\n                    else:\n                        value = datetime_.datetime.strptime(value, '%Y-%m-%d %H:%M:%S%z')\n                except ValueError:\n                    try:\n                        if 'T' in value:\n                            value = datetime_.datetime.strptime(value,\n                                                                '%Y/%m/%dT%H:%M:%S%z')\n                        else:\n                            value = datetime_.datetime.strptime(value,\n                                                                '%Y/%m/%d %H:%M:%S%z')\n                    except ValueError:\n                        try:\n                            if 'T' in value:\n                                value = datetime_.datetime.strptime(value, '%Y-%m-%dT%H:%M:%S.%f')\n                            else:\n                                value = datetime_.datetime.strptime(value, '%Y-%m-%d %H:%M:%S.%f')\n                        except ValueError:\n                            try:\n                                if 'T' in value:\n                                    value = datetime_.datetime.strptime(value, '%Y/%m/%dT%H:%M:%S')\n                                else:\n                                    value = datetime_.datetime.strptime(value, '%Y/%m/%d %H:%M:%S')\n                            except ValueError:\n                                try:\n                                    if 'T' in value:\n                                        value = datetime_.datetime.strptime(value, '%Y-%m-%dT%H:%M:%S')\n                                    else:\n                                        value = datetime_.datetime.strptime(value, '%Y-%m-%d %H:%M:%S')\n                                except ValueError:\n                                    try:\n                                        if 'T' in value:\n                                            value = datetime_.datetime.strptime(value,\n                                                                                '%Y/%m/%dT%H:%M:%S')\n                                        else:\n                                            value = datetime_.datetime.strptime(value,\n                                                                                '%Y/%m/%d %H:%M:%S')\n                                    except ValueError:\n                                        if coerce_value:\n                                            value = date(value)\n                                        else:\n                                            raise errors.CannotCoerceError(\n                                                'value (%s) must be a datetime object, '\n                                                'ISO 8601-formatted string, '\n                                                'or POSIX timestamp' % value\n                                            )\n    # pylint: enable=line-too-long\n    elif isinstance(value, numeric_types) and not coerce_value:\n        raise errors.CannotCoerceError(\n            'value (%s) must be a datetime object, '\n            'ISO 8601-formatted string, '\n            'or POSIX timestamp' % value\n        )\n\n    if isinstance(value, datetime_.date) and not isinstance(value, datetime_.datetime):\n        if coerce_value:\n            value = datetime_.datetime(value.year,                                  # pylint: disable=R0204\n                                       value.month,\n                                       value.day,\n                                       0,\n                                       0,\n                                       0,\n                                       0)\n        else:\n            raise errors.CannotCoerceError(\n                'value (%s) must be a datetime object, '\n                'ISO 8601-formatted string, '\n                'or POSIX timestamp' % value\n            )\n\n\n    if minimum and value and value < minimum:\n        raise errors.MinimumValueError(\n            'value (%s) is before the minimum given (%s)' % (value.isoformat(),\n                                                             minimum.isoformat())\n        )\n    if maximum and value and value > maximum:\n        raise errors.MaximumValueError(\n            'value (%s) is after the maximum given (%s)' % (value.isoformat(),\n                                                            maximum.isoformat())\n        )\n\n    return value", "response": "Validate that value is a valid datetime."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating that value is a valid time.", "response": "def time(value,\n         allow_empty = False,\n         minimum = None,\n         maximum = None,\n         coerce_value = True,\n         **kwargs):\n    \"\"\"Validate that ``value`` is a valid :class:`time <python:datetime.time>`.\n\n    .. caution::\n\n      This validator will **always** return the time as timezone naive (effectively\n      UTC). If ``value`` has a timezone / UTC offset applied, the validator will\n      coerce the value returned back to UTC.\n\n    :param value: The value to validate.\n    :type value: :func:`datetime <validator_collection.validators.datetime>` or\n      :func:`time <validator_collection.validators.time>`-compliant\n      :class:`str <python:str>` / :class:`datetime <python:datetime.datetime>` /\n      :class:`time <python:datetime.time> / numeric / :obj:`None <python:None>`\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param minimum: If supplied, will make sure that ``value`` is on or after this value.\n    :type minimum: :func:`datetime <validator_collection.validators.datetime>` or\n      :func:`time <validator_collection.validators.time>`-compliant\n      :class:`str <python:str>` / :class:`datetime <python:datetime.datetime>` /\n      :class:`time <python:datetime.time> / numeric / :obj:`None <python:None>`\n\n    :param maximum: If supplied, will make sure that ``value`` is on or before this\n      value.\n    :type maximum: :func:`datetime <validator_collection.validators.datetime>` or\n      :func:`time <validator_collection.validators.time>`-compliant\n      :class:`str <python:str>` / :class:`datetime <python:datetime.datetime>` /\n      :class:`time <python:datetime.time> / numeric / :obj:`None <python:None>`\n\n    :param coerce_value: If ``True``, will attempt to coerce/extract a\n      :class:`time <python:datetime.time>` from ``value``. If ``False``, will only\n      respect direct representations of time. Defaults to ``True``.\n    :type coerce_value: :class:`bool <python:bool>`\n\n    :returns: ``value`` in UTC time / :obj:`None <python:None>`\n    :rtype: :class:`time <python:datetime.time>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` cannot be coerced to a\n      :class:`time <python:datetime.time>` and is not :obj:`None <python:None>`\n    :raises MinimumValueError: if ``minimum`` is supplied but ``value`` occurs\n      before ``minimum``\n    :raises MaximumValueError: if ``maximum`` is supplied but ``value`` occurs\n      after ``minimum``\n\n    \"\"\"\n    # pylint: disable=too-many-branches\n    if not value and not allow_empty:\n        if isinstance(value, datetime_.time):\n            pass\n        else:\n            raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        if not isinstance(value, datetime_.time):\n            return None\n\n    minimum = time(minimum, allow_empty = True, force_run = True)               # pylint: disable=E1123\n    maximum = time(maximum, allow_empty = True, force_run = True)               # pylint: disable=E1123\n\n    if not isinstance(value, time_types):\n        raise errors.CannotCoerceError(\n            'value (%s) must be a datetime object, '\n            'ISO 8601-formatted string, '\n            'or POSIX timestamp, but was %s' % (value,\n                                                type(value))\n        )\n    elif isinstance(value, datetime_.datetime) and not coerce_value:\n        raise errors.CannotCoerceError(\n            'value (%s) must be a datetime object, '\n            'ISO 8601-formatted string, '\n            'or POSIX timestamp, but was %s' % (value,\n                                                type(value))\n        )\n    elif isinstance(value, datetime_.datetime) and coerce_value:\n        value = value.time()\n    elif isinstance(value, timestamp_types):\n        try:\n            datetime_value = datetime(value, force_run = True)                  # pylint: disable=E1123\n            if coerce_value:\n                value = datetime_value.time()\n            else:\n                raise errors.CannotCoerceError(\n                    'value (%s) must be a time object, '\n                    'ISO 8601-formatted string, '\n                    'but was %s' % (value,\n                                    type(value))\n                )\n        except ValueError:\n            raise errors.CannotCoerceError(\n                'value (%s) must be a datetime object, '\n                'ISO 8601-formatted string, '\n                'or POSIX timestamp, but was %s' % (value,\n                                                    type(value))\n            )\n    elif isinstance(value, basestring):\n        is_value_calculated = False\n        if len(value) > 10:\n            try:\n                datetime_value = datetime(value, force_run = True)              # pylint: disable=E1123\n                if coerce_value:\n                    value = datetime_value.time()\n                else:\n                    raise errors.CannotCoerceError(\n                        'value (%s) must be a time object, '\n                        'ISO 8601-formatted string, '\n                        'but was %s' % (value,\n                                        type(value))\n                    )\n                is_value_calculated = True\n            except ValueError:\n                pass\n\n        if not is_value_calculated:\n            try:\n                if '+' in value:\n                    components = value.split('+')\n                    is_offset_positive = True\n                elif '-' in value:\n                    components = value.split('-')\n                    is_offset_positive = False\n                else:\n                    raise ValueError()\n\n                time_string = components[0]\n                if len(components) > 1:\n                    utc_offset = components[1]\n                else:\n                    utc_offset = None\n\n                time_components = time_string.split(':')\n                hour = int(time_components[0])\n                minutes = int(time_components[1])\n                seconds = time_components[2]\n                if '.' in seconds:\n                    second_components = seconds.split('.')\n                    seconds = int(second_components[0])\n                    microseconds = int(second_components[1])\n                else:\n                    microseconds = 0\n\n                utc_offset = timezone(utc_offset,                               # pylint: disable=E1123\n                                      allow_empty = True,\n                                      positive = is_offset_positive,\n                                      force_run = True)\n\n                value = datetime_.time(hour = hour,\n                                       minute = minutes,\n                                       second = seconds,\n                                       microsecond = microseconds,\n                                       tzinfo = utc_offset)\n            except (ValueError, TypeError, IndexError):\n                raise errors.CannotCoerceError(\n                    'value (%s) must be a datetime object, '\n                    'ISO 8601-formatted string, '\n                    'or POSIX timestamp, but was %s' % (value,\n                                                        type(value))\n                )\n\n        if value is not None:\n            value = value.replace(tzinfo = None)\n\n    if minimum is not None and value and value < minimum:\n        raise errors.MinimumValueError(\n            'value (%s) is before the minimum given (%s)' % (value.isoformat(),\n                                                             minimum.isoformat())\n        )\n    if maximum is not None and value and value > maximum:\n        raise errors.MaximumValueError(\n            'value (%s) is after the maximum given (%s)' % (value.isoformat(),\n                                                            maximum.isoformat())\n        )\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef timezone(value,\n             allow_empty = False,\n             positive = True,\n             **kwargs):\n    \"\"\"Validate that ``value`` is a valid :class:`tzinfo <python:datetime.tzinfo>`.\n\n    .. caution::\n\n      This does **not** verify whether the value is a timezone that actually\n      exists, nor can it resolve timezone names (e.g. ``'Eastern'`` or ``'CET'``).\n\n      For that kind of functionality, we recommend you utilize:\n      `pytz <https://pypi.python.org/pypi/pytz>`_\n\n    :param value: The value to validate.\n    :type value: :class:`str <python:str>` / :class:`tzinfo <python:datetime.tzinfo>`\n      / numeric / :obj:`None <python:None>`\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param positive: Indicates whether the ``value`` is positive or negative\n      (only has meaning if ``value`` is a string). Defaults to ``True``.\n    :type positive: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`tzinfo <python:datetime.tzinfo>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` cannot be coerced to\n      :class:`tzinfo <python:datetime.tzinfo>` and is not :obj:`None <python:None>`\n    :raises PositiveOffsetMismatchError: if ``positive`` is ``True``, but the offset\n      indicated by ``value`` is actually negative\n    :raises NegativeOffsetMismatchError: if ``positive`` is ``False``, but the offset\n      indicated by ``value`` is actually positive\n\n    \"\"\"\n    # pylint: disable=too-many-branches\n    original_value = value\n\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if not isinstance(value, tzinfo_types):\n        raise errors.CannotCoerceError(\n            'value (%s) must be a tzinfo, '\n            'UTC offset in seconds expressed as a number, '\n            'UTC offset expressed as string of form +HH:MM, '\n            'but was %s' % (value, type(value))\n        )\n    elif isinstance(value, datetime_.datetime):\n        value = value.tzinfo\n    elif isinstance(value, datetime_.date):\n        return None\n    elif isinstance(value, datetime_.time):\n        return value.tzinfo\n    elif isinstance(value, timestamp_types):\n        return None\n    elif isinstance(value, str):\n        if '+' not in value and '-' not in value:\n            try:\n                datetime_value = datetime(value, force_run = True)              # pylint: disable=E1123\n                return datetime_value.tzinfo\n            except TypeError:\n                raise errors.CannotCoerceError(\n                    'value (%s) must be a tzinfo, '\n                    'UTC offset in seconds expressed as a number, '\n                    'UTC offset expressed as string of form +HH:MM, '\n                    'but was %s' % (value, type(value))\n                )\n        elif '-' in value:\n            try:\n                datetime_value = datetime(value, force_run = True)              # pylint: disable=E1123\n                return datetime_value.tzinfo\n            except TypeError:\n                pass\n\n        if '+' in value and not positive:\n            raise errors.NegativeOffsetMismatchError(\n                'expected a negative UTC offset but value is positive'\n            )\n        elif '-' in value and positive and len(value) == 6:\n            positive = False\n        elif '-' in value and positive:\n            raise errors.PositiveOffsetMismatchError(\n                'expected a positive UTC offset but value is negative'\n            )\n\n        if '+' in value:\n            value = value[value.find('+'):]\n        elif '-' in value:\n            value = value[value.rfind('-'):]\n\n        value = value[1:]\n\n        offset_components = value.split(':')\n        if len(offset_components) != 2:\n            raise errors.CannotCoerceError(\n                'value (%s) must be a tzinfo, '\n                'UTC offset in seconds expressed as a number, '\n                'UTC offset expressed as string of form +HH:MM, '\n                'but was %s' % (value, type(value))\n            )\n        hour = int(offset_components[0])\n        minutes = int(offset_components[1])\n\n        value = (hour * 60 * 60) + (minutes * 60)\n\n        if not positive:\n            value = 0 - value\n\n    if isinstance(value, numeric_types):\n        if value > 0:\n            positive = True\n        elif value < 0:\n            positive = False\n        elif value == 0:\n            return None\n\n        offset = datetime_.timedelta(seconds = value)\n        if is_py2:\n            value = TimeZone(offset = offset)\n        elif is_py3:\n            try:\n                value = TimeZone(offset)\n            except ValueError:\n                raise errors.UTCOffsetError(\n                    'value (%s) cannot exceed +/- 24h' % original_value\n                )\n        else:\n            raise NotImplementedError()\n\n    return value", "response": "Validate that the value is a valid timezone."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef numeric(value,\n            allow_empty = False,\n            minimum = None,\n            maximum = None,\n            **kwargs):\n    \"\"\"Validate that ``value`` is a numeric value.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if ``value``\n      is :obj:`None <python:None>`. If ``False``, raises an\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>` if\n      ``value`` is :obj:`None <python:None>`.\n      Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param minimum: If supplied, will make sure that ``value`` is greater than or\n      equal to this value.\n    :type minimum: numeric\n\n    :param maximum: If supplied, will make sure that ``value`` is less than or\n      equal to this value.\n    :type maximum: numeric\n\n    :returns: ``value`` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is :obj:`None <python:None>` and\n      ``allow_empty`` is ``False``\n    :raises MinimumValueError: if ``minimum`` is supplied and ``value`` is less\n      than the ``minimum``\n    :raises MaximumValueError: if ``maximum`` is supplied and ``value`` is more\n      than the ``maximum``\n    :raises CannotCoerceError: if ``value`` cannot be coerced to a numeric form\n\n    \"\"\"\n    if maximum is None:\n        maximum = POSITIVE_INFINITY\n    else:\n        maximum = numeric(maximum)\n    if minimum is None:\n        minimum = NEGATIVE_INFINITY\n    else:\n        minimum = numeric(minimum)\n\n    if value is None and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif value is not None:\n        if isinstance(value, str):\n            try:\n                value = float_(value)\n            except (ValueError, TypeError):\n                raise errors.CannotCoerceError(\n                    'value (%s) cannot be coerced to a numeric form' % value\n                )\n        elif not isinstance(value, numeric_types):\n            raise errors.CannotCoerceError(\n                'value (%s) is not a numeric type, was %s' % (value,\n                                                              type(value))\n            )\n\n    if value is not None and value > maximum:\n        raise errors.MaximumValueError(\n            'value (%s) exceeds maximum (%s)' % (value, maximum)\n        )\n\n    if value is not None and value < minimum:\n        raise errors.MinimumValueError(\n            'value (%s) less than minimum (%s)' % (value, minimum)\n        )\n\n    return value", "response": "Validate that value is a numeric value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef integer(value,\n            allow_empty = False,\n            coerce_value = False,\n            minimum = None,\n            maximum = None,\n            base = 10,\n            **kwargs):\n    \"\"\"Validate that ``value`` is an :class:`int <python:int>`.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is :obj:`None <python:None>`. If  ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>` if\n      ``value`` is :obj:`None <python:None>`.\n      Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param coerce_value: If ``True``, will force any numeric ``value`` to an integer\n      (always rounding up). If ``False``, will raise an error if ``value`` is numeric\n      but not a whole number. Defaults to ``False``.\n    :type coerce_value: :class:`bool <python:bool>`\n\n    :param minimum: If supplied, will make sure that ``value`` is greater than or\n      equal to this value.\n    :type minimum: numeric\n\n    :param maximum: If supplied, will make sure that ``value`` is less than or\n      equal to this value.\n    :type maximum: numeric\n\n    :param base: Indicates the base that is used to determine the integer value.\n      The allowed values are 0 and 2\u201336. Base-2, -8, and -16 literals can be\n      optionally prefixed with ``0b/0B``, ``0o/0O/0``, or ``0x/0X``, as with\n      integer literals in code. Base 0 means to interpret the string exactly as\n      an integer literal, so that the actual base is 2, 8, 10, or 16. Defaults to\n      ``10``.\n\n    :returns: ``value`` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is :obj:`None <python:None>` and\n      ``allow_empty`` is ``False``\n    :raises MinimumValueError: if ``minimum`` is supplied and ``value`` is less\n      than the ``minimum``\n    :raises MaximumValueError: if ``maximum`` is supplied and ``value`` is more\n      than the ``maximum``\n    :raises NotAnIntegerError: if ``coerce_value`` is ``False``, and ``value``\n      is not an integer\n    :raises CannotCoerceError: if ``value`` cannot be coerced to an\n      :class:`int <python:int>`\n\n    \"\"\"\n    value = numeric(value,                                                      # pylint: disable=E1123\n                    allow_empty = allow_empty,\n                    minimum = minimum,\n                    maximum = maximum,\n                    force_run = True)\n\n    if value is not None and hasattr(value, 'is_integer'):\n        if value.is_integer():\n            return int(value)\n\n    if value is not None and coerce_value:\n        float_value = math.ceil(value)\n        if is_py2:\n            value = int(float_value)                                            # pylint: disable=R0204\n        elif is_py3:\n            str_value = str(float_value)\n            value = int(str_value, base = base)\n        else:\n            raise NotImplementedError('Python %s not supported' % os.sys.version)\n    elif value is not None and not isinstance(value, integer_types):\n        raise errors.NotAnIntegerError('value (%s) is not an integer-type, '\n                                       'is a %s'% (value, type(value))\n                                      )\n\n    return value", "response": "Validates that the value is an integer value in the sequence of unambiguously increasing integers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef float(value,\n          allow_empty = False,\n          minimum = None,\n          maximum = None,\n          **kwargs):\n    \"\"\"Validate that ``value`` is a :class:`float <python:float>`.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is :obj:`None <python:None>`. If  ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>` if\n      ``value`` is :obj:`None <python:None>`. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`float <python:float>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is :obj:`None <python:None>` and\n      ``allow_empty`` is ``False``\n    :raises MinimumValueError: if ``minimum`` is supplied and ``value`` is less\n      than the ``minimum``\n    :raises MaximumValueError: if ``maximum`` is supplied and ``value`` is more\n      than the ``maximum``\n    :raises CannotCoerceError: if unable to coerce ``value`` to a\n      :class:`float <python:float>`\n\n    \"\"\"\n    try:\n        value = _numeric_coercion(value,\n                                  coercion_function = float_,\n                                  allow_empty = allow_empty,\n                                  minimum = minimum,\n                                  maximum = maximum)\n    except (errors.EmptyValueError,\n            errors.CannotCoerceError,\n            errors.MinimumValueError,\n            errors.MaximumValueError) as error:\n        raise error\n    except Exception as error:\n        raise errors.CannotCoerceError('unable to coerce value (%s) to float, '\n                                       'for an unknown reason - please see '\n                                       'stack trace' % value)\n\n    return value", "response": "Validate that value is a float."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates that value is a Fraction.", "response": "def fraction(value,\n             allow_empty = False,\n             minimum = None,\n             maximum = None,\n             **kwargs):\n    \"\"\"Validate that ``value`` is a :class:`Fraction <python:fractions.Fraction>`.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if ``value``\n      is :obj:`None <python:None>`. If  ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>` if\n      ``value`` is :obj:`None <python:None>`. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`Fraction <python:fractions.Fraction>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is :obj:`None <python:None>` and\n      ``allow_empty`` is ``False``\n    :raises MinimumValueError: if ``minimum`` is supplied and ``value`` is less\n      than the ``minimum``\n    :raises MaximumValueError: if ``maximum`` is supplied and ``value`` is more\n      than the ``maximum``\n    :raises CannotCoerceError: if unable to coerce ``value`` to a\n      :class:`Fraction <python:fractions.Fraction>`\n\n    \"\"\"\n    try:\n        value = _numeric_coercion(value,\n                                  coercion_function = fractions.Fraction,\n                                  allow_empty = allow_empty,\n                                  minimum = minimum,\n                                  maximum = maximum)\n    except (errors.EmptyValueError,\n            errors.CannotCoerceError,\n            errors.MinimumValueError,\n            errors.MaximumValueError) as error:\n        raise error\n    except Exception as error:\n        raise errors.CannotCoerceError('unable to coerce value (%s) to Fraction, '\n                                       'for an unknown reason - please see '\n                                       'stack trace' % value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decimal(value,\n            allow_empty = False,\n            minimum = None,\n            maximum = None,\n            **kwargs):\n    \"\"\"Validate that ``value`` is a :class:`Decimal <python:decimal.Decimal>`.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if ``value``\n      is :obj:`None <python:None>`. If  ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>` if\n      ``value`` is :obj:`None <python:None>`. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param minimum: If supplied, will make sure that ``value`` is greater than or\n      equal to this value.\n    :type minimum: numeric\n\n    :param maximum: If supplied, will make sure that ``value`` is less than or\n      equal to this value.\n    :type maximum: numeric\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`Decimal <python:decimal.Decimal>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is :obj:`None <python:None>` and\n      ``allow_empty`` is ``False``\n    :raises MinimumValueError: if ``minimum`` is supplied and ``value`` is less than the\n      ``minimum``\n    :raises MaximumValueError: if ``maximum`` is supplied and ``value`` is more than the\n      ``maximum``\n    :raises CannotCoerceError: if unable to coerce ``value`` to a\n      :class:`Decimal <python:decimal.Decimal>`\n\n    \"\"\"\n    if value is None and allow_empty:\n        return None\n    elif value is None:\n        raise errors.EmptyValueError('value cannot be None')\n\n    if isinstance(value, str):\n        try:\n            value = decimal_.Decimal(value.strip())\n        except decimal_.InvalidOperation:\n            raise errors.CannotCoerceError(\n                'value (%s) cannot be converted to a Decimal' % value\n            )\n    elif isinstance(value, fractions.Fraction):\n        try:\n            value = float(value, force_run = True)                              # pylint: disable=R0204, E1123\n        except ValueError:\n            raise errors.CannotCoerceError(\n                'value (%s) cannot be converted to a Decimal' % value\n            )\n\n    value = numeric(value,                                                      # pylint: disable=E1123\n                    allow_empty = False,\n                    maximum = maximum,\n                    minimum = minimum,\n                    force_run = True)\n\n    if not isinstance(value, decimal_.Decimal):\n        value = decimal_.Decimal(value)\n\n    return value", "response": "Validates that value is a Decimal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that value is numeric and coerce using coercion_function.", "response": "def _numeric_coercion(value,\n                      coercion_function = None,\n                      allow_empty = False,\n                      minimum = None,\n                      maximum = None):\n    \"\"\"Validate that ``value`` is numeric and coerce using ``coercion_function``.\n\n    :param value: The value to validate.\n\n    :param coercion_function: The function to use to coerce ``value`` to the desired\n      type.\n    :type coercion_function: callable\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is :obj:`None <python:None>`. If  ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>` if\n      ``value`` is :obj:`None <python:None>`. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: the type returned by ``coercion_function``\n\n    :raises CoercionFunctionEmptyError: if ``coercion_function`` is empty\n    :raises EmptyValueError: if ``value`` is :obj:`None <python:None>` and\n      ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``coercion_function`` raises an\n      :class:`ValueError <python:ValueError>`, :class:`TypeError <python:TypeError>`,\n      :class:`AttributeError <python:AttributeError>`,\n      :class:`IndexError <python:IndexError>, or\n      :class:`SyntaxError <python:SyntaxError>`\n\n    \"\"\"\n    if coercion_function is None:\n        raise errors.CoercionFunctionEmptyError('coercion_function cannot be empty')\n    elif not hasattr(coercion_function, '__call__'):\n        raise errors.NotCallableError('coercion_function must be callable')\n\n    value = numeric(value,                                                      # pylint: disable=E1123\n                    allow_empty = allow_empty,\n                    minimum = minimum,\n                    maximum = maximum,\n                    force_run = True)\n\n    if value is not None:\n        try:\n            value = coercion_function(value)\n        except (ValueError, TypeError, AttributeError, IndexError, SyntaxError):\n            raise errors.CannotCoerceError(\n                'cannot coerce value (%s) to desired type' % value\n            )\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating that value is a : class : io. BytesIO object.", "response": "def bytesIO(value,\n            allow_empty = False,\n            **kwargs):\n    \"\"\"Validate that ``value`` is a :class:`BytesIO <python:io.BytesIO>` object.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`BytesIO <python:io.BytesIO>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises NotBytesIOError: if ``value`` is not a :class:`BytesIO <python:io.BytesIO>`\n      object.\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if not isinstance(value, io.BytesIO):\n        raise errors.NotBytesIOError('value (%s) is not a BytesIO, '\n                                     'is a %s' % (value, type(value)))\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stringIO(value,\n             allow_empty = False,\n             **kwargs):\n    \"\"\"Validate that ``value`` is a :class:`StringIO <python:io.StringIO>` object.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`StringIO <python:io.StringIO>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises NotStringIOError: if ``value`` is not a :class:`StringIO <python:io.StringIO>`\n      object\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if not isinstance(value, io.StringIO):\n        raise ValueError('value (%s) is not an io.StringIO object, '\n                         'is a %s' % (value, type(value)))\n\n    return value", "response": "Validate that value is a string IO object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef path(value,\n         allow_empty = False,\n         **kwargs):\n    \"\"\"Validate that ``value`` is a valid path-like object.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: The path represented by ``value``.\n    :rtype: Path-like object / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value`` is empty\n    :raises NotPathlikeError: if ``value`` is not a valid path-like object\n\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if hasattr(os, 'PathLike'):\n        if not isinstance(value, (str, bytes, int, os.PathLike)):                    # pylint: disable=E1101\n            raise errors.NotPathlikeError('value (%s) is path-like' % value)\n    else:\n        if not isinstance(value, int):\n            try:\n                os.path.exists(value)\n            except TypeError:\n                raise errors.NotPathlikeError('value (%s) is not path-like' % value)\n\n    return value", "response": "Validate that value is a valid path - like object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate that value is a path - like object that exists on the local filesystem.", "response": "def path_exists(value,\n                allow_empty = False,\n                **kwargs):\n    \"\"\"Validate that ``value`` is a path-like object that exists on the local\n    filesystem.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: The file name represented by ``value``.\n    :rtype: Path-like object / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value``\n      is empty\n    :raises NotPathlikeError: if ``value`` is not a path-like object\n    :raises PathExistsError: if ``value`` does not exist\n\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    value = path(value, force_run = True)                                       # pylint: disable=E1123\n\n    if not os.path.exists(value):\n        raise errors.PathExistsError('value (%s) not found' % value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that value is a valid file that exists on the local filesystem.", "response": "def file_exists(value,\n                allow_empty = False,\n                **kwargs):\n    \"\"\"Validate that ``value`` is a valid file that exists on the local filesystem.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: The file name represented by ``value``.\n    :rtype: Path-like object / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value``\n      is empty\n    :raises NotPathlikeError: if ``value`` is not a path-like object\n    :raises PathExistsError: if ``value`` does not exist on the local filesystem\n    :raises NotAFileError: if ``value`` is not a valid file\n\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    value = path_exists(value, force_run = True)                                # pylint: disable=E1123\n\n    if not os.path.isfile(value):\n        raise errors.NotAFileError('value (%s) is not a file')\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that value is a valid directory that exists on the local filesystem.", "response": "def directory_exists(value,\n                     allow_empty = False,\n                     **kwargs):\n    \"\"\"Validate that ``value`` is a valid directory that exists on the local\n    filesystem.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: The file name represented by ``value``.\n    :rtype: Path-like object / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value``\n      is empty\n    :raises NotPathlikeError: if ``value`` is not a path-like object\n    :raises PathExistsError: if ``value`` does not exist on the local filesystem\n    :raises NotADirectoryError: if ``value`` is not a valid directory\n\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    value = path_exists(value, force_run = True)                                # pylint: disable=E1123\n\n    if not os.path.isdir(value):\n        raise errors.NotADirectoryError('value (%s) is not a directory' % value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating that value is a path to a readable file.", "response": "def readable(value,\n             allow_empty = False,\n             **kwargs):\n    \"\"\"Validate that ``value`` is a path to a readable file.\n\n    .. caution::\n\n      **Use of this validator is an anti-pattern and should be used with caution.**\n\n      Validating the readability of a file *before* attempting to read it\n      exposes your code to a bug called\n      `TOCTOU <https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use>`_.\n\n      This particular class of bug can expose your code to **security vulnerabilities**\n      and so this validator should only be used if you are an advanced user.\n\n      A better pattern to use when reading from a file is to apply the principle of\n      EAFP (\"easier to ask forgiveness than permission\"), and simply attempt to\n      write to the file using a ``try ... except`` block:\n\n      .. code-block:: python\n\n        try:\n            with open('path/to/filename.txt', mode = 'r') as file_object:\n                # read from file here\n        except (OSError, IOError) as error:\n            # Handle an error if unable to write.\n\n    :param value: The path to a file on the local filesystem whose readability\n      is to be validated.\n    :type value: Path-like object\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: Validated path-like object or :obj:`None <python:None>`\n    :rtype: Path-like object or :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value``\n      is empty\n    :raises NotPathlikeError: if ``value`` is not a path-like object\n    :raises PathExistsError: if ``value`` does not exist on the local filesystem\n    :raises NotAFileError: if ``value`` is not a valid file\n    :raises NotReadableError: if ``value`` cannot be opened for reading\n\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    value = file_exists(value, force_run = True)                                # pylint: disable=E1123\n\n    try:\n        with open(value, mode='r'):\n            pass\n    except (OSError, IOError):\n        raise errors.NotReadableError('file at %s could not be opened for '\n                                      'reading' % value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef writeable(value,\n              allow_empty = False,\n              **kwargs):\n    \"\"\"Validate that ``value`` is a path to a writeable file.\n\n    .. caution::\n\n      This validator does **NOT** work correctly on a Windows file system. This\n      is due to the vagaries of how Windows manages its file system and the\n      various ways in which it can manage file permission.\n\n      If called on a Windows file system, this validator will raise\n      :class:`NotImplementedError() <python:NotImplementedError>`.\n\n    .. caution::\n\n      **Use of this validator is an anti-pattern and should be used with caution.**\n\n      Validating the writability of a file *before* attempting to write to it\n      exposes your code to a bug called\n      `TOCTOU <https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use>`_.\n\n      This particular class of bug can expose your code to **security vulnerabilities**\n      and so this validator should only be used if you are an advanced user.\n\n      A better pattern to use when writing to file is to apply the principle of\n      EAFP (\"easier to ask forgiveness than permission\"), and simply attempt to\n      write to the file using a ``try ... except`` block:\n\n      .. code-block:: python\n\n        try:\n            with open('path/to/filename.txt', mode = 'a') as file_object:\n                # write to file here\n        except (OSError, IOError) as error:\n            # Handle an error if unable to write.\n\n    .. note::\n\n      This validator relies on :func:`os.access() <python:os.access>` to check\n      whether ``value`` is writeable. This function has certain limitations,\n      most especially that:\n\n      * It will **ignore** file-locking (yielding a false-positive) if the file\n        is locked.\n      * It focuses on *local operating system permissions*, which means if trying\n        to access a path over a network you might get a false positive or false\n        negative (because network paths may have more complicated authentication\n        methods).\n\n    :param value: The path to a file on the local filesystem whose writeability\n      is to be validated.\n    :type value: Path-like object\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: Validated absolute path or :obj:`None <python:None>`\n    :rtype: Path-like object or :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value``\n      is empty\n    :raises NotImplementedError: if used on a Windows system\n    :raises NotPathlikeError: if ``value`` is not a path-like object\n    :raises NotWriteableError: if ``value`` cannot be opened for writing\n\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    value = path(value, force_run = True)\n\n    if sys.platform in ['win32', 'cygwin']:\n        raise NotImplementedError('not supported on Windows')\n\n    is_valid = os.access(value, mode = os.W_OK)\n\n    if not is_valid:\n        raise errors.NotWriteableError('writing not allowed for file at %s' % value)\n\n    return value", "response": "Validate that value is a path to a writeable file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating that value is a path to an executable file.", "response": "def executable(value,\n               allow_empty = False,\n               **kwargs):\n    \"\"\"Validate that ``value`` is a path to an executable file.\n\n    .. caution::\n\n      This validator does **NOT** work correctly on a Windows file system. This\n      is due to the vagaries of how Windows manages its file system and the\n      various ways in which it can manage file permission.\n\n      If called on a Windows file system, this validator will raise\n      :class:`NotImplementedError() <python:NotImplementedError>`.\n\n    .. caution::\n\n      **Use of this validator is an anti-pattern and should be used with caution.**\n\n      Validating the executability of a file *before* attempting to execute it\n      exposes your code to a bug called\n      `TOCTOU <https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use>`_.\n\n      This particular class of bug can expose your code to **security vulnerabilities**\n      and so this validator should only be used if you are an advanced user.\n\n      A better pattern to use when writing to file is to apply the principle of\n      EAFP (\"easier to ask forgiveness than permission\"), and simply attempt to\n      execute the file using a ``try ... except`` block.\n\n    .. note::\n\n      This validator relies on :func:`os.access() <python:os.access>` to check\n      whether ``value`` is executable. This function has certain limitations,\n      most especially that:\n\n      * It will **ignore** file-locking (yielding a false-positive) if the file\n        is locked.\n      * It focuses on *local operating system permissions*, which means if trying\n        to access a path over a network you might get a false positive or false\n        negative (because network paths may have more complicated authentication\n        methods).\n\n    :param value: The path to a file on the local filesystem whose writeability\n      is to be validated.\n    :type value: Path-like object\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: Validated absolute path or :obj:`None <python:None>`\n    :rtype: Path-like object or :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value``\n      is empty\n    :raises NotImplementedError: if used on a Windows system\n    :raises NotPathlikeError: if ``value`` is not a path-like object\n    :raises NotAFileError: if ``value`` does not exist on the local file system\n    :raises NotExecutableError: if ``value`` cannot be executed\n\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    value = file_exists(value, force_run = True)\n\n    if sys.platform in ['win32', 'cygwin']:\n        raise NotImplementedError('not supported on Windows')\n\n    is_valid = os.access(value, mode = os.X_OK)\n\n    if not is_valid:\n        raise errors.NotExecutableError('execution not allowed for file at %s' % value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef email(value,\n          allow_empty = False,\n          **kwargs):\n    \"\"\"Validate that ``value`` is a valid email address.\n\n    .. note::\n\n      Email address validation is...complicated. The methodology that we have\n      adopted here is *generally* compliant with\n      `RFC 5322 <https://tools.ietf.org/html/rfc5322>`_ and uses a combination of\n      string parsing and regular expressions.\n\n      String parsing in particular is used to validate certain *highly unusual*\n      but still valid email patterns, including the use of escaped text and\n      comments within an email address' local address (the user name part).\n\n      This approach ensures more complete coverage for unusual edge cases, while\n      still letting us use regular expressions that perform quickly.\n\n    :param value: The value to validate.\n    :type value: :class:`str <python:str>` / :obj:`None <python:None>`\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`str <python:str>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` is not a :class:`str <python:str>` or\n      :obj:`None <python:None>`\n    :raises InvalidEmailError: if ``value`` is not a valid email address or\n      empty with ``allow_empty`` set to ``True``\n    \"\"\"\n    # pylint: disable=too-many-branches,too-many-statements,R0914\n\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if not isinstance(value, basestring):\n        raise errors.CannotCoerceError('value must be a valid string, '\n                                       'was %s' % type(value))\n\n    if '@' not in value:\n        raise errors.InvalidEmailError('value (%s) is not a valid email address' % value)\n    if '(' in value and ')' in value:\n        open_parentheses = value.find('(')\n        close_parentheses = value.find(')') + 1\n\n        if close_parentheses < open_parentheses:\n            raise errors.InvalidEmailError('value (%s) is not a valid email '\n                                           'address' % value)\n\n        commented_value = value[open_parentheses:close_parentheses]\n        value = value.replace(commented_value, '')\n    elif '(' in value:\n        raise errors.InvalidEmailError('value (%s) is not a valid email address' % value)\n    elif ')' in value:\n        raise errors.InvalidEmailError('value (%s) is not a valid email address' % value)\n\n    if '<' in value or '>' in value:\n        lt_position = value.find('<')\n        gt_position = value.find('>')\n        first_quote_position = -1\n        second_quote_position = -1\n\n        if lt_position >= 0:\n            first_quote_position = value.find('\"', 0, lt_position)\n        if gt_position >= 0:\n            second_quote_position = value.find('\"', gt_position)\n\n        if first_quote_position < 0 or second_quote_position < 0:\n            raise errors.InvalidEmailError('value (%s) is not a valid email '\n                                           'address' % value)\n\n    at_count = value.count('@')\n    if at_count > 1:\n        last_at_position = 0\n        last_quote_position = 0\n        for x in range(0, at_count):                                            # pylint: disable=W0612\n            at_position = value.find('@', last_at_position + 1)\n            if at_position >= 0:\n                first_quote_position = value.find('\"',\n                                                  last_quote_position,\n                                                  at_position)\n                second_quote_position = value.find('\"',\n                                                   first_quote_position)\n                if first_quote_position < 0 or second_quote_position < 0:\n                    raise errors.InvalidEmailError(\n                        'value (%s) is not a valid email address' % value\n                    )\n            last_at_position = at_position\n            last_quote_position = second_quote_position\n\n    split_values = value.split('@')\n    if len(split_values) < 2:\n        raise errors.InvalidEmailError('value (%s) is not a valid email address' % value)\n\n    local_value = ''.join(split_values[:-1])\n    domain_value = split_values[-1]\n    is_domain = False\n    is_ip = False\n    try:\n        if domain_value.startswith('[') and domain_value.endswith(']'):\n            domain_value = domain_value[1:-1]\n        domain(domain_value)\n        is_domain = True\n    except ValueError:\n        is_domain = False\n\n    if not is_domain:\n        try:\n            ip_address(domain_value, force_run = True)                          # pylint: disable=E1123\n            is_ip = True\n        except ValueError:\n            is_ip = False\n\n    if not is_domain and is_ip:\n        try:\n            email(local_value + '@test.com', force_run = True)                  # pylint: disable=E1123\n        except ValueError:\n            raise errors.InvalidEmailError('value (%s) is not a valid email '\n                                           'address' % value)\n\n        return value\n\n    if not is_domain:\n        raise errors.InvalidEmailError('value (%s) is not a valid email address' % value)\n    else:\n        is_valid = EMAIL_REGEX.search(value)\n\n        if not is_valid:\n            raise errors.InvalidEmailError('value (%s) is not a valid email '\n                                           'address' % value)\n\n        matched_string = is_valid.group(0)\n        position = value.find(matched_string)\n        if position > 0:\n            prefix = value[:position]\n            if prefix[0] in string_.punctuation:\n                raise errors.InvalidEmailError('value (%s) is not a valid email '\n                                               'address' % value)\n            if '..' in prefix:\n                raise errors.InvalidEmailError('value (%s) is not a valid email '\n                                               'address' % value)\n\n        end_of_match = position + len(matched_string)\n        suffix = value[end_of_match:]\n        if suffix:\n            raise errors.InvalidEmailError('value (%s) is not a valid email '\n                                           'address' % value)\n\n    return value", "response": "Validate that value is a valid email address."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate that value is a valid URL.", "response": "def url(value,\n        allow_empty = False,\n        allow_special_ips = False,\n        **kwargs):\n    \"\"\"Validate that ``value`` is a valid URL.\n\n    .. note::\n\n      URL validation is...complicated. The methodology that we have\n      adopted here is *generally* compliant with\n      `RFC 1738 <https://tools.ietf.org/html/rfc1738>`_,\n      `RFC 6761 <https://tools.ietf.org/html/rfc6761>`_,\n      `RFC 2181 <https://tools.ietf.org/html/rfc2181>`_  and uses a combination of\n      string parsing and regular expressions,\n\n      This approach ensures more complete coverage for unusual edge cases, while\n      still letting us use regular expressions that perform quickly.\n\n    :param value: The value to validate.\n    :type value: :class:`str <python:str>` / :obj:`None <python:None>`\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param allow_special_ips: If ``True``, will succeed when validating special IP\n      addresses, such as loopback IPs like ``127.0.0.1`` or ``0.0.0.0``. If ``False``,\n      will raise a :class:`InvalidURLError` if ``value`` is a special IP address. Defaults\n      to ``False``.\n    :type allow_special_ips: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`str <python:str>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` is not a :class:`str <python:str>` or\n      :obj:`None <python:None>`\n    :raises InvalidURLError: if ``value`` is not a valid URL or\n      empty with ``allow_empty`` set to ``True``\n\n    \"\"\"\n    is_recursive = kwargs.pop('is_recursive', False)\n\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if not isinstance(value, basestring):\n        raise errors.CannotCoerceError('value must be a valid string, '\n                                       'was %s' % type(value))\n\n    value = value.lower()\n\n    is_valid = False\n    stripped_value = None\n    for protocol in URL_PROTOCOLS:\n        if protocol in value:\n            stripped_value = value.replace(protocol, '')\n\n    for special_use_domain in SPECIAL_USE_DOMAIN_NAMES:\n        if special_use_domain in value:\n            if stripped_value:\n                try:\n                    domain(stripped_value,\n                           allow_empty = False,\n                           is_recursive = is_recursive)\n                    is_valid = True\n\n                except (ValueError, TypeError):\n                    pass\n\n    if not is_valid and allow_special_ips:\n        try:\n            ip_address(stripped_value, allow_empty = False)\n            is_valid = True\n        except (ValueError, TypeError):\n            pass\n\n    if not is_valid:\n        is_valid = URL_REGEX.match(value)\n\n    if not is_valid and allow_special_ips:\n        is_valid = URL_SPECIAL_IP_REGEX.match(value)\n\n    if not is_valid:\n        raise errors.InvalidURLError('value (%s) is not a valid URL' % value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that value is a valid domain name.", "response": "def domain(value,\n           allow_empty = False,\n           allow_ips = False,\n           **kwargs):\n    \"\"\"Validate that ``value`` is a valid domain name.\n\n    .. caution::\n\n      This validator does not verify that ``value`` **exists** as a domain. It\n      merely verifies that its contents *might* exist as a domain.\n\n    .. note::\n\n      This validator checks to validate that ``value`` resembles a valid\n      domain name. It is - generally - compliant with\n      `RFC 1035 <https://tools.ietf.org/html/rfc1035>`_ and\n      `RFC 6761 <https://tools.ietf.org/html/rfc6761>`_, however it diverges\n      in a number of key ways:\n\n        * Including authentication (e.g. ``username:password@domain.dev``) will\n          fail validation.\n        * Including a path (e.g. ``domain.dev/path/to/file``) will fail validation.\n        * Including a port (e.g. ``domain.dev:8080``) will fail validation.\n\n      If you are hoping to validate a more complete URL, we recommend that you\n      see :func:`url <validator_collection.validators.url>`.\n\n    .. hint::\n\n      Leading and trailing whitespace will be automatically stripped.\n\n    :param value: The value to validate.\n    :type value: :class:`str <python:str>` / :obj:`None <python:None>`\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :param allow_ips: If ``True``, will succeed when validating IP addresses,\n      If ``False``, will raise a :class:`InvalidDomainError` if ``value`` is an IP\n      address. Defaults to ``False``.\n    :type allow_ips: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`str <python:str>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` is not a :class:`str <python:str>` or\n      :obj:`None <python:None>`\n    :raises InvalidDomainError: if ``value`` is not a valid domain name or\n      empty with ``allow_empty`` set to ``True``\n    :raises SlashInDomainError: if ``value`` contains a slash or backslash\n    :raises AtInDomainError: if ``value`` contains an ``@`` symbol\n    :raises ColonInDomainError: if ``value`` contains a ``:`` symbol\n    :raises WhitespaceInDomainError: if ``value`` contains whitespace\n\n    \"\"\"\n    is_recursive = kwargs.pop('is_recursive', False)\n\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if not isinstance(value, basestring):\n        raise errors.CannotCoerceError('value must be a valid string, '\n                                       'was %s' % type(value))\n\n    if '/' in value:\n        raise errors.SlashInDomainError('valid domain name cannot contain \"/\"')\n    if '\\\\' in value:\n        raise errors.SlashInDomainError('valid domain name cannot contain \"\\\\\"')\n    if '@' in value:\n        raise errors.AtInDomainError('valid domain name cannot contain \"@\"')\n    if ':' in value:\n        raise errors.ColonInDomainError('valid domain name cannot contain \":\"')\n\n    value = value.strip().lower()\n\n    for item in string_.whitespace:\n        if item in value:\n            raise errors.WhitespaceInDomainError('valid domain name cannot contain '\n                                                 'whitespace')\n\n    if value in SPECIAL_USE_DOMAIN_NAMES:\n        return value\n\n    if allow_ips:\n        try:\n            ip_address(value, allow_empty = allow_empty)\n            is_valid = True\n        except (ValueError, TypeError, AttributeError):\n            is_valid = False\n\n        if is_valid:\n            return value\n\n    is_valid = DOMAIN_REGEX.match(value)\n\n    if not is_valid and not is_recursive:\n        with_prefix = 'http://' + value\n        try:\n            url(with_prefix, force_run = True, is_recursive = True)                                  # pylint: disable=E1123\n        except ValueError:\n            raise errors.InvalidDomainError('value (%s) is not a valid domain' % value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating that value is a valid IP address.", "response": "def ip_address(value,\n               allow_empty = False,\n               **kwargs):\n    \"\"\"Validate that ``value`` is a valid IP address.\n\n    .. note::\n\n      First, the validator will check if the address is a valid IPv6 address.\n      If that doesn't work, the validator will check if the address is a valid\n      IPv4 address.\n\n      If neither works, the validator will raise an error (as always).\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises InvalidIPAddressError: if ``value`` is not a valid IP address or empty with\n      ``allow_empty`` set to ``True``\n\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if is_py2 and value and isinstance(value, unicode):\n        value = value.encode('utf-8')\n\n    try:\n        value = ipv6(value, force_run = True)                                   # pylint: disable=E1123\n        ipv6_failed = False\n    except ValueError:\n        ipv6_failed = True\n\n    if ipv6_failed:\n        try:\n            value = ipv4(value, force_run = True)                               # pylint: disable=E1123\n        except ValueError:\n            raise errors.InvalidIPAddressError('value (%s) is not a valid IPv6 or '\n                                               'IPv4 address' % value)\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ipv4(value, allow_empty = False):\n    if not value and allow_empty is False:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    try:\n        components = value.split('.')\n    except AttributeError:\n        raise errors.InvalidIPAddressError('value (%s) is not a valid ipv4' % value)\n\n    if len(components) != 4 or not all(x.isdigit() for x in components):\n        raise errors.InvalidIPAddressError('value (%s) is not a valid ipv4' % value)\n\n    for x in components:\n        try:\n            x = integer(x,\n                        minimum = 0,\n                        maximum = 255)\n        except ValueError:\n            raise errors.InvalidIPAddressError('value (%s) is not a valid ipv4' % value)\n\n    return value", "response": "Validate that value is a valid IP version 4 address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ipv6(value,\n         allow_empty = False,\n         **kwargs):\n    \"\"\"Validate that ``value`` is a valid IP address version 6.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises InvalidIPAddressError: if ``value`` is not a valid IP version 6 address or\n      empty with ``allow_empty`` is not set to ``True``\n\n    \"\"\"\n    if not value and allow_empty is False:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if not isinstance(value, str):\n        raise errors.InvalidIPAddressError('value (%s) is not a valid ipv6' % value)\n\n    value = value.lower().strip()\n\n    is_valid = IPV6_REGEX.match(value)\n\n    if not is_valid:\n        raise errors.InvalidIPAddressError('value (%s) is not a valid ipv6' % value)\n\n    return value", "response": "Validate that value is a valid IP version 6 address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mac_address(value,\n                allow_empty = False,\n                **kwargs):\n    \"\"\"Validate that ``value`` is a valid MAC address.\n\n    :param value: The value to validate.\n    :type value: :class:`str <python:str>` / :obj:`None <python:None>`\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`str <python:str>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises CannotCoerceError: if ``value`` is not a valid :class:`str <python:str>`\n      or string-like object\n    :raises InvalidMACAddressError: if ``value`` is not a valid MAC address or empty with\n      ``allow_empty`` set to ``True``\n\n    \"\"\"\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if not isinstance(value, basestring):\n        raise errors.CannotCoerceError('value must be a valid string, '\n                                       'was %s' % type(value))\n\n    if '-' in value:\n        value = value.replace('-', ':')\n\n    value = value.lower().strip()\n\n    is_valid = MAC_ADDRESS_REGEX.match(value)\n\n    if not is_valid:\n        raise errors.InvalidMACAddressError('value (%s) is not a valid MAC '\n                                            'address' % value)\n\n    return value", "response": "Validate that value is a valid MAC address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_type(obj,\n            type_,\n            **kwargs):\n    \"\"\"Indicate if ``obj`` is a type in ``type_``.\n\n    .. hint::\n\n      This checker is particularly useful when you want to evaluate whether\n      ``obj`` is of a particular type, but importing that type directly to use\n      in :func:`isinstance() <python:isinstance>` would cause a circular import\n      error.\n\n      To use this checker in that kind of situation, you can instead pass the\n      *name* of the type you want to check as a string in ``type_``. The checker\n      will evaluate it and see whether ``obj`` is of a type or inherits from a\n      type whose name matches the string you passed.\n\n    :param obj: The object whose type should be checked.\n    :type obj: :class:`object <python:object>`\n\n    :param type_: The type(s) to check against.\n    :type type_: :class:`type <python:type>` / iterable of :class:`type <python:type>` /\n      :class:`str <python:str>` with type name / iterable of :class:`str <python:str>`\n      with type name\n\n    :returns: ``True`` if ``obj`` is a type in ``type_``. Otherwise, ``False``.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    if not is_iterable(type_):\n        type_ = [type_]\n\n    return_value = False\n    for check_for_type in type_:\n        if isinstance(check_for_type, type):\n            return_value = isinstance(obj, check_for_type)\n        elif obj.__class__.__name__ == check_for_type:\n            return_value = True\n        else:\n            return_value = _check_base_classes(obj.__class__.__bases__,\n                                               check_for_type)\n\n        if return_value is True:\n            break\n\n    return return_value", "response": "Indicate if obj is of a type in type_."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nindicate whether check_for_type exists in base_classes.", "response": "def _check_base_classes(base_classes, check_for_type):\n    \"\"\"Indicate whether ``check_for_type`` exists in ``base_classes``.\n    \"\"\"\n    return_value = False\n    for base in base_classes:\n        if base.__name__ == check_for_type:\n            return_value = True\n            break\n        else:\n            return_value = _check_base_classes(base.__bases__, check_for_type)\n            if return_value is True:\n                break\n\n    return return_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef are_equivalent(*args, **kwargs):\n    if len(args) == 1:\n        return True\n\n    first_item = args[0]\n    for item in args[1:]:\n        if type(item) != type(first_item):                                      # pylint: disable=C0123\n            return False\n\n        if isinstance(item, dict):\n            if not are_dicts_equivalent(item, first_item):\n                return False\n        elif hasattr(item, '__iter__') and not isinstance(item, (str, bytes, dict)):\n            if len(item) != len(first_item):\n                return False\n            for value in item:\n                if value not in first_item:\n                    return False\n            for value in first_item:\n                if value not in item:\n                    return False\n        else:\n            if item != first_item:\n                return False\n\n    return True", "response": "Indicate if arguments passed to this function are equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef are_dicts_equivalent(*args, **kwargs):\n    # pylint: disable=too-many-return-statements\n    if not args:\n        return False\n\n    if len(args) == 1:\n        return True\n\n    if not all(is_dict(x) for x in args):\n        return False\n\n    first_item = args[0]\n    for item in args[1:]:\n        if len(item) != len(first_item):\n            return False\n\n        for key in item:\n            if key not in first_item:\n                return False\n\n            if not are_equivalent(item[key], first_item[key]):\n                return False\n\n        for key in first_item:\n            if key not in item:\n                return False\n\n            if not are_equivalent(first_item[key], item[key]):\n                return False\n\n    return True", "response": "Indicate if all of the passed dicts have identical keys and values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_between(value,\n               minimum = None,\n               maximum = None,\n               **kwargs):\n    \"\"\"Indicate whether ``value`` is greater than or equal to a supplied ``minimum``\n    and/or less than or equal to ``maximum``.\n\n    .. note::\n\n      This function works on any ``value`` that support comparison operators,\n      whether they are numbers or not. Technically, this means that ``value``,\n      ``minimum``, or ``maximum`` need to implement the Python magic methods\n      :func:`__lte__ <python:object.__lte__>` and :func:`__gte__ <python:object.__gte__>`.\n\n      If ``value``, ``minimum``, or ``maximum`` do not support comparison\n      operators, they will raise :class:`NotImplemented <python:NotImplemented>`.\n\n    :param value: The ``value`` to check.\n    :type value: anything that supports comparison operators\n\n    :param minimum: If supplied, will return ``True`` if ``value`` is greater than or\n      equal to this value.\n    :type minimum: anything that supports comparison operators /\n      :obj:`None <python:None>`\n\n    :param maximum: If supplied, will return ``True`` if ``value`` is less than or\n      equal to this value.\n    :type maximum: anything that supports comparison operators /\n      :obj:`None <python:None>`\n\n    :returns: ``True`` if ``value`` is greater than or equal to a supplied ``minimum``\n      and less than or equal to a supplied ``maximum``. Otherwise, returns ``False``.\n    :rtype: :class:`bool <python:bool>`\n\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n    :raises NotImplemented: if ``value``, ``minimum``, or ``maximum`` do not\n      support comparison operators\n    :raises ValueError: if both ``minimum`` and ``maximum`` are\n      :obj:`None <python:None>`\n    \"\"\"\n    if minimum is None and maximum is None:\n        raise ValueError('minimum and maximum cannot both be None')\n\n    if value is None:\n        return False\n\n    if minimum is not None and maximum is None:\n        return value >= minimum\n    elif minimum is None and maximum is not None:\n        return value <= maximum\n    elif minimum is not None and maximum is not None:\n        return value >= minimum and value <= maximum", "response": "Indicate whether a value is between a supplied minimum and maximum."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nindicating whether value has a length greater than or equal to a supplied minimum and less than or equal to a supplied maximum.", "response": "def has_length(value,\n               minimum = None,\n               maximum = None,\n               **kwargs):\n    \"\"\"Indicate whether ``value`` has a length greater than or equal to a\n    supplied ``minimum`` and/or less than or equal to ``maximum``.\n\n    .. note::\n\n      This function works on any ``value`` that supports the\n      :func:`len() <python:len>` operation. This means that ``value`` must implement\n      the :func:`__len__ <python:__len__>` magic method.\n\n      If ``value`` does not support length evaluation, the checker will raise\n      :class:`NotImplemented <python:NotImplemented>`.\n\n    :param value: The ``value`` to check.\n    :type value: anything that supports length evaluation\n\n    :param minimum: If supplied, will return ``True`` if ``value`` is greater than or\n      equal to this value.\n    :type minimum: numeric\n\n    :param maximum: If supplied, will return ``True`` if ``value`` is less than or\n      equal to this value.\n    :type maximum: numeric\n\n    :returns: ``True`` if ``value`` has length greater than or equal to a\n      supplied ``minimum`` and less than or equal to a supplied ``maximum``.\n      Otherwise, returns ``False``.\n    :rtype: :class:`bool <python:bool>`\n\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n    :raises TypeError: if ``value`` does not support length evaluation\n    :raises ValueError: if both ``minimum`` and ``maximum`` are\n      :obj:`None <python:None>`\n    \"\"\"\n    if minimum is None and maximum is None:\n        raise ValueError('minimum and maximum cannot both be None')\n\n    length = len(value)\n    minimum = validators.numeric(minimum,\n                                 allow_empty = True)\n    maximum = validators.numeric(maximum,\n                                 allow_empty = True)\n\n    return is_between(length,\n                      minimum = minimum,\n                      maximum = maximum)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_dict(value, **kwargs):\n    if isinstance(value, dict):\n        return True\n\n    try:\n        value = validators.dict(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nindicates whether value is a JSON object.", "response": "def is_json(value,\n            schema = None,\n            json_serializer = None,\n            **kwargs):\n    \"\"\"Indicate whether ``value`` is a valid JSON object.\n\n    .. note::\n\n      ``schema`` supports JSON Schema Drafts 3 - 7. Unless the JSON Schema indicates the\n      meta-schema using a ``$schema`` property, the schema will be assumed to conform to\n      Draft 7.\n\n    :param value: The value to evaluate.\n\n    :param schema: An optional JSON schema against which ``value`` will be validated.\n    :type schema: :class:`dict <python:dict>` / :class:`str <python:str>` /\n      :obj:`None <python:None>`\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.json(value,\n                                schema = schema,\n                                json_serializer = json_serializer,\n                                **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_string(value,\n              coerce_value = False,\n              minimum_length = None,\n              maximum_length = None,\n              whitespace_padding = False,\n              **kwargs):\n    \"\"\"Indicate whether ``value`` is a string.\n\n    :param value: The value to evaluate.\n\n    :param coerce_value: If ``True``, will check whether ``value`` can be coerced\n      to a string if it is not already. Defaults to ``False``.\n    :type coerce_value: :class:`bool <python:bool>`\n\n    :param minimum_length: If supplied, indicates the minimum number of characters\n      needed to be valid.\n    :type minimum_length: :class:`int <python:int>`\n\n    :param maximum_length: If supplied, indicates the minimum number of characters\n      needed to be valid.\n    :type maximum_length: :class:`int <python:int>`\n\n    :param whitespace_padding: If ``True`` and the value is below the\n      ``minimum_length``, pad the value with spaces. Defaults to ``False``.\n    :type whitespace_padding: :class:`bool <python:bool>`\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    if value is None:\n        return False\n\n    minimum_length = validators.integer(minimum_length, allow_empty = True, **kwargs)\n    maximum_length = validators.integer(maximum_length, allow_empty = True, **kwargs)\n\n    if isinstance(value, basestring) and not value:\n        if minimum_length and minimum_length > 0 and not whitespace_padding:\n            return False\n\n        return True\n\n    try:\n        value = validators.string(value,\n                                  coerce_value = coerce_value,\n                                  minimum_length = minimum_length,\n                                  maximum_length = maximum_length,\n                                  whitespace_padding = whitespace_padding,\n                                  **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_iterable(obj,\n                forbid_literals = (str, bytes),\n                minimum_length = None,\n                maximum_length = None,\n                **kwargs):\n    \"\"\"Indicate whether ``obj`` is iterable.\n\n    :param forbid_literals: A collection of literals that will be considered invalid\n      even if they are (actually) iterable. Defaults to a :class:`tuple <python:tuple>`\n      containing :class:`str <python:str>` and :class:`bytes <python:bytes>`.\n    :type forbid_literals: iterable\n\n    :param minimum_length: If supplied, indicates the minimum number of members\n      needed to be valid.\n    :type minimum_length: :class:`int <python:int>`\n\n    :param maximum_length: If supplied, indicates the minimum number of members\n      needed to be valid.\n    :type maximum_length: :class:`int <python:int>`\n\n    :returns: ``True`` if ``obj`` is a valid iterable, ``False`` if not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    if obj is None:\n        return False\n\n    if obj in forbid_literals:\n        return False\n\n    try:\n        obj = validators.iterable(obj,\n                                  allow_empty = True,\n                                  forbid_literals = forbid_literals,\n                                  minimum_length = minimum_length,\n                                  maximum_length = maximum_length,\n                                  **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether the object is iterable."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nindicates whether value is not empty.", "response": "def is_not_empty(value, **kwargs):\n    \"\"\"Indicate whether ``value`` is empty.\n\n    :param value: The value to evaluate.\n\n    :returns: ``True`` if ``value`` is empty, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.not_empty(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_none(value, allow_empty = False, **kwargs):\n    try:\n        validators.none(value, allow_empty = allow_empty, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_variable_name(value, **kwargs):\n    try:\n        validators.variable_name(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a valid Python variable name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_uuid(value, **kwargs):\n    try:\n        validators.uuid(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value contains a UUID."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_date(value,\n            minimum = None,\n            maximum = None,\n            coerce_value = False,\n            **kwargs):\n    \"\"\"Indicate whether ``value`` is a :class:`date <python:datetime.date>`.\n\n    :param value: The value to evaluate.\n\n    :param minimum: If supplied, will make sure that ``value`` is on or after\n      this value.\n    :type minimum: :class:`datetime <python:datetime.datetime>` /\n      :class:`date <python:datetime.date>` / compliant :class:`str <python:str>`\n      / :obj:`None <python:None>`\n\n    :param maximum: If supplied, will make sure that ``value`` is on or before this\n      value.\n    :type maximum: :class:`datetime <python:datetime.datetime>` /\n      :class:`date <python:datetime.date>` / compliant :class:`str <python:str>`\n      / :obj:`None <python:None>`\n\n    :param coerce_value: If ``True``, will return ``True`` if ``value`` can be\n      coerced to a :class:`date <python:datetime.date>`. If ``False``,\n      will only return ``True`` if ``value`` is a date value only. Defaults to\n      ``False``.\n    :type coerce_value: :class:`bool <python:bool>`\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.date(value,\n                                minimum = minimum,\n                                maximum = maximum,\n                                coerce_value = coerce_value,\n                                **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a date value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nindicates whether value is a datetime.", "response": "def is_datetime(value,\n                minimum = None,\n                maximum = None,\n                coerce_value = False,\n                **kwargs):\n    \"\"\"Indicate whether ``value`` is a :class:`datetime <python:datetime.datetime>`.\n\n    :param value: The value to evaluate.\n\n    :param minimum: If supplied, will make sure that ``value`` is on or after\n      this value.\n    :type minimum: :class:`datetime <python:datetime.datetime>` /\n      :class:`date <python:datetime.date>` / compliant :class:`str <python:str>`\n      / :obj:`None <python:None>`\n\n    :param maximum: If supplied, will make sure that ``value`` is on or before this\n      value.\n    :type maximum: :class:`datetime <python:datetime.datetime>` /\n      :class:`date <python:datetime.date>` / compliant :class:`str <python:str>`\n      / :obj:`None <python:None>`\n\n    :param coerce_value: If ``True``, will return ``True`` if ``value`` can be\n      coerced to a :class:`datetime <python:datetime.datetime>`. If ``False``,\n      will only return ``True`` if ``value`` is a complete timestamp. Defaults to\n      ``False``.\n    :type coerce_value: :class:`bool <python:bool>`\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.datetime(value,\n                                    minimum = minimum,\n                                    maximum = maximum,\n                                    coerce_value = coerce_value,\n                                    **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_time(value,\n            minimum = None,\n            maximum = None,\n            coerce_value = False,\n            **kwargs):\n    \"\"\"Indicate whether ``value`` is a :class:`time <python:datetime.time>`.\n\n    :param value: The value to evaluate.\n\n    :param minimum: If supplied, will make sure that ``value`` is on or after this value.\n    :type minimum: :func:`datetime <validator_collection.validators.datetime>` or\n      :func:`time <validator_collection.validators.time>`-compliant\n      :class:`str <python:str>` / :class:`datetime <python:datetime.datetime>` /\n      :class:`time <python:datetime.time> / numeric / :obj:`None <python:None>`\n\n    :param maximum: If supplied, will make sure that ``value`` is on or before this\n      value.\n    :type maximum: :func:`datetime <validator_collection.validators.datetime>` or\n      :func:`time <validator_collection.validators.time>`-compliant\n      :class:`str <python:str>` / :class:`datetime <python:datetime.datetime>` /\n      :class:`time <python:datetime.time> / numeric / :obj:`None <python:None>`\n\n    :param coerce_value: If ``True``, will return ``True`` if ``value`` can be\n      coerced to a :class:`time <python:datetime.time>`. If ``False``,\n      will only return ``True`` if ``value`` is a valid time. Defaults to\n      ``False``.\n    :type coerce_value: :class:`bool <python:bool>`\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.time(value,\n                                minimum = minimum,\n                                maximum = maximum,\n                                coerce_value = coerce_value,\n                                **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_timezone(value,\n                positive = True,\n                **kwargs):\n    \"\"\"Indicate whether ``value`` is a :class:`tzinfo <python:datetime.tzinfo>`.\n\n    .. caution::\n\n      This does **not** validate whether the value is a timezone that actually\n      exists, nor can it resolve timzone names (e.g. ``'Eastern'`` or ``'CET'``).\n\n      For that kind of functionality, we recommend you utilize:\n      `pytz <https://pypi.python.org/pypi/pytz>`_\n\n    :param value: The value to evaluate.\n\n    :param positive: Indicates whether the ``value`` is positive or negative\n      (only has meaning if ``value`` is a string). Defaults to ``True``.\n    :type positive: :class:`bool <python:bool>`\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.timezone(value,\n                                    positive = positive,\n                                    **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether the value is a timezone."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_numeric(value,\n               minimum = None,\n               maximum = None,\n               **kwargs):\n    \"\"\"Indicate whether ``value`` is a numeric value.\n\n    :param value: The value to evaluate.\n\n    :param minimum: If supplied, will make sure that ``value`` is greater than or\n      equal to this value.\n    :type minimum: numeric\n\n    :param maximum: If supplied, will make sure that ``value`` is less than or\n      equal to this value.\n    :type maximum: numeric\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.numeric(value,\n                                   minimum = minimum,\n                                   maximum = maximum,\n                                   **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a numeric value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nindicate whether the value is an integer value.", "response": "def is_integer(value,\n               coerce_value = False,\n               minimum = None,\n               maximum = None,\n               base = 10,\n               **kwargs):\n    \"\"\"Indicate whether ``value`` contains a whole number.\n\n    :param value: The value to evaluate.\n\n    :param coerce_value: If ``True``, will return ``True`` if ``value`` can be coerced\n      to whole number. If ``False``, will only return ``True`` if ``value`` is already\n      a whole number (regardless of type). Defaults to ``False``.\n    :type coerce_value: :class:`bool <python:bool>`\n\n    :param minimum: If supplied, will make sure that ``value`` is greater than or\n      equal to this value.\n    :type minimum: numeric\n\n    :param maximum: If supplied, will make sure that ``value`` is less than or\n      equal to this value.\n    :type maximum: numeric\n\n    :param base: Indicates the base that is used to determine the integer value.\n      The allowed values are 0 and 2\u201336. Base-2, -8, and -16 literals can be\n      optionally prefixed with ``0b/0B``, ``0o/0O/0``, or ``0x/0X``, as with\n      integer literals in code. Base 0 means to interpret the string exactly as\n      an integer literal, so that the actual base is 2, 8, 10, or 16. Defaults to\n      ``10``.\n    :type base: :class:`int <python:int>`\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.integer(value,\n                                   coerce_value = coerce_value,\n                                   minimum = minimum,\n                                   maximum = maximum,\n                                   base = base,\n                                   **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nindicates whether value is a float.", "response": "def is_float(value,\n             minimum = None,\n             maximum = None,\n             **kwargs):\n    \"\"\"Indicate whether ``value`` is a :class:`float <python:float>`.\n\n    :param value: The value to evaluate.\n\n    :param minimum: If supplied, will make sure that ``value`` is greater than or\n      equal to this value.\n    :type minimum: numeric\n\n    :param maximum: If supplied, will make sure that ``value`` is less than or\n      equal to this value.\n    :type maximum: numeric\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.float(value,\n                                 minimum = minimum,\n                                 maximum = maximum,\n                                 **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nindicating whether value is a : class : fraction.", "response": "def is_fraction(value,\n                minimum = None,\n                maximum = None,\n                **kwargs):\n    \"\"\"Indicate whether ``value`` is a :class:`Fraction <python:fractions.Fraction>`.\n\n    :param value: The value to evaluate.\n\n    :param minimum: If supplied, will make sure that ``value`` is greater than or\n      equal to this value.\n    :type minimum: numeric\n\n    :param maximum: If supplied, will make sure that ``value`` is less than or\n      equal to this value.\n    :type maximum: numeric\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.fraction(value,\n                                    minimum = minimum,\n                                    maximum = maximum,\n                                    **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_decimal(value,\n               minimum = None,\n               maximum = None,\n               **kwargs):\n    \"\"\"Indicate whether ``value`` contains a :class:`Decimal <python:decimal.Decimal>`.\n\n    :param value: The value to evaluate.\n\n    :param minimum: If supplied, will make sure that ``value`` is greater than or\n      equal to this value.\n    :type minimum: numeric\n\n    :param maximum: If supplied, will make sure that ``value`` is less than or\n      equal to this value.\n    :type maximum: numeric\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.decimal(value,\n                                   minimum = minimum,\n                                   maximum = maximum,\n                                   **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value contains a Decimal."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nindicate whether value is a path - like object.", "response": "def is_pathlike(value, **kwargs):\n    \"\"\"Indicate whether ``value`` is a path-like object.\n\n    :param value: The value to evaluate.\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.path(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_on_filesystem(value, **kwargs):\n    try:\n        value = validators.path_exists(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a file or directory that exists on the local\n    filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nindicates whether value is a file that exists on the local filesystem.", "response": "def is_file(value, **kwargs):\n    \"\"\"Indicate whether ``value`` is a file that exists on the local filesystem.\n\n    :param value: The value to evaluate.\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.file_exists(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_directory(value, **kwargs):\n    try:\n        value = validators.directory_exists(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a directory that exists on the local filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_readable(value, **kwargs):\n    try:\n        validators.readable(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a readable file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nindicate whether value is a writeable file.", "response": "def is_writeable(value,\n                 **kwargs):\n    \"\"\"Indicate whether ``value`` is a writeable file.\n\n    .. caution::\n\n      This validator does **NOT** work correctly on a Windows file system. This\n      is due to the vagaries of how Windows manages its file system and the\n      various ways in which it can manage file permission.\n\n      If called on a Windows file system, this validator will raise\n      :class:`NotImplementedError() <python:NotImplementedError>`.\n\n    .. caution::\n\n      **Use of this validator is an anti-pattern and should be used with caution.**\n\n      Validating the writability of a file *before* attempting to write to it\n      exposes your code to a bug called\n      `TOCTOU <https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use>`_.\n\n      This particular class of bug can expose your code to **security vulnerabilities**\n      and so this validator should only be used if you are an advanced user.\n\n      A better pattern to use when writing to file is to apply the principle of\n      EAFP (\"easier to ask forgiveness than permission\"), and simply attempt to\n      write to the file using a ``try ... except`` block:\n\n      .. code-block:: python\n\n        try:\n            with open('path/to/filename.txt', mode = 'a') as file_object:\n                # write to file here\n        except (OSError, IOError) as error:\n            # Handle an error if unable to write.\n\n    .. note::\n\n      This validator relies on :func:`os.access() <python:os.access>` to check\n      whether ``value`` is writeable. This function has certain limitations,\n      most especially that:\n\n      * It will **ignore** file-locking (yielding a false-positive) if the file\n        is locked.\n      * It focuses on *local operating system permissions*, which means if trying\n        to access a path over a network you might get a false positive or false\n        negative (because network paths may have more complicated authentication\n        methods).\n\n    :param value: The value to evaluate.\n    :type value: Path-like object\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises NotImplementedError: if called on a Windows system\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    if sys.platform in ['win32', 'cygwin']:\n        raise NotImplementedError('not supported on Windows')\n\n    try:\n        validators.writeable(value,\n                             allow_empty = False,\n                             **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_executable(value,\n                  **kwargs):\n    \"\"\"Indicate whether ``value`` is an executable file.\n\n    .. caution::\n\n      This validator does **NOT** work correctly on a Windows file system. This\n      is due to the vagaries of how Windows manages its file system and the\n      various ways in which it can manage file permission.\n\n      If called on a Windows file system, this validator will raise\n      :class:`NotImplementedError() <python:NotImplementedError>`.\n\n    .. caution::\n\n      **Use of this validator is an anti-pattern and should be used with caution.**\n\n      Validating the writability of a file *before* attempting to execute it\n      exposes your code to a bug called\n      `TOCTOU <https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use>`_.\n\n      This particular class of bug can expose your code to **security vulnerabilities**\n      and so this validator should only be used if you are an advanced user.\n\n      A better pattern to use when writing to file is to apply the principle of\n      EAFP (\"easier to ask forgiveness than permission\"), and simply attempt to\n      execute the file using a ``try ... except`` block.\n\n    .. note::\n\n      This validator relies on :func:`os.access() <python:os.access>` to check\n      whether ``value`` is writeable. This function has certain limitations,\n      most especially that:\n\n      * It will **ignore** file-locking (yielding a false-positive) if the file\n        is locked.\n      * It focuses on *local operating system permissions*, which means if trying\n        to access a path over a network you might get a false positive or false\n        negative (because network paths may have more complicated authentication\n        methods).\n\n    :param value: The value to evaluate.\n    :type value: Path-like object\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises NotImplementedError: if called on a Windows system\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    if sys.platform in ['win32', 'cygwin']:\n        raise NotImplementedError('not supported on Windows')\n\n    try:\n        validators.executable(value,\n                              **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is an executable file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_email(value, **kwargs):\n    try:\n        value = validators.email(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is an email address."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_url(value, **kwargs):\n    try:\n        value = validators.url(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nindicates whether value is a valid domain name.", "response": "def is_domain(value, **kwargs):\n    \"\"\"Indicate whether ``value`` is a valid domain.\n\n    .. caution::\n\n      This validator does not verify that ``value`` **exists** as a domain. It\n      merely verifies that its contents *might* exist as a domain.\n\n    .. note::\n\n      This validator checks to validate that ``value`` resembles a valid\n      domain name. It is - generally - compliant with\n      `RFC 1035 <https://tools.ietf.org/html/rfc1035>`_ and\n      `RFC 6761 <https://tools.ietf.org/html/rfc6761>`_, however it diverges\n      in a number of key ways:\n\n        * Including authentication (e.g. ``username:password@domain.dev``) will\n          fail validation.\n        * Including a path (e.g. ``domain.dev/path/to/file``) will fail validation.\n        * Including a port (e.g. ``domain.dev:8080``) will fail validation.\n\n      If you are hoping to validate a more complete URL, we recommend that you\n      see :func:`url <validator_collection.validators.url>`.\n\n    :param value: The value to evaluate.\n\n    :param allow_ips: If ``True``, will succeed when validating IP addresses,\n      If ``False``, will fail if ``value`` is an IP address. Defaults to ``False``.\n    :type allow_ips: :class:`bool <python:bool>`\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.domain(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_ip_address(value, **kwargs):\n    try:\n        value = validators.ip_address(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a valid IP address."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nindicate whether value is a valid IP version 4 address.", "response": "def is_ipv4(value, **kwargs):\n    \"\"\"Indicate whether ``value`` is a valid IP version 4 address.\n\n    :param value: The value to evaluate.\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n\n    \"\"\"\n    try:\n        value = validators.ipv4(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nindicates whether value is a valid IP version 6 address.", "response": "def is_ipv6(value, **kwargs):\n    \"\"\"Indicate whether ``value`` is a valid IP version 6 address.\n\n    :param value: The value to evaluate.\n\n    :returns: ``True`` if ``value`` is valid, ``False`` if it is not.\n    :rtype: :class:`bool <python:bool>`\n\n    :raises SyntaxError: if ``kwargs`` contains duplicate keyword parameters or duplicates\n      keyword parameters passed to the underlying validator\n    \"\"\"\n    try:\n        value = validators.ipv6(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_mac_address(value, **kwargs):\n    try:\n        value = validators.mac_address(value, **kwargs)\n    except SyntaxError as error:\n        raise error\n    except Exception:\n        return False\n\n    return True", "response": "Indicate whether value is a valid MAC address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef draw_line(self, lx, ltor):\n    '''Draw a series of elements from left to right'''\n    tag = self.get_tag()\n    c = self.canvas\n    s = self.style\n    \n    sep = s.h_sep\n    exx = 0\n    exy = 0\n    \n    terms = lx if ltor else reversed(lx) # Reverse so we can draw left to right\n\n    for term in terms:\n      t, texx, texy = self.draw_diagram(term, ltor) # Draw each element\n      if exx > 0: # Second element onward\n        xn = exx + sep # Add space between elements\n        c.move(t, xn, exy) # Shift last element forward\n        arr = 'last' if ltor else 'first'\n        c.create_line(exx-1, exy, xn, exy, tags=(tag,), width=s.line_width, arrow=arr) # Connecting line (NOTE: -1 fudge)\n        exx = xn + texx # Start at end of this element\n      else: # First element\n        exx = texx # Start at end of this element\n      \n      exy = texy\n      \n      c.addtag_withtag(tag, t) # Retag this element\n      c.dtag(t, t) # Drop old tags\n    \n    if exx == 0: # Nothing drawn, Add a line segment with an arrow in the middle\n      exx = sep * 2    \n      c.create_line(0,0,sep,0, width=s.line_width, tags=(tag,), arrow='last')\n      c.create_line(sep, 0,exx,0, width=s.line_width, tags=(tag,))\n      exx = sep\n      \n    return [tag, exx, exy]", "response": "Draw a series of elements from left to right"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_file_fields():\n\n    # get models\n\n    all_models = apps.get_models()\n\n    # get fields\n\n    fields = []\n\n    for model in all_models:\n        for field in model._meta.get_fields():\n            if isinstance(field, models.FileField):\n                fields.append(field)\n\n    return fields", "response": "Get all fields which are inherited from FileField\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes files from media dir", "response": "def remove_media(files):\n    \"\"\"\n        Delete file from media dir\n    \"\"\"\n    for filename in files:\n        os.remove(os.path.join(settings.MEDIA_ROOT, filename))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_empty_dirs(path=None):\n\n    if not path:\n        path = settings.MEDIA_ROOT\n\n    if not os.path.isdir(path):\n        return False\n\n    listdir = [os.path.join(path, filename) for filename in os.listdir(path)]\n\n    if all(list(map(remove_empty_dirs, listdir))):\n        os.rmdir(path)\n        return True\n    else:\n        return False", "response": "Recursively delete empty directories ; return True if everything was deleted."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_used_media():\n\n    media = set()\n\n    for field in get_file_fields():\n        is_null = {\n            '%s__isnull' % field.name: True,\n        }\n        is_empty = {\n            '%s' % field.name: '',\n        }\n\n        storage = field.storage\n\n        for value in field.model.objects \\\n                .values_list(field.name, flat=True) \\\n                .exclude(**is_empty).exclude(**is_null):\n            if value not in EMPTY_VALUES:\n                media.add(storage.path(value))\n\n    return media", "response": "Get media which are still used in models\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_all_media(exclude=None):\n\n    if not exclude:\n        exclude = []\n\n    media = set()\n\n    for root, dirs, files in os.walk(six.text_type(settings.MEDIA_ROOT)):\n        for name in files:\n            path = os.path.abspath(os.path.join(root, name))\n            relpath = os.path.relpath(path, settings.MEDIA_ROOT)\n            for e in exclude:\n                if re.match(r'^%s$' % re.escape(e).replace('\\\\*', '.*'), relpath):\n                    break\n            else:\n                media.add(path)\n\n    return media", "response": "Get all media from MEDIA_ROOT"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_unused_media(exclude=None):\n\n    if not exclude:\n        exclude = []\n\n    all_media = get_all_media(exclude)\n    used_media = get_used_media()\n\n    return all_media - used_media", "response": "Get all media which are not used in models\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef createCluster(self, hzVersion, xmlconfig):\n        self.send_createCluster(hzVersion, xmlconfig)\n        return self.recv_createCluster()", "response": "This function is called to create a new cluster in the cluster store."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a shutdown request to the specified cluster and member.", "response": "def shutdownMember(self, clusterId, memberId):\n        \"\"\"\n        Parameters:\n         - clusterId\n         - memberId\n        \"\"\"\n        self.send_shutdownMember(clusterId, memberId)\n        return self.recv_shutdownMember()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef terminateMember(self, clusterId, memberId):\n        self.send_terminateMember(clusterId, memberId)\n        return self.recv_terminateMember()", "response": "This method sends a TERM to the specified cluster and returns the object that was received."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsuspend a member from a cluster", "response": "def suspendMember(self, clusterId, memberId):\n        \"\"\"\n        Parameters:\n         - clusterId\n         - memberId\n        \"\"\"\n        self.send_suspendMember(clusterId, memberId)\n        return self.recv_suspendMember()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resumeMember(self, clusterId, memberId):\n        self.send_resumeMember(clusterId, memberId)\n        return self.recv_resumeMember()", "response": "Resumes a member from a cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mergeMemberToCluster(self, clusterId, memberId):\n        self.send_mergeMemberToCluster(clusterId, memberId)\n        return self.recv_mergeMemberToCluster()", "response": "This method is used to send a merge member request to the cluster and receive the response."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef executeOnController(self, clusterId, script, lang):\n        self.send_executeOnController(clusterId, script, lang)\n        return self.recv_executeOnController()", "response": "Execute a script on the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deltaE(color1, color2,\n           input_space=\"sRGB1\", uniform_space=\"CAM02-UCS\"):\n    \"\"\"Computes the :math:`\\Delta E` distance between pairs of colors.\n\n    :param input_space: The space the colors start out in. Can be anything\n       recognized by :func:`cspace_convert`. Default: \"sRGB1\"\n    :param uniform_space: Which space to perform the distance measurement\n       in. This should be a uniform space like CAM02-UCS where\n       Euclidean distance approximates similarity judgements, because\n       otherwise the results of this function won't be very meaningful, but in\n       fact any color space known to :func:`cspace_convert` will be accepted.\n\n    By default, computes the euclidean distance in CAM02-UCS :math:`J'a'b'`\n    space (thus giving :math:`\\Delta E'`); for details, see\n    :cite:`CAM02-UCS`. If you want the classic :math:`\\Delta E^*_{ab}` defined\n    by CIE 1976, use ``uniform_space=\"CIELab\"``. Other good choices include\n    ``\"CAM02-LCD\"`` and ``\"CAM02-SCD\"``.\n\n    This function has no ability to perform :math:`\\Delta E` calculations like\n    CIEDE2000 that are not based on euclidean distances.\n\n    This function is vectorized, i.e., color1, color2 may be arrays with shape\n    (..., 3), in which case we compute the distance between corresponding\n    pairs of colors.\n\n    \"\"\"\n\n    uniform1 = cspace_convert(color1, input_space, uniform_space)\n    uniform2 = cspace_convert(color2, input_space, uniform_space)\n    return np.sqrt(np.sum((uniform1 - uniform2) ** 2, axis=-1))", "response": "Compute the delta E between two colors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert XYZ to linear sRGB where XYZ is normalized so that reference white D65 is 100 Y = 100 Z = 108. 90 Z = 100 Z = 0. 90", "response": "def XYZ100_to_sRGB1_linear(XYZ100):\n    \"\"\"Convert XYZ to linear sRGB, where XYZ is normalized so that reference\n    white D65 is X=95.05, Y=100, Z=108.90 and sRGB is on the 0-1 scale. Linear\n    sRGB has a linear relationship to actual light, so it is an appropriate\n    space for simulating light (e.g. for alpha blending).\n\n    \"\"\"\n    XYZ100 = np.asarray(XYZ100, dtype=float)\n    # this is broadcasting matrix * array-of-vectors, where the vector is the\n    # last dim\n    RGB_linear = np.einsum(\"...ij,...j->...i\", XYZ100_to_sRGB1_matrix, XYZ100 / 100)\n    return RGB_linear"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sRGB1_to_sRGB1_linear(sRGB1):\n    sRGB1 = np.asarray(sRGB1, dtype=float)\n    sRGB1_linear = C_linear(sRGB1)\n    return sRGB1_linear", "response": "Convert sRGB 1 to linear sRGB."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cspace_converter(start, end):\n    start = norm_cspace_id(start)\n    end = norm_cspace_id(end)\n    return GRAPH.get_transform(start, end)", "response": "Returns a function for converting from start to end colorspace."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the colors in arr from colorspace start to colorspace end.", "response": "def cspace_convert(arr, start, end):\n    \"\"\"Converts the colors in ``arr`` from colorspace ``start`` to colorspace\n    ``end``.\n\n    :param arr: An array-like of colors.\n    :param start, end: Any supported colorspace specifiers. See\n        :ref:`supported-colorspaces` for details.\n\n    \"\"\"\n    converter = cspace_converter(start, end)\n    return converter(arr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef XYZ100_to_CIECAM02(self, XYZ100, on_negative_A=\"raise\"):\n\n        #### Argument checking\n\n        XYZ100 = np.asarray(XYZ100, dtype=float)\n        if XYZ100.shape[-1] != 3:\n            raise ValueError(\"XYZ100 shape must be (..., 3)\")\n\n        #### Step 1\n\n        RGB = broadcasting_matvec(M_CAT02, XYZ100)\n\n        #### Step 2\n\n        RGB_C = self.D_RGB * RGB\n\n        #### Step 3\n\n        RGBprime = broadcasting_matvec(M_HPE_M_CAT02_inv, RGB_C)\n\n        #### Step 4\n\n        RGBprime_signs = np.sign(RGBprime)\n\n        tmp = (self.F_L * RGBprime_signs * RGBprime / 100) ** 0.42\n        RGBprime_a = RGBprime_signs * 400 * (tmp / (tmp + 27.13)) + 0.1\n\n        #### Step 5\n\n        a = broadcasting_matvec([1, -12. / 11, 1. / 11], RGBprime_a)\n        b = broadcasting_matvec([1. / 9, 1. / 9, -2. / 9], RGBprime_a)\n        h_rad = np.arctan2(b, a)\n        h = np.rad2deg(h_rad) % 360\n\n        # #### Step 6\n\n        # hprime = h, unless h < 20.14, in which case hprime = h + 360.\n        hprime = np.select([h < h_i[0], True], [h + 360, h])\n        # we use 0-based indexing, so our i is one less than the reference\n        # formulas' i.\n        i = np.searchsorted(h_i, hprime, side=\"right\") - 1\n        tmp = (hprime - h_i[i]) / e_i[i]\n        H = H_i[i] + ((100 * tmp)\n                      / (tmp + (h_i[i + 1] - hprime) / e_i[i + 1]))\n\n        #### Step 7\n\n        A = ((broadcasting_matvec([2, 1, 1. / 20], RGBprime_a) - 0.305)\n             * self.N_bb)\n\n        if on_negative_A == \"raise\":\n            if np.any(A < 0):\n                raise NegativeAError(\"attempted to convert a tristimulus \"\n                                     \"value whose achromatic signal was \"\n                                     \"negative, and on_negative_A=\\\"raise\\\"\")\n        elif on_negative_A == \"nan\":\n            A = np.select([A < 0, True], [np.nan, A])\n        else:\n            raise ValueError(\"Invalid on_negative_A argument: got %r, \"\n                             \"expected \\\"raise\\\" or \\\"nan\\\"\"\n                             % (on_negative_A,))\n\n        #### Step 8\n\n        J = 100 * (A / self.A_w) ** (self.c * self.z)\n\n        #### Step 9\n\n        Q = self._J_to_Q(J)\n\n        #### Step 10\n\n        e = (12500. / 13) * self.N_c * self.N_cb * (np.cos(h_rad + 2) + 3.8)\n        t = (e * np.sqrt(a ** 2 + b ** 2)\n             / broadcasting_matvec([1, 1, 21. / 20], RGBprime_a))\n\n        C = t**0.9 * (J / 100)**0.5 * (1.64 - 0.29**self.n)**0.73\n        M = C * self.F_L**0.25\n        s = 100 * (M / Q)**0.5\n\n        return JChQMsH(J, C, h, Q, M, s, H)", "response": "Computes CIECAM02 appearance correlates for the given tristimulus values XYZ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CIECAM02_to_XYZ100(self, J=None, C=None, h=None,\n                           Q=None, M=None, s=None, H=None):\n        \"\"\"Return the unique tristimulus values that have the given CIECAM02\n        appearance correlates under these viewing conditions.\n\n        You must specify 3 arguments:\n\n        * Exactly one of ``J`` and ``Q``\n        * Exactly one of ``C``, ``M``, and ``s``\n        * Exactly one of ``h`` and ``H``.\n\n        Arguments can be vectors, in which case they will be broadcast against\n        each other.\n\n        Returned tristimulus values will be on the 0-100 scale, not the 0-1\n        scale.\n        \"\"\"\n\n        #### Argument checking\n\n        require_exactly_one(J=J, Q=Q)\n        require_exactly_one(C=C, M=M, s=s)\n        require_exactly_one(h=h, H=H)\n\n        if J is not None:\n            J = np.asarray(J, dtype=float)\n        if C is not None:\n            C = np.asarray(C, dtype=float)\n        if h is not None:\n            h = np.asarray(h, dtype=float)\n        if Q is not None:\n            Q = np.asarray(Q, dtype=float)\n        if M is not None:\n            M = np.asarray(M, dtype=float)\n        if s is not None:\n            s = np.asarray(s, dtype=float)\n        if H is not None:\n            H = np.asarray(H, dtype=float)\n\n        #### Step 1: conversions to get JCh\n\n        if J is None:\n            J = 6.25 * ((self.c * Q) / ((self.A_w + 4) * self.F_L**0.25)) ** 2\n\n        if C is None:\n            if M is not None:\n                C = M / self.F_L**0.25\n            else:\n                assert s is not None\n                # when starting from s, we need Q\n                if Q is None:\n                    Q = self._J_to_Q(J)\n                C = (s / 100) ** 2 * (Q / self.F_L**0.25)\n\n        if h is None:\n            i = np.searchsorted(H_i, H, side=\"right\") - 1\n            # BROKEN:\n            num1 = (H - H_i[i]) * (e_i[i + 1] * h_i[i] - e_i[i] * h_i[i + 1])\n            num2 = -100 * h_i[i] * e_i[i + 1]\n            denom1 = (H - H_i[i]) * (e_i[i + 1] - e_i[i])\n            denom2 = -100 * e_i[i + 1]\n            hprime = (num1 + num2) / (denom1 + denom2)\n            h = np.select([hprime > 360, True], [hprime - 360, hprime])\n\n        J, C, h = np.broadcast_arrays(J, C, h)\n        target_shape = J.shape\n\n        # 0d arrays break indexing stuff\n        if J.ndim == 0:\n            J = np.atleast_1d(J)\n            C = np.atleast_1d(C)\n            h = np.atleast_1d(h)\n\n        #### Step 2\n\n        t = (C\n             / (np.sqrt(J / 100) * (1.64 - 0.29**self.n) ** 0.73)\n            ) ** (1 / 0.9)\n        e_t = 0.25 * (np.cos(np.deg2rad(h) + 2) + 3.8)\n        A = self.A_w * (J / 100) ** (1 / (self.c * self.z))\n\n        # an awkward way of calculating 1/t such that 1/0 -> inf\n        with np.errstate(divide=\"ignore\"):\n            one_over_t = 1 / t\n        one_over_t = np.select([np.isnan(one_over_t), True],\n                               [np.inf, one_over_t])\n\n        p_1 = (50000. / 13) * self.N_c * self.N_cb * e_t * one_over_t\n        p_2 = A / self.N_bb + 0.305\n        p_3 = 21. / 20\n\n        #### Step 3\n\n        sin_h = np.sin(np.deg2rad(h))\n        cos_h = np.cos(np.deg2rad(h))\n\n        # to avoid divide-by-zero (or divide-by-eps) issues, we use different\n        # computations when |sin_h| > |cos_h| and vice-versa\n        num = p_2 * (2 + p_3) * (460. / 1403)\n        denom_part2 = (2 + p_3) * (220. / 1403)\n        denom_part3 = (-27. / 1403) + p_3 * (6300. / 1403)\n\n        a = np.empty_like(h)\n        b = np.empty_like(h)\n\n        small_cos = (np.abs(sin_h) >= np.abs(cos_h))\n\n        # NB denom_part2 and denom_part3 are scalars\n        b[small_cos] = (num[small_cos]\n                        / (p_1[small_cos] / sin_h[small_cos]\n                           + (denom_part2\n                              * cos_h[small_cos] / sin_h[small_cos])\n                           + denom_part3))\n        a[small_cos] = b[small_cos] * cos_h[small_cos] / sin_h[small_cos]\n\n        a[~small_cos] = (num[~small_cos]\n                         / (p_1[~small_cos] / cos_h[~small_cos]\n                            + denom_part2\n                            + (denom_part3\n                               * sin_h[~small_cos] / cos_h[~small_cos])))\n        b[~small_cos] = a[~small_cos] * sin_h[~small_cos] / cos_h[~small_cos]\n\n        #### Step 4\n\n        p2ab = np.concatenate((p_2[..., np.newaxis],\n                               a[..., np.newaxis],\n                               b[..., np.newaxis]),\n                              axis=-1)\n        RGBprime_a_matrix = (1. / 1403\n                             * np.asarray([[ 460,  451,   288],\n                                           [ 460, -891,  -261],\n                                           [ 460, -220, -6300]], dtype=float))\n\n        RGBprime_a = broadcasting_matvec(RGBprime_a_matrix, p2ab)\n\n        #### Step 5\n\n        RGBprime = (np.sign(RGBprime_a - 0.1)\n                    * (100 / self.F_L)\n                    * ((27.13 * np.abs(RGBprime_a - 0.1))\n                       / (400 - np.abs(RGBprime_a - 0.1))) ** (1 / 0.42))\n\n        #### Step 6\n\n        RGB_C = broadcasting_matvec(M_CAT02_M_HPE_inv, RGBprime)\n\n        #### Step 7\n\n        RGB = RGB_C / self.D_RGB\n\n        #### Step 8\n\n        XYZ100 = broadcasting_matvec(M_CAT02_inv, RGB)\n\n        XYZ100 = XYZ100.reshape(target_shape + (3,))\n\n        return XYZ100", "response": "Return the unique tristimulus values that have the given CIECAM02 appearance correlates under these viewing conditions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a string naming a standard illuminant and returns its XYZ coordinates.", "response": "def standard_illuminant_XYZ100(name, observer=\"CIE 1931 2 deg\"):\n    \"\"\"Takes a string naming a standard illuminant, and returns its XYZ\n    coordinates (normalized to Y = 100).\n\n    We currently have the following standard illuminants in our database:\n\n    * ``\"A\"``\n    * ``\"C\"``\n    * ``\"D50\"``\n    * ``\"D55\"``\n    * ``\"D65\"``\n    * ``\"D75\"``\n\n    If you need another that isn't on this list, then feel free to send a pull\n    request.\n\n    When in doubt, use D65: it's the whitepoint used by the sRGB standard\n    (61966-2-1:1999) and ISO 10526:1999 says \"D65 should be used in all\n    colorimetric calculations requiring representative daylight, unless there\n    are specific reasons for using a different illuminant\".\n\n    By default, we return points in the XYZ space defined by the CIE 1931 2\n    degree standard observer. By specifying ``observer=\"CIE 1964 10 deg\"``,\n    you can instead get the whitepoint coordinates in XYZ space defined by the\n    CIE 1964 10 degree observer. This is probably only useful if you have XYZ\n    points you want to do calculations on that were somehow measured using the\n    CIE 1964 color matching functions, perhaps via a spectrophotometer;\n    consumer equipment (monitors, cameras, etc.) assumes the use of the CIE\n    1931 standard observer in all cases I know of.\n\n    \"\"\"\n    if observer == \"CIE 1931 2 deg\":\n        return np.asarray(cie1931[name], dtype=float)\n    elif observer == \"CIE 1964 10 deg\":\n        return np.asarray(cie1964[name], dtype=float)\n    else:\n        raise ValueError(\"observer must be 'CIE 1931 2 deg' or \"\n                         \"'CIE 1964 10 deg', not %s\" % (observer,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_XYZ100_w(whitepoint):\n    if isinstance(whitepoint, str):\n        return standard_illuminant_XYZ100(whitepoint)\n    else:\n        whitepoint = np.asarray(whitepoint, dtype=float)\n        if whitepoint.shape[-1] != 3:\n            raise ValueError(\"Bad whitepoint shape\")\n        return whitepoint", "response": "A convenience function for getting a single object from a set of XYZ values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef machado_et_al_2009_matrix(cvd_type, severity):\n\n    assert 0 <= severity <= 100\n\n    fraction = severity % 10\n\n    low = int(severity - fraction)\n    high = low + 10\n    assert low <= severity <= high\n\n    low_matrix = np.asarray(MACHADO_ET_AL_MATRICES[cvd_type][low])\n    if severity == 100:\n        # Don't try interpolating between 100 and 110, there is no 110...\n        return low_matrix\n    high_matrix = np.asarray(MACHADO_ET_AL_MATRICES[cvd_type][high])\n    return ((1 - fraction / 10.0) * low_matrix\n            + fraction / 10.0 * high_matrix)", "response": "Retrieve a matrix for simulating anomalous color vision."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_retval_checker(self):\n        rvchk = self.retval.checker\n        if rvchk:\n            def _checker(value):\n                if not rvchk.check(value):\n                    raise self._type_error(\n                        \"Incorrect return type in %s: expected %s got %s\" %\n                        (self.name_bt, rvchk.name(),\n                         checker_for_type(type(value)).name())\n                    )\n        else:\n            def _checker(value):\n                pass\n        return _checker", "response": "Create a function that checks the return value of the function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a function that checks the signature of the source function.", "response": "def _make_args_checker(self):\n        \"\"\"\n        Create a function that checks signature of the source function.\n        \"\"\"\n        def _checker(*args, **kws):\n            # Check if too many arguments are provided\n            nargs = len(args)\n            nnonvaargs = min(nargs, self._max_positional_args)\n            if nargs > self._max_positional_args and self._ivararg is None:\n                raise self._too_many_args_error(nargs)\n\n            # Check if there are too few positional arguments (without defaults)\n            if nargs < self._min_positional_args:\n                missing = [p.name\n                           for p in self.params[nargs:self._min_positional_args]\n                           if p.name not in kws]\n                # The \"missing\" arguments may still be provided as keywords, in\n                # which case it's not an error at all.\n                if missing:\n                    raise self._too_few_args_error(missing, \"positional\")\n\n            # Check if there are too few required keyword arguments\n            if self._required_kwonly_args:\n                missing = [kw\n                           for kw in self._required_kwonly_args\n                           if kw not in kws]\n                if missing:\n                    raise self._too_few_args_error(missing, \"keyword\")\n\n            # Check types of positional arguments\n            for i, argvalue in enumerate(args):\n                param = self.params[i if i < self._max_positional_args else\n                                    self._ivararg]\n                if param.checker and not (\n                    param.checker.check(argvalue) or\n                    param.has_default and\n                        (argvalue is param.default or argvalue == param.default)\n                ):\n                    raise self._param_type_error(param, param.name, argvalue)\n\n            # Check types of keyword arguments\n            for argname, argvalue in kws.items():\n                argindex = self._iargs.get(argname)\n                if argindex is not None and argindex < nnonvaargs:\n                    raise self._repeating_arg_error(argname)\n                index = self._iargs.get(argname)\n                if index is None:\n                    index = self._ivarkws\n                if index is None:\n                    s = \"%s got an unexpected keyword argument `%s`\" % \\\n                        (self.name_bt, argname)\n                    raise self._type_error(s)\n                param = self.params[index]\n                if param.checker and not (\n                    param.checker.check(argvalue) or\n                    param.has_default and\n                        (argvalue is param.default or argvalue == param.default)\n                ):\n                    raise self._param_type_error(param, argname, argvalue)\n\n        return _checker"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a checker function for the given type t.", "response": "def checker_for_type(t):\n    \"\"\"\n    Return \"checker\" function for the given type `t`.\n\n    This checker function will accept a single argument (of any type), and\n    return True if the argument matches type `t`, or False otherwise. For\n    example:\n\n        chkr = checker_for_type(int)\n        assert chkr.check(123) is True\n        assert chkr.check(\"5\") is False\n    \"\"\"\n    try:\n        if t is True:\n            return true_checker\n        if t is False:\n            return false_checker\n        checker = memoized_type_checkers.get(t)\n        if checker is not None:\n            return checker\n        hashable = True\n    except TypeError:\n        # Exception may be raised if `t` is not hashable (e.g. a dict)\n        hashable = False\n\n    # The type checker needs to be created\n    checker = _create_checker_for_type(t)\n    if hashable:\n        memoized_type_checkers[t] = checker\n    return checker"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prepare_value(val, maxlen=50, notype=False):\n    if val is None or val is True or val is False:\n        return str(val)\n    sval = repr(val)\n    sval = sval.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"`\", \"'\")\n    if len(sval) > maxlen:\n        sval = sval[:maxlen - 4] + \"...\" + sval[-1]\n    if notype:\n        return sval\n    else:\n        tval = checker_for_type(type(val)).name()\n        return \"%s of type %s\" % (sval, tval)", "response": "Prepare a value for the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning posessive form of numeral n.", "response": "def _nth_str(n):\n    \"\"\"Return posessive form of numeral `n`: 1st, 2nd, 3rd, etc.\"\"\"\n    if n % 10 == 1 and n % 100 != 11:\n        return \"%dst\" % n\n    if n % 10 == 2 and n % 100 != 12:\n        return \"%dnd\" % n\n    if n % 10 == 3 and n % 100 != 13:\n        return \"%drd\" % n\n    return \"%dth\" % n"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef train(self, text, className):\n        self.data.increaseClass(className)\n\n        tokens = self.tokenizer.tokenize(text)\n        for token in tokens:\n            token = self.tokenizer.remove_stop_words(token)\n            token = self.tokenizer.remove_punctuation(token)\n            self.data.increaseToken(token, className)", "response": "Train the data using the given text and class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hid_device_path_exists(device_path, guid = None):\r\n    # expecing HID devices\r\n    if not guid:\r\n        guid = winapi.GetHidGuid()\r\n\r\n    info_data         = winapi.SP_DEVINFO_DATA()\r\n    info_data.cb_size = sizeof(winapi.SP_DEVINFO_DATA)\r\n\r\n    with winapi.DeviceInterfaceSetInfo(guid) as h_info:\r\n        for interface_data in winapi.enum_device_interfaces(h_info, guid):\r\n            test_device_path = winapi.get_device_path(h_info,\r\n                    interface_data,\r\n                    byref(info_data))\r\n            if test_device_path == device_path:\r\n                return True\r\n    # Not any device now with that path\r\n    return False", "response": "Test if required device_path is still valid"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds all HID devices connected to the system", "response": "def find_all_hid_devices():\r\n    \"Finds all HID devices connected to the system\"\r\n    #\r\n    # From DDK documentation (finding and Opening HID collection):\r\n    # After a user-mode application is loaded, it does the following sequence\r\n    # of operations:\r\n    #\r\n    #   * Calls HidD_GetHidGuid to obtain the system-defined GUID for HIDClass\r\n    #     devices.\r\n    #\r\n    #   * Calls SetupDiGetClassDevs to obtain a handle to an opaque device\r\n    #     information set that describes the device interfaces supported by all\r\n    #     the HID collections currently installed in the system. The\r\n    #     application should specify DIGCF_PRESENT and DIGCF_INTERFACEDEVICE\r\n    #     in the Flags parameter passed to SetupDiGetClassDevs.\r\n    #\r\n    #   * Calls SetupDiEnumDeviceInterfaces repeatedly to retrieve all the\r\n    #     available interface information.\r\n    #\r\n    #   * Calls SetupDiGetDeviceInterfaceDetail to format interface information\r\n    #     for each collection as a SP_INTERFACE_DEVICE_DETAIL_DATA structure.\r\n    #     The device_path member of this structure contains the user-mode name\r\n    #     that the application uses with the Win32 function CreateFile to\r\n    #     obtain a file handle to a HID collection.\r\n    #\r\n    # get HID device class guid\r\n    guid = winapi.GetHidGuid()\r\n\r\n    # retrieve all the available interface information.\r\n    results = []\r\n    required_size = DWORD()\r\n\r\n    info_data         = winapi.SP_DEVINFO_DATA()\r\n    info_data.cb_size = sizeof(winapi.SP_DEVINFO_DATA)\r\n\r\n    with winapi.DeviceInterfaceSetInfo(guid) as h_info:\r\n        for interface_data in winapi.enum_device_interfaces(h_info, guid):\r\n            device_path = winapi.get_device_path(h_info,\r\n                    interface_data,\r\n                    byref(info_data))\r\n\r\n            parent_device = c_ulong()\r\n\r\n            #get parent instance id (so we can discriminate on port)\r\n            if setup_api.CM_Get_Parent(byref(parent_device),\r\n                    info_data.dev_inst, 0) != 0: #CR_SUCCESS = 0\r\n                parent_device.value = 0 #null\r\n\r\n            #get unique instance id string\r\n            required_size.value = 0\r\n            winapi.SetupDiGetDeviceInstanceId(h_info, byref(info_data),\r\n                    None, 0,\r\n                    byref(required_size) )\r\n\r\n            device_instance_id = create_unicode_buffer(required_size.value)\r\n            if required_size.value > 0:\r\n                winapi.SetupDiGetDeviceInstanceId(h_info, byref(info_data),\r\n                        device_instance_id, required_size,\r\n                        byref(required_size) )\r\n\r\n                hid_device = HidDevice(device_path,\r\n                        parent_device.value, device_instance_id.value )\r\n            else:\r\n                hid_device = HidDevice(device_path, parent_device.value )\r\n\r\n            # add device to results, if not protected\r\n            if hid_device.vendor_id:\r\n                results.append(hid_device)\r\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_hids(target_vid = 0, target_pid = 0, output = None):\r\n    # first be kind with local encodings\r\n    if not output:\r\n        # beware your script should manage encodings\r\n        output = sys.stdout\r\n    # then the big cheese...\r\n    from . import tools\r\n    all_hids = None\r\n    if target_vid:\r\n        if target_pid:\r\n            # both vendor and product Id provided\r\n            device_filter = HidDeviceFilter(vendor_id = target_vid,\r\n                    product_id = target_pid)\r\n        else:\r\n            # only vendor id\r\n            device_filter = HidDeviceFilter(vendor_id = target_vid)\r\n\r\n        all_hids = device_filter.get_devices()\r\n    else:\r\n        all_hids = find_all_hid_devices()\r\n    if all_hids:\r\n        print(\"Found HID class devices!, writting details...\")\r\n        for dev in all_hids:\r\n            device_name = str(dev)\r\n            output.write(device_name)\r\n            output.write('\\n\\n  Path:      %s\\n' % dev.device_path)\r\n            output.write('\\n  Instance:  %s\\n' % dev.instance_id)\r\n            output.write('\\n  Port (ID): %s\\n' % dev.get_parent_instance_id())\r\n            output.write('\\n  Port (str):%s\\n' % str(dev.get_parent_device()))\r\n            #\r\n            try:\r\n                dev.open()\r\n                tools.write_documentation(dev, output)\r\n            finally:\r\n                dev.close()\r\n        print(\"done!\")\r\n    else:\r\n        print(\"There's not any non system HID class device available\")", "response": "Show all HID devices conected to PC hosts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict of all devices that match the parent device id.", "response": "def get_devices_by_parent(self, hid_filter=None):\r\n        \"\"\"Group devices returned from filter query in order \\\r\n        by devcice parent id.\r\n        \"\"\"\r\n        all_devs = self.get_devices(hid_filter)\r\n        dev_group = dict()\r\n        for hid_device in all_devs:\r\n            #keep a list of known devices matching parent device Ids\r\n            parent_id = hid_device.get_parent_instance_id()\r\n            device_set = dev_group.get(parent_id, [])\r\n            device_set.append(hid_device)\r\n            if parent_id not in dev_group:\r\n                #add new\r\n                dev_group[parent_id] = device_set\r\n        return dev_group"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_parent_device(self):\r\n        if not self.parent_instance_id:\r\n            return \"\"\r\n        dev_buffer_type = winapi.c_tchar * MAX_DEVICE_ID_LEN\r\n        dev_buffer = dev_buffer_type()\r\n        try:\r\n            if winapi.CM_Get_Device_ID(self.parent_instance_id, byref(dev_buffer),\r\n                    MAX_DEVICE_ID_LEN, 0) == 0: #success\r\n                return dev_buffer.value\r\n            return \"\"\r\n        finally:\r\n            del dev_buffer\r\n            del dev_buffer_type", "response": "Retreive parent device string id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens the HID device and obtain Collection Information.", "response": "def open(self, output_only = False, shared = True):\r\n        \"\"\"Open HID device and obtain 'Collection Information'.\r\n        It effectively prepares the HidDevice object for reading and writing\r\n        \"\"\"\r\n        if self.is_opened():\r\n            raise HIDError(\"Device already opened\")\r\n        sharing_flags = 0\r\n        if shared:\r\n            sharing_flags = winapi.FILE_SHARE_READ | winapi.FILE_SHARE_WRITE\r\n        hid_handle = winapi.CreateFile(\r\n            self.device_path,\r\n            winapi.GENERIC_READ | winapi.GENERIC_WRITE,\r\n            sharing_flags,\r\n            None, # no security\r\n            winapi.OPEN_EXISTING,\r\n            winapi.FILE_ATTRIBUTE_NORMAL | winapi.FILE_FLAG_OVERLAPPED,\r\n            0 )\r\n\r\n        if not hid_handle or hid_handle == INVALID_HANDLE_VALUE:\r\n            raise HIDError(\"Error opening HID device: %s\\n\"%self.product_name)\r\n        #get pre parsed data\r\n        ptr_preparsed_data = ctypes.c_void_p()\r\n        if not hid_dll.HidD_GetPreparsedData(int(hid_handle),\r\n                byref(ptr_preparsed_data)):\r\n            winapi.CloseHandle(int(hid_handle))\r\n            raise HIDError(\"Failure to get HID pre parsed data\")\r\n        self.ptr_preparsed_data = ptr_preparsed_data\r\n\r\n        self.hid_handle = hid_handle\r\n\r\n        #get top level capabilities\r\n        self.hid_caps = winapi.HIDP_CAPS()\r\n        HidStatus( hid_dll.HidP_GetCaps(ptr_preparsed_data,\r\n            byref(self.hid_caps)) )\r\n\r\n        #proceed with button capabilities\r\n        caps_length = c_ulong()\r\n\r\n        all_items = [\\\r\n            (HidP_Input,   winapi.HIDP_BUTTON_CAPS,\r\n                self.hid_caps.number_input_button_caps,\r\n                hid_dll.HidP_GetButtonCaps\r\n            ),\r\n            (HidP_Input,   winapi.HIDP_VALUE_CAPS,\r\n                self.hid_caps.number_input_value_caps,\r\n                hid_dll.HidP_GetValueCaps\r\n            ),\r\n            (HidP_Output,  winapi.HIDP_BUTTON_CAPS,\r\n                self.hid_caps.number_output_button_caps,\r\n                hid_dll.HidP_GetButtonCaps\r\n            ),\r\n            (HidP_Output,  winapi.HIDP_VALUE_CAPS,\r\n                self.hid_caps.number_output_value_caps,\r\n                hid_dll.HidP_GetValueCaps\r\n            ),\r\n            (HidP_Feature, winapi.HIDP_BUTTON_CAPS,\r\n                self.hid_caps.number_feature_button_caps,\r\n                hid_dll.HidP_GetButtonCaps\r\n            ),\r\n            (HidP_Feature, winapi.HIDP_VALUE_CAPS,\r\n                self.hid_caps.number_feature_value_caps,\r\n                hid_dll.HidP_GetValueCaps\r\n            ),\r\n        ]\r\n\r\n        for report_kind, struct_kind, max_items, get_control_caps in all_items:\r\n            if not int(max_items):\r\n                continue #nothing here\r\n            #create storage for control/data\r\n            ctrl_array_type = struct_kind * max_items\r\n            ctrl_array_struct = ctrl_array_type()\r\n\r\n            #target max size for API function\r\n            caps_length.value = max_items\r\n            HidStatus( get_control_caps(\\\r\n                report_kind,\r\n                byref(ctrl_array_struct),\r\n                byref(caps_length),\r\n                ptr_preparsed_data) )\r\n            #keep reference of usages\r\n            for idx in range(caps_length.value):\r\n                usage_item = HidPUsageCaps( ctrl_array_struct[idx] )\r\n                #by report type\r\n                if report_kind not in self.usages_storage:\r\n                    self.usages_storage[report_kind] = list()\r\n                self.usages_storage[report_kind].append( usage_item )\r\n                #also add report_id to known reports set\r\n                if report_kind not in self.report_set:\r\n                    self.report_set[report_kind] = set()\r\n                self.report_set[report_kind].add( usage_item.report_id )\r\n            del ctrl_array_struct\r\n            del ctrl_array_type\r\n\r\n        # now is the time to consider the device opened, as report\r\n        # handling threads enforce it\r\n        self.__open_status = True\r\n\r\n        #now prepare the input report handler\r\n        self.__input_report_templates = dict()\r\n        if not output_only and self.hid_caps.input_report_byte_length and \\\r\n                HidP_Input in self.report_set:\r\n            #first make templates for easy parsing input reports\r\n            for report_id in self.report_set[HidP_Input]:\r\n                self.__input_report_templates[report_id] = \\\r\n                        HidReport( self, HidP_Input, report_id )\r\n            #prepare input reports handlers\r\n            self._input_report_queue = HidDevice.InputReportQueue( \\\r\n                    self.max_input_queue_size,\r\n                    self.hid_caps.input_report_byte_length)\r\n            self.__input_processing_thread = \\\r\n                    HidDevice.InputReportProcessingThread(self)\r\n            self.__reading_thread = HidDevice.InputReportReaderThread( \\\r\n                self, self.hid_caps.input_report_byte_length)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_physical_descriptor(self):\r\n        raw_data_type = c_ubyte * 1024\r\n        raw_data = raw_data_type()\r\n        if hid_dll.HidD_GetPhysicalDescriptor(self.hid_handle,\r\n            byref(raw_data), 1024 ):\r\n            return [x for x in raw_data]\r\n        return []", "response": "Returns a list of physical HID device descriptors."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends an output report to the HID device.", "response": "def send_output_report(self, data):\r\n        \"\"\"Send input/output/feature report ID = report_id, data should be a\r\n        c_ubyte object with included the required report data\r\n        \"\"\"\r\n        assert( self.is_opened() )\r\n        #make sure we have c_ubyte array storage\r\n        if not ( isinstance(data, ctypes.Array) and \\\r\n                issubclass(data._type_, c_ubyte) ):\r\n            raw_data_type = c_ubyte * len(data)\r\n            raw_data = raw_data_type()\r\n            for index in range( len(data) ):\r\n                raw_data[index] = data[index]\r\n        else:\r\n            raw_data = data\r\n        #\r\n        # Adding a lock when writing (overlapped writes)\r\n        over_write = winapi.OVERLAPPED()\r\n        over_write.h_event = winapi.CreateEvent(None, 0, 0, None)\r\n        if over_write.h_event:\r\n            try:\r\n                overlapped_write = over_write\r\n                winapi.WriteFile(int(self.hid_handle), byref(raw_data), len(raw_data),\r\n                    None, byref(overlapped_write)) #none overlapped\r\n                error = ctypes.GetLastError()\r\n                if error == winapi.ERROR_IO_PENDING:\r\n                    # overlapped operation in progress\r\n                    result = error\r\n                elif error == 1167:\r\n                    raise HIDError(\"Error device disconnected before write\")\r\n                else:\r\n                    raise HIDError(\"Error %d when trying to write to HID \"\\\r\n                        \"device: %s\"%(error, ctypes.FormatError(error)) )\r\n                result = winapi.WaitForSingleObject(overlapped_write.h_event, 10000 )\r\n                if result != winapi.WAIT_OBJECT_0:\r\n                    # If the write times out make sure to\r\n                    # cancel it, otherwise memory could\r\n                    # get corrupted if the async write\r\n                    # completes after this functions returns\r\n                    winapi.CancelIo( int(self.hid_handle) )\r\n                    raise HIDError(\"Write timed out\")\r\n            finally:\r\n                # Make sure the event is closed so resources aren't leaked\r\n                winapi.CloseHandle(over_write.h_event)\r\n        else:\r\n            return winapi.WriteFile(int(self.hid_handle), byref(raw_data),\r\n                len(raw_data),\r\n                None, None) #none overlapped\r\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_feature_report(self, data):\r\n        assert( self.is_opened() )\r\n        #make sure we have c_ubyte array storage\r\n        if not ( isinstance(data, ctypes.Array) and issubclass(data._type_,\r\n                c_ubyte) ):\r\n            raw_data_type = c_ubyte * len(data)\r\n            raw_data = raw_data_type()\r\n            for index in range( len(data) ):\r\n                raw_data[index] = data[index]\r\n        else:\r\n            raw_data = data\r\n\r\n        return hid_dll.HidD_SetFeature(int(self.hid_handle), byref(raw_data),\r\n                len(raw_data))", "response": "Send a feature report to the HID device."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __reset_vars(self):\r\n        self.__button_caps_storage = list()\r\n        self.usages_storage = dict()\r\n        self.report_set = dict()\r\n        self.ptr_preparsed_data = None\r\n        self.hid_handle = None\r\n\r\n        #don't clean up the report queue because the\r\n        #consumer & producer threads might needed it\r\n        self.__evt_handlers = dict()\r\n        #other\r\n        self.__reading_thread = None\r\n        self.__input_processing_thread = None\r\n        self._input_report_queue = None", "response": "Reset the internal state of the object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclose the HID device and all resources associated with the device.", "response": "def close(self):\r\n        \"\"\"Release system resources\"\"\"\r\n        # free parsed data\r\n        if not self.is_opened():\r\n            return\r\n        self.__open_status = False\r\n\r\n        # abort all running threads first\r\n        if self.__reading_thread and self.__reading_thread.is_alive():\r\n            self.__reading_thread.abort()\r\n\r\n        #avoid posting new reports\r\n        if self._input_report_queue:\r\n            self._input_report_queue.release_events()\r\n\r\n        if self.__input_processing_thread and \\\r\n                self.__input_processing_thread.is_alive():\r\n            self.__input_processing_thread.abort()\r\n\r\n        #properly close API handlers and pointers\r\n        if self.ptr_preparsed_data:\r\n            ptr_preparsed_data = self.ptr_preparsed_data\r\n            self.ptr_preparsed_data = None\r\n            hid_dll.HidD_FreePreparsedData(ptr_preparsed_data)\r\n\r\n        # wait for the reading thread to complete before closing device handle\r\n        if self.__reading_thread:\r\n            self.__reading_thread.join()\r\n\r\n        if self.hid_handle:\r\n            winapi.CloseHandle(self.hid_handle)\r\n\r\n        # make sure report procesing thread is closed\r\n        if self.__input_processing_thread:\r\n            self.__input_processing_thread.join()\r\n\r\n        #reset vars (for GC)\r\n        button_caps_storage = self.__button_caps_storage\r\n        self.__reset_vars()\r\n\r\n        while button_caps_storage:\r\n            item = button_caps_storage.pop()\r\n            del item"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind input report referencing HID usage control / data item", "response": "def __find_reports(self, report_type, usage_page, usage_id = 0):\r\n        \"Find input report referencing HID usage control/data item\"\r\n        if not self.is_opened():\r\n            raise HIDError(\"Device must be opened\")\r\n        #\r\n        results = list()\r\n        if usage_page:\r\n            for report_id in self.report_set.get( report_type, set() ):\r\n                #build report object, gathering usages matching report_id\r\n                report_obj = HidReport(self, report_type, report_id)\r\n                if get_full_usage_id(usage_page, usage_id) in report_obj:\r\n                    results.append( report_obj )\r\n        else:\r\n            #all (any one)\r\n            for report_id in self.report_set.get(report_type, set()):\r\n                report_obj = HidReport(self, report_type, report_id)\r\n                results.append( report_obj )\r\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_any_reports(self, usage_page = 0, usage_id = 0):\r\n        items = [\r\n            (HidP_Input,    self.find_input_reports(usage_page, usage_id)),\r\n            (HidP_Output,   self.find_output_reports(usage_page, usage_id)),\r\n            (HidP_Feature,  self.find_feature_reports(usage_page, usage_id)),\r\n        ]\r\n        return dict([(t, r) for t, r in items if r])", "response": "Find any reports in a usage control or data item."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndefaulting raw input report data handler", "response": "def _process_raw_report(self, raw_report):\r\n        \"Default raw input report data handler\"\r\n        if not self.is_opened():\r\n            return\r\n        if not self.__evt_handlers and not self.__raw_handler:\r\n            return\r\n\r\n        if not raw_report[0]  and \\\r\n                (raw_report[0] not in self.__input_report_templates):\r\n            # windows sends an empty array when disconnecting\r\n            # but, this might have a collision with report_id = 0\r\n            if not hid_device_path_exists(self.device_path):\r\n                #windows XP sends empty report when disconnecting\r\n                self.__reading_thread.abort() #device disconnected\r\n            return\r\n\r\n        if self.__raw_handler:\r\n            #this might slow down data throughput, but at the expense of safety\r\n            self.__raw_handler(helpers.ReadOnlyList(raw_report))\r\n            return\r\n\r\n        # using pre-parsed report templates, by report id\r\n        report_template = self.__input_report_templates[raw_report[0]]\r\n        # old condition snapshot\r\n        old_values = report_template.get_usages()\r\n        # parse incoming data\r\n        report_template.set_raw_data(raw_report)\r\n        # and compare it\r\n        event_applies = self.evt_decision\r\n        evt_handlers  = self.__evt_handlers\r\n        for key in report_template.keys():\r\n            if key not in evt_handlers:\r\n                continue\r\n            #check if event handler exist!\r\n            for event_kind, handlers in evt_handlers[key].items():\r\n                #key=event_kind, values=handler set\r\n                new_value = report_template[key].value\r\n                if not event_applies[event_kind](old_values[key], new_value):\r\n                    continue\r\n                #decision applies, call handlers\r\n                for function_handler in handlers:\r\n                    #check if the application wants some particular parameter\r\n                    if handlers[function_handler]:\r\n                        function_handler(new_value,\r\n                                event_kind, handlers[function_handler])\r\n                    else:\r\n                        function_handler(new_value, event_kind)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the input usage ID in the input reports set", "response": "def find_input_usage(self, full_usage_id):\r\n        \"\"\"Check if full usage Id included in input reports set\r\n        Parameters:\r\n            full_usage_id       Full target usage, use get_full_usage_id\r\n\r\n        Returns:\r\n            Report ID as integer value, or None if report does not exist with\r\n            target usage. Nottice that report ID 0 is a valid report.\r\n        \"\"\"\r\n        for report_id, report_obj in self.__input_report_templates.items():\r\n            if full_usage_id in report_obj:\r\n                return report_id\r\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_event_handler(self, full_usage_id, handler_function,\r\n            event_kind = HID_EVT_ALL, aux_data = None):\r\n        \"\"\"Add event handler for usage value/button changes,\r\n        returns True if the handler function was updated\"\"\"\r\n        report_id = self.find_input_usage(full_usage_id)\r\n        if report_id != None:\r\n            # allow first zero to trigger changes and releases events\r\n            self.__input_report_templates[report_id][full_usage_id].__value = None\r\n        if report_id == None or not handler_function:\r\n            # do not add handler\r\n            return False\r\n        assert(isinstance(handler_function, collections.Callable)) # must be a function\r\n        # get dictionary for full usages\r\n        top_map_handler = self.__evt_handlers.get(full_usage_id, dict())\r\n        event_handler_set = top_map_handler.get(event_kind, dict())\r\n        # update handler\r\n        event_handler_set[handler_function] = aux_data\r\n        if event_kind not in top_map_handler:\r\n            top_map_handler[event_kind] = event_handler_set\r\n        if full_usage_id not in self.__evt_handlers:\r\n            self.__evt_handlers[full_usage_id] = top_map_handler\r\n        return True", "response": "Adds an event handler for the given usage value and button changes returns True if the handler function was updated"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_value(self, value):\r\n        if self.__is_value_array:\r\n            if len(value) == self.__report_count:\r\n                for index, item in enumerate(value):\r\n                    self.__setitem__(index, item)\r\n            else:\r\n                raise ValueError(\"Value size should match report item size \"\\\r\n                    \"length\" )\r\n        else:\r\n            self.__value = value & ((1 << self.__bit_size) - 1)", "response": "Set usage value within report"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the usage representation string of the HID device if available.", "response": "def get_usage_string(self):\r\n        \"\"\"Returns usage representation string (as embedded in HID device\r\n        if available)\r\n        \"\"\"\r\n        if self.string_index:\r\n            usage_string_type = c_wchar * MAX_HID_STRING_LENGTH\r\n            # 128 max string length\r\n            abuffer = usage_string_type()\r\n            hid_dll.HidD_GetIndexedString(\r\n                self.hid_report.get_hid_object().hid_handle,\r\n                self.string_index,\r\n                byref(abuffer), MAX_HID_STRING_LENGTH-1 )\r\n            return abuffer.value\r\n        return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_usages(self):\r\n        \"Return a dictionary mapping full usages Ids to plain values\"\r\n        result = dict()\r\n        for key, usage in self.items():\r\n            result[key] = usage.value\r\n        return result", "response": "Return a dictionary mapping full usages Ids to plain values"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nallocates the raw data for the new version of the object.", "response": "def __alloc_raw_data(self, initial_values=None):\r\n        \"\"\"Pre-allocate re-usagle memory\"\"\"\r\n        #allocate c_ubyte storage\r\n        if self.__raw_data == None: #first time only, create storage\r\n            raw_data_type = c_ubyte * self.__raw_report_size\r\n            self.__raw_data = raw_data_type()\r\n        elif initial_values == self.__raw_data:\r\n            # already\r\n            return\r\n        else:\r\n            #initialize\r\n            ctypes.memset(self.__raw_data, 0, len(self.__raw_data))\r\n        if initial_values:\r\n            for index in range(len(initial_values)):\r\n                self.__raw_data[index] = initial_values[index]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_raw_data(self, raw_data):\r\n        #pre-parsed data should exist\r\n        assert(self.__hid_object.is_opened())\r\n        #valid length\r\n        if len(raw_data) != self.__raw_report_size:\r\n            raise HIDError( \"Report size has to be %d elements (bytes)\" \\\r\n                % self.__raw_report_size )\r\n        # copy to internal storage\r\n        self.__alloc_raw_data(raw_data)\r\n\r\n        if not self.__usage_data_list: # create HIDP_DATA buffer\r\n            max_items = hid_dll.HidP_MaxDataListLength(self.__report_kind,\r\n                self.__hid_object.ptr_preparsed_data)\r\n            data_list_type = winapi.HIDP_DATA * max_items\r\n            self.__usage_data_list = data_list_type()\r\n        #reference HIDP_DATA buffer\r\n        data_list = self.__usage_data_list\r\n        data_len = c_ulong(len(data_list))\r\n\r\n        #reset old values\r\n        for item in self.values():\r\n            if item.is_value_array():\r\n                item.value = [0, ]*len(item)\r\n            else:\r\n                item.value = 0\r\n        #ready, parse raw data\r\n        HidStatus( hid_dll.HidP_GetData(self.__report_kind,\r\n            byref(data_list), byref(data_len),\r\n            self.__hid_object.ptr_preparsed_data,\r\n            byref(self.__raw_data), len(self.__raw_data)) )\r\n\r\n        #set values on internal report item objects\r\n        for idx in range(data_len.value):\r\n            value_item = data_list[idx]\r\n            report_item = self.__idx_items.get(value_item.data_index)\r\n            if not report_item:\r\n                # This is not expected to happen\r\n                continue\r\n            if report_item.is_value():\r\n                report_item.value = value_item.value.raw_value\r\n            elif report_item.is_button():\r\n                report_item.value = value_item.value.on\r\n            else:\r\n                pass # HID API should give us either, at least one of 'em\r\n        #get values of array items\r\n        for item in self.__value_array_items:\r\n            #ask hid API to parse\r\n            HidStatus( hid_dll.HidP_GetUsageValueArray(self.__report_kind,\r\n                item.page_id,\r\n                0, #link collection\r\n                item.usage_id, #short usage\r\n                byref(item.value_array), #output data (c_ubyte storage)\r\n                len(item.value_array), self.__hid_object.ptr_preparsed_data,\r\n                byref(self.__raw_data), len(self.__raw_data)) )", "response": "Set usage values based on given raw data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __prepare_raw_data(self):\r\n        \"Format internal __raw_data storage according to usages setting\"\r\n        #pre-parsed data should exist\r\n        if not self.__hid_object.ptr_preparsed_data:\r\n            raise HIDError(\"HID object close or unable to request pre parsed \"\\\r\n                \"report data\")\r\n\r\n        # make sure pre-memory allocation already done\r\n        self.__alloc_raw_data()\r\n\r\n        try:\r\n            HidStatus( hid_dll.HidP_InitializeReportForID(self.__report_kind,\r\n                self.__report_id, self.__hid_object.ptr_preparsed_data,\r\n                byref(self.__raw_data), self.__raw_report_size) )\r\n            #\r\n        except HIDError:\r\n            self.__raw_data[0] = self.__report_id\r\n\r\n        #check if we have pre-allocated usage storage\r\n        if not self.__usage_data_list: # create HIDP_DATA buffer\r\n            max_items = hid_dll.HidP_MaxDataListLength(self.__report_kind,\r\n                self.__hid_object.ptr_preparsed_data)\r\n            if not max_items:\r\n                raise HIDError(\"Internal error while requesting usage length\")\r\n            data_list_type = winapi.HIDP_DATA * max_items\r\n            self.__usage_data_list = data_list_type()\r\n\r\n        #reference HIDP_DATA buffer\r\n        data_list = self.__usage_data_list\r\n        #set buttons and values usages first\r\n        n_total_usages = 0\r\n        single_usage = USAGE()\r\n        single_usage_len = c_ulong()\r\n        for data_index, report_item in self.__idx_items.items():\r\n            if (not report_item.is_value_array()) and \\\r\n                    report_item.value != None:\r\n                #set by user, include in request\r\n                if report_item.is_button() and report_item.value:\r\n                    # windows just can't handle button arrays!, we just don't\r\n                    # know if usage is button array or plain single usage, so\r\n                    # we set all usages at once\r\n                    single_usage.value = report_item.usage_id\r\n                    single_usage_len.value = 1\r\n                    HidStatus( hid_dll.HidP_SetUsages(self.__report_kind,\r\n                        report_item.page_id, 0,\r\n                        byref(single_usage), byref(single_usage_len),\r\n                        self.__hid_object.ptr_preparsed_data,\r\n                        byref(self.__raw_data), self.__raw_report_size) )\r\n                    continue\r\n                elif report_item.is_value() and \\\r\n                        not report_item.is_value_array():\r\n                    data_list[n_total_usages].value.raw_value = report_item.value\r\n                else:\r\n                    continue #do nothing\r\n                data_list[n_total_usages].reserved = 0 #reset\r\n                data_list[n_total_usages].data_index = data_index #reference\r\n                n_total_usages += 1\r\n        #set data if any usage is not 'none' (and not any value array)\r\n        if n_total_usages:\r\n            #some usages set\r\n            usage_len = c_ulong(n_total_usages)\r\n            HidStatus( hid_dll.HidP_SetData(self.__report_kind,\r\n                byref(data_list), byref(usage_len),\r\n                self.__hid_object.ptr_preparsed_data,\r\n                byref(self.__raw_data), self.__raw_report_size) )\r\n        #set values based on value arrays\r\n        for report_item in self.__value_array_items:\r\n            HidStatus( hid_dll.HidP_SetUsageValueArray(self.__report_kind,\r\n                report_item.page_id,\r\n                0, #all link collections\r\n                report_item.usage_id,\r\n                byref(report_item.value_array),\r\n                len(report_item.value_array),\r\n                self.__hid_object.ptr_preparsed_data, byref(self.__raw_data),\r\n                len(self.__raw_data)) )", "response": "Format internal __raw_data storage according to usages setting"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget raw HID report based on internal report item settings", "response": "def get_raw_data(self):\r\n        \"\"\"Get raw HID report based on internal report item settings,\r\n        creates new c_ubytes storage\r\n        \"\"\"\r\n        if self.__report_kind != HidP_Output \\\r\n                and self.__report_kind != HidP_Feature:\r\n            raise HIDError(\"Only for output or feature reports\")\r\n        self.__prepare_raw_data()\r\n        #return read-only object for internal storage\r\n        return helpers.ReadOnlyList(self.__raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self, raw_data = None):\r\n        if self.__report_kind != HidP_Output \\\r\n                and self.__report_kind != HidP_Feature:\r\n            raise HIDError(\"Only for output or feature reports\")\r\n        #valid length\r\n        if raw_data and (len(raw_data) != self.__raw_report_size):\r\n            raise HIDError(\"Report size has to be %d elements (bytes)\" \\\r\n                % self.__raw_report_size)\r\n        #should be valid report id\r\n        if raw_data and raw_data[0] != self.__report_id.value:\r\n            #hint, raw_data should be a plain list of integer values\r\n            raise HIDError(\"Not matching report id\")\r\n        #\r\n        if self.__report_kind != HidP_Output and \\\r\n                self.__report_kind != HidP_Feature:\r\n            raise HIDError(\"Can only send output or feature reports\")\r\n        #\r\n        if not raw_data:\r\n            # we'll construct the raw report\r\n            self.__prepare_raw_data()\r\n        elif not ( isinstance(raw_data, ctypes.Array) and \\\r\n                issubclass(raw_data._type_, c_ubyte) ):\r\n            # pre-memory allocation for performance\r\n            self.__alloc_raw_data(raw_data)\r\n        #reference proper object\r\n        raw_data = self.__raw_data\r\n        if self.__report_kind == HidP_Output:\r\n            return self.__hid_object.send_output_report(raw_data)\r\n        elif self.__report_kind == HidP_Feature:\r\n            return self.__hid_object.send_feature_report(raw_data)\r\n        else:\r\n            pass", "response": "Send a report to the HID device"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, do_process_raw_report = True):\r\n        \"Read report from device\"\r\n        assert(self.__hid_object.is_opened())\r\n        if self.__report_kind != HidP_Input and \\\r\n                self.__report_kind != HidP_Feature:\r\n            raise HIDError(\"Only for input or feature reports\")\r\n        # pre-alloc raw data\r\n        self.__alloc_raw_data()\r\n\r\n        # now use it\r\n        raw_data = self.__raw_data\r\n        raw_data[0] = self.__report_id\r\n        read_function = None\r\n        if self.__report_kind == HidP_Feature:\r\n            read_function = hid_dll.HidD_GetFeature\r\n        elif self.__report_kind == HidP_Input:\r\n            read_function = hid_dll.HidD_GetInputReport\r\n        if read_function and read_function(int(self.__hid_object.hid_handle),\r\n                                           byref(raw_data), len(raw_data)):\r\n            #success\r\n            if do_process_raw_report:\r\n                self.set_raw_data(raw_data)\r\n                self.__hid_object._process_raw_report(raw_data)\r\n            return helpers.ReadOnlyList(raw_data)\r\n        return helpers.ReadOnlyList([])", "response": "Read report from device"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbrowse for mute usages and set value", "response": "def set_mute(mute_value):\n    \"Browse for mute usages and set value\"\n    all_mutes = ( \\\n            (0x8, 0x9), # LED page\n            (0x1, 0xA7), # desktop page\n            (0xb, 0x2f),\n    )\n    all_target_usages = [hid.get_full_usage_id(u[0], u[1]) for u in all_mutes]\n\n    # usually you'll find and open the target device, here we'll browse for the\n    # current connected devices\n    all_devices = hid.find_all_hid_devices()\n\n    success = 0\n    if not all_devices:\n        print(\"Can't any HID device!\")\n    else:\n        # search for our target usage\n        # target pageId, usageId\n        for device in all_devices:\n            try:\n                device.open()\n                # target 'to set' value could be in feature or output reports\n                for report in device.find_output_reports() + device.find_feature_reports():\n                    for target_usage in all_target_usages:\n                        if target_usage in report:\n                            # set our value and send\n                            print(\"Found in report: {0}\\n\".format(report))\n\n                            old_raw_data = report.get_raw_data()\n                            print(\"  Empty raw report: {0}\".format(old_raw_data))\n                            report[target_usage] = value\n                            new_raw_data = report.get_raw_data()\n                            print(\"  Set raw report: {0}\\n\".format(new_raw_data))\n                            # validate that set_raw_data() is working properly\n                            report.set_raw_data( new_raw_data )\n                            report.send()\n                            success += 1\n            finally:\n                device.close()\n    # fit to sys.exit() proper result values\n    print(\"{0} Mute usage(s) set\\n\".format(success))\n    if success:\n        return 0\n    return -1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_hid_pnp(self, hid_event = None):\r\n        # keep old reference for UI updates\r\n        old_device = self.device\r\n\r\n        if hid_event:\r\n            print(\"Hey, a hid device just %s!\" % hid_event)\r\n\r\n        if hid_event == \"connected\":\r\n            # test if our device is available\r\n            if self.device:\r\n                # see, at this point we could detect multiple devices!\r\n                # but... we only want just one\r\n                pass\r\n            else:\r\n                self.test_for_connection()\r\n        elif hid_event == \"disconnected\":\r\n            # the hid object is automatically closed on disconnection we just\r\n            # test if still is plugged (important as the object might be\r\n            # closing)\r\n            if self.device and not self.device.is_plugged():\r\n                self.device = None\r\n                print(\"you removed my hid device!\")\r\n        else:\r\n            # poll for devices\r\n            self.test_for_connection()\r\n\r\n        if old_device != self.device:\r\n            # update ui\r\n            pass", "response": "Called by the HID module when the class event changes in the class of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef logging_decorator(func):\r\n    def you_will_never_see_this_name(*args, **kwargs):\r\n        \"\"\"Neither this docstring\"\"\"\r\n        print('calling %s ...' % func.__name__)\r\n        result = func(*args, **kwargs)\r\n        print('completed: %s' % func.__name__)\r\n        return result\r\n    return you_will_never_see_this_name", "response": "Decorator for logging functions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_hid_pnp(self, hid_event = None):\n        old_device = self.hid_device\n\n        if hid_event:\n            self.pnpChanged.emit(hid_event)\n\n        if hid_event == \"connected\":\n            # test if our device is available\n            if self.hid_device:\n                # see, at this point we could detect multiple devices!\n                # but... we only want just one\n                pass\n            else:\n                self.test_for_connection()\n        elif hid_event == \"disconnected\":\n            # the hid object is automatically closed on disconnection we just\n            # test if still is plugged (important as the object might be\n            # closing)\n            if self.hid_device and not self.hid_device.is_plugged():\n                self.hid_device = None\n        else:\n            # poll for devices\n            self.test_for_connection()\n        # update ui\n        if old_device != self.hid_device:\n            if hid_event == \"disconnected\":\n                self.hidConnected.emit(old_device, hid_event)\n            else:\n                self.hidConnected.emit(self.hid_device, hid_event)", "response": "Called by the HID module when the HID class has been connected or disconnected."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _on_hid_pnp(self, w_param, l_param):\r\n        \"Process WM_DEVICECHANGE system messages\"\r\n        new_status = \"unknown\"\r\n        if w_param == DBT_DEVICEARRIVAL:\r\n            # hid device attached\r\n            notify_obj = None\r\n            if int(l_param):\r\n                # Disable this error since pylint doesn't reconize\r\n                # that from_address actually exists\r\n                # pylint: disable=no-member\r\n                notify_obj = DevBroadcastDevInterface.from_address(l_param)\r\n                #confirm if the right message received\r\n            if notify_obj and \\\r\n                    notify_obj.dbcc_devicetype == DBT_DEVTYP_DEVICEINTERFACE:\r\n                #only connect if already disconnected\r\n                new_status = \"connected\"\r\n        elif w_param == DBT_DEVICEREMOVECOMPLETE:\r\n            # hid device removed\r\n            notify_obj = None\r\n            if int(l_param):\r\n                # Disable this error since pylint doesn't reconize\r\n                # that from_address actually exists\r\n                # pylint: disable=no-member\r\n                notify_obj = DevBroadcastDevInterface.from_address(l_param)\r\n            if notify_obj and \\\r\n                    notify_obj.dbcc_devicetype == DBT_DEVTYP_DEVICEINTERFACE:\r\n                #only connect if already disconnected\r\n                new_status = \"disconnected\"\r\n\r\n        #verify if need to call event handler\r\n        if new_status != \"unknown\" and new_status != self.current_status:\r\n            self.current_status = new_status\r\n            self.on_hid_pnp(self.current_status)\r\n        #\r\n        return True", "response": "Process WM_DEVICECHANGE system messages"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters HID notification events on any window returns a handler", "response": "def _register_hid_notification(self):\r\n        \"\"\"Register HID notification events on any window (passed by window\r\n        handler), returns a notification handler\"\"\"\r\n        # create structure, self initialized\r\n        notify_obj = DevBroadcastDevInterface()\r\n        h_notify = RegisterDeviceNotification(self.__hid_hwnd,\r\n                ctypes.byref(notify_obj), DEVICE_NOTIFY_WINDOW_HANDLE)\r\n        #\r\n        return int(h_notify)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _unregister_hid_notification(self):\r\n        \"Remove PnP notification handler\"\r\n        if not int(self.__h_notify):\r\n            return #invalid\r\n        result          = UnregisterDeviceNotification(self.__h_notify)\r\n        self.__h_notify = None\r\n        return int(result)", "response": "Remove PnP notification handler"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nissuing documentation report on output_file file like object", "response": "def write_documentation(self, output_file):\r\n    \"Issue documentation report on output_file file like object\"\r\n    if not self.is_opened():\r\n        raise helpers.HIDError(\"Device has to be opened to get documentation\")\r\n    #format\r\n    class CompundVarDict(object):\r\n        \"\"\"Compound variables dictionary.\r\n        Keys are strings mapping variables.\r\n        If any string has a '.' on it, it means that is an\r\n        object with an attribute. The attribute name will be\r\n        used then as the returned item value.\r\n        \"\"\"\r\n        def __init__(self, parent):\r\n            self.parent = parent\r\n        def __getitem__(self, key):\r\n            if '.' not in key:\r\n                return self.parent[key]\r\n            else:\r\n                all_keys = key.split('.')\r\n                curr_var = self.parent[all_keys[0]]\r\n                for item in all_keys[1:]:\r\n                    new_var = getattr(curr_var, item)\r\n                    curr_var = new_var\r\n                return new_var\r\n    dev_vars = vars(self)\r\n    dev_vars['main_usage_str'] = repr(\r\n            usage_pages.HidUsage(self.hid_caps.usage_page,\r\n                self.hid_caps.usage) )\r\n    output_file.write( \"\"\"\\n\\\r\nHID device documentation report\r\n===============================\r\n\r\nTop Level Details\r\n-----------------\r\n\r\nManufacturer String:    %(vendor_name)s\r\nProduct String:          %(product_name)s\r\nSerial Number:          %(serial_number)s\r\n\r\nVendor ID:              0x%(vendor_id)04x\r\nProduct ID:             0x%(product_id)04x\r\nVersion number:         0x%(version_number)04x\r\n\r\nDevice Path:            %(device_path)s\r\nDevice Instance Id:     %(instance_id)s\r\nParent Instance Id:     %(parent_instance_id)s\r\n\r\nTop level usage:        Page=0x%(hid_caps.usage_page)04x, Usage=0x%(hid_caps.usage)02x\r\nUsage identification:   %(main_usage_str)s\r\nLink collections:       %(hid_caps.number_link_collection_nodes)d collection(s)\r\n\r\nReports\r\n-------\r\n\r\nInput Report\r\n~~~~~~~~~~~~\r\nLength:     %(hid_caps.input_report_byte_length)d byte(s)\r\nButtons:    %(hid_caps.number_input_button_caps)d button(s)\r\nValues:     %(hid_caps.number_input_value_caps)d value(s)\r\n\r\nOutput Report\r\n~~~~~~~~~~~~~\r\nlength:     %(hid_caps.output_report_byte_length)d byte(s)\r\nButtons:    %(hid_caps.number_output_button_caps)d button(s)\r\nValues:     %(hid_caps.number_output_value_caps)d value(s)\r\n\r\nFeature Report\r\n~~~~~~~~~~~~~\r\nLength:     %(hid_caps.feature_report_byte_length)d byte(s)\r\nButtons:    %(hid_caps.number_feature_button_caps)d button(s)\r\nValues:     %(hid_caps.number_feature_value_caps)d value(s)\r\n\r\n\"\"\" % CompundVarDict(dev_vars)) #better than vars()!\r\n    #return\r\n    # inspect caps\r\n    for report_kind in [winapi.HidP_Input,\r\n            winapi.HidP_Output, winapi.HidP_Feature]:\r\n        all_usages = self.usages_storage.get(report_kind, [])\r\n        if all_usages:\r\n            output_file.write('*** %s Caps ***\\n\\n' % {\r\n                    winapi.HidP_Input   : \"Input\",\r\n                    winapi.HidP_Output  : \"Output\",\r\n                    winapi.HidP_Feature : \"Feature\"\r\n                    }[report_kind])\r\n            # normalize usages to allow sorting by usage or min range value\r\n            for item in all_usages:\r\n                if getattr(item, 'usage', None) != None:\r\n                    item.flat_id = item.usage\r\n                elif getattr(item, 'usage_min', None) != None:\r\n                    item.flat_id = item.usage_min\r\n                else:\r\n                    item.flat_id = None\r\n            sorted(all_usages, key=attrgetter('usage_page', 'flat_id'))\r\n            for usage_item in all_usages:\r\n                # remove helper attribute\r\n                del usage_item.flat_id\r\n\r\n                all_items = usage_item.inspect()\r\n                # sort first by 'usage_page'...\r\n                usage_page = all_items[\"usage_page\"]\r\n                del all_items[\"usage_page\"]\r\n                if \"usage\" in all_items:\r\n                    usage = all_items[\"usage\"]\r\n                    output_file.write(\"    Usage {0} ({0:#x}), \"\\\r\n                            \"Page {1:#x}\\n\".format(usage, usage_page))\r\n                    output_file.write(\"    ({0})\\n\".format(\r\n                        repr(usage_pages.HidUsage(usage_page, usage))) )\r\n                    del all_items[\"usage\"]\r\n                elif 'usage_min' in all_items:\r\n                    usage = (all_items[\"usage_min\"], all_items[\"usage_max\"])\r\n                    output_file.write(\"    Usage Range {0}~{1} ({0:#x}~{1:#x}),\"\r\n                            \" Page {2:#x} ({3})\\n\".format(\r\n                                usage[0], usage[1], usage_page,\r\n                                str(usage_pages.UsagePage(usage_page))) )\r\n                    del all_items[\"usage_min\"]\r\n                    del all_items[\"usage_max\"]\r\n                else:\r\n                    raise AttributeError(\"Expecting any usage id\")\r\n                attribs = list( all_items.keys() )\r\n                attribs.sort()\r\n                for key in attribs:\r\n                    if 'usage' in key:\r\n                        output_file.write(\"{0}{1}: {2} ({2:#x})\\n\".format(' '*8,\r\n                            key, all_items[key]))\r\n                    else:\r\n                        output_file.write(\"{0}{1}: {2}\\n\".format(' '*8,\r\n                            key, all_items[key]))\r\n                output_file.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hook_wnd_proc(self):\r\n        self.__local_wnd_proc_wrapped = WndProcType(self.local_wnd_proc)\r\n        self.__old_wnd_proc = SetWindowLong(self.__local_win_handle,\r\n                                        GWL_WNDPROC,\r\n                                        self.__local_wnd_proc_wrapped)", "response": "Attach to OS Window message handler"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unhook_wnd_proc(self):\r\n        if not self.__local_wnd_proc_wrapped:\r\n            return\r\n        SetWindowLong(self.__local_win_handle,\r\n                        GWL_WNDPROC,\r\n                        self.__old_wnd_proc)\r\n\r\n        ## Allow the ctypes wrapper to be garbage collected\r\n        self.__local_wnd_proc_wrapped = None", "response": "Restore previous Window message handler"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping for calling the handler if one exists.", "response": "def local_wnd_proc(self, h_wnd, msg, w_param, l_param):\r\n        \"\"\"\r\n        Call the handler if one exists.\r\n        \"\"\"\r\n        # performance note: has_key is the fastest way to check for a key when\r\n        # the key is unlikely to be found (which is the case here, since most\r\n        # messages will not have handlers).  This is called via a ctypes shim\r\n        # for every single windows message so dispatch speed is important\r\n        if msg in self.__msg_dict:\r\n            # if the handler returns false, we terminate the message here Note\r\n            # that we don't pass the hwnd or the message along Handlers should\r\n            # be really, really careful about returning false here\r\n            if self.__msg_dict[msg](w_param, l_param) == False:\r\n                return\r\n\r\n        # Restore the old WndProc on Destroy.\r\n        if msg == WM_DESTROY:\r\n            self.unhook_wnd_proc()\r\n\r\n        return CallWindowProc(self.__old_wnd_proc,\r\n                                h_wnd, msg, w_param, l_param)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_values(target_usage):\r\n    # browse all devices\r\n    all_devices = hid.HidDeviceFilter().get_devices()\r\n\r\n    if not all_devices:\r\n        print(\"Can't find any non system HID device connected\")\r\n    else:\r\n        # search for our target usage\r\n        usage_found = False\r\n        for device in all_devices:\r\n            try:\r\n                device.open()\r\n                # browse feature reports\r\n                for report in device.find_feature_reports():\r\n                    if target_usage in report:\r\n                        # we found our usage\r\n                        report.get()\r\n                        # print result\r\n                        print(\"The value:\", list(report[target_usage]))\r\n                        print(\"All the report: {0}\".format(report.get_raw_data()))\r\n                        usage_found = True\r\n            finally:\r\n                device.close()\r\n        if not usage_found:\r\n            print(\"The target device was found, but the requested usage does not exist!\\n\")", "response": "read feature report values"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_mute(mute_value):\n    \"Browse for mute usages and set value\"\n    all_mutes = ( \\\n            (0x8, 0x9), # LED page\n            (0x1, 0xA7), # desktop page\n            (0xb, 0x2f),\n    )\n    all_target_usages = [hid.get_full_usage_id(u[0], u[1]) for u in all_mutes]\n\n    # usually you'll find and open the target device, here we'll browse for the\n    # current connected devices\n    all_devices = hid.find_all_hid_devices()\n\n    success = 0\n    if not all_devices:\n        print(\"Can't any HID device!\")\n    else:\n        # search for our target usage\n        # target pageId, usageId\n        for device in all_devices:\n            try:\n                device.open()\n                # target 'to set' value could be in feature or output reports\n                for report in device.find_output_reports() + device.find_feature_reports():\n                    for target_usage in all_target_usages:\n                        if target_usage in report:\n                            # set our value and send\n                            report[target_usage] = value\n                            report.send()\n                            success += 1\n            finally:\n                device.close()\n    # fit to sys.exit() proper result values\n    print(\"{0} Mute usage(s) set\\n\".format(success))\n    if success:\n        return 0\n    return -1", "response": "Browse for mute usages and set value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate WINAPI BOOL result raise exception if failed", "response": "def winapi_result( result ):\r\n    \"\"\"Validate WINAPI BOOL result, raise exception if failed\"\"\"\r\n    if not result:\r\n        raise WinApiException(\"%d (%x): %s\" % (ctypes.GetLastError(),\r\n                ctypes.GetLastError(), ctypes.FormatError()))\r\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction generator that returns a device_interface_data enumerator for the given device interface info and GUID parameters", "response": "def enum_device_interfaces(h_info, guid):\r\n    \"\"\"Function generator that returns a device_interface_data enumerator\r\n    for the given device interface info and GUID parameters\r\n    \"\"\"\r\n    dev_interface_data = SP_DEVICE_INTERFACE_DATA()\r\n    dev_interface_data.cb_size = sizeof(dev_interface_data)\r\n\r\n    device_index = 0\r\n    while SetupDiEnumDeviceInterfaces(h_info,\r\n            None,\r\n            byref(guid),\r\n            device_index,\r\n            byref(dev_interface_data) ):\r\n        yield dev_interface_data\r\n        device_index += 1\r\n    del dev_interface_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns Hardware device path", "response": "def get_device_path(h_info, interface_data, ptr_info_data = None):\r\n    \"\"\"\"Returns Hardware device path\r\n    Parameters:\r\n        h_info,         interface set info handler\r\n        interface_data, device interface enumeration data\r\n        ptr_info_data,  pointer to SP_DEVINFO_DATA() instance to receive details\r\n    \"\"\"\r\n    required_size = c_ulong(0)\r\n\r\n    dev_inter_detail_data         = SP_DEVICE_INTERFACE_DETAIL_DATA()\r\n    dev_inter_detail_data.cb_size = sizeof(SP_DEVICE_INTERFACE_DETAIL_DATA)\r\n\r\n    # get actual storage requirement\r\n    SetupDiGetDeviceInterfaceDetail(h_info, byref(interface_data),\r\n            None, 0, byref(required_size),\r\n            None)\r\n    ctypes.resize(dev_inter_detail_data, required_size.value)\r\n\r\n    # read value\r\n    SetupDiGetDeviceInterfaceDetail(h_info, byref(interface_data),\r\n            byref(dev_inter_detail_data), required_size, None,\r\n            ptr_info_data)\r\n\r\n    # extract string only\r\n    return dev_inter_detail_data.get_string()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a handle to the opaque device that contains the device interfaces supported by all the USB collections currently installed in the system.", "response": "def open(self):\r\n        \"\"\"\r\n        Calls SetupDiGetClassDevs to obtain a handle to an opaque device\r\n        information set that describes the device interfaces supported by all\r\n        the USB collections currently installed in the system. The\r\n        application should specify DIGCF.PRESENT and DIGCF.INTERFACEDEVICE\r\n        in the Flags parameter passed to SetupDiGetClassDevs.\r\n        \"\"\"\r\n        self.h_info = SetupDiGetClassDevs(byref(self.guid), None, None,\r\n                (DIGCF.PRESENT | DIGCF.DEVICEINTERFACE) )\r\n\r\n        return self.h_info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching the list of old tz names and returns a mapping", "response": "def update_old_names():\n    \"\"\"Fetches the list of old tz names and returns a mapping\"\"\"\n\n    url = urlparse(ZONEINFO_URL)\n    log.info('Connecting to %s' % url.netloc)\n    ftp = ftplib.FTP(url.netloc)\n    ftp.login()\n    gzfile = BytesIO()\n\n    log.info('Fetching zoneinfo database')\n    ftp.retrbinary('RETR ' + url.path, gzfile.write)\n    gzfile.seek(0)\n\n    log.info('Extracting backwards data')\n    archive = tarfile.open(mode=\"r:gz\", fileobj=gzfile)\n    backward = {}\n    for line in archive.extractfile('backward').readlines():\n        if line[0] == '#':\n            continue\n        if len(line.strip()) == 0:\n            continue\n        parts = line.split()\n        if parts[0] != b'Link':\n            continue\n\n        backward[parts[2].decode('ascii')] = parts[1].decode('ascii')\n\n    return backward"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize a string to be used in the log file.", "response": "def _normalize_str(self, string):\n        \"\"\"\n        Remove special characters and strip spaces\n        \"\"\"\n        if string:\n            if not isinstance(string, str):\n                string = str(string, 'utf-8', 'replace')\n\n            return unicodedata.normalize('NFKD', string).encode(\n                'ASCII', 'ignore').decode('ASCII')\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef permission_required(perm, queryset=None,\n                        login_url=None, raise_exception=False):\n    \"\"\"\n    Permission check decorator for classbased generic view\n\n    This decorator works as class decorator\n    DO NOT use ``method_decorator`` or whatever while this decorator will use\n    ``self`` argument for method of classbased generic view.\n\n    Parameters\n    ----------\n    perm : string\n        A permission string\n    queryset : queryset or model\n        A queryset or model for finding object.\n        With classbased generic view, ``None`` for using view default queryset.\n        When the view does not define ``get_queryset``, ``queryset``,\n        ``get_object``, or ``object`` then ``obj=None`` is used to check\n        permission.\n        With functional generic view, ``None`` for using passed queryset.\n        When non queryset was passed then ``obj=None`` is used to check\n        permission.\n\n    Examples\n    --------\n    >>> @permission_required('auth.change_user')\n    >>> class UpdateAuthUserView(UpdateView):\n    ...     pass\n    \"\"\"\n    def wrapper(cls):\n        def view_wrapper(view_func):\n            @wraps(view_func, assigned=available_attrs(view_func))\n            def inner(self, request, *args, **kwargs):\n                # get object\n                obj = get_object_from_classbased_instance(\n                        self, queryset, request, *args, **kwargs\n                    )\n\n                if not request.user.has_perm(perm, obj=obj):\n                    if raise_exception:\n                        raise PermissionDenied\n                    else:\n                        return redirect_to_login(request, login_url)\n                return view_func(self, request, *args, **kwargs)\n            return inner\n        cls.dispatch = view_wrapper(cls.dispatch)\n        return cls\n    return wrapper", "response": "Decorator for class based generic view that checks if user has permission to view object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_object_from_classbased_instance(\n        instance, queryset, request, *args, **kwargs):\n    \"\"\"\n    Get object from an instance of classbased generic view\n\n    Parameters\n    ----------\n    instance : instance\n        An instance of classbased generic view\n    queryset : instance\n        A queryset instance\n    request : instance\n        A instance of HttpRequest\n\n    Returns\n    -------\n    instance\n        An instance of model object or None\n    \"\"\"\n    from django.views.generic.edit import BaseCreateView\n    # initialize request, args, kwargs of classbased_instance\n    # most of methods of classbased view assumed these attributes\n    # but these attributes is initialized in ``dispatch`` method.\n    instance.request = request\n    instance.args = args\n    instance.kwargs = kwargs\n\n    # get queryset from class if ``queryset_or_model`` is not specified\n    if hasattr(instance, 'get_queryset') and not queryset:\n        queryset = instance.get_queryset()\n    elif hasattr(instance, 'queryset') and not queryset:\n        queryset = instance.queryset\n    elif hasattr(instance, 'model') and not queryset:\n        queryset = instance.model._default_manager.all()\n\n    # get object\n    if hasattr(instance, 'get_object'):\n        try:\n            obj = instance.get_object(queryset)\n        except AttributeError as e:\n            # CreateView has ``get_object`` method but CreateView\n            # should not have any object before thus simply set\n            # None\n            if isinstance(instance, BaseCreateView):\n                obj = None\n            else:\n                raise e\n    elif hasattr(instance, 'object'):\n        obj = instance.object\n    else:\n        obj = None\n    return obj", "response": "Get object from an instance of classbased generic view."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if a user has permission of a specified object.", "response": "def has_perm(self, user_obj, perm, obj=None):\n        \"\"\"\n        Check if user have permission (of object)\n\n        If the user_obj is not authenticated, it return ``False``.\n\n        If no object is specified, it return ``True`` when the corresponding\n        permission was specified to ``True`` (changed from v0.7.0).\n        This behavior is based on the django system.\n        https://code.djangoproject.com/wiki/RowLevelPermissions\n\n        If an object is specified, it will return ``True`` if the user is\n        in group specified in ``group_names`` of this instance.\n        This permission logic is used mainly for group based role permission\n        system.\n        You can change this behavior to set ``any_permission``,\n        ``add_permission``, ``change_permission``, or ``delete_permission``\n        attributes of this instance.\n\n        Parameters\n        ----------\n        user_obj : django user model instance\n            A django user model instance which be checked\n        perm : string\n            `app_label.codename` formatted permission string\n        obj : None or django model instance\n            None or django model instance for object permission\n\n        Returns\n        -------\n        boolean\n            Whether the specified user have specified permission (of specified\n            object).\n        \"\"\"\n        if not is_authenticated(user_obj):\n            return False\n        # construct the permission full name\n        add_permission = self.get_full_permission_string('add')\n        change_permission = self.get_full_permission_string('change')\n        delete_permission = self.get_full_permission_string('delete')\n        if obj is None:\n            if user_obj.groups.filter(name__in=self.group_names):\n                if self.add_permission and perm == add_permission:\n                    return True\n                if self.change_permission and perm == change_permission:\n                    return True\n                if self.delete_permission and perm == delete_permission:\n                    return True\n                return self.any_permission\n            return False\n        elif user_obj.is_active:\n            if user_obj.groups.filter(name__in=self.group_names):\n                if self.any_permission:\n                    # have any kind of permissions to the obj\n                    return True\n                if (self.add_permission and\n                        perm == add_permission):\n                    return True\n                if (self.change_permission and\n                        perm == change_permission):\n                    return True\n                if (self.delete_permission and\n                        perm == delete_permission):\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_object_from_list_detail_view(request, *args, **kwargs):\n    queryset = kwargs['queryset']\n    object_id = kwargs.get('object_id', None)\n    slug = kwargs.get('slug', None)\n    slug_field = kwargs.get('slug_field', 'slug')\n    if object_id:\n        obj = get_object_or_404(queryset, pk=object_id)\n    elif slug and slug_field:\n        obj = get_object_or_404(queryset, **{slug_field: slug})\n    else:\n        raise AttributeError(\n                \"Generic detail view must be called with either an \"\n                \"object_id or a slug/slug_field.\"\n            )\n    return obj", "response": "Get object from generic list_detail. detail view"}
