{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the console output from the WebSocket and forward it to the output stream", "response": "def _read_console_output(self, ws, out):\n        \"\"\"\n        Read Websocket and forward it to the telnet\n\n        :param ws: Websocket connection\n        :param out: Output stream\n        \"\"\"\n\n        while True:\n            msg = yield from ws.receive()\n            if msg.tp == aiohttp.WSMsgType.text:\n                out.feed_data(msg.data.encode())\n            elif msg.tp == aiohttp.WSMsgType.BINARY:\n                out.feed_data(msg.data)\n            elif msg.tp == aiohttp.WSMsgType.ERROR:\n                log.critical(\"Docker WebSocket Error: {}\".format(msg.data))\n            else:\n                out.feed_eof()\n                ws.close()\n                break\n        yield from self.stop()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_running(self):\n        state = yield from self._get_container_state()\n        if state == \"running\":\n            return True\n        if self.status == \"started\":  # The container crashed we need to clean\n            yield from self.stop()\n        return False", "response": "Checks if the container is running."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restart(self):\n        yield from self.manager.query(\"POST\", \"containers/{}/restart\".format(self._cid))\n        log.info(\"Docker container '{name}' [{image}] restarted\".format(\n            name=self._name, image=self._image))", "response": "Restart this Docker container."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans the list of running console servers and remove the old ones.", "response": "def _clean_servers(self):\n        \"\"\"\n        Clean the list of running console servers\n        \"\"\"\n        if len(self._telnet_servers) > 0:\n            for telnet_server in self._telnet_servers:\n                telnet_server.close()\n                yield from telnet_server.wait_closed()\n            self._telnet_servers = []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop this Docker container.", "response": "def stop(self):\n        \"\"\"Stops this Docker container.\"\"\"\n\n        try:\n            yield from self._clean_servers()\n            yield from self._stop_ubridge()\n\n            try:\n                state = yield from self._get_container_state()\n            except DockerHttp404Error:\n                self.status = \"stopped\"\n                return\n\n            if state == \"paused\":\n                yield from self.unpause()\n\n            yield from self._fix_permissions()\n            state = yield from self._get_container_state()\n            if state != \"stopped\" or state != \"exited\":\n                # t=5 number of seconds to wait before killing the container\n                try:\n                    yield from self.manager.query(\"POST\", \"containers/{}/stop\".format(self._cid), params={\"t\": 5})\n                    log.info(\"Docker container '{name}' [{image}] stopped\".format(\n                        name=self._name, image=self._image))\n                except DockerHttp304Error:\n                    # Container is already stopped\n                    pass\n        # Ignore runtime error because when closing the server\n        except RuntimeError as e:\n            log.debug(\"Docker runtime error when closing: {}\".format(str(e)))\n            return\n        self.status = \"stopped\""}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npauses this Docker container.", "response": "def pause(self):\n        \"\"\"Pauses this Docker container.\"\"\"\n        yield from self.manager.query(\"POST\", \"containers/{}/pause\".format(self._cid))\n        self.status = \"suspended\"\n        log.info(\"Docker container '{name}' [{image}] paused\".format(name=self._name, image=self._image))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_ubridge_connection(self, nio, adapter_number):\n\n        try:\n            adapter = self._ethernet_adapters[adapter_number]\n        except IndexError:\n            raise DockerError(\"Adapter {adapter_number} doesn't exist on Docker container '{name}'\".format(name=self.name,\n                                                                                                           adapter_number=adapter_number))\n\n        for index in range(4096):\n            if \"tap-gns3-e{}\".format(index) not in psutil.net_if_addrs():\n                adapter.host_ifc = \"tap-gns3-e{}\".format(str(index))\n                break\n        if adapter.host_ifc is None:\n            raise DockerError(\"Adapter {adapter_number} couldn't allocate interface on Docker container '{name}'. Too many Docker interfaces already exists\".format(name=self.name,\n                                                                                                                                                                    adapter_number=adapter_number))\n        bridge_name = 'bridge{}'.format(adapter_number)\n        yield from self._ubridge_send('bridge create {}'.format(bridge_name))\n        self._bridges.add(bridge_name)\n        yield from self._ubridge_send('bridge add_nio_tap bridge{adapter_number} {hostif}'.format(adapter_number=adapter_number,\n                                                                                                  hostif=adapter.host_ifc))\n        log.debug(\"Move container %s adapter %s to namespace %s\", self.name, adapter.host_ifc, self._namespace)\n        try:\n            yield from self._ubridge_send('docker move_to_ns {ifc} {ns} eth{adapter}'.format(ifc=adapter.host_ifc,\n                                                                                             ns=self._namespace,\n                                                                                             adapter=adapter_number))\n        except UbridgeError as e:\n            raise UbridgeNamespaceError(e)\n\n        if nio:\n            yield from self._connect_nio(adapter_number, nio)", "response": "Add a connection to a uBridge adapter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a NIO binding to an adapter.", "response": "def adapter_add_nio_binding(self, adapter_number, nio):\n        \"\"\"Adds an adapter NIO binding.\n\n\n        :param adapter_number: adapter number\n        :param nio: NIO instance to add to the slot/port\n        \"\"\"\n        try:\n            adapter = self._ethernet_adapters[adapter_number]\n        except IndexError:\n            raise DockerError(\"Adapter {adapter_number} doesn't exist on Docker container '{name}'\".format(name=self.name,\n                                                                                                           adapter_number=adapter_number))\n\n        if self.status == \"started\" and self.ubridge:\n            yield from self._connect_nio(adapter_number, nio)\n\n        adapter.add_nio(0, nio)\n        log.info(\"Docker container '{name}' [{id}]: {nio} added to adapter {adapter_number}\".format(name=self.name,\n                                                                                                    id=self._id,\n                                                                                                    nio=nio,\n                                                                                                    adapter_number=adapter_number))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate a port NIO binding.", "response": "def adapter_update_nio_binding(self, adapter_number, nio):\n        \"\"\"\n        Update a port NIO binding.\n\n        :param adapter_number: adapter number\n        :param nio: NIO instance to add to the adapter\n        \"\"\"\n\n        if self.ubridge:\n            bridge_name = 'bridge{}'.format(adapter_number)\n            if bridge_name in self._bridges:\n                yield from self._ubridge_apply_filters(bridge_name, nio.filters)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef adapter_remove_nio_binding(self, adapter_number):\n        try:\n            adapter = self._ethernet_adapters[adapter_number]\n        except IndexError:\n            raise DockerError(\"Adapter {adapter_number} doesn't exist on Docker VM '{name}'\".format(name=self.name,\n                                                                                                    adapter_number=adapter_number))\n\n        if self.ubridge:\n            nio = adapter.get_nio(0)\n            bridge_name = 'bridge{}'.format(adapter_number)\n            yield from self._ubridge_send(\"bridge stop {}\".format(bridge_name))\n            yield from self._ubridge_send('bridge remove_nio_udp bridge{adapter} {lport} {rhost} {rport}'.format(adapter=adapter_number,\n                                                                                                                 lport=nio.lport,\n                                                                                                                 rhost=nio.rhost,\n                                                                                                                 rport=nio.rport))\n\n        adapter.remove_nio(0)\n\n        log.info(\"Docker VM '{name}' [{id}]: {nio} removed from adapter {adapter_number}\".format(name=self.name,\n                                                                                                 id=self.id,\n                                                                                                 nio=adapter.host_ifc,\n                                                                                                 adapter_number=adapter_number))", "response": "Removes an adapter NIO binding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npull image from docker repository", "response": "def pull_image(self, image):\n        \"\"\"\n        Pull image from docker repository\n        \"\"\"\n        def callback(msg):\n            self.project.emit(\"log.info\", {\"message\": msg})\n        yield from self.manager.pull_image(image, progress_callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _start_ubridge_capture(self, adapter_number, output_file):\n\n        adapter = \"bridge{}\".format(adapter_number)\n        if not self.ubridge:\n            raise DockerError(\"Cannot start the packet capture: uBridge is not running\")\n        yield from self._ubridge_send('bridge start_capture {name} \"{output_file}\"'.format(name=adapter, output_file=output_file))", "response": "Start a packet capture in uBridge."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstopping a packet capture in uBridge.", "response": "def _stop_ubridge_capture(self, adapter_number):\n        \"\"\"\n        Stop a packet capture in uBridge.\n\n        :param adapter_number: adapter number\n        \"\"\"\n\n        adapter = \"bridge{}\".format(adapter_number)\n        if not self.ubridge:\n            raise DockerError(\"Cannot stop the packet capture: uBridge is not running\")\n        yield from self._ubridge_send(\"bridge stop_capture {name}\".format(name=adapter))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_capture(self, adapter_number, output_file):\n\n        try:\n            adapter = self._ethernet_adapters[adapter_number]\n        except KeyError:\n            raise DockerError(\"Adapter {adapter_number} doesn't exist on Docker VM '{name}'\".format(name=self.name,\n                                                                                                    adapter_number=adapter_number))\n\n        nio = adapter.get_nio(0)\n\n        if not nio:\n            raise DockerError(\"Adapter {} is not connected\".format(adapter_number))\n\n        if nio.capturing:\n            raise DockerError(\"Packet capture is already activated on adapter {adapter_number}\".format(adapter_number=adapter_number))\n\n        nio.startPacketCapture(output_file)\n\n        if self.status == \"started\" and self.ubridge:\n            yield from self._start_ubridge_capture(adapter_number, output_file)\n\n        log.info(\"Docker VM '{name}' [{id}]: starting packet capture on adapter {adapter_number}\".format(name=self.name,\n                                                                                                         id=self.id,\n                                                                                                         adapter_number=adapter_number))", "response": "Starts a packet capture on the specified adapter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstopping a packet capture on the specified adapter.", "response": "def stop_capture(self, adapter_number):\n        \"\"\"\n        Stops a packet capture.\n\n        :param adapter_number: adapter number\n        \"\"\"\n\n        try:\n            adapter = self._ethernet_adapters[adapter_number]\n        except KeyError:\n            raise DockerError(\"Adapter {adapter_number} doesn't exist on Docker VM '{name}'\".format(name=self.name,\n                                                                                                    adapter_number=adapter_number))\n\n        nio = adapter.get_nio(0)\n\n        if not nio:\n            raise DockerError(\"Adapter {} is not connected\".format(adapter_number))\n\n        nio.stopPacketCapture()\n\n        if self.status == \"started\" and self.ubridge:\n            yield from self._stop_ubridge_capture(adapter_number)\n\n        log.info(\"Docker VM '{name}' [{id}]: stopping packet capture on adapter {adapter_number}\".format(name=self.name,\n                                                                                                         id=self.id,\n                                                                                                         adapter_number=adapter_number))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the log from the container", "response": "def _get_log(self):\n        \"\"\"\n        Return the log from the container\n\n        :returns: string\n        \"\"\"\n\n        result = yield from self.manager.query(\"GET\", \"containers/{}/logs\".format(self._cid), params={\"stderr\": 1, \"stdout\": 1})\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate free application_id from given nodes", "response": "def get_next_application_id(nodes):\n    \"\"\"\n    Calculates free application_id from given nodes\n\n    :param nodes:\n    :raises IOUError when exceeds number\n    :return: integer first free id\n    \"\"\"\n    used = set([n.application_id for n in nodes])\n    pool = set(range(1, 512))\n    try:\n        return (pool - used).pop()\n    except KeyError:\n        raise IOUError(\"Cannot create a new IOU VM (limit of 512 VMs on one host reached)\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrestart with a clean config", "response": "def clear(self):\n        \"\"\"Restart with a clean config\"\"\"\n        self._config = configparser.RawConfigParser()\n        # Override config from command line even if we modify the config file and live reload it.\n        self._override_config = {}\n\n        self.read_config()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreloading configuration of the current locale.", "response": "def reload(self):\n        \"\"\"\n        Reload configuration\n        \"\"\"\n\n        self.read_config()\n        for section in self._override_config:\n            self.set_section_config(section, self._override_config[section])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the configuration files and store the modification times of the files in the self. _watched_files dict.", "response": "def read_config(self):\n        \"\"\"\n        Read the configuration files.\n        \"\"\"\n\n        try:\n            parsed_files = self._config.read(self._files, encoding=\"utf-8\")\n        except configparser.Error as e:\n            log.error(\"Can't parse configuration file: %s\", str(e))\n            return\n        if not parsed_files:\n            log.warning(\"No configuration file could be found or read\")\n        else:\n            for file in parsed_files:\n                log.info(\"Load configuration file {}\".format(file))\n                self._watched_files[file] = os.stat(file).st_mtime"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_section_config(self, section):\n\n        if section not in self._config:\n            return self._config[\"DEFAULT\"]\n        return self._config[section]", "response": "Get a specific configuration section."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting a specific configuration section.", "response": "def set_section_config(self, section, content):\n        \"\"\"\n        Set a specific configuration section. It's not\n        dumped on the disk.\n\n        :param section: Section name\n        :param content: A dictionary with section content\n        \"\"\"\n\n        if not self._config.has_section(section):\n            self._config.add_section(section)\n        for key in content:\n            if isinstance(content[key], bool):\n                content[key] = str(content[key]).lower()\n            self._config.set(section, key, content[key])\n        self._override_config[section] = content"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set(self, section, key, value):\n\n        conf = self.get_section_config(section)\n        if isinstance(value, bool):\n            conf[key] = str(value)\n        else:\n            conf[key] = value\n        self.set_section_config(section, conf)", "response": "Set a config value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun a function in a different thread and wait for the result of the function.", "response": "def wait_run_in_executor(func, *args, **kwargs):\n    \"\"\"\n    Run blocking code in a different thread and wait\n    for the result.\n\n    :param func: Run this function in a different thread\n    :param args: Parameters of the function\n    :param kwargs: Keyword parameters of the function\n    :returns: Return the result of the function\n    \"\"\"\n\n    loop = asyncio.get_event_loop()\n    future = loop.run_in_executor(None, functools.partial(func, *args, **kwargs))\n    yield from asyncio.wait([future])\n    return future.result()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning a command and capture output", "response": "def subprocess_check_output(*args, cwd=None, env=None, stderr=False):\n    \"\"\"\n    Run a command and capture output\n\n    :param *args: List of command arguments\n    :param cwd: Current working directory\n    :param env: Command environment\n    :param stderr: Read on stderr\n    :returns: Command output\n    \"\"\"\n\n    if stderr:\n        proc = yield from asyncio.create_subprocess_exec(*args, stderr=asyncio.subprocess.PIPE, cwd=cwd, env=env)\n        output = yield from proc.stderr.read()\n    else:\n        proc = yield from asyncio.create_subprocess_exec(*args, stdout=asyncio.subprocess.PIPE, cwd=cwd, env=env)\n        output = yield from proc.stdout.read()\n    if output is None:\n        return \"\"\n    # If we received garbage we ignore invalid characters\n    # it should happens only when user try to use another binary\n    # and the code of VPCS, dynamips... Will detect it's not the correct binary\n    return output.decode(\"utf-8\", errors=\"ignore\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait_for_process_termination(process, timeout=10):\n\n    if sys.version_info >= (3, 5):\n        try:\n            yield from asyncio.wait_for(process.wait(), timeout=timeout)\n        except ProcessLookupError:\n            return\n    else:\n        while timeout > 0:\n            if process.returncode is not None:\n                return\n            yield from asyncio.sleep(0.1)\n            timeout -= 0.1\n        raise asyncio.TimeoutError()", "response": "Wait for a process to terminate and raise asyncio. TimeoutError if it doesn t terminate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrenaming this bridge. :param new_name: New name for this bridge", "response": "def set_name(self, new_name):\n        \"\"\"\n        Renames this bridge.\n\n        :param new_name: New name for this bridge\n        \"\"\"\n\n        yield from self._hypervisor.send('nio_bridge rename \"{name}\" \"{new_name}\"'.format(name=self._name,\n                                                                                          new_name=new_name))\n\n        self._name = new_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a NIO instance as new port on this bridge.", "response": "def add_nio(self, nio):\n        \"\"\"\n        Adds a NIO as new port on this bridge.\n\n        :param nio: NIO instance to add\n        \"\"\"\n\n        yield from self._hypervisor.send('nio_bridge add_nio \"{name}\" {nio}'.format(name=self._name, nio=nio))\n        self._nios.append(nio)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_nio(self, nio):\n        if self._hypervisor:\n            yield from self._hypervisor.send('nio_bridge remove_nio \"{name}\" {nio}'.format(name=self._name, nio=nio))\n        self._nios.remove(nio)", "response": "Removes the specified NIO as member of this bridge."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_project(controller, project_id, stream, location=None, name=None, keep_compute_id=False):\n\n    if location and \".gns3\" in location:\n        raise aiohttp.web.HTTPConflict(text=\"The destination path should not contain .gns3\")\n\n    try:\n        with zipfile.ZipFile(stream) as myzip:\n\n            try:\n                topology = json.loads(myzip.read(\"project.gns3\").decode())\n\n                # We import the project on top of an existing project (snapshots)\n                if topology[\"project_id\"] == project_id:\n                    project_name = topology[\"name\"]\n                else:\n                    # If the project name is already used we generate a new one\n                    if name:\n                        project_name = controller.get_free_project_name(name)\n                    else:\n                        project_name = controller.get_free_project_name(topology[\"name\"])\n            except KeyError:\n                raise aiohttp.web.HTTPConflict(text=\"Can't import topology the .gns3 is corrupted or missing\")\n\n            if location:\n                path = location\n            else:\n                projects_path = controller.projects_directory()\n                path = os.path.join(projects_path, project_id)\n            try:\n                os.makedirs(path, exist_ok=True)\n            except UnicodeEncodeError as e:\n                raise aiohttp.web.HTTPConflict(text=\"The project name contain non supported or invalid characters\")\n            myzip.extractall(path)\n\n            topology = load_topology(os.path.join(path, \"project.gns3\"))\n            topology[\"name\"] = project_name\n            # To avoid unexpected behavior (project start without manual operations just after import)\n            topology[\"auto_start\"] = False\n            topology[\"auto_open\"] = False\n            topology[\"auto_close\"] = True\n\n            # Generate a new node id\n            node_old_to_new = {}\n            for node in topology[\"topology\"][\"nodes\"]:\n                if \"node_id\" in node:\n                    node_old_to_new[node[\"node_id\"]] = str(uuid.uuid4())\n                    _move_node_file(path, node[\"node_id\"], node_old_to_new[node[\"node_id\"]])\n                    node[\"node_id\"] = node_old_to_new[node[\"node_id\"]]\n                else:\n                    node[\"node_id\"] = str(uuid.uuid4())\n\n            # Update link to use new id\n            for link in topology[\"topology\"][\"links\"]:\n                link[\"link_id\"] = str(uuid.uuid4())\n                for node in link[\"nodes\"]:\n                    node[\"node_id\"] = node_old_to_new[node[\"node_id\"]]\n\n            # Generate new drawings id\n            for drawing in topology[\"topology\"][\"drawings\"]:\n                drawing[\"drawing_id\"] = str(uuid.uuid4())\n\n            # Modify the compute id of the node depending of compute capacity\n            if not keep_compute_id:\n                # For some VM type we move them to the GNS3 VM if possible\n                # unless it's a linux host without GNS3 VM\n                if not sys.platform.startswith(\"linux\") or controller.has_compute(\"vm\"):\n                    for node in topology[\"topology\"][\"nodes\"]:\n                        if node[\"node_type\"] in (\"docker\", \"qemu\", \"iou\", \"nat\"):\n                            node[\"compute_id\"] = \"vm\"\n                else:\n                    # Round-robin through available compute resources.\n                    compute_nodes = itertools.cycle(controller.computes)\n                    for node in topology[\"topology\"][\"nodes\"]:\n                        node[\"compute_id\"] = next(compute_nodes)\n\n            compute_created = set()\n            for node in topology[\"topology\"][\"nodes\"]:\n\n                if node[\"compute_id\"] != \"local\":\n                    # Project created on the remote GNS3 VM?\n                    if node[\"compute_id\"] not in compute_created:\n                        compute = controller.get_compute(node[\"compute_id\"])\n                        yield from compute.post(\"/projects\", data={\n                            \"name\": project_name,\n                            \"project_id\": project_id,\n                        })\n                        compute_created.add(node[\"compute_id\"])\n\n                    yield from _move_files_to_compute(compute, project_id, path, os.path.join(\"project-files\", node[\"node_type\"], node[\"node_id\"]))\n\n            # And we dump the updated.gns3\n            dot_gns3_path = os.path.join(path, project_name + \".gns3\")\n            # We change the project_id to avoid erasing the project\n            topology[\"project_id\"] = project_id\n            with open(dot_gns3_path, \"w+\") as f:\n                json.dump(topology, f, indent=4)\n            os.remove(os.path.join(path, \"project.gns3\"))\n\n            if os.path.exists(os.path.join(path, \"images\")):\n                _import_images(controller, path)\n\n        project = yield from controller.load_project(dot_gns3_path, load=False)\n        return project\n    except zipfile.BadZipFile:\n        raise aiohttp.web.HTTPConflict(text=\"Can't import topology the file is corrupted or not a GNS3 project (invalid zip)\")", "response": "Imports a project from a zip file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmoves the files from a node when changing his id", "response": "def _move_node_file(path, old_id, new_id):\n    \"\"\"\n    Move the files from a node when changing his id\n\n    :param path: Path of the project\n    :param old_id: ID before change\n    :param new_id: New node UUID\n    \"\"\"\n    root = os.path.join(path, \"project-files\")\n    if os.path.exists(root):\n        for dirname in os.listdir(root):\n            module_dir = os.path.join(root, dirname)\n            if os.path.isdir(module_dir):\n                node_dir = os.path.join(module_dir, old_id)\n                if os.path.exists(node_dir):\n                    shutil.move(node_dir, os.path.join(module_dir, new_id))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _move_files_to_compute(compute, project_id, directory, files_path):\n    location = os.path.join(directory, files_path)\n    if os.path.exists(location):\n        for (dirpath, dirnames, filenames) in os.walk(location):\n            for filename in filenames:\n                path = os.path.join(dirpath, filename)\n                dst = os.path.relpath(path, directory)\n                yield from _upload_file(compute, project_id, path, dst)\n        shutil.rmtree(os.path.join(directory, files_path))", "response": "Move the files to a remote compute"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _upload_file(compute, project_id, file_path, path):\n    path = \"/projects/{}/files/{}\".format(project_id, path.replace(\"\\\\\", \"/\"))\n    with open(file_path, \"rb\") as f:\n        yield from compute.http_query(\"POST\", path, f, timeout=None)", "response": "Upload a file to a remote project"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _import_images(controller, path):\n    image_dir = controller.images_path()\n\n    root = os.path.join(path, \"images\")\n    for (dirpath, dirnames, filenames) in os.walk(root):\n        for filename in filenames:\n            path = os.path.join(dirpath, filename)\n            dst = os.path.join(image_dir, os.path.relpath(path, root))\n            os.makedirs(os.path.dirname(dst), exist_ok=True)\n            shutil.move(path, dst)", "response": "Copy images to the images directory or delete them if they already exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_add_loopback():\n\n    class Add(argparse.Action):\n\n        def __call__(self, parser, args, values, option_string=None):\n            try:\n                ipaddress.IPv4Interface(\"{}/{}\".format(values[1], values[2]))\n            except ipaddress.AddressValueError as e:\n                raise argparse.ArgumentTypeError(\"Invalid IP address: {}\".format(e))\n            except ipaddress.NetmaskValueError as e:\n                raise argparse.ArgumentTypeError(\"Invalid subnet mask: {}\".format(e))\n            setattr(args, self.dest, values)\n    return Add", "response": "Validate params when adding a loopback adapter"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_size(data, default_width=0, default_height=0):\n\n    height = default_height\n    width = default_width\n    filetype = None\n\n    # Original version:\n    # https://github.com/shibukawa/imagesize_py\n    #\n    # The MIT License (MIT)\n    #\n    # Copyright \u00a9 2016 Yoshiki Shibukawa\n    #\n    # Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n    #\n    # The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n    #\n    # THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n    size = len(data)\n    # handle GIFs\n    if size >= 10 and data[:6] in (b'GIF87a', b'GIF89a'):\n        # Check to see if content_type is correct\n        try:\n            width, height = struct.unpack(\"<hh\", data[6:10])\n            filetype = \"gif\"\n        except struct.error:\n            raise ValueError(\"Invalid GIF file\")\n    # see png edition spec bytes are below chunk length then and finally the\n    elif size >= 24 and data.startswith(b'\\211PNG\\r\\n\\032\\n') and data[12:16] == b'IHDR':\n        try:\n            width, height = struct.unpack(\">LL\", data[16:24])\n            filetype = \"png\"\n        except struct.error:\n            raise ValueError(\"Invalid PNG file\")\n    # Maybe this is for an older PNG version.\n    elif size >= 16 and data.startswith(b'\\211PNG\\r\\n\\032\\n'):\n        # Check to see if we have the right content type\n        try:\n            width, height = struct.unpack(\">LL\", data[8:16])\n            filetype = \"png\"\n        except struct.error:\n            raise ValueError(\"Invalid PNG file\")\n    # handle JPEGs\n    elif size >= 2 and data.startswith(b'\\377\\330'):\n        try:\n            # Not very efficient to copy data to a buffer\n            fhandle = io.BytesIO(data)\n            size = 2\n            ftype = 0\n            while not 0xc0 <= ftype <= 0xcf:\n                fhandle.seek(size, 1)\n                byte = fhandle.read(1)\n                while ord(byte) == 0xff:\n                    byte = fhandle.read(1)\n                ftype = ord(byte)\n                size = struct.unpack('>H', fhandle.read(2))[0] - 2\n            # We are at a SOFn block\n            fhandle.seek(1, 1)  # Skip `precision' byte.\n            height, width = struct.unpack('>HH', fhandle.read(4))\n            filetype = \"jpg\"\n        except struct.error:\n            raise ValueError(\"Invalid JPEG file\")\n    # End of https://github.com/shibukawa/imagesize_py\n\n    # handle SVG\n    elif size >= 10 and data.startswith(b'<?xml'):\n        filetype = \"svg\"\n        fhandle = io.BytesIO(data)\n        tree = ElementTree()\n        try:\n            tree.parse(fhandle)\n        except ParseError:\n            raise ValueError(\"Invalid SVG file\")\n\n        root = tree.getroot()\n\n        try:\n            width = _svg_convert_size(root.attrib.get(\"width\", \"0\"))\n            height = _svg_convert_size(root.attrib.get(\"height\", \"0\"))\n        except IndexError:\n            raise ValueError(\"Invalid SVG file\")\n\n    return width, height, filetype", "response": "Get image size from a buffer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _svg_convert_size(size):\n\n    # https://www.w3.org/TR/SVG/coords.html#Units\n    conversion_table = {\n        \"pt\": 1.25,\n        \"pc\": 15,\n        \"mm\": 3.543307,\n        \"cm\": 35.43307,\n        \"in\": 90,\n        \"px\": 1\n    }\n    if len(size) > 3:\n        if size[-2:] in conversion_table:\n            return round(float(size[:-2]) * conversion_table[size[-2:]])\n\n    return round(float(size))", "response": "Convert svg size to px version"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nscan directories for available image for a type", "response": "def list_images(type):\n    \"\"\"\n    Scan directories for available image for a type\n\n    :param type: emulator type (dynamips, qemu, iou)\n    \"\"\"\n    files = set()\n    images = []\n\n    server_config = Config.instance().get_section_config(\"Server\")\n    general_images_directory = os.path.expanduser(server_config.get(\"images_path\", \"~/GNS3/images\"))\n\n    # Subfolder of the general_images_directory specific to this VM type\n    default_directory = default_images_directory(type)\n\n    for directory in images_directories(type):\n\n        # We limit recursion to path outside the default images directory\n        # the reason is in the default directory manage file organization and\n        # it should be flatten to keep things simple\n        recurse = True\n        if os.path.commonprefix([directory, general_images_directory]) == general_images_directory:\n            recurse = False\n\n        directory = os.path.normpath(directory)\n        for root, _, filenames in _os_walk(directory, recurse=recurse):\n            for filename in filenames:\n                path = os.path.join(root, filename)\n                if filename not in files:\n                    if filename.endswith(\".md5sum\") or filename.startswith(\".\"):\n                        continue\n                    elif ((filename.endswith(\".image\") or filename.endswith(\".bin\")) and type == \"dynamips\") \\\n                            or ((filename.endswith(\".bin\") or filename.startswith(\"i86bi\")) and type == \"iou\") \\\n                            or (not filename.endswith(\".bin\") and not filename.endswith(\".image\") and type == \"qemu\"):\n                        files.add(filename)\n\n                        # It the image is located in the standard directory the path is relative\n                        if os.path.commonprefix([root, default_directory]) != default_directory:\n                            path = os.path.join(root, filename)\n                        else:\n                            path = os.path.relpath(os.path.join(root, filename), default_directory)\n\n                        try:\n                            if type in [\"dynamips\", \"iou\"]:\n                                with open(os.path.join(root, filename), \"rb\") as f:\n                                    # read the first 7 bytes of the file.\n                                    elf_header_start = f.read(7)\n                                # valid IOS images must start with the ELF magic number, be 32-bit, big endian and have an ELF version of 1\n                                if not elf_header_start == b'\\x7fELF\\x01\\x02\\x01' and not elf_header_start == b'\\x7fELF\\x01\\x01\\x01':\n                                    continue\n\n                            images.append({\n                                \"filename\": filename,\n                                \"path\": force_unix_path(path),\n                                \"md5sum\": md5sum(os.path.join(root, filename)),\n                                \"filesize\": os.stat(os.path.join(root, filename)).st_size})\n                        except OSError as e:\n                            log.warn(\"Can't add image {}: {}\".format(path, str(e)))\n    return images"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _os_walk(directory, recurse=True, **kwargs):\n    if recurse:\n        for root, dirs, files in os.walk(directory, **kwargs):\n            yield root, dirs, files\n    else:\n        files = []\n        for filename in os.listdir(directory):\n            if os.path.isfile(os.path.join(directory, filename)):\n                files.append(filename)\n        yield directory, [], files", "response": "A generator function that returns a list of all files in a directory and directories."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef default_images_directory(type):\n    server_config = Config.instance().get_section_config(\"Server\")\n    img_dir = os.path.expanduser(server_config.get(\"images_path\", \"~/GNS3/images\"))\n    if type == \"qemu\":\n        return os.path.join(img_dir, \"QEMU\")\n    elif type == \"iou\":\n        return os.path.join(img_dir, \"IOU\")\n    elif type == \"dynamips\":\n        return os.path.join(img_dir, \"IOS\")\n    else:\n        raise NotImplementedError(\"%s node type is not supported\", type)", "response": "Returns the default directory for a node type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef images_directories(type):\n    server_config = Config.instance().get_section_config(\"Server\")\n\n    paths = []\n    img_dir = os.path.expanduser(server_config.get(\"images_path\", \"~/GNS3/images\"))\n    type_img_directory = default_images_directory(type)\n    try:\n        os.makedirs(type_img_directory, exist_ok=True)\n        paths.append(type_img_directory)\n    except (OSError, PermissionError):\n        pass\n    for directory in server_config.get(\"additional_images_path\", \"\").split(\";\"):\n        paths.append(directory)\n    # Compatibility with old topologies we look in parent directory\n    paths.append(img_dir)\n    # Return only the existings paths\n    return [force_unix_path(p) for p in paths if os.path.exists(p)]", "response": "Return all directories where we will look for images by priority"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the md5sum of an image and cache it on disk", "response": "def md5sum(path):\n    \"\"\"\n    Return the md5sum of an image and cache it on disk\n\n    :param path: Path to the image\n    :returns: Digest of the image\n    \"\"\"\n\n    if path is None or len(path) == 0 or not os.path.exists(path):\n        return None\n\n    try:\n        with open(path + '.md5sum') as f:\n            md5 = f.read()\n            if len(md5) == 32:\n                return md5\n    # Unicode error is when user rename an image to .md5sum ....\n    except (OSError, UnicodeDecodeError):\n        pass\n\n    try:\n        m = hashlib.md5()\n        with open(path, 'rb') as f:\n            while True:\n                buf = f.read(128)\n                if not buf:\n                    break\n                m.update(buf)\n        digest = m.hexdigest()\n    except OSError as e:\n        log.error(\"Can't create digest of %s: %s\", path, str(e))\n        return None\n\n    try:\n        with open('{}.md5sum'.format(path), 'w+') as f:\n            f.write(digest)\n    except OSError as e:\n        log.error(\"Can't write digest of %s: %s\", path, str(e))\n\n    return digest"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_checksum(path):\n\n    path = '{}.md5sum'.format(path)\n    if os.path.exists(path):\n        os.remove(path)", "response": "Remove the checksum of an image from cache if exists"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a reserved TCP port number to this project.", "response": "def record_tcp_port(self, port):\n        \"\"\"\n        Associate a reserved TCP port number with this project.\n\n        :param port: TCP port number\n        \"\"\"\n\n        if port not in self._used_tcp_ports:\n            self._used_tcp_ports.add(port)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef record_udp_port(self, port):\n\n        if port not in self._used_udp_ports:\n            self._used_udp_ports.add(port)", "response": "Adds a reserved UDP port number to this project."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove an associated TCP port number from this project.", "response": "def remove_tcp_port(self, port):\n        \"\"\"\n        Removes an associated TCP port number from this project.\n\n        :param port: TCP port number\n        \"\"\"\n\n        if port in self._used_tcp_ports:\n            self._used_tcp_ports.remove(port)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_udp_port(self, port):\n\n        if port in self._used_udp_ports:\n            self._used_udp_ports.remove(port)", "response": "Removes an associated UDP port number from this project."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a working directory for the module with the given name.", "response": "def module_working_directory(self, module_name):\n        \"\"\"\n        Returns a working directory for the module\n        The directory is created if the directory doesn't exist.\n\n        :param module_name: name for the module\n        :returns: working directory\n        \"\"\"\n\n        workdir = self.module_working_path(module_name)\n        if not self._deleted:\n            try:\n                os.makedirs(workdir, exist_ok=True)\n            except OSError as e:\n                raise aiohttp.web.HTTPInternalServerError(text=\"Could not create module working directory: {}\".format(e))\n        return workdir"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a working directory for a specific node.", "response": "def node_working_directory(self, node):\n        \"\"\"\n        Returns a working directory for a specific node.\n        If the directory doesn't exist, the directory is created.\n\n        :param node: Node instance\n\n        :returns: Node working directory\n        \"\"\"\n\n        workdir = self.node_working_path(node)\n        if not self._deleted:\n            try:\n                os.makedirs(workdir, exist_ok=True)\n            except OSError as e:\n                raise aiohttp.web.HTTPInternalServerError(text=\"Could not create the node working directory: {}\".format(e))\n        return workdir"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a node working path for node. It doesn t create structure if not present on system.", "response": "def node_working_path(self, node):\n        \"\"\"\n        Returns a node working path for node. It doesn't create structure if not present on system.\n        :param node: Node instance\n        :return: Node working path\n        \"\"\"\n        return os.path.join(self._path, \"project-files\", node.manager.module_name.lower(), node.id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef capture_working_directory(self):\n\n        workdir = os.path.join(self._path, \"tmp\", \"captures\")\n        if not self._deleted:\n            try:\n                os.makedirs(workdir, exist_ok=True)\n            except OSError as e:\n                raise aiohttp.web.HTTPInternalServerError(text=\"Could not create the capture working directory: {}\".format(e))\n        return workdir", "response": "Returns a working directory where to temporary store packet capture files."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a Node instance.", "response": "def get_node(self, node_id):\n        \"\"\"\n        Returns a Node instance.\n\n        :param node_id: Node identifier\n\n        :returns: Node instance\n        \"\"\"\n\n        try:\n            UUID(node_id, version=4)\n        except ValueError:\n            raise aiohttp.web.HTTPBadRequest(text=\"Node ID {} is not a valid UUID\".format(node_id))\n\n        for node in self._nodes:\n            if node.id == node_id:\n                return node\n\n        raise aiohttp.web.HTTPNotFound(text=\"Node ID {} doesn't exist\".format(node_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_node(self, node):\n\n        if node in self._nodes:\n            yield from node.delete()\n            self._nodes.remove(node)", "response": "Removes a node from the project."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose the project but keep information on disk", "response": "def close(self):\n        \"\"\"\n        Closes the project, but keep information on disk\n        \"\"\"\n\n        project_nodes_id = set([n.id for n in self.nodes])\n\n        for module in self.compute():\n            module_nodes_id = set([n.id for n in module.instance().nodes])\n            # We close the project only for the modules using it\n            if len(module_nodes_id & project_nodes_id):\n                yield from module.instance().project_closing(self)\n\n        yield from self._close_and_clean(False)\n\n        for module in self.compute():\n            module_nodes_id = set([n.id for n in module.instance().nodes])\n            # We close the project only for the modules using it\n            if len(module_nodes_id & project_nodes_id):\n                yield from module.instance().project_closed(self)\n\n        try:\n            if os.path.exists(self.tmp_working_directory()):\n                shutil.rmtree(self.tmp_working_directory())\n        except OSError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing the project and cleans up the disk.", "response": "def _close_and_clean(self, cleanup):\n        \"\"\"\n        Closes the project, and cleanup the disk if cleanup is True\n\n        :param cleanup: Whether to delete the project directory\n        \"\"\"\n\n        tasks = []\n        for node in self._nodes:\n            tasks.append(asyncio.async(node.manager.close_node(node.id)))\n\n        if tasks:\n            done, _ = yield from asyncio.wait(tasks)\n            for future in done:\n                try:\n                    future.result()\n                except (Exception, GeneratorExit) as e:\n                    log.error(\"Could not close node {}\".format(e), exc_info=1)\n\n        if cleanup and os.path.exists(self.path):\n            self._deleted = True\n            try:\n                yield from wait_run_in_executor(shutil.rmtree, self.path)\n                log.info(\"Project {id} with path '{path}' deleted\".format(path=self._path, id=self._id))\n            except OSError as e:\n                raise aiohttp.web.HTTPInternalServerError(text=\"Could not delete the project directory: {}\".format(e))\n        else:\n            log.info(\"Project {id} with path '{path}' closed\".format(path=self._path, id=self._id))\n\n        if self._used_tcp_ports:\n            log.warning(\"Project {} has TCP ports still in use: {}\".format(self.id, self._used_tcp_ports))\n        if self._used_udp_ports:\n            log.warning(\"Project {} has UDP ports still in use: {}\".format(self.id, self._used_udp_ports))\n\n        # clean the remaining ports that have not been cleaned by their respective node.\n        port_manager = PortManager.instance()\n        for port in self._used_tcp_ports.copy():\n            port_manager.release_tcp_port(port, self)\n        for port in self._used_udp_ports.copy():\n            port_manager.release_udp_port(port, self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self):\n\n        for module in self.compute():\n            yield from module.instance().project_closing(self)\n        yield from self._close_and_clean(True)\n        for module in self.compute():\n            yield from module.instance().project_closed(self)", "response": "Removes project from disk"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend an event to all the client listening for notifications", "response": "def emit(self, action, event):\n        \"\"\"\n        Send an event to all the client listening for notifications\n\n        :param action: Action name\n        :param event: Event to send\n        \"\"\"\n        NotificationManager.instance().emit(action, event, project_id=self.id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_files(self):\n\n        files = []\n        for dirpath, dirnames, filenames in os.walk(self.path):\n            for filename in filenames:\n                if not filename.endswith(\".ghost\"):\n                    path = os.path.relpath(dirpath, self.path)\n                    path = os.path.join(path, filename)\n                    path = os.path.normpath(path)\n                    file_info = {\"path\": path}\n\n                    try:\n                        file_info[\"md5sum\"] = yield from wait_run_in_executor(self._hash_file, os.path.join(dirpath, filename))\n                    except OSError:\n                        continue\n                    files.append(file_info)\n\n        return files", "response": "Returns an array of files in the project without temporary files."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes and md5 hash for file", "response": "def _hash_file(self, path):\n        \"\"\"\n        Compute and md5 hash for file\n\n        :returns: hexadecimal md5\n        \"\"\"\n\n        m = hashlib.md5()\n        with open(path, \"rb\") as f:\n            while True:\n                buf = f.read(128)\n                if not buf:\n                    break\n                m.update(buf)\n        return m.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mac(self):\n        res = 'Port       Mac                VLAN\\n'\n        result = (yield from self._node._hypervisor.send('ethsw show_mac_addr_table {}'.format(self._node.name)))\n        for line in result:\n            mac, vlan, nio = line.replace('  ', ' ').split(' ')\n            mac = mac.replace('.', '')\n            mac = \"{}:{}:{}:{}:{}:{}\".format(\n                mac[0:2],\n                mac[2:4],\n                mac[4:6],\n                mac[6:8],\n                mac[8:10],\n                mac[10:12])\n            for port_number, switch_nio in self._node.nios.items():\n                if switch_nio.name == nio:\n                    res += 'Ethernet' + str(port_number) + '  ' + mac + '  ' + vlan + '\\n'\n                    break\n        return res", "response": "Show MAC address table"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_name(self, new_name):\n\n        yield from self._hypervisor.send('ethsw rename \"{name}\" \"{new_name}\"'.format(name=self._name, new_name=new_name))\n        log.info('Ethernet switch \"{name}\" [{id}]: renamed to \"{new_name}\"'.format(name=self._name,\n                                                                                   id=self._id,\n                                                                                   new_name=new_name))\n        self._name = new_name", "response": "Renames this Ethernet switch."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete this Ethernet switch.", "response": "def close(self):\n        \"\"\"\n        Deletes this Ethernet switch.\n        \"\"\"\n        yield from self._telnet.close()\n        self._telnet_server.close()\n        \n        for nio in self._nios.values():\n            if nio:\n                yield from nio.close()\n        self.manager.port_manager.release_tcp_port(self._console, self._project)\n        if self._hypervisor:\n            try:\n                yield from self._hypervisor.send('ethsw delete \"{}\"'.format(self._name))\n                log.info('Ethernet switch \"{name}\" [{id}] has been deleted'.format(name=self._name, id=self._id))\n            except DynamipsError:\n                log.debug(\"Could not properly delete Ethernet switch {}\".format(self._name))\n        if self._hypervisor and self in self._hypervisor.devices:\n            self._hypervisor.devices.remove(self)\n        if self._hypervisor and not self._hypervisor.devices:\n            yield from self.hypervisor.stop()\n            self._hypervisor = None\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a NIO instance to the switch.", "response": "def add_nio(self, nio, port_number):\n        \"\"\"\n        Adds a NIO as new port on Ethernet switch.\n\n        :param nio: NIO instance to add\n        :param port_number: port to allocate for the NIO\n        \"\"\"\n\n        if port_number in self._nios:\n            raise DynamipsError(\"Port {} isn't free\".format(port_number))\n\n        yield from self._hypervisor.send('ethsw add_nio \"{name}\" {nio}'.format(name=self._name, nio=nio))\n\n        log.info('Ethernet switch \"{name}\" [{id}]: NIO {nio} bound to port {port}'.format(name=self._name,\n                                                                                          id=self._id,\n                                                                                          nio=nio,\n                                                                                          port=port_number))\n        self._nios[port_number] = nio\n        for port_settings in self._ports:\n            if port_settings[\"port_number\"] == port_number:\n                yield from self.set_port_settings(port_number, port_settings)\n                break"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_nio(self, port_number):\n\n        if port_number not in self._nios:\n            raise DynamipsError(\"Port {} is not allocated\".format(port_number))\n\n        nio = self._nios[port_number]\n        if isinstance(nio, NIOUDP):\n            self.manager.port_manager.release_udp_port(nio.lport, self._project)\n        if self._hypervisor:\n            yield from self._hypervisor.send('ethsw remove_nio \"{name}\" {nio}'.format(name=self._name, nio=nio))\n\n        log.info('Ethernet switch \"{name}\" [{id}]: NIO {nio} removed from port {port}'.format(name=self._name,\n                                                                                              id=self._id,\n                                                                                              nio=nio,\n                                                                                              port=port_number))\n\n        del self._nios[port_number]\n        if port_number in self._mappings:\n            del self._mappings[port_number]\n\n        return nio", "response": "Removes the specified NIO as member of this Ethernet switch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_port_settings(self, port_number, settings):\n\n        if settings[\"type\"] == \"access\":\n            yield from self.set_access_port(port_number, settings[\"vlan\"])\n        elif settings[\"type\"] == \"dot1q\":\n            yield from self.set_dot1q_port(port_number, settings[\"vlan\"])\n        elif settings[\"type\"] == \"qinq\":\n            yield from self.set_qinq_port(port_number, settings[\"vlan\"], settings.get(\"ethertype\"))", "response": "Applies port settings to a specific port."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the specified port as an ACCESS port.", "response": "def set_access_port(self, port_number, vlan_id):\n        \"\"\"\n        Sets the specified port as an ACCESS port.\n\n        :param port_number: allocated port number\n        :param vlan_id: VLAN number membership\n        \"\"\"\n\n        if port_number not in self._nios:\n            raise DynamipsError(\"Port {} is not allocated\".format(port_number))\n\n        nio = self._nios[port_number]\n        yield from self._hypervisor.send('ethsw set_access_port \"{name}\" {nio} {vlan_id}'.format(name=self._name,\n                                                                                                 nio=nio,\n                                                                                                 vlan_id=vlan_id))\n\n        log.info('Ethernet switch \"{name}\" [{id}]: port {port} set as an access port in VLAN {vlan_id}'.format(name=self._name,\n                                                                                                               id=self._id,\n                                                                                                               port=port_number,\n                                                                                                               vlan_id=vlan_id))\n        self._mappings[port_number] = (\"access\", vlan_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the specified port as a QinQ trunk.", "response": "def set_qinq_port(self, port_number, outer_vlan, ethertype):\n        \"\"\"\n        Sets the specified port as a trunk (QinQ) port.\n\n        :param port_number: allocated port number\n        :param outer_vlan: outer VLAN (transport VLAN) for this QinQ port\n        \"\"\"\n\n        if port_number not in self._nios:\n            raise DynamipsError(\"Port {} is not allocated\".format(port_number))\n\n        nio = self._nios[port_number]\n        if ethertype != \"0x8100\" and parse_version(self.hypervisor.version) < parse_version('0.2.16'):\n            raise DynamipsError(\"Dynamips version required is >= 0.2.16 to change the default QinQ Ethernet type, detected version is {}\".format(self.hypervisor.version))\n\n        yield from self._hypervisor.send('ethsw set_qinq_port \"{name}\" {nio} {outer_vlan} {ethertype}'.format(name=self._name,\n                                                                                                              nio=nio,\n                                                                                                              outer_vlan=outer_vlan,\n                                                                                                              ethertype=ethertype if ethertype != \"0x8100\" else \"\"))\n\n        log.info('Ethernet switch \"{name}\" [{id}]: port {port} set as a QinQ ({ethertype}) port with outer VLAN {vlan_id}'.format(name=self._name,\n                                                                                                                                  id=self._id,\n                                                                                                                                  port=port_number,\n                                                                                                                                  vlan_id=outer_vlan,\n                                                                                                                                  ethertype=ethertype))\n        self._mappings[port_number] = (\"qinq\", outer_vlan, ethertype)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mac_addr_table(self):\n\n        mac_addr_table = yield from self._hypervisor.send('ethsw show_mac_addr_table \"{}\"'.format(self._name))\n        return mac_addr_table", "response": "Returns the MAC address table for this Ethernet switch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shutdown_server(self):\n\n        if not self._closing:\n            self._closing = True\n        else:\n            log.warning(\"Close is already in progress\")\n            return\n\n        if self._server:\n            self._server.close()\n            yield from self._server.wait_closed()\n        if self._app:\n            yield from self._app.shutdown()\n        if self._handler:\n            try:\n                # aiohttp < 2.3\n                yield from self._handler.finish_connections(2)  # Parameter is timeout\n            except AttributeError:\n                # aiohttp >= 2.3\n                yield from self._handler.shutdown(2)  # Parameter is timeout\n        if self._app:\n            yield from self._app.cleanup()\n\n        yield from Controller.instance().stop()\n\n        for module in MODULES:\n            log.debug(\"Unloading module {}\".format(module.__name__))\n            m = module.instance()\n            yield from m.unload()\n\n        if PortManager.instance().tcp_ports:\n            log.warning(\"TCP ports are still used {}\".format(PortManager.instance().tcp_ports))\n\n        if PortManager.instance().udp_ports:\n            log.warning(\"UDP ports are still used {}\".format(PortManager.instance().udp_ports))\n\n        for task in asyncio.Task.all_tasks():\n            task.cancel()\n            try:\n                yield from asyncio.wait_for(task, 1)\n            except BaseException:\n                pass\n\n        self._loop.stop()", "response": "Cleanly shutdown the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose the asyncio loop.", "response": "def _exit_handling(self):\n        \"\"\"\n        Makes sure the asyncio loop is closed.\n        \"\"\"\n\n        def close_asyncio_loop():\n            loop = None\n            try:\n                loop = asyncio.get_event_loop()\n            except AttributeError:\n                pass\n            if loop is not None:\n                loop.close()\n\n        atexit.register(close_asyncio_loop)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls when the HTTP server starts", "response": "def _on_startup(self, *args):\n        \"\"\"\n        Called when the HTTP server start\n        \"\"\"\n        yield from Controller.instance().start()\n        # Because with a large image collection\n        # without md5sum already computed we start the\n        # computing with server start\n        asyncio.async(Qemu.instance().list_images())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding inaccessible disk files in VirtualBox", "response": "def _find_inaccessible_hdd_files(self):\n        \"\"\"\n        Finds inaccessible disk files (to clean up the VirtualBox media manager)\n        \"\"\"\n\n        hdds = []\n        try:\n            properties = yield from self.execute(\"list\", [\"hdds\"])\n        # If VirtualBox is not available we have no inaccessible hdd\n        except VirtualBoxError:\n            return hdds\n\n        flag_inaccessible = False\n        for prop in properties:\n            try:\n                name, value = prop.split(':', 1)\n            except ValueError:\n                continue\n            if name.strip() == \"State\" and value.strip() == \"inaccessible\":\n                flag_inaccessible = True\n            if flag_inaccessible and name.strip() == \"Location\":\n                hdds.append(value.strip())\n                flag_inaccessible = False\n        return reversed(hdds)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when a project is closed.", "response": "def project_closed(self, project):\n        \"\"\"\n        Called when a project is closed.\n\n        :param project: Project instance\n        \"\"\"\n\n        yield from super().project_closed(project)\n        hdd_files_to_close = yield from self._find_inaccessible_hdd_files()\n        for hdd_file in hdd_files_to_close:\n            log.info(\"Closing VirtualBox VM disk file {}\".format(os.path.basename(hdd_file)))\n            try:\n                yield from self.execute(\"closemedium\", [\"disk\", hdd_file])\n            except VirtualBoxError as e:\n                log.warning(\"Could not close VirtualBox VM disk file {}: {}\".format(os.path.basename(hdd_file), e))\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_vms(self, allow_clone=False):\n\n        vbox_vms = []\n        result = yield from self.execute(\"list\", [\"vms\"])\n        for line in result:\n            if len(line) == 0 or line[0] != '\"' or line[-1:] != \"}\":\n                continue  # Broken output (perhaps a carriage return in VM name)\n            vmname, _ = line.rsplit(' ', 1)\n            vmname = vmname.strip('\"')\n            if vmname == \"<inaccessible>\":\n                continue  # ignore inaccessible VMs\n            extra_data = yield from self.execute(\"getextradata\", [vmname, \"GNS3/Clone\"])\n            if allow_clone or len(extra_data) == 0 or not extra_data[0].strip() == \"Value: yes\":\n                # get the amount of RAM\n                info_results = yield from self.execute(\"showvminfo\", [vmname, \"--machinereadable\"])\n                ram = 0\n                for info in info_results:\n                    try:\n                        name, value = info.split('=', 1)\n                        if name.strip() == \"memory\":\n                            ram = int(value.strip())\n                            break\n                    except ValueError:\n                        continue\n                vbox_vms.append({\"vmname\": vmname, \"ram\": ram})\n        return vbox_vms", "response": "Gets a list of VirtualBox VM names."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_npe(self, npe):\n\n        if (yield from self.is_running()):\n            raise DynamipsError(\"Cannot change NPE on running router\")\n\n        yield from self._hypervisor.send('c7200 set_npe \"{name}\" {npe}'.format(name=self._name, npe=npe))\n\n        log.info('Router \"{name}\" [{id}]: NPE updated from {old_npe} to {new_npe}'.format(name=self._name,\n                                                                                          id=self._id,\n                                                                                          old_npe=self._npe,\n                                                                                          new_npe=npe))\n        self._npe = npe", "response": "Sets the NPE model for this router."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the midplane model.", "response": "def set_midplane(self, midplane):\n        \"\"\"\n        Sets the midplane model.\n\n        :returns: midplane model string (e.g. \"vxr\" or \"std\")\n        \"\"\"\n\n        yield from self._hypervisor.send('c7200 set_midplane \"{name}\" {midplane}'.format(name=self._name, midplane=midplane))\n\n        log.info('Router \"{name}\" [{id}]: midplane updated from {old_midplane} to {new_midplane}'.format(name=self._name,\n                                                                                                         id=self._id,\n                                                                                                         old_midplane=self._midplane,\n                                                                                                         new_midplane=midplane))\n        self._midplane = midplane"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_sensors(self, sensors):\n\n        sensor_id = 0\n        for sensor in sensors:\n            yield from self._hypervisor.send('c7200 set_temp_sensor \"{name}\" {sensor_id} {temp}'.format(name=self._name,\n                                                                                                        sensor_id=sensor_id,\n                                                                                                        temp=sensor))\n\n            log.info('Router \"{name}\" [{id}]: sensor {sensor_id} temperature updated from {old_temp}C to {new_temp}C'.format(name=self._name,\n                                                                                                                             id=self._id,\n                                                                                                                             sensor_id=sensor_id,\n                                                                                                                             old_temp=self._sensors[sensor_id],\n                                                                                                                             new_temp=sensors[sensor_id]))\n\n            sensor_id += 1\n        self._sensors = sensors", "response": "Sets the sensors with temperature in degree Celcius."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the power supplies of the router.", "response": "def set_power_supplies(self, power_supplies):\n        \"\"\"\n        Sets the 2 power supplies with 0 = off, 1 = on.\n\n        :param power_supplies: list of 2 power supplies.\n        Example: [1, 0] = first power supply is on, second is off.\n        \"\"\"\n\n        power_supply_id = 0\n        for power_supply in power_supplies:\n            yield from self._hypervisor.send('c7200 set_power_supply \"{name}\" {power_supply_id} {powered_on}'.format(name=self._name,\n                                                                                                                     power_supply_id=power_supply_id,\n                                                                                                                     powered_on=power_supply))\n\n            log.info('Router \"{name}\" [{id}]: power supply {power_supply_id} state updated to {powered_on}'.format(name=self._name,\n                                                                                                                   id=self._id,\n                                                                                                                   power_supply_id=power_supply_id,\n                                                                                                                   powered_on=power_supply))\n            power_supply_id += 1\n\n        self._power_supplies = power_supplies"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start(self):\n\n        # trick: we must send sensors and power supplies info after starting the router\n        # otherwise they are not taken into account (Dynamips bug?)\n        yield from Router.start(self)\n        if self._sensors != [22, 22, 22, 22]:\n            yield from self.set_sensors(self._sensors)\n        if self._power_supplies != [1, 1]:\n            yield from self.set_power_supplies(self._power_supplies)", "response": "Starts this router.\n        At least the IOS image must be set before starting it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting the project as zip.", "response": "def export_project(project, temporary_dir, include_images=False, keep_compute_id=False,\n                   allow_all_nodes=False, ignore_prefixes=None):\n    \"\"\"\n    Export the project as zip. It's a ZipStream object.\n    The file will be read chunk by chunk when you iterate on\n    the zip.\n\n    It will ignore some files like snapshots and\n\n    :param temporary_dir: A temporary dir where to store intermediate data\n    :param keep_compute_id: If false replace all compute id by local it's the standard behavior for .gns3project to make them portable\n    :param allow_all_nodes: Allow all nodes type to be include in the zip even if not portable default False\n    :returns: ZipStream object\n    \"\"\"\n\n    # To avoid issue with data not saved we disallow the export of a running topologie\n    if project.is_running():\n        raise aiohttp.web.HTTPConflict(text=\"Running topology could not be exported\")\n\n    # Make sure we save the project\n    project.dump()\n\n    z = zipstream.ZipFile(allowZip64=True)\n\n    if not os.path.exists(project._path):\n        raise aiohttp.web.HTTPNotFound(text=\"The project doesn't exist at location {}\".format(project._path))\n\n    # First we process the .gns3 in order to be sure we don't have an error\n    for file in os.listdir(project._path):\n        if file.endswith(\".gns3\"):\n            images = yield from _export_project_file(project, os.path.join(project._path, file),\n                                          z, include_images, keep_compute_id, allow_all_nodes, temporary_dir)\n\n    for root, dirs, files in os.walk(project._path, topdown=True):\n        files = [f for f in files if not _filter_files(os.path.join(root, f))]\n        for file in files:\n            path = os.path.join(root, file)\n            # Try open the file\n            try:\n                open(path).close()\n            except OSError as e:\n                msg = \"Could not export file {}: {}\".format(path, e)\n                log.warn(msg)\n                project.controller.notification.emit(\"log.warning\", {\"message\": msg})\n                continue\n            if file.endswith(\".gns3\"):\n                pass\n            else:\n                z.write(path, os.path.relpath(path, project._path), compress_type=zipfile.ZIP_DEFLATED)\n\n    downloaded_files = set()\n\n    for compute in project.computes:\n        if compute.id != \"local\":\n            compute_files = yield from compute.list_files(project)\n            for compute_file in compute_files:\n                if not _filter_files(compute_file[\"path\"]):\n                    (fd, temp_path) = tempfile.mkstemp(dir=temporary_dir)\n                    f = open(fd, \"wb\", closefd=True)\n                    response = yield from compute.download_file(project, compute_file[\"path\"])\n                    while True:\n                        data = yield from response.content.read(512)\n                        if not data:\n                            break\n                        f.write(data)\n                    response.close()\n                    f.close()\n                    z.write(temp_path, arcname=compute_file[\"path\"], compress_type=zipfile.ZIP_DEFLATED)\n                    downloaded_files.add(compute_file['path'])\n\n    return z"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter files in the archive to see if they should be included in the final archive.", "response": "def _filter_files(path):\n    \"\"\"\n    :returns: True if file should not be included in the final archive\n    \"\"\"\n    s = os.path.normpath(path).split(os.path.sep)\n\n    if path.endswith(\"snapshots\"):\n        return True\n\n    # filter directory of snapshots\n    if \"{sep}snapshots{sep}\".format(sep=os.path.sep) in path:\n        return True\n\n    try:\n        i = s.index(\"project-files\")\n        if s[i + 1] in (\"tmp\", \"captures\", \"snapshots\"):\n            return True\n    except (ValueError, IndexError):\n        pass\n\n    file_name = os.path.basename(path)\n    # Ignore log files and OS noises\n    if file_name.endswith('_log.txt') or file_name.endswith('.log') or file_name == '.DS_Store':\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _export_project_file(project, path, z, include_images, keep_compute_id, allow_all_nodes, temporary_dir):\n\n    # Image file that we need to include in the exported archive\n    images = []\n\n    with open(path) as f:\n        topology = json.load(f)\n\n    if \"topology\" in topology:\n        if \"nodes\" in topology[\"topology\"]:\n            for node in topology[\"topology\"][\"nodes\"]:\n                compute_id = node.get('compute_id', 'local')\n\n                if node[\"node_type\"] == \"virtualbox\" and node.get(\"properties\", {}).get(\"linked_clone\"):\n                    raise aiohttp.web.HTTPConflict(text=\"Topology with a linked {} clone could not be exported. Use qemu instead.\".format(node[\"node_type\"]))\n                if not allow_all_nodes and node[\"node_type\"] in [\"virtualbox\", \"vmware\", \"cloud\"]:\n                    raise aiohttp.web.HTTPConflict(text=\"Topology with a {} could not be exported\".format(node[\"node_type\"]))\n\n                if not keep_compute_id:\n                    node[\"compute_id\"] = \"local\"  # To make project portable all node by default run on local\n\n                if \"properties\" in node and node[\"node_type\"] != \"docker\":\n                    for prop, value in node[\"properties\"].items():\n\n                        if node[\"node_type\"] == \"iou\":\n                            if not prop == \"path\":\n                                continue\n                        elif not prop.endswith(\"image\"):\n                            continue\n                        if value is None or value.strip() == '':\n                            continue\n\n                        if not keep_compute_id:  # If we keep the original compute we can keep the image path\n                            node[\"properties\"][prop] = os.path.basename(value)\n\n                        if include_images is True:\n                            images.append({\n                                'compute_id': compute_id,\n                                'image': value,\n                                'image_type': node['node_type']\n                            })\n\n        if not keep_compute_id:\n            topology[\"topology\"][\"computes\"] = []  # Strip compute information because could contain secret info like password\n\n    local_images = set([i['image'] for i in images if i['compute_id'] == 'local'])\n\n    for image in local_images:\n        _export_local_images(project, image, z)\n\n    remote_images = set([\n        (i['compute_id'], i['image_type'], i['image'])\n        for i in images if i['compute_id'] != 'local'])\n\n    for compute_id, image_type, image in remote_images:\n        yield from _export_remote_images(project, compute_id, image_type, image, z, temporary_dir)\n\n    z.writestr(\"project.gns3\", json.dumps(topology).encode())\n\n    return images", "response": "Export a project file to the client"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport local images to the zipfile", "response": "def _export_local_images(project, image, z):\n    \"\"\"\n    Take a project file (.gns3) and export images to the zip\n\n    :param image: Image path\n    :param z: Zipfile instance for the export\n    \"\"\"\n    from ..compute import MODULES\n\n    for module in MODULES:\n        try:\n            img_directory = module.instance().get_images_directory()\n        except NotImplementedError:\n            # Some modules don't have images\n            continue\n\n        directory = os.path.split(img_directory)[-1:][0]\n        if os.path.exists(image):\n            path = image\n        else:\n            path = os.path.join(img_directory, image)\n\n        if os.path.exists(path):\n            arcname = os.path.join(\"images\", directory, os.path.basename(image))\n            z.write(path, arcname)\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _export_remote_images(project, compute_id, image_type, image, project_zipfile, temporary_dir):\n\n    log.info(\"Obtaining image `{}` from `{}`\".format(image, compute_id))\n\n    try:\n        compute = [compute for compute in project.computes if compute.id == compute_id][0]\n    except IndexError:\n        raise aiohttp.web.HTTPConflict(\n            text=\"Cannot export image from `{}` compute. Compute doesn't exist.\".format(compute_id))\n\n    (fd, temp_path) = tempfile.mkstemp(dir=temporary_dir)\n    f = open(fd, \"wb\", closefd=True)\n    response = yield from compute.download_image(image_type, image)\n\n    if response.status != 200:\n        raise aiohttp.web.HTTPConflict(\n            text=\"Cannot export image from `{}` compute. Compute sent `{}` status.\".format(\n                compute_id, response.status))\n\n    while True:\n        data = yield from response.content.read(512)\n        if not data:\n            break\n        f.write(data)\n    response.close()\n    f.close()\n    arcname = os.path.join(\"images\", image_type, image)\n    log.info(\"Saved {}\".format(arcname))\n    project_zipfile.write(temp_path, arcname=arcname, compress_type=zipfile.ZIP_DEFLATED)", "response": "Export specific image from remote compute"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a queue of notifications that have been processed", "response": "def queue(self, project):\n        \"\"\"\n        Get a queue of notifications\n\n        Use it with Python with\n        \"\"\"\n        queue = NotificationQueue()\n        self._listeners.setdefault(project.id, set())\n        self._listeners[project.id].add(queue)\n        yield queue\n        self._listeners[project.id].remove(queue)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef project_has_listeners(self, project):\n        return project.id in self._listeners and len(self._listeners[project.id]) > 0", "response": "Returns True if the client has listen this project"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dispatch(self, action, event, compute_id):\n        if action == \"node.updated\":\n            try:\n                # Update controller node data and send the event node.updated\n                project = self._controller.get_project(event[\"project_id\"])\n                node = project.get_node(event[\"node_id\"])\n                yield from node.parse_node_response(event)\n\n                self.emit(\"node.updated\", node.__json__())\n            except (aiohttp.web.HTTPNotFound, aiohttp.web.HTTPForbidden):  # Project closing\n                return\n        elif action == \"ping\":\n            event[\"compute_id\"] = compute_id\n            self.emit(action, event)\n        else:\n            self.emit(action, event)", "response": "Dispatches an event from the compute node to the client."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef emit(self, action, event):\n\n        # If use in tests for documentation we save a sample\n        if os.environ.get(\"PYTEST_BUILD_DOCUMENTATION\") == \"1\":\n            os.makedirs(\"docs/api/notifications\", exist_ok=True)\n            try:\n                import json\n                data = json.dumps(event, indent=4, sort_keys=True)\n                if \"MagicMock\" not in data:\n                    with open(os.path.join(\"docs/api/notifications\", action + \".json\"), 'w+') as f:\n                        f.write(data)\n            except TypeError:  # If we receive a mock as an event it will raise TypeError when using json dump\n                pass\n\n        if \"project_id\" in event:\n            self._send_event_to_project(event[\"project_id\"], action, event)\n        else:\n            self._send_event_to_all(action, event)", "response": "Send a notification to all clients scoped by projects"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _send_event_to_project(self, project_id, action, event):\n        try:\n            project_listeners = self._listeners[project_id]\n        except KeyError:\n            return\n        for listener in project_listeners:\n            listener.put_nowait((action, event, {}))", "response": "Send an event to all the client listening for notifications for the specified action and event."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends an event to all the client listening for notifications on all the projects that have been added to the cluster.", "response": "def _send_event_to_all(self, action, event):\n        \"\"\"\n        Send an event to all the client listening for notifications on all\n        projects\n\n        :param action: Action name\n        :param event: Event to send\n        \"\"\"\n        for project_listeners in self._listeners.values():\n            for listener in project_listeners:\n                listener.put_nowait((action, event, {}))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the user and password of the local server.", "response": "def _update_config(self):\n        \"\"\"\n        Call this when the server configuration file\n        change\n        \"\"\"\n        if self._local_server:\n            server_config = Config.instance().get_section_config(\"Server\")\n            self._local_server.user = server_config.get(\"user\")\n            self._local_server.password = server_config.get(\"password\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the controller configuration on disk", "response": "def save(self):\n        \"\"\"\n        Save the controller configuration on disk\n        \"\"\"\n        # We don't save during the loading otherwise we could lost stuff\n        if self._settings is None:\n            return\n        data = {\n            \"computes\": [],\n            \"settings\": self._settings,\n            \"gns3vm\": self.gns3vm.__json__(),\n            \"version\": __version__\n        }\n\n        for c in self._computes.values():\n            if c.id != \"local\" and c.id != \"vm\":\n                data[\"computes\"].append({\n                    \"host\": c.host,\n                    \"name\": c.name,\n                    \"port\": c.port,\n                    \"protocol\": c.protocol,\n                    \"user\": c.user,\n                    \"password\": c.password,\n                    \"compute_id\": c.id\n                })\n        try:\n            os.makedirs(os.path.dirname(self._config_file), exist_ok=True)\n            with open(self._config_file, 'w+') as f:\n                json.dump(data, f, indent=4)\n        except OSError as e:\n            log.error(\"Can't write the configuration {}: {}\".format(self._config_file, str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_controller_settings(self):\n        try:\n            if not os.path.exists(self._config_file):\n                yield from self._import_gns3_gui_conf()\n                self.save()\n            with open(self._config_file) as f:\n                data = json.load(f)\n        except (OSError, ValueError) as e:\n            log.critical(\"Cannot load %s: %s\", self._config_file, str(e))\n            self._settings = {}\n            return []\n\n        if \"settings\" in data and data[\"settings\"] is not None:\n            self._settings = data[\"settings\"]\n        else:\n            self._settings = {}\n        if \"gns3vm\" in data:\n            self.gns3vm.settings = data[\"gns3vm\"]\n\n        self.load_appliances()\n        return data.get(\"computes\", [])", "response": "Load the controller settings from disk"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the list of projects from disk", "response": "def load_projects(self):\n        \"\"\"\n        Preload the list of projects from disk\n        \"\"\"\n        server_config = Config.instance().get_section_config(\"Server\")\n        projects_path = os.path.expanduser(server_config.get(\"projects_path\", \"~/GNS3/projects\"))\n        os.makedirs(projects_path, exist_ok=True)\n        try:\n            for project_path in os.listdir(projects_path):\n                project_dir = os.path.join(projects_path, project_path)\n                if os.path.isdir(project_dir):\n                    for file in os.listdir(project_dir):\n                        if file.endswith(\".gns3\"):\n                            try:\n                                yield from self.load_project(os.path.join(project_dir, file), load=False)\n                            except (aiohttp.web_exceptions.HTTPConflict, NotImplementedError):\n                                pass  # Skip not compatible projects\n        except OSError as e:\n            log.error(str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading base files from the user location to allow them to customize it", "response": "def load_base_files(self):\n        \"\"\"\n        At startup we copy base file to the user location to allow\n        them to customize it\n        \"\"\"\n        dst_path = self.configs_path()\n        src_path = get_resource('configs')\n        try:\n            for file in os.listdir(src_path):\n                if not os.path.exists(os.path.join(dst_path, file)):\n                    shutil.copy(os.path.join(src_path, file), os.path.join(dst_path, file))\n        except OSError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef images_path(self):\n        server_config = Config.instance().get_section_config(\"Server\")\n        images_path = os.path.expanduser(server_config.get(\"images_path\", \"~/GNS3/projects\"))\n        os.makedirs(images_path, exist_ok=True)\n        return images_path", "response": "Get the image storage directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef appliances_path(self):\n        server_config = Config.instance().get_section_config(\"Server\")\n        appliances_path = os.path.expanduser(server_config.get(\"appliances_path\", \"~/GNS3/projects\"))\n        os.makedirs(appliances_path, exist_ok=True)\n        return appliances_path", "response": "Get the image storage directory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nimport old config from GNS3 GUI", "response": "def _import_gns3_gui_conf(self):\n        \"\"\"\n        Import old config from GNS3 GUI\n        \"\"\"\n        config_file = os.path.join(os.path.dirname(self._config_file), \"gns3_gui.conf\")\n        if os.path.exists(config_file):\n            with open(config_file) as f:\n                data = json.load(f)\n                server_settings = data.get(\"Servers\", {})\n                for remote in server_settings.get(\"remote_servers\", []):\n                    try:\n                        yield from self.add_compute(\n                            host=remote.get(\"host\", \"localhost\"),\n                            port=remote.get(\"port\", 3080),\n                            protocol=remote.get(\"protocol\", \"http\"),\n                            name=remote.get(\"url\"),\n                            user=remote.get(\"user\"),\n                            password=remote.get(\"password\")\n                        )\n                    except aiohttp.web.HTTPConflict:\n                        pass  # if the server is broken we skip it\n                if \"vm\" in server_settings:\n                    vmname = None\n                    vm_settings = server_settings[\"vm\"]\n                    if vm_settings[\"virtualization\"] == \"VMware\":\n                        engine = \"vmware\"\n                        vmname = vm_settings.get(\"vmname\", \"\")\n                    elif vm_settings[\"virtualization\"] == \"VirtualBox\":\n                        engine = \"virtualbox\"\n                        vmname = vm_settings.get(\"vmname\", \"\")\n                    else:\n                        engine = \"remote\"\n                        # In case of remote server we match the compute with url parameter\n                        for compute in self._computes.values():\n                            if compute.host == vm_settings.get(\"remote_vm_host\") and compute.port == vm_settings.get(\"remote_vm_port\"):\n                                vmname = compute.name\n\n                    if vm_settings.get(\"auto_stop\", True):\n                        when_exit = \"stop\"\n                    else:\n                        when_exit = \"keep\"\n\n                    self.gns3vm.settings = {\n                        \"engine\": engine,\n                        \"enable\": vm_settings.get(\"auto_start\", False),\n                        \"when_exit\": when_exit,\n                        \"headless\": vm_settings.get(\"headless\", False),\n                        \"vmname\": vmname\n                    }\n                self._settings = {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_compute(self, compute_id=None, name=None, force=False, connect=True, **kwargs):\n\n        if compute_id not in self._computes:\n\n            # We disallow to create from the outside the local and VM server\n            if (compute_id == 'local' or compute_id == 'vm') and not force:\n                return None\n\n            # It seem we have error with a gns3vm imported as a remote server and conflict\n            # with GNS3 VM settings. That's why we ignore server name gns3vm\n            if name == 'gns3vm':\n                return None\n\n            for compute in self._computes.values():\n                if name and compute.name == name and not force:\n                    raise aiohttp.web.HTTPConflict(text='Compute name \"{}\" already exists'.format(name))\n\n            compute = Compute(compute_id=compute_id, controller=self, name=name, **kwargs)\n            self._computes[compute.id] = compute\n            self.save()\n            if connect:\n                yield from compute.connect()\n            self.notification.emit(\"compute.created\", compute.__json__())\n            return compute\n        else:\n            if connect:\n                yield from self._computes[compute_id].connect()\n            self.notification.emit(\"compute.updated\", self._computes[compute_id].__json__())\n            return self._computes[compute_id]", "response": "Add a compute to the controller."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close_compute_projects(self, compute):\n        for project in self._projects.values():\n            if compute in project.computes:\n                yield from project.close()", "response": "Close all projects running on a compute"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_compute(self, compute_id):\n        try:\n            compute = self.get_compute(compute_id)\n        except aiohttp.web.HTTPNotFound:\n            return\n        yield from self.close_compute_projects(compute)\n        yield from compute.close()\n        del self._computes[compute_id]\n        self.save()\n        self.notification.emit(\"compute.deleted\", compute.__json__())", "response": "Delete a compute node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a compute server or raises a 404 error.", "response": "def get_compute(self, compute_id):\n        \"\"\"\n        Returns a compute server or raise a 404 error.\n        \"\"\"\n        try:\n            return self._computes[compute_id]\n        except KeyError:\n            if compute_id == \"vm\":\n                raise aiohttp.web.HTTPNotFound(text=\"You try to use a node on the GNS3 VM server but the GNS3 VM is not configured\")\n            raise aiohttp.web.HTTPNotFound(text=\"Compute ID {} doesn't exist\".format(compute_id))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_project(self, project_id=None, name=None, **kwargs):\n        if project_id not in self._projects:\n\n            for project in self._projects.values():\n                if name and project.name == name:\n                    raise aiohttp.web.HTTPConflict(text='Project name \"{}\" already exists'.format(name))\n            project = Project(project_id=project_id, controller=self, name=name, **kwargs)\n            self._projects[project.id] = project\n            return self._projects[project.id]\n        return self._projects[project_id]", "response": "Creates a project or returns an existing project"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a project object or raises a 404 error.", "response": "def get_project(self, project_id):\n        \"\"\"\n        Returns a project or raise a 404 error.\n        \"\"\"\n        try:\n            return self._projects[project_id]\n        except KeyError:\n            raise aiohttp.web.HTTPNotFound(text=\"Project ID {} doesn't exist\".format(project_id))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a project from the cache or raise a 404 error.", "response": "def get_loaded_project(self, project_id):\n        \"\"\"\n        Returns a project or raise a 404 error.\n\n        If project is not finished to load wait for it\n        \"\"\"\n        project = self.get_project(project_id)\n        yield from project.wait_loaded()\n        return project"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_project(self, path, load=True):\n        topo_data = load_topology(path)\n        topo_data.pop(\"topology\")\n        topo_data.pop(\"version\")\n        topo_data.pop(\"revision\")\n        topo_data.pop(\"type\")\n\n        if topo_data[\"project_id\"] in self._projects:\n            project = self._projects[topo_data[\"project_id\"]]\n        else:\n            project = yield from self.add_project(path=os.path.dirname(path), status=\"closed\", filename=os.path.basename(path), **topo_data)\n        if load or project.auto_open:\n            yield from project.open()\n        return project", "response": "Load a project from a. gns3 file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _project_auto_open(self):\n        for project in self._projects.values():\n            if project.auto_open:\n                yield from project.open()", "response": "Auto open the project with auto open enable enable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a free project name base on the base name", "response": "def get_free_project_name(self, base_name):\n        \"\"\"\n        Generate a free project name base on the base name\n        \"\"\"\n        names = [p.name for p in self._projects.values()]\n        if base_name not in names:\n            return base_name\n        i = 1\n\n        projects_path = self.projects_directory()\n\n        while True:\n            new_name = \"{}-{}\".format(base_name, i)\n            if new_name not in names and not os.path.exists(os.path.join(projects_path, new_name)):\n                break\n            i += 1\n            if i > 1000000:\n                raise aiohttp.web.HTTPConflict(text=\"A project name could not be allocated (node limit reached?)\")\n        return new_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef autoidlepc(self, compute_id, platform, image, ram):\n        compute = self.get_compute(compute_id)\n        for project in list(self._projects.values()):\n            if project.name == \"AUTOIDLEPC\":\n                yield from project.delete()\n                self.remove_project(project)\n        project = yield from self.add_project(name=\"AUTOIDLEPC\")\n        node = yield from project.add_node(compute, \"AUTOIDLEPC\", str(uuid.uuid4()), node_type=\"dynamips\", platform=platform, image=image, ram=ram)\n        res = yield from node.dynamips_auto_idlepc()\n        yield from project.delete()\n        self.remove_project(project)\n        return res", "response": "Auto - idle PC value for an image"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the VM state of the VM.", "response": "def _get_vm_state(self):\n        \"\"\"\n        Returns the VM state (e.g. running, paused etc.)\n\n        :returns: state (string)\n        \"\"\"\n\n        results = yield from self.manager.execute(\"showvminfo\", [self._vmname, \"--machinereadable\"])\n        for info in results:\n            if '=' in info:\n                name, value = info.split('=', 1)\n                if name == \"VMState\":\n                    return value.strip('\"')\n        raise VirtualBoxError(\"Could not get VM state for {}\".format(self._vmname))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _control_vm(self, params):\n\n        args = shlex.split(params)\n        result = yield from self.manager.execute(\"controlvm\", [self._vmname] + args)\n        return result", "response": "Control VM related attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_duplicate_linked_clone(self):\n        trial = 0\n\n        while True:\n            found = False\n            for node in self.manager.nodes:\n                if node != self and node.vmname == self.vmname:\n                    found = True\n                    if node.project != self.project:\n                        if trial >= 30:\n                            raise VirtualBoxError(\"Sorry a node without the linked clone setting enabled can only be used once on your server.\\n{} is already used by {} in project {}\".format(self.vmname, node.name, self.project.name))\n                    else:\n                        if trial >= 5:\n                            raise VirtualBoxError(\"Sorry a node without the linked clone setting enabled can only be used once on your server.\\n{} is already used by {} in this project\".format(self.vmname, node.name))\n            if not found:\n                return\n            trial += 1\n            yield from asyncio.sleep(1)", "response": "Check if there is a duplicate linked clone for this VM."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfix the VM uuid in the case of linked clone", "response": "def _patch_vm_uuid(self):\n        \"\"\"\n        Fix the VM uuid in the case of linked clone\n        \"\"\"\n        if os.path.exists(self._linked_vbox_file()):\n            try:\n                tree = ET.parse(self._linked_vbox_file())\n            except ET.ParseError:\n                raise VirtualBoxError(\"Cannot modify VirtualBox linked nodes file. \"\n                                      \"File {} is corrupted.\".format(self._linked_vbox_file()))\n\n            machine = tree.getroot().find(\"{http://www.virtualbox.org/}Machine\")\n            if machine is not None and machine.get(\"uuid\") != \"{\" + self.id + \"}\":\n\n                for image in tree.getroot().findall(\"{http://www.virtualbox.org/}Image\"):\n                    currentSnapshot = machine.get(\"currentSnapshot\")\n                    if currentSnapshot:\n                        newSnapshot = re.sub(\"\\{.*\\}\", \"{\" + str(uuid.uuid4()) + \"}\", currentSnapshot)\n                    shutil.move(os.path.join(self.working_dir, self._vmname, \"Snapshots\", currentSnapshot) + \".vdi\",\n                                os.path.join(self.working_dir, self._vmname, \"Snapshots\", newSnapshot) + \".vdi\")\n                    image.set(\"uuid\", newSnapshot)\n\n                machine.set(\"uuid\", \"{\" + self.id + \"}\")\n                tree.write(self._linked_vbox_file())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start(self):\n\n        if self.status == \"started\":\n            return\n\n        # resume the VM if it is paused\n        vm_state = yield from self._get_vm_state()\n        if vm_state == \"paused\":\n            yield from self.resume()\n            return\n\n        # VM must be powered off to start it\n        if vm_state != \"poweroff\":\n            raise VirtualBoxError(\"VirtualBox VM not powered off\")\n\n        yield from self._set_network_options()\n        yield from self._set_serial_console()\n\n        # check if there is enough RAM to run\n        self.check_available_ram(self.ram)\n\n        args = [self._vmname]\n        if self._headless:\n            args.extend([\"--type\", \"headless\"])\n        result = yield from self.manager.execute(\"startvm\", args)\n        self.status = \"started\"\n        log.info(\"VirtualBox VM '{name}' [{id}] started\".format(name=self.name, id=self.id))\n        log.debug(\"Start result: {}\".format(result))\n\n        # add a guest property to let the VM know about the GNS3 name\n        yield from self.manager.execute(\"guestproperty\", [\"set\", self._vmname, \"NameInGNS3\", self.name])\n        # add a guest property to let the VM know about the GNS3 project directory\n        yield from self.manager.execute(\"guestproperty\", [\"set\", self._vmname, \"ProjectDirInGNS3\", self.working_dir])\n\n        yield from self._start_ubridge()\n        for adapter_number in range(0, self._adapters):\n            nio = self._ethernet_adapters[adapter_number].get_nio(0)\n            if nio:\n                yield from self.add_ubridge_udp_connection(\"VBOX-{}-{}\".format(self._id, adapter_number),\n                                                           self._local_udp_tunnels[adapter_number][1],\n                                                           nio)\n\n        yield from self._start_console()\n\n        if (yield from self.check_hw_virtualization()):\n            self._hw_virtualization = True", "response": "Starts this VirtualBox VM."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops this VirtualBox VM.", "response": "def stop(self):\n        \"\"\"\n        Stops this VirtualBox VM.\n        \"\"\"\n\n        self._hw_virtualization = False\n        yield from self._stop_ubridge()\n        yield from self._stop_remote_console()\n        vm_state = yield from self._get_vm_state()\n        if vm_state == \"running\" or vm_state == \"paused\" or vm_state == \"stuck\":\n            if self.acpi_shutdown:\n                # use ACPI to shutdown the VM\n                result = yield from self._control_vm(\"acpipowerbutton\")\n                trial = 0\n                while True:\n                    vm_state = yield from self._get_vm_state()\n                    if vm_state == \"poweroff\":\n                        break\n                    yield from asyncio.sleep(1)\n                    trial += 1\n                    if trial >= 120:\n                        yield from self._control_vm(\"poweroff\")\n                        break\n                self.status = \"stopped\"\n                log.debug(\"ACPI shutdown result: {}\".format(result))\n            else:\n                # power off the VM\n                result = yield from self._control_vm(\"poweroff\")\n                self.status = \"stopped\"\n                log.debug(\"Stop result: {}\".format(result))\n\n            log.info(\"VirtualBox VM '{name}' [{id}] stopped\".format(name=self.name, id=self.id))\n            yield from asyncio.sleep(0.5)  # give some time for VirtualBox to unlock the VM\n            try:\n                # deactivate the first serial port\n                yield from self._modify_vm(\"--uart1 off\")\n            except VirtualBoxError as e:\n                log.warn(\"Could not deactivate the first serial port: {}\".format(e))\n\n            for adapter_number in range(0, self._adapters):\n                nio = self._ethernet_adapters[adapter_number].get_nio(0)\n                if nio:\n                    yield from self._modify_vm(\"--nictrace{} off\".format(adapter_number + 1))\n                    yield from self._modify_vm(\"--cableconnected{} off\".format(adapter_number + 1))\n                    yield from self._modify_vm(\"--nic{} null\".format(adapter_number + 1))\n        yield from super().stop()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef suspend(self):\n\n        vm_state = yield from self._get_vm_state()\n        if vm_state == \"running\":\n            yield from self._control_vm(\"pause\")\n            self.status = \"suspended\"\n            log.info(\"VirtualBox VM '{name}' [{id}] suspended\".format(name=self.name, id=self.id))\n        else:\n            log.warn(\"VirtualBox VM '{name}' [{id}] cannot be suspended, current state: {state}\".format(name=self.name,\n                                                                                                        id=self.id,\n                                                                                                        state=vm_state))", "response": "Suspends this VirtualBox VM."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresumes this VirtualBox VM.", "response": "def resume(self):\n        \"\"\"\n        Resumes this VirtualBox VM.\n        \"\"\"\n\n        yield from self._control_vm(\"resume\")\n        self.status = \"started\"\n        log.info(\"VirtualBox VM '{name}' [{id}] resumed\".format(name=self.name, id=self.id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reload(self):\n\n        result = yield from self._control_vm(\"reset\")\n        log.info(\"VirtualBox VM '{name}' [{id}] reloaded\".format(name=self.name, id=self.id))\n        log.debug(\"Reload result: {}\".format(result))", "response": "Reloads this VirtualBox VM."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _reattach_linked_hdds(self):\n\n        hdd_info_file = os.path.join(self.working_dir, self._vmname, \"hdd_info.json\")\n        try:\n            with open(hdd_info_file, \"r\", encoding=\"utf-8\") as f:\n                hdd_table = json.load(f)\n        except (ValueError, OSError) as e:\n            # The VM has never be started\n            return\n\n        for hdd_info in hdd_table:\n            hdd_file = os.path.join(self.working_dir, self._vmname, \"Snapshots\", hdd_info[\"hdd\"])\n            if os.path.exists(hdd_file):\n                log.info(\"VirtualBox VM '{name}' [{id}] attaching HDD {controller} {port} {device} {medium}\".format(name=self.name,\n                                                                                                                    id=self.id,\n                                                                                                                    controller=hdd_info[\"controller\"],\n                                                                                                                    port=hdd_info[\"port\"],\n                                                                                                                    device=hdd_info[\"device\"],\n                                                                                                                    medium=hdd_file))\n\n                try:\n                    yield from self._storage_attach('--storagectl \"{}\" --port {} --device {} --type hdd --medium \"{}\"'.format(hdd_info[\"controller\"],\n                                                                                                                              hdd_info[\"port\"],\n                                                                                                                              hdd_info[\"device\"],\n                                                                                                                              hdd_file))\n\n                except VirtualBoxError as e:\n                    log.warn(\"VirtualBox VM '{name}' [{id}] error reattaching HDD {controller} {port} {device} {medium}: {error}\".format(name=self.name,\n                                                                                                                                         id=self.id,\n                                                                                                                                         controller=hdd_info[\"controller\"],\n                                                                                                                                         port=hdd_info[\"port\"],\n                                                                                                                                         device=hdd_info[\"device\"],\n                                                                                                                                         medium=hdd_file,\n                                                                                                                                         error=e))\n                    continue", "response": "Reattach linked hard disks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_linked_hdds_info(self):\n\n        hdd_table = []\n        if self.linked_clone:\n            if os.path.exists(self.working_dir):\n                hdd_files = yield from self._get_all_hdd_files()\n                vm_info = yield from self._get_vm_info()\n                for entry, value in vm_info.items():\n                    match = re.search(\"^([\\s\\w]+)\\-(\\d)\\-(\\d)$\", entry)  # match Controller-PortNumber-DeviceNumber entry\n                    if match:\n                        controller = match.group(1)\n                        port = match.group(2)\n                        device = match.group(3)\n                        if value in hdd_files and os.path.exists(os.path.join(self.working_dir, self._vmname, \"Snapshots\", os.path.basename(value))):\n                            log.info(\"VirtualBox VM '{name}' [{id}] detaching HDD {controller} {port} {device}\".format(name=self.name,\n                                                                                                                       id=self.id,\n                                                                                                                       controller=controller,\n                                                                                                                       port=port,\n                                                                                                                       device=device))\n                            hdd_table.append(\n                                {\n                                    \"hdd\": os.path.basename(value),\n                                    \"controller\": controller,\n                                    \"port\": port,\n                                    \"device\": device,\n                                }\n                            )\n\n            if hdd_table:\n                try:\n                    hdd_info_file = os.path.join(self.working_dir, self._vmname, \"hdd_info.json\")\n                    with open(hdd_info_file, \"w\", encoding=\"utf-8\") as f:\n                        json.dump(hdd_table, f, indent=4)\n                except OSError as e:\n                    log.warning(\"VirtualBox VM '{name}' [{id}] could not write HHD info file: {error}\".format(name=self.name,\n                                                                                                              id=self.id,\n                                                                                                              error=e.strerror))\n\n        return hdd_table", "response": "Save linked cloned hard disks information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close(self):\n\n        if self._closed:\n            # VM is already closed\n            return\n\n        if not (yield from super().close()):\n            return False\n\n        log.debug(\"VirtualBox VM '{name}' [{id}] is closing\".format(name=self.name, id=self.id))\n        if self._console:\n            self._manager.port_manager.release_tcp_port(self._console, self._project)\n            self._console = None\n\n        for adapter in self._ethernet_adapters.values():\n            if adapter is not None:\n                for nio in adapter.ports.values():\n                    if nio and isinstance(nio, NIOUDP):\n                        self.manager.port_manager.release_udp_port(nio.lport, self._project)\n\n        for udp_tunnel in self._local_udp_tunnels.values():\n            self.manager.port_manager.release_udp_port(udp_tunnel[0].lport, self._project)\n            self.manager.port_manager.release_udp_port(udp_tunnel[1].lport, self._project)\n        self._local_udp_tunnels = {}\n\n        self.acpi_shutdown = False\n        yield from self.stop()\n\n        if self.linked_clone:\n            hdd_table = yield from self.save_linked_hdds_info()\n            for hdd in hdd_table.copy():\n                log.info(\"VirtualBox VM '{name}' [{id}] detaching HDD {controller} {port} {device}\".format(name=self.name,\n                                                                                                           id=self.id,\n                                                                                                           controller=hdd[\"controller\"],\n                                                                                                           port=hdd[\"port\"],\n                                                                                                           device=hdd[\"device\"]))\n                try:\n                    yield from self._storage_attach('--storagectl \"{}\" --port {} --device {} --type hdd --medium none'.format(hdd[\"controller\"],\n                                                                                                                              hdd[\"port\"],\n                                                                                                                              hdd[\"device\"]))\n                except VirtualBoxError as e:\n                    log.warn(\"VirtualBox VM '{name}' [{id}] error detaching HDD {controller} {port} {device}: {error}\".format(name=self.name,\n                                                                                                                              id=self.id,\n                                                                                                                              controller=hdd[\"controller\"],\n                                                                                                                              port=hdd[\"port\"],\n                                                                                                                              device=hdd[\"device\"],\n                                                                                                                              error=e))\n                    continue\n\n            log.info(\"VirtualBox VM '{name}' [{id}] unregistering\".format(name=self.name, id=self.id))\n            yield from self.manager.execute(\"unregistervm\", [self._name])\n\n        log.info(\"VirtualBox VM '{name}' [{id}] closed\".format(name=self.name, id=self.id))\n        self._closed = True", "response": "Closes this VirtualBox VM."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the amount of RAM allocated to this VirtualBox VM.", "response": "def set_ram(self, ram):\n        \"\"\"\n        Set the amount of RAM allocated to this VirtualBox VM.\n\n        :param ram: amount RAM in MB (integer)\n        \"\"\"\n\n        if ram == 0:\n            return\n\n        yield from self._modify_vm('--memory {}'.format(ram))\n\n        log.info(\"VirtualBox VM '{name}' [{id}] has set amount of RAM to {ram}\".format(name=self.name, id=self.id, ram=ram))\n        self._ram = ram"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrenaming the VirtualBox VM.", "response": "def set_vmname(self, vmname):\n        \"\"\"\n        Renames the VirtualBox VM.\n\n        :param vmname: VirtualBox VM name\n        \"\"\"\n\n        if vmname == self._vmname:\n            return\n\n        if self.linked_clone:\n            if self.status == \"started\":\n                raise VirtualBoxError(\"You can't change the name of running VM {}\".format(self._name))\n            # We can't rename a VM to name that already exists\n            vms = yield from self.manager.list_vms(allow_clone=True)\n            if vmname in [vm[\"vmname\"] for vm in vms]:\n                raise VirtualBoxError(\"You can't change the name to {} it's already use in VirtualBox\".format(vmname))\n            yield from self._modify_vm('--name \"{}\"'.format(vmname))\n\n        log.info(\"VirtualBox VM '{name}' [{id}] has set the VM name to '{vmname}'\".format(name=self.name, id=self.id, vmname=vmname))\n        self._vmname = vmname"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_adapters(self, adapters):\n\n        # check for the maximum adapters supported by the VM\n        vm_info = yield from self._get_vm_info()\n        chipset = \"piix3\"  # default chipset for VirtualBox VMs\n        self._maximum_adapters = 8  # default maximum network adapter count for PIIX3 chipset\n        if \"chipset\" in vm_info:\n            chipset = vm_info[\"chipset\"]\n            max_adapter_string = \"Maximum {} Network Adapter count\".format(chipset.upper())\n            if max_adapter_string in self._system_properties:\n                try:\n                    self._maximum_adapters = int(self._system_properties[max_adapter_string])\n                except ValueError:\n                    log.error(\"Could not convert system property to integer: {} = {}\".format(max_adapter_string, self._system_properties[max_adapter_string]))\n            else:\n                log.warning(\"Could not find system property '{}' for chipset {}\".format(max_adapter_string, chipset))\n\n        log.info(\"VirtualBox VM '{name}' [{id}] can have a maximum of {max} network adapters for chipset {chipset}\".format(name=self.name,\n                                                                                                                           id=self.id,\n                                                                                                                           max=self._maximum_adapters,\n                                                                                                                           chipset=chipset.upper()))\n        if adapters > self._maximum_adapters:\n            raise VirtualBoxError(\"The configured {} chipset limits the VM to {} network adapters. The chipset can be changed outside GNS3 in the VirtualBox VM settings.\".format(chipset.upper(),\n                                                                                                                                                                                  self._maximum_adapters))\n\n        self._ethernet_adapters.clear()\n        for adapter_number in range(0, adapters):\n            self._ethernet_adapters[adapter_number] = EthernetAdapter()\n\n        self._adapters = len(self._ethernet_adapters)\n        log.info(\"VirtualBox VM '{name}' [{id}] has changed the number of Ethernet adapters to {adapters}\".format(name=self.name,\n                                                                                                                  id=self.id,\n                                                                                                                  adapters=adapters))", "response": "Sets the number of Ethernet adapters for this VirtualBox VM instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn this VM info.", "response": "def _get_vm_info(self):\n        \"\"\"\n        Returns this VM info.\n\n        :returns: dict of info\n        \"\"\"\n\n        vm_info = {}\n        results = yield from self.manager.execute(\"showvminfo\", [self._vmname, \"--machinereadable\"])\n        for info in results:\n            try:\n                name, value = info.split('=', 1)\n            except ValueError:\n                continue\n            vm_info[name.strip('\"')] = value.strip('\"')\n        return vm_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_serial_console(self):\n\n        # activate the first serial port\n        yield from self._modify_vm(\"--uart1 0x3F8 4\")\n\n        # set server mode with a pipe on the first serial port\n        pipe_name = self._get_pipe_name()\n        args = [self._vmname, \"--uartmode1\", \"server\", pipe_name]\n        yield from self.manager.execute(\"modifyvm\", args)", "response": "Configure the first serial console connection."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange storage medium in this VM.", "response": "def _storage_attach(self, params):\n        \"\"\"\n        Change storage medium in this VM.\n\n        :param params: params to use with sub-command storageattach\n        \"\"\"\n\n        args = shlex.split(params)\n        yield from self.manager.execute(\"storageattach\", [self._vmname] + args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of NIC attachements.", "response": "def _get_nic_attachements(self, maximum_adapters):\n        \"\"\"\n        Returns NIC attachements.\n\n        :param maximum_adapters: maximum number of supported adapters\n        :returns: list of adapters with their Attachment setting (NAT, bridged etc.)\n        \"\"\"\n\n        nics = []\n        vm_info = yield from self._get_vm_info()\n        for adapter_number in range(0, maximum_adapters):\n            entry = \"nic{}\".format(adapter_number + 1)\n            if entry in vm_info:\n                value = vm_info[entry]\n                nics.append(value.lower())\n            else:\n                nics.append(None)\n        return nics"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_network_options(self):\n\n        nic_attachments = yield from self._get_nic_attachements(self._maximum_adapters)\n        for adapter_number in range(0, self._adapters):\n            attachment = nic_attachments[adapter_number]\n            if attachment == \"null\":\n                # disconnect the cable if no backend is attached.\n                yield from self._modify_vm(\"--cableconnected{} off\".format(adapter_number + 1))\n            if attachment == \"none\":\n                # set the backend to null to avoid a difference in the number of interfaces in the Guest.\n                yield from self._modify_vm(\"--nic{} null\".format(adapter_number + 1))\n                yield from self._modify_vm(\"--cableconnected{} off\".format(adapter_number + 1))\n\n            # use a local UDP tunnel to connect to uBridge instead\n            if adapter_number not in self._local_udp_tunnels:\n                self._local_udp_tunnels[adapter_number] = self._create_local_udp_tunnel()\n            nio = self._local_udp_tunnels[adapter_number][0]\n\n            if nio:\n                if not self._use_any_adapter and attachment in (\"nat\", \"bridged\", \"intnet\", \"hostonly\", \"natnetwork\"):\n                    continue\n\n                yield from self._modify_vm(\"--nictrace{} off\".format(adapter_number + 1))\n                vbox_adapter_type = \"82540EM\"\n                if self._adapter_type == \"PCnet-PCI II (Am79C970A)\":\n                    vbox_adapter_type = \"Am79C970A\"\n                if self._adapter_type == \"PCNet-FAST III (Am79C973)\":\n                    vbox_adapter_type = \"Am79C973\"\n                if self._adapter_type == \"Intel PRO/1000 MT Desktop (82540EM)\":\n                    vbox_adapter_type = \"82540EM\"\n                if self._adapter_type == \"Intel PRO/1000 T Server (82543GC)\":\n                    vbox_adapter_type = \"82543GC\"\n                if self._adapter_type == \"Intel PRO/1000 MT Server (82545EM)\":\n                    vbox_adapter_type = \"82545EM\"\n                if self._adapter_type == \"Paravirtualized Network (virtio-net)\":\n                    vbox_adapter_type = \"virtio\"\n                args = [self._vmname, \"--nictype{}\".format(adapter_number + 1), vbox_adapter_type]\n                yield from self.manager.execute(\"modifyvm\", args)\n\n                if isinstance(nio, NIOUDP):\n                    log.debug(\"setting UDP params on adapter {}\".format(adapter_number))\n                    yield from self._modify_vm(\"--nic{} generic\".format(adapter_number + 1))\n                    yield from self._modify_vm(\"--nicgenericdrv{} UDPTunnel\".format(adapter_number + 1))\n                    yield from self._modify_vm(\"--nicproperty{} sport={}\".format(adapter_number + 1, nio.lport))\n                    yield from self._modify_vm(\"--nicproperty{} dest={}\".format(adapter_number + 1, nio.rhost))\n                    yield from self._modify_vm(\"--nicproperty{} dport={}\".format(adapter_number + 1, nio.rport))\n                    yield from self._modify_vm(\"--cableconnected{} on\".format(adapter_number + 1))\n\n                if nio.capturing:\n                    yield from self._modify_vm(\"--nictrace{} on\".format(adapter_number + 1))\n                    yield from self._modify_vm('--nictracefile{} \"{}\"'.format(adapter_number + 1, nio.pcap_output_file))\n\n                if not self._ethernet_adapters[adapter_number].get_nio(0):\n                    yield from self._modify_vm(\"--cableconnected{} off\".format(adapter_number + 1))\n\n        for adapter_number in range(self._adapters, self._maximum_adapters):\n            log.debug(\"disabling remaining adapter {}\".format(adapter_number))\n            yield from self._modify_vm(\"--nic{} none\".format(adapter_number + 1))", "response": "Sets the network options for the Guest."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_linked_clone(self):\n\n        gns3_snapshot_exists = False\n        vm_info = yield from self._get_vm_info()\n        for entry, value in vm_info.items():\n            if entry.startswith(\"SnapshotName\") and value == \"GNS3 Linked Base for clones\":\n                gns3_snapshot_exists = True\n\n        if not gns3_snapshot_exists:\n            result = yield from self.manager.execute(\"snapshot\", [self._vmname, \"take\", \"GNS3 Linked Base for clones\"])\n            log.debug(\"GNS3 snapshot created: {}\".format(result))\n\n        args = [self._vmname,\n                \"--snapshot\",\n                \"GNS3 Linked Base for clones\",\n                \"--options\",\n                \"link\",\n                \"--name\",\n                self.name,\n                \"--basefolder\",\n                self.working_dir,\n                \"--register\"]\n\n        result = yield from self.manager.execute(\"clonevm\", args)\n        log.debug(\"VirtualBox VM: {} cloned\".format(result))\n\n        self._vmname = self._name\n        yield from self.manager.execute(\"setextradata\", [self._vmname, \"GNS3/Clone\", \"yes\"])\n\n        # We create a reset snapshot in order to simplify life of user who want to rollback their VM\n        # Warning: Do not document this it's seem buggy we keep it because Raizo students use it.\n        try:\n            args = [self._vmname, \"take\", \"reset\"]\n            result = yield from self.manager.execute(\"snapshot\", args)\n            log.debug(\"Snapshot 'reset' created: {}\".format(result))\n        # It seem sometimes this failed due to internal race condition of Vbox\n        # we have no real explanation of this.\n        except VirtualBoxError:\n            log.warn(\"Snapshot 'reset' not created\")\n\n        os.makedirs(os.path.join(self.working_dir, self._vmname), exist_ok=True)", "response": "Creates a new linked clone."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef adapter_add_nio_binding(self, adapter_number, nio):\n\n        try:\n            adapter = self._ethernet_adapters[adapter_number]\n        except KeyError:\n            raise VirtualBoxError(\"Adapter {adapter_number} doesn't exist on VirtualBox VM '{name}'\".format(name=self.name,\n                                                                                                            adapter_number=adapter_number))\n\n        # check if trying to connect to a nat, bridged, host-only or any other special adapter\n        nic_attachments = yield from self._get_nic_attachements(self._maximum_adapters)\n        attachment = nic_attachments[adapter_number]\n        if attachment in (\"nat\", \"bridged\", \"intnet\", \"hostonly\", \"natnetwork\"):\n            if not self._use_any_adapter:\n                raise VirtualBoxError(\"Attachment '{attachment}' is already configured on adapter {adapter_number}. \"\n                                      \"Please remove it or allow VirtualBox VM '{name}' to use any adapter.\".format(attachment=attachment,\n                                                                                                                    adapter_number=adapter_number,\n                                                                                                                    name=self.name))\n            elif self.is_running():\n                # dynamically configure an UDP tunnel attachment if the VM is already running\n                local_nio = self._local_udp_tunnels[adapter_number][0]\n                if local_nio and isinstance(local_nio, NIOUDP):\n                    yield from self._control_vm(\"nic{} generic UDPTunnel\".format(adapter_number + 1))\n                    yield from self._control_vm(\"nicproperty{} sport={}\".format(adapter_number + 1, local_nio.lport))\n                    yield from self._control_vm(\"nicproperty{} dest={}\".format(adapter_number + 1, local_nio.rhost))\n                    yield from self._control_vm(\"nicproperty{} dport={}\".format(adapter_number + 1, local_nio.rport))\n                    yield from self._control_vm(\"setlinkstate{} on\".format(adapter_number + 1))\n\n        if self.is_running():\n            try:\n                yield from self.add_ubridge_udp_connection(\"VBOX-{}-{}\".format(self._id, adapter_number),\n                                                           self._local_udp_tunnels[adapter_number][1],\n                                                           nio)\n            except KeyError:\n                raise VirtualBoxError(\"Adapter {adapter_number} doesn't exist on VirtualBox VM '{name}'\".format(name=self.name,\n                                                                                                                adapter_number=adapter_number))\n            yield from self._control_vm(\"setlinkstate{} on\".format(adapter_number + 1))\n\n        adapter.add_nio(0, nio)\n        log.info(\"VirtualBox VM '{name}' [{id}]: {nio} added to adapter {adapter_number}\".format(name=self.name,\n                                                                                                 id=self.id,\n                                                                                                 nio=nio,\n                                                                                                 adapter_number=adapter_number))", "response": "Adds a NIO binding to the adapter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef adapter_update_nio_binding(self, adapter_number, nio):\n\n        if self.is_running():\n            try:\n                yield from self.update_ubridge_udp_connection(\n                    \"VBOX-{}-{}\".format(self._id, adapter_number),\n                    self._local_udp_tunnels[adapter_number][1],\n                    nio)\n            except IndexError:\n                raise VirtualBoxError('Adapter {adapter_number} does not exist on VirtualBox VM \"{name}\"'.format(\n                    name=self._name,\n                    adapter_number=adapter_number\n                ))", "response": "Update a port NIO binding."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove an adapter NIO binding.", "response": "def adapter_remove_nio_binding(self, adapter_number):\n        \"\"\"\n        Removes an adapter NIO binding.\n\n        :param adapter_number: adapter number\n\n        :returns: NIO instance\n        \"\"\"\n\n        try:\n            adapter = self._ethernet_adapters[adapter_number]\n        except KeyError:\n            raise VirtualBoxError(\"Adapter {adapter_number} doesn't exist on VirtualBox VM '{name}'\".format(name=self.name,\n                                                                                                            adapter_number=adapter_number))\n\n        if self.is_running():\n            yield from self._ubridge_send(\"bridge delete {name}\".format(name=\"VBOX-{}-{}\".format(self._id, adapter_number)))\n        vm_state = yield from self._get_vm_state()\n        if vm_state == \"running\":\n            yield from self._control_vm(\"setlinkstate{} off\".format(adapter_number + 1))\n\n        nio = adapter.get_nio(0)\n        if isinstance(nio, NIOUDP):\n            self.manager.port_manager.release_udp_port(nio.lport, self._project)\n        adapter.remove_nio(0)\n\n        log.info(\"VirtualBox VM '{name}' [{id}]: {nio} removed from adapter {adapter_number}\".format(name=self.name,\n                                                                                                     id=self.id,\n                                                                                                     nio=nio,\n                                                                                                     adapter_number=adapter_number))\n        return nio"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_capture(self, adapter_number, output_file):\n\n        try:\n            adapter = self._ethernet_adapters[adapter_number]\n        except KeyError:\n            raise VirtualBoxError(\"Adapter {adapter_number} doesn't exist on VirtualBox VM '{name}'\".format(name=self.name,\n                                                                                                            adapter_number=adapter_number))\n\n        nio = adapter.get_nio(0)\n\n        if not nio:\n            raise VirtualBoxError(\"Adapter {} is not connected\".format(adapter_number))\n\n        if nio.capturing:\n            raise VirtualBoxError(\"Packet capture is already activated on adapter {adapter_number}\".format(adapter_number=adapter_number))\n\n        nio.startPacketCapture(output_file)\n\n        if self.ubridge:\n            yield from self._ubridge_send('bridge start_capture {name} \"{output_file}\"'.format(name=\"VBOX-{}-{}\".format(self._id, adapter_number),\n                                                                                               output_file=output_file))\n\n        log.info(\"VirtualBox VM '{name}' [{id}]: starting packet capture on adapter {adapter_number}\".format(name=self.name,\n                                                                                                             id=self.id,\n                                                                                                             adapter_number=adapter_number))", "response": "Starts a packet capture on the specified adapter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstopping a packet capture on the specified adapter.", "response": "def stop_capture(self, adapter_number):\n        \"\"\"\n        Stops a packet capture.\n\n        :param adapter_number: adapter number\n        \"\"\"\n\n        try:\n            adapter = self._ethernet_adapters[adapter_number]\n        except KeyError:\n            raise VirtualBoxError(\"Adapter {adapter_number} doesn't exist on VirtualBox VM '{name}'\".format(name=self.name,\n                                                                                                            adapter_number=adapter_number))\n\n        nio = adapter.get_nio(0)\n\n        if not nio:\n            raise VirtualBoxError(\"Adapter {} is not connected\".format(adapter_number))\n\n        nio.stopPacketCapture()\n\n        if self.ubridge:\n            yield from self._ubridge_send('bridge stop_capture {name}'.format(name=\"VBOX-{}-{}\".format(self._id, adapter_number)))\n\n        log.info(\"VirtualBox VM '{name}' [{id}]: stopping packet capture on adapter {adapter_number}\".format(name=self.name,\n                                                                                                             id=self.id,\n                                                                                                             adapter_number=adapter_number))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_ubridge_version(self, env=None):\n        try:\n            output = yield from subprocess_check_output(self._path, \"-v\", cwd=self._working_dir, env=env)\n            match = re.search(\"ubridge version ([0-9a-z\\.]+)\", output)\n            if match:\n                self._version = match.group(1)\n                if sys.platform.startswith(\"win\") or sys.platform.startswith(\"darwin\"):\n                    minimum_required_version = \"0.9.12\"\n                else:\n                    # uBridge version 0.9.14 is required for packet filters \n                    # to work for IOU nodes.\n                    minimum_required_version = \"0.9.14\"\n                if parse_version(self._version) < parse_version(minimum_required_version):\n                    raise UbridgeError(\"uBridge executable version must be >= {}\".format(minimum_required_version))\n            else:\n                raise UbridgeError(\"Could not determine uBridge version for {}\".format(self._path))\n        except (OSError, subprocess.SubprocessError) as e:\n            raise UbridgeError(\"Error while looking for uBridge version: {}\".format(e))", "response": "Checks if the uBridge executable version is greater than the current uBridge version."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start(self):\n\n        env = os.environ.copy()\n        if sys.platform.startswith(\"win\"):\n            # add the Npcap directory to $PATH to force uBridge to use npcap DLL instead of Winpcap (if installed)\n            system_root = os.path.join(os.path.expandvars(\"%SystemRoot%\"), \"System32\", \"Npcap\")\n            if os.path.isdir(system_root):\n                env[\"PATH\"] = system_root + ';' + env[\"PATH\"]\n        yield from self._check_ubridge_version(env)\n        try:\n            command = self._build_command()\n            log.info(\"starting ubridge: {}\".format(command))\n            self._stdout_file = os.path.join(self._working_dir, \"ubridge.log\")\n            log.info(\"logging to {}\".format(self._stdout_file))\n            with open(self._stdout_file, \"w\", encoding=\"utf-8\") as fd:\n                self._process = yield from asyncio.create_subprocess_exec(*command,\n                                                                          stdout=fd,\n                                                                          stderr=subprocess.STDOUT,\n                                                                          cwd=self._working_dir,\n                                                                          env=env)\n\n            log.info(\"ubridge started PID={}\".format(self._process.pid))\n        except (OSError, PermissionError, subprocess.SubprocessError) as e:\n            ubridge_stdout = self.read_stdout()\n            log.error(\"Could not start ubridge: {}\\n{}\".format(e, ubridge_stdout))\n            raise UbridgeError(\"Could not start ubridge: {}\\n{}\".format(e, ubridge_stdout))", "response": "Starts the uBridge hypervisor process."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls when the process has stopped.", "response": "def _termination_callback(self, returncode):\n        \"\"\"\n        Called when the process has stopped.\n\n        :param returncode: Process returncode\n        \"\"\"\n\n        log.info(\"uBridge process has stopped, return code: %d\", returncode)\n        if returncode != 0:\n            self._project.emit(\"log.error\", {\"message\": \"uBridge process has stopped, return code: {}\\n{}\".format(returncode, self.read_stdout())})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild command to start the uBridge hypervisor process.", "response": "def _build_command(self):\n        \"\"\"\n        Command to start the uBridge hypervisor process.\n        (to be passed to subprocess.Popen())\n        \"\"\"\n\n        command = [self._path]\n        command.extend([\"-H\", \"{}:{}\".format(self._host, self._port)])\n        if log.getEffectiveLevel() == logging.DEBUG:\n            command.extend([\"-d\", \"2\"])\n        return command"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_vmrun(self):\n\n        # look for vmrun\n        vmrun_path = self.config.get_section_config(\"VMware\").get(\"vmrun_path\")\n        if not vmrun_path:\n            if sys.platform.startswith(\"win\"):\n                vmrun_path = shutil.which(\"vmrun\")\n                if vmrun_path is None:\n                    # look for vmrun.exe using the VMware Workstation directory listed in the registry\n                    vmrun_path = self._find_vmrun_registry(r\"SOFTWARE\\Wow6432Node\\VMware, Inc.\\VMware Workstation\")\n                    if vmrun_path is None:\n                        # look for vmrun.exe using the VIX directory listed in the registry\n                        vmrun_path = self._find_vmrun_registry(r\"SOFTWARE\\Wow6432Node\\VMware, Inc.\\VMware VIX\")\n            elif sys.platform.startswith(\"darwin\"):\n                vmrun_path = \"/Applications/VMware Fusion.app/Contents/Library/vmrun\"\n            else:\n                vmrun_path = \"vmrun\"\n\n        if vmrun_path and not os.path.isabs(vmrun_path):\n            vmrun_path = shutil.which(vmrun_path)\n\n        if not vmrun_path:\n            raise VMwareError(\"Could not find VMware vmrun, please make sure it is installed\")\n        if not os.path.isfile(vmrun_path):\n            raise VMwareError(\"vmrun {} is not accessible\".format(vmrun_path))\n        if not os.access(vmrun_path, os.X_OK):\n            raise VMwareError(\"vmrun is not executable\")\n        if os.path.basename(vmrun_path).lower() not in [\"vmrun\", \"vmrun.exe\"]:\n            raise VMwareError(\"Invalid vmrun executable name {}\".format(os.path.basename(vmrun_path)))\n\n        self._vmrun_path = vmrun_path\n        return vmrun_path", "response": "Searches for vmrun.\n\n        :returns: path to vmrun"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_vmware_player_requirements(self, player_version):\n\n        player_version = int(player_version)\n        if player_version < 6:\n            raise VMwareError(\"Using VMware Player requires version 6 or above\")\n        elif player_version == 6:\n            yield from self.check_vmrun_version(minimum_required_version=\"1.13.0\")\n        elif player_version == 7:\n            yield from self.check_vmrun_version(minimum_required_version=\"1.14.0\")\n        elif player_version >= 12:\n            yield from self.check_vmrun_version(minimum_required_version=\"1.15.0\")\n        self._host_type = \"player\"", "response": "Check minimum requirements to use VMware Player."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_vmware_workstation_requirements(self, ws_version):\n\n        ws_version = int(ws_version)\n        if ws_version < 10:\n            raise VMwareError(\"Using VMware Workstation requires version 10 or above\")\n        elif ws_version == 10:\n            yield from self.check_vmrun_version(minimum_required_version=\"1.13.0\")\n        elif ws_version == 11:\n            yield from self.check_vmrun_version(minimum_required_version=\"1.14.0\")\n        elif ws_version >= 12:\n            yield from self.check_vmrun_version(minimum_required_version=\"1.15.0\")\n        self._host_type = \"ws\"", "response": "Check minimum requirements to use VMware Workstation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_vmrun_version(self, minimum_required_version=\"1.13.0\"):\n\n        vmrun_path = self.vmrun_path\n        if not vmrun_path:\n            vmrun_path = self.find_vmrun()\n\n        try:\n            output = yield from subprocess_check_output(vmrun_path)\n            match = re.search(\"vmrun version ([0-9\\.]+)\", output)\n            version = None\n            if match:\n                version = match.group(1)\n                log.debug(\"VMware vmrun version {} detected, minimum required: {}\".format(version, minimum_required_version))\n                if parse_version(version) < parse_version(minimum_required_version):\n                    raise VMwareError(\"VMware vmrun executable version must be >= version {}\".format(minimum_required_version))\n            if version is None:\n                log.warning(\"Could not find VMware vmrun version. Output: {}\".format(output))\n                raise VMwareError(\"Could not find VMware vmrun version. Output: {}\".format(output))\n        except (OSError, subprocess.SubprocessError) as e:\n            log.error(\"Error while looking for the VMware vmrun version: {}\".format(e))\n            raise VMwareError(\"Error while looking for the VMware vmrun version: {}\".format(e))", "response": "Checks the vmrun version and raises VMwareError if it is not."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a linked clone from the VMware inventory file.", "response": "def remove_from_vmware_inventory(self, vmx_path):\n        \"\"\"\n        Removes a linked clone from the VMware inventory file.\n\n        :param vmx_path: path of the linked clone VMX file\n        \"\"\"\n\n        with (yield from self._vmware_inventory_lock):\n            inventory_path = self.get_vmware_inventory_path()\n            if os.path.exists(inventory_path):\n                try:\n                    inventory_pairs = self.parse_vmware_file(inventory_path)\n                except OSError as e:\n                    log.warning('Could not read VMware inventory file \"{}\": {}'.format(inventory_path, e))\n                    return\n\n                vmlist_entry = None\n                for name, value in inventory_pairs.items():\n                    if value == vmx_path:\n                        vmlist_entry = name.split(\".\", 1)[0]\n                        break\n\n                if vmlist_entry is not None:\n                    for name in inventory_pairs.copy().keys():\n                        if name.startswith(vmlist_entry):\n                            del inventory_pairs[name]\n\n                try:\n                    self.write_vmware_file(inventory_path, inventory_pairs)\n                except OSError as e:\n                    raise VMwareError('Could not write VMware inventory file \"{}\": {}'.format(inventory_path, e))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_vmware_file(path):\n\n        pairs = OrderedDict()\n        encoding = \"utf-8\"\n        # get the first line to read the .encoding value\n        with open(path, \"rb\") as f:\n            line = f.readline().decode(encoding, errors=\"ignore\")\n            if line.startswith(\"#!\"):\n                # skip the shebang\n                line = f.readline().decode(encoding, errors=\"ignore\")\n            try:\n                key, value = line.split('=', 1)\n                if key.strip().lower() == \".encoding\":\n                    file_encoding = value.strip('\" ')\n                    try:\n                        codecs.lookup(file_encoding)\n                        encoding = file_encoding\n                    except LookupError:\n                        log.warning(\"Invalid file encoding detected in '{}': {}\".format(path, file_encoding))\n            except ValueError:\n                log.warning(\"Couldn't find file encoding in {}, using {}...\".format(path, encoding))\n\n        # read the file with the correct encoding\n        with open(path, encoding=encoding, errors=\"ignore\") as f:\n            for line in f.read().splitlines():\n                try:\n                    key, value = line.split('=', 1)\n                    pairs[key.strip().lower()] = value.strip('\" ')\n                except ValueError:\n                    continue\n        return pairs", "response": "Parses a VMware file and returns a dictionary of the keys and values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_vmware_file(path, pairs):\n\n        encoding = \"utf-8\"\n        if \".encoding\" in pairs:\n            file_encoding = pairs[\".encoding\"]\n            try:\n                codecs.lookup(file_encoding)\n                encoding = file_encoding\n            except LookupError:\n                log.warning(\"Invalid file encoding detected in '{}': {}\".format(path, file_encoding))\n        with open(path, \"w\", encoding=encoding, errors=\"ignore\") as f:\n            for key, value in pairs.items():\n                entry = '{} = \"{}\"\\n'.format(key, value)\n                f.write(entry)", "response": "Writes a VMware file excepting VMX file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a VMware VMX file.", "response": "def write_vmx_file(path, pairs):\n        \"\"\"\n        Write a VMware VMX file.\n\n        :param path: path to the VMX file\n        :param pairs: settings to write\n        \"\"\"\n\n        encoding = \"utf-8\"\n        if \".encoding\" in pairs:\n            file_encoding = pairs[\".encoding\"]\n            try:\n                codecs.lookup(file_encoding)\n                encoding = file_encoding\n            except LookupError:\n                log.warning(\"Invalid file encoding detected in '{}': {}\".format(path, file_encoding))\n        with open(path, \"w\", encoding=encoding, errors=\"ignore\") as f:\n            if sys.platform.startswith(\"linux\"):\n                # write the shebang on the first line on Linux\n                vmware_path = VMware._get_linux_vmware_binary()\n                if vmware_path:\n                    f.write(\"#!{}\\n\".format(vmware_path))\n            for key, value in pairs.items():\n                entry = '{} = \"{}\"\\n'.format(key, value)\n                f.write(entry)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_vms_from_inventory(self, inventory_path):\n\n        vm_entries = {}\n        vmware_vms = []\n        log.info('Searching for VMware VMs in inventory file \"{}\"'.format(inventory_path))\n        try:\n            pairs = self.parse_vmware_file(inventory_path)\n            for key, value in pairs.items():\n                if key.startswith(\"vmlist\"):\n                    try:\n                        vm_entry, variable_name = key.split('.', 1)\n                    except ValueError:\n                        continue\n                    if vm_entry not in vm_entries:\n                        vm_entries[vm_entry] = {}\n                    vm_entries[vm_entry][variable_name.strip()] = value\n        except OSError as e:\n            log.warning(\"Could not read VMware inventory file {}: {}\".format(inventory_path, e))\n\n        for vm_settings in vm_entries.values():\n            if \"displayname\" in vm_settings and \"config\" in vm_settings:\n                if os.path.exists(vm_settings[\"config\"]):\n                    log.debug('Found VM named \"{}\" with VMX file \"{}\"'.format(vm_settings[\"displayname\"], vm_settings[\"config\"]))\n                    vmware_vms.append({\"vmname\": vm_settings[\"displayname\"], \"vmx_path\": vm_settings[\"config\"]})\n        return vmware_vms", "response": "Searches for VMs in an inventory file and returns a list of dictionaries containing the VM name and VMX file paths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch for VMs in a given directory and returns a list of dictionaries containing the VM name and path to the VMX file.", "response": "def _get_vms_from_directory(self, directory):\n        \"\"\"\n        Searches for VMs in a given directory.\n\n        :param directory: path to the directory\n\n        :returns: list of VMs\n        \"\"\"\n\n        vmware_vms = []\n        log.info('Searching for VMware VMs in directory \"{}\"'.format(directory))\n        for path, _, filenames in os.walk(directory):\n            for filename in filenames:\n                if os.path.splitext(filename)[1] == \".vmx\":\n                    vmx_path = os.path.join(path, filename)\n                    log.debug('Reading VMware VMX file \"{}\"'.format(vmx_path))\n                    try:\n                        pairs = self.parse_vmware_file(vmx_path)\n                        if \"displayname\" in pairs:\n                            log.debug('Found VM named \"{}\"'.format(pairs[\"displayname\"]))\n                            vmware_vms.append({\"vmname\": pairs[\"displayname\"], \"vmx_path\": vmx_path})\n                    except OSError as e:\n                        log.warning('Could not read VMware VMX file \"{}\": {}'.format(vmx_path, e))\n                        continue\n        return vmware_vms"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the path to the VMware inventory file.", "response": "def get_vmware_inventory_path():\n        \"\"\"\n        Returns VMware inventory file path.\n\n        :returns: path to the inventory file\n        \"\"\"\n\n        if sys.platform.startswith(\"win\"):\n            return os.path.expandvars(r\"%APPDATA%\\Vmware\\Inventory.vmls\")\n        elif sys.platform.startswith(\"darwin\"):\n            return os.path.expanduser(\"~/Library/Application Support/VMware Fusion/vmInventory\")\n        else:\n            return os.path.expanduser(\"~/.vmware/inventory.vmls\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the path to the VMware preferences file.", "response": "def get_vmware_preferences_path():\n        \"\"\"\n        Returns VMware preferences file path.\n\n        :returns: path to the preferences file\n        \"\"\"\n\n        if sys.platform.startswith(\"win\"):\n            return os.path.expandvars(r\"%APPDATA%\\VMware\\preferences.ini\")\n        elif sys.platform.startswith(\"darwin\"):\n            return os.path.expanduser(\"~/Library/Preferences/VMware Fusion/preferences\")\n        else:\n            return os.path.expanduser(\"~/.vmware/preferences\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_vmware_default_vm_paths():\n\n        if sys.platform.startswith(\"win\"):\n            import ctypes\n            import ctypes.wintypes\n            path = ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH)\n            ctypes.windll.shell32.SHGetFolderPathW(None, 5, None, 0, path)\n            documents_folder = path.value\n            return ['{}\\My Virtual Machines'.format(documents_folder), '{}\\Virtual Machines'.format(documents_folder)]\n        elif sys.platform.startswith(\"darwin\"):\n            return [os.path.expanduser(\"~/Documents/Virtual Machines.localized\")]\n        else:\n            return [os.path.expanduser(\"~/vmware\")]", "response": "Returns a list of VMware default VM directory paths."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_vms(self):\n\n        # check for the right VMware version\n        yield from self.check_vmware_version()\n        vmware_vms = []\n        inventory_path = self.get_vmware_inventory_path()\n        if os.path.exists(inventory_path) and self.host_type != \"player\":\n            # inventory may exist for VMware player if VMware workstation has been previously installed\n            vmware_vms = self._get_vms_from_inventory(inventory_path)\n        if not vmware_vms:\n            # backup methods when no VMware inventory file exists or for VMware player which has no inventory file\n            vmware_preferences_path = self.get_vmware_preferences_path()\n            pairs = {}\n            if os.path.exists(vmware_preferences_path):\n                # the default vm path may be present in VMware preferences file.\n                try:\n                    pairs = self.parse_vmware_file(vmware_preferences_path)\n                except OSError as e:\n                    log.warning('Could not read VMware preferences file \"{}\": {}'.format(vmware_preferences_path, e))\n                if \"prefvmx.defaultvmpath\" in pairs:\n                    default_vm_path = pairs[\"prefvmx.defaultvmpath\"]\n                    if not os.path.isdir(default_vm_path):\n                        raise VMwareError('Could not find or access the default VM directory: \"{default_vm_path}\". Please change \"prefvmx.defaultvmpath={default_vm_path}\" in \"{vmware_preferences_path}\"'.format(default_vm_path=default_vm_path,\n                                                                                                                                                                                                                  vmware_preferences_path=vmware_preferences_path))\n                    vmware_vms = self._get_vms_from_directory(default_vm_path)\n\n            if not vmware_vms:\n                # the default vm path is not in the VMware preferences file or that directory is empty\n                # let's search the default locations for VMs\n                for default_vm_path in self.get_vmware_default_vm_paths():\n                    if os.path.isdir(default_vm_path):\n                        vmware_vms.extend(self._get_vms_from_directory(default_vm_path))\n\n            if not vmware_vms:\n                log.warning(\"Could not find any VMware VM in default locations\")\n\n            # look for VMX paths in the preferences file in case not all VMs are in a default directory\n            for key, value in pairs.items():\n                m = re.match(r'pref.mruVM(\\d+)\\.filename', key)\n                if m:\n                    display_name = \"pref.mruVM{}.displayName\".format(m.group(1))\n                    if display_name in pairs:\n                        found = False\n                        for vmware_vm in vmware_vms:\n                            if vmware_vm[\"vmname\"] == display_name:\n                                found = True\n                        if found is False:\n                            vmware_vms.append({\"vmname\": pairs[display_name], \"vmx_path\": value})\n        return vmware_vms", "response": "Gets VMware VM list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving the specified NIO as member of this ATM switch.", "response": "def remove_nio(self, port_number):\n        \"\"\"\n        Removes the specified NIO as member of this ATM switch.\n\n        :param port_number: allocated port number\n        \"\"\"\n\n        if port_number not in self._nios:\n            raise DynamipsError(\"Port {} is not allocated\".format(port_number))\n\n        # remove VCs mapped with the port\n        for source, destination in self._active_mappings.copy().items():\n            if len(source) == 3 and len(destination) == 3:\n                # remove the virtual channels mapped with this port/nio\n                source_port, source_vpi, source_vci = source\n                destination_port, destination_vpi, destination_vci = destination\n                if port_number == source_port:\n                    log.info('ATM switch \"{name}\" [{id}]: unmapping VCC between port {source_port} VPI {source_vpi} VCI {source_vci} and port {destination_port} VPI {destination_vpi} VCI {destination_vci}'.format(name=self._name,\n                                                                                                                                                                                                                     id=self._id,\n                                                                                                                                                                                                                     source_port=source_port,\n                                                                                                                                                                                                                     source_vpi=source_vpi,\n                                                                                                                                                                                                                     source_vci=source_vci,\n                                                                                                                                                                                                                     destination_port=destination_port,\n                                                                                                                                                                                                                     destination_vpi=destination_vpi,\n                                                                                                                                                                                                                     destination_vci=destination_vci))\n                    yield from self.unmap_pvc(source_port, source_vpi, source_vci, destination_port, destination_vpi, destination_vci)\n                    yield from self.unmap_pvc(destination_port, destination_vpi, destination_vci, source_port, source_vpi, source_vci)\n            else:\n                # remove the virtual paths mapped with this port/nio\n                source_port, source_vpi = source\n                destination_port, destination_vpi = destination\n                if port_number == source_port:\n                    log.info('ATM switch \"{name}\" [{id}]: unmapping VPC between port {source_port} VPI {source_vpi} and port {destination_port} VPI {destination_vpi}'.format(name=self._name,\n                                                                                                                                                                              id=self._id,\n                                                                                                                                                                              source_port=source_port,\n                                                                                                                                                                              source_vpi=source_vpi,\n                                                                                                                                                                              destination_port=destination_port,\n                                                                                                                                                                              destination_vpi=destination_vpi))\n                    yield from self.unmap_vp(source_port, source_vpi, destination_port, destination_vpi)\n                    yield from self.unmap_vp(destination_port, destination_vpi, source_port, source_vpi)\n\n        nio = self._nios[port_number]\n        if isinstance(nio, NIOUDP):\n            self.manager.port_manager.release_udp_port(nio.lport, self._project)\n        log.info('ATM switch \"{name}\" [{id}]: NIO {nio} removed from port {port}'.format(name=self._name,\n                                                                                         id=self._id,\n                                                                                         nio=nio,\n                                                                                         port=port_number))\n\n        del self._nios[port_number]\n        return nio"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_mappings(self, mappings):\n\n        pvc_entry = re.compile(r\"\"\"^([0-9]*):([0-9]*):([0-9]*)$\"\"\")\n        for source, destination in mappings.items():\n            if not isinstance(source, str) or not isinstance(destination, str):\n                raise DynamipsError(\"Invalid ATM mappings\")\n            match_source_pvc = pvc_entry.search(source)\n            match_destination_pvc = pvc_entry.search(destination)\n            if match_source_pvc and match_destination_pvc:\n                # add the virtual channels\n                source_port, source_vpi, source_vci = map(int, match_source_pvc.group(1, 2, 3))\n                destination_port, destination_vpi, destination_vci = map(int, match_destination_pvc.group(1, 2, 3))\n                if self.has_port(destination_port):\n                    if (source_port, source_vpi, source_vci) not in self._active_mappings and \\\n                       (destination_port, destination_vpi, destination_vci) not in self._active_mappings:\n                        log.info('ATM switch \"{name}\" [{id}]: mapping VCC between port {source_port} VPI {source_vpi} VCI {source_vci} and port {destination_port} VPI {destination_vpi} VCI {destination_vci}'.format(name=self._name,\n                                                                                                                                                                                                                       id=self._id,\n                                                                                                                                                                                                                       source_port=source_port,\n                                                                                                                                                                                                                       source_vpi=source_vpi,\n                                                                                                                                                                                                                       source_vci=source_vci,\n                                                                                                                                                                                                                       destination_port=destination_port,\n                                                                                                                                                                                                                       destination_vpi=destination_vpi,\n                                                                                                                                                                                                                       destination_vci=destination_vci))\n                        yield from self.map_pvc(source_port, source_vpi, source_vci, destination_port, destination_vpi, destination_vci)\n                        yield from self.map_pvc(destination_port, destination_vpi, destination_vci, source_port, source_vpi, source_vci)\n            else:\n                # add the virtual paths\n                source_port, source_vpi = map(int, source.split(':'))\n                destination_port, destination_vpi = map(int, destination.split(':'))\n                if self.has_port(destination_port):\n                    if (source_port, source_vpi) not in self._active_mappings and (destination_port, destination_vpi) not in self._active_mappings:\n                        log.info('ATM switch \"{name}\" [{id}]: mapping VPC between port {source_port} VPI {source_vpi} and port {destination_port} VPI {destination_vpi}'.format(name=self._name,\n                                                                                                                                                                                id=self._id,\n                                                                                                                                                                                source_port=source_port,\n                                                                                                                                                                                source_vpi=source_vpi,\n                                                                                                                                                                                destination_port=destination_port,\n                                                                                                                                                                                destination_vpi=destination_vpi))\n                        yield from self.map_vp(source_port, source_vpi, destination_port, destination_vpi)\n                        yield from self.map_vp(destination_port, destination_vpi, source_port, source_vpi)", "response": "Applies mappings between source and destination VC."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new VPC connection.", "response": "def map_vp(self, port1, vpi1, port2, vpi2):\n        \"\"\"\n        Creates a new Virtual Path connection.\n\n        :param port1: input port\n        :param vpi1: input vpi\n        :param port2: output port\n        :param vpi2: output vpi\n        \"\"\"\n\n        if port1 not in self._nios:\n            return\n\n        if port2 not in self._nios:\n            return\n\n        nio1 = self._nios[port1]\n        nio2 = self._nios[port2]\n\n        yield from self._hypervisor.send('atmsw create_vpc \"{name}\" {input_nio} {input_vpi} {output_nio} {output_vpi}'.format(name=self._name,\n                                                                                                                              input_nio=nio1,\n                                                                                                                              input_vpi=vpi1,\n                                                                                                                              output_nio=nio2,\n                                                                                                                              output_vpi=vpi2))\n\n        log.info('ATM switch \"{name}\" [{id}]: VPC from port {port1} VPI {vpi1} to port {port2} VPI {vpi2} created'.format(name=self._name,\n                                                                                                                          id=self._id,\n                                                                                                                          port1=port1,\n                                                                                                                          vpi1=vpi1,\n                                                                                                                          port2=port2,\n                                                                                                                          vpi2=vpi2))\n\n        self._active_mappings[(port1, vpi1)] = (port2, vpi2)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef map_pvc(self, port1, vpi1, vci1, port2, vpi2, vci2):\n\n        if port1 not in self._nios:\n            return\n\n        if port2 not in self._nios:\n            return\n\n        nio1 = self._nios[port1]\n        nio2 = self._nios[port2]\n\n        yield from self._hypervisor.send('atmsw create_vcc \"{name}\" {input_nio} {input_vpi} {input_vci} {output_nio} {output_vpi} {output_vci}'.format(name=self._name,\n                                                                                                                                                       input_nio=nio1,\n                                                                                                                                                       input_vpi=vpi1,\n                                                                                                                                                       input_vci=vci1,\n                                                                                                                                                       output_nio=nio2,\n                                                                                                                                                       output_vpi=vpi2,\n                                                                                                                                                       output_vci=vci2))\n\n        log.info('ATM switch \"{name}\" [{id}]: VCC from port {port1} VPI {vpi1} VCI {vci1} to port {port2} VPI {vpi2} VCI {vci2} created'.format(name=self._name,\n                                                                                                                                                id=self._id,\n                                                                                                                                                port1=port1,\n                                                                                                                                                vpi1=vpi1,\n                                                                                                                                                vci1=vci1,\n                                                                                                                                                port2=port2,\n                                                                                                                                                vpi2=vpi2,\n                                                                                                                                                vci2=vci2))\n\n        self._active_mappings[(port1, vpi1, vci1)] = (port2, vpi2, vci2)", "response": "Creates a new Virtual Channel connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering a template :params template: Template name :params kwargs: Template parameters", "response": "def template(self, template_filename, **kwargs):\n        \"\"\"\n        Render a template\n\n        :params template: Template name\n        :params kwargs: Template parameters\n        \"\"\"\n        template = renderer.get_template(template_filename)\n        kwargs[\"gns3_version\"] = __version__\n        kwargs[\"gns3_host\"] = self._request.host\n        self.html(template.render(**kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the response content type to application / json and serialize the content.", "response": "def json(self, answer):\n        \"\"\"\n        Set the response content type to application/json and serialize\n        the content.\n\n        :param anwser The response as a Python object\n        \"\"\"\n\n        self.content_type = \"application/json\"\n        if hasattr(answer, '__json__'):\n            answer = answer.__json__()\n        elif isinstance(answer, list):\n            newanswer = []\n            for elem in answer:\n                if hasattr(elem, '__json__'):\n                    elem = elem.__json__()\n                newanswer.append(elem)\n            answer = newanswer\n        if self._output_schema is not None:\n            try:\n                jsonschema.validate(answer, self._output_schema)\n            except jsonschema.ValidationError as e:\n                log.error(\"Invalid output query. JSON schema error: {}\".format(e.message))\n                raise aiohttp.web.HTTPBadRequest(text=\"{}\".format(e))\n        self.body = json.dumps(answer, indent=4, sort_keys=True).encode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef file(self, path, status=200, set_content_length=True):\n        ct, encoding = mimetypes.guess_type(path)\n        if not ct:\n            ct = 'application/octet-stream'\n        if encoding:\n            self.headers[aiohttp.hdrs.CONTENT_ENCODING] = encoding\n        self.content_type = ct\n\n        if set_content_length:\n            st = os.stat(path)\n            self.last_modified = st.st_mtime\n            self.headers[aiohttp.hdrs.CONTENT_LENGTH] = str(st.st_size)\n        else:\n            self.enable_chunked_encoding()\n\n        self.set_status(status)\n\n        try:\n            with open(path, 'rb') as fobj:\n                yield from self.prepare(self._request)\n\n                while True:\n                    data = fobj.read(4096)\n                    if not data:\n                        break\n                    yield from self.write(data)\n                    yield from self.drain()\n\n        except FileNotFoundError:\n            raise aiohttp.web.HTTPNotFound()\n        except PermissionError:\n            raise aiohttp.web.HTTPForbidden()", "response": "Return a file as a response"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrestore the snapshot from disk", "response": "def restore(self):\n        \"\"\"\n        Restore the snapshot\n        \"\"\"\n\n        yield from self._project.delete_on_computes()\n        # We don't send close notif to clients because the close / open dance is purely internal\n        yield from self._project.close(ignore_notification=True)\n        self._project.controller.notification.emit(\"snapshot.restored\", self.__json__())\n        try:\n            if os.path.exists(os.path.join(self._project.path, \"project-files\")):\n                shutil.rmtree(os.path.join(self._project.path, \"project-files\"))\n            with open(self._path, \"rb\") as f:\n                project = yield from import_project(self._project.controller, self._project.id, f, location=self._project.path)\n        except (OSError, PermissionError) as e:\n            raise aiohttp.web.HTTPConflict(text=str(e))\n        yield from project.open()\n        return project"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_resource(resource_name):\n\n    resource_path = None\n    if hasattr(sys, \"frozen\"):\n        resource_path = os.path.normpath(os.path.join(os.path.dirname(sys.executable), resource_name))\n    elif not hasattr(sys, \"frozen\") and pkg_resources.resource_exists(\"gns3server\", resource_name):\n        resource_path = pkg_resources.resource_filename(\"gns3server\", resource_name)\n        resource_path = os.path.normpath(resource_path)\n    return resource_path", "response": "Return a resource in current directory or frozen package\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _nvram_changed(self, path):\n        log.debug(\"NVRAM changed: {}\".format(path))\n        self.save_configs()\n        self.updated()", "response": "Called when the NVRAM file has changed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n\n        if not (yield from super().close()):\n            return False\n\n        adapters = self._ethernet_adapters + self._serial_adapters\n        for adapter in adapters:\n            if adapter is not None:\n                for nio in adapter.ports.values():\n                    if nio and isinstance(nio, NIOUDP):\n                        self.manager.port_manager.release_udp_port(nio.lport, self._project)\n\n        yield from self.stop()", "response": "Closes this IOU VM."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef path(self, path):\n\n        self._path = self.manager.get_abs_image_path(path)\n        log.info('IOU \"{name}\" [{id}]: IOU image updated to \"{path}\"'.format(name=self._name, id=self._id, path=self._path))", "response": "Sets the path of the IOU image executable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef use_default_iou_values(self, state):\n\n        self._use_default_iou_values = state\n        if state:\n            log.info('IOU \"{name}\" [{id}]: uses the default IOU image values'.format(name=self._name, id=self._id))\n        else:\n            log.info('IOU \"{name}\" [{id}]: does not use the default IOU image values'.format(name=self._name, id=self._id))", "response": "Sets if this device uses the default IOU image values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate RAM and NVRAM values for the IOU image.", "response": "def update_default_iou_values(self):\n        \"\"\"\n        Finds the default RAM and NVRAM values for the IOU image.\n        \"\"\"\n\n        try:\n            output = yield from gns3server.utils.asyncio.subprocess_check_output(self._path, \"-h\", cwd=self.working_dir, stderr=True)\n            match = re.search(\"-n <n>\\s+Size of nvram in Kb \\(default ([0-9]+)KB\\)\", output)\n            if match:\n                self.nvram = int(match.group(1))\n            match = re.search(\"-m <n>\\s+Megabytes of router memory \\(default ([0-9]+)MB\\)\", output)\n            if match:\n                self.ram = int(match.group(1))\n        except (ValueError, OSError, subprocess.SubprocessError) as e:\n            log.warning(\"could not find default RAM and NVRAM values for {}: {}\".format(os.path.basename(self._path), e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks the IOU image and ELF file for requirements.", "response": "def _check_requirements(self):\n        \"\"\"\n        Checks the IOU image.\n        \"\"\"\n\n        if not self._path:\n            raise IOUError(\"IOU image is not configured\")\n        if not os.path.isfile(self._path) or not os.path.exists(self._path):\n            if os.path.islink(self._path):\n                raise IOUError(\"IOU image '{}' linked to '{}' is not accessible\".format(self._path, os.path.realpath(self._path)))\n            else:\n                raise IOUError(\"IOU image '{}' is not accessible\".format(self._path))\n\n        try:\n            with open(self._path, \"rb\") as f:\n                # read the first 7 bytes of the file.\n                elf_header_start = f.read(7)\n        except OSError as e:\n            raise IOUError(\"Cannot read ELF header for IOU image '{}': {}\".format(self._path, e))\n\n        # IOU images must start with the ELF magic number, be 32-bit or 64-bit, little endian\n        # and have an ELF version of 1 normal IOS image are big endian!\n        if elf_header_start != b'\\x7fELF\\x01\\x01\\x01' and elf_header_start != b'\\x7fELF\\x02\\x01\\x01':\n            raise IOUError(\"'{}' is not a valid IOU image\".format(self._path))\n\n        if not os.access(self._path, os.X_OK):\n            raise IOUError(\"IOU image '{}' is not executable\".format(self._path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iourc_path(self):\n\n        iourc_path = self._config().get(\"iourc_path\")\n        if not iourc_path:\n            # look for the iourc file in the temporary  dir.\n            path = os.path.join(self.temporary_directory, \"iourc\")\n            if os.path.exists(path):\n                return path\n            # look for the iourc file in the user home dir.\n            path = os.path.join(os.path.expanduser(\"~/\"), \".iourc\")\n            if os.path.exists(path):\n                return path\n            # look for the iourc file in the current working dir.\n            path = os.path.join(self.working_dir, \"iourc\")\n            if os.path.exists(path):\n                return path\n        return iourc_path", "response": "Returns the IOURC file path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the name of this IOU VM.", "response": "def name(self, new_name):\n        \"\"\"\n        Sets the name of this IOU VM.\n\n        :param new_name: name\n        \"\"\"\n\n        if self.startup_config_file:\n            content = self.startup_config_content\n            content = re.sub(r\"^hostname .+$\", \"hostname \" + new_name, content, flags=re.MULTILINE)\n            self.startup_config_content = content\n\n        super(IOUVM, IOUVM).name.__set__(self, new_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _library_check(self):\n\n        try:\n            output = yield from gns3server.utils.asyncio.subprocess_check_output(\"ldd\", self._path)\n        except (FileNotFoundError, subprocess.SubprocessError) as e:\n            log.warn(\"Could not determine the shared library dependencies for {}: {}\".format(self._path, e))\n            return\n\n        p = re.compile(\"([\\.\\w]+)\\s=>\\s+not found\")\n        missing_libs = p.findall(output)\n        if missing_libs:\n            raise IOUError(\"The following shared library dependencies cannot be found for IOU image {}: {}\".format(self._path,\n                                                                                                                   \", \".join(missing_libs)))", "response": "Checks for missing shared library dependencies in the IOU image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a valid IOU key in the iourc file is valid and if not raises an exception.", "response": "def _check_iou_licence(self):\n        \"\"\"\n        Checks for a valid IOU key in the iourc file (paranoid mode).\n        \"\"\"\n        try:\n            license_check = self._config().getboolean(\"license_check\", True)\n        except ValueError:\n            raise IOUError(\"Invalid licence check setting\")\n        if license_check is False:\n            return\n\n        config = configparser.ConfigParser()\n        try:\n            with open(self.iourc_path, encoding=\"utf-8\") as f:\n                config.read_file(f)\n        except OSError as e:\n            raise IOUError(\"Could not open iourc file {}: {}\".format(self.iourc_path, e))\n        except configparser.Error as e:\n            raise IOUError(\"Could not parse iourc file {}: {}\".format(self.iourc_path, e))\n        except UnicodeDecodeError as e:\n            raise IOUError(\"Non ascii characters in iourc file {}, please remove them: {}\".format(self.iourc_path, e))\n        if \"license\" not in config:\n            raise IOUError(\"License section not found in iourc file {}\".format(self.iourc_path))\n        hostname = socket.gethostname()\n        if len(hostname) > 15:\n            log.warning(\"Older IOU images may not boot because hostname '{}' length is above 15 characters\".format(hostname))\n        if hostname not in config[\"license\"]:\n            raise IOUError(\"Hostname \\\"{}\\\" not found in iourc file {}\".format(hostname, self.iourc_path))\n        user_ioukey = config[\"license\"][hostname]\n        if user_ioukey[-1:] != ';':\n            raise IOUError(\"IOU key not ending with ; in iourc file {}\".format(self.iourc_path))\n        if len(user_ioukey) != 17:\n            raise IOUError(\"IOU key length is not 16 characters in iourc file {}\".format(self.iourc_path))\n        user_ioukey = user_ioukey[:16]\n\n        # We can't test this because it's mean distributing a valid licence key\n        # in tests or generating one\n        if not hasattr(sys, \"_called_from_test\"):\n            try:\n                hostid = (yield from gns3server.utils.asyncio.subprocess_check_output(\"hostid\")).strip()\n            except FileNotFoundError as e:\n                raise IOUError(\"Could not find hostid: {}\".format(e))\n            except subprocess.SubprocessError as e:\n                raise IOUError(\"Could not execute hostid: {}\".format(e))\n\n            try:\n                ioukey = int(hostid, 16)\n            except ValueError:\n                raise IOUError(\"Invalid hostid detected: {}\".format(hostid))\n            for x in hostname:\n                ioukey += ord(x)\n            pad1 = b'\\x4B\\x58\\x21\\x81\\x56\\x7B\\x0D\\xF3\\x21\\x43\\x9B\\x7E\\xAC\\x1D\\xE6\\x8A'\n            pad2 = b'\\x80' + 39 * b'\\0'\n            ioukey = hashlib.md5(pad1 + pad2 + struct.pack('!I', ioukey) + pad1).hexdigest()[:16]\n            if ioukey != user_ioukey:\n                raise IOUError(\"Invalid IOU license key {} detected in iourc file {} for host {}\".format(user_ioukey,\n                                                                                                         self.iourc_path,\n                                                                                                         hostname))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _nvram_file(self):\n        return os.path.join(self.working_dir, \"nvram_{:05d}\".format(self.application_id))", "response": "Return the path to the nvram file for this application."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npushes the startup - config and private - config content to the NVRAM.", "response": "def _push_configs_to_nvram(self):\n        \"\"\"\n        Push the startup-config and private-config content to the NVRAM.\n        \"\"\"\n\n        startup_config_content = self.startup_config_content\n        if startup_config_content:\n            nvram_file = self._nvram_file()\n            try:\n                if not os.path.exists(nvram_file):\n                    open(nvram_file, \"a\").close()\n                    nvram_content = None\n                else:\n                    with open(nvram_file, \"rb\") as file:\n                        nvram_content = file.read()\n            except OSError as e:\n                raise IOUError(\"Cannot read nvram file {}: {}\".format(nvram_file, e))\n\n            startup_config_content = startup_config_content.encode(\"utf-8\")\n            private_config_content = self.private_config_content\n            if private_config_content is not None:\n                private_config_content = private_config_content.encode(\"utf-8\")\n            try:\n                nvram_content = nvram_import(nvram_content, startup_config_content, private_config_content, self.nvram)\n            except ValueError as e:\n                raise IOUError(\"Cannot push configs to nvram {}: {}\".format(nvram_file, e))\n            try:\n                with open(nvram_file, \"wb\") as file:\n                    file.write(nvram_content)\n            except OSError as e:\n                raise IOUError(\"Cannot write nvram file {}: {}\".format(nvram_file, e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the IOU process.", "response": "def start(self):\n        \"\"\"\n        Starts the IOU process.\n        \"\"\"\n\n        self._check_requirements()\n        if not self.is_running():\n\n            yield from self._library_check()\n\n            try:\n                self._rename_nvram_file()\n            except OSError as e:\n                raise IOUError(\"Could not rename nvram files: {}\".format(e))\n\n            iourc_path = self.iourc_path\n            if not iourc_path:\n                raise IOUError(\"Could not find an iourc file (IOU license)\")\n            if not os.path.isfile(iourc_path):\n                raise IOUError(\"The iourc path '{}' is not a regular file\".format(iourc_path))\n\n            yield from self._check_iou_licence()\n            yield from self._start_ubridge()\n\n            self._create_netmap_config()\n            if self.use_default_iou_values:\n                # make sure we have the default nvram amount to correctly push the configs\n                yield from self.update_default_iou_values()\n            self._push_configs_to_nvram()\n\n            # check if there is enough RAM to run\n            self.check_available_ram(self.ram)\n\n            self._nvram_watcher = FileWatcher(self._nvram_file(), self._nvram_changed, delay=2)\n\n            # created a environment variable pointing to the iourc file.\n            env = os.environ.copy()\n\n            if \"IOURC\" not in os.environ:\n                env[\"IOURC\"] = iourc_path\n            command = yield from self._build_command()\n            try:\n                log.info(\"Starting IOU: {}\".format(command))\n                self.command_line = ' '.join(command)\n                self._iou_process = yield from asyncio.create_subprocess_exec(\n                    *command,\n                    stdout=asyncio.subprocess.PIPE,\n                    stdin=asyncio.subprocess.PIPE,\n                    stderr=subprocess.STDOUT,\n                    cwd=self.working_dir,\n                    env=env)\n                log.info(\"IOU instance {} started PID={}\".format(self._id, self._iou_process.pid))\n                self._started = True\n                self.status = \"started\"\n                callback = functools.partial(self._termination_callback, \"IOU\")\n                gns3server.utils.asyncio.monitor_process(self._iou_process, callback)\n            except FileNotFoundError as e:\n                raise IOUError(\"Could not start IOU: {}: 32-bit binary support is probably not installed\".format(e))\n            except (OSError, subprocess.SubprocessError) as e:\n                iou_stdout = self.read_iou_stdout()\n                log.error(\"Could not start IOU {}: {}\\n{}\".format(self._path, e, iou_stdout))\n                raise IOUError(\"Could not start IOU {}: {}\\n{}\".format(self._path, e, iou_stdout))\n\n            server = AsyncioTelnetServer(reader=self._iou_process.stdout, writer=self._iou_process.stdin, binary=True, echo=True)\n            self._telnet_server = yield from asyncio.start_server(server.run, self._manager.port_manager.console_host, self.console)\n\n            # configure networking support\n            yield from self._networking()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _networking(self):\n\n        bridge_name = \"IOL-BRIDGE-{}\".format(self.application_id + 512)\n        try:\n            # delete any previous bridge if it exists\n            yield from self._ubridge_send(\"iol_bridge delete {name}\".format(name=bridge_name))\n        except UbridgeError:\n            pass\n        yield from self._ubridge_send(\"iol_bridge create {name} {bridge_id}\".format(name=bridge_name, bridge_id=self.application_id + 512))\n\n        bay_id = 0\n        for adapter in self._adapters:\n            unit_id = 0\n            for unit in adapter.ports.keys():\n                nio = adapter.get_nio(unit)\n                if nio and isinstance(nio, NIOUDP):\n                    yield from self._ubridge_send(\"iol_bridge add_nio_udp {name} {iol_id} {bay} {unit} {lport} {rhost} {rport}\".format(name=bridge_name,\n                                                                                                                                       iol_id=self.application_id,\n                                                                                                                                       bay=bay_id,\n                                                                                                                                       unit=unit_id,\n                                                                                                                                       lport=nio.lport,\n                                                                                                                                       rhost=nio.rhost,\n                                                                                                                                       rport=nio.rport))\n                    if nio.capturing:\n                        yield from self._ubridge_send('iol_bridge start_capture {name} \"{output_file}\" {data_link_type}'.format(name=bridge_name,\n                                                                                                                                output_file=nio.pcap_output_file,\n                                                                                                                                data_link_type=re.sub(\"^DLT_\", \"\", nio.pcap_data_link_type)))\n\n                    yield from self._ubridge_apply_filters(bay_id, unit_id, nio.filters)\n                unit_id += 1\n            bay_id += 1\n\n        yield from self._ubridge_send(\"iol_bridge start {name}\".format(name=bridge_name))", "response": "Configures the IOL bridge in uBridge."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls when the process has stopped.", "response": "def _termination_callback(self, process_name, returncode):\n        \"\"\"\n        Called when the process has stopped.\n\n        :param returncode: Process returncode\n        \"\"\"\n\n        self._terminate_process_iou()\n        if returncode != 0:\n            if returncode == -11:\n                message = 'IOU VM \"{}\" process has stopped with return code: {} (segfault). This could be an issue with the IOU image, using a different image may fix this.\\n{}'.format(self.name,\n                                                                                                                                                                                         returncode,\n                                                                                                                                                                                         self.read_iou_stdout())\n            else:\n                message = 'IOU VM \"{}\" process has stopped with return code: {}\\n{}'.format(self.name, returncode, self.read_iou_stdout())\n            log.warning(message)\n            self.project.emit(\"log.error\", {\"message\": message})\n        if self._telnet_server:\n            self._telnet_server.close()\n            self._telnet_server = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _rename_nvram_file(self):\n\n        destination = self._nvram_file()\n        for file_path in glob.glob(os.path.join(glob.escape(self.working_dir), \"nvram_*\")):\n            shutil.move(file_path, destination)\n        destination = os.path.join(self.working_dir, \"vlan.dat-{:05d}\".format(self.application_id))\n        for file_path in glob.glob(os.path.join(glob.escape(self.working_dir), \"vlan.dat-*\")):\n            shutil.move(file_path, destination)", "response": "Rename the nvram and vlan. dat files with the correct IOU application identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops the IOU process.", "response": "def stop(self):\n        \"\"\"\n        Stops the IOU process.\n        \"\"\"\n\n        yield from self._stop_ubridge()\n        if self._nvram_watcher:\n            self._nvram_watcher.close()\n            self._nvram_watcher = None\n\n        if self._telnet_server:\n            self._telnet_server.close()\n            self._telnet_server = None\n\n        if self.is_running():\n            self._terminate_process_iou()\n            if self._iou_process.returncode is None:\n                try:\n                    yield from gns3server.utils.asyncio.wait_for_process_termination(self._iou_process, timeout=3)\n                except asyncio.TimeoutError:\n                    if self._iou_process.returncode is None:\n                        log.warning(\"IOU process {} is still running... killing it\".format(self._iou_process.pid))\n                        try:\n                            self._iou_process.kill()\n                        except ProcessLookupError:\n                            pass\n            self._iou_process = None\n\n        self._started = False\n        self.save_configs()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _terminate_process_iou(self):\n\n        if self._iou_process:\n            log.info('Stopping IOU process for IOU VM \"{}\" PID={}'.format(self.name, self._iou_process.pid))\n            try:\n                self._iou_process.terminate()\n            # Sometime the process can already be dead when we garbage collect\n            except ProcessLookupError:\n                pass\n        self._started = False\n        self.status = \"stopped\"", "response": "Terminate the IOU process if running."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the NETMAP file.", "response": "def _create_netmap_config(self):\n        \"\"\"\n        Creates the NETMAP file.\n        \"\"\"\n\n        netmap_path = os.path.join(self.working_dir, \"NETMAP\")\n        try:\n            with open(netmap_path, \"w\", encoding=\"utf-8\") as f:\n                for bay in range(0, 16):\n                    for unit in range(0, 4):\n                        f.write(\"{ubridge_id}:{bay}/{unit}{iou_id:>5d}:{bay}/{unit}\\n\".format(ubridge_id=str(self.application_id + 512),\n                                                                                              bay=bay,\n                                                                                              unit=unit,\n                                                                                              iou_id=self.application_id))\n            log.info(\"IOU {name} [id={id}]: NETMAP file created\".format(name=self._name,\n                                                                        id=self._id))\n        except OSError as e:\n            raise IOUError(\"Could not create {}: {}\".format(netmap_path, e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild IOU command line for the current IOS instance.", "response": "def _build_command(self):\n        \"\"\"\n        Command to start the IOU process.\n        (to be passed to subprocess.Popen())\n\n        IOU command line:\n        Usage: <image> [options] <application id>\n        <image>: unix-js-m | unix-is-m | unix-i-m | ...\n        <application id>: instance identifier (0 < id <= 1024)\n        Options:\n        -e <n>        Number of Ethernet interfaces (default 2)\n        -s <n>        Number of Serial interfaces (default 2)\n        -n <n>        Size of nvram in Kb (default 64KB)\n        -b <string>   IOS debug string\n        -c <name>     Configuration file name\n        -d            Generate debug information\n        -t            Netio message trace\n        -q            Suppress informational messages\n        -h            Display this help\n        -C            Turn off use of host clock\n        -m <n>        Megabytes of router memory (default 256MB)\n        -L            Disable local console, use remote console\n        -l            Enable Layer 1 keepalive messages\n        -u <n>        UDP port base for distributed networks\n        -R            Ignore options from the IOURC file\n        -U            Disable unix: file system location\n        -W            Disable watchdog timer\n        -N            Ignore the NETMAP file\n        \"\"\"\n\n        command = [self._path]\n        if len(self._ethernet_adapters) != 2:\n            command.extend([\"-e\", str(len(self._ethernet_adapters))])\n        if len(self._serial_adapters) != 2:\n            command.extend([\"-s\", str(len(self._serial_adapters))])\n        if not self.use_default_iou_values:\n            command.extend([\"-n\", str(self._nvram)])\n            command.extend([\"-m\", str(self._ram)])\n\n        # do not let IOU create the NVRAM anymore\n        #startup_config_file = self.startup_config_file\n        # if startup_config_file:\n        #    command.extend([\"-c\", os.path.basename(startup_config_file)])\n\n        if self._l1_keepalives:\n            yield from self._enable_l1_keepalives(command)\n        command.extend([str(self.application_id)])\n        return command"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the standard output of the IOU process.", "response": "def read_iou_stdout(self):\n        \"\"\"\n        Reads the standard output of the IOU process.\n        Only use when the process has been stopped or has crashed.\n        \"\"\"\n\n        output = \"\"\n        if self._iou_stdout_file:\n            try:\n                with open(self._iou_stdout_file, \"rb\") as file:\n                    output = file.read().decode(\"utf-8\", errors=\"replace\")\n            except OSError as e:\n                log.warn(\"could not read {}: {}\".format(self._iou_stdout_file, e))\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ethernet_adapters(self, ethernet_adapters):\n\n        self._ethernet_adapters.clear()\n        for _ in range(0, ethernet_adapters):\n            self._ethernet_adapters.append(EthernetAdapter(interfaces=4))\n\n        log.info('IOU \"{name}\" [{id}]: number of Ethernet adapters changed to {adapters}'.format(name=self._name,\n                                                                                                 id=self._id,\n                                                                                                 adapters=len(self._ethernet_adapters)))\n\n        self._adapters = self._ethernet_adapters + self._serial_adapters", "response": "Sets the number of Ethernet adapters for this IOU VM."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef serial_adapters(self, serial_adapters):\n\n        self._serial_adapters.clear()\n        for _ in range(0, serial_adapters):\n            self._serial_adapters.append(SerialAdapter(interfaces=4))\n\n        log.info('IOU \"{name}\" [{id}]: number of Serial adapters changed to {adapters}'.format(name=self._name,\n                                                                                               id=self._id,\n                                                                                               adapters=len(self._serial_adapters)))\n\n        self._adapters = self._ethernet_adapters + self._serial_adapters", "response": "Sets the number of Serial adapters for this IOU VM."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef adapter_add_nio_binding(self, adapter_number, port_number, nio):\n\n        try:\n            adapter = self._adapters[adapter_number]\n        except IndexError:\n            raise IOUError('Adapter {adapter_number} does not exist for IOU \"{name}\"'.format(name=self._name,\n                                                                                             adapter_number=adapter_number))\n\n        if not adapter.port_exists(port_number):\n            raise IOUError(\"Port {port_number} does not exist in adapter {adapter}\".format(adapter=adapter,\n                                                                                           port_number=port_number))\n\n        adapter.add_nio(port_number, nio)\n        log.info('IOU \"{name}\" [{id}]: {nio} added to {adapter_number}/{port_number}'.format(name=self._name,\n                                                                                             id=self._id,\n                                                                                             nio=nio,\n                                                                                             adapter_number=adapter_number,\n                                                                                             port_number=port_number))\n\n        if self.ubridge:\n            bridge_name = \"IOL-BRIDGE-{}\".format(self.application_id + 512)\n            yield from self._ubridge_send(\"iol_bridge add_nio_udp {name} {iol_id} {bay} {unit} {lport} {rhost} {rport}\".format(name=bridge_name,\n                                                                                                                               iol_id=self.application_id,\n                                                                                                                               bay=adapter_number,\n                                                                                                                               unit=port_number,\n                                                                                                                               lport=nio.lport,\n                                                                                                                               rhost=nio.rhost,\n                                                                                                                               rport=nio.rport))\n            yield from self._ubridge_apply_filters(adapter_number, port_number, nio.filters)", "response": "Adds a NIO binding to an existing adapter."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates a port NIO binding.", "response": "def adapter_update_nio_binding(self, adapter_number, port_number, nio):\n        \"\"\"\n        Update a port NIO binding.\n\n        :param adapter_number: adapter number\n        :param port_number: port number\n        :param nio: NIO instance to add to the adapter\n        \"\"\"\n\n        if self.ubridge:\n            yield from self._ubridge_apply_filters(adapter_number, port_number, nio.filters)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply filters to the specified adapter and port.", "response": "def _ubridge_apply_filters(self, adapter_number, port_number, filters):\n        \"\"\"\n        Apply filter like rate limiting\n\n        :param adapter_number: adapter number\n        :param port_number: port number\n        :param filters: Array of filter dictionnary\n        \"\"\"\n        bridge_name = \"IOL-BRIDGE-{}\".format(self.application_id + 512)\n        location = '{bridge_name} {bay} {unit}'.format(\n            bridge_name=bridge_name,\n            bay=adapter_number,\n            unit=port_number)\n        yield from self._ubridge_send('iol_bridge reset_packet_filters ' + location)\n        for filter in self._build_filter_list(filters):\n            cmd = 'iol_bridge add_packet_filter {} {}'.format(\n                location,\n                filter)\n            yield from self._ubridge_send(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef adapter_remove_nio_binding(self, adapter_number, port_number):\n\n        try:\n            adapter = self._adapters[adapter_number]\n        except IndexError:\n            raise IOUError('Adapter {adapter_number} does not exist on IOU \"{name}\"'.format(name=self._name,\n                                                                                            adapter_number=adapter_number))\n\n        if not adapter.port_exists(port_number):\n            raise IOUError(\"Port {port_number} does not exist in adapter {adapter}\".format(adapter=adapter,\n                                                                                           port_number=port_number))\n\n        nio = adapter.get_nio(port_number)\n        if isinstance(nio, NIOUDP):\n            self.manager.port_manager.release_udp_port(nio.lport, self._project)\n        adapter.remove_nio(port_number)\n        log.info('IOU \"{name}\" [{id}]: {nio} removed from {adapter_number}/{port_number}'.format(name=self._name,\n                                                                                                 id=self._id,\n                                                                                                 nio=nio,\n                                                                                                 adapter_number=adapter_number,\n                                                                                                 port_number=port_number))\n\n        if self.ubridge:\n            bridge_name = \"IOL-BRIDGE-{}\".format(self.application_id + 512)\n            yield from self._ubridge_send(\"iol_bridge delete_nio_udp {name} {bay} {unit}\".format(name=bridge_name,\n                                                                                                 bay=adapter_number,\n                                                                                                 unit=port_number))\n\n        return nio", "response": "Removes an NIO binding."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef l1_keepalives(self, state):\n\n        self._l1_keepalives = state\n        if state:\n            log.info('IOU \"{name}\" [{id}]: has activated layer 1 keepalive messages'.format(name=self._name, id=self._id))\n        else:\n            log.info('IOU \"{name}\" [{id}]: has deactivated layer 1 keepalive messages'.format(name=self._name, id=self._id))", "response": "Sets or disables layer 1 keepalive messages."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling L1 keepalive messages if supported.", "response": "def _enable_l1_keepalives(self, command):\n        \"\"\"\n        Enables L1 keepalive messages if supported.\n\n        :param command: command line\n        \"\"\"\n\n        env = os.environ.copy()\n        if \"IOURC\" not in os.environ:\n            env[\"IOURC\"] = self.iourc_path\n        try:\n            output = yield from gns3server.utils.asyncio.subprocess_check_output(self._path, \"-h\", cwd=self.working_dir, env=env, stderr=True)\n            if re.search(\"-l\\s+Enable Layer 1 keepalive messages\", output):\n                command.extend([\"-l\"])\n            else:\n                raise IOUError(\"layer 1 keepalive messages are not supported by {}\".format(os.path.basename(self._path)))\n        except (OSError, subprocess.SubprocessError) as e:\n            log.warning(\"could not determine if layer 1 keepalive messages are supported by {}: {}\".format(os.path.basename(self._path), e))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef startup_config_content(self):\n\n        config_file = self.startup_config_file\n        if config_file is None:\n            return None\n\n        try:\n            with open(config_file, \"rb\") as f:\n                return f.read().decode(\"utf-8\", errors=\"replace\")\n        except OSError as e:\n            raise IOUError(\"Can't read startup-config file '{}': {}\".format(config_file, e))", "response": "Returns the content of the current startup - config file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef startup_config_content(self, startup_config):\n\n        try:\n            startup_config_path = os.path.join(self.working_dir, \"startup-config.cfg\")\n\n            if startup_config is None:\n                startup_config = ''\n\n            # We disallow erasing the startup config file\n            if len(startup_config) == 0 and os.path.exists(startup_config_path):\n                return\n\n            with open(startup_config_path, 'w+', encoding='utf-8') as f:\n                if len(startup_config) == 0:\n                    f.write('')\n                else:\n                    startup_config = startup_config.replace(\"%h\", self._name)\n                    f.write(startup_config)\n\n            vlan_file = os.path.join(self.working_dir, \"vlan.dat-{:05d}\".format(self.application_id))\n            if os.path.exists(vlan_file):\n                try:\n                    os.remove(vlan_file)\n                except OSError as e:\n                    log.error(\"Could not delete VLAN file '{}': {}\".format(vlan_file, e))\n\n        except OSError as e:\n            raise IOUError(\"Can't write startup-config file '{}': {}\".format(startup_config_path, e))", "response": "Update the startup config file with the content of the startup configuration file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef private_config_content(self):\n\n        config_file = self.private_config_file\n        if config_file is None:\n            return None\n\n        try:\n            with open(config_file, \"rb\") as f:\n                return f.read().decode(\"utf-8\", errors=\"replace\")\n        except OSError as e:\n            raise IOUError(\"Can't read private-config file '{}': {}\".format(config_file, e))", "response": "Returns the content of the current private - config file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the private - config file with the content of the private - config. cfg file.", "response": "def private_config_content(self, private_config):\n        \"\"\"\n        Update the private config\n\n        :param private_config: content of the private configuration file\n        \"\"\"\n\n        try:\n            private_config_path = os.path.join(self.working_dir, \"private-config.cfg\")\n\n            if private_config is None:\n                private_config = ''\n\n            # We disallow erasing the private config file\n            if len(private_config) == 0 and os.path.exists(private_config_path):\n                return\n\n            with open(private_config_path, 'w+', encoding='utf-8') as f:\n                if len(private_config) == 0:\n                    f.write('')\n                else:\n                    private_config = private_config.replace(\"%h\", self._name)\n                    f.write(private_config)\n        except OSError as e:\n            raise IOUError(\"Can't write private-config file '{}': {}\".format(private_config_path, e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the startup - config file for this IOU VM.", "response": "def startup_config_file(self):\n        \"\"\"\n        Returns the startup-config file for this IOU VM.\n\n        :returns: path to config file. None if the file doesn't exist\n        \"\"\"\n\n        path = os.path.join(self.working_dir, 'startup-config.cfg')\n        if os.path.exists(path):\n            return path\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef private_config_file(self):\n\n        path = os.path.join(self.working_dir, 'private-config.cfg')\n        if os.path.exists(path):\n            return path\n        else:\n            return None", "response": "Returns the private - config file for this IOU VM."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef relative_startup_config_file(self):\n\n        path = os.path.join(self.working_dir, 'startup-config.cfg')\n        if os.path.exists(path):\n            return 'startup-config.cfg'\n        else:\n            return None", "response": "Returns the startup - config file relative to the project directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef relative_private_config_file(self):\n\n        path = os.path.join(self.working_dir, 'private-config.cfg')\n        if os.path.exists(path):\n            return 'private-config.cfg'\n        else:\n            return None", "response": "Returns the private - config file relative to the project directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the startup - config and private - config from NVRAM.", "response": "def extract_configs(self):\n        \"\"\"\n        Gets the contents of the config files\n        startup-config and private-config from NVRAM.\n\n        :returns: tuple (startup-config, private-config)\n        \"\"\"\n\n        nvram_file = os.path.join(self.working_dir, \"nvram_{:05d}\".format(self.application_id))\n        if not os.path.exists(nvram_file):\n            return None, None\n        try:\n            with open(nvram_file, \"rb\") as file:\n                nvram_content = file.read()\n        except OSError as e:\n            log.warning(\"Cannot read nvram file {}: {}\".format(nvram_file, e))\n            return None, None\n\n        try:\n            startup_config_content, private_config_content = nvram_export(nvram_content)\n        except ValueError as e:\n            log.warning(\"Could not export configs from nvram file {}: {}\".format(nvram_file, e))\n            return None, None\n\n        return startup_config_content, private_config_content"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves the startup - config and private - config to files.", "response": "def save_configs(self):\n        \"\"\"\n        Saves the startup-config and private-config to files.\n        \"\"\"\n\n        if self.startup_config_content or self.private_config_content:\n            startup_config_content, private_config_content = self.extract_configs()\n            if startup_config_content:\n                config_path = os.path.join(self.working_dir, \"startup-config.cfg\")\n                try:\n                    config = startup_config_content.decode(\"utf-8\", errors=\"replace\")\n                    with open(config_path, \"wb\") as f:\n                        log.info(\"saving startup-config to {}\".format(config_path))\n                        f.write(config.encode(\"utf-8\"))\n                except (binascii.Error, OSError) as e:\n                    raise IOUError(\"Could not save the startup configuration {}: {}\".format(config_path, e))\n\n            if private_config_content and private_config_content != b'\\nend\\n':\n                config_path = os.path.join(self.working_dir, \"private-config.cfg\")\n                try:\n                    config = private_config_content.decode(\"utf-8\", errors=\"replace\")\n                    with open(config_path, \"wb\") as f:\n                        log.info(\"saving private-config to {}\".format(config_path))\n                        f.write(config.encode(\"utf-8\"))\n                except (binascii.Error, OSError) as e:\n                    raise IOUError(\"Could not save the private configuration {}: {}\".format(config_path, e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts a packet capture on the specified IOU.", "response": "def start_capture(self, adapter_number, port_number, output_file, data_link_type=\"DLT_EN10MB\"):\n        \"\"\"\n        Starts a packet capture.\n\n        :param adapter_number: adapter number\n        :param port_number: port number\n        :param output_file: PCAP destination file for the capture\n        :param data_link_type: PCAP data link type (DLT_*), default is DLT_EN10MB\n        \"\"\"\n\n        try:\n            adapter = self._adapters[adapter_number]\n        except IndexError:\n            raise IOUError('Adapter {adapter_number} does not exist on IOU \"{name}\"'.format(name=self._name,\n                                                                                            adapter_number=adapter_number))\n\n        if not adapter.port_exists(port_number):\n            raise IOUError(\"Port {port_number} does not exist in adapter {adapter}\".format(adapter=adapter,\n                                                                                           port_number=port_number))\n\n        nio = adapter.get_nio(port_number)\n        if not nio:\n            raise IOUError(\"NIO {port_number} does not exist in adapter {adapter}\".format(adapter=adapter,\n                                                                                          port_number=port_number))\n\n        if nio.capturing:\n            raise IOUError(\"Packet capture is already activated on {adapter_number}/{port_number}\".format(adapter_number=adapter_number,\n                                                                                                          port_number=port_number))\n\n        nio.startPacketCapture(output_file, data_link_type)\n        log.info('IOU \"{name}\" [{id}]: starting packet capture on {adapter_number}/{port_number} to {output_file}'.format(name=self._name,\n                                                                                                                          id=self._id,\n                                                                                                                          adapter_number=adapter_number,\n                                                                                                                          port_number=port_number,\n                                                                                                                          output_file=output_file))\n\n        if self.ubridge:\n            bridge_name = \"IOL-BRIDGE-{}\".format(self.application_id + 512)\n            yield from self._ubridge_send('iol_bridge start_capture {name} {bay} {unit} \"{output_file}\" {data_link_type}'.format(name=bridge_name,\n                                                                                                                                 bay=adapter_number,\n                                                                                                                                 unit=port_number,\n                                                                                                                                 output_file=output_file,\n                                                                                                                                 data_link_type=re.sub(\"^DLT_\", \"\", data_link_type)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stop_capture(self, adapter_number, port_number):\n\n        try:\n            adapter = self._adapters[adapter_number]\n        except IndexError:\n            raise IOUError('Adapter {adapter_number} does not exist on IOU \"{name}\"'.format(name=self._name,\n                                                                                            adapter_number=adapter_number))\n\n        if not adapter.port_exists(port_number):\n            raise IOUError(\"Port {port_number} does not exist in adapter {adapter}\".format(adapter=adapter,\n                                                                                           port_number=port_number))\n\n        nio = adapter.get_nio(port_number)\n        if not nio:\n            raise IOUError(\"NIO {port_number} does not exist in adapter {adapter}\".format(adapter=adapter,\n                                                                                          port_number=port_number))\n\n        nio.stopPacketCapture()\n        log.info('IOU \"{name}\" [{id}]: stopping packet capture on {adapter_number}/{port_number}'.format(name=self._name,\n                                                                                                         id=self._id,\n                                                                                                         adapter_number=adapter_number,\n                                                                                                         port_number=port_number))\n        if self.ubridge:\n            bridge_name = \"IOL-BRIDGE-{}\".format(self.application_id + 512)\n            yield from self._ubridge_send('iol_bridge stop_capture {name} {bay} {unit}'.format(name=bridge_name,\n                                                                                               bay=adapter_number,\n                                                                                               unit=port_number))", "response": "Stops a packet capture."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset SVG field value.", "response": "def svg(self, value):\n        \"\"\"\n        Set SVG field value.\n\n        If the svg has embed base64 element we will extract them\n        to disk in order to avoid duplication of content\n        \"\"\"\n\n        if len(value) < 500:\n            self._svg = value\n            return\n\n        try:\n            root = ET.fromstring(value)\n        except ET.ParseError as e:\n            log.error(\"Can't parse SVG: {}\".format(e))\n            return\n        # SVG is the default namespace no need to prefix it\n        ET.register_namespace('xmlns', \"http://www.w3.org/2000/svg\")\n        ET.register_namespace('xmlns:xlink', \"http://www.w3.org/1999/xlink\")\n\n        if len(root.findall(\"{http://www.w3.org/2000/svg}image\")) == 1:\n            href = \"{http://www.w3.org/1999/xlink}href\"\n            elem = root.find(\"{http://www.w3.org/2000/svg}image\")\n            if elem.get(href, \"\").startswith(\"data:image/\"):\n                changed = True\n                data = elem.get(href, \"\")\n                extension = re.sub(r\"[^a-z0-9]\", \"\", data.split(\";\")[0].split(\"/\")[1].lower())\n\n                data = base64.decodebytes(data.split(\",\", 1)[1].encode())\n\n                # We compute an hash of the image file to avoid duplication\n                filename = hashlib.md5(data).hexdigest() + \".\" + extension\n                elem.set(href, filename)\n\n                file_path = os.path.join(self._project.pictures_directory, filename)\n                if not os.path.exists(file_path):\n                    with open(file_path, \"wb\") as f:\n                        f.write(data)\n                value = filename\n\n        # We dump also large svg on disk to keep .gns3 small\n        if len(value) > 1000:\n            filename = hashlib.md5(value.encode()).hexdigest() + \".svg\"\n            file_path = os.path.join(self._project.pictures_directory, filename)\n            if not os.path.exists(file_path):\n                with open(file_path, \"w+\", encoding=\"utf-8\") as f:\n                    f.write(value)\n            self._svg = filename\n        else:\n            self._svg = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, **kwargs):\n\n        # Update node properties with additional elements\n        svg_changed = False\n        for prop in kwargs:\n            if prop == \"drawing_id\":\n                pass  # No good reason to change a drawing_id\n            elif getattr(self, prop) != kwargs[prop]:\n                if prop == \"svg\":\n                    # To avoid spamming client with large data we don't send the svg if the SVG didn't change\n                    svg_changed = True\n                setattr(self, prop, kwargs[prop])\n        data = self.__json__()\n        if not svg_changed:\n            del data[\"svg\"]\n        self._project.controller.notification.emit(\"drawing.updated\", data)\n        self._project.dump()", "response": "Update the drawing properties of the node with the given properties."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_node(self, *args, **kwargs):\n\n        node = yield from super().create_node(*args, **kwargs)\n        self._free_mac_ids.setdefault(node.project.id, list(range(0, 255)))\n        try:\n            self._used_mac_ids[node.id] = self._free_mac_ids[node.project.id].pop(0)\n        except IndexError:\n            raise VPCSError(\"Cannot create a new VPCS VM (limit of 255 VMs reached on this host)\")\n        return node", "response": "Creates a new VPCS VM."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close_node(self, node_id, *args, **kwargs):\n\n        node = self.get_node(node_id)\n        if node_id in self._used_mac_ids:\n            i = self._used_mac_ids[node_id]\n            self._free_mac_ids[node.project.id].insert(0, i)\n            del self._used_mac_ids[node_id]\n        yield from super().close_node(node_id, *args, **kwargs)\n        return node", "response": "Closes a VPCS VM."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrename this NIO object.", "response": "def rename(self, new_name):\n        \"\"\"\n        Renames this NIO\n\n        :param new_name: new NIO name\n        \"\"\"\n\n        yield from self._hypervisor.send(\"nio rename {name} {new_name}\".format(name=self._name, new_name=new_name))\n\n        log.info(\"NIO {name} renamed to {new_name}\".format(name=self._name, new_name=new_name))\n        self._name = new_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef debug(self, debug):\n\n        yield from self._hypervisor.send(\"nio set_debug {name} {debug}\".format(name=self._name, debug=debug))", "response": "Enables or disables debugging for this NIO."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bind_filter(self, direction, filter_name):\n\n        if direction not in self._dynamips_direction:\n            raise DynamipsError(\"Unknown direction {} to bind filter {}:\".format(direction, filter_name))\n        dynamips_direction = self._dynamips_direction[direction]\n\n        yield from self._hypervisor.send(\"nio bind_filter {name} {direction} {filter}\".format(name=self._name,\n                                                                                              direction=dynamips_direction,\n                                                                                              filter=filter_name))\n\n        if direction == \"in\":\n            self._input_filter = filter_name\n        elif direction == \"out\":\n            self._output_filter = filter_name\n        elif direction == \"both\":\n            self._input_filter = filter_name\n            self._output_filter = filter_name", "response": "Binds a packet filter to this NIO."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove packet filter for this NIO.", "response": "def unbind_filter(self, direction):\n        \"\"\"\n        Removes packet filter for this NIO.\n\n        :param direction: \"in\", \"out\" or \"both\"\n        \"\"\"\n\n        if direction not in self._dynamips_direction:\n            raise DynamipsError(\"Unknown direction {} to unbind filter:\".format(direction))\n        dynamips_direction = self._dynamips_direction[direction]\n\n        yield from self._hypervisor.send(\"nio unbind_filter {name} {direction}\".format(name=self._name,\n                                                                                       direction=dynamips_direction))\n\n        if direction == \"in\":\n            self._input_filter = None\n        elif direction == \"out\":\n            self._output_filter = None\n        elif direction == \"both\":\n            self._input_filter = None\n            self._output_filter = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_filter(self, direction, options):\n\n        if direction not in self._dynamips_direction:\n            raise DynamipsError(\"Unknown direction {} to setup filter:\".format(direction))\n        dynamips_direction = self._dynamips_direction[direction]\n\n        yield from self._hypervisor.send(\"nio setup_filter {name} {direction} {options}\".format(name=self._name,\n                                                                                                direction=dynamips_direction,\n                                                                                                options=options))\n\n        if direction == \"in\":\n            self._input_filter_options = options\n        elif direction == \"out\":\n            self._output_filter_options = options\n        elif direction == \"both\":\n            self._input_filter_options = options\n            self._output_filter_options = options", "response": "Sets up a packet filter bound with this NIO."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_stats(self):\n\n        stats = yield from self._hypervisor.send(\"nio get_stats {}\".format(self._name))\n        return stats[0]", "response": "Retrieves statistics for this NIO."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the bandwidth constraint.", "response": "def set_bandwidth(self, bandwidth):\n        \"\"\"\n        Sets bandwidth constraint.\n\n        :param bandwidth: bandwidth integer value (in Kb/s)\n        \"\"\"\n\n        yield from self._hypervisor.send(\"nio set_bandwidth {name} {bandwidth}\".format(name=self._name, bandwidth=bandwidth))\n        self._bandwidth = bandwidth"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self):\n        data = self._node_data()\n        data[\"node_id\"] = self._id\n        if self._node_type == \"docker\":\n            timeout = None\n        else:\n            timeout = 1200\n        trial = 0\n        while trial != 6:\n            try:\n                response = yield from self._compute.post(\"/projects/{}/{}/nodes\".format(self._project.id, self._node_type), data=data, timeout=timeout)\n            except ComputeConflict as e:\n                if e.response.get(\"exception\") == \"ImageMissingError\":\n                    res = yield from self._upload_missing_image(self._node_type, e.response[\"image\"])\n                    if not res:\n                        raise e\n                else:\n                    raise e\n            else:\n                yield from self.parse_node_response(response.json)\n                return True\n            trial += 1", "response": "Create the node on the compute server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, **kwargs):\n\n        # When updating properties used only on controller we don't need to call the compute\n        update_compute = False\n\n        old_json = self.__json__()\n\n        compute_properties = None\n        # Update node properties with additional elements\n        for prop in kwargs:\n            if getattr(self, prop) != kwargs[prop]:\n                if prop not in self.CONTROLLER_ONLY_PROPERTIES:\n                    update_compute = True\n\n                # We update properties on the compute and wait for the anwser from the compute node\n                if prop == \"properties\":\n                    compute_properties = kwargs[prop]\n                else:\n                    setattr(self, prop, kwargs[prop])\n\n        self._list_ports()\n        # We send notif only if object has changed\n        if old_json != self.__json__():\n            self.project.controller.notification.emit(\"node.updated\", self.__json__())\n        if update_compute:\n            data = self._node_data(properties=compute_properties)\n            response = yield from self.put(None, data=data)\n            yield from self.parse_node_response(response.json)\n        self.project.dump()", "response": "Update the node properties on the compute server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the response from a node update the object with the remote node object", "response": "def parse_node_response(self, response):\n        \"\"\"\n        Update the object with the remote node object\n        \"\"\"\n        for key, value in response.items():\n            if key == \"console\":\n                self._console = value\n            elif key == \"node_directory\":\n                self._node_directory = value\n            elif key == \"command_line\":\n                self._command_line = value\n            elif key == \"status\":\n                self._status = value\n            elif key == \"console_type\":\n                self._console_type = value\n            elif key == \"name\":\n                self.name = value\n            elif key in [\"node_id\", \"project_id\", \"console_host\",\n                         \"startup_config_content\",\n                         \"private_config_content\",\n                         \"startup_script\"]:\n                if key in self._properties:\n                    del self._properties[key]\n            else:\n                self._properties[key] = value\n        self._list_ports()\n        for link in self._links:\n            yield from link.node_updated(self)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _node_data(self, properties=None):\n        if properties:\n            data = copy.copy(properties)\n        else:\n            data = copy.copy(self._properties)\n            # We replace the startup script name by the content of the file\n            mapping = {\n                \"base_script_file\": \"startup_script\",\n                \"startup_config\": \"startup_config_content\",\n                \"private_config\": \"private_config_content\",\n            }\n            for k, v in mapping.items():\n                if k in list(self._properties.keys()):\n                    data[v] = self._base_config_file_content(self._properties[k])\n                    del data[k]\n                    del self._properties[k]  # We send the file only one time\n        data[\"name\"] = self._name\n        if self._console:\n            # console is optional for builtin nodes\n            data[\"console\"] = self._console\n        if self._console_type:\n            data[\"console_type\"] = self._console_type\n\n        # None properties are not be send. Because it can mean the emulator doesn't support it\n        for key in list(data.keys()):\n            if data[key] is None or data[key] is {} or key in self.CONTROLLER_ONLY_PROPERTIES:\n                del data[key]\n        return data", "response": "Prepare the node data to send to the remote controller."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreloads the node s attributes.", "response": "def reload(self):\n        \"\"\"\n        Suspend a node\n        \"\"\"\n        try:\n            yield from self.post(\"/reload\", timeout=240)\n        except asyncio.TimeoutError:\n            raise aiohttp.web.HTTPRequestTimeout(text=\"Timeout when reloading {}\".format(self._name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupload an image to remote compute", "response": "def _upload_missing_image(self, type, img):\n        \"\"\"\n        Search an image on local computer and upload it to remote compute\n        if the image exists\n        \"\"\"\n        for directory in images_directories(type):\n            image = os.path.join(directory, img)\n            if os.path.exists(image):\n                self.project.controller.notification.emit(\"log.info\", {\"message\": \"Uploading missing image {}\".format(img)})\n                try:\n                    with open(image, 'rb') as f:\n                        yield from self._compute.post(\"/{}/images/{}\".format(self._node_type, os.path.basename(img)), data=f, timeout=None)\n                except OSError as e:\n                    raise aiohttp.web.HTTPConflict(text=\"Can't upload {}: {}\".format(image, str(e)))\n                self.project.controller.notification.emit(\"log.info\", {\"message\": \"Upload finished for {}\".format(img)})\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the idle PC for a dynamips node", "response": "def dynamips_auto_idlepc(self):\n        \"\"\"\n        Compute the idle PC for a dynamips node\n        \"\"\"\n        return (yield from self._compute.get(\"/projects/{}/{}/nodes/{}/auto_idlepc\".format(self._project.id, self._node_type, self._id), timeout=240)).json"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_port(self, adapter_number, port_number):\n        for port in self.ports:\n            if port.adapter_number == adapter_number and port.port_number == port_number:\n                return port\n        return None", "response": "Returns the port for this adapter_number and port_number."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate the list of ports in the client.", "response": "def _list_ports(self):\n        \"\"\"\n        Generate the list of port display in the client\n        if the compute has sent a list we return it (use by\n        node where you can not personnalize the port naming).\n        \"\"\"\n        self._ports = []\n        # Some special cases\n        if self._node_type == \"atm_switch\":\n            atm_port = set()\n            # Mapping is like {\"1:0:100\": \"10:0:200\"}\n            for source, dest in self._properties[\"mappings\"].items():\n                atm_port.add(int(source.split(\":\")[0]))\n                atm_port.add(int(dest.split(\":\")[0]))\n            atm_port = sorted(atm_port)\n            for port in atm_port:\n                self._ports.append(PortFactory(\"{}\".format(port), 0, 0, port, \"atm\"))\n            return\n\n        elif self._node_type == \"frame_relay_switch\":\n            frame_relay_port = set()\n            # Mapping is like {\"1:101\": \"10:202\"}\n            for source, dest in self._properties[\"mappings\"].items():\n                frame_relay_port.add(int(source.split(\":\")[0]))\n                frame_relay_port.add(int(dest.split(\":\")[0]))\n            frame_relay_port = sorted(frame_relay_port)\n            for port in frame_relay_port:\n                self._ports.append(PortFactory(\"{}\".format(port), 0, 0, port, \"frame_relay\"))\n            return\n        elif self._node_type == \"dynamips\":\n            self._ports = DynamipsPortFactory(self._properties)\n            return\n        elif self._node_type == \"docker\":\n            for adapter_number in range(0, self._properties[\"adapters\"]):\n                self._ports.append(PortFactory(\"eth{}\".format(adapter_number), 0, adapter_number, 0, \"ethernet\", short_name=\"eth{}\".format(adapter_number)))\n        elif self._node_type in (\"ethernet_switch\", \"ethernet_hub\"):\n            # Basic node we don't want to have adapter number\n            port_number = 0\n            for port in self._properties[\"ports_mapping\"]:\n                self._ports.append(PortFactory(port[\"name\"], 0, 0, port_number, \"ethernet\", short_name=\"e{}\".format(port_number)))\n                port_number += 1\n        elif self._node_type in (\"vpcs\"):\n            self._ports.append(PortFactory(\"Ethernet0\", 0, 0, 0, \"ethernet\", short_name=\"e0\"))\n        elif self._node_type in (\"cloud\", \"nat\"):\n            port_number = 0\n            for port in self._properties[\"ports_mapping\"]:\n                self._ports.append(PortFactory(port[\"name\"], 0, 0, port_number, \"ethernet\", short_name=port[\"name\"]))\n                port_number += 1\n        else:\n            self._ports = StandardPortFactory(self._properties, self._port_by_adapter, self._first_port_name, self._port_name_format, self._port_segment_size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setup_chassis(self):\n\n        # With 1751 and 1760, WICs in WIC slot 1 show up as in slot 1, not 0\n        # e.g. s1/0 not s0/2\n        if self._chassis in ['1751', '1760']:\n            self._create_slots(2)\n            self._slots[1] = C1700_MB_WIC1()\n        else:\n            self._create_slots(1)\n        self._slots[0] = C1700_MB_1FE()", "response": "Sets up the router with the corresponding chassis."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_chassis(self, chassis):\n\n        yield from self._hypervisor.send('c1700 set_chassis \"{name}\" {chassis}'.format(name=self._name, chassis=chassis))\n\n        log.info('Router \"{name}\" [{id}]: chassis set to {chassis}'.format(name=self._name,\n                                                                           id=self._id,\n                                                                           chassis=chassis))\n\n        self._chassis = chassis\n        self._setup_chassis()", "response": "Sets the chassis.\n\n        :param: chassis string:\n        1720, 1721, 1750, 1751 or 1760"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a GNS3 project to a. gns3 topology.", "response": "def project_to_topology(project):\n    \"\"\"\n    :return: A dictionnary with the topology ready to dump to a .gns3\n    \"\"\"\n    data = {\n        \"project_id\": project.id,\n        \"name\": project.name,\n        \"auto_start\": project.auto_start,\n        \"auto_open\": project.auto_open,\n        \"auto_close\": project.auto_close,\n        \"scene_width\": project.scene_width,\n        \"scene_height\": project.scene_height,\n        \"zoom\": project.zoom,\n        \"show_layers\": project.show_layers,\n        \"snap_to_grid\": project.snap_to_grid,\n        \"show_grid\": project.show_grid,\n        \"show_interface_labels\": project.show_interface_labels,\n        \"topology\": {\n            \"nodes\": [],\n            \"links\": [],\n            \"computes\": [],\n            \"drawings\": []\n        },\n        \"type\": \"topology\",\n        \"revision\": GNS3_FILE_FORMAT_REVISION,\n        \"version\": __version__\n    }\n\n    computes = set()\n    for node in project.nodes.values():\n        computes.add(node.compute)\n        data[\"topology\"][\"nodes\"].append(node.__json__(topology_dump=True))\n    for link in project.links.values():\n        data[\"topology\"][\"links\"].append(link.__json__(topology_dump=True))\n    for drawing in project.drawings.values():\n        data[\"topology\"][\"drawings\"].append(drawing.__json__(topology_dump=True))\n    for compute in computes:\n        if hasattr(compute, \"__json__\"):\n            compute = compute.__json__(topology_dump=True)\n            if compute[\"compute_id\"] not in (\"vm\", \"local\", ):\n                data[\"topology\"][\"computes\"].append(compute)\n    _check_topology_schema(data)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_topology(path):\n    log.debug(\"Read topology %s\", path)\n    try:\n        with open(path, encoding=\"utf-8\") as f:\n            topo = json.load(f)\n    except (OSError, UnicodeDecodeError, ValueError) as e:\n        raise aiohttp.web.HTTPConflict(text=\"Could not load topology {}: {}\".format(path, str(e)))\n\n    if topo.get(\"revision\", 0) > GNS3_FILE_FORMAT_REVISION:\n        raise aiohttp.web.HTTPConflict(text=\"This project is designed for a more recent version of GNS3 please update GNS3 to version {} or later\".format(topo[\"version\"]))\n\n    changed = False\n    if \"revision\" not in topo or topo[\"revision\"] < GNS3_FILE_FORMAT_REVISION:\n        # If it's an old GNS3 file we need to convert it\n        # first we backup the file\n        try:\n            shutil.copy(path, path + \".backup{}\".format(topo.get(\"revision\", 0)))\n        except (OSError) as e:\n            raise aiohttp.web.HTTPConflict(text=\"Can't write backup of the topology {}: {}\".format(path, str(e)))\n        changed = True\n\n    if \"revision\" not in topo or topo[\"revision\"] < 5:\n        topo = _convert_1_3_later(topo, path)\n\n    # Version before GNS3 2.0 alpha 4\n    if topo[\"revision\"] < 6:\n        topo = _convert_2_0_0_alpha(topo, path)\n\n    # Version before GNS3 2.0 beta 3\n    if topo[\"revision\"] < 7:\n        topo = _convert_2_0_0_beta_2(topo, path)\n\n    # Version before GNS3 2.1\n    if topo[\"revision\"] < 8:\n        topo = _convert_2_0_0(topo, path)\n\n    try:\n        _check_topology_schema(topo)\n    except aiohttp.web.HTTPConflict as e:\n        log.error(\"Can't load the topology %s\", path)\n        raise e\n\n    if changed:\n        try:\n            with open(path, \"w+\", encoding=\"utf-8\") as f:\n                json.dump(topo, f, indent=4, sort_keys=True)\n        except (OSError) as e:\n            raise aiohttp.web.HTTPConflict(text=\"Can't write the topology {}: {}\".format(path, str(e)))\n    return topo", "response": "Load a topology file and patch it for last GNS3 release and return it"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting topologies from GNS3 2. 0. 0 to 2. 1.", "response": "def _convert_2_0_0(topo, topo_path):\n    \"\"\"\n    Convert topologies from GNS3 2.0.0 to 2.1\n\n    Changes:\n     * Remove startup_script_path from VPCS and base config file for IOU and Dynamips\n    \"\"\"\n    topo[\"revision\"] = 8\n\n    for node in topo.get(\"topology\", {}).get(\"nodes\", []):\n        if \"properties\" in node:\n            if node[\"node_type\"] == \"vpcs\":\n                if \"startup_script_path\" in node[\"properties\"]:\n                    del node[\"properties\"][\"startup_script_path\"]\n                if \"startup_script\" in node[\"properties\"]:\n                    del node[\"properties\"][\"startup_script\"]\n            elif node[\"node_type\"] == \"dynamips\" or node[\"node_type\"] == \"iou\":\n                if \"startup_config\" in node[\"properties\"]:\n                    del node[\"properties\"][\"startup_config\"]\n                if \"private_config\" in node[\"properties\"]:\n                    del node[\"properties\"][\"private_config\"]\n                if \"startup_config_content\" in node[\"properties\"]:\n                    del node[\"properties\"][\"startup_config_content\"]\n                if \"private_config_content\" in node[\"properties\"]:\n                    del node[\"properties\"][\"private_config_content\"]\n    return topo"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts topologies from GNS3 2. 0. 0 beta 2 to beta 3.", "response": "def _convert_2_0_0_beta_2(topo, topo_path):\n    \"\"\"\n    Convert topologies from GNS3 2.0.0 beta 2 to beta 3.\n\n    Changes:\n     * Node id folders for dynamips\n    \"\"\"\n    topo_dir = os.path.dirname(topo_path)\n    topo[\"revision\"] = 7\n\n    for node in topo.get(\"topology\", {}).get(\"nodes\", []):\n        if node[\"node_type\"] == \"dynamips\":\n            node_id = node[\"node_id\"]\n            dynamips_id = node[\"properties\"][\"dynamips_id\"]\n\n            dynamips_dir = os.path.join(topo_dir, \"project-files\", \"dynamips\")\n            node_dir = os.path.join(dynamips_dir, node_id)\n            try:\n                os.makedirs(os.path.join(node_dir, \"configs\"), exist_ok=True)\n                for path in glob.glob(os.path.join(glob.escape(dynamips_dir), \"*_i{}_*\".format(dynamips_id))):\n                    shutil.move(path, os.path.join(node_dir, os.path.basename(path)))\n                for path in glob.glob(os.path.join(glob.escape(dynamips_dir), \"configs\", \"i{}_*\".format(dynamips_id))):\n                    shutil.move(path, os.path.join(node_dir, \"configs\", os.path.basename(path)))\n            except OSError as e:\n                raise aiohttp.web.HTTPConflict(text=\"Can't convert project {}: {}\".format(topo_path, str(e)))\n    return topo"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts topologies from GNS3 2. 0. 0 alpha to 2. 0. 0 final.", "response": "def _convert_2_0_0_alpha(topo, topo_path):\n    \"\"\"\n    Convert topologies from GNS3 2.0.0 alpha to 2.0.0 final.\n\n    Changes:\n     * No more serial console\n     * No more option for VMware / VirtualBox remote console (always use telnet)\n    \"\"\"\n    topo[\"revision\"] = 6\n    for node in topo.get(\"topology\", {}).get(\"nodes\", []):\n        if node.get(\"console_type\") == \"serial\":\n            node[\"console_type\"] = \"telnet\"\n        if node[\"node_type\"] in (\"vmware\", \"virtualbox\"):\n            prop = node.get(\"properties\")\n            if \"enable_remote_console\" in prop:\n                del prop[\"enable_remote_console\"]\n    return topo"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert topologies from 1_3 to new format", "response": "def _convert_1_3_later(topo, topo_path):\n    \"\"\"\n    Convert topologies from 1_3 to the new file format\n\n    Look in tests/topologies/README.rst for instructions to test changes here\n    \"\"\"\n    topo_dir = os.path.dirname(topo_path)\n\n    _convert_snapshots(topo_dir)\n\n    new_topo = {\n        \"type\": \"topology\",\n        \"revision\": 5,\n        \"version\": __version__,\n        \"auto_start\": topo.get(\"auto_start\", False),\n        \"name\": topo[\"name\"],\n        \"project_id\": topo.get(\"project_id\"),\n        \"topology\": {\n            \"links\": [],\n            \"drawings\": [],\n            \"computes\": [],\n            \"nodes\": []\n        }\n    }\n    if new_topo[\"project_id\"] is None:\n        new_topo[\"project_id\"] = str(uuid.uuid4())  # Could arrive for topologues with drawing only\n    if \"topology\" not in topo:\n        return new_topo\n    topo = topo[\"topology\"]\n\n    # Create computes\n    server_id_to_compute_id = {}\n    for server in topo.get(\"servers\", []):\n        compute = {\n            \"host\": server.get(\"host\", \"localhost\"),\n            \"port\": server.get(\"port\", 3080),\n            \"protocol\": server.get(\"protocol\", \"http\")\n        }\n        if server[\"local\"]:\n            compute[\"compute_id\"] = \"local\"\n            compute[\"name\"] = \"Local\"\n        elif server.get(\"vm\", False):\n            compute[\"compute_id\"] = \"vm\"\n            compute[\"name\"] = \"GNS3 VM\"\n        else:\n            compute[\"name\"] = \"Remote {}\".format(server[\"id\"])\n            compute[\"compute_id\"] = str(uuid.uuid4())\n        server_id_to_compute_id[server[\"id\"]] = compute[\"compute_id\"]\n        new_topo[\"topology\"][\"computes\"].append(compute)\n\n    # Create nodes\n    ports = {}\n    node_id_to_node_uuid = {}\n    for old_node in topo.get(\"nodes\", []):\n        node = {}\n        node[\"console\"] = old_node[\"properties\"].get(\"console\", None)\n        try:\n            node[\"compute_id\"] = server_id_to_compute_id[old_node[\"server_id\"]]\n        except KeyError:\n            node[\"compute_id\"] = \"local\"\n        node[\"console_type\"] = old_node[\"properties\"].get(\"console_type\", \"telnet\")\n        if \"label\" in old_node:\n            node[\"name\"] = old_node[\"label\"][\"text\"]\n            node[\"label\"] = _convert_label(old_node[\"label\"])\n        else:\n            node[\"name\"] = old_node[\"properties\"][\"name\"]\n        node[\"node_id\"] = old_node.get(\"vm_id\", str(uuid.uuid4()))\n\n        node[\"symbol\"] = old_node.get(\"symbol\", None)\n        # Compatibility with <= 1.3\n        if node[\"symbol\"] is None and \"default_symbol\" in old_node:\n            if old_node[\"default_symbol\"].endswith(\"normal.svg\"):\n                node[\"symbol\"] = old_node[\"default_symbol\"][:-11] + \".svg\"\n            else:\n                node[\"symbol\"] = old_node[\"default_symbol\"]\n\n        node[\"x\"] = int(old_node[\"x\"])\n        node[\"y\"] = int(old_node[\"y\"])\n        node[\"z\"] = int(old_node.get(\"z\", 1))\n        node[\"port_name_format\"] = old_node.get(\"port_name_format\", \"Ethernet{0}\")\n        node[\"port_segment_size\"] = int(old_node.get(\"port_segment_size\", \"0\"))\n        node[\"first_port_name\"] = old_node.get(\"first_port_name\")\n\n        node[\"properties\"] = {}\n\n        # Some old dynamips node don't have type\n        if \"type\" not in old_node:\n            old_node[\"type\"] = old_node[\"properties\"][\"platform\"].upper()\n\n        if old_node[\"type\"] == \"VPCSDevice\":\n            node[\"node_type\"] = \"vpcs\"\n        elif old_node[\"type\"] == \"QemuVM\":\n            node = _convert_qemu_node(node, old_node)\n        elif old_node[\"type\"] == \"DockerVM\":\n            node[\"node_type\"] = \"docker\"\n            if node[\"symbol\"] is None:\n                node[\"symbol\"] = \":/symbols/docker_guest.svg\"\n        elif old_node[\"type\"] == \"ATMSwitch\":\n            node[\"node_type\"] = \"atm_switch\"\n            node[\"symbol\"] = \":/symbols/atm_switch.svg\"\n            node[\"console_type\"] = None\n        elif old_node[\"type\"] == \"EthernetHub\":\n            node[\"node_type\"] = \"ethernet_hub\"\n            node[\"console_type\"] = None\n            node[\"symbol\"] = \":/symbols/hub.svg\"\n            node[\"properties\"][\"ports_mapping\"] = []\n            for port in old_node.get(\"ports\", []):\n                node[\"properties\"][\"ports_mapping\"].append({\n                    \"name\": \"Ethernet{}\".format(port[\"port_number\"] - 1),\n                    \"port_number\": port[\"port_number\"] - 1\n                })\n        elif old_node[\"type\"] == \"EthernetSwitch\":\n            node[\"node_type\"] = \"ethernet_switch\"\n            node[\"symbol\"] = \":/symbols/ethernet_switch.svg\"\n            node[\"console_type\"] = None\n            node[\"properties\"][\"ports_mapping\"] = []\n            for port in old_node.get(\"ports\", []):\n                node[\"properties\"][\"ports_mapping\"].append({\n                    \"name\": \"Ethernet{}\".format(port[\"port_number\"] - 1),\n                    \"port_number\": port[\"port_number\"] - 1,\n                    \"type\": port[\"type\"],\n                    \"vlan\": port[\"vlan\"]\n                })\n        elif old_node[\"type\"] == \"FrameRelaySwitch\":\n            node[\"node_type\"] = \"frame_relay_switch\"\n            node[\"symbol\"] = \":/symbols/frame_relay_switch.svg\"\n            node[\"console_type\"] = None\n        elif old_node[\"type\"].upper() in [\"C1700\", \"C2600\", \"C2691\", \"C3600\", \"C3620\", \"C3640\", \"C3660\", \"C3725\", \"C3745\", \"C7200\", \"EtherSwitchRouter\"]:\n            if node[\"symbol\"] is None:\n                node[\"symbol\"] = \":/symbols/router.svg\"\n            node[\"node_type\"] = \"dynamips\"\n            node[\"properties\"][\"dynamips_id\"] = old_node.get(\"dynamips_id\")\n            if \"platform\" not in node[\"properties\"] and old_node[\"type\"].upper().startswith(\"C\"):\n                node[\"properties\"][\"platform\"] = old_node[\"type\"].lower()\n                if node[\"properties\"][\"platform\"].startswith(\"c36\"):\n                    node[\"properties\"][\"platform\"] = \"c3600\"\n            if \"ram\" not in node[\"properties\"] and old_node[\"type\"].startswith(\"C\"):\n                node[\"properties\"][\"ram\"] = PLATFORMS_DEFAULT_RAM[old_node[\"type\"].lower()]\n        elif old_node[\"type\"] == \"VMwareVM\":\n            node[\"node_type\"] = \"vmware\"\n            node[\"properties\"][\"linked_clone\"] = old_node.get(\"linked_clone\", False)\n            if node[\"symbol\"] is None:\n                node[\"symbol\"] = \":/symbols/vmware_guest.svg\"\n        elif old_node[\"type\"] == \"VirtualBoxVM\":\n            node[\"node_type\"] = \"virtualbox\"\n            node[\"properties\"][\"linked_clone\"] = old_node.get(\"linked_clone\", False)\n            if node[\"symbol\"] is None:\n                node[\"symbol\"] = \":/symbols/vbox_guest.svg\"\n        elif old_node[\"type\"] == \"IOUDevice\":\n            node[\"node_type\"] = \"iou\"\n            node[\"port_name_format\"] = old_node.get(\"port_name_format\", \"Ethernet{segment0}/{port0}\")\n            node[\"port_segment_size\"] = int(old_node.get(\"port_segment_size\", \"4\"))\n            if node[\"symbol\"] is None:\n                if \"l2\" in node[\"properties\"].get(\"path\", \"\"):\n                    node[\"symbol\"] = \":/symbols/multilayer_switch.svg\"\n                else:\n                    node[\"symbol\"] = \":/symbols/router.svg\"\n\n        elif old_node[\"type\"] == \"Cloud\":\n            symbol = old_node.get(\"symbol\", \":/symbols/cloud.svg\")\n            old_node[\"ports\"] = _create_cloud(node, old_node, symbol)\n        elif old_node[\"type\"] == \"Host\":\n            symbol = old_node.get(\"symbol\", \":/symbols/computer.svg\")\n            old_node[\"ports\"] = _create_cloud(node, old_node, symbol)\n        else:\n            raise NotImplementedError(\"Conversion of {} is not supported\".format(old_node[\"type\"]))\n\n        for prop in old_node.get(\"properties\", {}):\n            if prop not in [\"console\", \"name\", \"console_type\", \"console_host\", \"use_ubridge\"]:\n                node[\"properties\"][prop] = old_node[\"properties\"][prop]\n\n        node_id_to_node_uuid[old_node[\"id\"]] = node[\"node_id\"]\n        for port in old_node.get(\"ports\", []):\n            if node[\"node_type\"] in (\"ethernet_hub\", \"ethernet_switch\"):\n                port[\"port_number\"] -= 1\n            ports[port[\"id\"]] = port\n        new_topo[\"topology\"][\"nodes\"].append(node)\n\n    # Create links\n    for old_link in topo.get(\"links\", []):\n        try:\n            nodes = []\n            source_node = {\n                \"adapter_number\": ports[old_link[\"source_port_id\"]].get(\"adapter_number\", 0),\n                \"port_number\": ports[old_link[\"source_port_id\"]].get(\"port_number\", 0),\n                \"node_id\": node_id_to_node_uuid[old_link[\"source_node_id\"]]\n            }\n            nodes.append(source_node)\n\n            destination_node = {\n                \"adapter_number\": ports[old_link[\"destination_port_id\"]].get(\"adapter_number\", 0),\n                \"port_number\": ports[old_link[\"destination_port_id\"]].get(\"port_number\", 0),\n                \"node_id\": node_id_to_node_uuid[old_link[\"destination_node_id\"]]\n            }\n            nodes.append(destination_node)\n        except KeyError:\n            continue\n\n        link = {\n            \"link_id\": str(uuid.uuid4()),\n            \"nodes\": nodes\n        }\n        new_topo[\"topology\"][\"links\"].append(link)\n\n    # Ellipse\n    for ellipse in topo.get(\"ellipses\", []):\n        svg = '<svg height=\"{height}\" width=\"{width}\"><ellipse cx=\"{cx}\" cy=\"{cy}\" fill=\"{fill}\" fill-opacity=\"1.0\" rx=\"{rx}\" ry=\"{ry}\" {border_style} /></svg>'.format(\n            height=int(ellipse[\"height\"]),\n            width=int(ellipse[\"width\"]),\n            cx=int(ellipse[\"width\"] / 2),\n            cy=int(ellipse[\"height\"] / 2),\n            rx=int(ellipse[\"width\"] / 2),\n            ry=int(ellipse[\"height\"] / 2),\n            fill=ellipse.get(\"color\", \"#ffffff\"),\n            border_style=_convert_border_style(ellipse)\n        )\n        new_ellipse = {\n            \"drawing_id\": str(uuid.uuid4()),\n            \"x\": int(ellipse[\"x\"]),\n            \"y\": int(ellipse[\"y\"]),\n            \"z\": int(ellipse.get(\"z\", 0)),\n            \"rotation\": int(ellipse.get(\"rotation\", 0)),\n            \"svg\": svg\n        }\n        new_topo[\"topology\"][\"drawings\"].append(new_ellipse)\n\n    # Notes\n    for note in topo.get(\"notes\", []):\n        font_info = note.get(\"font\", \"TypeWriter,10,-1,5,75,0,0,0,0,0\").split(\",\")\n\n        if font_info[4] == \"75\":\n            weight = \"bold\"\n        else:\n            weight = \"normal\"\n        if font_info[5] == \"1\":\n            style = \"italic\"\n        else:\n            style = \"normal\"\n\n        svg = '<svg height=\"{height}\" width=\"{width}\"><text fill=\"{fill}\" fill-opacity=\"{opacity}\" font-family=\"{family}\" font-size=\"{size}\" font-weight=\"{weight}\" font-style=\"{style}\">{text}</text></svg>'.format(\n            height=int(font_info[1]) * 2,\n            width=int(font_info[1]) * len(note[\"text\"]),\n            fill=\"#\" + note.get(\"color\", \"#00000000\")[-6:],\n            opacity=round(1.0 / 255 * int(note.get(\"color\", \"#ffffffff\")[:3][-2:], base=16), 2),  # Extract the alpha channel from the hexa version\n            family=font_info[0],\n            size=int(font_info[1]),\n            weight=weight,\n            style=style,\n            text=html.escape(note[\"text\"])\n        )\n        new_note = {\n            \"drawing_id\": str(uuid.uuid4()),\n            \"x\": int(note[\"x\"]),\n            \"y\": int(note[\"y\"]),\n            \"z\": int(note.get(\"z\", 0)),\n            \"rotation\": int(note.get(\"rotation\", 0)),\n            \"svg\": svg\n        }\n        new_topo[\"topology\"][\"drawings\"].append(new_note)\n\n    # Images\n    for image in topo.get(\"images\", []):\n        img_path = image[\"path\"]\n        # Absolute image path are rewrite to project specific image\n        if os.path.abspath(img_path):\n            try:\n                os.makedirs(os.path.join(topo_dir, \"images\"), exist_ok=True)\n                shutil.copy(img_path, os.path.join(topo_dir, \"images\", os.path.basename(img_path)))\n            except OSError:\n                pass\n        new_image = {\n            \"drawing_id\": str(uuid.uuid4()),\n            \"x\": int(image[\"x\"]),\n            \"y\": int(image[\"y\"]),\n            \"z\": int(image.get(\"z\", 0)),\n            \"rotation\": int(image.get(\"rotation\", 0)),\n            \"svg\": os.path.basename(img_path)\n        }\n        new_topo[\"topology\"][\"drawings\"].append(new_image)\n\n    # Rectangles\n    for rectangle in topo.get(\"rectangles\", []):\n        svg = '<svg height=\"{height}\" width=\"{width}\"><rect fill=\"{fill}\" fill-opacity=\"1.0\" height=\"{height}\" width=\"{width}\" {border_style} /></svg>'.format(\n            height=int(rectangle[\"height\"]),\n            width=int(rectangle[\"width\"]),\n            fill=rectangle.get(\"color\", \"#ffffff\"),\n            border_style=_convert_border_style(rectangle)\n        )\n        new_rectangle = {\n            \"drawing_id\": str(uuid.uuid4()),\n            \"x\": int(rectangle[\"x\"]),\n            \"y\": int(rectangle[\"y\"]),\n            \"z\": int(rectangle.get(\"z\", 0)),\n            \"rotation\": int(rectangle.get(\"rotation\", 0)),\n            \"svg\": svg\n        }\n        new_topo[\"topology\"][\"drawings\"].append(new_rectangle)\n\n    # Convert instructions.txt to README.txt\n    instructions_path = os.path.join(topo_dir, \"instructions.txt\")\n    readme_path = os.path.join(topo_dir, \"README.txt\")\n    if os.path.exists(instructions_path) and not os.path.exists(readme_path):\n        shutil.move(instructions_path, readme_path)\n\n    return new_topo"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a label from 1. X to the new format", "response": "def _convert_label(label):\n    \"\"\"\n    Convert a label from 1.X to the new format\n    \"\"\"\n    style = qt_font_to_style(label.get(\"font\"), label.get(\"color\"))\n    return {\n        \"text\": html.escape(label[\"text\"]),\n        \"rotation\": 0,\n        \"style\": style,\n        \"x\": int(label[\"x\"]),\n        \"y\": int(label[\"y\"])\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _convert_snapshots(topo_dir):\n    old_snapshots_dir = os.path.join(topo_dir, \"project-files\", \"snapshots\")\n    if os.path.exists(old_snapshots_dir):\n        new_snapshots_dir = os.path.join(topo_dir, \"snapshots\")\n        os.makedirs(new_snapshots_dir)\n\n        for snapshot in os.listdir(old_snapshots_dir):\n            snapshot_dir = os.path.join(old_snapshots_dir, snapshot)\n            if os.path.isdir(snapshot_dir):\n                is_gns3_topo = False\n\n                # In .gns3project fileformat the .gns3 should be name project.gns3\n                for file in os.listdir(snapshot_dir):\n                    if file.endswith(\".gns3\"):\n                        shutil.move(os.path.join(snapshot_dir, file), os.path.join(snapshot_dir, \"project.gns3\"))\n                        is_gns3_topo = True\n\n                if is_gns3_topo:\n                    snapshot_arc = os.path.join(new_snapshots_dir, snapshot + \".gns3project\")\n                    with zipfile.ZipFile(snapshot_arc, 'w', allowZip64=True) as myzip:\n                        for root, dirs, files in os.walk(snapshot_dir):\n                            for file in files:\n                                myzip.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), snapshot_dir), compress_type=zipfile.ZIP_DEFLATED)\n\n        shutil.rmtree(old_snapshots_dir)", "response": "Convert 1. x snapshot to the new format"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _convert_qemu_node(node, old_node):\n\n    # In 2.0 the internet VM is replaced by the NAT node\n    if old_node.get(\"properties\", {}).get(\"hda_disk_image_md5sum\") == \"8ebc5a6ec53a1c05b7aa101b5ceefe31\":\n        node[\"console\"] = None\n        node[\"console_type\"] = None\n        node[\"node_type\"] = \"nat\"\n        del old_node[\"properties\"]\n        node[\"properties\"] = {\n            \"ports\": [\n                {\n                    \"interface\": \"eth1\",\n                    \"name\": \"nat0\",\n                    \"port_number\": 0,\n                    \"type\": \"ethernet\"\n                }\n            ]\n        }\n        if node[\"symbol\"] is None:\n            node[\"symbol\"] = \":/symbols/cloud.svg\"\n        return node\n\n    node[\"node_type\"] = \"qemu\"\n    if node[\"symbol\"] is None:\n        node[\"symbol\"] = \":/symbols/qemu_guest.svg\"\n    return node", "response": "Convert qemu node from 1. X to 2. 0 node"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse this decorator to raise an error if the project is not opened", "response": "def open_required(func):\n    \"\"\"\n    Use this decorator to raise an error if the project is not opened\n    \"\"\"\n\n    def wrapper(self, *args, **kwargs):\n        if self._status == \"closed\":\n            raise aiohttp.web.HTTPForbidden(text=\"The project is not opened\")\n        return func(self, *args, **kwargs)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, **kwargs):\n\n        old_json = self.__json__()\n\n        for prop in kwargs:\n            setattr(self, prop, kwargs[prop])\n\n        # We send notif only if object has changed\n        if old_json != self.__json__():\n            self.controller.notification.emit(\"project.updated\", self.__json__())\n            self.dump()", "response": "Update the project s properties with the given kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self):\n        self._allocated_node_names = set()\n        self._nodes = {}\n        self._links = {}\n        self._drawings = {}\n        self._snapshots = {}\n\n        # List the available snapshots\n        snapshot_dir = os.path.join(self.path, \"snapshots\")\n        if os.path.exists(snapshot_dir):\n            for snap in os.listdir(snapshot_dir):\n                if snap.endswith(\".gns3project\"):\n                    snapshot = Snapshot(self, filename=snap)\n                    self._snapshots[snapshot.id] = snapshot\n\n        # Create the project on demand on the compute node\n        self._project_created_on_compute = set()", "response": "Reset internal state of the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef captures_directory(self):\n        path = os.path.join(self._path, \"project-files\", \"captures\")\n        os.makedirs(path, exist_ok=True)\n        return path", "response": "Return the path to the captures directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove an allocated node name from the set of allocated node names.", "response": "def remove_allocated_node_name(self, name):\n        \"\"\"\n        Removes an allocated node name\n\n        :param name: allocated node name\n        \"\"\"\n\n        if name in self._allocated_node_names:\n            self._allocated_node_names.remove(name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating a node name or generates a new one if no node name is available.", "response": "def update_allocated_node_name(self, base_name):\n        \"\"\"\n        Updates a node name or generate a new if no node\n        name is available.\n\n        :param base_name: new node base name\n        \"\"\"\n\n        if base_name is None:\n            return None\n        base_name = re.sub(r\"[ ]\", \"\", base_name)\n        if base_name in self._allocated_node_names:\n            base_name = re.sub(r\"[0-9]+$\", \"{0}\", base_name)\n\n        if '{0}' in base_name or '{id}' in base_name:\n            # base name is a template, replace {0} or {id} by an unique identifier\n            for number in range(1, 1000000):\n                try:\n                    name = base_name.format(number, id=number, name=\"Node\")\n                except KeyError as e:\n                    raise aiohttp.web.HTTPConflict(text=\"{\" + e.args[0] + \"} is not a valid replacement string in the node name\")\n                except (ValueError, IndexError) as e:\n                    raise aiohttp.web.HTTPConflict(text=\"{} is not a valid replacement string in the node name\".format(base_name))\n                if name not in self._allocated_node_names:\n                    self._allocated_node_names.add(name)\n                    return name\n        else:\n            if base_name not in self._allocated_node_names:\n                self._allocated_node_names.add(base_name)\n                return base_name\n            # base name is not unique, let's find a unique name by appending a number\n            for number in range(1, 1000000):\n                name = base_name + str(number)\n                if name not in self._allocated_node_names:\n                    self._allocated_node_names.add(name)\n                    return name\n        raise aiohttp.web.HTTPConflict(text=\"A node name could not be allocated (node limit reached?)\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_node_from_appliance(self, appliance_id, x=0, y=0, compute_id=None):\n        try:\n            template = self.controller.appliances[appliance_id].data\n        except KeyError:\n            msg = \"Appliance {} doesn't exist\".format(appliance_id)\n            log.error(msg)\n            raise aiohttp.web.HTTPNotFound(text=msg)\n        template[\"x\"] = x\n        template[\"y\"] = y\n        node_type = template.pop(\"node_type\")\n        compute = self.controller.get_compute(template.pop(\"server\", compute_id))\n        name = template.pop(\"name\")\n        default_name_format = template.pop(\"default_name_format\", \"{name}-{0}\")\n        name = default_name_format.replace(\"{name}\", name)\n        node_id = str(uuid.uuid4())\n        node = yield from self.add_node(compute, name, node_id, node_type=node_type, **template)\n        return node", "response": "Create a node from an appliance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_node(self, compute, name, node_id, dump=True, node_type=None, **kwargs):\n\n        if node_id in self._nodes:\n            return self._nodes[node_id]\n\n        node = Node(self, compute, name, node_id=node_id, node_type=node_type, **kwargs)\n        if compute not in self._project_created_on_compute:\n            # For a local server we send the project path\n            if compute.id == \"local\":\n                yield from compute.post(\"/projects\", data={\n                    \"name\": self._name,\n                    \"project_id\": self._id,\n                    \"path\": self._path\n                })\n            else:\n                yield from compute.post(\"/projects\", data={\n                    \"name\": self._name,\n                    \"project_id\": self._id,\n                })\n\n            self._project_created_on_compute.add(compute)\n        yield from node.create()\n        self._nodes[node.id] = node\n        self.controller.notification.emit(\"node.created\", node.__json__())\n        if dump:\n            self.dump()\n        return node", "response": "Add a node to the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes all links connected to this node.", "response": "def __delete_node_links(self, node):\n        \"\"\"\n        Delete all link connected to this node.\n\n        The operation use a lock to avoid cleaning links from\n        multiple nodes at the same time.\n        \"\"\"\n        for link in list(self._links.values()):\n            if node in link.nodes:\n                yield from self.delete_link(link.id, force_delete=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the node object or raise a 404 if the node is unknown", "response": "def get_node(self, node_id):\n        \"\"\"\n        Return the node or raise a 404 if the node is unknown\n        \"\"\"\n        try:\n            return self._nodes[node_id]\n        except KeyError:\n            raise aiohttp.web.HTTPNotFound(text=\"Node ID {} doesn't exist\".format(node_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_drawing(self, drawing_id=None, dump=True, **kwargs):\n        if drawing_id not in self._drawings:\n            drawing = Drawing(self, drawing_id=drawing_id, **kwargs)\n            self._drawings[drawing.id] = drawing\n            self.controller.notification.emit(\"drawing.created\", drawing.__json__())\n            if dump:\n                self.dump()\n            return drawing\n        return self._drawings[drawing_id]", "response": "Create a drawing or return an existing drawing"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_drawing(self, drawing_id):\n        try:\n            return self._drawings[drawing_id]\n        except KeyError:\n            raise aiohttp.web.HTTPNotFound(text=\"Drawing ID {} doesn't exist\".format(drawing_id))", "response": "Get a drawing object from the cache or raise a 404 if the drawing is unknown"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a link. By default the link is empty.", "response": "def add_link(self, link_id=None, dump=True):\n        \"\"\"\n        Create a link. By default the link is empty\n\n        :param dump: Dump topology to disk\n        \"\"\"\n        if link_id and link_id in self._links:\n            return self._links[link_id]\n        link = UDPLink(self, link_id=link_id)\n        self._links[link.id] = link\n        if dump:\n            self.dump()\n        return link"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the Link object for the given link_id.", "response": "def get_link(self, link_id):\n        \"\"\"\n        Return the Link or raise a 404 if the link is unknown\n        \"\"\"\n        try:\n            return self._links[link_id]\n        except KeyError:\n            raise aiohttp.web.HTTPNotFound(text=\"Link ID {} doesn't exist\".format(link_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_snapshot(self, snapshot_id):\n        try:\n            return self._snapshots[snapshot_id]\n        except KeyError:\n            raise aiohttp.web.HTTPNotFound(text=\"Snapshot ID {} doesn't exist\".format(snapshot_id))", "response": "Return the snapshot or raise a 404 if the snapshot is unknown"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef snapshot(self, name):\n\n        if name in [snap.name for snap in self.snapshots.values()]:\n            raise aiohttp.web_exceptions.HTTPConflict(text=\"The snapshot {} already exist\".format(name))\n\n        snapshot = Snapshot(self, name=name)\n        try:\n            if os.path.exists(snapshot.path):\n                raise aiohttp.web_exceptions.HTTPConflict(text=\"The snapshot {} already exist\".format(name))\n\n            os.makedirs(os.path.join(self.path, \"snapshots\"), exist_ok=True)\n\n            with tempfile.TemporaryDirectory() as tmpdir:\n                zipstream = yield from export_project(self, tmpdir, keep_compute_id=True, allow_all_nodes=True)\n                try:\n                    with open(snapshot.path, \"wb\") as f:\n                        for data in zipstream:\n                            f.write(data)\n                except OSError as e:\n                    raise aiohttp.web.HTTPConflict(text=\"Could not write snapshot file '{}': {}\".format(snapshot.path, e))\n        except OSError as e:\n            raise aiohttp.web.HTTPInternalServerError(text=\"Could not create project directory: {}\".format(e))\n\n        self._snapshots[snapshot.id] = snapshot\n        return snapshot", "response": "Creates a snapshot of the project with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _cleanPictures(self):\n\n        # Project have been deleted\n        if not os.path.exists(self.path):\n            return\n        try:\n            pictures = set(os.listdir(self.pictures_directory))\n            for drawing in self._drawings.values():\n                try:\n                    pictures.remove(drawing.ressource_filename)\n                except KeyError:\n                    pass\n\n            for pict in pictures:\n                os.remove(os.path.join(self.pictures_directory, pict))\n        except OSError as e:\n            log.warning(str(e))", "response": "Delete unused images and drawings from the pictures directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the project on computes but not on controller", "response": "def delete_on_computes(self):\n        \"\"\"\n        Delete the project on computes but not on controller\n        \"\"\"\n        for compute in list(self._project_created_on_compute):\n            if compute.id != \"local\":\n                yield from compute.delete(\"/projects/{}\".format(self._id))\n                self._project_created_on_compute.remove(compute)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open(self):\n        if self._status == \"opened\":\n            return\n\n        self.reset()\n        self._loading = True\n        self._status = \"opened\"\n\n        path = self._topology_file()\n        if not os.path.exists(path):\n            self._loading = False\n            return\n        try:\n            shutil.copy(path, path + \".backup\")\n        except OSError:\n            pass\n        try:\n            project_data = load_topology(path)\n\n            #load meta of project\n            keys_to_load = [\n                \"auto_start\",\n                \"auto_close\",\n                \"auto_open\",\n                \"scene_height\",\n                \"scene_width\",\n                \"zoom\",\n                \"show_layers\",\n                \"snap_to_grid\",\n                \"show_grid\",\n                \"show_interface_labels\"\n            ]\n\n            for key in keys_to_load:\n                val = project_data.get(key, None)\n                if val is not None:\n                    setattr(self, key, val)\n\n            topology = project_data[\"topology\"]\n            for compute in topology.get(\"computes\", []):\n                yield from self.controller.add_compute(**compute)\n            for node in topology.get(\"nodes\", []):\n                compute = self.controller.get_compute(node.pop(\"compute_id\"))\n                name = node.pop(\"name\")\n                node_id = node.pop(\"node_id\", str(uuid.uuid4()))\n                yield from self.add_node(compute, name, node_id, dump=False, **node)\n            for link_data in topology.get(\"links\", []):\n                if 'link_id' not in link_data.keys():\n                    # skip the link\n                    continue\n                link = yield from self.add_link(link_id=link_data[\"link_id\"])\n                if \"filters\" in link_data:\n                    yield from link.update_filters(link_data[\"filters\"])\n                for node_link in link_data[\"nodes\"]:\n                    node = self.get_node(node_link[\"node_id\"])\n                    port = node.get_port(node_link[\"adapter_number\"], node_link[\"port_number\"])\n                    if port is None:\n                        log.warning(\"Port {}/{} for {} not found\".format(node_link[\"adapter_number\"], node_link[\"port_number\"], node.name))\n                        continue\n                    if port.link is not None:\n                        log.warning(\"Port {}/{} is already connected to link ID {}\".format(node_link[\"adapter_number\"], node_link[\"port_number\"], port.link.id))\n                        continue\n                    yield from link.add_node(node, node_link[\"adapter_number\"], node_link[\"port_number\"], label=node_link.get(\"label\"), dump=False)\n                if len(link.nodes) != 2:\n                    # a link should have 2 attached nodes, this can happen with corrupted projects\n                    yield from self.delete_link(link.id, force_delete=True)\n            for drawing_data in topology.get(\"drawings\", []):\n                yield from self.add_drawing(dump=False, **drawing_data)\n\n            self.dump()\n        # We catch all error to be able to rollback the .gns3 to the previous state\n        except Exception as e:\n            for compute in list(self._project_created_on_compute):\n                try:\n                    yield from compute.post(\"/projects/{}/close\".format(self._id))\n                # We don't care if a compute is down at this step\n                except (ComputeError, aiohttp.web.HTTPNotFound, aiohttp.web.HTTPConflict, aiohttp.ServerDisconnectedError):\n                    pass\n            try:\n                if os.path.exists(path + \".backup\"):\n                    shutil.copy(path + \".backup\", path)\n            except (PermissionError, OSError):\n                pass\n            self._status = \"closed\"\n            self._loading = False\n            if isinstance(e, ComputeError):\n                raise aiohttp.web.HTTPConflict(text=str(e))\n            else:\n                raise e\n        try:\n            os.remove(path + \".backup\")\n        except OSError:\n            pass\n\n        self._loading = False\n        # Should we start the nodes when project is open\n        if self._auto_start:\n            # Start all in the background without waiting for completion\n            # we ignore errors because we want to let the user open\n            # their project and fix it\n            asyncio.async(self.start_all())", "response": "Open the current instance of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nduplicates a project It's the save as feature of the 1.X. It's implemented on top of the export / import features. It will generate a gns3p and reimport it. It's a little slower but we have only one implementation to maintain. :param name: Name of the new project. A new one will be generated in case of conflicts :param location: Parent directory of the new project", "response": "def duplicate(self, name=None, location=None):\n        \"\"\"\n        Duplicate a project\n\n        It's the save as feature of the 1.X. It's implemented on top of the\n        export / import features. It will generate a gns3p and reimport it.\n        It's a little slower but we have only one implementation to maintain.\n\n        :param name: Name of the new project. A new one will be generated in case of conflicts\n        :param location: Parent directory of the new project\n        \"\"\"\n        # If the project was not open we open it temporary\n        previous_status = self._status\n        if self._status == \"closed\":\n            yield from self.open()\n\n        self.dump()\n        try:\n            with tempfile.TemporaryDirectory() as tmpdir:\n                zipstream = yield from export_project(self, tmpdir, keep_compute_id=True, allow_all_nodes=True)\n                with open(os.path.join(tmpdir, \"project.gns3p\"), \"wb\") as f:\n                    for data in zipstream:\n                        f.write(data)\n                with open(os.path.join(tmpdir, \"project.gns3p\"), \"rb\") as f:\n                    project = yield from import_project(self._controller, str(uuid.uuid4()), f, location=location, name=name, keep_compute_id=True)\n        except (OSError, UnicodeEncodeError) as e:\n            raise aiohttp.web.HTTPConflict(text=\"Can not duplicate project: {}\".format(str(e)))\n\n        if previous_status == \"closed\":\n            yield from self.close()\n\n        return project"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_running(self):\n        for node in self._nodes.values():\n            # Some node type are always running we ignore them\n            if node.status != \"stopped\" and not node.is_always_running():\n                return True\n        return False", "response": "Returns True if a node is running False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump(self):\n        try:\n            topo = project_to_topology(self)\n            path = self._topology_file()\n            log.debug(\"Write %s\", path)\n            with open(path + \".tmp\", \"w+\", encoding=\"utf-8\") as f:\n                json.dump(topo, f, indent=4, sort_keys=True)\n            shutil.move(path + \".tmp\", path)\n        except OSError as e:\n            raise aiohttp.web.HTTPInternalServerError(text=\"Could not write topology: {}\".format(e))", "response": "Dump the topology to disk"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart all nodes in the cluster", "response": "def start_all(self):\n        \"\"\"\n        Start all nodes\n        \"\"\"\n        pool = Pool(concurrency=3)\n        for node in self.nodes.values():\n            pool.append(node.start)\n        yield from pool.join()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstopping all nodes in the cluster.", "response": "def stop_all(self):\n        \"\"\"\n        Stop all nodes\n        \"\"\"\n        pool = Pool(concurrency=3)\n        for node in self.nodes.values():\n            pool.append(node.stop)\n        yield from pool.join()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nduplicate a node :param node: Node instance :param x: X position :param y: Y position :param z: Z position :returns: New node", "response": "def duplicate_node(self, node, x, y, z):\n        \"\"\"\n        Duplicate a node\n\n        :param node: Node instance\n        :param x: X position\n        :param y: Y position\n        :param z: Z position\n        :returns: New node\n        \"\"\"\n        if node.status != \"stopped\" and not node.is_always_running():\n            raise aiohttp.web.HTTPConflict(text=\"Cannot duplicate node data while the node is running\")\n\n        data = copy.deepcopy(node.__json__(topology_dump=True))\n        # Some properties like internal ID should not be duplicated\n        for unique_property in (\n                'node_id',\n                'name',\n                'mac_addr',\n                'mac_address',\n                'compute_id',\n                'application_id',\n                'dynamips_id'):\n            data.pop(unique_property, None)\n            if 'properties' in data:\n                data['properties'].pop(unique_property, None)\n        node_type = data.pop('node_type')\n        data['x'] = x\n        data['y'] = y\n        data['z'] = z\n        new_node_uuid = str(uuid.uuid4())\n        new_node = yield from self.add_node(\n            node.compute,\n            node.name,\n            new_node_uuid,\n            node_type=node_type,\n            **data)\n        try:\n            yield from node.post(\"/duplicate\", timeout=None, data={\n                \"destination_node_id\": new_node_uuid\n            })\n        except aiohttp.web.HTTPNotFound as e:\n            yield from self.delete_node(new_node_uuid)\n            raise aiohttp.web.HTTPConflict(text=\"This node type cannot be duplicated\")\n        except aiohttp.web.HTTPConflict as e:\n            yield from self.delete_node(new_node_uuid)\n            raise e\n        return new_node"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the key that maps to this prefix.", "response": "def _get_match(self, prefix):\n        \"\"\"\n        Return the key that maps to this prefix.\n        \"\"\"\n        # (hard coded) If we match a CPR response, return Keys.CPRResponse.\n        # (This one doesn't fit in the ANSI_SEQUENCES, because it contains\n        # integer variables.)\n        if _cpr_response_re.match(prefix):\n            return Keys.CPRResponse\n\n        elif _mouse_event_re.match(prefix):\n            return Keys.Vt100MouseEvent\n\n        # Otherwise, use the mappings.\n        try:\n            return ANSI_SEQUENCES[prefix]\n        except KeyError:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _input_parser_generator(self):\n        prefix = ''\n        retry = False\n        flush = False\n\n        while True:\n            flush = False\n\n            if retry:\n                retry = False\n            else:\n                # Get next character.\n                c = yield\n\n                if c == _Flush:\n                    flush = True\n                else:\n                    prefix += c\n\n            # If we have some data, check for matches.\n            if prefix:\n                is_prefix_of_longer_match = _IS_PREFIX_OF_LONGER_MATCH_CACHE[prefix]\n                match = self._get_match(prefix)\n\n                # Exact matches found, call handlers..\n                if (flush or not is_prefix_of_longer_match) and match:\n                    self._call_handler(match, prefix)\n                    prefix = ''\n\n                # No exact match found.\n                elif (flush or not is_prefix_of_longer_match) and not match:\n                    found = False\n                    retry = True\n\n                    # Loop over the input, try the longest match first and\n                    # shift.\n                    for i in range(len(prefix), 0, -1):\n                        match= self._get_match(prefix[:i])\n                        if match:\n                            self._call_handler(match, prefix[:i])\n                            prefix = prefix[i:]\n                            found = True\n\n                    if not found:\n                        self._call_handler(prefix[0], prefix[0])\n                        prefix = prefix[1:]", "response": "A generator that yields the input from the input parser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfeed the input string.", "response": "def feed(self, data):\n        \"\"\"\n        Feed the input stream.\n\n        :param data: Input string (unicode).\n        \"\"\"\n        assert isinstance(data, six.text_type)\n\n        if _DEBUG_RENDERER_INPUT:\n            self.LOG.write(repr(data).encode('utf-8') + b'\\n')\n            self.LOG.flush()\n\n        # Handle bracketed paste. (We bypass the parser that matches all other\n        # key presses and keep reading input until we see the end mark.)\n        # This is much faster then parsing character by character.\n        if self._in_bracketed_paste:\n            self._paste_buffer += data\n            end_mark = '\\x1b[201~'\n\n            if end_mark in self._paste_buffer:\n                end_index = self._paste_buffer.index(end_mark)\n\n                # Feed content to key bindings.\n                paste_content = self._paste_buffer[:end_index]\n                self.feed_key_callback(KeyPress(Keys.BracketedPaste, paste_content))\n\n                # Quit bracketed paste mode and handle remaining input.\n                self._in_bracketed_paste = False\n                remaining = self._paste_buffer[end_index + len(end_mark):]\n                self._paste_buffer = ''\n\n                self.feed(remaining)\n\n        # Handle normal input character by character.\n        else:\n            for i, c in enumerate(data):\n                if self._in_bracketed_paste:\n                    # Quit loop and process from this position when the parser\n                    # entered bracketed paste.\n                    self.feed(data[i:])\n                    break\n                else:\n                    # Replace \\r by \\n. (Some clients send \\r instead of \\n\n                    # when enter is pressed. E.g. telnet and some other\n                    # terminals.)\n\n                    # XXX: We should remove this in a future version. It *is*\n                    #      now possible to recognise the difference.\n                    #      (We remove ICRNL/INLCR/IGNCR below.)\n                    #      However, this breaks IPython and maybe other applications,\n                    #      because they bind ControlJ (\\n) for handling the Enter key.\n\n                    #      When this is removed, replace Enter=ControlJ by\n                    #      Enter=ControlM in keys.py.\n                    if c == '\\r':\n                        c = '\\n'\n                    self._input_parser.send(c)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a query to the Docker daemon and decode the response", "response": "def query(self, method, path, data={}, params={}):\n        \"\"\"\n        Make a query to the docker daemon and decode the request\n\n        :param method: HTTP method\n        :param path: Endpoint in API\n        :param data: Dictionary with the body. Will be transformed to a JSON\n        :param params: Parameters added as a query arg\n        \"\"\"\n\n        response = yield from self.http_query(method, path, data=data, params=params)\n        body = yield from response.read()\n        if body and len(body):\n            if response.headers['CONTENT-TYPE'] == 'application/json':\n                body = json.loads(body.decode(\"utf-8\"))\n            else:\n                body = body.decode(\"utf-8\")\n        log.debug(\"Query Docker %s %s params=%s data=%s Response: %s\", method, path, params, data, body)\n        return body"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a HTTP request to the Docker daemon and return the response.", "response": "def http_query(self, method, path, data={}, params={}, timeout=300):\n        \"\"\"\n        Make a query to the docker daemon\n\n        :param method: HTTP method\n        :param path: Endpoint in API\n        :param data: Dictionnary with the body. Will be transformed to a JSON\n        :param params: Parameters added as a query arg\n        :param timeout: Timeout\n        :returns: HTTP response\n        \"\"\"\n        data = json.dumps(data)\n        if timeout is None:\n            timeout = 60 * 60 * 24 * 31  # One month timeout\n\n        if path == 'version':\n            url = \"http://docker/v1.12/\" + path         # API of docker v1.0\n        else:\n            url = \"http://docker/v\" + DOCKER_MINIMUM_API_VERSION + \"/\" + path\n        try:\n            if path != \"version\":  # version is use by check connection\n                yield from self._check_connection()\n            if self._session is None or self._session.closed:\n                connector = self.connector()\n                self._session = aiohttp.ClientSession(connector=connector)\n            response = yield from self._session.request(\n                method,\n                url,\n                params=params,\n                data=data,\n                headers={\"content-type\": \"application/json\", },\n                timeout=timeout\n            )\n        except (aiohttp.ClientResponseError, aiohttp.ClientOSError) as e:\n            raise DockerError(\"Docker has returned an error: {}\".format(str(e)))\n        except (asyncio.TimeoutError):\n            raise DockerError(\"Docker timeout \" + method + \" \" + path)\n        if response.status >= 300:\n            body = yield from response.read()\n            try:\n                body = json.loads(body.decode(\"utf-8\"))[\"message\"]\n            except ValueError:\n                pass\n            log.debug(\"Query Docker %s %s params=%s data=%s Response: %s\", method, path, params, data, body)\n            if response.status == 304:\n                raise DockerHttp304Error(\"Docker has returned an error: {} {}\".format(response.status, body))\n            elif response.status == 404:\n                raise DockerHttp404Error(\"Docker has returned an error: {} {}\".format(response.status, body))\n            else:\n                raise DockerError(\"Docker has returned an error: {} {}\".format(response.status, body))\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen a websocket connection to the Docker API.", "response": "def websocket_query(self, path, params={}):\n        \"\"\"\n        Open a websocket connection\n\n        :param path: Endpoint in API\n        :param params: Parameters added as a query arg\n        :returns: Websocket\n        \"\"\"\n\n        url = \"http://docker/v\" + self._api_version + \"/\" + path\n        connection = yield from self._session.ws_connect(url,\n                                                         origin=\"http://docker\",\n                                                         autoping=True)\n        return connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pull_image(self, image, progress_callback=None):\n\n        try:\n            yield from self.query(\"GET\", \"images/{}/json\".format(image))\n            return  # We already have the image skip the download\n        except DockerHttp404Error:\n            pass\n\n        if progress_callback:\n            progress_callback(\"Pull {} from docker hub\".format(image))\n        response = yield from self.http_query(\"POST\", \"images/create\", params={\"fromImage\": image}, timeout=None)\n        # The pull api will stream status via an HTTP JSON stream\n        content = \"\"\n        while True:\n            try:\n                chunk = yield from response.content.read(1024)\n            except aiohttp.ServerDisconnectedError:\n                break\n            if not chunk:\n                break\n            content += chunk.decode(\"utf-8\")\n\n            try:\n                while True:\n                    content = content.lstrip(\" \\r\\n\\t\")\n                    answer, index = json.JSONDecoder().raw_decode(content)\n                    if \"progress\" in answer and progress_callback:\n                        progress_callback(\"Pulling image {}:{}: {}\".format(image, answer[\"id\"], answer[\"progress\"]))\n                    content = content[index:]\n            except ValueError:  # Partial JSON\n                pass\n        response.close()\n        if progress_callback:\n            progress_callback(\"Success pulling image {}\".format(image))", "response": "Pull an image from the Docker repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets Docker image list.", "response": "def list_images(self):\n        \"\"\"Gets Docker image list.\n\n        :returns: list of dicts\n        :rtype: list\n        \"\"\"\n        images = []\n        for image in (yield from self.query(\"GET\", \"images/json\", params={\"all\": 0})):\n            if image['RepoTags']:\n                for tag in image['RepoTags']:\n                    if tag != \"<none>:<none>\":\n                        images.append({'image': tag})\n        return sorted(images, key=lambda i: i['image'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_kvm_archs():\n        kvm = []\n\n        if not os.path.exists(\"/dev/kvm\"):\n            return kvm\n\n        arch = platform.machine()\n        if arch == \"x86_64\":\n            kvm.append(\"x86_64\")\n            kvm.append(\"i386\")\n        elif arch == \"i386\":\n            kvm.append(\"i386\")\n        else:\n            kvm.append(platform.machine())\n        return kvm", "response": "Gets a list of architectures for which KVM is available on this server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef paths_list():\n\n        paths = set()\n        try:\n            paths.add(os.getcwd())\n        except FileNotFoundError:\n            log.warning(\"The current working directory doesn't exist\")\n        if \"PATH\" in os.environ:\n            paths.update(os.environ[\"PATH\"].split(os.pathsep))\n        else:\n            log.warning(\"The PATH environment variable doesn't exist\")\n        # look for Qemu binaries in the current working directory and $PATH\n        if sys.platform.startswith(\"win\"):\n            # add specific Windows paths\n            if hasattr(sys, \"frozen\"):\n                # add any qemu dir in the same location as gns3server.exe to the list of paths\n                exec_dir = os.path.dirname(os.path.abspath(sys.executable))\n                for f in os.listdir(exec_dir):\n                    if f.lower().startswith(\"qemu\"):\n                        paths.add(os.path.join(exec_dir, f))\n\n            if \"PROGRAMFILES(X86)\" in os.environ and os.path.exists(os.environ[\"PROGRAMFILES(X86)\"]):\n                paths.add(os.path.join(os.environ[\"PROGRAMFILES(X86)\"], \"qemu\"))\n            if \"PROGRAMFILES\" in os.environ and os.path.exists(os.environ[\"PROGRAMFILES\"]):\n                paths.add(os.path.join(os.environ[\"PROGRAMFILES\"], \"qemu\"))\n        elif sys.platform.startswith(\"darwin\"):\n            if hasattr(sys, \"frozen\"):\n                # add specific locations on Mac OS X regardless of what's in $PATH\n                paths.update([\"/usr/bin\", \"/usr/local/bin\", \"/opt/local/bin\"])\n                try:\n                    exec_dir = os.path.dirname(os.path.abspath(sys.executable))\n                    paths.add(os.path.abspath(os.path.join(exec_dir, \"../Resources/qemu/bin/\")))\n                # If the user run the server by hand from outside\n                except FileNotFoundError:\n                    paths.add(\"/Applications/GNS3.app/Contents/Resources/qemu/bin\")\n        return paths", "response": "Gets a list of possibly available QEMU binaries on the host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the list of QEMU binaries available on the host.", "response": "def binary_list(archs=None):\n        \"\"\"\n        Gets QEMU binaries list available on the host.\n\n        :returns: Array of dictionary {\"path\": Qemu binary path, \"version\": version of Qemu}\n        \"\"\"\n\n        qemus = []\n        for path in Qemu.paths_list():\n            try:\n                for f in os.listdir(path):\n                    if f.endswith(\"-spice\"):\n                        continue\n                    if (f.startswith(\"qemu-system\") or f.startswith(\"qemu-kvm\") or f == \"qemu\" or f == \"qemu.exe\") and \\\n                            os.access(os.path.join(path, f), os.X_OK) and \\\n                            os.path.isfile(os.path.join(path, f)):\n                        if archs is not None:\n                            for arch in archs:\n                                if f.endswith(arch) or f.endswith(\"{}.exe\".format(arch)) or f.endswith(\"{}w.exe\".format(arch)):\n                                    qemu_path = os.path.join(path, f)\n                                    version = yield from Qemu.get_qemu_version(qemu_path)\n                                    qemus.append({\"path\": qemu_path, \"version\": version})\n                        else:\n                            qemu_path = os.path.join(path, f)\n                            version = yield from Qemu.get_qemu_version(qemu_path)\n                            qemus.append({\"path\": qemu_path, \"version\": version})\n\n            except OSError:\n                continue\n\n        return qemus"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef img_binary_list():\n        qemu_imgs = []\n        for path in Qemu.paths_list():\n            try:\n                for f in os.listdir(path):\n                    if (f == \"qemu-img\" or f == \"qemu-img.exe\") and \\\n                            os.access(os.path.join(path, f), os.X_OK) and \\\n                            os.path.isfile(os.path.join(path, f)):\n                        qemu_path = os.path.join(path, f)\n                        version = yield from Qemu._get_qemu_img_version(qemu_path)\n                        qemu_imgs.append({\"path\": qemu_path, \"version\": version})\n            except OSError:\n                continue\n\n        return qemu_imgs", "response": "Gets the list of QEMU - img binary paths available on the host."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_qemu_version(qemu_path):\n\n        if sys.platform.startswith(\"win\"):\n            # Qemu on Windows doesn't return anything with parameter -version\n            # look for a version number in version.txt file in the same directory instead\n            version_file = os.path.join(os.path.dirname(qemu_path), \"version.txt\")\n            if os.path.isfile(version_file):\n                try:\n                    with open(version_file, \"rb\") as file:\n                        version = file.read().decode(\"utf-8\").strip()\n                        match = re.search(\"[0-9\\.]+\", version)\n                        if match:\n                            return version\n                except (UnicodeDecodeError, OSError) as e:\n                    log.warn(\"could not read {}: {}\".format(version_file, e))\n            return \"\"\n        else:\n            try:\n                output = yield from subprocess_check_output(qemu_path, \"-version\")\n                match = re.search(\"version\\s+([0-9a-z\\-\\.]+)\", output)\n                if match:\n                    version = match.group(1)\n                    return version\n                else:\n                    raise QemuError(\"Could not determine the Qemu version for {}\".format(qemu_path))\n            except subprocess.SubprocessError as e:\n                raise QemuError(\"Error while looking for the Qemu version: {}\".format(e))", "response": "Gets the Qemu version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the version of the Qemu - img executable.", "response": "def _get_qemu_img_version(qemu_img_path):\n        \"\"\"\n        Gets the Qemu-img version.\n\n        :param qemu_img_path: path to Qemu-img executable.\n        \"\"\"\n\n        try:\n            output = yield from subprocess_check_output(qemu_img_path, \"--version\")\n            match = re.search(\"version\\s+([0-9a-z\\-\\.]+)\", output)\n            if match:\n                version = match.group(1)\n                return version\n            else:\n                raise QemuError(\"Could not determine the Qemu-img version for {}\".format(qemu_img_path))\n        except subprocess.SubprocessError as e:\n            raise QemuError(\"Error while looking for the Qemu-img version: {}\".format(e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_disk(self, qemu_img, path, options):\n\n        try:\n            img_format = options.pop(\"format\")\n            img_size = options.pop(\"size\")\n\n            if not os.path.isabs(path):\n                directory = self.get_images_directory()\n                os.makedirs(directory, exist_ok=True)\n                path = os.path.join(directory, os.path.basename(path))\n\n            try:\n                if os.path.exists(path):\n                    raise QemuError(\"Could not create disk image {} already exist\".format(path))\n            except UnicodeEncodeError:\n                raise QemuError(\"Could not create disk image {}, \"\n                                \"path contains characters not supported by filesystem\".format(path))\n\n            command = [qemu_img, \"create\", \"-f\", img_format]\n            for option in sorted(options.keys()):\n                command.extend([\"-o\", \"{}={}\".format(option, options[option])])\n            command.append(path)\n            command.append(\"{}M\".format(img_size))\n\n            process = yield from asyncio.create_subprocess_exec(*command)\n            yield from process.wait()\n        except (OSError, subprocess.SubprocessError) as e:\n            raise QemuError(\"Could not create disk image {}:{}\".format(path, e))", "response": "Create a qemu - img disk with qemu - img - create - f qemu - img - size qemu - img - format qemu - img - size qemu - img - create - f qemu - img - format qemu - img - size qemu - img - size qemu - img - format qemu - img - size qemu - img - size qemu - img - format qemu - img - size qemu - img - format qemu - img"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_npartitions(self, npartitions=None):\n        if npartitions is None:\n            self._npartitions = cpu_count() * 2\n        else:\n            self._npartitions = npartitions\n        return self", "response": "Sets the number of partitions to use for dask."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rolling(self, window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None):\n        kwds = {\n            \"window\": window,\n            \"min_periods\": min_periods,\n            \"center\": center,\n            \"win_type\": win_type,\n            \"on\": on,\n            \"axis\": axis,\n            \"closed\": closed,\n        }\n        return Rolling(self._obj, self._npartitions, self._dask_threshold, self._scheduler, self._progress_bar, **kwds)", "response": "Create a rolling object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies the function to the transformed swifter object", "response": "def apply(self, func, *args, **kwds):\n        \"\"\"\n        Apply the function to the transformed swifter object\n        \"\"\"\n        # estimate time to pandas apply\n        wrapped = self._wrapped_apply(func, *args, **kwds)\n        n_repeats = 3\n        timed = timeit.timeit(wrapped, number=n_repeats)\n        samp_proc_est = timed / n_repeats\n        est_apply_duration = samp_proc_est / self._SAMP_SIZE * self._nrows\n\n        # if pandas apply takes too long, use dask\n        if est_apply_duration > self._dask_threshold:\n            return self._dask_apply(func, *args, **kwds)\n        else:  # use pandas\n            if self._progress_bar:\n                tqdm.pandas(desc=\"Pandas Apply\")\n                return self._obj_pd.progress_apply(func, *args, **kwds)\n            else:\n                return self._obj_pd.apply(func, *args, **kwds)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef uploaded_filepath(instance, filename):\n    ext = filename.split('.')[-1]\n    filename = \"%s.%s\" % (uuid.uuid4(), ext)\n    today = datetime.now().strftime('%Y-%m-%d')\n    return os.path.join('django-summernote', today, filename)", "response": "Returns default filepath for uploaded files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_attachment_model():\n\n    try:\n        from .models import AbstractAttachment\n        klass = apps.get_model(config[\"attachment_model\"])\n        if not issubclass(klass, AbstractAttachment):\n            raise ImproperlyConfigured(\n                \"SUMMERNOTE_CONFIG['attachment_model'] refers to model '%s' that is not \"\n                \"inherited from 'django_summernote.models.AbstractAttachment'\" % config[\"attachment_model\"]\n            )\n        return klass\n    except ValueError:\n        raise ImproperlyConfigured(\"SUMMERNOTE_CONFIG['attachment_model'] must be of the form 'app_label.model_name'\")\n    except LookupError:\n        raise ImproperlyConfigured(\n            \"SUMMERNOTE_CONFIG['attachment_model'] refers to model '%s' that has not been installed\" % config[\"attachment_model\"]\n        )", "response": "Returns the model that is active in this project."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _copy_old_configs(self, user, default):\n        DEPRECATED_SUMMERNOTE_CONFIGS = (\n            'width',\n            'height',\n            'lang',\n            'toolbar',\n        )\n        for key in DEPRECATED_SUMMERNOTE_CONFIGS:\n            if user.get(key):\n                self.config['summernote'][key] = user.get(key)\n            if not self.config['summernote'].get(key):\n                self.config['summernote'][key] = default['summernote'].get(key)", "response": "Copy old - style settings for backword - compatibility."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the leading numeric part of a string.", "response": "def numeric_part(s):\n    \"\"\"Returns the leading numeric part of a string.\n\n    >>> numeric_part(\"20-alpha\")\n    20\n    >>> numeric_part(\"foo\")\n    >>> numeric_part(\"16b\")\n    16\n    \"\"\"\n\n    m = re_numeric_part.match(s)\n    if m:\n        return int(m.group(1))\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef literal(self, o):\n        if isinstance(o, unicode):\n            s = self.string_literal(o.encode(self.encoding))\n        elif isinstance(o, bytearray):\n            s = self._bytes_literal(o)\n        elif isinstance(o, bytes):\n            if PY2:\n                s = self.string_literal(o)\n            else:\n                s = self._bytes_literal(o)\n        elif isinstance(o, (tuple, list)):\n            s = self._tuple_literal(o)\n        else:\n            s = self.escape(o, self.encoders)\n            if isinstance(s, unicode):\n                s = s.encode(self.encoding)\n        assert isinstance(s, bytes)\n        return s", "response": "Returns a SQL literal for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the connection character set to charset.", "response": "def set_character_set(self, charset):\n        \"\"\"Set the connection character set to charset. The character\n        set can only be changed in MySQL-4.1 and newer. If you try\n        to change the character set from the current value in an\n        older version, NotSupportedError will be raised.\"\"\"\n        if charset in (\"utf8mb4\", \"utf8mb3\"):\n            py_charset = \"utf8\"\n        else:\n            py_charset = charset\n        if self.character_set_name() != charset:\n            try:\n                super(Connection, self).set_character_set(charset)\n            except AttributeError:\n                if self._server_version < (4, 1):\n                    raise NotSupportedError(\"server is too old to set charset\")\n                self.query('SET NAMES %s' % charset)\n                self.store_result()\n        self.encoding = py_charset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the connection sql_mode. See MySQL documentation for legal values.", "response": "def set_sql_mode(self, sql_mode):\n        \"\"\"Set the connection sql_mode. See MySQL documentation for\n        legal values.\"\"\"\n        if self._server_version < (4, 1):\n            raise NotSupportedError(\"server is too old to set sql_mode\")\n        self.query(\"SET SESSION sql_mode='%s'\" % sql_mode)\n        self.store_result()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning detailed information about warnings as a sequence of tuples of level Code Message.", "response": "def show_warnings(self):\n        \"\"\"Return detailed information about warnings as a\n        sequence of tuples of (Level, Code, Message). This\n        is only supported in MySQL-4.1 and up. If your server\n        is an earlier version, an empty sequence is returned.\"\"\"\n        if self._server_version < (4,1): return ()\n        self.query(\"SHOW WARNINGS\")\n        r = self.store_result()\n        warnings = r.fetch_row(0)\n        return warnings"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses the cursor. No further queries will be possible.", "response": "def close(self):\n        \"\"\"Close the cursor. No further queries will be possible.\"\"\"\n        try:\n            if self.connection is None:\n                return\n            while self.nextset():\n                pass\n        finally:\n            self.connection = None\n            self._result = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute(self, query, args=None):\n        while self.nextset():\n            pass\n        db = self._get_db()\n\n        if isinstance(query, unicode):\n            query = query.encode(db.encoding)\n\n        if args is not None:\n            if isinstance(args, dict):\n                nargs = {}\n                for key, item in args.items():\n                    if isinstance(key, unicode):\n                        key = key.encode(db.encoding)\n                    nargs[key] = db.literal(item)\n                args = nargs\n            else:\n                args = tuple(map(db.literal, args))\n            try:\n                query = query % args\n            except TypeError as m:\n                raise ProgrammingError(str(m))\n\n        assert isinstance(query, (bytes, bytearray))\n        res = self._query(query)\n        return res", "response": "Execute a query on the server and return the number of affected rows."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef callproc(self, procname, args=()):\n        db = self._get_db()\n        if isinstance(procname, unicode):\n            procname = procname.encode(db.encoding)\n        if args:\n            fmt = b'@_' + procname + b'_%d=%s'\n            q = b'SET %s' % b','.join(fmt % (index, db.literal(arg))\n                                      for index, arg in enumerate(args))\n            self._query(q)\n            self.nextset()\n\n        q = b\"CALL %s(%s)\" % (procname,\n                              b','.join([b'@_%s_%d' % (procname, i)\n                                         for i in range(len(args))]))\n        self._query(q)\n        return args", "response": "Execute a stored procedure procname with args returning the original args."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscrolling the cursor in the result set to a new position according to mode.", "response": "def scroll(self, value, mode='relative'):\n        \"\"\"Scroll the cursor in the result set to a new position according\n        to mode.\n\n        If mode is 'relative' (default), value is taken as offset to\n        the current position in the result set, if set to 'absolute',\n        value states an absolute target position.\"\"\"\n        self._check_executed()\n        if mode == 'relative':\n            r = self.rownumber + value\n        elif mode == 'absolute':\n            r = value\n        else:\n            raise ProgrammingError(\"unknown scroll mode %s\" % repr(mode))\n        if r < 0 or r >= len(self._rows):\n            raise IndexError(\"out of range\")\n        self.rownumber = r"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def fetch(self, n, *, timeout=None):\n        self._check_ready()\n        if n <= 0:\n            raise exceptions.InterfaceError('n must be greater than zero')\n        if self._exhausted:\n            return []\n        recs = await self._exec(n, timeout)\n        if len(recs) < n:\n            self._exhausted = True\n        return recs", "response": "r Return the next n rows as a list of Record objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the next record in the table.", "response": "async def fetchrow(self, *, timeout=None):\n        r\"\"\"Return the next row.\n\n        :param float timeout: Optional timeout value in seconds.\n\n        :return: A :class:`Record` instance.\n        \"\"\"\n        self._check_ready()\n        if self._exhausted:\n            return None\n        recs = await self._exec(1, timeout)\n        if len(recs) < 1:\n            self._exhausted = True\n            return None\n        return recs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def forward(self, n, *, timeout=None) -> int:\n        self._check_ready()\n        if n <= 0:\n            raise exceptions.InterfaceError('n must be greater than zero')\n\n        protocol = self._connection._protocol\n        status = await protocol.query('MOVE FORWARD {:d} {}'.format(\n            n, self._portal_name), timeout)\n\n        advanced = int(status.split()[1])\n        if advanced < n:\n            self._exhausted = True\n\n        return advanced", "response": "Skip over the next n rows."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef guarded(meth):\n\n    @functools.wraps(meth)\n    def _check(self, *args, **kwargs):\n        self._check_conn_validity(meth.__name__)\n        return meth(self, *args, **kwargs)\n\n    return _check", "response": "A decorator to add a sanity check to ConnectionResource methods."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreloads the server configuration.", "response": "def reload(self):\n        \"\"\"Reload server configuration.\"\"\"\n        status = self.get_status()\n        if status != 'running':\n            raise ClusterError('cannot reload: cluster is not running')\n\n        process = subprocess.run(\n            [self._pg_ctl, 'reload', '-D', self._data_dir],\n            stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        stderr = process.stderr\n\n        if process.returncode != 0:\n            raise ClusterError(\n                'pg_ctl stop exited with status {:d}: {}'.format(\n                    process.returncode, stderr.decode()))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset_hba(self):\n        status = self.get_status()\n        if status == 'not-initialized':\n            raise ClusterError(\n                'cannot modify HBA records: cluster is not initialized')\n\n        pg_hba = os.path.join(self._data_dir, 'pg_hba.conf')\n\n        try:\n            with open(pg_hba, 'w'):\n                pass\n        except IOError as e:\n            raise ClusterError(\n                'cannot modify HBA records: {}'.format(e)) from e", "response": "Remove all records from pg_hba. conf."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_hba_entry(self, *, type='host', database, user, address=None,\n                      auth_method, auth_options=None):\n        \"\"\"Add a record to pg_hba.conf.\"\"\"\n        status = self.get_status()\n        if status == 'not-initialized':\n            raise ClusterError(\n                'cannot modify HBA records: cluster is not initialized')\n\n        if type not in {'local', 'host', 'hostssl', 'hostnossl'}:\n            raise ValueError('invalid HBA record type: {!r}'.format(type))\n\n        pg_hba = os.path.join(self._data_dir, 'pg_hba.conf')\n\n        record = '{} {} {}'.format(type, database, user)\n\n        if type != 'local':\n            if address is None:\n                raise ValueError(\n                    '{!r} entry requires a valid address'.format(type))\n            else:\n                record += ' {}'.format(address)\n\n        record += ' {}'.format(auth_method)\n\n        if auth_options is not None:\n            record += ' ' + ' '.join(\n                '{}={}'.format(k, v) for k, v in auth_options)\n\n        try:\n            with open(pg_hba, 'a') as f:\n                print(record, file=f)\n        except IOError as e:\n            raise ClusterError(\n                'cannot modify HBA records: {}'.format(e)) from e", "response": "Add a record to pg_hba. conf."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def connect(dsn=None, *,\n                  host=None, port=None,\n                  user=None, password=None, passfile=None,\n                  database=None,\n                  loop=None,\n                  timeout=60,\n                  statement_cache_size=100,\n                  max_cached_statement_lifetime=300,\n                  max_cacheable_statement_size=1024 * 15,\n                  command_timeout=None,\n                  ssl=None,\n                  connection_class=Connection,\n                  server_settings=None):\n    r\"\"\"A coroutine to establish a connection to a PostgreSQL server.\n\n    The connection parameters may be specified either as a connection\n    URI in *dsn*, or as specific keyword arguments, or both.\n    If both *dsn* and keyword arguments are specified, the latter\n    override the corresponding values parsed from the connection URI.\n    The default values for the majority of arguments can be specified\n    using `environment variables <postgres envvars>`_.\n\n    Returns a new :class:`~asyncpg.connection.Connection` object.\n\n    :param dsn:\n        Connection arguments specified using as a single string in the\n        `libpq connection URI format`_:\n        ``postgres://user:password@host:port/database?option=value``.\n        The following options are recognized by asyncpg: host, port,\n        user, database (or dbname), password, passfile, sslmode.\n        Unlike libpq, asyncpg will treat unrecognized options\n        as `server settings`_ to be used for the connection.\n\n    :param host:\n        Database host address as one of the following:\n\n        - an IP address or a domain name;\n        - an absolute path to the directory containing the database\n          server Unix-domain socket (not supported on Windows);\n        - a sequence of any of the above, in which case the addresses\n          will be tried in order, and the first successful connection\n          will be returned.\n\n        If not specified, asyncpg will try the following, in order:\n\n        - host address(es) parsed from the *dsn* argument,\n        - the value of the ``PGHOST`` environment variable,\n        - on Unix, common directories used for PostgreSQL Unix-domain\n          sockets: ``\"/run/postgresql\"``, ``\"/var/run/postgresl\"``,\n          ``\"/var/pgsql_socket\"``, ``\"/private/tmp\"``, and ``\"/tmp\"``,\n        - ``\"localhost\"``.\n\n    :param port:\n        Port number to connect to at the server host\n        (or Unix-domain socket file extension).  If multiple host\n        addresses were specified, this parameter may specify a\n        sequence of port numbers of the same length as the host sequence,\n        or it may specify a single port number to be used for all host\n        addresses.\n\n        If not specified, the value parsed from the *dsn* argument is used,\n        or the value of the ``PGPORT`` environment variable, or ``5432`` if\n        neither is specified.\n\n    :param user:\n        The name of the database role used for authentication.\n\n        If not specified, the value parsed from the *dsn* argument is used,\n        or the value of the ``PGUSER`` environment variable, or the\n        operating system name of the user running the application.\n\n    :param database:\n        The name of the database to connect to.\n\n        If not specified, the value parsed from the *dsn* argument is used,\n        or the value of the ``PGDATABASE`` environment variable, or the\n        operating system name of the user running the application.\n\n    :param password:\n        Password to be used for authentication, if the server requires\n        one.  If not specified, the value parsed from the *dsn* argument\n        is used, or the value of the ``PGPASSWORD`` environment variable.\n        Note that the use of the environment variable is discouraged as\n        other users and applications may be able to read it without needing\n        specific privileges.  It is recommended to use *passfile* instead.\n\n    :param passfile:\n        The name of the file used to store passwords\n        (defaults to ``~/.pgpass``, or ``%APPDATA%\\postgresql\\pgpass.conf``\n        on Windows).\n\n    :param loop:\n        An asyncio event loop instance.  If ``None``, the default\n        event loop will be used.\n\n    :param float timeout:\n        Connection timeout in seconds.\n\n    :param int statement_cache_size:\n        The size of prepared statement LRU cache.  Pass ``0`` to\n        disable the cache.\n\n    :param int max_cached_statement_lifetime:\n        The maximum time in seconds a prepared statement will stay\n        in the cache.  Pass ``0`` to allow statements be cached\n        indefinitely.\n\n    :param int max_cacheable_statement_size:\n        The maximum size of a statement that can be cached (15KiB by\n        default).  Pass ``0`` to allow all statements to be cached\n        regardless of their size.\n\n    :param float command_timeout:\n        The default timeout for operations on this connection\n        (the default is ``None``: no timeout).\n\n    :param ssl:\n        Pass ``True`` or an `ssl.SSLContext <SSLContext_>`_ instance to\n        require an SSL connection.  If ``True``, a default SSL context\n        returned by `ssl.create_default_context() <create_default_context_>`_\n        will be used.\n\n    :param dict server_settings:\n        An optional dict of server runtime parameters.  Refer to\n        PostgreSQL documentation for\n        a `list of supported options <server settings>`_.\n\n    :param Connection connection_class:\n        Class of the returned connection object.  Must be a subclass of\n        :class:`~asyncpg.connection.Connection`.\n\n    :return: A :class:`~asyncpg.connection.Connection` instance.\n\n    Example:\n\n    .. code-block:: pycon\n\n        >>> import asyncpg\n        >>> import asyncio\n        >>> async def run():\n        ...     con = await asyncpg.connect(user='postgres')\n        ...     types = await con.fetch('SELECT * FROM pg_type')\n        ...     print(types)\n        ...\n        >>> asyncio.get_event_loop().run_until_complete(run())\n        [<Record typname='bool' typnamespace=11 ...\n\n    .. versionadded:: 0.10.0\n       Added ``max_cached_statement_use_count`` parameter.\n\n    .. versionchanged:: 0.11.0\n       Removed ability to pass arbitrary keyword arguments to set\n       server settings.  Added a dedicated parameter ``server_settings``\n       for that.\n\n    .. versionadded:: 0.11.0\n       Added ``connection_class`` parameter.\n\n    .. versionadded:: 0.16.0\n       Added ``passfile`` parameter\n       (and support for password files in general).\n\n    .. versionadded:: 0.18.0\n       Added ability to specify multiple hosts in the *dsn*\n       and *host* arguments.\n\n    .. _SSLContext: https://docs.python.org/3/library/ssl.html#ssl.SSLContext\n    .. _create_default_context:\n        https://docs.python.org/3/library/ssl.html#ssl.create_default_context\n    .. _server settings:\n        https://www.postgresql.org/docs/current/static/runtime-config.html\n    .. _postgres envvars:\n        https://www.postgresql.org/docs/current/static/libpq-envars.html\n    .. _libpq connection URI format:\n        https://www.postgresql.org/docs/current/static/\\\n        libpq-connect.html#LIBPQ-CONNSTRING\n    \"\"\"\n    if not issubclass(connection_class, Connection):\n        raise TypeError(\n            'connection_class is expected to be a subclass of '\n            'asyncpg.Connection, got {!r}'.format(connection_class))\n\n    if loop is None:\n        loop = asyncio.get_event_loop()\n\n    return await connect_utils._connect(\n        loop=loop, timeout=timeout, connection_class=connection_class,\n        dsn=dsn, host=host, port=port, user=user,\n        password=password, passfile=passfile,\n        ssl=ssl, database=database,\n        server_settings=server_settings,\n        command_timeout=command_timeout,\n        statement_cache_size=statement_cache_size,\n        max_cached_statement_lifetime=max_cached_statement_lifetime,\n        max_cacheable_statement_size=max_cacheable_statement_size)", "response": "A coroutine to establish a PostgreSQL connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def add_listener(self, channel, callback):\n        self._check_open()\n        if channel not in self._listeners:\n            await self.fetch('LISTEN {}'.format(utils._quote_ident(channel)))\n            self._listeners[channel] = set()\n        self._listeners[channel].add(callback)", "response": "Add a listener for Postgres notifications."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def remove_listener(self, channel, callback):\n        if self.is_closed():\n            return\n        if channel not in self._listeners:\n            return\n        if callback not in self._listeners[channel]:\n            return\n        self._listeners[channel].remove(callback)\n        if not self._listeners[channel]:\n            del self._listeners[channel]\n            await self.fetch('UNLISTEN {}'.format(utils._quote_ident(channel)))", "response": "Remove a listening callback on the specified channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a callback to be called when a Postgres log message is received.", "response": "def add_log_listener(self, callback):\n        \"\"\"Add a listener for Postgres log messages.\n\n        It will be called when asyncronous NoticeResponse is received\n        from the connection.  Possible message types are: WARNING, NOTICE,\n        DEBUG, INFO, or LOG.\n\n        :param callable callback:\n            A callable receiving the following arguments:\n            **connection**: a Connection the callback is registered with;\n            **message**: the `exceptions.PostgresLogMessage` message.\n\n        .. versionadded:: 0.12.0\n        \"\"\"\n        if self.is_closed():\n            raise exceptions.InterfaceError('connection is closed')\n        self._log_listeners.add(callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transaction(self, *, isolation='read_committed', readonly=False,\n                    deferrable=False):\n        \"\"\"Create a :class:`~transaction.Transaction` object.\n\n        Refer to `PostgreSQL documentation`_ on the meaning of transaction\n        parameters.\n\n        :param isolation: Transaction isolation mode, can be one of:\n                          `'serializable'`, `'repeatable_read'`,\n                          `'read_committed'`.\n\n        :param readonly: Specifies whether or not this transaction is\n                         read-only.\n\n        :param deferrable: Specifies whether or not this transaction is\n                           deferrable.\n\n        .. _`PostgreSQL documentation`:\n                https://www.postgresql.org/docs/\n                current/static/sql-set-transaction.html\n        \"\"\"\n        self._check_open()\n        return transaction.Transaction(self, isolation, readonly, deferrable)", "response": "Create a new transaction object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute an SQL command or commands.", "response": "async def execute(self, query: str, *args, timeout: float=None) -> str:\n        \"\"\"Execute an SQL command (or commands).\n\n        This method can execute many SQL commands at once, when no arguments\n        are provided.\n\n        Example:\n\n        .. code-block:: pycon\n\n            >>> await con.execute('''\n            ...     CREATE TABLE mytab (a int);\n            ...     INSERT INTO mytab (a) VALUES (100), (200), (300);\n            ... ''')\n            INSERT 0 3\n\n            >>> await con.execute('''\n            ...     INSERT INTO mytab (a) VALUES ($1), ($2)\n            ... ''', 10, 20)\n            INSERT 0 2\n\n        :param args: Query arguments.\n        :param float timeout: Optional timeout value in seconds.\n        :return str: Status of the last SQL command.\n\n        .. versionchanged:: 0.5.4\n           Made it possible to pass query arguments.\n        \"\"\"\n        self._check_open()\n\n        if not args:\n            return await self._protocol.query(query, timeout)\n\n        _, status, _ = await self._execute(query, args, 0, timeout, True)\n        return status.decode()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def executemany(self, command: str, args, *, timeout: float=None):\n        self._check_open()\n        return await self._executemany(command, args, timeout)", "response": "Execute an SQL command for each sequence of arguments in args*. Returns a list of the result rows."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a cursor factory for the specified query.", "response": "def cursor(self, query, *args, prefetch=None, timeout=None):\n        \"\"\"Return a *cursor factory* for the specified query.\n\n        :param args: Query arguments.\n        :param int prefetch: The number of rows the *cursor iterator*\n                             will prefetch (defaults to ``50``.)\n        :param float timeout: Optional timeout in seconds.\n\n        :return: A :class:`~cursor.CursorFactory` object.\n        \"\"\"\n        self._check_open()\n        return cursor.CursorFactory(self, query, None, args,\n                                    prefetch, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def prepare(self, query, *, timeout=None):\n        return await self._prepare(query, timeout=timeout, use_cache=False)", "response": "Create a prepared statement for the specified query."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def fetch(self, query, *args, timeout=None) -> list:\n        self._check_open()\n        return await self._execute(query, args, 0, timeout)", "response": "Run a query and return the results as a list of Record instances."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a query and return a value in the first row.", "response": "async def fetchval(self, query, *args, column=0, timeout=None):\n        \"\"\"Run a query and return a value in the first row.\n\n        :param str query: Query text.\n        :param args: Query arguments.\n        :param int column: Numeric index within the record of the value to\n                           return (defaults to 0).\n        :param float timeout: Optional timeout value in seconds.\n                            If not specified, defaults to the value of\n                            ``command_timeout`` argument to the ``Connection``\n                            instance constructor.\n\n        :return: The value of the specified column of the first record, or\n                 None if no records were returned by the query.\n        \"\"\"\n        self._check_open()\n        data = await self._execute(query, args, 1, timeout)\n        if not data:\n            return None\n        return data[0][column]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def fetchrow(self, query, *args, timeout=None):\n        self._check_open()\n        data = await self._execute(query, args, 1, timeout)\n        if not data:\n            return None\n        return data[0]", "response": "Run a query and return the first row."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def copy_from_table(self, table_name, *, output,\n                              columns=None, schema_name=None, timeout=None,\n                              format=None, oids=None, delimiter=None,\n                              null=None, header=None, quote=None,\n                              escape=None, force_quote=None, encoding=None):\n        \"\"\"Copy table contents to a file or file-like object.\n\n        :param str table_name:\n            The name of the table to copy data from.\n\n        :param output:\n            A :term:`path-like object <python:path-like object>`,\n            or a :term:`file-like object <python:file-like object>`, or\n            a :term:`coroutine function <python:coroutine function>`\n            that takes a ``bytes`` instance as a sole argument.\n\n        :param list columns:\n            An optional list of column names to copy.\n\n        :param str schema_name:\n            An optional schema name to qualify the table.\n\n        :param float timeout:\n            Optional timeout value in seconds.\n\n        The remaining keyword arguments are ``COPY`` statement options,\n        see `COPY statement documentation`_ for details.\n\n        :return: The status string of the COPY command.\n\n        Example:\n\n        .. code-block:: pycon\n\n            >>> import asyncpg\n            >>> import asyncio\n            >>> async def run():\n            ...     con = await asyncpg.connect(user='postgres')\n            ...     result = await con.copy_from_table(\n            ...         'mytable', columns=('foo', 'bar'),\n            ...         output='file.csv', format='csv')\n            ...     print(result)\n            ...\n            >>> asyncio.get_event_loop().run_until_complete(run())\n            'COPY 100'\n\n        .. _`COPY statement documentation`:\n            https://www.postgresql.org/docs/current/static/sql-copy.html\n\n        .. versionadded:: 0.11.0\n        \"\"\"\n        tabname = utils._quote_ident(table_name)\n        if schema_name:\n            tabname = utils._quote_ident(schema_name) + '.' + tabname\n\n        if columns:\n            cols = '({})'.format(\n                ', '.join(utils._quote_ident(c) for c in columns))\n        else:\n            cols = ''\n\n        opts = self._format_copy_opts(\n            format=format, oids=oids, delimiter=delimiter,\n            null=null, header=header, quote=quote, escape=escape,\n            force_quote=force_quote, encoding=encoding\n        )\n\n        copy_stmt = 'COPY {tab}{cols} TO STDOUT {opts}'.format(\n            tab=tabname, cols=cols, opts=opts)\n\n        return await self._copy_out(copy_stmt, output, timeout)", "response": "Copy data from a table to a file or file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def copy_from_query(self, query, *args, output,\n                              timeout=None, format=None, oids=None,\n                              delimiter=None, null=None, header=None,\n                              quote=None, escape=None, force_quote=None,\n                              encoding=None):\n        \"\"\"Copy the results of a query to a file or file-like object.\n\n        :param str query:\n            The query to copy the results of.\n\n        :param args:\n            Query arguments.\n\n        :param output:\n            A :term:`path-like object <python:path-like object>`,\n            or a :term:`file-like object <python:file-like object>`, or\n            a :term:`coroutine function <python:coroutine function>`\n            that takes a ``bytes`` instance as a sole argument.\n\n        :param float timeout:\n            Optional timeout value in seconds.\n\n        The remaining keyword arguments are ``COPY`` statement options,\n        see `COPY statement documentation`_ for details.\n\n        :return: The status string of the COPY command.\n\n        Example:\n\n        .. code-block:: pycon\n\n            >>> import asyncpg\n            >>> import asyncio\n            >>> async def run():\n            ...     con = await asyncpg.connect(user='postgres')\n            ...     result = await con.copy_from_query(\n            ...         'SELECT foo, bar FROM mytable WHERE foo > $1', 10,\n            ...         output='file.csv', format='csv')\n            ...     print(result)\n            ...\n            >>> asyncio.get_event_loop().run_until_complete(run())\n            'COPY 10'\n\n        .. _`COPY statement documentation`:\n            https://www.postgresql.org/docs/current/static/sql-copy.html\n\n        .. versionadded:: 0.11.0\n        \"\"\"\n        opts = self._format_copy_opts(\n            format=format, oids=oids, delimiter=delimiter,\n            null=null, header=header, quote=quote, escape=escape,\n            force_quote=force_quote, encoding=encoding\n        )\n\n        if args:\n            query = await utils._mogrify(self, query, args)\n\n        copy_stmt = 'COPY ({query}) TO STDOUT {opts}'.format(\n            query=query, opts=opts)\n\n        return await self._copy_out(copy_stmt, output, timeout)", "response": "Copy the results of a query to a file or file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy data from source to table_name.", "response": "async def copy_to_table(self, table_name, *, source,\n                            columns=None, schema_name=None, timeout=None,\n                            format=None, oids=None, freeze=None,\n                            delimiter=None, null=None, header=None,\n                            quote=None, escape=None, force_quote=None,\n                            force_not_null=None, force_null=None,\n                            encoding=None):\n        \"\"\"Copy data to the specified table.\n\n        :param str table_name:\n            The name of the table to copy data to.\n\n        :param source:\n            A :term:`path-like object <python:path-like object>`,\n            or a :term:`file-like object <python:file-like object>`, or\n            an :term:`asynchronous iterable <python:asynchronous iterable>`\n            that returns ``bytes``, or an object supporting the\n            :ref:`buffer protocol <python:bufferobjects>`.\n\n        :param list columns:\n            An optional list of column names to copy.\n\n        :param str schema_name:\n            An optional schema name to qualify the table.\n\n        :param float timeout:\n            Optional timeout value in seconds.\n\n        The remaining keyword arguments are ``COPY`` statement options,\n        see `COPY statement documentation`_ for details.\n\n        :return: The status string of the COPY command.\n\n        Example:\n\n        .. code-block:: pycon\n\n            >>> import asyncpg\n            >>> import asyncio\n            >>> async def run():\n            ...     con = await asyncpg.connect(user='postgres')\n            ...     result = await con.copy_to_table(\n            ...         'mytable', source='datafile.tbl')\n            ...     print(result)\n            ...\n            >>> asyncio.get_event_loop().run_until_complete(run())\n            'COPY 140000'\n\n        .. _`COPY statement documentation`:\n            https://www.postgresql.org/docs/current/static/sql-copy.html\n\n        .. versionadded:: 0.11.0\n        \"\"\"\n        tabname = utils._quote_ident(table_name)\n        if schema_name:\n            tabname = utils._quote_ident(schema_name) + '.' + tabname\n\n        if columns:\n            cols = '({})'.format(\n                ', '.join(utils._quote_ident(c) for c in columns))\n        else:\n            cols = ''\n\n        opts = self._format_copy_opts(\n            format=format, oids=oids, freeze=freeze, delimiter=delimiter,\n            null=null, header=header, quote=quote, escape=escape,\n            force_not_null=force_not_null, force_null=force_null,\n            encoding=encoding\n        )\n\n        copy_stmt = 'COPY {tab}{cols} FROM STDIN {opts}'.format(\n            tab=tabname, cols=cols, opts=opts)\n\n        return await self._copy_in(copy_stmt, source, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying a list of records to a specified table using binary COPY.", "response": "async def copy_records_to_table(self, table_name, *, records,\n                                    columns=None, schema_name=None,\n                                    timeout=None):\n        \"\"\"Copy a list of records to the specified table using binary COPY.\n\n        :param str table_name:\n            The name of the table to copy data to.\n\n        :param records:\n            An iterable returning row tuples to copy into the table.\n\n        :param list columns:\n            An optional list of column names to copy.\n\n        :param str schema_name:\n            An optional schema name to qualify the table.\n\n        :param float timeout:\n            Optional timeout value in seconds.\n\n        :return: The status string of the COPY command.\n\n        Example:\n\n        .. code-block:: pycon\n\n            >>> import asyncpg\n            >>> import asyncio\n            >>> async def run():\n            ...     con = await asyncpg.connect(user='postgres')\n            ...     result = await con.copy_records_to_table(\n            ...         'mytable', records=[\n            ...             (1, 'foo', 'bar'),\n            ...             (2, 'ham', 'spam')])\n            ...     print(result)\n            ...\n            >>> asyncio.get_event_loop().run_until_complete(run())\n            'COPY 2'\n\n        .. versionadded:: 0.11.0\n        \"\"\"\n        tabname = utils._quote_ident(table_name)\n        if schema_name:\n            tabname = utils._quote_ident(schema_name) + '.' + tabname\n\n        if columns:\n            col_list = ', '.join(utils._quote_ident(c) for c in columns)\n            cols = '({})'.format(col_list)\n        else:\n            col_list = '*'\n            cols = ''\n\n        intro_query = 'SELECT {cols} FROM {tab} LIMIT 1'.format(\n            tab=tabname, cols=col_list)\n\n        intro_ps = await self._prepare(intro_query, use_cache=True)\n\n        opts = '(FORMAT binary)'\n\n        copy_stmt = 'COPY {tab}{cols} FROM STDIN {opts}'.format(\n            tab=tabname, cols=cols, opts=opts)\n\n        return await self._copy_in_records(\n            copy_stmt, records, intro_ps._state, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def set_type_codec(self, typename, *,\n                             schema='public', encoder, decoder,\n                             format='text'):\n        \"\"\"Set an encoder/decoder pair for the specified data type.\n\n        :param typename:\n            Name of the data type the codec is for.\n\n        :param schema:\n            Schema name of the data type the codec is for\n            (defaults to ``'public'``)\n\n        :param format:\n            The type of the argument received by the *decoder* callback,\n            and the type of the *encoder* callback return value.\n\n            If *format* is ``'text'`` (the default), the exchange datum is a\n            ``str`` instance containing valid text representation of the\n            data type.\n\n            If *format* is ``'binary'``, the exchange datum is a ``bytes``\n            instance containing valid _binary_ representation of the\n            data type.\n\n            If *format* is ``'tuple'``, the exchange datum is a type-specific\n            ``tuple`` of values.  The table below lists supported data\n            types and their format for this mode.\n\n            +-----------------+---------------------------------------------+\n            |  Type           |                Tuple layout                 |\n            +=================+=============================================+\n            | ``interval``    | (``months``, ``days``, ``microseconds``)    |\n            +-----------------+---------------------------------------------+\n            | ``date``        | (``date ordinal relative to Jan 1 2000``,)  |\n            |                 | ``-2^31`` for negative infinity timestamp   |\n            |                 | ``2^31-1`` for positive infinity timestamp. |\n            +-----------------+---------------------------------------------+\n            | ``timestamp``   | (``microseconds relative to Jan 1 2000``,)  |\n            |                 | ``-2^63`` for negative infinity timestamp   |\n            |                 | ``2^63-1`` for positive infinity timestamp. |\n            +-----------------+---------------------------------------------+\n            | ``timestamp     | (``microseconds relative to Jan 1 2000      |\n            | with time zone``| UTC``,)                                     |\n            |                 | ``-2^63`` for negative infinity timestamp   |\n            |                 | ``2^63-1`` for positive infinity timestamp. |\n            +-----------------+---------------------------------------------+\n            | ``time``        | (``microseconds``,)                         |\n            +-----------------+---------------------------------------------+\n            | ``time with     | (``microseconds``,                          |\n            | time zone``     | ``time zone offset in seconds``)            |\n            +-----------------+---------------------------------------------+\n\n        :param encoder:\n            Callable accepting a Python object as a single argument and\n            returning a value encoded according to *format*.\n\n        :param decoder:\n            Callable accepting a single argument encoded according to *format*\n            and returning a decoded Python object.\n\n        Example:\n\n        .. code-block:: pycon\n\n            >>> import asyncpg\n            >>> import asyncio\n            >>> import datetime\n            >>> from dateutil.relativedelta import relativedelta\n            >>> async def run():\n            ...     con = await asyncpg.connect(user='postgres')\n            ...     def encoder(delta):\n            ...         ndelta = delta.normalized()\n            ...         return (ndelta.years * 12 + ndelta.months,\n            ...                 ndelta.days,\n            ...                 ((ndelta.hours * 3600 +\n            ...                    ndelta.minutes * 60 +\n            ...                    ndelta.seconds) * 1000000 +\n            ...                  ndelta.microseconds))\n            ...     def decoder(tup):\n            ...         return relativedelta(months=tup[0], days=tup[1],\n            ...                              microseconds=tup[2])\n            ...     await con.set_type_codec(\n            ...         'interval', schema='pg_catalog', encoder=encoder,\n            ...         decoder=decoder, format='tuple')\n            ...     result = await con.fetchval(\n            ...         \"SELECT '2 years 3 mons 1 day'::interval\")\n            ...     print(result)\n            ...     print(datetime.datetime(2002, 1, 1) + result)\n            ...\n            >>> asyncio.get_event_loop().run_until_complete(run())\n            relativedelta(years=+2, months=+3, days=+1)\n            2004-04-02 00:00:00\n\n        .. versionadded:: 0.12.0\n            Added the ``format`` keyword argument and support for 'tuple'\n            format.\n\n        .. versionchanged:: 0.12.0\n            The ``binary`` keyword argument is deprecated in favor of\n            ``format``.\n\n        .. versionchanged:: 0.13.0\n            The ``binary`` keyword argument was removed in favor of\n            ``format``.\n        \"\"\"\n        self._check_open()\n\n        typeinfo = await self.fetchrow(\n            introspection.TYPE_BY_NAME, typename, schema)\n        if not typeinfo:\n            raise ValueError('unknown type: {}.{}'.format(schema, typename))\n\n        if not introspection.is_scalar_type(typeinfo):\n            raise ValueError(\n                'cannot use custom codec on non-scalar type {}.{}'.format(\n                    schema, typename))\n\n        oid = typeinfo['oid']\n        self._protocol.get_settings().add_python_codec(\n            oid, typename, schema, 'scalar',\n            encoder, decoder, format)\n\n        # Statement cache is no longer valid due to codec changes.\n        self._drop_local_statement_cache()", "response": "Set an encoder and decoder pair for the specified data type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def reset_type_codec(self, typename, *, schema='public'):\n\n        typeinfo = await self.fetchrow(\n            introspection.TYPE_BY_NAME, typename, schema)\n        if not typeinfo:\n            raise ValueError('unknown type: {}.{}'.format(schema, typename))\n\n        oid = typeinfo['oid']\n\n        self._protocol.get_settings().remove_python_codec(\n            oid, typename, schema)\n\n        # Statement cache is no longer valid due to codec changes.\n        self._drop_local_statement_cache()", "response": "Reset the codec for the given data type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def set_builtin_type_codec(self, typename, *,\n                                     schema='public', codec_name,\n                                     format=None):\n        \"\"\"Set a builtin codec for the specified scalar data type.\n\n        This method has two uses.  The first is to register a builtin\n        codec for an extension type without a stable OID, such as 'hstore'.\n        The second use is to declare that an extension type or a\n        user-defined type is wire-compatible with a certain builtin\n        data type and should be exchanged as such.\n\n        :param typename:\n            Name of the data type the codec is for.\n\n        :param schema:\n            Schema name of the data type the codec is for\n            (defaults to ``'public'``).\n\n        :param codec_name:\n            The name of the builtin codec to use for the type.\n            This should be either the name of a known core type\n            (such as ``\"int\"``), or the name of a supported extension\n            type.  Currently, the only supported extension type is\n            ``\"pg_contrib.hstore\"``.\n\n        :param format:\n            If *format* is ``None`` (the default), all formats supported\n            by the target codec are declared to be supported for *typename*.\n            If *format* is ``'text'`` or ``'binary'``, then only the\n            specified format is declared to be supported for *typename*.\n\n        .. versionchanged:: 0.18.0\n            The *codec_name* argument can be the name of any known\n            core data type.  Added the *format* keyword argument.\n        \"\"\"\n        self._check_open()\n\n        typeinfo = await self.fetchrow(\n            introspection.TYPE_BY_NAME, typename, schema)\n        if not typeinfo:\n            raise exceptions.InterfaceError(\n                'unknown type: {}.{}'.format(schema, typename))\n\n        if not introspection.is_scalar_type(typeinfo):\n            raise exceptions.InterfaceError(\n                'cannot alias non-scalar type {}.{}'.format(\n                    schema, typename))\n\n        oid = typeinfo['oid']\n\n        self._protocol.get_settings().set_builtin_type_codec(\n            oid, typename, schema, 'scalar', codec_name, format)\n\n        # Statement cache is no longer valid due to codec changes.\n        self._drop_local_statement_cache()", "response": "Set a builtin codec for the specified scalar data type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncloses the connection gracefully.", "response": "async def close(self, *, timeout=None):\n        \"\"\"Close the connection gracefully.\n\n        :param float timeout:\n            Optional timeout value in seconds.\n\n        .. versionchanged:: 0.14.0\n           Added the *timeout* parameter.\n        \"\"\"\n        try:\n            if not self.is_closed():\n                await self._protocol.close(timeout)\n        except Exception:\n            # If we fail to close gracefully, abort the connection.\n            self._abort()\n            raise\n        finally:\n            self._cleanup()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _release(self):\n        if self._in_use is None:\n            # The holder is not checked out.\n            return\n\n        if not self._in_use.done():\n            self._in_use.set_result(None)\n        self._in_use = None\n\n        # Deinitialize the connection proxy.  All subsequent\n        # operations on it will fail.\n        if self._proxy is not None:\n            self._proxy._detach()\n            self._proxy = None\n\n        # Put ourselves back to the pool queue.\n        self._pool._queue.put_nowait(self)", "response": "Release this connection holder."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the new connection arguments for this pool.", "response": "def set_connect_args(self, dsn=None, **connect_kwargs):\n        r\"\"\"Set the new connection arguments for this pool.\n\n        The new connection arguments will be used for all subsequent\n        new connection attempts.  Existing connections will remain until\n        they expire. Use :meth:`Pool.expire_connections()\n        <asyncpg.pool.Pool.expire_connections>` to expedite the connection\n        expiry.\n\n        :param str dsn:\n            Connection arguments specified using as a single string in\n            the following format:\n            ``postgres://user:pass@host:port/database?option=value``.\n\n        :param \\*\\*connect_kwargs:\n            Keyword arguments for the :func:`~asyncpg.connection.connect`\n            function.\n\n        .. versionadded:: 0.16.0\n        \"\"\"\n\n        self._connect_args = [dsn]\n        self._connect_kwargs = connect_kwargs\n        self._working_addr = None\n        self._working_config = None\n        self._working_params = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes an SQL command or commands.", "response": "async def execute(self, query: str, *args, timeout: float=None) -> str:\n        \"\"\"Execute an SQL command (or commands).\n\n        Pool performs this operation using one of its connections.  Other than\n        that, it behaves identically to\n        :meth:`Connection.execute() <connection.Connection.execute>`.\n\n        .. versionadded:: 0.10.0\n        \"\"\"\n        async with self.acquire() as con:\n            return await con.execute(query, *args, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def executemany(self, command: str, args, *, timeout: float=None):\n        async with self.acquire() as con:\n            return await con.executemany(command, args, timeout=timeout)", "response": "Execute an SQL command for each sequence of arguments in args*."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a query and return the results as a list of Record objects.", "response": "async def fetch(self, query, *args, timeout=None) -> list:\n        \"\"\"Run a query and return the results as a list of :class:`Record`.\n\n        Pool performs this operation using one of its connections.  Other than\n        that, it behaves identically to\n        :meth:`Connection.fetch() <connection.Connection.fetch>`.\n\n        .. versionadded:: 0.10.0\n        \"\"\"\n        async with self.acquire() as con:\n            return await con.fetch(query, *args, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def fetchval(self, query, *args, column=0, timeout=None):\n        async with self.acquire() as con:\n            return await con.fetchval(\n                query, *args, column=column, timeout=timeout)", "response": "Run a query and return a value in the first row."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns a query and return the first row.", "response": "async def fetchrow(self, query, *args, timeout=None):\n        \"\"\"Run a query and return the first row.\n\n        Pool performs this operation using one of its connections.  Other than\n        that, it behaves identically to\n        :meth:`Connection.fetchrow() <connection.Connection.fetchrow>`.\n\n        .. versionadded:: 0.10.0\n        \"\"\"\n        async with self.acquire() as con:\n            return await con.fetchrow(query, *args, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrelease a database connection back to the pool.", "response": "async def release(self, connection, *, timeout=None):\n        \"\"\"Release a database connection back to the pool.\n\n        :param Connection connection:\n            A :class:`~asyncpg.connection.Connection` object to release.\n        :param float timeout:\n            A timeout for releasing the connection.  If not specified, defaults\n            to the timeout provided in the corresponding call to the\n            :meth:`Pool.acquire() <asyncpg.pool.Pool.acquire>` method.\n\n        .. versionchanged:: 0.14.0\n            Added the *timeout* parameter.\n        \"\"\"\n        if (type(connection) is not PoolConnectionProxy or\n                connection._holder._pool is not self):\n            raise exceptions.InterfaceError(\n                'Pool.release() received invalid connection: '\n                '{connection!r} is not a member of this pool'.format(\n                    connection=connection))\n\n        if connection._con is None:\n            # Already released, do nothing.\n            return\n\n        self._check_init()\n\n        # Let the connection do its internal housekeeping when its released.\n        connection._con._on_release()\n\n        ch = connection._holder\n        if timeout is None:\n            timeout = ch._timeout\n\n        # Use asyncio.shield() to guarantee that task cancellation\n        # does not prevent the connection from being returned to the\n        # pool properly.\n        return await asyncio.shield(ch.release(timeout), loop=self._loop)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def close(self):\n        if self._closed:\n            return\n        self._check_init()\n\n        self._closing = True\n\n        warning_callback = None\n        try:\n            warning_callback = self._loop.call_later(\n                60, self._warn_on_long_close)\n\n            release_coros = [\n                ch.wait_until_released() for ch in self._holders]\n            await asyncio.gather(*release_coros, loop=self._loop)\n\n            close_coros = [\n                ch.close() for ch in self._holders]\n            await asyncio.gather(*close_coros, loop=self._loop)\n\n        except Exception:\n            self.terminate()\n            raise\n\n        finally:\n            if warning_callback is not None:\n                warning_callback.cancel()\n            self._closed = True\n            self._closing = False", "response": "Attempt to gracefully close all connections in the pool."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef terminate(self):\n        if self._closed:\n            return\n        self._check_init()\n        for ch in self._holders:\n            ch.terminate()\n        self._closed = True", "response": "Terminate all connections in the pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the transaction or savepoint block.", "response": "async def start(self):\n        \"\"\"Enter the transaction or savepoint block.\"\"\"\n        self.__check_state_base('start')\n        if self._state is TransactionState.STARTED:\n            raise apg_errors.InterfaceError(\n                'cannot start; the transaction is already started')\n\n        con = self._connection\n\n        if con._top_xact is None:\n            if con._protocol.is_in_transaction():\n                raise apg_errors.InterfaceError(\n                    'cannot use Connection.transaction() in '\n                    'a manually started transaction')\n            con._top_xact = self\n        else:\n            # Nested transaction block\n            top_xact = con._top_xact\n            if self._isolation != top_xact._isolation:\n                raise apg_errors.InterfaceError(\n                    'nested transaction has a different isolation level: '\n                    'current {!r} != outer {!r}'.format(\n                        self._isolation, top_xact._isolation))\n            self._nested = True\n\n        if self._nested:\n            self._id = con._get_unique_id('savepoint')\n            query = 'SAVEPOINT {};'.format(self._id)\n        else:\n            if self._isolation == 'read_committed':\n                query = 'BEGIN;'\n            elif self._isolation == 'repeatable_read':\n                query = 'BEGIN ISOLATION LEVEL REPEATABLE READ;'\n            else:\n                query = 'BEGIN ISOLATION LEVEL SERIALIZABLE'\n                if self._readonly:\n                    query += ' READ ONLY'\n                if self._deferrable:\n                    query += ' DEFERRABLE'\n                query += ';'\n\n        try:\n            await self._connection.execute(query)\n        except BaseException:\n            self._state = TransactionState.FAILED\n            raise\n        else:\n            self._state = TransactionState.STARTED"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_statusmsg(self) -> str:\n        if self._last_status is None:\n            return self._last_status\n        return self._last_status.decode()", "response": "Return the status of the executed command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a cursor factory for the prepared statement.", "response": "def cursor(self, *args, prefetch=None,\n               timeout=None) -> cursor.CursorFactory:\n        \"\"\"Return a *cursor factory* for the prepared statement.\n\n        :param args: Query arguments.\n        :param int prefetch: The number of rows the *cursor iterator*\n                             will prefetch (defaults to ``50``.)\n        :param float timeout: Optional timeout in seconds.\n\n        :return: A :class:`~cursor.CursorFactory` object.\n        \"\"\"\n        return cursor.CursorFactory(self._connection, self._query,\n                                    self._state, args, prefetch,\n                                    timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the execution plan of the SQL query.", "response": "async def explain(self, *args, analyze=False):\n        \"\"\"Return the execution plan of the statement.\n\n        :param args: Query arguments.\n        :param analyze: If ``True``, the statement will be executed and\n                        the run time statitics added to the return value.\n\n        :return: An object representing the execution plan.  This value\n                 is actually a deserialized JSON output of the SQL\n                 ``EXPLAIN`` command.\n        \"\"\"\n        query = 'EXPLAIN (FORMAT JSON, VERBOSE'\n        if analyze:\n            query += ', ANALYZE) '\n        else:\n            query += ') '\n        query += self._state.query\n\n        if analyze:\n            # From PostgreSQL docs:\n            # Important: Keep in mind that the statement is actually\n            # executed when the ANALYZE option is used. Although EXPLAIN\n            # will discard any output that a SELECT would return, other\n            # side effects of the statement will happen as usual. If you\n            # wish to use EXPLAIN ANALYZE on an INSERT, UPDATE, DELETE,\n            # CREATE TABLE AS, or EXECUTE statement without letting the\n            # command affect your data, use this approach:\n            #     BEGIN;\n            #     EXPLAIN ANALYZE ...;\n            #     ROLLBACK;\n            tr = self._connection.transaction()\n            await tr.start()\n            try:\n                data = await self._connection.fetchval(query, *args)\n            finally:\n                await tr.rollback()\n        else:\n            data = await self._connection.fetchval(query, *args)\n\n        return json.loads(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting the statement and return a list of Record objects.", "response": "async def fetch(self, *args, timeout=None):\n        r\"\"\"Execute the statement and return a list of :class:`Record` objects.\n\n        :param str query: Query text\n        :param args: Query arguments\n        :param float timeout: Optional timeout value in seconds.\n\n        :return: A list of :class:`Record` instances.\n        \"\"\"\n        data = await self.__bind_execute(args, 0, timeout)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute the statement and return a value in the first row.", "response": "async def fetchval(self, *args, column=0, timeout=None):\n        \"\"\"Execute the statement and return a value in the first row.\n\n        :param args: Query arguments.\n        :param int column: Numeric index within the record of the value to\n                           return (defaults to 0).\n        :param float timeout: Optional timeout value in seconds.\n                            If not specified, defaults to the value of\n                            ``command_timeout`` argument to the ``Connection``\n                            instance constructor.\n\n        :return: The value of the specified column of the first record.\n        \"\"\"\n        data = await self.__bind_execute(args, 1, timeout)\n        if not data:\n            return None\n        return data[0][column]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def fetchrow(self, *args, timeout=None):\n        data = await self.__bind_execute(args, 1, timeout)\n        if not data:\n            return None\n        return data[0]", "response": "Execute the statement and return the first row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the password from pgpass file and return the matching password.", "response": "def _read_password_from_pgpass(\n        *, passfile: typing.Optional[pathlib.Path],\n        hosts: typing.List[str],\n        ports: typing.List[int],\n        database: str,\n        user: str):\n    \"\"\"Parse the pgpass file and return the matching password.\n\n    :return:\n        Password string, if found, ``None`` otherwise.\n    \"\"\"\n\n    passtab = _read_password_file(passfile)\n    if not passtab:\n        return None\n\n    for host, port in zip(hosts, ports):\n        if host.startswith('/'):\n            # Unix sockets get normalized into 'localhost'\n            host = 'localhost'\n\n        for phost, pport, pdatabase, puser, ppassword in passtab:\n            if phost != '*' and phost != host:\n                continue\n            if pport != '*' and pport != str(port):\n                continue\n            if pdatabase != '*' and pdatabase != database:\n                continue\n            if puser != '*' and puser != user:\n                continue\n\n            # Found a match.\n            return ppassword\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def _mogrify(conn, query, args):\n    # Introspect the target query for argument types and\n    # build a list of safely-quoted fully-qualified type names.\n    ps = await conn.prepare(query)\n    paramtypes = []\n    for t in ps.get_parameters():\n        if t.name.endswith('[]'):\n            pname = '_' + t.name[:-2]\n        else:\n            pname = t.name\n\n        paramtypes.append('{}.{}'.format(\n            _quote_ident(t.schema), _quote_ident(pname)))\n    del ps\n\n    # Use Postgres to convert arguments to text representation\n    # by casting each value to text.\n    cols = ['quote_literal(${}::{}::text)'.format(i, t)\n            for i, t in enumerate(paramtypes, start=1)]\n\n    textified = await conn.fetchrow(\n        'SELECT {cols}'.format(cols=', '.join(cols)), *args)\n\n    # Finally, replace $n references with text values.\n    return re.sub(\n        r'\\$(\\d+)\\b', lambda m: textified[int(m.group(1)) - 1], query)", "response": "Safely inline arguments to query text."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n\n    global totalMethod\n\n    formatNames = list(formats.keys())\n    formatNames.sort()\n\n    themeNames = list(themes.keys())\n    themeNames.sort()\n\n    labelNames = list(labels.keys())\n    labelNames.sort()\n\n    optparser = optparse.OptionParser(\n        usage=\"\\n\\t%prog [options] [file] ...\")\n    optparser.add_option(\n        '-o', '--output', metavar='FILE',\n        type=\"string\", dest=\"output\",\n        help=\"output filename [stdout]\")\n    optparser.add_option(\n        '-n', '--node-thres', metavar='PERCENTAGE',\n        type=\"float\", dest=\"node_thres\", default=0.5,\n        help=\"eliminate nodes below this threshold [default: %default]\")\n    optparser.add_option(\n        '-e', '--edge-thres', metavar='PERCENTAGE',\n        type=\"float\", dest=\"edge_thres\", default=0.1,\n        help=\"eliminate edges below this threshold [default: %default]\")\n    optparser.add_option(\n        '-f', '--format',\n        type=\"choice\", choices=formatNames,\n        dest=\"format\", default=\"prof\",\n        help=\"profile format: %s [default: %%default]\" % naturalJoin(formatNames))\n    optparser.add_option(\n        '--total',\n        type=\"choice\", choices=('callratios', 'callstacks'),\n        dest=\"totalMethod\", default=totalMethod,\n        help=\"preferred method of calculating total time: callratios or callstacks (currently affects only perf format) [default: %default]\")\n    optparser.add_option(\n        '-c', '--colormap',\n        type=\"choice\", choices=themeNames,\n        dest=\"theme\", default=\"color\",\n        help=\"color map: %s [default: %%default]\" % naturalJoin(themeNames))\n    optparser.add_option(\n        '-s', '--strip',\n        action=\"store_true\",\n        dest=\"strip\", default=False,\n        help=\"strip function parameters, template parameters, and const modifiers from demangled C++ function names\")\n    optparser.add_option(\n        '--color-nodes-by-selftime',\n        action=\"store_true\",\n        dest=\"color_nodes_by_selftime\", default=False,\n        help=\"color nodes by self time, rather than by total time (sum of self and descendants)\")\n    optparser.add_option(\n        '--colour-nodes-by-selftime',\n        action=\"store_true\",\n        dest=\"color_nodes_by_selftime\",\n        help=optparse.SUPPRESS_HELP)\n    optparser.add_option(\n        '-w', '--wrap',\n        action=\"store_true\",\n        dest=\"wrap\", default=False,\n        help=\"wrap function names\")\n    optparser.add_option(\n        '--show-samples',\n        action=\"store_true\",\n        dest=\"show_samples\", default=False,\n        help=\"show function samples\")\n    optparser.add_option(\n        '--node-label', metavar='MEASURE',\n        type='choice', choices=labelNames,\n        action='append',\n        dest='node_labels',\n        help=\"measurements to on show the node (can be specified multiple times): %s [default: %s]\" % (\n            naturalJoin(labelNames), ', '.join(defaultLabelNames)))\n    # add option to create subtree or show paths\n    optparser.add_option(\n        '-z', '--root',\n        type=\"string\",\n        dest=\"root\", default=\"\",\n        help=\"prune call graph to show only descendants of specified root function\")\n    optparser.add_option(\n        '-l', '--leaf',\n        type=\"string\",\n        dest=\"leaf\", default=\"\",\n        help=\"prune call graph to show only ancestors of specified leaf function\")\n    optparser.add_option(\n        '--depth',\n        type=\"int\",\n        dest=\"depth\", default=-1,\n        help=\"prune call graph to show only descendants or ancestors until specified depth\")\n    # add a new option to control skew of the colorization curve\n    optparser.add_option(\n        '--skew',\n        type=\"float\", dest=\"theme_skew\", default=1.0,\n        help=\"skew the colorization curve.  Values < 1.0 give more variety to lower percentages.  Values > 1.0 give less variety to lower percentages\")\n    # add option for filtering by file path\n    optparser.add_option(\n        '-p', '--path', action=\"append\",\n        type=\"string\", dest=\"filter_paths\",\n        help=\"Filter all modules not in a specified path\")\n    (options, args) = optparser.parse_args(sys.argv[1:])\n\n    if len(args) > 1 and options.format != 'pstats':\n        optparser.error('incorrect number of arguments')\n\n    try:\n        theme = themes[options.theme]\n    except KeyError:\n        optparser.error('invalid colormap \\'%s\\'' % options.theme)\n\n    # set skew on the theme now that it has been picked.\n    if options.theme_skew:\n        theme.skew = options.theme_skew\n\n    totalMethod = options.totalMethod\n\n    try:\n        Format = formats[options.format]\n    except KeyError:\n        optparser.error('invalid format \\'%s\\'' % options.format)\n\n    if Format.stdinInput:\n        if not args:\n            fp = sys.stdin\n        elif PYTHON_3:\n            fp = open(args[0], 'rt', encoding='UTF-8')\n        else:\n            fp = open(args[0], 'rt')\n        parser = Format(fp)\n    elif Format.multipleInput:\n        if not args:\n            optparser.error('at least a file must be specified for %s input' % options.format)\n        parser = Format(*args)\n    else:\n        if len(args) != 1:\n            optparser.error('exactly one file must be specified for %s input' % options.format)\n        parser = Format(args[0])\n\n    profile = parser.parse()\n\n    if options.output is None:\n        if PYTHON_3:\n            output = open(sys.stdout.fileno(), mode='wt', encoding='UTF-8', closefd=False)\n        else:\n            output = sys.stdout\n    else:\n        if PYTHON_3:\n            output = open(options.output, 'wt', encoding='UTF-8')\n        else:\n            output = open(options.output, 'wt')\n\n    dot = DotWriter(output)\n    dot.strip = options.strip\n    dot.wrap = options.wrap\n\n    labelNames = options.node_labels or defaultLabelNames\n    dot.show_function_events = [labels[l] for l in labelNames]\n    if options.show_samples:\n        dot.show_function_events.append(SAMPLES)\n\n    profile = profile\n    profile.prune(options.node_thres/100.0, options.edge_thres/100.0, options.filter_paths, options.color_nodes_by_selftime)\n\n    if options.root:\n        rootIds = profile.getFunctionIds(options.root)\n        if not rootIds:\n            sys.stderr.write('root node ' + options.root + ' not found (might already be pruned : try -e0 -n0 flags)\\n')\n            sys.exit(1)\n        profile.prune_root(rootIds, options.depth)\n    if options.leaf:\n        leafIds = profile.getFunctionIds(options.leaf)\n        if not leafIds:\n            sys.stderr.write('leaf node ' + options.leaf + ' not found (maybe already pruned : try -e0 -n0 flags)\\n')\n            sys.exit(1)\n        profile.prune_leaf(leafIds, options.depth)\n\n    dot.graph(profile, theme)", "response": "Main function for the\n    program."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\naggregating two event values.", "response": "def aggregate(self, val1, val2):\n        \"\"\"Aggregate two event values.\"\"\"\n        assert val1 is not None\n        assert val2 is not None\n        return self._aggregator(val1, val2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stripped_name(self):\n\n        name = self.name\n\n        # Strip function parameters from name by recursively removing paired parenthesis\n        while True:\n            name, n = self._parenthesis_re.subn('', name)\n            if not n:\n                break\n\n        # Strip const qualifier\n        name = self._const_re.sub('', name)\n\n        # Strip template parameters from name by recursively removing paired angles\n        while True:\n            name, n = self._angles_re.subn('', name)\n            if not n:\n                break\n\n        return name", "response": "Remove extraneous information from C ++ demangled function names."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind cycles using Tarjan s strongly connected components algorithm.", "response": "def find_cycles(self):\n        \"\"\"Find cycles using Tarjan's strongly connected components algorithm.\"\"\"\n\n        # Apply the Tarjan's algorithm successively until all functions are visited\n        stack = []\n        data = {}\n        order = 0\n        for function in compat_itervalues(self.functions):\n            order = self._tarjan(function, order, stack, data)\n        cycles = []\n        for function in compat_itervalues(self.functions):\n            if function.cycle is not None and function.cycle not in cycles:\n                cycles.append(function.cycle)\n        self.cycles = cycles\n        if 0:\n            for cycle in cycles:\n                sys.stderr.write(\"Cycle:\\n\")\n                for member in cycle.functions:\n                    sys.stderr.write(\"\\tFunction %s\\n\" % member.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tarjan(self, function, order, stack, data):\n\n        try:\n            func_data = data[function.id]\n            return order\n        except KeyError:\n            func_data = self._TarjanData(order)\n            data[function.id] = func_data\n        order += 1\n        pos = len(stack)\n        stack.append(function)\n        func_data.onstack = True\n        for call in compat_itervalues(function.calls):\n            try:\n                callee_data = data[call.callee_id]\n                if callee_data.onstack:\n                    func_data.lowlink = min(func_data.lowlink, callee_data.order)\n            except KeyError:\n                callee = self.functions[call.callee_id]\n                order = self._tarjan(callee, order, stack, data)\n                callee_data = data[call.callee_id]\n                func_data.lowlink = min(func_data.lowlink, callee_data.lowlink)\n        if func_data.lowlink == func_data.order:\n            # Strongly connected component found\n            members = stack[pos:]\n            del stack[pos:]\n            if len(members) > 1:\n                cycle = Cycle()\n                for member in members:\n                    cycle.add_function(member)\n                    data[member.id].onstack = False\n            else:\n                for member in members:\n                    data[member.id].onstack = False\n        return order", "response": "Tarjan s strongly connected components algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nintegrates function time ratio along the function calls.", "response": "def integrate(self, outevent, inevent):\n        \"\"\"Propagate function time ratio along the function calls.\n\n        Must be called after finding the cycles.\n\n        See also:\n        - http://citeseer.ist.psu.edu/graham82gprof.html\n        \"\"\"\n\n        # Sanity checking\n        assert outevent not in self\n        for function in compat_itervalues(self.functions):\n            assert outevent not in function\n            assert inevent in function\n            for call in compat_itervalues(function.calls):\n                assert outevent not in call\n                if call.callee_id != function.id:\n                    assert call.ratio is not None\n\n        # Aggregate the input for each cycle \n        for cycle in self.cycles:\n            total = inevent.null()\n            for function in compat_itervalues(self.functions):\n                total = inevent.aggregate(total, function[inevent])\n            self[inevent] = total\n\n        # Integrate along the edges\n        total = inevent.null()\n        for function in compat_itervalues(self.functions):\n            total = inevent.aggregate(total, function[inevent])\n            self._integrate_function(function, outevent, inevent)\n        self[outevent] = total"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _rank_cycle_function(self, cycle, function, ranks):\n\n        import heapq\n        Q = []\n        Qd = {}\n        p = {}\n        visited = set([function])\n\n        ranks[function] = 0\n        for call in compat_itervalues(function.calls):\n            if call.callee_id != function.id:\n                callee = self.functions[call.callee_id]\n                if callee.cycle is cycle:\n                    ranks[callee] = 1\n                    item = [ranks[callee], function, callee]\n                    heapq.heappush(Q, item)\n                    Qd[callee] = item\n\n        while Q:\n            cost, parent, member = heapq.heappop(Q)\n            if member not in visited:\n                p[member]= parent\n                visited.add(member)\n                for call in compat_itervalues(member.calls):\n                    if call.callee_id != member.id:\n                        callee = self.functions[call.callee_id]\n                        if callee.cycle is cycle:\n                            member_rank = ranks[member]\n                            rank = ranks.get(callee)\n                            if rank is not None:\n                                if rank > 1 + member_rank:\n                                    rank = 1 + member_rank\n                                    ranks[callee] = rank\n                                    Qd_callee = Qd[callee]\n                                    Qd_callee[0] = rank\n                                    Qd_callee[1] = member\n                                    heapq._siftdown(Q, 0, Q.index(Qd_callee))\n                            else:\n                                rank = 1 + member_rank\n                                ranks[callee] = rank\n                                item = [rank, member, callee]\n                                heapq.heappush(Q, item)\n                                Qd[callee] = item", "response": "Ranks a cycle function in the heap."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\naggregates an event for the whole profile.", "response": "def aggregate(self, event):\n        \"\"\"Aggregate an event for the whole profile.\"\"\"\n\n        total = event.null()\n        for function in compat_itervalues(self.functions):\n            try:\n                total = event.aggregate(total, function[event])\n            except UndefinedEvent:\n                return\n        self[event] = total"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts a structure from a match object while translating the types in the process.", "response": "def translate(self, mo):\n        \"\"\"Extract a structure from a match object, while translating the types in the process.\"\"\"\n        attrs = {}\n        groupdict = mo.groupdict()\n        for name, value in compat_iteritems(groupdict):\n            if value is None:\n                value = None\n            elif self._int_re.match(value):\n                value = int(value)\n            elif self._float_re.match(value):\n                value = float(value)\n            attrs[name] = (value)\n        return Struct(attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_cg(self):\n\n        # skip call graph header\n        line = self.readline()\n        while self._cg_header_re.match(line):\n            line = self.readline()\n\n        # process call graph entries\n        entry_lines = []\n        # An EOF in readline terminates the program without returning.\n        while not self._cg_footer_re.match(line):\n            if line.isspace():\n                self.parse_cg_entry(entry_lines)\n                entry_lines = []\n            else:\n                entry_lines.append(line)            \n            line = self.readline()", "response": "Parse the call graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a color from HSL color - model to RGB.", "response": "def hsl_to_rgb(self, h, s, l):\n        \"\"\"Convert a color from HSL color-model to RGB.\n\n        See also:\n        - http://www.w3.org/TR/css3-color/#hsl-color\n        \"\"\"\n\n        h = h % 1.0\n        s = min(max(s, 0.0), 1.0)\n        l = min(max(l, 0.0), 1.0)\n\n        if l <= 0.5:\n            m2 = l*(s + 1.0)\n        else:\n            m2 = l + s - l*s\n        m1 = l*2.0 - m2\n        r = self._hue_to_rgb(m1, m2, h + 1.0/3.0)\n        g = self._hue_to_rgb(m1, m2, h)\n        b = self._hue_to_rgb(m1, m2, h - 1.0/3.0)\n\n        # Apply gamma correction\n        r **= self.gamma\n        g **= self.gamma\n        b **= self.gamma\n\n        return (r, g, b)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wrap_function_name(self, name):\n\n        if len(name) > 32:\n            ratio = 2.0/3.0\n            height = max(int(len(name)/(1.0 - ratio) + 0.5), 1)\n            width = max(len(name)/height, 32)\n            # TODO: break lines in symbols\n            name = textwrap.fill(name, width, break_long_words=False)\n\n        # Take away spaces\n        name = name.replace(\", \", \",\")\n        name = name.replace(\"> >\", \">>\")\n        name = name.replace(\"> >\", \">>\") # catch consecutive\n\n        return name", "response": "Split the function name on multiple lines."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_cncomment_xml(comment):\n\n    comment = comment.replace(' xmlns=\"http://www.asam.net/mdf/v4\"', \"\")\n    try:\n        comment = ET.fromstring(comment)\n        match = comment.find(\".//TX\")\n        if match is None:\n            common_properties = comment.find(\".//common_properties\")\n            if common_properties is not None:\n                comment = []\n                for e in common_properties:\n                    field = f'{e.get(\"name\")}: {e.text}'\n                    comment.append(field)\n                comment = \"\\n\".join(field)\n            else:\n                comment = \"\"\n        else:\n            comment = match.text or \"\"\n    except ET.ParseError:\n        pass\n\n    return comment", "response": "extracts the CN comment from a xml comment"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef matlab_compatible(name):\n\n    compatible_name = [ch if ch in ALLOWED_MATLAB_CHARS else \"_\" for ch in name]\n    compatible_name = \"\".join(compatible_name)\n\n    if compatible_name[0] not in string.ascii_letters:\n        compatible_name = \"M_\" + compatible_name\n\n    # max variable name is 63 and 3 chars are reserved\n    # for get_unique_name in case of multiple channel name occurence\n    return compatible_name[:60]", "response": "make a channel name compatible with Matlab variable naming\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfasts way to extract strings from mdf versions 2 and 3 TextBlock", "response": "def get_text_v3(address, stream, mapped=False):\n    \"\"\" faster way to extract strings from mdf versions 2 and 3 TextBlock\n\n    Parameters\n    ----------\n    address : int\n        TextBlock address\n    stream : handle\n        file IO handle\n\n    Returns\n    -------\n    text : str\n        unicode string\n\n    \"\"\"\n\n    if address == 0:\n        return \"\"\n\n    if mapped:\n        size, = UINT16_uf(stream, address + 2)\n        text_bytes = stream[address + 4: address + size]\n    else:\n        stream.seek(address + 2)\n        size = UINT16_u(stream.read(2))[0] - 4\n        text_bytes = stream.read(size)\n    try:\n        text = text_bytes.strip(b\" \\r\\t\\n\\0\").decode(\"latin-1\")\n    except UnicodeDecodeError as err:\n        try:\n            from cchardet import detect\n\n            encoding = detect(text_bytes)[\"encoding\"]\n            text = text_bytes.strip(b\" \\r\\t\\n\\0\").decode(encoding)\n        except ImportError:\n            logger.warning(\n                'Unicode exception occured and \"cChardet\" package is '\n                'not installed. Mdf version 3 expects \"latin-1\" '\n                \"strings and this package may detect if a different\"\n                \" encoding was used\"\n            )\n            raise err\n\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfasts way to extract strings from mdf version 4 TextBlock", "response": "def get_text_v4(address, stream, mapped=False):\n    \"\"\" faster way to extract strings from mdf version 4 TextBlock\n\n    Parameters\n    ----------\n    address : int\n        TextBlock address\n    stream : handle\n        file IO handle\n\n    Returns\n    -------\n    text : str\n        unicode string\n\n    \"\"\"\n\n    if address == 0:\n        return \"\"\n\n    if mapped:\n        size, _ = TWO_UINT64_uf(stream, address + 8)\n        text_bytes = stream[address + 24: address + size]\n    else:\n        stream.seek(address + 8)\n        size, _ = TWO_UINT64_u(stream.read(16))\n        text_bytes = stream.read(size - 24)\n    try:\n        text = text_bytes.strip(b\" \\r\\t\\n\\0\").decode(\"utf-8\")\n    except UnicodeDecodeError as err:\n        try:\n            from cchardet import detect\n\n            encoding = detect(text_bytes)[\"encoding\"]\n            text = text_bytes.decode(encoding).strip(\" \\r\\t\\n\\0\")\n        except ImportError:\n            logger.warning(\n                'Unicode exception occured and \"cChardet\" package is '\n                'not installed. Mdf version 4 expects \"utf-8\" '\n                \"strings and this package may detect if a different\"\n                \" encoding was used\"\n            )\n            raise err\n\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts mdf versions 2 and 3 channel data type to numpy dtype format string", "response": "def get_fmt_v3(data_type, size):\n    \"\"\"convert mdf versions 2 and 3 channel data type to numpy dtype format\n    string\n\n    Parameters\n    ----------\n    data_type : int\n        mdf channel data type\n    size : int\n        data bit size\n    Returns\n    -------\n    fmt : str\n        numpy compatible data type format string\n\n    \"\"\"\n    if data_type in {v3c.DATA_TYPE_STRING, v3c.DATA_TYPE_BYTEARRAY}:\n        size = size // 8\n        if data_type == v3c.DATA_TYPE_STRING:\n            fmt = f\"S{size}\"\n        elif data_type == v3c.DATA_TYPE_BYTEARRAY:\n            fmt = f\"({size},)u1\"\n    else:\n        if size <= 8:\n            size = 1\n        elif size <= 16:\n            size = 2\n        elif size <= 32:\n            size = 4\n        elif size <= 64:\n            size = 8\n        else:\n            size = size // 8\n\n        if data_type in (v3c.DATA_TYPE_UNSIGNED_INTEL, v3c.DATA_TYPE_UNSIGNED):\n            fmt = f\"<u{size}\".format()\n\n        elif data_type == v3c.DATA_TYPE_UNSIGNED_MOTOROLA:\n            fmt = f\">u{size}\"\n\n        elif data_type in (v3c.DATA_TYPE_SIGNED_INTEL, v3c.DATA_TYPE_SIGNED):\n            fmt = f\"<i{size}\"\n\n        elif data_type == v3c.DATA_TYPE_SIGNED_MOTOROLA:\n            fmt = f\">i{size}\"\n\n        elif data_type in {\n            v3c.DATA_TYPE_FLOAT,\n            v3c.DATA_TYPE_DOUBLE,\n            v3c.DATA_TYPE_FLOAT_INTEL,\n            v3c.DATA_TYPE_DOUBLE_INTEL,\n        }:\n            fmt = f\"<f{size}\"\n\n        elif data_type in (v3c.DATA_TYPE_FLOAT_MOTOROLA, v3c.DATA_TYPE_DOUBLE_MOTOROLA):\n            fmt = f\">f{size}\"\n\n    return fmt"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fmt_v4(data_type, size, channel_type=v4c.CHANNEL_TYPE_VALUE):\n    if data_type in v4c.NON_SCALAR_TYPES:\n        size = size // 8\n\n        if data_type == v4c.DATA_TYPE_BYTEARRAY:\n            if channel_type == v4c.CHANNEL_TYPE_VALUE:\n                fmt = f\"({size},)u1\"\n            else:\n                if size == 4:\n                    fmt = \"<u4\"\n                elif size == 8:\n                    fmt = \"<u8\"\n\n        elif data_type in v4c.STRING_TYPES:\n            if channel_type == v4c.CHANNEL_TYPE_VALUE:\n                fmt = f\"S{size}\"\n            else:\n                if size == 4:\n                    fmt = \"<u4\"\n                elif size == 8:\n                    fmt = \"<u8\"\n\n        elif data_type == v4c.DATA_TYPE_CANOPEN_DATE:\n            fmt = \"V7\"\n\n        elif data_type == v4c.DATA_TYPE_CANOPEN_TIME:\n            fmt = \"V6\"\n\n    else:\n\n        if size <= 8:\n            size = 1\n        elif size <= 16:\n            size = 2\n        elif size <= 32:\n            size = 4\n        elif size <= 64:\n            size = 8\n        else:\n            size = size // 8\n\n        if data_type == v4c.DATA_TYPE_UNSIGNED_INTEL:\n            fmt = f\"<u{size}\"\n\n        elif data_type == v4c.DATA_TYPE_UNSIGNED_MOTOROLA:\n            fmt = f\">u{size}\"\n\n        elif data_type == v4c.DATA_TYPE_SIGNED_INTEL:\n            fmt = f\"<i{size}\"\n\n        elif data_type == v4c.DATA_TYPE_SIGNED_MOTOROLA:\n            fmt = f\">i{size}\"\n\n        elif data_type == v4c.DATA_TYPE_REAL_INTEL:\n            fmt = f\"<f{size}\"\n\n        elif data_type == v4c.DATA_TYPE_REAL_MOTOROLA:\n            fmt = f\">f{size}\"\n\n    return fmt", "response": "convert mdf version 4 channel data type to numpy dtype format string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert numpy dtype format string to mdf versions 2 and 3", "response": "def fmt_to_datatype_v3(fmt, shape, array=False):\n    \"\"\"convert numpy dtype format string to mdf versions 2 and 3\n    channel data type and size\n\n    Parameters\n    ----------\n    fmt : numpy.dtype\n        numpy data type\n    shape : tuple\n        numpy array shape\n    array : bool\n        disambiguate between bytearray and channel array\n\n    Returns\n    -------\n    data_type, size : int, int\n        integer data type as defined by ASAM MDF and bit size\n\n    \"\"\"\n    size = fmt.itemsize * 8\n\n    if not array and shape[1:] and fmt.itemsize == 1 and fmt.kind == \"u\":\n        data_type = v3c.DATA_TYPE_BYTEARRAY\n        for dim in shape[1:]:\n            size *= dim\n    else:\n        if fmt.kind == \"u\":\n            if fmt.byteorder in \"=<|\":\n                data_type = v3c.DATA_TYPE_UNSIGNED\n            else:\n                data_type = v3c.DATA_TYPE_UNSIGNED_MOTOROLA\n        elif fmt.kind == \"i\":\n            if fmt.byteorder in \"=<|\":\n                data_type = v3c.DATA_TYPE_SIGNED\n            else:\n                data_type = v3c.DATA_TYPE_SIGNED_MOTOROLA\n        elif fmt.kind == \"f\":\n            if fmt.byteorder in \"=<\":\n                if size == 32:\n                    data_type = v3c.DATA_TYPE_FLOAT\n                else:\n                    data_type = v3c.DATA_TYPE_DOUBLE\n            else:\n                if size == 32:\n                    data_type = v3c.DATA_TYPE_FLOAT_MOTOROLA\n                else:\n                    data_type = v3c.DATA_TYPE_DOUBLE_MOTOROLA\n        elif fmt.kind in \"SV\":\n            data_type = v3c.DATA_TYPE_STRING\n        elif fmt.kind == \"b\":\n            data_type = v3c.DATA_TYPE_UNSIGNED\n            size = 1\n        else:\n            message = f\"Unknown type: dtype={fmt}, shape={shape}\"\n            logger.exception(message)\n            raise MdfException(message)\n\n    return data_type, size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap CAN signal to MDF integer types", "response": "def info_to_datatype_v4(signed, little_endian):\n    \"\"\"map CAN signal to MDF integer types\n\n    Parameters\n    ----------\n    signed : bool\n        signal is flagged as signed in the CAN database\n    little_endian : bool\n        signal is flagged as little endian (Intel) in the CAN database\n\n    Returns\n    -------\n    datatype : int\n        integer code for MDF channel data type\n\n    \"\"\"\n\n    if signed:\n        if little_endian:\n            datatype = v4c.DATA_TYPE_SIGNED_INTEL\n        else:\n            datatype = v4c.DATA_TYPE_SIGNED_MOTOROLA\n    else:\n        if little_endian:\n            datatype = v4c.DATA_TYPE_UNSIGNED_INTEL\n        else:\n            datatype = v4c.DATA_TYPE_UNSIGNED_MOTOROLA\n\n    return datatype"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts numpy dtype format string to mdf version 4 channel data type and size", "response": "def fmt_to_datatype_v4(fmt, shape, array=False):\n    \"\"\"convert numpy dtype format string to mdf version 4 channel data\n    type and size\n\n    Parameters\n    ----------\n    fmt : numpy.dtype\n        numpy data type\n    shape : tuple\n        numpy array shape\n    array : bool\n        disambiguate between bytearray and channel array\n\n    Returns\n    -------\n    data_type, size : int, int\n        integer data type as defined by ASAM MDF and bit size\n\n    \"\"\"\n    size = fmt.itemsize * 8\n\n    if not array and shape[1:] and fmt.itemsize == 1 and fmt.kind == \"u\":\n        data_type = v4c.DATA_TYPE_BYTEARRAY\n        for dim in shape[1:]:\n            size *= dim\n\n    else:\n        if fmt.kind == \"u\":\n            if fmt.byteorder in \"=<|\":\n                data_type = v4c.DATA_TYPE_UNSIGNED_INTEL\n            else:\n                data_type = v4c.DATA_TYPE_UNSIGNED_MOTOROLA\n        elif fmt.kind == \"i\":\n            if fmt.byteorder in \"=<|\":\n                data_type = v4c.DATA_TYPE_SIGNED_INTEL\n            else:\n                data_type = v4c.DATA_TYPE_SIGNED_MOTOROLA\n        elif fmt.kind == \"f\":\n            if fmt.byteorder in \"=<\":\n                data_type = v4c.DATA_TYPE_REAL_INTEL\n            else:\n                data_type = v4c.DATA_TYPE_REAL_MOTOROLA\n        elif fmt.kind in \"SV\":\n            data_type = v4c.DATA_TYPE_STRING_LATIN_1\n        elif fmt.kind == \"b\":\n            data_type = v4c.DATA_TYPE_UNSIGNED_INTEL\n            size = 1\n        else:\n            message = f\"Unknown type: dtype={fmt}, shape={shape}\"\n            logger.exception(message)\n            raise MdfException(message)\n\n    return data_type, size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_non_byte_sized_signed_int(integer_array, bit_length):\n\n    if integer_array.flags.writeable:\n        integer_array &= (1 << bit_length) - 1  # Zero out the unwanted bits\n        truncated_integers = integer_array\n    else:\n        truncated_integers = integer_array & (\n            (1 << bit_length) - 1\n        )  # Zero out the unwanted bits\n    return where(\n        truncated_integers >> bit_length - 1,  # sign bit as a truth series (True when negative)\n        (2 ** bit_length - truncated_integers) * -1,  # when negative, do two's complement\n        truncated_integers,  # when positive, return the truncated int\n    )", "response": "Returns the array of integers that are not byte - sized and are not signed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nuse this to print debug information in case of errors Parameters ---------- mdf : MDF source MDF object group : dict group channel : Channel channel object dependency : ChannelDependency channel dependency object", "response": "def debug_channel(mdf, group, channel, dependency, file=None):\n    \"\"\" use this to print debug information in case of errors\n\n    Parameters\n    ----------\n    mdf : MDF\n        source MDF object\n    group : dict\n        group\n    channel : Channel\n        channel object\n    dependency : ChannelDependency\n        channel dependency object\n\n    \"\"\"\n    print(\"MDF\", \"=\" * 76, file=file)\n    print(\"name:\", mdf.name, file=file)\n    print(\"version:\", mdf.version, file=file)\n    print(\"read fragment size:\", mdf._read_fragment_size, file=file)\n    print(\"write fragment size:\", mdf._write_fragment_size, file=file)\n    print()\n\n    parents, dtypes = mdf._prepare_record(group)\n    print(\"GROUP\", \"=\" * 74, file=file)\n    print(\"sorted:\", group[\"sorted\"], file=file)\n    print(\"data location:\", group[\"data_location\"], file=file)\n    print(\"data size:\", group[\"data_size\"], file=file)\n    print(\"data blocks:\", group.data_blocks, file=file)\n    print(\"dependencies\", group[\"channel_dependencies\"], file=file)\n    print(\"parents:\", parents, file=file)\n    print(\"dtypes:\", dtypes, file=file)\n    print(file=file)\n\n    cg = group[\"channel_group\"]\n    print(\"CHANNEL GROUP\", \"=\" * 66, file=file)\n    print(cg, file=file)\n    print(file=file)\n\n    print(\"CHANNEL\", \"=\" * 72, file=file)\n    print(channel, file=file)\n    print(file=file)\n\n    print(\"CHANNEL ARRAY\", \"=\" * 66, file=file)\n    print(dependency, file=file)\n    print(file=file)\n\n    print(\"MASTER CACHE\", \"=\" * 67, file=file)\n    print(\n        [(key, len(val)) for key, val in mdf._master_channel_cache.items()], file=file\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef count_channel_groups(stream, include_channels=False):\n\n    count = 0\n    ch_count = 0\n\n    stream.seek(64)\n    blk_id = stream.read(2)\n    if blk_id == b\"HD\":\n        version = 3\n    else:\n        blk_id += stream.read(2)\n        if blk_id == b\"##HD\":\n            version = 4\n        else:\n            raise MdfException(f'\"{stream.name}\" is not a valid MDF file')\n\n    if version >= 4:\n        stream.seek(88, 0)\n        dg_addr = UINT64_u(stream.read(8))[0]\n        while dg_addr:\n            stream.seek(dg_addr + 32)\n            cg_addr = UINT64_u(stream.read(8))[0]\n            while cg_addr:\n                count += 1\n                if include_channels:\n                    stream.seek(cg_addr + 32)\n                    ch_addr = UINT64_u(stream.read(8))[0]\n                    while ch_addr:\n                        ch_count += 1\n                        stream.seek(ch_addr + 24)\n                        ch_addr = UINT64_u(stream.read(8))[0]\n                stream.seek(cg_addr + 24)\n                cg_addr = UINT64_u(stream.read(8))[0]\n\n            stream.seek(dg_addr + 24)\n            dg_addr = UINT64_u(stream.read(8))[0]\n\n    else:\n        stream.seek(68, 0)\n        dg_addr = UINT32_u(stream.read(4))[0]\n        while dg_addr:\n            stream.seek(dg_addr + 8)\n            cg_addr = UINT32_u(stream.read(4))[0]\n            while cg_addr:\n                count += 1\n                if include_channels:\n                    stream.seek(cg_addr + 8)\n                    ch_addr = UINT32_u(stream.read(4))[0]\n                    while ch_addr:\n                        ch_count += 1\n                        stream.seek(ch_addr + 4)\n                        ch_addr = UINT32_u(stream.read(4))[0]\n                stream.seek(cg_addr + 4)\n                cg_addr = UINT32_u(stream.read(4))[0]\n\n            stream.seek(dg_addr + 4)\n            dg_addr = UINT32_u(stream.read(4))[0]\n\n    return count, ch_count", "response": "Count all channel groups as fast as possible."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_version_argument(version, hint=4):\n    if version not in SUPPORTED_VERSIONS:\n        if hint == 2:\n            valid_version = \"2.14\"\n        elif hint == 3:\n            valid_version = \"3.30\"\n        else:\n            valid_version = \"4.10\"\n        message = (\n            'Unknown mdf version \"{}\".'\n            \" The available versions are {};\"\n            ' automatically using version \"{}\"'\n        )\n        message = message.format(version, SUPPORTED_VERSIONS, valid_version)\n        logger.warning(message)\n    else:\n        valid_version = version\n    return valid_version", "response": "Validate the version argument against the supported MDF versions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncut video stream from start to end", "response": "def cut_video_stream(stream, start, end, fmt):\n    \"\"\" cut video stream from `start` to `end` time\n\n    Parameters\n    ----------\n    stream : bytes\n        video file content\n    start : float\n        start time\n    end : float\n        end time\n\n    Returns\n    -------\n    result : bytes\n        content of cut video\n\n    \"\"\"\n    with TemporaryDirectory() as tmp:\n        in_file = Path(tmp) / f\"in{fmt}\"\n        out_file = Path(tmp) / f\"out{fmt}\"\n\n        in_file.write_bytes(stream)\n\n        try:\n            ret = subprocess.run(\n                [\n                    \"ffmpeg\",\n                    \"-ss\",\n                    f\"{start}\",\n                    \"-i\",\n                    f\"{in_file}\",\n                    \"-to\",\n                    f\"{end}\",\n                    \"-c\",\n                    \"copy\",\n                    f\"{out_file}\",\n                ],\n                capture_output=True,\n            )\n        except FileNotFoundError:\n            result = stream\n        else:\n            if ret.returncode:\n                result = stream\n            else:\n                result = out_file.read_bytes()\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield pandas Series and unique name based on the ndarray object.", "response": "def components(channel, channel_name, unique_names, prefix=\"\", master=None):\n    \"\"\" yield pandas Series and unique name based on the ndarray object\n\n    Parameters\n    ----------\n    channel : numpy.ndarray\n        channel to be used foir Series\n    channel_name : str\n        channel name\n    unique_names : UniqueDB\n        unique names object\n    prefix : str\n        prefix used in case of nested recarrays\n\n    Returns\n    -------\n    name, series : (str, pandas.Series)\n        tuple of unqiue name and Series object\n    \"\"\"\n    names = channel.dtype.names\n\n    # channel arrays\n    if names[0] == channel_name:\n        name = names[0]\n\n        if prefix:\n            name_ = unique_names.get_unique_name(f\"{prefix}.{name}\")\n        else:\n            name_ = unique_names.get_unique_name(name)\n\n        values = channel[name]\n        if len(values.shape) > 1:\n            arr = [values]\n            types = [(\"\", values.dtype, values.shape[1:])]\n            values = fromarrays(arr, dtype=types)\n            del arr\n        yield name_, Series(values, index=master, dtype=\"O\")\n\n        for name in names[1:]:\n            values = channel[name]\n            axis_name = unique_names.get_unique_name(f\"{name_}.{name}\")\n            if len(values.shape) > 1:\n                arr = [values]\n                types = [(\"\", values.dtype, values.shape[1:])]\n                values = fromarrays(arr, dtype=types)\n                del arr\n            yield axis_name, Series(values, index=master, dtype=\"O\")\n\n    # structure composition\n    else:\n        for name in channel.dtype.names:\n            values = channel[name]\n\n            if values.dtype.names:\n                yield from components(\n                    values, name, unique_names,\n                    prefix=f\"{prefix}.{channel_name}\" if prefix else f\"{channel_name}\",\n                    master=master\n                )\n\n            else:\n                name_ = unique_names.get_unique_name(\n                    f\"{prefix}.{channel_name}.{name}\" if prefix else f\"{channel_name}.{name}\"\n                )\n                if len(values.shape) > 1:\n                    arr = [values]\n                    types = [(\"\", values.dtype, values.shape[1:])]\n                    values = fromarrays(arr, dtype=types)\n                    del arr\n                yield name_, Series(values, index=master)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef master_using_raster(mdf, raster, endpoint=False):\n    if not raster:\n        master = np.array([], dtype='<f8')\n    else:\n\n        t_min = []\n        t_max = []\n        for i, group in enumerate(mdf.groups):\n            cycles_nr = group.channel_group.cycles_nr\n            if cycles_nr:\n                master_min = mdf.get_master(\n                    i,\n                    record_offset=0,\n                    record_count=1,\n                )\n                if len(master_min):\n                    t_min.append(master_min[0])\n                mdf._master_channel_cache.clear()\n                master_max = mdf.get_master(\n                    i,\n                    record_offset=cycles_nr-1,\n                    record_count=1,\n                )\n                if len(master_max):\n                    t_max.append(master_max[0])\n                mdf._master_channel_cache.clear()\n\n        if t_min:\n            t_min = np.amin(t_min)\n            t_max = np.amax(t_max)\n\n            num = float(np.float32((t_max - t_min) / raster))\n            if int(num) == num:\n                master = np.linspace(t_min, t_max, int(num) + 1)\n            else:\n                master = np.arange(t_min, t_max, raster)\n                if endpoint:\n                    master = np.concatenate([master, [t_max]])\n\n        else:\n            master = np.array([], dtype='<f8')\n\n    return master", "response": "get single master based on the raster"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, channel_name, entry):\n        if channel_name:\n            if channel_name not in self:\n                self[channel_name] = [entry]\n            else:\n                self[channel_name].append(entry)\n\n            if \"\\\\\" in channel_name:\n                channel_name = channel_name.split(\"\\\\\")[0]\n\n                if channel_name not in self:\n                    self[channel_name] = [entry]\n                else:\n                    self[channel_name].append(entry)", "response": "add name to channels database and check if it contains a source\n            path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_unique_name(self, name):\n\n        if name not in self._db:\n            self._db[name] = 0\n            return name\n        else:\n            index = self._db[name]\n            self._db[name] = index + 1\n            return f\"{name}_{index}\"", "response": "returns an available unique name for a given name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _included_channels(self, index, skip_master=True):\n\n        group = self.groups[index]\n\n        included_channels = set(range(len(group.channels)))\n        master_index = self.masters_db.get(index, None)\n        if master_index is not None and skip_master:\n            included_channels.remove(master_index)\n\n        channels = group.channels\n\n        if self.version in MDF2_VERSIONS + MDF3_VERSIONS:\n            for dep in group.channel_dependencies:\n                if dep is None:\n                    continue\n                for gp_nr, ch_nr in dep.referenced_channels:\n                    if gp_nr == index:\n                        included_channels.add(ch_nr)\n        else:\n            if group.CAN_logging:\n                where = (\n                    self.whereis(\"CAN_DataFrame\")\n                    + self.whereis(\"CAN_ErrorFrame\")\n                    + self.whereis(\"CAN_RemoteFrame\")\n                )\n                for dg_cntr, ch_cntr in where:\n                    if dg_cntr == index:\n                        break\n                else:\n                    raise MdfException(\n                        f\"CAN_DataFrame or CAN_ErrorFrame not found in group {index}\"\n                    )\n                channel = channels[ch_cntr]\n\n                frame_bytes = range(\n                    channel.byte_offset, channel.byte_offset + channel.bit_count // 8\n                )\n\n                for i, channel in enumerate(channels):\n                    if channel.byte_offset in frame_bytes:\n                        included_channels.remove(i)\n\n                included_channels.add(ch_cntr)\n\n                if group.CAN_database:\n                    dbc_addr = group.dbc_addr\n                    message_id = group.message_id\n                    can_msg = self._dbc_cache[dbc_addr].frameById(message_id)\n\n                    for i, _ in enumerate(can_msg.signals, 1):\n                        included_channels.add(-i)\n\n            for dependencies in group.channel_dependencies:\n                if dependencies is None:\n                    continue\n                if all(not isinstance(dep, ChannelArrayBlock) for dep in dependencies):\n                    for _, ch_nr in dependencies:\n                        try:\n                            included_channels.remove(ch_nr)\n                        except KeyError:\n                            pass\n                else:\n                    for dep in dependencies:\n                        for gp_nr, ch_nr in dep.referenced_channels:\n                            if gp_nr == index:\n                                try:\n                                    included_channels.remove(ch_nr)\n                                except KeyError:\n                                    pass\n\n        return included_channels", "response": "get the minimum channels needed to extract all information from the CAN_DataFrame or CAN_RemoteFrame"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting MDF to other version", "response": "def convert(self, version):\n        \"\"\"convert *MDF* to other version\n\n        Parameters\n        ----------\n        version : str\n            new mdf file version from ('2.00', '2.10', '2.14', '3.00', '3.10',\n            '3.20', '3.30', '4.00', '4.10', '4.11'); default '4.10'\n\n        Returns\n        -------\n        out : MDF\n            new *MDF* object\n\n        \"\"\"\n        version = validate_version_argument(version)\n\n        out = MDF(version=version)\n\n        out.header.start_time = self.header.start_time\n\n        try:\n            for can_id, info in self.can_logging_db.items():\n                if can_id not in out.can_logging_db:\n                    out.can_logging_db[can_id] = {}\n                out.can_logging_db[can_id].update(info)\n        except AttributeError:\n            pass\n\n        groups_nr = len(self.groups)\n\n        if self._callback:\n            self._callback(0, groups_nr)\n\n        cg_nr = -1\n\n        # walk through all groups and get all channels\n        for i, group in enumerate(self.groups):\n\n            encodings = []\n            included_channels = self._included_channels(i)\n            if included_channels:\n                cg_nr += 1\n            else:\n                continue\n\n            parents, dtypes = self._prepare_record(group)\n\n            data = self._load_data(group)\n            for idx, fragment in enumerate(data):\n                if dtypes.itemsize:\n                    group.record = np.core.records.fromstring(fragment[0], dtype=dtypes)\n                else:\n                    group.record = None\n                    continue\n\n                # the first fragment triggers and append that will add the\n                # metadata for all channels\n                if idx == 0:\n                    sigs = []\n                    for j in included_channels:\n                        sig = self.get(\n                            group=i,\n                            index=j,\n                            data=fragment,\n                            raw=True,\n                            ignore_invalidation_bits=True,\n                            copy_master=False,\n                        )\n                        if version < \"4.00\":\n                            if sig.samples.dtype.kind == \"S\":\n                                encodings.append(sig.encoding)\n                                strsig = self.get(\n                                    group=i,\n                                    index=j,\n                                    samples_only=True,\n                                    ignore_invalidation_bits=True,\n                                )[0]\n                                sig.samples = sig.samples.astype(strsig.dtype)\n                                del strsig\n                                if sig.encoding != \"latin-1\":\n\n                                    if sig.encoding == \"utf-16-le\":\n                                        sig.samples = (\n                                            sig.samples.view(np.uint16)\n                                            .byteswap()\n                                            .view(sig.samples.dtype)\n                                        )\n                                        sig.samples = encode(\n                                            decode(sig.samples, \"utf-16-be\"), \"latin-1\"\n                                        )\n                                    else:\n                                        sig.samples = encode(\n                                            decode(sig.samples, sig.encoding), \"latin-1\"\n                                        )\n                            else:\n                                encodings.append(None)\n                        if not sig.samples.flags.writeable:\n                            sig.samples = sig.samples.copy()\n                        sigs.append(sig)\n                    source_info = f\"Converted from {self.version} to {version}\"\n\n                    if sigs:\n                        out.append(sigs, source_info, common_timebase=True)\n                        try:\n                            if group.channel_group.flags & v4c.FLAG_CG_BUS_EVENT:\n                                out.groups[-1].channel_group.flags = group.channel_group.flags\n                                out.groups[-1].channel_group.acq_name = group.channel_group.acq_name\n                                out.groups[-1].channel_group.acq_source = group.channel_group.acq_source\n                                out.groups[-1].channel_group.comment = group.channel_group.comment\n                        except AttributeError:\n                            pass\n                    else:\n                        break\n\n                # the other fragments will trigger onl the extension of\n                # samples records to the data block\n                else:\n                    sigs = [\n                        (self.get_master(i, data=fragment, copy_master=False), None)\n                    ]\n\n                    for k, j in enumerate(included_channels):\n                        sig = self.get(\n                            group=i,\n                            index=j,\n                            data=fragment,\n                            raw=True,\n                            samples_only=True,\n                            ignore_invalidation_bits=True,\n                        )\n\n                        if version < \"4.00\":\n                            encoding = encodings[k]\n                            samples = sig[0]\n                            if encoding:\n                                if encoding != \"latin-1\":\n\n                                    if encoding == \"utf-16-le\":\n                                        samples = (\n                                            samples.view(np.uint16)\n                                            .byteswap()\n                                            .view(samples.dtype)\n                                        )\n                                        samples = encode(\n                                            decode(samples, \"utf-16-be\"), \"latin-1\"\n                                        )\n                                    else:\n                                        samples = encode(\n                                            decode(samples, encoding), \"latin-1\"\n                                        )\n                                    sig.samples = samples\n\n                        if not sig[0].flags.writeable:\n                            sig = sig[0].copy(), sig[1]\n                        sigs.append(sig)\n                    out.extend(cg_nr, sigs)\n\n                group.record = None\n\n            if self._callback:\n                self._callback(i + 1, groups_nr)\n\n            if self._terminate:\n                return\n\n        out._transfer_events(self)\n        if self._callback:\n            out._callback = out._mdf._callback = self._callback\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncutting the MDF file into a new MDF object.", "response": "def cut(\n        self,\n        start=None,\n        stop=None,\n        whence=0,\n        version=None,\n        include_ends=True,\n        time_from_zero=False,\n    ):\n        \"\"\"cut *MDF* file. *start* and *stop* limits are absolute values\n        or values relative to the first timestamp depending on the *whence*\n        argument.\n\n        Parameters\n        ----------\n        start : float\n            start time, default *None*. If *None* then the start of measurement\n            is used\n        stop : float\n            stop time, default *None*. If *None* then the end of measurement is\n            used\n        whence : int\n            how to search for the start and stop values\n\n            * 0 : absolute\n            * 1 : relative to first timestamp\n        version : str\n            new mdf file version from ('2.00', '2.10', '2.14', '3.00', '3.10',\n            '3.20', '3.30', '4.00', '4.10', '4.11'); default *None* and in this\n            case the original file version is used\n        include_ends : bool\n            include the *start* and *stop* timestamps after cutting the signal.\n            If *start* and *stop* are found in the original timestamps, then\n            the new samples will be computed using interpolation. Default *True*\n        time_from_zero : bool\n            start time stamps from 0s in the cut measurement\n\n        Returns\n        -------\n        out : MDF\n            new MDF object\n\n        \"\"\"\n\n        if version is None:\n            version = self.version\n        else:\n            version = validate_version_argument(version)\n\n        out = MDF(version=version)\n\n        if whence == 1:\n            timestamps = []\n            for i, group in enumerate(self.groups):\n                fragment = next(self._load_data(group))\n                master = self.get_master(\n                    i,\n                    record_offset=0,\n                    record_count=1,\n                )\n                if master.size:\n                    timestamps.append(master[0])\n            self._master_channel_cache.clear()\n\n            if timestamps:\n                first_timestamp = np.amin(timestamps)\n            else:\n                first_timestamp = 0\n\n            if start is not None:\n                start += first_timestamp\n            if stop is not None:\n                stop += first_timestamp\n\n        if time_from_zero:\n            delta = start\n            t_epoch = self.header.start_time.timestamp() + delta\n            out.header.start_time = datetime.fromtimestamp(t_epoch)\n        else:\n            delta = 0\n            out.header.start_time = self.header.start_time\n\n        groups_nr = len(self.groups)\n\n        if self._callback:\n            self._callback(0, groups_nr)\n\n        cg_nr = -1\n\n        interpolation_mode = self._integer_interpolation\n\n        # walk through all groups and get all channels\n        for i, group in enumerate(self.groups):\n            included_channels = self._included_channels(i)\n            if included_channels:\n                cg_nr += 1\n            else:\n                continue\n\n            data = self._load_data(group)\n            parents, dtypes = self._prepare_record(group)\n\n            idx = 0\n            for fragment in data:\n                if dtypes.itemsize:\n                    group.record = np.core.records.fromstring(fragment[0], dtype=dtypes)\n                else:\n                    group.record = None\n                master = self.get_master(i, fragment, copy_master=False)\n                if not len(master):\n                    continue\n\n                needs_cutting = True\n\n                # check if this fragement is within the cut interval or\n                # if the cut interval has ended\n                if start is None and stop is None:\n                    fragment_start = None\n                    fragment_stop = None\n                    start_index = 0\n                    stop_index = len(master)\n                    needs_cutting = False\n                elif start is None:\n                    fragment_start = None\n                    start_index = 0\n                    if master[0] > stop:\n                        break\n                    else:\n                        fragment_stop = min(stop, master[-1])\n                        stop_index = np.searchsorted(\n                            master, fragment_stop, side=\"right\"\n                        )\n                        if stop_index == len(master):\n                            needs_cutting = False\n                elif stop is None:\n                    fragment_stop = None\n                    if master[-1] < start:\n                        continue\n                    else:\n                        fragment_start = max(start, master[0])\n                        start_index = np.searchsorted(\n                            master, fragment_start, side=\"left\"\n                        )\n                        stop_index = len(master)\n                        if start_index == 0:\n                            needs_cutting = False\n                else:\n                    if master[0] > stop:\n                        break\n                    elif master[-1] < start:\n                        continue\n                    else:\n                        fragment_start = max(start, master[0])\n                        start_index = np.searchsorted(\n                            master, fragment_start, side=\"left\"\n                        )\n                        fragment_stop = min(stop, master[-1])\n                        stop_index = np.searchsorted(\n                            master, fragment_stop, side=\"right\"\n                        )\n                        if start_index == 0 and stop_index == len(master):\n                            needs_cutting = False\n\n                if needs_cutting:\n                    cut_timebase = (\n                        Signal(master, master, name=\"_\")\n                        .cut(fragment_start, fragment_stop, include_ends, interpolation_mode=interpolation_mode)\n                        .timestamps\n                    )\n\n                # the first fragment triggers and append that will add the\n                # metadata for all channels\n                if idx == 0:\n                    sigs = []\n                    for j in included_channels:\n                        sig = self.get(\n                            group=i,\n                            index=j,\n                            data=fragment,\n                            raw=True,\n                            ignore_invalidation_bits=True,\n                            copy_master=False,\n                        )\n                        if needs_cutting:\n                            sig = sig.interp(cut_timebase, interpolation_mode=interpolation_mode)\n\n                        # if sig.stream_sync and False:\n                        #     attachment, _name = sig.attachment\n                        #     duration = get_video_stream_duration(attachment)\n                        #     if start is None:\n                        #         start_t = 0\n                        #     else:\n                        #         start_t = start\n                        #\n                        #     if stop is None:\n                        #         end_t = duration\n                        #     else:\n                        #         end_t = stop\n                        #\n                        #     attachment = cut_video_stream(\n                        #         attachment,\n                        #         start_t,\n                        #         end_t,\n                        #         Path(_name).suffix,\n                        #     )\n                        #     sig.attachment = attachment, _name\n\n                        if not sig.samples.flags.writeable:\n                            sig.samples = sig.samples.copy()\n                        sigs.append(sig)\n\n                    if sigs:\n                        if time_from_zero:\n                            new_timestamps = cut_timebase - delta\n                            for sig in sigs:\n                                sig.timestamps = new_timestamps\n                        if start:\n                            start_ = f\"{start}s\"\n                        else:\n                            start_ = \"start of measurement\"\n                        if stop:\n                            stop_ = f\"{stop}s\"\n                        else:\n                            stop_ = \"end of measurement\"\n                        out.append(\n                            sigs, f\"Cut from {start_} to {stop_}\", common_timebase=True\n                        )\n                        try:\n                            if group.channel_group.flags & v4c.FLAG_CG_BUS_EVENT:\n                                out.groups[-1].channel_group.flags = group.channel_group.flags\n                                out.groups[-1].channel_group.acq_name = group.channel_group.acq_name\n                                out.groups[-1].channel_group.acq_source = group.channel_group.acq_source\n                                out.groups[-1].channel_group.comment = group.channel_group.comment\n                        except AttributeError:\n                            pass\n                    else:\n                        break\n\n                    idx += 1\n\n                # the other fragments will trigger onl the extension of\n                # samples records to the data block\n                else:\n                    if needs_cutting:\n                        timestamps = cut_timebase\n                    else:\n                        timestamps = master\n\n                    if time_from_zero:\n                        timestamps = timestamps - delta\n\n                    sigs = [(timestamps, None)]\n\n                    for j in included_channels:\n                        sig = self.get(\n                            group=i,\n                            index=j,\n                            data=fragment,\n                            raw=True,\n                            samples_only=True,\n                            ignore_invalidation_bits=True,\n                        )\n                        if needs_cutting:\n                            _sig = Signal(\n                                sig[0], master, name=\"_\", invalidation_bits=sig[1]\n                            ).interp(cut_timebase, interpolation_mode=interpolation_mode)\n                            sig = (_sig.samples, _sig.invalidation_bits)\n\n                            del _sig\n                        sigs.append(sig)\n\n                    if sigs:\n                        out.extend(cg_nr, sigs)\n\n                    idx += 1\n\n                group.record = None\n\n            # if the cut interval is not found in the measurement\n            # then append an empty data group\n            if idx == 0:\n\n                self.configure(read_fragment_size=1)\n                sigs = []\n\n                fragment = next(self._load_data(group))\n\n                fragment = (fragment[0], -1, None)\n\n                for j in included_channels:\n                    sig = self.get(\n                        group=i,\n                        index=j,\n                        data=fragment,\n                        raw=True,\n                        ignore_invalidation_bits=True,\n                    )\n                    sig.samples = sig.samples[:0]\n                    sig.timestamps = sig.timestamps[:0]\n                    sigs.append(sig)\n\n                if start:\n                    start_ = f\"{start}s\"\n                else:\n                    start_ = \"start of measurement\"\n                if stop:\n                    stop_ = f\"{stop}s\"\n                else:\n                    stop_ = \"end of measurement\"\n                out.append(sigs, f\"Cut from {start_} to {stop_}\", common_timebase=True)\n\n                self.configure(read_fragment_size=0)\n\n            if self._callback:\n                self._callback(i + 1, groups_nr)\n\n            if self._terminate:\n                return\n\n        out._transfer_events(self)\n        if self._callback:\n            out._callback = out._mdf._callback = self._callback\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports to other formats", "response": "def export(self, fmt, filename=None, **kargs):\n        \"\"\" export *MDF* to other formats. The *MDF* file name is used is\n        available, else the *filename* argument must be provided.\n\n        The *pandas* export option was removed. you should use the method\n        *to_dataframe* instead.\n\n        Parameters\n        ----------\n        fmt : string\n            can be one of the following:\n\n            * `csv` : CSV export that uses the \";\" delimiter. This option\n              will generate a new csv file for each data group\n              (<MDFNAME>_DataGroup_<cntr>.csv)\n\n            * `hdf5` : HDF5 file output; each *MDF* data group is mapped to\n              a *HDF5* group with the name 'DataGroup_<cntr>'\n              (where <cntr> is the index)\n\n            * `excel` : Excel file output (very slow). This option will\n              generate a new excel file for each data group\n              (<MDFNAME>_DataGroup_<cntr>.xlsx)\n\n            * `mat` : Matlab .mat version 4, 5 or 7.3 export. If\n              *single_time_base==False* the channels will be renamed in the mat\n              file to 'D<cntr>_<channel name>'. The channel group\n              master will be renamed to 'DM<cntr>_<channel name>'\n              ( *<cntr>* is the data group index starting from 0)\n\n            * `parquet` : export to Apache parquet format\n\n        filename : string | pathlib.Path\n            export file name\n\n        **kwargs\n\n            * `single_time_base`: resample all channels to common time base,\n              default *False*\n            * `raster`: float time raster for resampling. Valid if\n              *single_time_base* is *True*\n            * `time_from_zero`: adjust time channel to start from 0\n            * `use_display_names`: use display name instead of standard channel\n              name, if available.\n            * `empty_channels`: behaviour for channels without samples; the\n              options are *skip* or *zeros*; default is *skip*\n            * `format`: only valid for *mat* export; can be '4', '5' or '7.3',\n              default is '5'\n            * `oned_as`: only valid for *mat* export; can be 'row' or 'column'\n            * `keep_arrays` : keep arrays and structure channels as well as the\n              component channels. If *True* this can be very slow. If *False*\n              only the component channels are saved, and their names will be\n              prefixed with the parent channel.\n            * reduce_memory_usage : bool\n              reduce memory usage by converting all float columns to float32 and\n              searching for minimum dtype that can reprezent the values found\n              in integer columns; default *False*\n            * compression : str\n              compression to be used\n\n              * for ``parquet`` : \"GZIP\" or \"SANPPY\"\n              * for ``hfd5`` : \"gzip\", \"lzf\" or \"szip\"\n              * for ``mat`` : bool\n\n        \"\"\"\n\n        from time import perf_counter as pc\n\n        header_items = (\"date\", \"time\", \"author\", \"department\", \"project\", \"subject\")\n\n        if fmt != \"pandas\" and filename is None and self.name is None:\n            message = (\n                \"Must specify filename for export\"\n                \"if MDF was created without a file name\"\n            )\n            logger.warning(message)\n            return\n\n        single_time_base = kargs.get(\"single_time_base\", False)\n        raster = kargs.get(\"raster\", 0)\n        time_from_zero = kargs.get(\"time_from_zero\", True)\n        use_display_names = kargs.get(\"use_display_names\", True)\n        empty_channels = kargs.get(\"empty_channels\", \"skip\")\n        format = kargs.get(\"format\", \"5\")\n        oned_as = kargs.get(\"oned_as\", \"row\")\n        reduce_memory_usage = kargs.get(\"reduce_memory_usage\", False)\n        compression = kargs.get(\"compression\", \"\")\n\n        if compression == 'SNAPPY':\n            try:\n                import snappy\n            except ImportError:\n                logger.warning(\n                    \"snappy compressor is not installed; compression will be set to GZIP\"\n                )\n                compression = \"GZIP\"\n\n        name = Path(filename) if filename else self.name\n\n        if fmt == \"parquet\":\n            try:\n                from fastparquet import write as write_parquet\n            except ImportError:\n                logger.warning(\n                    \"fastparquet not found; export to parquet is unavailable\"\n                )\n                return\n\n        elif fmt == \"hdf5\":\n            try:\n                from h5py import File as HDF5\n            except ImportError:\n                logger.warning(\"h5py not found; export to HDF5 is unavailable\")\n                return\n\n        elif fmt == \"excel\":\n            try:\n                import xlsxwriter\n            except ImportError:\n                logger.warning(\"xlsxwriter not found; export to Excel unavailable\")\n                return\n\n        elif fmt == \"mat\":\n            if format == \"7.3\":\n                try:\n                    from hdf5storage import savemat\n                except ImportError:\n                    logger.warning(\n                        \"hdf5storage not found; export to mat v7.3 is unavailable\"\n                    )\n                    return\n            else:\n                try:\n                    from scipy.io import savemat\n                except ImportError:\n                    logger.warning(\"scipy not found; export to mat is unavailable\")\n                    return\n\n        if single_time_base or fmt == \"parquet\":\n            df = self.to_dataframe(\n                raster=raster,\n                time_from_zero=time_from_zero,\n                use_display_names=use_display_names,\n                empty_channels=empty_channels,\n                reduce_memory_usage=reduce_memory_usage,\n            )\n            units = OrderedDict()\n            comments = OrderedDict()\n            used_names = UniqueDB()\n\n            for i, grp in enumerate(self.groups):\n                if self._terminate:\n                    return\n\n                included_channels = self._included_channels(i)\n\n                for j in included_channels:\n                    ch = grp.channels[j]\n\n                    if use_display_names:\n                        channel_name = ch.display_name or ch.name\n                    else:\n                        channel_name = ch.name\n\n                    channel_name = used_names.get_unique_name(channel_name)\n\n                    if hasattr(ch, 'unit'):\n                        unit = ch.unit\n                        if ch.conversion:\n                            unit = unit or ch.conversion.unit\n                    comment = ch.comment\n\n                    units[channel_name] = unit\n                    comments[channel_name] = comment\n\n        if fmt == \"hdf5\":\n            name = name.with_suffix(\".hdf\")\n\n            if single_time_base:\n                with HDF5(str(name), \"w\") as hdf:\n                    # header information\n                    group = hdf.create_group(str(name))\n\n                    if self.version in MDF2_VERSIONS + MDF3_VERSIONS:\n                        for item in header_items:\n                            group.attrs[item] = self.header[item]\n\n                    # save each data group in a HDF5 group called\n                    # \"DataGroup_<cntr>\" with the index starting from 1\n                    # each HDF5 group will have a string attribute \"master\"\n                    # that will hold the name of the master channel\n\n                    for channel in df:\n                        samples = df[channel]\n                        unit = units[channel]\n                        comment = comments[channel]\n\n                        if samples.dtype.kind == 'O':\n                            continue\n\n                        if compression:\n                            dataset = group.create_dataset(\n                                channel,\n                                data=samples,\n                                compression=compression,\n                            )\n                        else:\n                            dataset = group.create_dataset(channel, data=samples)\n                        unit = unit.replace(\"\\0\", \"\")\n                        if unit:\n                            dataset.attrs[\"unit\"] = unit\n                        comment = comment.replace(\"\\0\", \"\")\n                        if comment:\n                            dataset.attrs[\"comment\"] = comment\n\n            else:\n                with HDF5(str(name), \"w\") as hdf:\n                    # header information\n                    group = hdf.create_group(str(name))\n\n                    if self.version in MDF2_VERSIONS + MDF3_VERSIONS:\n                        for item in header_items:\n                            group.attrs[item] = self.header[item]\n\n                    # save each data group in a HDF5 group called\n                    # \"DataGroup_<cntr>\" with the index starting from 1\n                    # each HDF5 group will have a string attribute \"master\"\n                    # that will hold the name of the master channel\n                    for i, grp in enumerate(self.groups):\n                        names = UniqueDB()\n                        if self._terminate:\n                            return\n                        group_name = r\"/\" + f\"DataGroup_{i+1}\"\n                        group = hdf.create_group(group_name)\n\n                        master_index = self.masters_db.get(i, -1)\n\n                        if master_index:\n                            group.attrs[\"master\"] = grp.channels[master_index].name\n\n                        channels = self.select(\n                            [(ch.name, i, None) for ch in grp.channels]\n                        )\n\n                        for j, sig in enumerate(channels):\n                            if use_display_names:\n                                name = sig.display_name or sig.name\n                            else:\n                                name = sig.name\n                            name = names.get_unique_name(name)\n                            if reduce_memory_usage:\n                                sig.samples = downcast(sig.samples)\n                            if compression:\n                                dataset = group.create_dataset(\n                                    name,\n                                    data=sig.samples,\n                                    compression=compression,\n                                )\n                            else:\n                                dataset = group.create_dataset(name, data=sig.samples, dtype=sig.samples.dtype)\n                            unit = sig.unit\n                            if unit:\n                                dataset.attrs[\"unit\"] = unit\n                            comment = sig.comment.replace(\"\\0\", \"\")\n                            if comment:\n                                dataset.attrs[\"comment\"] = comment\n\n        elif fmt == \"excel\":\n\n            if single_time_base:\n                name = name.with_suffix(\".xlsx\")\n                message = f'Writing excel export to file \"{name}\"'\n                logger.info(message)\n\n                workbook = xlsxwriter.Workbook(str(name))\n                sheet = workbook.add_worksheet(\"Channels\")\n\n                for col, (channel_name, channel_unit) in enumerate(units.items()):\n                    if self._terminate:\n                        return\n                    samples = df[channel_name]\n                    sig_description = f\"{channel_name} [{channel_unit}]\"\n                    sheet.write(0, col, sig_description)\n                    try:\n                        sheet.write_column(1, col, samples.astype(str))\n                    except:\n                        vals = [str(e) for e in sig.samples]\n                        sheet.write_column(1, col, vals)\n\n                workbook.close()\n\n            else:\n                while name.suffix == \".xlsx\":\n                    name = name.stem\n\n                count = len(self.groups)\n\n                for i, grp in enumerate(self.groups):\n                    if self._terminate:\n                        return\n                    message = f\"Exporting group {i+1} of {count}\"\n                    logger.info(message)\n\n                    data = self._load_data(grp)\n\n                    data = b\"\".join(d[0] for d in data)\n                    data = (data, 0, -1)\n\n                    master_index = self.masters_db.get(i, None)\n                    if master_index is not None:\n                        master = self.get(group=i, index=master_index, data=data)\n\n                        if raster and len(master):\n                            raster_ = np.arange(\n                                master[0], master[-1], raster, dtype=np.float64\n                            )\n                            master = master.interp(raster_)\n                        else:\n                            raster_ = None\n                    else:\n                        master = None\n                        raster_ = None\n\n                    if time_from_zero:\n                        master.samples -= master.samples[0]\n\n                    group_name = f\"DataGroup_{i+1}\"\n                    wb_name = Path(f\"{name.stem}_{group_name}.xlsx\")\n                    workbook = xlsxwriter.Workbook(str(wb_name))\n\n                    sheet = workbook.add_worksheet(group_name)\n\n                    if master is not None:\n\n                        sig_description = f\"{master.name} [{master.unit}]\"\n                        sheet.write(0, 0, sig_description)\n                        sheet.write_column(1, 0, master.samples.astype(str))\n\n                        offset = 1\n                    else:\n                        offset = 0\n\n                    for col, _ in enumerate(grp.channels):\n                        if self._terminate:\n                            return\n                        if col == master_index:\n                            offset -= 1\n                            continue\n\n                        sig = self.get(group=i, index=col, data=data)\n                        if raster_ is not None:\n                            sig = sig.interp(raster_, self._integer_interpolation)\n\n                        sig_description = f\"{sig.name} [{sig.unit}]\"\n                        sheet.write(0, col + offset, sig_description)\n\n                        try:\n                            sheet.write_column(1, col + offset, sig.samples.astype(str))\n                        except:\n                            vals = [str(e) for e in sig.samples]\n                            sheet.write_column(1, col + offset, vals)\n\n                    workbook.close()\n                    del self._master_channel_cache[(i, 0, -1)]\n\n        elif fmt == \"csv\":\n\n            if single_time_base:\n                name = name.with_suffix(\".csv\")\n                message = f'Writing csv export to file \"{name}\"'\n                logger.info(message)\n                with open(name, \"w\", newline=\"\") as csvfile:\n                    writer = csv.writer(csvfile)\n\n                    names_row = [\n                        f\"{channel_name} [{channel_unit}]\"\n                        for (channel_name, channel_unit) in units.items()\n                    ]\n                    writer.writerow(names_row)\n\n                    vals = [df[name] for name in df]\n\n                    if self._terminate:\n                        return\n\n                    for row in zip(*vals):\n                        writer.writerow(row)\n\n            else:\n\n                name = name.with_suffix(\".csv\")\n\n                count = len(self.groups)\n                for i, grp in enumerate(self.groups):\n                    if self._terminate:\n                        return\n                    message = f\"Exporting group {i+1} of {count}\"\n                    logger.info(message)\n                    data = self._load_data(grp)\n\n                    data = b\"\".join(d[0] for d in data)\n                    data = (data, 0, -1)\n\n                    group_csv_name = name.parent / f\"{name.stem}_DataGroup_{i+1}.csv\"\n\n                    with open(group_csv_name, \"w\", newline=\"\") as csvfile:\n                        writer = csv.writer(csvfile)\n\n                        master_index = self.masters_db.get(i, None)\n                        if master_index is not None:\n                            master = self.get(group=i, index=master_index, data=data)\n\n                            if raster and len(master):\n                                raster_ = np.arange(\n                                    master[0], master[-1], raster, dtype=np.float64\n                                )\n                                master = master.interp(raster_)\n                            else:\n                                raster_ = None\n                        else:\n                            master = None\n                            raster_ = None\n\n                        if time_from_zero:\n                            if master is None:\n                                pass\n                            elif len(master):\n                                master.samples -= master.samples[0]\n\n                        ch_nr = len(grp.channels)\n                        if master is None:\n                            channels = [\n                                self.get(group=i, index=j, data=data)\n                                for j in range(ch_nr)\n                            ]\n                        else:\n                            if raster_ is not None:\n                                channels = [\n                                    self.get(group=i, index=j, data=data).interp(\n                                        raster_, self._integer_interpolation\n                                    )\n                                    for j in range(ch_nr)\n                                    if j != master_index\n                                ]\n                            else:\n                                channels = [\n                                    self.get(group=i, index=j, data=data)\n                                    for j in range(ch_nr)\n                                    if j != master_index\n                                ]\n\n                        if raster_ is not None:\n                            cycles = len(raster_)\n                        else:\n                            cycles = grp.channel_group[\"cycles_nr\"]\n\n                        if empty_channels == \"zeros\":\n                            for channel in channels:\n                                if not len(channel):\n                                    channel.samples = np.zeros(\n                                        cycles, dtype=channel.samples.dtype\n                                    )\n\n                        if master is not None:\n                            names_row = [master.name]\n                            vals = [master.samples]\n                        else:\n                            names_row = []\n                            vals = []\n                        names_row += [f\"{ch.name} [{ch.unit}]\" for ch in channels]\n                        writer.writerow(names_row)\n\n                        vals += [ch.samples for ch in channels]\n\n                        for idx, row in enumerate(zip(*vals)):\n                            writer.writerow(row)\n\n                    del self._master_channel_cache[(i, 0, -1)]\n\n        elif fmt == \"mat\":\n\n            name = name.with_suffix(\".mat\")\n\n            if not single_time_base:\n                mdict = {}\n\n                master_name_template = \"DGM{}_{}\"\n                channel_name_template = \"DG{}_{}\"\n                used_names = UniqueDB()\n\n                for i, grp in enumerate(self.groups):\n                    if self._terminate:\n                        return\n\n                    included_channels = self._included_channels(i)\n\n                    master_index = self.masters_db.get(i, -1)\n\n                    if master_index >= 0:\n                        included_channels.add(master_index)\n\n                    channels = self.select(\n                        [(None, i, idx) for idx in included_channels]\n                    )\n\n                    for j, sig in zip(included_channels, channels):\n\n                        if j == master_index:\n                            channel_name = master_name_template.format(i, sig.name)\n                        else:\n                            if use_display_names:\n                                channel_name = sig.display_name or sig.name\n                            else:\n                                channel_name = sig.name\n                            channel_name = channel_name_template.format(i, channel_name)\n\n                        channel_name = matlab_compatible(channel_name)\n                        channel_name = used_names.get_unique_name(channel_name)\n\n                        if sig.samples.dtype.names:\n                            sig.samples.dtype.names = [\n                                matlab_compatible(name)\n                                for name in sig.samples.dtype.names\n                            ]\n\n                        mdict[channel_name] = sig.samples\n\n            else:\n                used_names = UniqueDB()\n                mdict = {}\n\n                for name in df.columns:\n                    channel_name = matlab_compatible(name)\n                    channel_name = used_names.get_unique_name(channel_name)\n\n                    mdict[channel_name] = df[name].values\n\n                mdict['time'] = df.index.values\n\n            if format == \"7.3\":\n\n                savemat(\n                    str(name),\n                    mdict,\n                    long_field_names=True,\n                    format=\"7.3\",\n                    delete_unused_variables=False,\n                    oned_as=oned_as,\n                )\n            else:\n                savemat(\n                    str(name),\n                    mdict,\n                    long_field_names=True,\n                    oned_as=oned_as,\n                    do_compression=bool(compression),\n                )\n\n        elif fmt == \"parquet\":\n            name = name.with_suffix(\".parquet\")\n            if compression:\n                write_parquet(name, df, compression=compression)\n            else:\n                write_parquet(name, df)\n\n        else:\n            message = (\n                'Unsopported export type \"{}\". '\n                'Please select \"csv\", \"excel\", \"hdf5\", \"mat\" or \"pandas\"'\n            )\n            message.format(fmt)\n            logger.warning(message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new MDF object that contains only the items listed in the given channels.", "response": "def filter(self, channels, version=None):\n        \"\"\" return new *MDF* object that contains only the channels listed in\n        *channels* argument\n\n        Parameters\n        ----------\n        channels : list\n            list of items to be filtered; each item can be :\n\n                * a channel name string\n                * (channel name, group index, channel index) list or tuple\n                * (channel name, group index) list or tuple\n                * (None, group index, channel index) list or tuple\n\n        version : str\n            new mdf file version from ('2.00', '2.10', '2.14', '3.00', '3.10',\n            '3.20', '3.30', '4.00', '4.10', '4.11'); default *None* and in this\n            case the original file version is used\n\n        Returns\n        -------\n        mdf : MDF\n            new *MDF* file\n\n        Examples\n        --------\n        >>> from asammdf import MDF, Signal\n        >>> import numpy as np\n        >>> t = np.arange(5)\n        >>> s = np.ones(5)\n        >>> mdf = MDF()\n        >>> for i in range(4):\n        ...     sigs = [Signal(s*(i*10+j), t, name='SIG') for j in range(1,4)]\n        ...     mdf.append(sigs)\n        ...\n        >>> filtered = mdf.filter(['SIG', ('SIG', 3, 1), ['SIG', 2], (None, 1, 2)])\n        >>> for gp_nr, ch_nr in filtered.channels_db['SIG']:\n        ...     print(filtered.get(group=gp_nr, index=ch_nr))\n        ...\n        <Signal SIG:\n                samples=[ 1.  1.  1.  1.  1.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        <Signal SIG:\n                samples=[ 31.  31.  31.  31.  31.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        <Signal SIG:\n                samples=[ 21.  21.  21.  21.  21.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        <Signal SIG:\n                samples=[ 12.  12.  12.  12.  12.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n\n        \"\"\"\n\n        if version is None:\n            version = self.version\n        else:\n            version = validate_version_argument(version)\n\n        # group channels by group index\n        gps = {}\n\n        for item in channels:\n            if isinstance(item, (list, tuple)):\n                if len(item) not in (2, 3):\n                    raise MdfException(\n                        \"The items used for filtering must be strings, \"\n                        \"or they must match the first 3 argumens of the get \"\n                        \"method\"\n                    )\n                else:\n                    group, index = self._validate_channel_selection(*item)\n                    if group not in gps:\n                        gps[group] = {index}\n                    else:\n                        gps[group].add(index)\n            else:\n                name = item\n                group, index = self._validate_channel_selection(name)\n                if group not in gps:\n                    gps[group] = {index}\n                else:\n                    gps[group].add(index)\n\n        # see if there are exluded channels in the filter list\n        for group_index, indexes in gps.items():\n            grp = self.groups[group_index]\n            included_channels = set(indexes)\n            master_index = self.masters_db.get(group_index, None)\n            if master_index is not None:\n                to_exclude = {master_index}\n            else:\n                to_exclude = set()\n            for index in indexes:\n                if self.version in MDF2_VERSIONS + MDF3_VERSIONS:\n                    dep = grp.channel_dependencies[index]\n                    if dep:\n                        for gp_nr, ch_nr in dep.referenced_channels:\n                            if gp_nr == group:\n                                included_channels.remove(ch_nr)\n                else:\n                    dependencies = grp.channel_dependencies[index]\n                    if dependencies is None:\n                        continue\n                    if all(\n                        not isinstance(dep, ChannelArrayBlock) for dep in dependencies\n                    ):\n                        channels = grp.channels\n                        for _, channel in dependencies:\n                            to_exclude.add(channel)\n                    else:\n                        for dep in dependencies:\n                            for gp_nr, ch_nr in dep.referenced_channels:\n                                if gp_nr == group:\n                                    to_exclude.add(ch_nr)\n\n            gps[group_index] = sorted(included_channels - to_exclude)\n\n        mdf = MDF(version=version)\n\n        mdf.header.start_time = self.header.start_time\n\n        if self.name:\n            origin = self.name.parent\n        else:\n            origin = \"New MDF\"\n\n        groups_nr = len(gps)\n\n        if self._callback:\n            self._callback(0, groups_nr)\n\n        # append filtered channels to new MDF\n        for new_index, (group_index, indexes) in enumerate(gps.items()):\n            if version < \"4.00\":\n                encodings = []\n\n            group = self.groups[group_index]\n\n            data = self._load_data(group)\n            _, dtypes = self._prepare_record(group)\n\n            for idx, fragment in enumerate(data):\n\n                if dtypes.itemsize:\n                    group.record = np.core.records.fromstring(fragment[0], dtype=dtypes)\n                else:\n                    group.record = None\n\n                # the first fragment triggers and append that will add the\n                # metadata for all channels\n                if idx == 0:\n                    sigs = []\n                    for j in indexes:\n                        sig = self.get(\n                            group=group_index,\n                            index=j,\n                            data=fragment,\n                            raw=True,\n                            ignore_invalidation_bits=True,\n                            copy_master=False,\n                        )\n                        if version < \"4.00\":\n                            if sig.samples.dtype.kind == \"S\":\n                                encodings.append(sig.encoding)\n                                strsig = self.get(\n                                    group=group_index,\n                                    index=j,\n                                    samples_only=True,\n                                    ignore_invalidation_bits=True,\n                                )[0]\n                                sig.samples = sig.samples.astype(strsig.dtype)\n                                del strsig\n                                if sig.encoding != \"latin-1\":\n\n                                    if sig.encoding == \"utf-16-le\":\n                                        sig.samples = (\n                                            sig.samples.view(np.uint16)\n                                            .byteswap()\n                                            .view(sig.samples.dtype)\n                                        )\n                                        sig.samples = encode(\n                                            decode(sig.samples, \"utf-16-be\"), \"latin-1\"\n                                        )\n                                    else:\n                                        sig.samples = encode(\n                                            decode(sig.samples, sig.encoding), \"latin-1\"\n                                        )\n                            else:\n                                encodings.append(None)\n                        if not sig.samples.flags.writeable:\n                            sig.samples = sig.samples.copy()\n                        sigs.append(sig)\n\n                    source_info = f\"Signals filtered from <{origin}>\"\n\n                    if sigs:\n                        mdf.append(sigs, source_info, common_timebase=True)\n                        try:\n                            if group.channel_group.flags & v4c.FLAG_CG_BUS_EVENT:\n                                mdf.groups[-1].channel_group.flags = group.channel_group.flags\n                                mdf.groups[-1].channel_group.acq_name = group.channel_group.acq_name\n                                mdf.groups[-1].channel_group.acq_source = group.channel_group.acq_source\n                                mdf.groups[-1].channel_group.comment = group.channel_group.comment\n                        except AttributeError:\n                            pass\n                    else:\n                        break\n\n                # the other fragments will trigger onl the extension of\n                # samples records to the data block\n                else:\n                    sigs = [\n                        (\n                            self.get_master(\n                                group_index,\n                                data=fragment,\n                                copy_master=False\n                            ),\n                            None,\n                        )\n                    ]\n\n                    for k, j in enumerate(indexes):\n                        sig = self.get(\n                            group=group_index,\n                            index=j,\n                            data=fragment,\n                            samples_only=True,\n                            raw=True,\n                            ignore_invalidation_bits=True,\n                        )\n                        if version < \"4.00\":\n                            encoding = encodings[k]\n                            samples = sig[0]\n                            if encoding:\n                                if encoding != \"latin-1\":\n\n                                    if encoding == \"utf-16-le\":\n                                        samples = (\n                                            samples.view(np.uint16)\n                                            .byteswap()\n                                            .view(samples.dtype)\n                                        )\n                                        samples = encode(\n                                            decode(samples, \"utf-16-be\"), \"latin-1\"\n                                        )\n                                    else:\n                                        samples = encode(\n                                            decode(samples, encoding), \"latin-1\"\n                                        )\n                                    sig.samples = samples\n\n                        sigs.append(sig)\n\n                    if sigs:\n                        mdf.extend(new_index, sigs)\n\n                group.record = None\n\n            if self._callback:\n                self._callback(new_index + 1, groups_nr)\n\n            if self._terminate:\n                return\n\n        mdf._transfer_events(self)\n        if self._callback:\n            mdf._callback = mdf._mdf._callback = self._callback\n        return mdf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef concatenate(files, version=\"4.10\", sync=True, add_samples_origin=False, **kwargs):\n        if not files:\n            raise MdfException(\"No files given for merge\")\n\n        callback = kwargs.get(\"callback\", None)\n        if callback:\n            callback(0, 100)\n\n        mdf_nr = len(files)\n\n        versions = []\n        if sync:\n            timestamps = []\n            for file in files:\n                if isinstance(file, MDF):\n                    timestamps.append(file.header.start_time)\n                    versions.append(file.version)\n                else:\n                    with open(file, \"rb\") as mdf:\n                        mdf.seek(64)\n                        blk_id = mdf.read(2)\n                        if blk_id == b\"HD\":\n                            header = HeaderV3\n                            versions.append(\"3.00\")\n                        else:\n                            versions.append(\"4.00\")\n                            blk_id += mdf.read(2)\n                            if blk_id == b\"##HD\":\n                                header = HeaderV4\n                            else:\n                                raise MdfException(f'\"{file}\" is not a valid MDF file')\n\n                        header = header(address=64, stream=mdf)\n\n                        timestamps.append(header.start_time)\n\n            try:\n                oldest = min(timestamps)\n            except TypeError:\n                timestamps = [\n                    timestamp.astimezone(timezone.utc)\n                    for timestamp in timestamps\n                ]\n                oldest = min(timestamps)\n\n            offsets = [(timestamp - oldest).total_seconds() for timestamp in timestamps]\n            offsets = [offset if offset > 0 else 0 for offset in offsets]\n\n        else:\n            file = files[0]\n            if isinstance(file, MDF):\n                oldest = file.header.start_time\n                versions.append(file.version)\n            else:\n                with open(file, \"rb\") as mdf:\n                    mdf.seek(64)\n                    blk_id = mdf.read(2)\n                    if blk_id == b\"HD\":\n                        versions.append(\"3.00\")\n                        header = HeaderV3\n                    else:\n                        versions.append(\"4.00\")\n                        blk_id += mdf.read(2)\n                        if blk_id == b\"##HD\":\n                            header = HeaderV4\n                        else:\n                            raise MdfException(f'\"{file}\" is not a valid MDF file')\n\n                    header = header(address=64, stream=mdf)\n\n                    oldest = header.start_time\n\n            offsets = [0 for _ in files]\n\n        version = validate_version_argument(version)\n\n        merged = MDF(version=version, callback=callback)\n\n        merged.header.start_time = oldest\n\n        encodings = []\n        included_channel_names = []\n\n        if add_samples_origin:\n            origin_conversion = {}\n            for i, mdf in enumerate(files):\n                origin_conversion[f'val_{i}'] = i\n                if isinstance(mdf, MDF):\n                    origin_conversion[f'text_{i}'] = str(mdf.name)\n                else:\n                    origin_conversion[f'text_{i}'] = str(mdf)\n            origin_conversion = from_dict(origin_conversion)\n\n        for mdf_index, (offset, mdf) in enumerate(zip(offsets, files)):\n            if not isinstance(mdf, MDF):\n                mdf = MDF(mdf)\n\n            try:\n                for can_id, info in mdf.can_logging_db.items():\n                    if can_id not in merged.can_logging_db:\n                        merged.can_logging_db[can_id] = {}\n                    merged.can_logging_db[can_id].update(info)\n            except AttributeError:\n                pass\n\n            if mdf_index == 0:\n                last_timestamps = [None for gp in mdf.groups]\n                groups_nr = len(mdf.groups)\n\n            cg_nr = -1\n\n            for i, group in enumerate(mdf.groups):\n                included_channels = mdf._included_channels(i)\n\n                if mdf_index == 0:\n                    included_channel_names.append(\n                        [group.channels[k].name for k in included_channels]\n                    )\n                else:\n                    names = [group.channels[k].name for k in included_channels]\n                    if names != included_channel_names[i]:\n                        if sorted(names) != sorted(included_channel_names[i]):\n                            raise MdfException(f\"internal structure of file {mdf_index} is different\")\n                        else:\n                            logger.warning(\n                                f'Different channel order in channel group {i} of file {mdf_index}.'\n                                ' Data can be corrupted if the there are channels with the same '\n                                'name in this channel group'\n                            )\n                            included_channels = [\n                                mdf._validate_channel_selection(\n                                    name=name_,\n                                    group=i,\n                                )[1]\n                                for name_ in included_channel_names[i]\n                            ]\n\n                if included_channels:\n                    cg_nr += 1\n                else:\n                    continue\n                channels_nr = len(group.channels)\n\n                y_axis = MERGE\n\n                idx = np.searchsorted(CHANNEL_COUNT, channels_nr, side=\"right\") - 1\n                if idx < 0:\n                    idx = 0\n                read_size = y_axis[idx]\n\n                idx = 0\n                last_timestamp = last_timestamps[i]\n                first_timestamp = None\n                original_first_timestamp = None\n\n                if read_size:\n                    mdf.configure(read_fragment_size=int(read_size))\n\n                parents, dtypes = mdf._prepare_record(group)\n\n                data = mdf._load_data(group)\n\n                for fragment in data:\n\n                    if dtypes.itemsize:\n                        group.record = np.core.records.fromstring(\n                            fragment[0], dtype=dtypes\n                        )\n                    else:\n                        group.record = None\n\n                    if mdf_index == 0 and idx == 0:\n                        encodings_ = []\n                        encodings.append(encodings_)\n                        signals = []\n                        for j in included_channels:\n                            sig = mdf.get(\n                                group=i,\n                                index=j,\n                                data=fragment,\n                                raw=True,\n                                ignore_invalidation_bits=True,\n                                copy_master=False,\n                            )\n\n                            if version < \"4.00\":\n                                if sig.samples.dtype.kind == \"S\":\n                                    encodings_.append(sig.encoding)\n                                    strsig = mdf.get(\n                                        group=i,\n                                        index=j,\n                                        samples_only=True,\n                                        ignore_invalidation_bits=True,\n                                    )[0]\n                                    sig.samples = sig.samples.astype(strsig.dtype)\n                                    del strsig\n                                    if sig.encoding != \"latin-1\":\n\n                                        if sig.encoding == \"utf-16-le\":\n                                            sig.samples = (\n                                                sig.samples.view(np.uint16)\n                                                .byteswap()\n                                                .view(sig.samples.dtype)\n                                            )\n                                            sig.samples = encode(\n                                                decode(sig.samples, \"utf-16-be\"),\n                                                \"latin-1\",\n                                            )\n                                        else:\n                                            sig.samples = encode(\n                                                decode(sig.samples, sig.encoding),\n                                                \"latin-1\",\n                                            )\n                                else:\n                                    encodings_.append(None)\n\n                            if not sig.samples.flags.writeable:\n                                sig.samples = sig.samples.copy()\n                            signals.append(sig)\n\n                        if signals and len(signals[0]):\n                            if offset > 0:\n                                timestamps = sig[0].timestamps + offset\n                                for sig in signals:\n                                    sig.timestamps = timestamps\n                            last_timestamp = signals[0].timestamps[-1]\n                            first_timestamp = signals[0].timestamps[0]\n                            original_first_timestamp = first_timestamp\n\n                        if add_samples_origin:\n                            if signals:\n                                _s = signals[-1]\n                                signals.append(\n                                    Signal(\n                                        samples=np.ones(len(_s), dtype='<u2')*mdf_index,\n                                        timestamps=_s.timestamps,\n                                        conversion=origin_conversion,\n                                        name='__samples_origin',\n                                    )\n                                )\n                                _s = None\n\n                        if signals:\n                            merged.append(signals, common_timebase=True)\n                            try:\n                                if group.channel_group.flags & v4c.FLAG_CG_BUS_EVENT:\n                                    merged.groups[-1].channel_group.flags = group.channel_group.flags\n                                    merged.groups[-1].channel_group.acq_name = group.channel_group.acq_name\n                                    merged.groups[-1].channel_group.acq_source = group.channel_group.acq_source\n                                    merged.groups[-1].channel_group.comment = group.channel_group.comment\n                            except AttributeError:\n                                pass\n                        else:\n                            break\n                        idx += 1\n                    else:\n                        master = mdf.get_master(i, fragment, copy_master=False)\n                        _copied = False\n\n                        if len(master):\n                            if original_first_timestamp is None:\n                                original_first_timestamp = master[0]\n                            if offset > 0:\n                                master = master + offset\n                                _copied = True\n                            if last_timestamp is None:\n                                last_timestamp = master[-1]\n                            else:\n                                if last_timestamp >= master[0]:\n                                    if len(master) >= 2:\n                                        delta = master[1] - master[0]\n                                    else:\n                                        delta = 0.001\n                                    if _copied:\n                                        master -= master[0]\n                                    else:\n                                        master = master - master[0]\n                                        _copied = True\n                                    master += last_timestamp + delta\n                                last_timestamp = master[-1]\n\n                            signals = [(master, None)]\n\n                            for k, j in enumerate(included_channels):\n                                sig = mdf.get(\n                                    group=i,\n                                    index=j,\n                                    data=fragment,\n                                    raw=True,\n                                    samples_only=True,\n                                    ignore_invalidation_bits=True,\n                                )\n\n                                signals.append(sig)\n\n                                if version < \"4.00\":\n                                    encoding = encodings[i][k]\n                                    samples = sig[0]\n                                    if encoding:\n                                        if encoding != \"latin-1\":\n\n                                            if encoding == \"utf-16-le\":\n                                                samples = (\n                                                    samples.view(np.uint16)\n                                                    .byteswap()\n                                                    .view(samples.dtype)\n                                                )\n                                                samples = encode(\n                                                    decode(samples, \"utf-16-be\"),\n                                                    \"latin-1\",\n                                                )\n                                            else:\n                                                samples = encode(\n                                                    decode(samples, encoding), \"latin-1\"\n                                                )\n                                            sig.samples = samples\n\n\n                            if signals:\n                                if add_samples_origin:\n                                    _s = signals[-1][0]\n                                    signals.append(\n                                        (np.ones(len(_s), dtype='<u2')*mdf_index, None)\n                                    )\n                                    _s = None\n                                merged.extend(cg_nr, signals)\n\n                            if first_timestamp is None:\n                                first_timestamp = master[0]\n                        idx += 1\n\n                    group.record = None\n\n                last_timestamps[i] = last_timestamp\n\n            if callback:\n                callback(i + 1 + mdf_index * groups_nr, groups_nr * mdf_nr)\n\n            if MDF._terminate:\n                return\n\n            merged._transfer_events(mdf)\n\n        return merged", "response": "Concatenates several files into a single MDF object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stack(files, version=\"4.10\", sync=True, **kwargs):\n        if not files:\n            raise MdfException(\"No files given for stack\")\n\n        version = validate_version_argument(version)\n\n        callback = kwargs.get(\"callback\", None)\n\n        stacked = MDF(version=version, callback=callback)\n\n        files_nr = len(files)\n\n        if callback:\n            callback(0, files_nr)\n\n        if sync:\n            timestamps = []\n            for file in files:\n                if isinstance(file, MDF):\n                    timestamps.append(file.header.start_time)\n                else:\n                    with open(file, \"rb\") as mdf:\n                        mdf.seek(64)\n                        blk_id = mdf.read(2)\n                        if blk_id == b\"HD\":\n                            header = HeaderV3\n                        else:\n                            blk_id += mdf.read(2)\n                            if blk_id == b\"##HD\":\n                                header = HeaderV4\n                            else:\n                                raise MdfException(f'\"{file}\" is not a valid MDF file')\n\n                        header = header(address=64, stream=mdf)\n\n                        timestamps.append(header.start_time)\n\n            try:\n                oldest = min(timestamps)\n            except TypeError:\n                timestamps = [\n                    timestamp.astimezone(timezone.utc)\n                    for timestamp in timestamps\n                ]\n                oldest = min(timestamps)\n\n            offsets = [(timestamp - oldest).total_seconds() for timestamp in timestamps]\n\n            stacked.header.start_time = oldest\n        else:\n            offsets = [0 for file in files]\n\n        cg_nr = -1\n        for offset, mdf in zip(offsets, files):\n            if not isinstance(mdf, MDF):\n                mdf = MDF(mdf)\n\n            cg_offset = cg_nr + 1\n\n            for i, group in enumerate(mdf.groups):\n                idx = 0\n                if version < \"4.00\":\n                    encodings = []\n                included_channels = mdf._included_channels(i)\n                if included_channels:\n                    cg_nr += 1\n                else:\n                    continue\n\n                try:\n                    for can_id, info in mdf.can_logging_db.items():\n                        if can_id not in mdf.can_logging_db:\n                            mdf.can_logging_db[can_id] = {}\n                        mdf.can_logging_db[can_id].update(\n                            {\n                                message_id: cg_index + cg_offset\n                                for message_id, cg_index in info.items()\n                            }\n                        )\n                except AttributeError:\n                    pass\n\n                _, dtypes = mdf._prepare_record(group)\n\n                data = mdf._load_data(group)\n\n                for fragment in data:\n\n                    if dtypes.itemsize:\n                        group.record = np.core.records.fromstring(\n                            fragment[0], dtype=dtypes\n                        )\n                    else:\n                        group.record = None\n                    if idx == 0:\n                        signals = []\n                        for j in included_channels:\n                            sig = mdf.get(\n                                group=i,\n                                index=j,\n                                data=fragment,\n                                raw=True,\n                                ignore_invalidation_bits=True,\n                                copy_master=False,\n                            )\n\n                            if version < \"4.00\":\n                                if sig.samples.dtype.kind == \"S\":\n                                    encodings.append(sig.encoding)\n                                    strsig = mdf.get(\n                                        group=i,\n                                        index=j,\n                                        samples_only=True,\n                                        ignore_invalidation_bits=True,\n                                    )[0]\n                                    sig.samples = sig.samples.astype(strsig.dtype)\n                                    del strsig\n                                    if sig.encoding != \"latin-1\":\n\n                                        if sig.encoding == \"utf-16-le\":\n                                            sig.samples = (\n                                                sig.samples.view(np.uint16)\n                                                .byteswap()\n                                                .view(sig.samples.dtype)\n                                            )\n                                            sig.samples = encode(\n                                                decode(sig.samples, \"utf-16-be\"),\n                                                \"latin-1\",\n                                            )\n                                        else:\n                                            sig.samples = encode(\n                                                decode(sig.samples, sig.encoding),\n                                                \"latin-1\",\n                                            )\n                                else:\n                                    encodings.append(None)\n\n                            if not sig.samples.flags.writeable:\n                                sig.samples = sig.samples.copy()\n                            signals.append(sig)\n\n                        if signals:\n                            if sync:\n                                timestamps = signals[0].timestamps + offset\n                                for sig in signals:\n                                    sig.timestamps = timestamps\n                            stacked.append(signals, common_timebase=True)\n                            try:\n                                if group.channel_group.flags & v4c.FLAG_CG_BUS_EVENT:\n                                    stacked.groups[-1].channel_group.flags = group.channel_group.flags\n                                    stacked.groups[-1].channel_group.acq_name = group.channel_group.acq_name\n                                    stacked.groups[-1].channel_group.acq_source = group.channel_group.acq_source\n                                    stacked.groups[-1].channel_group.comment = group.channel_group.comment\n                            except AttributeError:\n                                pass\n                        idx += 1\n                    else:\n                        master = mdf.get_master(i, fragment, copy_master=False)\n                        if sync:\n                            master = master + offset\n                        if len(master):\n\n                            signals = [(master, None)]\n\n                            for k, j in enumerate(included_channels):\n                                sig = mdf.get(\n                                    group=i,\n                                    index=j,\n                                    data=fragment,\n                                    raw=True,\n                                    samples_only=True,\n                                    ignore_invalidation_bits=True,\n                                )\n                                signals.append(sig)\n\n                                if version < \"4.00\":\n                                    encoding = encodings[k]\n                                    samples = sig[0]\n                                    if encoding:\n                                        if encoding != \"latin-1\":\n\n                                            if encoding == \"utf-16-le\":\n                                                samples = (\n                                                    samples.view(np.uint16)\n                                                    .byteswap()\n                                                    .view(samples.dtype)\n                                                )\n                                                samples = encode(\n                                                    decode(samples, \"utf-16-be\"),\n                                                    \"latin-1\",\n                                                )\n                                            else:\n                                                samples = encode(\n                                                    decode(samples, encoding), \"latin-1\"\n                                                )\n                                            sig.samples = samples\n\n                            if signals:\n                                stacked.extend(cg_nr, signals)\n                        idx += 1\n\n                    group.record = None\n\n                stacked.groups[\n                    -1\n                ].channel_group.comment = (\n                    f'stacked from channel group {i} of \"{mdf.name.parent}\"'\n                )\n\n            if callback:\n                callback(idx, files_nr)\n\n            if MDF._terminate:\n                return\n\n        return stacked", "response": "Stack several MDF files and return the stacked MDF object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_channels(self, skip_master=True, copy_master=True):\n\n        for i, group in enumerate(self.groups):\n\n            included_channels = self._included_channels(i, skip_master=skip_master)\n\n            channels = [\n                (None, i, idx)\n                for idx in included_channels\n            ]\n\n            channels = self.select(\n                channels,\n                copy_master=copy_master,\n            )\n\n            for channel in channels:\n                yield channel", "response": "generator that yields a *Signal* for each master channel"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_groups(self):\n\n        for i, _ in enumerate(self.groups):\n            yield self.get_group(i)", "response": "generator that yields the groups as pandas DataFrames."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resample(self, raster, version=None, time_from_zero=False):\n\n        if version is None:\n            version = self.version\n        else:\n            version = validate_version_argument(version)\n\n        interpolation_mode = self._integer_interpolation\n\n        mdf = MDF(version=version)\n\n        mdf.header.start_time = self.header.start_time\n\n        groups_nr = len(self.groups)\n\n        if self._callback:\n            self._callback(0, groups_nr)\n\n        try:\n            raster = float(raster)\n            assert raster > 0\n        except (TypeError, ValueError):\n            if isinstance(raster, str):\n                raster = self.get(raster).timestamps\n            else:\n                raster = np.array(raster)\n        else:\n            raster = master_using_raster(self, raster)\n\n        if time_from_zero and len(raster):\n            delta = raster[0]\n            new_raster = raster - delta\n            t_epoch = self.header.start_time.timestamp() + delta\n            mdf.header.start_time = datetime.fromtimestamp(t_epoch)\n        else:\n            delta = 0\n            new_raster = None\n            mdf.header.start_time = self.header.start_time\n\n        for i, group in enumerate(self.groups):\n            included_channels = self._included_channels(i)\n            channels = [\n                (None, i, idx)\n                for idx in included_channels\n            ]\n            sigs = self.select(\n                channels,\n                raw=True,\n                copy_master=False,\n            )\n            sigs = [\n                sig.interp(raster, interpolation_mode=interpolation_mode)\n                for sig in sigs\n            ]\n\n            if new_raster is not None:\n                for sig in sigs:\n                    sig.timestamps = new_raster\n\n            mdf.append(sigs, common_timebase=True)\n            try:\n                if group.channel_group.flags & v4c.FLAG_CG_BUS_EVENT:\n                    mdf.groups[-1].channel_group.flags = group.channel_group.flags\n                    mdf.groups[-1].channel_group.acq_name = group.channel_group.acq_name\n                    mdf.groups[-1].channel_group.acq_source = group.channel_group.acq_source\n                    mdf.groups[-1].channel_group.comment = group.channel_group.comment\n            except AttributeError:\n                pass\n\n            if self._callback:\n                self._callback(i + 1, groups_nr)\n\n            if self._terminate:\n                return\n\n        if self._callback:\n            self._callback(groups_nr, groups_nr)\n\n        mdf._transfer_events(self)\n        if self._callback:\n            mdf._callback = mdf._mdf._callback = self._callback\n        return mdf", "response": "Resample all channels using the given raster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the items listed in channels and return a DataFrame of all the signals that are in the same channel group.", "response": "def select(self, channels, dataframe=False, record_offset=0, raw=False, copy_master=True):\n        \"\"\" retrieve the channels listed in *channels* argument as *Signal*\n        objects\n\n        Parameters\n        ----------\n        channels : list\n            list of items to be filtered; each item can be :\n\n                * a channel name string\n                * (channel name, group index, channel index) list or tuple\n                * (channel name, group index) list or tuple\n                * (None, group index, channel index) list or tuple\n        dataframe : bool\n            return pandas dataframe instead of list of Signals; default *False*\n        record_offset : int\n            record number offset; optimization to get the last part of signal samples\n        raw : bool\n            get raw channel samples; default *False*\n        copy_master : bool\n            option to get a new timestamps array for each selected Signal or to\n            use a shared array for channels of the same channel group; default *False*\n\n        dataframe: bool\n            return a pandas DataFrame instead of a list of *Signals*; in this\n            case the signals will be interpolated using the union of all\n            timestamps\n\n        Returns\n        -------\n        signals : list\n            list of *Signal* objects based on the input channel list\n\n        Examples\n        --------\n        >>> from asammdf import MDF, Signal\n        >>> import numpy as np\n        >>> t = np.arange(5)\n        >>> s = np.ones(5)\n        >>> mdf = MDF()\n        >>> for i in range(4):\n        ...     sigs = [Signal(s*(i*10+j), t, name='SIG') for j in range(1,4)]\n        ...     mdf.append(sigs)\n        ...\n        >>> # select SIG group 0 default index 1 default, SIG group 3 index 1, SIG group 2 index 1 default and channel index 2 from group 1\n        ...\n        >>> mdf.select(['SIG', ('SIG', 3, 1), ['SIG', 2],  (None, 1, 2)])\n        [<Signal SIG:\n                samples=[ 1.  1.  1.  1.  1.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        , <Signal SIG:\n                samples=[ 31.  31.  31.  31.  31.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        , <Signal SIG:\n                samples=[ 21.  21.  21.  21.  21.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        , <Signal SIG:\n                samples=[ 12.  12.  12.  12.  12.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        ]\n\n        \"\"\"\n\n        # group channels by group index\n        gps = {}\n\n        indexes = []\n\n        for item in channels:\n            if isinstance(item, (list, tuple)):\n                if len(item) not in (2, 3):\n                    raise MdfException(\n                        \"The items used for filtering must be strings, \"\n                        \"or they must match the first 3 argumens of the get \"\n                        \"method\"\n                    )\n                else:\n                    group, index = self._validate_channel_selection(*item)\n                    indexes.append((group, index))\n                    if group not in gps:\n                        gps[group] = {index}\n                    else:\n                        gps[group].add(index)\n            else:\n                name = item\n                group, index = self._validate_channel_selection(name)\n                indexes.append((group, index))\n                if group not in gps:\n                    gps[group] = {index}\n                else:\n                    gps[group].add(index)\n\n        signals = {}\n\n        for group in gps:\n            grp = self.groups[group]\n            data = self._load_data(grp, record_offset=record_offset)\n            parents, dtypes = self._prepare_record(grp)\n\n            channel_indexes = gps[group]\n\n            signal_parts = defaultdict(list)\n            master_parts = []\n\n            sigs = {}\n\n            for i, fragment in enumerate(data):\n                if dtypes.itemsize:\n                    grp.record = np.core.records.fromstring(fragment[0], dtype=dtypes)\n                else:\n                    grp.record = None\n\n                if i == 0:\n                    for index in channel_indexes:\n                        signal = self.get(\n                            group=group,\n                            index=index,\n                            data=fragment,\n                            copy_master=False,\n                            raw=True,\n                            ignore_invalidation_bits=True,\n                        )\n\n                        sigs[index] = signal\n                        signal_parts[(group, index)].append(\n                            (signal.samples, signal.invalidation_bits)\n                        )\n\n                    master_parts.append(signal.timestamps)\n                else:\n                    for index in channel_indexes:\n                        signal = self.get(\n                            group=group,\n                            index=index,\n                            data=fragment,\n                            copy_master=False,\n                            raw=True,\n                            samples_only=True,\n                            ignore_invalidation_bits=True,\n                        )\n\n                        signal_parts[(group, index)].append(signal)\n\n                    master_parts.append(\n                        self.get_master(\n                            group,\n                            data=fragment,\n                            copy_master=False,\n                        )\n                    )\n                grp.record = None\n\n            pieces = len(master_parts)\n            if pieces > 1:\n                master = np.concatenate(master_parts)\n            else:\n                master = master_parts[0]\n            master_parts = None\n            pairs = list(signal_parts.keys())\n            for pair in pairs:\n                group, index = pair\n                parts = signal_parts.pop(pair)\n                sig = sigs.pop(index)\n\n                if pieces > 1:\n                    samples = np.concatenate(\n                        [part[0] for part in parts]\n                    )\n                    if sig.invalidation_bits is not None:\n                        invalidation_bits = np.concatenate(\n                            [part[0] for part in parts]\n                        )\n                    else:\n                        invalidation_bits = None\n                else:\n                    samples = parts[0][0]\n                    if sig.invalidation_bits is not None:\n                        invalidation_bits = parts[0][1]\n                    else:\n                        invalidation_bits = None\n\n                signals[pair] = Signal(\n                    samples=samples,\n                    timestamps=master,\n                    name=sig.name,\n                    unit=sig.unit,\n                    bit_count=sig.bit_count,\n                    attachment=sig.attachment,\n                    comment=sig.comment,\n                    conversion=sig.conversion,\n                    display_name=sig.display_name,\n                    encoding=sig.encoding,\n                    master_metadata=sig.master_metadata,\n                    raw=True,\n                    source=sig.source,\n                    stream_sync=sig.stream_sync,\n                    invalidation_bits=invalidation_bits,\n                )\n\n        signals = [\n            signals[pair]\n            for pair in indexes\n        ]\n\n        if copy_master:\n            for signal in signals:\n                signal.timestamps = signal.timestamps.copy()\n\n        if not raw:\n            for signal in signals:\n                conversion = signal.conversion\n                if conversion:\n                    signal.samples = conversion.convert(signal.samples)\n                raw = False\n                signal.conversion = None\n\n        if dataframe:\n\n            interpolation_mode = self._integer_interpolation\n            times = [s.timestamps for s in signals]\n            master = reduce(np.union1d, times).flatten().astype(np.float64)\n            signals = [\n                s.interp(master, interpolation_mode=interpolation_mode)\n                for s in signals\n            ]\n\n            df = pd.DataFrame()\n\n            df[\"time\"] = pd.Series(master, index=np.arange(len(master)))\n\n            df.set_index('time', inplace=True)\n\n            used_names = UniqueDB()\n            used_names.get_unique_name(\"time\")\n\n            for k, sig in enumerate(signals):\n                # byte arrays\n                if len(sig.samples.shape) > 1:\n                    arr = [sig.samples]\n                    types = [(sig.name, sig.samples.dtype, sig.samples.shape[1:])]\n                    sig.samples = np.core.records.fromarrays(arr, dtype=types)\n\n                    channel_name = used_names.get_unique_name(sig.name)\n                    df[channel_name] = pd.Series(sig.samples, index=master, dtype=\"O\")\n\n                # arrays and structures\n                elif sig.samples.dtype.names:\n                    for name, series in components(sig.samples, sig.name, used_names, master=master):\n                        df[name] = series\n\n                # scalars\n                else:\n                    channel_name = used_names.get_unique_name(sig.name)\n                    df[channel_name] = pd.Series(sig.samples, index=master)\n\n            return df\n        else:\n            return signals"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef whereis(self, channel):\n        if channel in self:\n            return tuple(self.channels_db[channel])\n        else:\n            return tuple()", "response": "get ocurrences of channel name in the file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nscrambles text blocks and keep original file structure", "response": "def scramble(name, **kwargs):\n        \"\"\" scramble text blocks and keep original file structure\n\n        Parameters\n        ----------\n        name : str | pathlib.Path\n            file name\n\n        Returns\n        -------\n        name : str\n            scrambled file name\n\n        \"\"\"\n\n        name = Path(name)\n\n        mdf = MDF(name)\n        texts = {}\n\n        callback = kwargs.get(\"callback\", None)\n        if callback:\n            callback(0, 100)\n\n        count = len(mdf.groups)\n\n        if mdf.version >= \"4.00\":\n            ChannelConversion = ChannelConversionV4\n\n            stream = mdf._file\n\n            if mdf.header.comment_addr:\n                stream.seek(mdf.header.comment_addr + 8)\n                size = UINT64_u(stream.read(8))[0] - 24\n                texts[mdf.header.comment_addr] = randomized_string(size)\n\n            for fh in mdf.file_history:\n                addr = fh.comment_addr\n                if addr and addr not in texts:\n                    stream.seek(addr + 8)\n                    size = UINT64_u(stream.read(8))[0] - 24\n                    texts[addr] = randomized_string(size)\n\n            for ev in mdf.events:\n                for addr in (ev.comment_addr, ev.name_addr):\n                    if addr and addr not in texts:\n                        stream.seek(addr + 8)\n                        size = UINT64_u(stream.read(8))[0] - 24\n                        texts[addr] = randomized_string(size)\n\n            for idx, gp in enumerate(mdf.groups, 1):\n\n                addr = gp.data_group.comment_addr\n                if addr and addr not in texts:\n                    stream.seek(addr + 8)\n                    size = UINT64_u(stream.read(8))[0] - 24\n                    texts[addr] = randomized_string(size)\n\n                cg = gp.channel_group\n                for addr in (cg.acq_name_addr, cg.comment_addr):\n                    if cg.flags & v4c.FLAG_CG_BUS_EVENT:\n                        continue\n\n                    if addr and addr not in texts:\n                        stream.seek(addr + 8)\n                        size = UINT64_u(stream.read(8))[0] - 24\n                        texts[addr] = randomized_string(size)\n\n                    source = cg.acq_source_addr\n                    if source:\n                        source = SourceInformation(address=source, stream=stream)\n                        for addr in (\n                            source.name_addr,\n                            source.path_addr,\n                            source.comment_addr,\n                        ):\n                            if addr and addr not in texts:\n                                stream.seek(addr + 8)\n                                size = UINT64_u(stream.read(8))[0] - 24\n                                texts[addr] = randomized_string(size)\n\n                for ch in gp.channels:\n\n                    for addr in (ch.name_addr, ch.unit_addr, ch.comment_addr):\n                        if addr and addr not in texts:\n                            stream.seek(addr + 8)\n                            size = UINT64_u(stream.read(8))[0] - 24\n                            texts[addr] = randomized_string(size)\n\n                    source = ch.source_addr\n                    if source:\n                        source = SourceInformation(address=source, stream=stream)\n                        for addr in (\n                            source.name_addr,\n                            source.path_addr,\n                            source.comment_addr,\n                        ):\n                            if addr and addr not in texts:\n                                stream.seek(addr + 8)\n                                size = UINT64_u(stream.read(8))[0] - 24\n                                texts[addr] = randomized_string(size)\n\n                    conv = ch.conversion_addr\n                    if conv:\n                        conv = ChannelConversion(address=conv, stream=stream)\n                        for addr in (conv.name_addr, conv.unit_addr, conv.comment_addr):\n                            if addr and addr not in texts:\n                                stream.seek(addr + 8)\n                                size = UINT64_u(stream.read(8))[0] - 24\n                                texts[addr] = randomized_string(size)\n                        if conv.conversion_type == v4c.CONVERSION_TYPE_ALG:\n                            addr = conv.formula_addr\n                            if addr and addr not in texts:\n                                stream.seek(addr + 8)\n                                size = UINT64_u(stream.read(8))[0] - 24\n                                texts[addr] = randomized_string(size)\n\n                        if conv.referenced_blocks:\n                            for key, block in conv.referenced_blocks.items():\n                                if block:\n                                    if block.id == b\"##TX\":\n                                        addr = block.address\n                                        if addr not in texts:\n                                            stream.seek(addr + 8)\n                                            size = block.block_len - 24\n                                            texts[addr] = randomized_string(size)\n\n                if callback:\n                    callback(int(idx/count*66), 100)\n\n            mdf.close()\n\n            dst = name.with_suffix(\".scrambled.mf4\")\n\n            copy(name, dst)\n\n            with open(dst, \"rb+\") as mdf:\n                count = len(texts)\n                chunk = max(count // 34, 1)\n                idx = 0\n                for index, (addr, bts) in enumerate(texts.items()):\n                    mdf.seek(addr + 24)\n                    mdf.write(bts)\n                    if index % chunk == 0:\n                        if callback:\n                            callback(66 + idx, 100)\n\n            if callback:\n                callback(100, 100)\n\n        else:\n            ChannelConversion = ChannelConversionV3\n\n            stream = mdf._file\n\n            if mdf.header.comment_addr:\n                stream.seek(mdf.header.comment_addr + 2)\n                size = UINT16_u(stream.read(2))[0] - 4\n                texts[mdf.header.comment_addr + 4] = randomized_string(size)\n            texts[36 + 0x40] = randomized_string(32)\n            texts[68 + 0x40] = randomized_string(32)\n            texts[100 + 0x40] = randomized_string(32)\n            texts[132 + 0x40] = randomized_string(32)\n\n            for idx, gp in enumerate(mdf.groups, 1):\n\n                cg = gp.channel_group\n                addr = cg.comment_addr\n\n                if addr and addr not in texts:\n                    stream.seek(addr + 2)\n                    size = UINT16_u(stream.read(2))[0] - 4\n                    texts[addr + 4] = randomized_string(size)\n\n                if gp.trigger:\n                    addr = gp.trigger.text_addr\n                    if addr:\n                        stream.seek(addr + 2)\n                        size = UINT16_u(stream.read(2))[0] - 4\n                        texts[addr + 4] = randomized_string(size)\n\n                for ch in gp.channels:\n\n                    for key in (\"long_name_addr\", \"display_name_addr\", \"comment_addr\"):\n                        if hasattr(ch, key):\n                            addr = getattr(ch, key)\n                        else:\n                            addr = 0\n                        if addr and addr not in texts:\n                            stream.seek(addr + 2)\n                            size = UINT16_u(stream.read(2))[0] - 4\n                            texts[addr + 4] = randomized_string(size)\n\n                    texts[ch.address + 26] = randomized_string(32)\n                    texts[ch.address + 58] = randomized_string(128)\n\n                    source = ch.source_addr\n                    if source:\n                        source = ChannelExtension(address=source, stream=stream)\n                        if source.type == v23c.SOURCE_ECU:\n                            texts[source.address + 12] = randomized_string(80)\n                            texts[source.address + 92] = randomized_string(32)\n                        else:\n                            texts[source.address + 14] = randomized_string(36)\n                            texts[source.address + 50] = randomized_string(36)\n\n                    conv = ch.conversion_addr\n                    if conv:\n                        texts[conv + 22] = randomized_string(20)\n\n                        conv = ChannelConversion(address=conv, stream=stream)\n\n                        if conv.conversion_type == v23c.CONVERSION_TYPE_FORMULA:\n                            texts[conv + 36] = randomized_string(conv.block_len - 36)\n\n                        if conv.referenced_blocks:\n                            for key, block in conv.referenced_blocks.items():\n                                if block:\n                                    if block.id == b\"TX\":\n                                        addr = block.address\n                                        if addr and addr not in texts:\n                                            stream.seek(addr + 2)\n                                            size = UINT16_u(stream.read(2))[0] - 4\n                                            texts[addr + 4] = randomized_string(size)\n                if callback:\n                    callback(int(idx/count*66), 100)\n\n            mdf.close()\n\n            dst = name.with_suffix(\".scrambled.mf4\")\n\n            copy(name, dst)\n\n            with open(dst, \"rb+\") as mdf:\n                chunk = count // 34\n                idx = 0\n                for index, (addr, bts) in enumerate(texts.items()):\n                    mdf.seek(addr)\n                    mdf.write(bts)\n                    if chunk and index % chunk == 0:\n                        if callback:\n                            callback(66 + idx, 100)\n\n            if callback:\n                callback(100, 100)\n\n        return dst"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting channel group as pandas DataFrames", "response": "def get_group(\n        self,\n        index,\n        raster=None,\n        time_from_zero=False,\n        use_display_names=False,\n        reduce_memory_usage=False,\n    ):\n        \"\"\" get channel group as pandas DataFrames. If there are multiple\n        occurences for the same channel name, then a counter will be used to\n        make the names unique (<original_name>_<counter>)\n\n        Parameters\n        ----------\n        index : int\n            channel group index\n        use_display_names : bool\n            use display name instead of standard channel name, if available.\n        reduce_memory_usage : bool\n            reduce memory usage by converting all float columns to float32 and\n            searching for minimum dtype that can reprezent the values found\n            in integer columns; default *False*\n\n        Returns\n        -------\n        df : pandas.DataFrame\n\n        \"\"\"\n\n        interpolation_mode = self._integer_interpolation\n\n        df = pd.DataFrame()\n        master = self.get_master(index, copy_master=False)\n\n        if raster and len(master):\n            if len(master) > 1:\n                num = float(np.float32((master[-1] - master[0]) / raster))\n                if num.is_integer():\n                    master = np.linspace(master[0], master[-1], int(num))\n                else:\n                    master = np.arange(master[0], master[-1], raster, dtype=np.float64)\n\n        df[\"time\"] = pd.Series(master, index=np.arange(len(master)))\n\n        df.set_index('time', inplace=True)\n\n        used_names = UniqueDB()\n        used_names.get_unique_name(\"time\")\n\n        included_channels = self._included_channels(index)\n\n        signals = [\n            self.get(group=index, index=idx, copy_master=False)\n            for idx in included_channels\n        ]\n\n        for i, sig in enumerate(signals):\n            if sig.conversion:\n                signals[i] = sig.physical()\n\n        if raster and len(master):\n            signals = [\n                sig.interp(master, interpolation_mode=interpolation_mode)\n                for sig in signals\n            ]\n\n        for sig in signals:\n            # byte arrays\n            if len(sig.samples.shape) > 1:\n                arr = [sig.samples]\n                types = [(sig.name, sig.samples.dtype, sig.samples.shape[1:])]\n                sig.samples = np.core.records.fromarrays(arr, dtype=types)\n\n                if use_display_names:\n                    channel_name = sig.display_name or sig.name\n                else:\n                    channel_name = sig.name\n\n                channel_name = used_names.get_unique_name(channel_name)\n\n                df[channel_name] = pd.Series(sig.samples, index=master, dtype=\"O\")\n\n            # arrays and structures\n            elif sig.samples.dtype.names:\n                for name, series in components(sig.samples, sig.name, used_names, master=master):\n                    df[name] = series\n\n            # scalars\n            else:\n                if use_display_names:\n                    channel_name = sig.display_name or sig.name\n                else:\n                    channel_name = sig.name\n\n                channel_name = used_names.get_unique_name(channel_name)\n\n                if sig.samples.dtype.kind in 'SU':\n                    unique = np.unique(sig.samples)\n                    if len(sig.samples) / len(unique) >= 2:\n                        df[channel_name] = pd.Series(sig.samples, index=master, dtype=\"category\")\n                    else:\n                        df[channel_name] = pd.Series(sig.samples, index=master)\n                else:\n                    if reduce_memory_usage:\n                        sig.samples = downcast(sig.samples)\n                    df[channel_name] = pd.Series(sig.samples, index=master)\n\n            if use_display_names:\n                channel_name = sig.display_name or sig.name\n            else:\n                channel_name = sig.name\n\n        if time_from_zero and len(master):\n            df.set_index(df.index - df.index[0], inplace=True)\n\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating pandas DataFrame of the entry set", "response": "def to_dataframe(\n        self,\n        channels=None,\n        raster=None,\n        time_from_zero=True,\n        empty_channels=\"skip\",\n        keep_arrays=False,\n        use_display_names=False,\n        time_as_date=False,\n        reduce_memory_usage=False,\n    ):\n        \"\"\" generate pandas DataFrame\n\n        Parameters\n        ----------\n        channels : list\n            filter a subset of channels; default *None*\n        raster : float | np.array | str\n            new raster that can be\n\n            * a float step value\n            * a channel name who's timestamps will be used as raster\n            * an array\n\n            see `resample` for examples of urisng this argument\n\n        time_from_zero : bool\n            adjust time channel to start from 0; default *True*\n        empty_channels : str\n            behaviour for channels without samples; the options are *skip* or\n            *zeros*; default is *skip*\n        use_display_names : bool\n            use display name instead of standard channel name, if available.\n        keep_arrays : bool\n            keep arrays and structure channels as well as the\n            component channels. If *True* this can be very slow. If *False*\n            only the component channels are saved, and their names will be\n            prefixed with the parent channel.\n        time_as_date : bool\n            the dataframe index will contain the datetime timestamps\n            according to the measurement start time; default *False*. If\n            *True* then the argument ``time_from_zero`` will be ignored.\n        reduce_memory_usage : bool\n            reduce memory usage by converting all float columns to float32 and\n            searching for minimum dtype that can reprezent the values found\n            in integer columns; default *False*\n\n        Returns\n        -------\n        dataframe : pandas.DataFrame\n\n        \"\"\"\n\n        if channels:\n            mdf = self.filter(channels)\n            return mdf.to_dataframe(\n                raster=raster,\n                time_from_zero=time_from_zero,\n                empty_channels=empty_channels,\n                keep_arrays=keep_arrays,\n                use_display_names=use_display_names,\n                time_as_date=time_as_date,\n                reduce_memory_usage=reduce_memory_usage,\n            )\n\n        df = pd.DataFrame()\n\n        if raster:\n            try:\n                raster = float(raster)\n                assert raster > 0\n            except (TypeError, ValueError):\n                if isinstance(raster, str):\n                    raster = self.get(raster).timestamps\n                else:\n                    raster = np.array(raster)\n            else:\n                raster = master_using_raster(self, raster)\n        else:\n            masters = [\n                self.get_master(i, copy_master=False)\n                for i, _ in enumerate(self.groups)\n            ]\n            self._master_channel_cache.clear()\n            master = reduce(np.union1d, masters)\n\n        df[\"time\"] = pd.Series(master, index=np.arange(len(master)))\n\n        df.set_index('time', inplace=True)\n\n        used_names = UniqueDB()\n        used_names.get_unique_name(\"time\")\n\n        for group_index, grp in enumerate(self.groups):\n            if grp.channel_group.cycles_nr == 0 and empty_channels == \"skip\":\n                continue\n\n            included_channels = self._included_channels(group_index)\n\n            channels = grp.channels\n\n            data = self._load_data(grp)\n            _, dtypes = self._prepare_record(grp)\n\n            signals = [[] for _ in included_channels]\n            invalidation_bits = [[] for _ in included_channels]\n            timestamps = []\n\n            for fragment in data:\n                if dtypes.itemsize:\n                    grp.record = np.core.records.fromstring(fragment[0], dtype=dtypes)\n                else:\n                    grp.record = None\n                for k, index in enumerate(included_channels):\n                    signal = self.get(\n                        group=group_index, index=index, data=fragment, samples_only=True, ignore_invalidation_bits=True\n                    )\n                    signals[k].append(signal[0])\n                    invalidation_bits[k].append(signal[1])\n                timestamps.append(\n                    self.get_master(\n                        group_index,\n                        data=fragment,\n                        copy_master=False,\n                    )\n                )\n\n                grp.record = None\n\n            if len(timestamps):\n                signals = [np.concatenate(parts) for parts in signals]\n\n                timestamps = np.concatenate(timestamps)\n                for idx, parts in enumerate(invalidation_bits):\n                    if parts[0] is None:\n                        invalidation_bits[idx] = None\n                    else:\n                        invalidation_bits[idx] = np.concatenate(parts)\n\n            signals = [\n                Signal(\n                    samples,\n                    timestamps,\n                    name=channels[idx].name,\n                    invalidation_bits=invalidation,\n                )\n                .validate()\n                .interp(\n                    master,\n                    self._integer_interpolation,\n                )\n                for samples, invalidation, idx in zip(signals, invalidation_bits, included_channels)\n            ]\n\n            for i, sig in enumerate(signals):\n                if sig.conversion:\n                    signals[i] = sig.physical()\n\n            for sig in signals:\n                if len(sig) == 0:\n                    if empty_channels == \"zeros\":\n                        sig.samples = np.zeros(len(master), dtype=sig.samples.dtype)\n                    else:\n                        continue\n\n            signals = [sig for sig in signals if len(sig)]\n\n            for k, sig in enumerate(signals):\n                # byte arrays\n                if len(sig.samples.shape) > 1:\n                    arr = [sig.samples]\n                    types = [(sig.name, sig.samples.dtype, sig.samples.shape[1:])]\n                    sig.samples = np.core.records.fromarrays(arr, dtype=types)\n\n                    if use_display_names:\n                        channel_name = sig.display_name or sig.name\n                    else:\n                        channel_name = sig.name\n\n                    channel_name = used_names.get_unique_name(channel_name)\n\n                    df[channel_name] = pd.Series(sig.samples, index=master, dtype=\"O\")\n\n                # arrays and structures\n                elif sig.samples.dtype.names:\n                    for name, series in components(sig.samples, sig.name, used_names, master=master):\n                        df[name] = series\n\n                # scalars\n                else:\n                    if use_display_names:\n                        channel_name = sig.display_name or sig.name\n                    else:\n                        channel_name = sig.name\n\n                    channel_name = used_names.get_unique_name(channel_name)\n\n                    if sig.samples.dtype.kind in 'SU':\n                        unique = np.unique(sig.samples)\n                        if len(sig.samples) / len(unique) >= 2:\n                            df[channel_name] = pd.Series(sig.samples, index=master, dtype=\"category\")\n                        else:\n                            df[channel_name] = pd.Series(sig.samples, index=master)\n                    else:\n                        if reduce_memory_usage:\n                            sig.samples = downcast(sig.samples)\n                        df[channel_name] = pd.Series(sig.samples, index=master)\n\n        if time_as_date:\n            new_index = np.array(df.index) + self.header.start_time.timestamp()\n            new_index = pd.to_datetime(new_index, unit='s')\n\n            df.set_index(new_index, inplace=True)\n        elif time_from_zero and len(master):\n            df.set_index(df.index - df.index[0], inplace=True)\n\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_can_logging(self, dbc_files, version=None):\n        if version is None:\n            version = self.version\n        else:\n            version = validate_version_argument(version)\n\n        out = MDF(version=version, callback=self._callback)\n        out.header.start_time = self.header.start_time\n\n        if self._callback:\n            out._callback = out._mdf._callback = self._callback\n\n        valid_dbc_files = []\n        for dbc_name in dbc_files:\n            dbc_name = Path(dbc_name)\n\n            if dbc_name.exists() and dbc_name.suffix.lower() in ('.dbc', '.arxml'):\n                import_type = dbc_name.suffix.lower().strip('.')\n                contents = dbc_name.read_bytes()\n                try:\n                    dbc = loads(\n                        contents,\n                        import_type=import_type,\n                        key=\"db\",\n                    )[\"db\"]\n                    valid_dbc_files.append(dbc)\n                except UnicodeDecodeError:\n                    try:\n                        from cchardet import detect\n\n                        encoding = detect(contents)[\"encoding\"]\n                        contents = contents.decode(\n                            encoding\n                        )\n                        dbc = loads(\n                            contents,\n                            importType=import_type,\n                            key=\"db\",\n                            encoding=encoding,\n                        )[\"db\"]\n                        valid_dbc_files.append(dbc)\n                    except ImportError:\n                        message = (\n                            \"Unicode exception occured while processing the database \"\n                            f'\"{dbc_name}\" and \"cChardet\" package is '\n                            'not installed. Mdf version 4 expects \"utf-8\" '\n                            \"strings and this package may detect if a different\"\n                            \" encoding was used\"\n                        )\n                        logger.warning(message)\n                        continue\n\n        count = sum(\n            1\n            for group in self.groups\n            if group.CAN_logging\n        )\n        count *= len(valid_dbc_files)\n\n        cntr = 0\n\n        for dbc in valid_dbc_files:\n            is_j1939 = dbc.contains_j1939\n            if is_j1939:\n                messages = {\n                    message.arbitration_id.pgn: message\n                    for message in dbc\n                }\n            else:\n                messages = {\n                    message.arbitration_id.id: message\n                    for message in dbc\n                }\n\n            msg_map = {}\n\n            for i, group in enumerate(self.groups):\n                if not group.CAN_logging:\n                    continue\n\n                parents, dtypes = self._prepare_record(group)\n                data = self._load_data(group)\n\n                for fragment_index, fragment in enumerate(data):\n                    if dtypes.itemsize:\n                        group.record = np.core.records.fromstring(fragment[0], dtype=dtypes)\n                    else:\n                        group.record = None\n                        continue\n\n                    bus_ids = self.get(\n                        'CAN_DataFrame.BusChannel',\n                        group=i,\n                        data=fragment,\n                        samples_only=True,\n                    )[0].astype('<u1')\n\n                    msg_ids = self.get(\n                        'CAN_DataFrame.ID',\n                        group=i,\n                        data=fragment,\n                        copy_master=False,\n                    )\n\n                    if is_j1939:\n                        ps = (msg_ids.samples >> 8) & 0xFF\n                        pf = (msg_ids.samples >> 16) & 0xFF\n                        _pgn = pf << 8\n                        msg_ids.samples = np.where(\n                            pf >= 240,\n                            _pgn + ps,\n                            _pgn,\n                        )\n\n                    data_bytes = self.get(\n                        'CAN_DataFrame.DataBytes',\n                        group=i,\n                        data=fragment,\n                        samples_only=True,\n                    )[0]\n\n                    buses = np.unique(bus_ids)\n\n                    for bus in buses:\n                        idx = np.argwhere(bus_ids == bus).ravel()\n                        bus_t = msg_ids.timestamps[idx]\n                        bus_msg_ids = msg_ids.samples[idx]\n                        bus_data_bytes = data_bytes[idx]\n\n                        unique_ids = np.unique(bus_msg_ids)\n\n                        for msg_id in unique_ids:\n                            message = messages.get(msg_id, None)\n                            if message is None:\n                                continue\n\n                            idx = np.argwhere(bus_msg_ids == msg_id).ravel()\n                            payload = bus_data_bytes[idx]\n                            t = bus_t[idx]\n\n                            entry = bus, msg_id\n                            if entry not in msg_map:\n                                sigs = []\n                                for signal in sorted(message.signals, key=lambda x: x.name):\n                                    sig = Signal(\n                                        samples=extract_can_signal(signal, payload),\n                                        timestamps=t,\n                                        name=signal.name,\n                                        comment=signal.comment or \"\",\n                                        unit=signal.unit or \"\"\n                                    )\n                                    sig.comment = f\"\"\"\\\n<CNcomment>\n<TX>{sig.comment}</TX>\n<names>\n    <display>\n        CAN{bus}.{message.name}.{signal.name}\n    </display>\n</names>\n</CNcomment>\"\"\"\n\n                                    sigs.append(sig)\n\n                                msg_map[entry] = len(out.groups)\n                                out.append(\n                                    sigs,\n                                    f'from CAN{bus} message ID=0x{msg_id:X}',\n                                    common_timebase=True,\n                                )\n                            else:\n                                index = msg_map[entry]\n                                sigs = [(t, None)]\n                                for signal in sorted(message.signals, key=lambda x: x.name):\n                                    sigs.append(\n                                        (extract_can_signal(signal, payload), None)\n                                    )\n                                out.extend(index, sigs)\n\n                cntr += 1\n                if self._callback:\n                    self._callback(cntr, count)\n\n        if self._callback:\n            self._callback(100, 100)\n        if not out.groups:\n            logger.warning(\n                f'No CAN signals could be extracted from \"{self.name}\". The'\n                'output file will be empty.'\n            )\n        return out", "response": "Extract all possible CAN signal using the provided databases."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef conversion_transfer(conversion, version=3):\n\n    if version <= 3:\n        if conversion is None:\n            conversion = v3b.ChannelConversion(conversion_type=v3c.CONVERSION_TYPE_NONE)\n        else:\n            conversion_type = conversion[\"conversion_type\"]\n            if conversion[\"id\"] == b\"CC\":\n                pass\n            else:\n                unit = conversion.unit.strip(\" \\r\\n\\t\\0\").encode(\"latin-1\")\n\n                if conversion_type == v4c.CONVERSION_TYPE_NON:\n                    conversion = v3b.ChannelConversion(\n                        unit=unit, conversion_type=v3c.CONVERSION_TYPE_NONE\n                    )\n\n                elif conversion_type == v4c.CONVERSION_TYPE_LIN:\n                    conversion = v3b.ChannelConversion(\n                        unit=unit,\n                        conversion_type=v3c.CONVERSION_TYPE_LINEAR,\n                        a=conversion.a,\n                        b=conversion.b,\n                    )\n\n                elif conversion_type == v4c.CONVERSION_TYPE_RAT:\n                    conversion = v3b.ChannelConversion(\n                        unit=unit,\n                        conversion_type=v3c.CONVERSION_TYPE_RAT,\n                        P1=conversion.P1,\n                        P2=conversion.P2,\n                        P3=conversion.P3,\n                        P4=conversion.P4,\n                        P5=conversion.P5,\n                        P6=conversion.P6,\n                    )\n\n                elif conversion_type == v4c.CONVERSION_TYPE_TAB:\n                    conversion_ = {}\n                    conversion_[\"ref_param_nr\"] = conversion.val_param_nr // 2\n                    for i in range(conversion.val_param_nr // 2):\n                        conversion_[f\"raw_{i}\"] = conversion[f\"raw_{i}\"]\n                        conversion_[f\"phys_{i}\"] = conversion[f\"phys_{i}\"]\n\n                    conversion = v3b.ChannelConversion(\n                        unit=unit,\n                        conversion_type=v3c.CONVERSION_TYPE_TAB,\n                        **conversion_,\n                    )\n\n                elif conversion_type == v4c.CONVERSION_TYPE_TABI:\n                    conversion_ = {}\n                    conversion_[\"ref_param_nr\"] = conversion.val_param_nr // 2\n                    for i in range(conversion.val_param_nr // 2):\n                        conversion_[f\"raw_{i}\"] = conversion[f\"raw_{i}\"]\n                        conversion_[f\"phys_{i}\"] = conversion[f\"phys_{i}\"]\n\n                    conversion = v3b.ChannelConversion(\n                        unit=unit,\n                        conversion_type=v3c.CONVERSION_TYPE_TABI,\n                        **conversion_,\n                    )\n\n                elif conversion_type == v4c.CONVERSION_TYPE_ALG:\n                    formula = conversion.formula.replace(\"X\", \"X1\")\n                    conversion = v3b.ChannelConversion(\n                        formula=formula,\n                        unit=unit,\n                        conversion_type=v3c.CONVERSION_TYPE_FORMULA,\n                    )\n\n                elif conversion_type == v4c.CONVERSION_TYPE_RTAB:\n                    nr = (conversion.val_param_nr - 1) // 3\n                    kargs = {\n                        \"ref_param_nr\": nr,\n                        \"conversion_type\": v3c.CONVERSION_TYPE_TABI,\n                    }\n\n                    for i in range(nr):\n                        l_ = conversion[f\"lower_{i}\"]\n                        u_ = conversion[f\"upper_{i}\"]\n                        p_ = conversion[f\"phys_{i}\"]\n                        kargs[f\"raw_{i}\"] = l_\n                        kargs[f\"raw_{i}\"] = u_ - 0.000_001\n                        kargs[f\"phys_{i}\"] = p_\n                        kargs[f\"phys_{i}\"] = p_\n\n                    conversion = v3b.ChannelConversion(unit=unit, **kargs)\n\n                elif conversion_type == v4c.CONVERSION_TYPE_TABX:\n                    nr = conversion.val_param_nr\n                    kargs = {\n                        \"ref_param_nr\": nr,\n                        \"unit\": unit,\n                        \"conversion_type\": v3c.CONVERSION_TYPE_TABX,\n                    }\n                    for i in range(nr):\n                        kargs[f\"param_val_{i}\"] = conversion[f\"val_{i}\"]\n                        kargs[f\"text_{i}\"] = conversion.referenced_blocks[\n                            f\"text_{i}\"\n                        ].text\n\n                    conversion = v3b.ChannelConversion(**kargs)\n\n                elif conversion_type == v4c.CONVERSION_TYPE_RTABX:\n                    nr = conversion.val_param_nr // 2\n                    kargs = {\n                        \"ref_param_nr\": nr + 1,\n                        \"unit\": unit,\n                        \"conversion_type\": v3c.CONVERSION_TYPE_RTABX,\n                    }\n                    for i in range(nr):\n                        kargs[f\"lower_{i}\"] = conversion[f\"lower_{i}\"]\n                        kargs[f\"upper_{i}\"] = conversion[f\"upper_{i}\"]\n                        kargs[f\"text_{i}\"] = conversion.referenced_blocks[\n                            f\"text_{i}\"\n                        ].text\n\n                    new_conversion = v3b.ChannelConversion(**kargs)\n\n                    new_conversion.referenced_blocks[\"default_addr\"] = v3b.TextBlock(\n                        text=conversion.referenced_blocks[\"default_addr\"].text\n                    )\n                    new_conversion\n\n                    conversion = new_conversion\n\n    else:\n        if conversion is None or conversion[\"id\"] == b\"##CC\":\n            pass\n        else:\n            conversion_type = conversion[\"conversion_type\"]\n            unit = conversion[\"unit_field\"].decode(\"latin-1\").strip(\" \\r\\n\\t\\0\")\n            if conversion_type == v3c.CONVERSION_TYPE_NONE:\n                conversion = v4b.ChannelConversion(\n                    conversion_type=v4c.CONVERSION_TYPE_NON\n                )\n\n            elif conversion_type == v3c.CONVERSION_TYPE_LINEAR:\n                conversion = v4b.ChannelConversion(\n                    conversion_type=v4c.CONVERSION_TYPE_LIN,\n                    a=conversion.a,\n                    b=conversion.b,\n                )\n\n            elif conversion_type == v3c.CONVERSION_TYPE_RAT:\n                conversion = v4b.ChannelConversion(\n                    conversion_type=v4c.CONVERSION_TYPE_RAT,\n                    P1=conversion.P1,\n                    P2=conversion.P2,\n                    P3=conversion.P3,\n                    P4=conversion.P4,\n                    P5=conversion.P5,\n                    P6=conversion.P6,\n                )\n\n            elif conversion_type == v3c.CONVERSION_TYPE_FORMULA:\n                formula = conversion.formula\n                conversion = v4b.ChannelConversion(\n                    conversion_type=v4c.CONVERSION_TYPE_ALG, formula=formula\n                )\n\n            elif conversion_type == v3c.CONVERSION_TYPE_TAB:\n                conversion_ = {}\n                conversion_[\"val_param_nr\"] = conversion.ref_param_nr * 2\n                for i in range(conversion.val_param_nr):\n                    conversion_[f\"raw_{i}\"] = conversion[f\"raw_{i}\"]\n                    conversion_[f\"phys_{i}\"] = conversion[f\"phys_{i}\"]\n\n                conversion = v4b.ChannelConversion(\n                    conversion_type=v4c.CONVERSION_TYPE_TAB, **conversion_\n                )\n\n            elif conversion_type == v3c.CONVERSION_TYPE_TABI:\n                conversion_ = {}\n                conversion_[\"val_param_nr\"] = conversion.ref_param_nr * 2\n                for i in range(conversion.ref_param_nr):\n                    conversion_[f\"raw_{i}\"] = conversion[f\"raw_{i}\"]\n                    conversion_[f\"phys_{i}\"] = conversion[f\"phys_{i}\"]\n\n                conversion = v4b.ChannelConversion(\n                    conversion_type=v4c.CONVERSION_TYPE_TABI, **conversion_\n                )\n\n            elif conversion_type == v3c.CONVERSION_TYPE_TABX:\n\n                nr = conversion[\"ref_param_nr\"]\n                kargs = {\n                    \"val_param_nr\": nr,\n                    \"ref_param_nr\": nr + 1,\n                    \"conversion_type\": v4c.CONVERSION_TYPE_TABX,\n                }\n                for i in range(nr):\n                    kargs[f\"val_{i}\"] = conversion[f\"param_val_{i}\"]\n                    kargs[f\"text_{i}\"] = conversion[f\"text_{i}\"]\n\n                conversion = v4b.ChannelConversion(**kargs)\n\n            elif conversion_type == v3c.CONVERSION_TYPE_RTABX:\n#                print('IN', conversion)\n\n                nr = conversion[\"ref_param_nr\"] - 1\n                kargs = {\n                    \"val_param_nr\": nr * 2,\n                    \"ref_param_nr\": nr + 1,\n                    \"conversion_type\": v4c.CONVERSION_TYPE_RTABX,\n                    \"default_addr\": conversion.referenced_blocks[\"default_addr\"].text,\n                }\n                for i in range(nr):\n                    kargs[f\"lower_{i}\"] = conversion[f\"lower_{i}\"]\n                    kargs[f\"upper_{i}\"] = conversion[f\"upper_{i}\"]\n                    kargs[f\"text_{i}\"] = conversion.referenced_blocks[f\"text_{i}\"].text\n\n                conversion = v4b.ChannelConversion(**kargs)\n\n#                print('OUT', conversion)\n#                print('\\n'*3)\n\n            conversion.unit = unit\n\n    return conversion", "response": "convert between mdf4 and mdf3 channel conversions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_signal_data(self, address=None, stream=None, group=None, index=None):\n\n        if address == 0:\n            data = b\"\"\n\n        elif address is not None and stream is not None:\n            stream.seek(address)\n            blk_id = stream.read(4)\n            if blk_id == b\"##SD\":\n                data = DataBlock(address=address, stream=stream)\n                data = data.data\n            elif blk_id == b\"##DZ\":\n                data = DataZippedBlock(address=address, stream=stream)\n                data = data.data\n            elif blk_id == b\"##CG\":\n                group = self.groups[self._cg_map[address]]\n                data = b\"\".join(fragment[0] for fragment in self._load_data(group))\n            elif blk_id == b\"##DL\":\n                data = []\n                while address:\n                    # the data list will contain only links to SDBLOCK's\n                    data_list = DataList(address=address, stream=stream)\n                    nr = data_list.links_nr\n                    # aggregate data from all SDBLOCK\n                    for i in range(nr - 1):\n                        addr = data_list[f\"data_block_addr{i}\"]\n                        stream.seek(addr)\n                        blk_id = stream.read(4)\n                        if blk_id == b\"##SD\":\n                            block = DataBlock(address=addr, stream=stream)\n                            data.append(block.data)\n                        elif blk_id == b\"##DZ\":\n                            block = DataZippedBlock(address=addr, stream=stream)\n                            data.append(block.data)\n                        else:\n                            message = f'Expected SD, DZ or DL block at {hex(address)} but found id=\"{blk_id}\"'\n                            logger.warning(message)\n                            return b\"\"\n                    address = data_list.next_dl_addr\n                data = b\"\".join(data)\n            elif blk_id == b\"##CN\":\n                data = b\"\"\n            elif blk_id == b\"##HL\":\n                hl = HeaderList(address=address, stream=stream)\n\n                data = self._load_signal_data(\n                    address=hl.first_dl_addr, stream=stream, group=group, index=index\n                )\n            elif blk_id == b\"##AT\":\n                data = b\"\"\n            else:\n                message = f'Expected AT, CG, SD, DL, DZ or CN block at {hex(address)} but found id=\"{blk_id}\"'\n                logger.warning(message)\n                data = b\"\"\n\n        elif group is not None and index is not None:\n            if group.data_location == v4c.LOCATION_ORIGINAL_FILE:\n                data = self._load_signal_data(\n                    address=group.signal_data[index], stream=self._file\n                )\n            elif group.data_location == v4c.LOCATION_MEMORY:\n                data = group.signal_data[index]\n            else:\n                data = []\n                stream = self._tempfile\n                address = group.signal_data[index]\n                if address:\n                    if address[0] in self._cg_map:\n                        group = self.groups[self._cg_map[address[0]]]\n                        data.append(\n                            b''.join(e[0] for e in self._load_data(group))\n                        )\n                    else:\n                        for addr, size in zip(\n                            group.signal_data[index], group.signal_data_size[index]\n                        ):\n                            if not size:\n                                continue\n                            stream.seek(addr)\n                            data.append(stream.read(size))\n                data = b\"\".join(data)\n        else:\n            data = b\"\"\n\n        return data", "response": "This method loads the signal data from the specified address."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_data(self, group, record_offset=0, record_count=None):\n        offset = 0\n        has_yielded = False\n        _count = record_count\n        data_group = group.data_group\n        channel_group = group.channel_group\n\n        if group.data_location == v4c.LOCATION_ORIGINAL_FILE:\n            stream = self._file\n        else:\n            stream = self._tempfile\n\n        read = stream.read\n        seek = stream.seek\n\n        samples_size = (\n            channel_group.samples_byte_nr + channel_group.invalidation_bytes_nr\n        )\n\n        record_offset *= samples_size\n\n        finished = False\n        if record_count is not None:\n            record_count *= samples_size\n\n        if not samples_size:\n            yield b\"\", offset, _count\n        else:\n\n            if self._read_fragment_size:\n                split_size = self._read_fragment_size // samples_size\n                split_size *= samples_size\n            else:\n                channels_nr = len(group.channels)\n\n                y_axis = CONVERT\n\n                idx = searchsorted(CHANNEL_COUNT, channels_nr, side=\"right\") - 1\n                if idx < 0:\n                    idx = 0\n                split_size = y_axis[idx]\n\n                split_size = split_size // samples_size\n                split_size *= samples_size\n\n            if split_size == 0:\n                split_size = samples_size\n\n            if not group.sorted:\n                cg_size = group.record_size\n                record_id = channel_group.record_id\n                record_id_nr = data_group.record_id_len\n\n                if record_id_nr == 1:\n                    _unpack_stuct = UINT8_u\n                elif record_id_nr == 2:\n                    _unpack_stuct = UINT16_u\n                elif record_id_nr == 4:\n                    _unpack_stuct = UINT32_u\n                elif record_id_nr == 8:\n                    _unpack_stuct = UINT64_u\n                else:\n                    message = f\"invalid record id size {record_id_nr}\"\n                    raise MdfException(message)\n\n            blocks = iter(group.data_blocks)\n\n            if group.data_blocks:\n\n                cur_size = 0\n                data = []\n\n                while True:\n                    try:\n                        info = next(blocks)\n                        address, size, block_size, block_type, param = (\n                            info.address,\n                            info.raw_size,\n                            info.size,\n                            info.block_type,\n                            info.param,\n                        )\n                        current_address = address\n                    except StopIteration:\n                        break\n\n                    if group.sorted:\n                        if offset + size < record_offset + 1:\n                            offset += size\n                            continue\n\n                    seek(address)\n                    new_data = read(block_size)\n                    if block_type == v4c.DZ_BLOCK_DEFLATE:\n                        new_data = decompress(new_data, 0, size)\n                    elif block_type == v4c.DZ_BLOCK_TRANSPOSED:\n                        new_data = decompress(new_data, 0, size)\n                        cols = param\n                        lines = size // cols\n\n                        nd = fromstring(new_data[: lines * cols], dtype=uint8)\n                        nd = nd.reshape((cols, lines))\n                        new_data = nd.T.tostring() + new_data[lines * cols :]\n\n                    if not group.sorted:\n                        rec_data = []\n\n                        i = 0\n                        size = len(new_data)\n                        while i < size:\n                            rec_id, = _unpack_stuct(new_data[i : i + record_id_nr])\n                            # skip record id\n                            i += record_id_nr\n                            rec_size = cg_size[rec_id]\n                            if rec_size:\n                                endpoint = i + rec_size\n                                if rec_id == record_id:\n                                    rec_data.append(new_data[i : endpoint])\n                                i = endpoint\n                            else:\n                                rec_size, = UINT32_u(new_data[i : i + 4])\n                                endpoint = i + rec_size + 4\n                                if rec_id == record_id:\n                                    rec_data.append(new_data[i : endpoint])\n                                i = endpoint\n                        new_data = b\"\".join(rec_data)\n\n                        size = len(new_data)\n\n                    if offset < record_offset:\n                        delta = record_offset - offset\n                        new_data = new_data[delta:]\n                        size -= delta\n                        offset = record_offset\n\n                    while size >= split_size - cur_size:\n                        if data:\n                            data.append(new_data[:split_size - cur_size])\n                            new_data = new_data[split_size - cur_size:]\n                            data_ = b\"\".join(data)\n                            if record_count is not None:\n                                yield data_[:record_count], offset, _count\n                                has_yielded = True\n                                record_count -= len(data_)\n                                if record_count <= 0:\n                                    finished = True\n                                    break\n                            else:\n                                yield data_, offset, _count\n                                has_yielded = True\n                            current_address += split_size - cur_size\n                        else:\n                            data_, new_data = new_data[:split_size], new_data[split_size:]\n                            if record_count is not None:\n                                yield data_[:record_count], offset, _count\n                                has_yielded = True\n                                record_count -= len(data_)\n                                if record_count <= 0:\n                                    finished = True\n                                    break\n                            else:\n                                yield data_, offset, _count\n                                has_yielded = True\n                            current_address += split_size\n                        offset += split_size\n\n                        size -= split_size - cur_size\n                        data = []\n                        cur_size = 0\n\n                    if finished:\n                        data = []\n                        break\n\n                    if size:\n                        data.append(new_data)\n                        cur_size += size\n                if data:\n                    data_ = b\"\".join(data)\n                    if record_count is not None:\n                        yield data_[:record_count], offset, _count\n                        has_yielded = True\n                        record_count -= len(data_)\n                    else:\n                        yield data_, offset, _count\n                        has_yielded = True\n\n                if not has_yielded:\n                    yield b\"\", 0, _count\n            else:\n                yield b\"\", offset, _count", "response": "load the data block of the specified group into memory"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the record dtype and parents dict fro this group dict", "response": "def _prepare_record(self, group):\n        \"\"\" compute record dtype and parents dict fro this group\n\n        Parameters\n        ----------\n        group : dict\n            MDF group dict\n\n        Returns\n        -------\n        parents, dtypes : dict, numpy.dtype\n            mapping of channels to records fields, records fields dtype\n\n        \"\"\"\n\n        parents, dtypes = group.parents, group.types\n        no_parent = None, None\n        if parents is None:\n            channel_group = group.channel_group\n            channels = group.channels\n\n            bus_event = channel_group.flags & v4c.FLAG_CG_BUS_EVENT\n\n            record_size = channel_group.samples_byte_nr\n            invalidation_bytes_nr = channel_group.invalidation_bytes_nr\n            next_byte_aligned_position = 0\n            types = []\n            current_parent = \"\"\n            parent_start_offset = 0\n            parents = {}\n            group_channels = UniqueDB()\n\n            neg_index = -1\n\n            sortedchannels = sorted(enumerate(channels), key=lambda i: i[1])\n            for original_index, new_ch in sortedchannels:\n\n                start_offset = new_ch.byte_offset\n                bit_offset = new_ch.bit_offset\n                data_type = new_ch.data_type\n                bit_count = new_ch.bit_count\n                ch_type = new_ch.channel_type\n                dependency_list = group.channel_dependencies[original_index]\n                name = new_ch.name\n\n                # handle multiple occurance of same channel name\n                name = group_channels.get_unique_name(name)\n\n                if start_offset >= next_byte_aligned_position:\n                    if ch_type not in v4c.VIRTUAL_TYPES:\n                        if not dependency_list:\n                            parent_start_offset = start_offset\n\n                            # check if there are byte gaps in the record\n                            gap = parent_start_offset - next_byte_aligned_position\n                            if gap:\n                                types.append((\"\", f\"V{gap}\"))\n\n                            # adjust size to 1, 2, 4 or 8 bytes\n                            size = bit_offset + bit_count\n                            if data_type not in v4c.NON_SCALAR_TYPES:\n                                if size > 32:\n                                    size = 8\n                                elif size > 16:\n                                    size = 4\n                                elif size > 8:\n                                    size = 2\n                                else:\n                                    size = 1\n                            else:\n                                size = size // 8\n\n                            next_byte_aligned_position = parent_start_offset + size\n                            bit_count = size * 8\n                            if next_byte_aligned_position <= record_size:\n                                if not new_ch.dtype_fmt:\n                                    new_ch.dtype_fmt = get_fmt_v4(\n                                        data_type, bit_count, ch_type\n                                    )\n                                dtype_pair = (name, new_ch.dtype_fmt)\n                                types.append(dtype_pair)\n                                parents[original_index] = name, bit_offset\n                            else:\n                                next_byte_aligned_position = parent_start_offset\n\n                            current_parent = name\n                        else:\n                            if isinstance(dependency_list[0], ChannelArrayBlock):\n                                ca_block = dependency_list[0]\n\n                                # check if there are byte gaps in the record\n                                gap = start_offset - next_byte_aligned_position\n                                if gap:\n                                    dtype_pair = \"\", f\"V{gap}\"\n                                    types.append(dtype_pair)\n\n                                size = max(bit_count // 8, 1)\n                                shape = tuple(\n                                    ca_block[f\"dim_size_{i}\"]\n                                    for i in range(ca_block.dims)\n                                )\n\n                                if (\n                                    ca_block.byte_offset_base // size > 1\n                                    and len(shape) == 1\n                                ):\n                                    shape += (ca_block.byte_offset_base // size,)\n                                dim = 1\n                                for d in shape:\n                                    dim *= d\n\n                                if not new_ch.dtype_fmt:\n                                    new_ch.dtype_fmt = get_fmt_v4(data_type, bit_count)\n                                dtype_pair = (name, new_ch.dtype_fmt, shape)\n                                types.append(dtype_pair)\n\n                                current_parent = name\n                                next_byte_aligned_position = start_offset + size * dim\n                                parents[original_index] = name, 0\n\n                            else:\n                                parents[original_index] = no_parent\n                                if bus_event:\n                                    for logging_channel in group.logging_channels:\n                                        parents[neg_index] = (\n                                            \"CAN_DataFrame.DataBytes\",\n                                            logging_channel.bit_offset,\n                                        )\n                                        neg_index -= 1\n\n                    # virtual channels do not have bytes in the record\n                    else:\n                        parents[original_index] = no_parent\n\n                else:\n                    max_overlapping_size = (next_byte_aligned_position - start_offset) * 8\n                    needed_size = bit_offset + bit_count\n                    if max_overlapping_size >= needed_size:\n                        parents[original_index] = (\n                            current_parent,\n                            ((start_offset - parent_start_offset) * 8) + bit_offset,\n                        )\n                if next_byte_aligned_position > record_size:\n                    break\n\n            gap = record_size - next_byte_aligned_position\n            if gap > 0:\n                dtype_pair = \"\", f\"V{gap}\"\n                types.append(dtype_pair)\n\n            dtype_pair = \"invalidation_bytes\", \"<u1\", (invalidation_bytes_nr,)\n            types.append(dtype_pair)\n\n            dtypes = dtype(types)\n\n            group.parents, group.types = parents, dtypes\n\n        return parents, dtypes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_channel_selection(\n        self, name=None, group=None, index=None, source=None\n    ):\n        \"\"\"Gets channel comment.\n        Channel can be specified in two ways:\n\n        * using the first positional argument *name*\n\n            * if there are multiple occurrences for this channel then the\n            *group* and *index* arguments can be used to select a specific\n            group.\n            * if there are multiple occurrences for this channel and either the\n            *group* or *index* arguments is None then a warning is issued\n\n        * using the group number (keyword argument *group*) and the channel\n        number (keyword argument *index*). Use *info* method for group and\n        channel numbers\n\n\n        If the *raster* keyword argument is not *None* the output is\n        interpolated accordingly.\n\n        Parameters\n        ----------\n        name : string\n            name of channel\n        group : int\n            0-based group index\n        index : int\n            0-based channel index\n\n        Returns\n        -------\n        group_index, channel_index : (int, int)\n            selected channel's group and channel index\n\n        \"\"\"\n        suppress = True\n        if name is None:\n            if group is None or index is None:\n                message = (\n                    \"Invalid arguments for channel selection: \"\n                    'must give \"name\" or, \"group\" and \"index\"'\n                )\n                raise MdfException(message)\n            else:\n                gp_nr, ch_nr = group, index\n                if ch_nr >= 0:\n                    try:\n                        grp = self.groups[gp_nr]\n                    except IndexError:\n                        raise MdfException(\"Group index out of range\")\n\n                    try:\n                        grp.channels[ch_nr]\n                    except IndexError:\n                        raise MdfException(\"Channel index out of range\")\n        else:\n            if name not in self.channels_db:\n                raise MdfException(f'Channel \"{name}\" not found')\n            else:\n                if source is not None:\n                    for gp_nr, ch_nr in self.channels_db[name]:\n                        source_name = self._get_source_name(gp_nr, ch_nr)\n                        if source_name == source:\n                            break\n                    else:\n                        raise MdfException(f\"{name} with source {source} not found\")\n                elif group is None:\n\n                    gp_nr, ch_nr = self.channels_db[name][0]\n                    if len(self.channels_db[name]) > 1 and not suppress:\n                        message = (\n                            f'Multiple occurances for channel \"{name}\". '\n                            f\"Using first occurance from data group {gp_nr}. \"\n                            'Provide both \"group\" and \"index\" arguments'\n                            \" to select another data group\"\n                        )\n                        logger.warning(message)\n\n                else:\n                    if index is not None and index < 0:\n                        gp_nr = group\n                        ch_nr = index\n                    else:\n                        for gp_nr, ch_nr in self.channels_db[name]:\n                            if gp_nr == group:\n                                if index is None:\n                                    break\n                                elif index == ch_nr:\n                                    break\n                        else:\n                            if index is None:\n                                message = f'Channel \"{name}\" not found in group {group}'\n                            else:\n                                message = f'Channel \"{name}\" not found in group {group} at index {index}'\n                            raise MdfException(message)\n\n        return gp_nr, ch_nr", "response": "Validate the selection of a specific channel in a specific group and index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_invalidation_bits(self, group_index, channel, fragment):\n        group = self.groups[group_index]\n        dtypes = group.types\n\n        data_bytes, offset, _count = fragment\n        try:\n            invalidation = self._invalidation_cache[(group_index, offset, _count)]\n        except KeyError:\n            record = group.record\n            if record is None:\n                dtypes = group.types\n                if dtypes.itemsize:\n                    record = fromstring(data_bytes, dtype=dtypes)\n                else:\n                    record = None\n\n            invalidation = record[\"invalidation_bytes\"].copy()\n            self._invalidation_cache[(group_index, offset, _count)] = invalidation\n\n        ch_invalidation_pos = channel.pos_invalidation_bit\n        pos_byte, pos_offset = divmod(ch_invalidation_pos, 8)\n\n        mask = 1 << pos_offset\n\n        invalidation_bits = invalidation[:, pos_byte] & mask\n        invalidation_bits = invalidation_bits.astype(bool)\n\n        return invalidation_bits", "response": "get the invalidation bits for the channel"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure(\n        self,\n        *,\n        read_fragment_size=None,\n        write_fragment_size=None,\n        use_display_names=None,\n        single_bit_uint_as_bool=None,\n        integer_interpolation=None,\n    ):\n        \"\"\" configure MDF parameters\n\n        Parameters\n        ----------\n        read_fragment_size : int\n            size hint of split data blocks, default 8MB; if the initial size is\n            smaller, then no data list is used. The actual split size depends on\n            the data groups' records size\n        write_fragment_size : int\n            size hint of split data blocks, default 4MB; if the initial size is\n            smaller, then no data list is used. The actual split size depends on\n            the data groups' records size. Maximum size is 4MB to ensure\n            compatibility with CANape\n        use_display_names : bool\n            search for display name in the Channel XML comment\n        single_bit_uint_as_bool : bool\n            return single bit channels are np.bool arrays\n        integer_interpolation : int\n            interpolation mode for integer channels:\n\n                * 0 - repeat previous sample\n                * 1 - use linear interpolation\n\n        \"\"\"\n\n        if read_fragment_size is not None:\n            self._read_fragment_size = int(read_fragment_size)\n\n        if write_fragment_size:\n            self._write_fragment_size = min(int(write_fragment_size), 4 * 2 ** 20)\n\n        if use_display_names is not None:\n            self._use_display_names = bool(use_display_names)\n\n        if single_bit_uint_as_bool is not None:\n            self._single_bit_uint_as_bool = bool(single_bit_uint_as_bool)\n\n        if integer_interpolation in (0, 1):\n            self._integer_interpolation = int(integer_interpolation)", "response": "This method is used to configure the internal parameters of the CANA3GHZ class attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef append(self, signals, source_info=\"Python\", common_timebase=False, units=None):\n        if isinstance(signals, Signal):\n            signals = [signals]\n        elif isinstance(signals, DataFrame):\n            self._append_dataframe(signals, source_info, units=units)\n            return\n\n        interp_mode = self._integer_interpolation\n\n        # check if the signals have a common timebase\n        # if not interpolate the signals using the union of all timbases\n        if signals:\n            t_ = signals[0].timestamps\n            if not common_timebase:\n                for s in signals[1:]:\n                    if not array_equal(s.timestamps, t_):\n                        different = True\n                        break\n                else:\n                    different = False\n\n                if different:\n                    times = [s.timestamps for s in signals]\n                    t = reduce(union1d, times).flatten().astype(float64)\n                    signals = [s.interp(t, interpolation_mode=interp_mode) for s in signals]\n                    times = None\n                else:\n                    t = t_\n            else:\n                t = t_\n        else:\n            t = []\n\n        dg_cntr = len(self.groups)\n\n        gp = Group(None)\n        gp.signal_data = gp_sdata = []\n        gp.signal_data_size = gp_sdata_size = []\n        gp.channels = gp_channels = []\n        gp.channel_dependencies = gp_dep = []\n        gp.signal_types = gp_sig_types = []\n        gp.logging_channels = []\n\n        # channel group\n        kwargs = {\"cycles_nr\": 0, \"samples_byte_nr\": 0}\n        gp.channel_group = ChannelGroup(**kwargs)\n        gp.channel_group.name = source_info\n\n        if any(sig.invalidation_bits is not None for sig in signals):\n            invalidation_bytes_nr = 1\n            gp.channel_group.invalidation_bytes_nr = invalidation_bytes_nr\n\n            inval_bits = []\n\n        else:\n            invalidation_bytes_nr = 0\n            inval_bits = []\n        inval_cntr = 0\n\n        self.groups.append(gp)\n\n        cycles_nr = len(t)\n        fields = []\n        types = []\n        parents = {}\n        ch_cntr = 0\n        offset = 0\n        field_names = UniqueDB()\n\n        defined_texts = {}\n        si_map = self._si_map\n        cc_map = self._cc_map\n\n        # setup all blocks related to the time master channel\n\n        file = self._tempfile\n        tell = file.tell\n        seek = file.seek\n\n        seek(0, 2)\n\n        if signals:\n            master_metadata = signals[0].master_metadata\n        else:\n            master_metadata = None\n        if master_metadata:\n            time_name, sync_type = master_metadata\n            if sync_type in (0, 1):\n                time_unit = \"s\"\n            elif sync_type == 2:\n                time_unit = \"deg\"\n            elif sync_type == 3:\n                time_unit = \"m\"\n            elif sync_type == 4:\n                time_unit = \"index\"\n        else:\n            time_name, sync_type = \"time\", v4c.SYNC_TYPE_TIME\n            time_unit = \"s\"\n\n        source_block = SourceInformation()\n        source_block.name = source_block.path = source_info\n\n        gp.channel_group.acq_source = source_block\n\n        if signals:\n            # time channel\n            t_type, t_size = fmt_to_datatype_v4(t.dtype, t.shape)\n            kwargs = {\n                \"channel_type\": v4c.CHANNEL_TYPE_MASTER,\n                \"data_type\": t_type,\n                \"sync_type\": sync_type,\n                \"byte_offset\": 0,\n                \"bit_offset\": 0,\n                \"bit_count\": t_size,\n            }\n\n            ch = Channel(**kwargs)\n            ch.unit = time_unit\n            ch.name = time_name\n            ch.source = source_block\n            name = time_name\n\n            gp_channels.append(ch)\n\n            gp_sdata.append(None)\n            gp_sdata_size.append(0)\n            self.channels_db.add(name, (dg_cntr, ch_cntr))\n            self.masters_db[dg_cntr] = 0\n            # data group record parents\n            parents[ch_cntr] = name, 0\n\n            # time channel doesn't have channel dependencies\n            gp_dep.append(None)\n\n            fields.append(t)\n            types.append((name, t.dtype))\n            field_names.get_unique_name(name)\n\n            offset += t_size // 8\n            ch_cntr += 1\n\n            gp_sig_types.append(0)\n\n        for signal in signals:\n            sig = signal\n            samples = sig.samples\n            sig_dtype = samples.dtype\n            sig_shape = samples.shape\n            names = sig_dtype.names\n            name = signal.name\n\n            if names is None:\n                sig_type = v4c.SIGNAL_TYPE_SCALAR\n                if sig_dtype.kind in \"SV\":\n                    sig_type = v4c.SIGNAL_TYPE_STRING\n            else:\n                if names in (v4c.CANOPEN_TIME_FIELDS, v4c.CANOPEN_DATE_FIELDS):\n                    sig_type = v4c.SIGNAL_TYPE_CANOPEN\n                elif names[0] != sig.name:\n                    sig_type = v4c.SIGNAL_TYPE_STRUCTURE_COMPOSITION\n                else:\n                    sig_type = v4c.SIGNAL_TYPE_ARRAY\n\n            gp_sig_types.append(sig_type)\n\n            # first add the signals in the simple signal list\n            if sig_type == v4c.SIGNAL_TYPE_SCALAR:\n\n                # compute additional byte offset for large records size\n                s_type, s_size = fmt_to_datatype_v4(sig_dtype, sig_shape)\n\n                byte_size = max(s_size // 8, 1)\n\n                if sig_dtype.kind == \"u\" and signal.bit_count <= 4:\n                    s_size = signal.bit_count\n\n                if signal.stream_sync:\n                    channel_type = v4c.CHANNEL_TYPE_SYNC\n                    at_data, at_name = signal.attachment\n                    attachment_addr = self.attach(\n                        at_data, at_name, mime=\"video/avi\", embedded=False\n                    )\n                    data_block_addr = attachment_addr\n                    sync_type = v4c.SYNC_TYPE_TIME\n                else:\n                    channel_type = v4c.CHANNEL_TYPE_VALUE\n                    data_block_addr = 0\n                    sync_type = v4c.SYNC_TYPE_NONE\n\n                kwargs = {\n                    \"channel_type\": channel_type,\n                    \"sync_type\": sync_type,\n                    \"bit_count\": s_size,\n                    \"byte_offset\": offset,\n                    \"bit_offset\": 0,\n                    \"data_type\": s_type,\n                    \"data_block_addr\": data_block_addr,\n                    \"flags\": 0,\n                }\n\n                if invalidation_bytes_nr and signal.invalidation_bits is not None:\n                    inval_bits.append(signal.invalidation_bits)\n                    kwargs[\"flags\"] = v4c.FLAG_CN_INVALIDATION_PRESENT\n                    kwargs[\"pos_invalidation_bit\"] = inval_cntr\n                    inval_cntr += 1\n\n                ch = Channel(**kwargs)\n                ch.name = name\n                ch.unit = signal.unit\n                ch.comment = signal.comment\n                ch.display_name = signal.display_name\n\n                # conversions for channel\n                if signal.raw:\n                    ch.conversion = conversion_transfer(signal.conversion, version=4)\n\n                # source for channel\n                source = signal.source\n                if source:\n                    if source in si_map:\n                        ch.source = si_map[source]\n                    else:\n                        new_source = SourceInformation(\n                            source_type=source.source_type, bus_type=source.bus_type\n                        )\n                        new_source.name = source.name\n                        new_source.path = source.path\n                        new_source.comment = source.comment\n\n                        si_map[source] = new_source\n\n                        ch.source = new_source\n\n                gp_channels.append(ch)\n\n                offset += byte_size\n\n                gp_sdata.append(None)\n                gp_sdata_size.append(0)\n                entry = (dg_cntr, ch_cntr)\n                self.channels_db.add(name, entry)\n                if ch.display_name:\n                    self.channels_db.add(ch.display_name, entry)\n\n                # update the parents as well\n                field_name = field_names.get_unique_name(name)\n                parents[ch_cntr] = field_name, 0\n\n                fields.append(samples)\n                types.append((field_name, sig_dtype, sig_shape[1:]))\n\n                ch_cntr += 1\n\n                # simple channels don't have channel dependencies\n                gp_dep.append(None)\n\n            elif sig_type == v4c.SIGNAL_TYPE_CANOPEN:\n\n                field_name = field_names.get_unique_name(name)\n\n                if names == v4c.CANOPEN_TIME_FIELDS:\n\n                    vals = signal.samples.tostring()\n\n                    fields.append(frombuffer(vals, dtype=\"V6\"))\n                    types.append((field_name, \"V6\"))\n                    byte_size = 6\n                    s_type = v4c.DATA_TYPE_CANOPEN_TIME\n\n                else:\n                    vals = []\n                    for field in (\"ms\", \"min\", \"hour\", \"day\", \"month\", \"year\"):\n                        if field == \"hour\":\n                            vals.append(\n                                signal.samples[field]\n                                + (signal.samples[\"summer_time\"] << 7)\n                            )\n                        elif field == \"day\":\n                            vals.append(\n                                signal.samples[field]\n                                + (signal.samples[\"day_of_week\"] << 4)\n                            )\n                        else:\n                            vals.append(signal.samples[field])\n                    vals = fromarrays(vals).tostring()\n\n                    fields.append(frombuffer(vals, dtype=\"V7\"))\n                    types.append((field_name, \"V7\"))\n                    byte_size = 7\n                    s_type = v4c.DATA_TYPE_CANOPEN_DATE\n\n                s_size = byte_size << 3\n\n                # there is no channel dependency\n                gp_dep.append(None)\n\n                # add channel block\n                kwargs = {\n                    \"channel_type\": v4c.CHANNEL_TYPE_VALUE,\n                    \"bit_count\": s_size,\n                    \"byte_offset\": offset,\n                    \"bit_offset\": 0,\n                    \"data_type\": s_type,\n                    \"flags\": 0,\n                }\n                if invalidation_bytes_nr and signal.invalidation_bits is not None:\n                    inval_bits.append(signal.invalidation_bits)\n                    kwargs[\"flags\"] |= v4c.FLAG_CN_INVALIDATION_PRESENT\n                    kwargs[\"pos_invalidation_bit\"] = inval_cntr\n                    inval_cntr += 1\n\n                ch = Channel(**kwargs)\n                ch.name = name\n                ch.unit = signal.unit\n                ch.comment = signal.comment\n                ch.display_name = signal.display_name\n\n                # source for channel\n                source = signal.source\n                if source:\n                    if source in si_map:\n                        ch.source = si_map[source]\n                    else:\n                        new_source = SourceInformation(\n                            source_type=source.source_type, bus_type=source.bus_type\n                        )\n                        new_source.name = source.name\n                        new_source.path = source.path\n                        new_source.comment = source.comment\n\n                        si_map[source] = new_source\n\n                        ch.source = new_source\n\n                gp_channels.append(ch)\n\n                offset += byte_size\n\n                entry = (dg_cntr, ch_cntr)\n                self.channels_db.add(name, entry)\n                if ch.display_name:\n                    self.channels_db.add(ch.display_name, entry)\n\n                # update the parents as well\n                parents[ch_cntr] = field_name, 0\n\n                gp_sdata.append(0)\n                gp_sdata_size.append(0)\n\n                ch_cntr += 1\n\n            elif sig_type == v4c.SIGNAL_TYPE_STRUCTURE_COMPOSITION:\n                offset, dg_cntr, ch_cntr, struct_self, new_fields, new_types, inval_cntr = self._append_structure_composition(\n                    gp,\n                    signal,\n                    field_names,\n                    offset,\n                    dg_cntr,\n                    ch_cntr,\n                    parents,\n                    defined_texts,\n                    invalidation_bytes_nr,\n                    inval_bits,\n                    inval_cntr,\n                )\n                fields.extend(new_fields)\n                types.extend(new_types)\n\n            elif sig_type == v4c.SIGNAL_TYPE_ARRAY:\n                # here we have channel arrays or mdf v3 channel dependencies\n                samples = signal.samples[names[0]]\n                shape = samples.shape[1:]\n\n                if len(shape) > 1:\n                    # add channel dependency block for composed parent channel\n                    dims_nr = len(shape)\n                    names_nr = len(names)\n\n                    if names_nr == 0:\n                        kwargs = {\n                            \"dims\": dims_nr,\n                            \"ca_type\": v4c.CA_TYPE_LOOKUP,\n                            \"flags\": v4c.FLAG_CA_FIXED_AXIS,\n                            \"byte_offset_base\": samples.dtype.itemsize,\n                        }\n                        for i in range(dims_nr):\n                            kwargs[f\"dim_size_{i}\"] = shape[i]\n\n                    elif len(names) == 1:\n                        kwargs = {\n                            \"dims\": dims_nr,\n                            \"ca_type\": v4c.CA_TYPE_ARRAY,\n                            \"flags\": 0,\n                            \"byte_offset_base\": samples.dtype.itemsize,\n                        }\n                        for i in range(dims_nr):\n                            kwargs[f\"dim_size_{i}\"] = shape[i]\n\n                    else:\n                        kwargs = {\n                            \"dims\": dims_nr,\n                            \"ca_type\": v4c.CA_TYPE_LOOKUP,\n                            \"flags\": v4c.FLAG_CA_AXIS,\n                            \"byte_offset_base\": samples.dtype.itemsize,\n                        }\n                        for i in range(dims_nr):\n                            kwargs[f\"dim_size_{i}\"] = shape[i]\n\n                    parent_dep = ChannelArrayBlock(**kwargs)\n                    gp_dep.append([parent_dep])\n\n                else:\n                    # add channel dependency block for composed parent channel\n                    kwargs = {\n                        \"dims\": 1,\n                        \"ca_type\": v4c.CA_TYPE_SCALE_AXIS,\n                        \"flags\": 0,\n                        \"byte_offset_base\": samples.dtype.itemsize,\n                        \"dim_size_0\": shape[0],\n                    }\n                    parent_dep = ChannelArrayBlock(**kwargs)\n                    gp_dep.append([parent_dep])\n\n                field_name = field_names.get_unique_name(name)\n\n                fields.append(samples)\n                dtype_pair = field_name, samples.dtype, shape\n                types.append(dtype_pair)\n\n                # first we add the structure channel\n                s_type, s_size = fmt_to_datatype_v4(samples.dtype, samples.shape, True)\n\n                # add channel block\n                kwargs = {\n                    \"channel_type\": v4c.CHANNEL_TYPE_VALUE,\n                    \"bit_count\": s_size,\n                    \"byte_offset\": offset,\n                    \"bit_offset\": 0,\n                    \"data_type\": s_type,\n                    \"flags\": 0,\n                }\n\n                if invalidation_bytes_nr:\n                    if signal.invalidation_bits is not None:\n                        inval_bits.append(signal.invalidation_bits)\n                        kwargs[\"flags\"] |= v4c.FLAG_CN_INVALIDATION_PRESENT\n                        kwargs[\"pos_invalidation_bit\"] = inval_cntr\n                        inval_cntr += 1\n\n                ch = Channel(**kwargs)\n                ch.name = name\n                ch.unit = signal.unit\n                ch.comment = signal.comment\n                ch.display_name = signal.display_name\n\n                # source for channel\n                source = signal.source\n                if source:\n                    if source in si_map:\n                        ch.source = si_map[source]\n                    else:\n                        new_source = SourceInformation(\n                            source_type=source.source_type, bus_type=source.bus_type\n                        )\n                        new_source.name = source.name\n                        new_source.path = source.path\n                        new_source.comment = source.comment\n\n                        si_map[source] = new_source\n\n                        ch.source = new_source\n\n                gp_channels.append(ch)\n\n                size = s_size // 8\n                for dim in shape:\n                    size *= dim\n                offset += size\n\n                gp_sdata.append(None)\n                gp_sdata_size.append(0)\n                entry = (dg_cntr, ch_cntr)\n                self.channels_db.add(name, entry)\n                if ch.display_name:\n                    self.channels_db.add(ch.display_name, entry)\n\n                # update the parents as well\n                parents[ch_cntr] = name, 0\n\n                ch_cntr += 1\n\n                for name in names[1:]:\n                    field_name = field_names.get_unique_name(name)\n\n                    samples = signal.samples[name]\n                    shape = samples.shape[1:]\n                    fields.append(samples)\n                    types.append((field_name, samples.dtype, shape))\n\n                    # add channel dependency block\n                    kwargs = {\n                        \"dims\": 1,\n                        \"ca_type\": v4c.CA_TYPE_SCALE_AXIS,\n                        \"flags\": 0,\n                        \"byte_offset_base\": samples.dtype.itemsize,\n                        \"dim_size_0\": shape[0],\n                    }\n                    dep = ChannelArrayBlock(**kwargs)\n                    gp_dep.append([dep])\n\n                    # add components channel\n                    s_type, s_size = fmt_to_datatype_v4(samples.dtype, ())\n                    byte_size = max(s_size // 8, 1)\n                    kwargs = {\n                        \"channel_type\": v4c.CHANNEL_TYPE_VALUE,\n                        \"bit_count\": s_size,\n                        \"byte_offset\": offset,\n                        \"bit_offset\": 0,\n                        \"data_type\": s_type,\n                        \"flags\": 0,\n                    }\n\n                    if invalidation_bytes_nr:\n                        if signal.invalidation_bits is not None:\n                            inval_bits.append(signal.invalidation_bits)\n                            kwargs[\"flags\"] |= v4c.FLAG_CN_INVALIDATION_PRESENT\n                            kwargs[\"pos_invalidation_bit\"] = inval_cntr\n                            inval_cntr += 1\n\n                    ch = Channel(**kwargs)\n                    ch.name = name\n                    ch.unit = signal.unit\n                    ch.comment = signal.comment\n                    ch.display_name = signal.display_name\n\n                    gp_channels.append(ch)\n\n                    entry = dg_cntr, ch_cntr\n                    parent_dep.referenced_channels.append(entry)\n                    for dim in shape:\n                        byte_size *= dim\n                    offset += byte_size\n\n                    gp_sdata.append(None)\n                    gp_sdata_size.append(0)\n                    self.channels_db.add(name, entry)\n\n                    # update the parents as well\n                    parents[ch_cntr] = field_name, 0\n\n                    ch_cntr += 1\n\n            else:\n\n                encoding = signal.encoding\n                samples = signal.samples\n                sig_dtype = samples.dtype\n\n                if encoding == \"utf-8\":\n                    data_type = v4c.DATA_TYPE_STRING_UTF_8\n                elif encoding == \"latin-1\":\n                    data_type = v4c.DATA_TYPE_STRING_LATIN_1\n                elif encoding == \"utf-16-be\":\n                    data_type = v4c.DATA_TYPE_STRING_UTF_16_BE\n                elif encoding == \"utf-16-le\":\n                    data_type = v4c.DATA_TYPE_STRING_UTF_16_LE\n                else:\n                    raise MdfException(f'wrong encoding \"{encoding}\" for string signal')\n\n                offsets = arange(len(samples), dtype=uint64) * (\n                    signal.samples.itemsize + 4\n                )\n\n                values = [full(len(samples), samples.itemsize, dtype=uint32), samples]\n\n                types_ = [(\"o\", uint32), (\"s\", sig_dtype)]\n\n                data = fromarrays(values, dtype=types_)\n\n                data_size = len(data) * data.itemsize\n                if data_size:\n                    data_addr = tell()\n                    gp_sdata.append([data_addr])\n                    gp_sdata_size.append([data_size])\n                    data.tofile(file)\n                else:\n                    data_addr = 0\n                    gp_sdata.append([])\n                    gp_sdata_size.append([])\n\n                # compute additional byte offset for large records size\n                byte_size = 8\n                kwargs = {\n                    \"channel_type\": v4c.CHANNEL_TYPE_VLSD,\n                    \"bit_count\": 64,\n                    \"byte_offset\": offset,\n                    \"bit_offset\": 0,\n                    \"data_type\": data_type,\n                    \"data_block_addr\": data_addr,\n                    \"flags\": 0,\n                }\n\n                if invalidation_bytes_nr:\n                    if signal.invalidation_bits is not None:\n                        inval_bits.append(signal.invalidation_bits)\n                        kwargs[\"flags\"] |= v4c.FLAG_CN_INVALIDATION_PRESENT\n                        kwargs[\"pos_invalidation_bit\"] = inval_cntr\n                        inval_cntr += 1\n\n                ch = Channel(**kwargs)\n                ch.name = name\n                ch.unit = signal.unit\n                ch.comment = signal.comment\n                ch.display_name = signal.display_name\n\n                # conversions for channel\n                conversion = conversion_transfer(signal.conversion, version=4)\n                if signal.raw:\n                    ch.conversion = conversion\n\n                # source for channel\n                source = signal.source\n                if source:\n                    if source in si_map:\n                        ch.source = si_map[source]\n                    else:\n                        new_source = SourceInformation(\n                            source_type=source.source_type, bus_type=source.bus_type\n                        )\n                        new_source.name = source.name\n                        new_source.path = source.path\n                        new_source.comment = source.comment\n\n                        si_map[source] = new_source\n\n                        ch.source = new_source\n\n                gp_channels.append(ch)\n\n                offset += byte_size\n\n                entry = (dg_cntr, ch_cntr)\n                self.channels_db.add(name, entry)\n                if ch.display_name:\n                    self.channels_db.add(ch.display_name, entry)\n\n                # update the parents as well\n                field_name = field_names.get_unique_name(name)\n                parents[ch_cntr] = field_name, 0\n\n                fields.append(offsets)\n                types.append((field_name, uint64))\n\n                ch_cntr += 1\n\n                # simple channels don't have channel dependencies\n                gp_dep.append(None)\n\n        if invalidation_bytes_nr:\n            invalidation_bytes_nr = len(inval_bits)\n\n            for _ in range(8 - invalidation_bytes_nr % 8):\n                inval_bits.append(zeros(cycles_nr, dtype=bool))\n\n            inval_bits.reverse()\n\n            invalidation_bytes_nr = len(inval_bits) // 8\n\n            gp.channel_group.invalidation_bytes_nr = invalidation_bytes_nr\n\n            inval_bits = fliplr(\n                packbits(array(inval_bits).T).reshape(\n                    (cycles_nr, invalidation_bytes_nr)\n                )\n            )\n\n            fields.append(inval_bits)\n            types.append((\"invalidation_bytes\", inval_bits.dtype, inval_bits.shape[1:]))\n\n        gp.channel_group.cycles_nr = cycles_nr\n        gp.channel_group.samples_byte_nr = offset\n\n        # data group\n        gp.data_group = DataGroup()\n\n        # data block\n        types = dtype(types)\n\n        gp.sorted = True\n        gp.types = types\n        gp.parents = parents\n\n        if signals:\n            samples = fromarrays(fields, dtype=types)\n        else:\n            samples = array([])\n\n        signals = None\n        del signals\n\n        size = len(samples) * samples.itemsize\n\n        if size:\n            data_address = self._tempfile.tell()\n            gp.data_location = v4c.LOCATION_TEMPORARY_FILE\n            samples.tofile(self._tempfile)\n\n            gp.data_blocks.append(\n                DataBlockInfo(\n                    address=data_address,\n                    block_type=v4c.DT_BLOCK,\n                    raw_size=size,\n                    size=size,\n                    param=0,\n                )\n            )\n        else:\n            gp.data_location = v4c.LOCATION_TEMPORARY_FILE", "response": "Append a new data group to the current data group."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend a new data group from a Pandas data frame.", "response": "def _append_dataframe(self, df, source_info=\"\", units=None):\n        \"\"\"\n        Appends a new data group from a Pandas data frame.\n\n        \"\"\"\n\n        units = units or {}\n\n        t = df.index\n        index_name = df.index.name\n        time_name = index_name or \"time\"\n        sync_type = v4c.SYNC_TYPE_TIME\n        time_unit = \"s\"\n\n        dg_cntr = len(self.groups)\n\n        gp = Group(None)\n        gp.signal_data = gp_sdata = []\n        gp.signal_data_size = gp_sdata_size = []\n        gp.channels = gp_channels = []\n        gp.channel_dependencies = gp_dep = []\n        gp.signal_types = gp_sig_types = []\n        gp.logging_channels = []\n\n        # channel group\n        kwargs = {\"cycles_nr\": 0, \"samples_byte_nr\": 0}\n        gp.channel_group = ChannelGroup(**kwargs)\n        gp.channel_group.acq_name = source_info\n\n        invalidation_bytes_nr = 0\n\n        self.groups.append(gp)\n\n        cycles_nr = len(t)\n        fields = []\n        types = []\n        parents = {}\n        ch_cntr = 0\n        offset = 0\n        field_names = UniqueDB()\n\n        # setup all blocks related to the time master channel\n\n        file = self._tempfile\n        tell = file.tell\n        seek = file.seek\n\n        seek(0, 2)\n\n        source_block = SourceInformation()\n        source_block.name = source_block.path = source_info\n\n        if df.shape[0]:\n            # time channel\n            t_type, t_size = fmt_to_datatype_v4(t.dtype, t.shape)\n            kwargs = {\n                \"channel_type\": v4c.CHANNEL_TYPE_MASTER,\n                \"data_type\": t_type,\n                \"sync_type\": sync_type,\n                \"byte_offset\": 0,\n                \"bit_offset\": 0,\n                \"bit_count\": t_size,\n                \"min_raw_value\": t[0] if cycles_nr else 0,\n                \"max_raw_value\": t[-1] if cycles_nr else 0,\n                \"lower_limit\": t[0] if cycles_nr else 0,\n                \"upper_limit\": t[-1] if cycles_nr else 0,\n                \"flags\": v4c.FLAG_PHY_RANGE_OK | v4c.FLAG_VAL_RANGE_OK,\n            }\n            ch = Channel(**kwargs)\n            ch.unit = time_unit\n            ch.name = time_name\n            ch.source = source_block\n            name = time_name\n            gp_channels.append(ch)\n\n            gp_sdata.append(None)\n            gp_sdata_size.append(0)\n            self.channels_db.add(name, (dg_cntr, ch_cntr))\n            self.masters_db[dg_cntr] = 0\n            # data group record parents\n            parents[ch_cntr] = name, 0\n\n            # time channel doesn't have channel dependencies\n            gp_dep.append(None)\n\n            fields.append(t)\n            types.append((name, t.dtype))\n            field_names.get_unique_name(name)\n\n            offset += t_size // 8\n            ch_cntr += 1\n\n            gp_sig_types.append(0)\n\n        for signal in df:\n            if index_name == signal:\n                continue\n\n            sig = df[signal]\n            name = signal\n\n            sig_type = v4c.SIGNAL_TYPE_SCALAR\n            if sig.dtype.kind in \"SV\":\n                sig_type = v4c.SIGNAL_TYPE_STRING\n\n            gp_sig_types.append(sig_type)\n\n            # first add the signals in the simple signal list\n            if sig_type == v4c.SIGNAL_TYPE_SCALAR:\n\n                # compute additional byte offset for large records size\n                s_type, s_size = fmt_to_datatype_v4(sig.dtype, sig.shape)\n\n                byte_size = max(s_size // 8, 1)\n\n                channel_type = v4c.CHANNEL_TYPE_VALUE\n                data_block_addr = 0\n                sync_type = v4c.SYNC_TYPE_NONE\n\n                kwargs = {\n                    \"channel_type\": channel_type,\n                    \"sync_type\": sync_type,\n                    \"bit_count\": s_size,\n                    \"byte_offset\": offset,\n                    \"bit_offset\": 0,\n                    \"data_type\": s_type,\n                    \"data_block_addr\": data_block_addr,\n                }\n\n                ch = Channel(**kwargs)\n                ch.name = name\n                ch.unit = units.get(name, \"\")\n\n                gp_channels.append(ch)\n\n                offset += byte_size\n\n                gp_sdata.append(None)\n                gp_sdata_size.append(0)\n                self.channels_db.add(name, (dg_cntr, ch_cntr))\n\n                # update the parents as well\n                field_name = field_names.get_unique_name(name)\n                parents[ch_cntr] = field_name, 0\n\n                fields.append(sig)\n                types.append((field_name, sig.dtype, sig.shape[1:]))\n\n                ch_cntr += 1\n\n                # simple channels don't have channel dependencies\n                gp_dep.append(None)\n\n            elif sig_type == v4c.SIGNAL_TYPE_STRING:\n                offsets = arange(len(sig), dtype=uint64) * (sig.itemsize + 4)\n\n                values = [full(len(signal), sig.itemsize, dtype=uint32), sig]\n\n                types_ = [(\"\", uint32), (\"\", sig.dtype)]\n\n                data = fromarrays(values, dtype=types_)\n\n                data_size = len(data) * data.itemsize\n                if data_size:\n                    data_addr = tell()\n                    gp_sdata.append([data_addr])\n                    gp_sdata_size.append([data_size])\n                    data.tofile(file)\n                else:\n                    data_addr = 0\n                    gp_sdata.append([])\n                    gp_sdata_size.append([])\n\n                # compute additional byte offset for large records size\n                byte_size = 8\n                kwargs = {\n                    \"channel_type\": v4c.CHANNEL_TYPE_VLSD,\n                    \"bit_count\": 64,\n                    \"byte_offset\": offset,\n                    \"bit_offset\": 0,\n                    \"data_type\": v4c.DATA_TYPE_STRING_UTF_8,\n                    \"min_raw_value\": 0,\n                    \"max_raw_value\": 0,\n                    \"lower_limit\": 0,\n                    \"upper_limit\": 0,\n                    \"flags\": 0,\n                    \"data_block_addr\": data_addr,\n                }\n\n                ch = Channel(**kwargs)\n                ch.name = name\n                ch.unit = units.get(name, \"\")\n\n                gp_channels.append(ch)\n\n                offset += byte_size\n\n                self.channels_db.add(name, (dg_cntr, ch_cntr))\n\n                # update the parents as well\n                field_name = field_names.field_names.get_unique_name(name)\n                parents[ch_cntr] = field_name, 0\n\n                fields.append(offsets)\n                types.append((field_name, uint64))\n\n                ch_cntr += 1\n\n                # simple channels don't have channel dependencies\n                gp_dep.append(None)\n\n        gp.channel_group.cycles_nr = cycles_nr\n        gp.channel_group.samples_byte_nr = offset\n\n        # data group\n        gp.data_group = DataGroup()\n\n        # data block\n        types = dtype(types)\n\n        gp.sorted = True\n        gp.types = types\n        gp.parents = parents\n\n        if df.shape[0]:\n            samples = fromarrays(fields, dtype=types)\n        else:\n            samples = array([])\n\n        size = len(samples) * samples.itemsize\n        if size:\n            data_address = self._tempfile.tell()\n            gp.data_location = v4c.LOCATION_TEMPORARY_FILE\n\n            samples.tofile(self._tempfile)\n\n            gp.data_blocks.append(\n                DataBlockInfo(\n                    address=data_address,\n                    block_type=v4c.DT_BLOCK,\n                    raw_size=size,\n                    size=size,\n                    param=0,\n                )\n            )\n        else:\n            gp.data_location = v4c.LOCATION_TEMPORARY_FILE"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extend(self, index, signals):\n        gp = self.groups[index]\n        if not signals:\n            message = '\"append\" requires a non-empty list of Signal objects'\n            raise MdfException(message)\n\n        stream = self._tempfile\n\n        fields = []\n        types = []\n        inval_bits = []\n\n        invalidation_bytes_nr = gp.channel_group.invalidation_bytes_nr\n        for i, ((signal, invalidation_bits), sig_type) in enumerate(\n            zip(signals, gp.signal_types)\n        ):\n\n            # first add the signals in the simple signal list\n            if sig_type == v4c.SIGNAL_TYPE_SCALAR:\n\n                fields.append(signal)\n                types.append((\"\", signal.dtype, signal.shape[1:]))\n\n                if invalidation_bytes_nr and invalidation_bits is not None:\n                    inval_bits.append(invalidation_bits)\n\n            elif sig_type == v4c.SIGNAL_TYPE_CANOPEN:\n                names = signal.dtype.names\n\n                if names == v4c.CANOPEN_TIME_FIELDS:\n\n                    vals = signal.tostring()\n\n                    fields.append(frombuffer(vals, dtype=\"V6\"))\n                    types.append((\"\", \"V6\"))\n\n                else:\n                    vals = []\n                    for field in (\"ms\", \"min\", \"hour\", \"day\", \"month\", \"year\"):\n                        vals.append(signal[field])\n                    vals = fromarrays(vals).tostring()\n\n                    fields.append(frombuffer(vals, dtype=\"V7\"))\n                    types.append((\"\", \"V7\"))\n\n                if invalidation_bytes_nr and invalidation_bits is not None:\n                    inval_bits.append(invalidation_bits)\n\n            elif sig_type == v4c.SIGNAL_TYPE_STRUCTURE_COMPOSITION:\n\n                if invalidation_bytes_nr and invalidation_bits is not None:\n                    inval_bits.append(invalidation_bits)\n\n                fields.append(signal)\n                types.append((\"\", signal.dtype))\n\n            elif sig_type == v4c.SIGNAL_TYPE_ARRAY:\n                names = signal.dtype.names\n\n                samples = signal[names[0]]\n\n                shape = samples.shape[1:]\n\n                fields.append(samples)\n                types.append((\"\", samples.dtype, shape))\n\n                if invalidation_bytes_nr and invalidation_bits is not None:\n                    inval_bits.append(invalidation_bits)\n\n                for name in names[1:]:\n\n                    samples = signal[name]\n                    shape = samples.shape[1:]\n                    fields.append(samples)\n                    types.append((\"\", samples.dtype, shape))\n\n                    if invalidation_bytes_nr and invalidation_bits is not None:\n                        inval_bits.append(invalidation_bits)\n\n            else:\n                cur_offset = sum(gp.signal_data_size[i])\n\n                offsets = (\n                    arange(len(signal), dtype=uint64) * (signal.itemsize + 4)\n                    + cur_offset\n                )\n                values = [full(len(signal), signal.itemsize, dtype=uint32), signal]\n\n                types_ = [(\"\", uint32), (\"\", signal.dtype)]\n\n                values = fromarrays(values, dtype=types_)\n\n                stream.seek(0, 2)\n                addr = stream.tell()\n                block_size = len(values) * values.itemsize\n                if block_size:\n                    values.tofile(stream)\n                    gp.signal_data[i].append(addr)\n                    gp.signal_data_size[i].append(block_size)\n\n                fields.append(offsets)\n                types.append((\"\", uint64))\n\n                if invalidation_bytes_nr and invalidation_bits is not None:\n                    inval_bits.append(invalidation_bits)\n\n        if invalidation_bytes_nr:\n            invalidation_bytes_nr = len(inval_bits)\n            cycles_nr = len(inval_bits[0])\n\n            for _ in range(8 - invalidation_bytes_nr % 8):\n                inval_bits.append(zeros(cycles_nr, dtype=bool))\n\n            inval_bits.reverse()\n\n            invalidation_bytes_nr = len(inval_bits) // 8\n\n            gp.channel_group.invalidation_bytes_nr = invalidation_bytes_nr\n\n            inval_bits = fliplr(\n                packbits(array(inval_bits).T).reshape(\n                    (cycles_nr, invalidation_bytes_nr)\n                )\n            )\n\n            fields.append(inval_bits)\n            types.append((\"invalidation_bytes\", inval_bits.dtype, inval_bits.shape[1:]))\n\n        samples = fromarrays(fields, dtype=types)\n\n        del fields\n        del types\n\n        stream.seek(0, 2)\n        addr = stream.tell()\n        size = len(samples) * samples.itemsize\n\n        if size:\n            samples.tofile(stream)\n\n            gp.data_blocks.append(\n                DataBlockInfo(\n                    address=addr,\n                    block_type=v4c.DT_BLOCK,\n                    raw_size=size,\n                    size=size,\n                    param=0,\n                )\n            )\n\n            record_size = gp.channel_group.samples_byte_nr\n            record_size += gp.data_group.record_id_len\n            record_size += gp.channel_group.invalidation_bytes_nr\n            added_cycles = size // record_size\n            gp.channel_group.cycles_nr += added_cycles\n\n        del samples", "response": "This method is used to extend the master channel with new samples. The master channel must have raw samples and physical values according to the signals used for the initial append. The master channel must have raw samples and physical values according to the signals used for the initial append."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattach embedded attachment as application / octet - stream", "response": "def attach(\n        self,\n        data,\n        file_name=None,\n        comment=None,\n        compression=True,\n        mime=r\"application/octet-stream\",\n        embedded=True,\n    ):\n        \"\"\" attach embedded attachment as application/octet-stream\n\n        Parameters\n        ----------\n        data : bytes\n            data to be attached\n        file_name : str\n            string file name\n        comment : str\n            attachment comment\n        compression : bool\n            use compression for embedded attachment data\n        mime : str\n            mime type string\n        embedded : bool\n            attachment is embedded in the file\n\n        Returns\n        -------\n        index : int\n            new attachment index\n\n        \"\"\"\n        if data in self._attachments_cache:\n            return self._attachments_cache[data]\n        else:\n            creator_index = len(self.file_history)\n            fh = FileHistory()\n            fh.comment = \"\"\"<FHcomment>\n<TX>Added new embedded attachment from {}</TX>\n<tool_id>asammdf</tool_id>\n<tool_vendor>asammdf</tool_vendor>\n<tool_version>{}</tool_version>\n</FHcomment>\"\"\".format(\n                file_name if file_name else \"bin.bin\", __version__\n            )\n\n            self.file_history.append(fh)\n\n            file_name = file_name or \"bin.bin\"\n\n            at_block = AttachmentBlock(\n                data=data,\n                compression=compression,\n                embedded=embedded,\n                file_name=file_name,\n            )\n            at_block[\"creator_index\"] = creator_index\n            index = v4c.MAX_UINT64 - 1\n            while index in self._attachments_map:\n                index -= 1\n            self.attachments.append(at_block)\n\n            at_block.mime = mime\n            at_block.comment = comment\n\n            self._attachments_cache[data] = index\n            self._attachments_map[index] = len(self.attachments) - 1\n\n            return index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(\n        self,\n        name=None,\n        group=None,\n        index=None,\n        raster=None,\n        samples_only=False,\n        data=None,\n        raw=False,\n        ignore_invalidation_bits=False,\n        source=None,\n        record_offset=0,\n        record_count=None,\n        copy_master=True,\n    ):\n        \"\"\"Gets channel samples. The raw data group samples are not loaded to\n        memory so it is advised to use ``filter`` or ``select`` instead of\n        performing several ``get`` calls.\n\n        Channel can be specified in two ways:\n\n        * using the first positional argument *name*\n\n            * if *source* is given this will be first used to validate the\n              channel selection\n            * if there are multiple occurances for this channel then the\n              *group* and *index* arguments can be used to select a specific\n              group.\n            * if there are multiple occurances for this channel and either the\n              *group* or *index* arguments is None then a warning is issued\n\n        * using the group number (keyword argument *group*) and the channel\n          number (keyword argument *index*). Use *info* method for group and\n          channel numbers\n\n        If the *raster* keyword argument is not *None* the output is\n        interpolated accordingly\n\n        Parameters\n        ----------\n        name : string\n            name of channel\n        group : int\n            0-based group index\n        index : int\n            0-based channel index\n        raster : float\n            time raster in seconds\n        samples_only : bool\n            if *True* return only the channel samples as numpy array; if\n                *False* return a *Signal* object\n        data : bytes\n            prevent redundant data read by providing the raw data group samples\n        raw : bool\n            return channel samples without appling the conversion rule; default\n            `False`\n        ignore_invalidation_bits : bool\n            option to ignore invalidation bits\n        source : str\n            source name used to select the channel\n        record_offset : int\n            if *data=None* use this to select the record offset from which the\n            group data should be loaded\n        record_count : int\n            number of records to read; default *None* and in this case all\n            available records are used\n        copy_master : bool\n            make a copy of the timebase for this channel\n\n        Returns\n        -------\n        res : (numpy.array, numpy.array) | Signal\n            returns *Signal* if *samples_only*=*False* (default option),\n            otherwise returns a (numpy.array, numpy.array) tuple of samples and\n            invalidation bits. If invalidation bits are not used or if\n            *ignore_invalidation_bits* if False, then the second item will be\n            None.\n\n            The *Signal* samples are:\n\n                * numpy recarray for channels that have composition/channel\n                  array address or for channel of type\n                  CANOPENDATE, CANOPENTIME\n                * numpy array for all the rest\n\n        Raises\n        ------\n        MdfException :\n\n        * if the channel name is not found\n        * if the group index is out of range\n        * if the channel index is out of range\n\n        Examples\n        --------\n        >>> from asammdf import MDF, Signal\n        >>> import numpy as np\n        >>> t = np.arange(5)\n        >>> s = np.ones(5)\n        >>> mdf = MDF(version='4.10')\n        >>> for i in range(4):\n        ...     sigs = [Signal(s*(i*10+j), t, name='Sig') for j in range(1, 4)]\n        ...     mdf.append(sigs)\n        ...\n        >>> # first group and channel index of the specified channel name\n        ...\n        >>> mdf.get('Sig')\n        UserWarning: Multiple occurances for channel \"Sig\". Using first occurance from data group 4. Provide both \"group\" and \"index\" arguments to select another data group\n        <Signal Sig:\n                samples=[ 1.  1.  1.  1.  1.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        >>> # first channel index in the specified group\n        ...\n        >>> mdf.get('Sig', 1)\n        <Signal Sig:\n                samples=[ 11.  11.  11.  11.  11.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        >>> # channel named Sig from group 1 channel index 2\n        ...\n        >>> mdf.get('Sig', 1, 2)\n        <Signal Sig:\n                samples=[ 12.  12.  12.  12.  12.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        >>> # channel index 1 or group 2\n        ...\n        >>> mdf.get(None, 2, 1)\n        <Signal Sig:\n                samples=[ 21.  21.  21.  21.  21.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        >>> mdf.get(group=2, index=1)\n        <Signal Sig:\n                samples=[ 21.  21.  21.  21.  21.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        >>> # validation using source name\n        ...\n        >>> mdf.get('Sig', source='VN7060')\n        <Signal Sig:\n                samples=[ 12.  12.  12.  12.  12.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n\n        \"\"\"\n\n        gp_nr, ch_nr = self._validate_channel_selection(\n            name, group, index, source=source\n        )\n\n        grp = self.groups[gp_nr]\n\n        interp_mode = self._integer_interpolation\n\n        original_data = data\n\n        if ch_nr >= 0:\n\n            # get the channel object\n            channel = grp.channels[ch_nr]\n\n            dependency_list = grp.channel_dependencies[ch_nr]\n\n            # get data group record\n            parents, dtypes = grp.parents, grp.types\n            if parents is None:\n                parents, dtypes = self._prepare_record(grp)\n\n            # get group data\n            if data is None:\n                data = self._load_data(\n                    grp, record_offset=record_offset, record_count=record_count\n                )\n            else:\n                data = (data,)\n\n            channel_invalidation_present = (\n                channel.flags\n                & (v4c.FLAG_INVALIDATION_BIT_VALID | v4c.FLAG_ALL_SAMPLES_VALID)\n                == v4c.FLAG_INVALIDATION_BIT_VALID\n            )\n\n            bit_count = channel.bit_count\n        else:\n            # get data group record\n            parents, dtypes = self._prepare_record(grp)\n\n            parent, bit_offset = parents[ch_nr]\n\n            channel_invalidation_present = False\n            dependency_list = None\n\n            channel = grp.logging_channels[-ch_nr - 1]\n\n            # get group data\n            if data is None:\n                data = self._load_data(\n                    grp, record_offset=record_offset, record_count=record_count\n                )\n            else:\n                data = (data,)\n\n            bit_count = channel.bit_count\n\n        data_type = channel.data_type\n        channel_type = channel.channel_type\n        stream_sync = channel_type == v4c.CHANNEL_TYPE_SYNC\n\n        encoding = None\n\n        master_is_required = not samples_only or raster\n\n        # check if this is a channel array\n        if dependency_list:\n            if not isinstance(dependency_list[0], ChannelArrayBlock):\n                # structure channel composition\n\n                _dtype = dtype(channel.dtype_fmt)\n                if _dtype.itemsize == bit_count // 8:\n                    fast_path = True\n                    channel_values = []\n                    timestamps = []\n                    invalidation_bits = []\n\n                    byte_offset = channel.byte_offset\n                    record_size = (\n                        grp.channel_group.samples_byte_nr\n                        + grp.channel_group.invalidation_bytes_nr\n                    )\n\n                    count = 0\n                    for fragment in data:\n\n                        bts = fragment[0]\n                        types = [\n                            (\"\", f\"V{byte_offset}\"),\n                            (\"vals\", _dtype),\n                            (\"\", f\"V{record_size - _dtype.itemsize - byte_offset}\"),\n                        ]\n\n                        channel_values.append(fromstring(bts, types)[\"vals\"].copy())\n\n                        if master_is_required:\n                            timestamps.append(\n                                self.get_master(\n                                    gp_nr, fragment, copy_master=copy_master\n                                )\n                            )\n                        if channel_invalidation_present:\n                            invalidation_bits.append(\n                                self.get_invalidation_bits(gp_nr, channel, fragment)\n                            )\n\n                        count += 1\n                else:\n                    fast_path = False\n                    names = [grp.channels[ch_nr].name for _, ch_nr in dependency_list]\n\n                    channel_values = [[] for _ in dependency_list]\n                    timestamps = []\n                    invalidation_bits = []\n\n                    count = 0\n                    for fragment in data:\n                        for i, (dg_nr, ch_nr) in enumerate(dependency_list):\n                            vals = self.get(\n                                group=dg_nr,\n                                index=ch_nr,\n                                samples_only=True,\n                                data=fragment,\n                                ignore_invalidation_bits=ignore_invalidation_bits,\n                                record_offset=record_offset,\n                                record_count=record_count,\n                            )[0]\n                            channel_values[i].append(vals)\n                        if master_is_required:\n                            timestamps.append(\n                                self.get_master(\n                                    gp_nr, fragment, copy_master=copy_master\n                                )\n                            )\n                        if channel_invalidation_present:\n                            invalidation_bits.append(\n                                self.get_invalidation_bits(gp_nr, channel, fragment)\n                            )\n\n                        count += 1\n\n                if fast_path:\n                    if count > 1:\n                        vals = concatenate(channel_values)\n                    else:\n                        vals = channel_values[0]\n                else:\n                    if count > 1:\n                        arrays = [concatenate(lst) for lst in channel_values]\n                    else:\n                        arrays = [lst[0] for lst in channel_values]\n                    types = [\n                        (name_, arr.dtype, arr.shape[1:])\n                        for name_, arr in zip(names, arrays)\n                    ]\n                    types = dtype(types)\n\n                    vals = fromarrays(arrays, dtype=types)\n\n                if master_is_required:\n                    if count > 1:\n                        timestamps = concatenate(timestamps)\n                    else:\n                        timestamps = timestamps[0]\n\n                if channel_invalidation_present:\n                    if count > 1:\n                        invalidation_bits = concatenate(invalidation_bits)\n                    else:\n                        invalidation_bits = invalidation_bits[0]\n                    if not ignore_invalidation_bits:\n                        vals = vals[nonzero(~invalidation_bits)[0]]\n                        if master_is_required:\n                            timestamps = timestamps[nonzero(~invalidation_bits)[0]]\n\n                if raster and len(timestamps) > 1:\n                    t = arange(timestamps[0], timestamps[-1], raster)\n\n                    vals = (\n                        Signal(vals, timestamps, name=\"_\")\n                        .interp(t, interpolation_mode=interp_mode)\n                    )\n\n                    vals, timestamps, invalidation_bits = (\n                        vals.samples,\n                        vals.timestamps,\n                        vals.invalidation_bits,\n                    )\n\n            else:\n                # channel arrays\n                channel_group = grp.channel_group\n                samples_size = (\n                    channel_group.samples_byte_nr + channel_group.invalidation_bytes_nr\n                )\n\n                channel_values = []\n                timestamps = []\n                invalidation_bits = []\n                count = 0\n                for fragment in data:\n\n                    data_bytes, offset, _count = fragment\n\n                    cycles = len(data_bytes) // samples_size\n\n                    arrays = []\n                    types = []\n                    try:\n                        parent, bit_offset = parents[ch_nr]\n                    except KeyError:\n                        parent, bit_offset = None, None\n\n                    if parent is not None:\n                        if grp.record is None:\n                            dtypes = grp.types\n                            if dtypes.itemsize:\n                                record = fromstring(data_bytes, dtype=dtypes)\n                            else:\n                                record = None\n\n                        else:\n                            record = grp.record\n\n                        vals = record[parent]\n                    else:\n                        vals = self._get_not_byte_aligned_data(data_bytes, grp, ch_nr)\n\n                    vals = vals.copy()\n\n                    dep = dependency_list[0]\n                    if dep.flags & v4c.FLAG_CA_INVERSE_LAYOUT:\n                        shape = vals.shape\n                        shape = (shape[0],) + shape[1:][::-1]\n                        vals = vals.reshape(shape)\n\n                        axes = (0,) + tuple(range(len(shape) - 1, 0, -1))\n                        vals = transpose(vals, axes=axes)\n\n                    cycles_nr = len(vals)\n\n                    for ca_block in dependency_list[:1]:\n                        dims_nr = ca_block.dims\n\n                        if ca_block.ca_type == v4c.CA_TYPE_SCALE_AXIS:\n                            shape = (ca_block.dim_size_0,)\n                            arrays.append(vals)\n                            dtype_pair = channel.name, vals.dtype, shape\n                            types.append(dtype_pair)\n\n                        elif ca_block.ca_type == v4c.CA_TYPE_LOOKUP:\n                            shape = vals.shape[1:]\n                            arrays.append(vals)\n                            dtype_pair = channel.name, vals.dtype, shape\n                            types.append(dtype_pair)\n\n                            if ca_block.flags & v4c.FLAG_CA_FIXED_AXIS:\n                                for i in range(dims_nr):\n                                    shape = (ca_block[f\"dim_size_{i}\"],)\n                                    axis = []\n                                    for j in range(shape[0]):\n                                        key = f\"axis_{i}_value_{j}\"\n                                        axis.append(ca_block[key])\n                                    axis = array([axis for _ in range(cycles_nr)])\n                                    arrays.append(axis)\n                                    dtype_pair = (f\"axis_{i}\", axis.dtype, shape)\n                                    types.append(dtype_pair)\n                            else:\n                                for i in range(dims_nr):\n                                    try:\n                                        ref_dg_nr, ref_ch_nr = ca_block.referenced_channels[\n                                            i\n                                        ]\n                                    except:\n                                        debug_channel(\n                                            self, grp, channel, dependency_list\n                                        )\n                                        raise\n\n                                    axisname = (\n                                        self.groups[ref_dg_nr].channels[ref_ch_nr].name\n                                    )\n\n                                    shape = (ca_block[f\"dim_size_{i}\"],)\n                                    if ref_dg_nr == gp_nr:\n                                        axis_values = self.get(\n                                            group=ref_dg_nr,\n                                            index=ref_ch_nr,\n                                            samples_only=True,\n                                            data=fragment,\n                                            ignore_invalidation_bits=ignore_invalidation_bits,\n                                            record_offset=record_offset,\n                                            record_count=cycles,\n                                        )[0]\n                                    else:\n                                        channel_group = grp.channel_group\n                                        record_size = channel_group.samples_byte_nr\n                                        record_size += (\n                                            channel_group.invalidation_bytes_nr\n                                        )\n                                        start = offset // record_size\n                                        end = start + len(data_bytes) // record_size + 1\n                                        ref = self.get(\n                                            group=ref_dg_nr,\n                                            index=ref_ch_nr,\n                                            samples_only=True,\n                                            ignore_invalidation_bits=ignore_invalidation_bits,\n                                            record_offset=record_offset,\n                                            record_count=cycles,\n                                        )[0]\n                                        axis_values = ref[start:end].copy()\n                                    axis_values = axis_values[axisname]\n\n                                    arrays.append(axis_values)\n                                    dtype_pair = (axisname, axis_values.dtype, shape)\n                                    types.append(dtype_pair)\n\n                        elif ca_block.ca_type == v4c.CA_TYPE_ARRAY:\n                            shape = vals.shape[1:]\n                            arrays.append(vals)\n                            dtype_pair = channel.name, vals.dtype, shape\n                            types.append(dtype_pair)\n\n                    for ca_block in dependency_list[1:]:\n                        dims_nr = ca_block.dims\n\n                        if ca_block.flags & v4c.FLAG_CA_FIXED_AXIS:\n                            for i in range(dims_nr):\n                                shape = (ca_block[f\"dim_size_{i}\"],)\n                                axis = []\n                                for j in range(shape[0]):\n                                    key = f\"axis_{i}_value_{j}\"\n                                    axis.append(ca_block[key])\n                                axis = array([axis for _ in range(cycles_nr)])\n                                arrays.append(axis)\n                                types.append((f\"axis_{i}\", axis.dtype, shape))\n                        else:\n                            for i in range(dims_nr):\n                                ref_dg_nr, ref_ch_nr = ca_block.referenced_channels[i]\n\n                                axisname = (\n                                    self.groups[ref_dg_nr].channels[ref_ch_nr].name\n                                )\n\n                                shape = (ca_block[f\"dim_size_{i}\"],)\n                                if ref_dg_nr == gp_nr:\n                                    axis_values = self.get(\n                                        group=ref_dg_nr,\n                                        index=ref_ch_nr,\n                                        samples_only=True,\n                                        data=fragment,\n                                        ignore_invalidation_bits=ignore_invalidation_bits,\n                                        record_offset=record_offset,\n                                        record_count=cycles,\n                                    )[0]\n                                else:\n                                    channel_group = grp.channel_group\n                                    record_size = channel_group.samples_byte_nr\n                                    record_size += channel_group.invalidation_bytes_nr\n                                    start = offset // record_size\n                                    end = start + len(data_bytes) // record_size + 1\n                                    ref = self.get(\n                                        group=ref_dg_nr,\n                                        index=ref_ch_nr,\n                                        samples_only=True,\n                                        ignore_invalidation_bits=ignore_invalidation_bits,\n                                        record_offset=record_offset,\n                                        record_count=cycles,\n                                    )[0]\n                                    axis_values = ref[start:end].copy()\n                                axis_values = axis_values[axisname]\n\n                                arrays.append(axis_values)\n                                dtype_pair = axisname, axis_values.dtype, shape\n                                types.append(dtype_pair)\n\n                    vals = fromarrays(arrays, dtype(types))\n\n                    if master_is_required:\n                        timestamps.append(\n                            self.get_master(gp_nr, fragment, copy_master=copy_master)\n                        )\n                    if channel_invalidation_present:\n                        invalidation_bits.append(\n                            self.get_invalidation_bits(gp_nr, channel, fragment)\n                        )\n\n                    channel_values.append(vals)\n                    count += 1\n\n                if count > 1:\n                    vals = concatenate(channel_values)\n                elif count == 1:\n                    vals = channel_values[0]\n                else:\n                    vals = []\n\n                if master_is_required:\n                    if count > 1:\n                        timestamps = concatenate(timestamps)\n                    else:\n                        timestamps = timestamps[0]\n\n                if channel_invalidation_present:\n                    if count > 1:\n                        invalidation_bits = concatenate(invalidation_bits)\n                    else:\n                        invalidation_bits = invalidation_bits[0]\n                    if not ignore_invalidation_bits:\n                        vals = vals[nonzero(~invalidation_bits)[0]]\n                        if master_is_required:\n                            timestamps = timestamps[nonzero(~invalidation_bits)[0]]\n\n                if raster and len(timestamps) > 1:\n                    t = arange(timestamps[0], timestamps[-1], raster)\n\n                    vals = (\n                        Signal(vals, timestamps, name=\"_\")\n                        .interp(t, interpolation_mode=interp_mode)\n                    )\n\n                    vals, timestamps, invalidation_bits = (\n                        vals.samples,\n                        vals.timestamps,\n                        vals.invalidation_bits,\n                    )\n\n            conversion = channel.conversion\n\n        else:\n            # get channel values\n            if channel_type in {\n                v4c.CHANNEL_TYPE_VIRTUAL,\n                v4c.CHANNEL_TYPE_VIRTUAL_MASTER,\n            }:\n                if not channel.dtype_fmt:\n                    channel.dtype_fmt = get_fmt_v4(data_type, 64)\n                ch_dtype = dtype(channel.dtype_fmt)\n\n                channel_values = []\n                timestamps = []\n                invalidation_bits = []\n\n                channel_group = grp.channel_group\n                record_size = channel_group.samples_byte_nr\n                record_size += channel_group.invalidation_bytes_nr\n\n                count = 0\n                for fragment in data:\n                    data_bytes, offset, _count = fragment\n                    offset = offset // record_size\n\n                    vals = arange(len(data_bytes) // record_size, dtype=ch_dtype)\n                    vals += offset\n\n                    if master_is_required:\n                        timestamps.append(\n                            self.get_master(gp_nr, fragment, copy_master=copy_master)\n                        )\n                    if channel_invalidation_present:\n                        invalidation_bits.append(\n                            self.get_invalidation_bits(gp_nr, channel, fragment)\n                        )\n\n                    channel_values.append(vals)\n                    count += 1\n\n                if count > 1:\n                    vals = concatenate(channel_values)\n                elif count == 1:\n                    vals = channel_values[0]\n                else:\n                    vals = []\n\n                if master_is_required:\n                    if count > 1:\n                        timestamps = concatenate(timestamps)\n                    else:\n                        timestamps = timestamps[0]\n\n                if channel_invalidation_present:\n                    if count > 1:\n                        invalidation_bits = concatenate(invalidation_bits)\n                    else:\n                        invalidation_bits = invalidation_bits[0]\n                    if not ignore_invalidation_bits:\n                        vals = vals[nonzero(~invalidation_bits)[0]]\n                        if master_is_required:\n                            timestamps = timestamps[nonzero(~invalidation_bits)[0]]\n\n                if raster and len(timestamps) > 1:\n                    num = float(float32((timestamps[-1] - timestamps[0]) / raster))\n                    if num.is_integer():\n                        t = linspace(timestamps[0], timestamps[-1], int(num))\n                    else:\n                        t = arange(timestamps[0], timestamps[-1], raster)\n\n                    vals = (\n                        Signal(vals, timestamps, name=\"_\")\n                        .interp(t, interpolation_mode=interp_mode)\n                    )\n\n                    vals, timestamps, invalidation_bits = (\n                        vals.samples,\n                        vals.timestamps,\n                        vals.invalidation_bits,\n                    )\n\n            else:\n                channel_values = []\n                timestamps = []\n                invalidation_bits = []\n\n                count = 0\n                for fragment in data:\n\n                    data_bytes, offset, _count = fragment\n                    try:\n                        parent, bit_offset = parents[ch_nr]\n                    except KeyError:\n                        parent, bit_offset = None, None\n\n                    if parent is not None:\n                        if grp.record is None:\n                            record = fromstring(data_bytes, dtype=dtypes)\n\n                        else:\n                            record = grp.record\n\n                        vals = record[parent]\n\n                        dtype_ = vals.dtype\n                        shape_ = vals.shape\n                        size = vals.dtype.itemsize\n                        for dim in shape_[1:]:\n                            size *= dim\n\n                        kind_ = dtype_.kind\n\n                        if kind_ == \"b\":\n                            pass\n                        elif len(shape_) > 1 and data_type != v4c.DATA_TYPE_BYTEARRAY:\n                            vals = self._get_not_byte_aligned_data(\n                                data_bytes, grp, ch_nr\n                            )\n                        elif kind_ not in \"ui\":\n                            if bit_offset:\n                                vals = self._get_not_byte_aligned_data(\n                                    data_bytes, grp, ch_nr\n                                )\n                            else:\n                                if bit_count != size * 8:\n                                    if (\n                                        bit_count % 8 == 0\n                                        and size in (2, 4, 8)\n                                        and data_type <= 3\n                                    ):  # integer types\n                                        vals = vals.view(f\"<u{size}\")\n                                        if data_type in v4c.SIGNED_INT:\n                                            vals = as_non_byte_sized_signed_int(\n                                                vals, bit_count\n                                            )\n                                        else:\n                                            mask = (1 << bit_count) - 1\n                                            if vals.flags.writeable:\n                                                vals &= mask\n                                            else:\n                                                vals = vals & mask\n                                    else:\n                                        vals = self._get_not_byte_aligned_data(\n                                            data_bytes, grp, ch_nr\n                                        )\n                                else:\n                                    if data_type <= 3:\n                                        if not channel.dtype_fmt:\n                                            channel.dtype_fmt = get_fmt_v4(data_type, bit_count, channel_type)\n                                        channel_dtype = dtype(channel.dtype_fmt.split(')')[-1])\n                                        vals = vals.view(channel_dtype)\n\n                        else:\n                            if data_type <= 3:\n                                if dtype_.byteorder == \">\":\n                                    if bit_offset or bit_count != size << 3:\n                                        vals = self._get_not_byte_aligned_data(\n                                            data_bytes, grp, ch_nr\n                                        )\n                                else:\n                                    if bit_offset:\n                                        if kind_ == \"i\":\n                                            vals = vals.astype(\n                                                dtype(f\"{dtype_.byteorder}u{size}\")\n                                            )\n                                            vals >>= bit_offset\n                                        else:\n                                            vals = vals >> bit_offset\n\n                                    if bit_count != size << 3:\n                                        if data_type in v4c.SIGNED_INT:\n                                            vals = as_non_byte_sized_signed_int(\n                                                vals, bit_count\n                                            )\n                                        else:\n                                            mask = (1 << bit_count) - 1\n                                            if vals.flags.writeable:\n                                                vals &= mask\n                                            else:\n                                                vals = vals & mask\n                            else:\n                                if bit_count != size * 8:\n                                    vals = self._get_not_byte_aligned_data(\n                                        data_bytes, grp, ch_nr\n                                    )\n                                else:\n                                    if not channel.dtype_fmt:\n                                        channel.dtype_fmt = get_fmt_v4(data_type, bit_count, channel_type)\n                                    channel_dtype = dtype(channel.dtype_fmt.split(')')[-1])\n                                    vals = vals.view(channel_dtype)\n\n                    else:\n                        vals = self._get_not_byte_aligned_data(data_bytes, grp, ch_nr)\n\n                    if bit_count == 1 and self._single_bit_uint_as_bool:\n                        vals = array(vals, dtype=bool)\n                    else:\n                        if not channel.dtype_fmt:\n                            channel.dtype_fmt = get_fmt_v4(\n                                data_type, bit_count, channel_type,\n                            )\n                        channel_dtype = dtype(channel.dtype_fmt.split(\")\")[-1])\n                        if vals.dtype != channel_dtype:\n                            vals = vals.astype(channel_dtype)\n\n                    if master_is_required:\n                        timestamps.append(\n                            self.get_master(gp_nr, fragment, copy_master=copy_master)\n                        )\n                    if channel_invalidation_present:\n                        invalidation_bits.append(\n                            self.get_invalidation_bits(gp_nr, channel, fragment)\n                        )\n\n                    if vals.flags.writeable:\n                        channel_values.append(vals)\n                    else:\n                        channel_values.append(vals.copy())\n                    count += 1\n\n                if count > 1:\n                    vals = concatenate(channel_values)\n                elif count == 1:\n                    vals = channel_values[0]\n                else:\n                    vals = []\n\n                if master_is_required:\n                    if count > 1:\n                        timestamps = concatenate(timestamps)\n                    elif count == 1:\n                        timestamps = timestamps[0]\n                    else:\n                        timestamps = []\n\n                if channel_invalidation_present:\n                    if count > 1:\n                        invalidation_bits = concatenate(invalidation_bits)\n                    elif count == 1:\n                        invalidation_bits = invalidation_bits[0]\n                    else:\n                        invalidation_bits = []\n                    if not ignore_invalidation_bits:\n                        vals = vals[nonzero(~invalidation_bits)[0]]\n                        if master_is_required:\n                            timestamps = timestamps[nonzero(~invalidation_bits)[0]]\n\n                if raster and len(timestamps) > 1:\n\n                    num = float(float32((timestamps[-1] - timestamps[0]) / raster))\n                    if num.is_integer():\n                        t = linspace(timestamps[0], timestamps[-1], int(num))\n                    else:\n                        t = arange(timestamps[0], timestamps[-1], raster)\n\n                    vals = (\n                        Signal(vals, timestamps, name=\"_\")\n                        .interp(t, interpolation_mode=interp_mode)\n                    )\n\n                    vals, timestamps, invalidation_bits = (\n                        vals.samples,\n                        vals.timestamps,\n                        vals.invalidation_bits,\n                    )\n\n            # get the channel conversion\n            conversion = channel.conversion\n\n            if channel_type == v4c.CHANNEL_TYPE_VLSD:\n                signal_data = self._load_signal_data(group=grp, index=ch_nr)\n                if signal_data:\n                    values = []\n\n                    vals = vals.tolist()\n\n                    for offset in vals:\n                        (str_size,) = UINT32_uf(signal_data, offset)\n                        offset += 4\n                        values.append(signal_data[offset : offset + str_size])\n\n                    if data_type == v4c.DATA_TYPE_BYTEARRAY:\n\n                        vals = array(values)\n                        vals = vals.view(dtype=f\"({vals.itemsize},)u1\")\n\n                    else:\n\n                        vals = array(values)\n\n                        if data_type == v4c.DATA_TYPE_STRING_UTF_16_BE:\n                            encoding = \"utf-16-be\"\n\n                        elif data_type == v4c.DATA_TYPE_STRING_UTF_16_LE:\n                            encoding = \"utf-16-le\"\n\n                        elif data_type == v4c.DATA_TYPE_STRING_UTF_8:\n                            encoding = \"utf-8\"\n\n                        elif data_type == v4c.DATA_TYPE_STRING_LATIN_1:\n                            encoding = \"latin-1\"\n\n                        else:\n                            raise MdfException(\n                                f'wrong data type \"{data_type}\" for vlsd channel'\n                            )\n\n                else:\n                    # no VLSD signal data samples\n                    if data_type != v4c.DATA_TYPE_BYTEARRAY:\n                        vals = array([], dtype=\"S\")\n\n                        if data_type == v4c.DATA_TYPE_STRING_UTF_16_BE:\n                            encoding = \"utf-16-be\"\n\n                        elif data_type == v4c.DATA_TYPE_STRING_UTF_16_LE:\n                            encoding = \"utf-16-le\"\n\n                        elif data_type == v4c.DATA_TYPE_STRING_UTF_8:\n                            encoding = \"utf-8\"\n\n                        elif data_type == v4c.DATA_TYPE_STRING_LATIN_1:\n                            encoding = \"latin-1\"\n\n                        else:\n                            raise MdfException(\n                                f'wrong data type \"{data_type}\" for vlsd channel'\n                            )\n                    else:\n                        vals = array([], dtype=get_fmt_v4(data_type, bit_count, v4c.CHANNEL_TYPE_VALUE))\n\n            elif channel_type in {\n                v4c.CHANNEL_TYPE_VALUE,\n                v4c.CHANNEL_TYPE_MLSD,\n            } and (\n                v4c.DATA_TYPE_STRING_LATIN_1\n                <= data_type\n                <= v4c.DATA_TYPE_STRING_UTF_16_BE\n            ):\n\n                if data_type == v4c.DATA_TYPE_STRING_UTF_16_BE:\n                    encoding = \"utf-16-be\"\n\n                elif data_type == v4c.DATA_TYPE_STRING_UTF_16_LE:\n                    encoding = \"utf-16-le\"\n\n                elif data_type == v4c.DATA_TYPE_STRING_UTF_8:\n                    encoding = \"utf-8\"\n\n                elif data_type == v4c.DATA_TYPE_STRING_LATIN_1:\n                    encoding = \"latin-1\"\n\n                else:\n                    raise MdfException(\n                        f'wrong data type \"{data_type}\" for string channel'\n                    )\n\n            # CANopen date\n            if data_type == v4c.DATA_TYPE_CANOPEN_DATE:\n\n                vals = vals.tostring()\n\n                types = dtype(\n                    [\n                        (\"ms\", \"<u2\"),\n                        (\"min\", \"<u1\"),\n                        (\"hour\", \"<u1\"),\n                        (\"day\", \"<u1\"),\n                        (\"month\", \"<u1\"),\n                        (\"year\", \"<u1\"),\n                    ]\n                )\n                vals = vals.view(types)\n\n                arrays = []\n                arrays.append(vals[\"ms\"])\n                # bit 6 and 7 of minutes are reserved\n                arrays.append(vals[\"min\"] & 0x3F)\n                # only firt 4 bits of hour are used\n                arrays.append(vals[\"hour\"] & 0xF)\n                # the first 4 bits are the day number\n                arrays.append(vals[\"day\"] & 0xF)\n                # bit 6 and 7 of month are reserved\n                arrays.append(vals[\"month\"] & 0x3F)\n                # bit 7 of year is reserved\n                arrays.append(vals[\"year\"] & 0x7F)\n                # add summer or standard time information for hour\n                arrays.append((vals[\"hour\"] & 0x80) >> 7)\n                # add day of week information\n                arrays.append((vals[\"day\"] & 0xF0) >> 4)\n\n                names = [\n                    \"ms\",\n                    \"min\",\n                    \"hour\",\n                    \"day\",\n                    \"month\",\n                    \"year\",\n                    \"summer_time\",\n                    \"day_of_week\",\n                ]\n                vals = fromarrays(arrays, names=names)\n\n                del arrays\n                conversion = None\n\n            # CANopen time\n            elif data_type == v4c.DATA_TYPE_CANOPEN_TIME:\n\n                types = dtype([(\"ms\", \"<u4\"), (\"days\", \"<u2\")])\n                vals = vals.view(types)\n\n                arrays = []\n                # bits 28 to 31 are reserverd for ms\n                arrays.append(vals[\"ms\"] & 0xFFFFFFF)\n                arrays.append(vals[\"days\"] & 0x3F)\n\n                names = [\"ms\", \"days\"]\n                vals = fromarrays(arrays, names=names)\n\n                del arrays\n\n            if not raw:\n                if conversion:\n                    vals = conversion.convert(vals)\n                    conversion = None\n\n        if samples_only:\n            if not channel_invalidation_present or not ignore_invalidation_bits:\n                invalidation_bits = None\n            res = vals, invalidation_bits\n        else:\n            # search for unit in conversion texts\n\n            if name is None:\n                name = channel.name\n\n            unit = conversion and conversion.unit or channel.unit\n\n            comment = channel.comment\n\n            source = channel.source\n            cg_source = grp.channel_group.acq_source\n            if source:\n                source = SignalSource(\n                    source.name,\n                    source.path,\n                    source.comment,\n                    source.source_type,\n                    source.bus_type,\n                )\n            elif cg_source:\n                source = SignalSource(\n                    cg_source.name,\n                    cg_source.path,\n                    cg_source.comment,\n                    cg_source.source_type,\n                    cg_source.bus_type,\n                )\n            else:\n                source = None\n\n            if hasattr(channel, \"attachment_addr\"):\n                index = self._attachments_map[channel.attachment_addr]\n                attachment = self.extract_attachment(index=index)\n            elif channel_type == v4c.CHANNEL_TYPE_SYNC:\n                index = self._attachments_map[channel.data_block_addr]\n                attachment = self.extract_attachment(index=index)\n            else:\n                attachment = ()\n\n            master_metadata = self._master_channel_metadata.get(gp_nr, None)\n\n            if not channel_invalidation_present or not ignore_invalidation_bits:\n                invalidation_bits = None\n\n            try:\n                res = Signal(\n                    samples=vals,\n                    timestamps=timestamps,\n                    unit=unit,\n                    name=name,\n                    comment=comment,\n                    conversion=conversion,\n                    raw=raw,\n                    master_metadata=master_metadata,\n                    attachment=attachment,\n                    source=source,\n                    display_name=channel.display_name,\n                    bit_count=bit_count,\n                    stream_sync=stream_sync,\n                    invalidation_bits=invalidation_bits,\n                    encoding=encoding,\n                )\n            except:\n                debug_channel(self, grp, channel, dependency_list)\n                raise\n\n        return res", "response": "Returns the raw data of the specified channel entry in the base - level log file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning master channel samples for given group", "response": "def get_master(\n        self,\n        index,\n        data=None,\n        raster=None,\n        record_offset=0,\n        record_count=None,\n        copy_master=True,\n    ):\n        \"\"\" returns master channel samples for given group\n\n        Parameters\n        ----------\n        index : int\n            group index\n        data : (bytes, int)\n            (data block raw bytes, fragment offset); default None\n        raster : float\n            raster to be used for interpolation; default None\n        record_offset : int\n            if *data=None* use this to select the record offset from which the\n            group data should be loaded\n        record_count : int\n            number of records to read; default *None* and in this case all\n            available records are used\n        copy_master : bool\n            return a copy of the cached master\n\n        Returns\n        -------\n        t : numpy.array\n            master channel samples\n\n        \"\"\"\n        fragment = data\n        if fragment:\n            data_bytes, offset, _count = fragment\n            try:\n                timestamps = self._master_channel_cache[(index, offset, _count)]\n                if raster and len(timestamps):\n                    timestamps = arange(timestamps[0], timestamps[-1], raster)\n                    return timestamps\n                else:\n                    if copy_master:\n                        return timestamps.copy()\n                    else:\n                        return timestamps\n            except KeyError:\n                pass\n        else:\n            try:\n                timestamps = self._master_channel_cache[index]\n                if raster and len(timestamps):\n                    timestamps = arange(timestamps[0], timestamps[-1], raster)\n                    return timestamps\n                else:\n                    if copy_master:\n                        return timestamps.copy()\n                    else:\n                        return timestamps\n            except KeyError:\n                offset = 0\n\n        group = self.groups[index]\n\n        original_data = fragment\n\n        if group.data_location == v4c.LOCATION_ORIGINAL_FILE:\n            stream = self._file\n        else:\n            stream = self._tempfile\n\n        time_ch_nr = self.masters_db.get(index, None)\n        channel_group = group.channel_group\n        record_size = channel_group.samples_byte_nr\n        record_size += channel_group.invalidation_bytes_nr\n        cycles_nr = group.channel_group.cycles_nr\n\n        if original_data:\n            cycles_nr = len(data_bytes) // record_size\n        else:\n            _count = record_count\n\n        if time_ch_nr is None:\n            if record_size:\n                offset = offset // record_size\n                t = arange(cycles_nr, dtype=float64)\n                t += offset\n            else:\n                t = array([], dtype=float64)\n            metadata = (\"time\", v4c.SYNC_TYPE_TIME)\n        else:\n\n            time_ch = group.channels[time_ch_nr]\n            time_conv = time_ch.conversion\n            time_name = time_ch.name\n\n            metadata = (time_name, time_ch.sync_type)\n\n            if time_ch.channel_type == v4c.CHANNEL_TYPE_VIRTUAL_MASTER:\n                offset = offset // record_size\n                time_a = time_conv[\"a\"]\n                time_b = time_conv[\"b\"]\n                t = arange(cycles_nr, dtype=float64)\n                t += offset\n                t *= time_a\n                t += time_b\n\n                if record_count is None:\n                    t = t[record_offset:]\n                else:\n                    t = t[record_offset: record_offset+record_count]\n\n            else:\n                # get data group parents and dtypes\n                parents, dtypes = group.parents, group.types\n                if parents is None:\n                    parents, dtypes = self._prepare_record(group)\n\n                # get data\n                if fragment is None:\n                    data = self._load_data(\n                        group, record_offset=record_offset, record_count=record_count\n                    )\n                else:\n                    data = (fragment,)\n                time_values = []\n\n                for fragment in data:\n                    data_bytes, offset, _count = fragment\n                    try:\n                        parent, _ = parents[time_ch_nr]\n                    except KeyError:\n                        parent = None\n                    if parent is not None:\n                        if group.record is None:\n                            dtypes = group.types\n                            if dtypes.itemsize:\n                                record = fromstring(data_bytes, dtype=dtypes)\n                            else:\n                                record = None\n                        else:\n                            record = group.record\n\n                        t = record[parent]\n                    else:\n                        t = self._get_not_byte_aligned_data(\n                            data_bytes, group, time_ch_nr\n                        )\n\n                    time_values.append(t.copy())\n\n                if len(time_values) > 1:\n                    t = concatenate(time_values)\n                else:\n                    t = time_values[0]\n\n                # get timestamps\n                if time_conv:\n                    t = time_conv.convert(t)\n\n        self._master_channel_metadata[index] = metadata\n\n        if not t.dtype == float64:\n            t = t.astype(float64)\n\n        if original_data is None:\n            self._master_channel_cache[index] = t\n        else:\n            data_bytes, offset, _ = original_data\n            self._master_channel_cache[(index, offset, _count)] = t\n\n        if raster and t.size:\n            timestamps = t\n            if len(t) > 1:\n                num = float(float32((timestamps[-1] - timestamps[0]) / raster))\n                if int(num) == num:\n                    timestamps = linspace(t[0], t[-1], int(num))\n                else:\n                    timestamps = arange(t[0], t[-1], raster)\n            return timestamps\n        else:\n            timestamps = t\n            if copy_master:\n                return timestamps.copy()\n            else:\n                return timestamps"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_can_signal(\n        self, name, database=None, db=None, ignore_invalidation_bits=False\n    ):\n        \"\"\" get CAN message signal. You can specify an external CAN database (\n        *database* argument) or canmatrix databse object that has already been\n        loaded from a file (*db* argument).\n\n        The signal name can be specified in the following ways\n\n        * ``CAN<ID>.<MESSAGE_NAME>.<SIGNAL_NAME>`` - the `ID` value starts from 1\n          and must match the ID found in the measurement (the source CAN bus ID)\n          Example: CAN1.Wheels.FL_WheelSpeed\n\n        * ``CAN<ID>.CAN_DataFrame_<MESSAGE_ID>.<SIGNAL_NAME>`` - the `ID` value\n          starts from 1 and the `MESSAGE_ID` is the decimal message ID as found\n          in the database. Example: CAN1.CAN_DataFrame_218.FL_WheelSpeed\n\n        * ``<MESSAGE_NAME>.SIGNAL_NAME`` - in this case the first occurence of\n          the message name and signal are returned (the same message could be\n          found on muplit CAN buses; for example on CAN1 and CAN3)\n          Example: Wheels.FL_WheelSpeed\n\n        * ``CAN_DataFrame_<MESSAGE_ID>.<SIGNAL_NAME>`` - in this case the first\n          occurence of the message name and signal are returned (the same\n          message could be found on muplit CAN buses; for example on CAN1 and\n          CAN3). Example: CAN_DataFrame_218.FL_WheelSpeed\n\n        * ``<SIGNAL_NAME>`` - in this case the first occurence of the signal\n          name is returned ( the same signal anme coudl be found in multiple\n          messages and on multiple CAN buses). Example: FL_WheelSpeed\n\n\n        Parameters\n        ----------\n        name : str\n            signal name\n        database : str\n            path of external CAN database file (.dbc or .arxml); default *None*\n        db : canmatrix.database\n            canmatrix CAN database object; default *None*\n        ignore_invalidation_bits : bool\n            option to ignore invalidation bits\n\n        Returns\n        -------\n        sig : Signal\n            Signal object with the physical values\n\n        \"\"\"\n\n        if database is None and db is None:\n            return self.get(name)\n\n        if db is None:\n\n            if not database.lower().endswith((\"dbc\", \"arxml\")):\n                message = f'Expected .dbc or .arxml file as CAN channel attachment but got \"{database}\"'\n                logger.exception(message)\n                raise MdfException(message)\n            else:\n                import_type = \"dbc\" if database.lower().endswith(\"dbc\") else \"arxml\"\n                with open(database, \"rb\") as db:\n                    db_string = db.read()\n                md5_sum = md5(db_string).digest()\n\n                if md5_sum in self._external_dbc_cache:\n                    db = self._external_dbc_cache[md5_sum]\n                else:\n                    try:\n                        db_string = db_string.decode(\"utf-8\")\n                        db = self._external_dbc_cache[md5_sum] = loads(\n                            db_string, importType=import_type, key=\"db\"\n                        )[\"db\"]\n                    except UnicodeDecodeError:\n                        try:\n                            from cchardet import detect\n\n                            encoding = detect(db_string)[\"encoding\"]\n                            db_string = db_string.decode(encoding)\n                            db = self._dbc_cache[md5_sum] = loads(\n                                db_string,\n                                importType=import_type,\n                                key=\"db\",\n                                encoding=encoding,\n                            )[\"db\"]\n                        except ImportError:\n                            message = (\n                                \"Unicode exception occured while processing the database \"\n                                f'attachment \"{database}\" and \"cChardet\" package is '\n                                'not installed. Mdf version 4 expects \"utf-8\" '\n                                \"strings and this package may detect if a different\"\n                                \" encoding was used\"\n                            )\n                            logger.warning(message)\n\n        name_ = name.split(\".\")\n\n        if len(name_) == 3:\n            can_id_str, message_id_str, signal = name_\n\n            can_id = v4c.CAN_ID_PATTERN.search(can_id_str)\n            if can_id is None:\n                raise MdfException(\n                    f'CAN id \"{can_id_str}\" of signal name \"{name}\" is not recognised by this library'\n                )\n            else:\n                can_id = f'CAN{can_id.group(\"id\")}'\n\n            message_id = v4c.CAN_DATA_FRAME_PATTERN.search(message_id_str)\n            if message_id is None:\n                message_id = message_id_str\n            else:\n                message_id = int(message_id)\n\n        elif len(name_) == 2:\n            message_id_str, signal = name_\n\n            can_id = None\n\n            message_id = v4c.CAN_DATA_FRAME_PATTERN.search(message_id_str)\n            if message_id is None:\n                message_id = message_id_str\n            else:\n                message_id = int(message_id.group('id'))\n\n        else:\n            can_id = message_id = None\n            signal = name\n\n        if isinstance(message_id, str):\n            message = db.frame_by_name(message_id)\n        else:\n            message = db.frame_by_id(message_id)\n\n        for sig in message.signals:\n            if sig.name == signal:\n                signal = sig\n                break\n        else:\n            raise MdfException(\n                f'Signal \"{signal}\" not found in message \"{message.name}\" of \"{database}\"'\n            )\n\n        if can_id is None:\n            for _can_id, messages in self.can_logging_db.items():\n                message_id = message.id\n\n                if message_id > 0x80000000:\n                    message_id -= 0x80000000\n\n                if message_id in messages:\n                    index = messages[message.id]\n                    break\n            else:\n                raise MdfException(\n                    f'Message \"{message.name}\" (ID={hex(message.id)}) not found in the measurement'\n                )\n        else:\n            if can_id in self.can_logging_db:\n                if message.is_j1939:\n                    test_ids = [\n                        canmatrix.ArbitrationId(id_, extended=True).pgn\n                        for id_ in self.can_logging_db[can_id]\n                    ]\n                    id_ = message.arbitration_id.pgn\n\n                else:\n                    id_ = message.arbitration_id.id\n                    test_ids = self.can_logging_db[can_id]\n\n                if id_ in test_ids:\n                    if message.is_j1939:\n                        for id__, idx in self.can_logging_db[can_id].items():\n                            if canmatrix.ArbitrationId(id__, extended=True).pgn == id_:\n                                index = idx\n                                break\n                    else:\n                        index = self.can_logging_db[can_id][message.arbitration_id.id]\n                else:\n                    raise MdfException(\n                        f'Message \"{message.name}\" (ID={hex(message.arbitration_id.id)}) not found in the measurement'\n                    )\n            else:\n                raise MdfException(\n                    f'No logging from \"{can_id}\" was found in the measurement'\n                )\n\n        can_ids = self.get(\n            \"CAN_DataFrame.ID\",\n            group=index,\n            ignore_invalidation_bits=ignore_invalidation_bits,\n        )\n        payload = self.get(\n            \"CAN_DataFrame.DataBytes\",\n            group=index,\n            samples_only=True,\n            ignore_invalidation_bits=ignore_invalidation_bits,\n        )[0]\n\n        if message.is_j1939:\n            ps = (can_ids.samples >> 8) & 0xFF\n            pf = (can_ids.samples >> 16) & 0xFF\n            _pgn = pf << 8\n            _pgn = where(\n                pf >= 240,\n                _pgn + ps,\n                _pgn,\n            )\n\n            idx = nonzero(_pgn == message.arbitration_id.pgn)[0]\n        else:\n            idx = nonzero(can_ids.samples == message.arbitration_id.id)[0]\n\n        vals = payload[idx]\n        t = can_ids.timestamps[idx].copy()\n        if can_ids.invalidation_bits is not None:\n            invalidation_bits = can_ids.invalidation_bits[idx]\n        else:\n            invalidation_bits = None\n\n        vals = extract_can_signal(signal, vals)\n\n        comment = signal.comment or \"\"\n\n        if ignore_invalidation_bits:\n\n            return Signal(\n                samples=vals,\n                timestamps=t,\n                name=name,\n                unit=signal.unit or \"\",\n                comment=comment,\n                invalidation_bits=invalidation_bits,\n            )\n\n        else:\n\n            if invalidation_bits is not None:\n                vals = vals[nonzero(~invalidation_bits)[0]]\n                t = t[nonzero(~invalidation_bits)[0]]\n\n            return Signal(\n                samples=vals,\n                timestamps=t,\n                name=name,\n                unit=signal.unit or \"\",\n                comment=comment,\n            )", "response": "get a CAN message signal from the specified name database and db."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef info(self):\n        info = {}\n        info[\"version\"] = (\n            self.identification[\"version_str\"].decode(\"utf-8\").strip(\" \\n\\t\\0\")\n        )\n        info[\"groups\"] = len(self.groups)\n        for i, gp in enumerate(self.groups):\n            inf = {}\n            info[f\"group {i}\"] = inf\n            inf[\"cycles\"] = gp.channel_group.cycles_nr\n            inf[\"comment\"] = gp.channel_group.comment\n            inf[\"channels count\"] = len(gp.channels)\n            for j, channel in enumerate(gp.channels):\n                name = channel.name\n\n                ch_type = v4c.CHANNEL_TYPE_TO_DESCRIPTION[channel.channel_type]\n                inf[f\"channel {j}\"] = f'name=\"{name}\" type={ch_type}'\n\n        return info", "response": "get MDF information as a dict\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the MDF to dst.", "response": "def save(self, dst, overwrite=False, compression=0):\n        \"\"\"Save MDF to *dst*. If overwrite is *True* then the destination file\n        is overwritten, otherwise the file name is appened with '.<cntr>', were\n        '<cntr>' is the first conter that produces a new file name\n        (that does not already exist in the filesystem)\n\n        Parameters\n        ----------\n        dst : str\n            destination file name, Default ''\n        overwrite : bool\n            overwrite flag, default *False*\n        compression : int\n            use compressed data blocks, default 0; valid since version 4.10\n\n            * 0 - no compression\n            * 1 - deflate (slower, but produces smaller files)\n            * 2 - transposition + deflate (slowest, but produces\n              the smallest files)\n\n        Returns\n        -------\n        output_file : pathlib.Path\n            path to saved file\n\n        \"\"\"\n\n        if is_file_like(dst):\n            dst_ = dst\n            file_like = True\n            dst = Path(\"__file_like.mf4\")\n            dst_.seek(0)\n        else:\n            file_like = False\n            dst = Path(dst).with_suffix(\".mf4\")\n\n            destination_dir = dst.parent\n            destination_dir.mkdir(parents=True, exist_ok=True)\n\n            if overwrite is False:\n                if dst.is_file():\n                    cntr = 0\n                    while True:\n                        name = dst.with_suffix(f\".{cntr}.mf4\")\n                        if not name.exists():\n                            break\n                        else:\n                            cntr += 1\n                    message = (\n                        f'Destination file \"{dst}\" already exists '\n                        f'and \"overwrite\" is False. Saving MDF file as \"{name}\"'\n                    )\n                    logger.warning(message)\n                    dst = name\n\n            if dst == self.name:\n                destination = dst.with_suffix(\".savetemp\")\n            else:\n                destination = dst\n\n            dst_ = open(destination, \"wb+\")\n\n        if not self.file_history:\n            comment = \"created\"\n        else:\n            comment = \"updated\"\n\n        fh = FileHistory()\n        fh.comment = f\"\"\"<FHcomment>\n<TX>{comment}</TX>\n<tool_id>asammdf</tool_id>\n<tool_vendor>asammdf</tool_vendor>\n<tool_version>{__version__}</tool_version>\n</FHcomment>\"\"\"\n\n        self.file_history.append(fh)\n\n        try:\n            defined_texts = {}\n            cc_map = {}\n            si_map = {}\n\n            groups_nr = len(self.groups)\n\n            write = dst_.write\n            tell = dst_.tell\n            seek = dst_.seek\n\n            blocks = []\n\n            write(bytes(self.identification))\n\n            self.header.to_blocks(dst_.tell(), blocks)\n            for block in blocks:\n                write(bytes(block))\n\n            original_data_addresses = []\n\n            if compression == 1:\n                zip_type = v4c.FLAG_DZ_DEFLATE\n            else:\n                zip_type = v4c.FLAG_DZ_TRANPOSED_DEFLATE\n\n            # write DataBlocks first\n            for gp_nr, gp in enumerate(self.groups):\n                original_data_addresses.append(gp.data_group.data_block_addr)\n\n                if gp.channel_group.flags & v4c.FLAG_CG_VLSD:\n                    continue\n\n                address = tell()\n\n                total_size = (\n                    gp.channel_group.samples_byte_nr\n                    + gp.channel_group.invalidation_bytes_nr\n                ) * gp.channel_group.cycles_nr\n\n                if total_size:\n\n                    if self._write_fragment_size:\n\n                        samples_size = (\n                            gp.channel_group.samples_byte_nr\n                            + gp.channel_group.invalidation_bytes_nr\n                        )\n                        if samples_size:\n                            split_size = self._write_fragment_size // samples_size\n                            split_size *= samples_size\n                            if split_size == 0:\n                                split_size = samples_size\n                            chunks = float(total_size) / split_size\n                            chunks = int(ceil(chunks))\n                        else:\n                            chunks = 1\n                    else:\n                        chunks = 1\n\n                    self.configure(read_fragment_size=split_size)\n                    data = self._load_data(gp)\n\n                    if chunks == 1:\n                        data_ = next(data)[0]\n                        if compression and self.version > \"4.00\":\n                            if compression == 1:\n                                param = 0\n                            else:\n                                param = (\n                                    gp.channel_group.samples_byte_nr\n                                    + gp.channel_group.invalidation_bytes_nr\n                                )\n                            kwargs = {\"data\": data_, \"zip_type\": zip_type, \"param\": param}\n                            data_block = DataZippedBlock(**kwargs)\n                        else:\n                            data_block = DataBlock(data=data_)\n                        write(bytes(data_block))\n\n                        align = data_block.block_len % 8\n                        if align:\n                            write(b\"\\0\" * (8 - align))\n\n                        if gp.channel_group.cycles_nr:\n                            gp.data_group.data_block_addr = address\n                        else:\n                            gp.data_group.data_block_addr = 0\n                    else:\n                        kwargs = {\"flags\": v4c.FLAG_DL_EQUAL_LENGHT, \"zip_type\": zip_type}\n                        hl_block = HeaderList(**kwargs)\n\n                        kwargs = {\n                            \"flags\": v4c.FLAG_DL_EQUAL_LENGHT,\n                            \"links_nr\": chunks + 1,\n                            \"data_block_nr\": chunks,\n                            \"data_block_len\": split_size,\n                        }\n                        dl_block = DataList(**kwargs)\n\n                        for i, data__ in enumerate(data):\n                            data_ = data__[0]\n\n                            if compression and self.version > \"4.00\":\n                                if compression == 1:\n                                    zip_type = v4c.FLAG_DZ_DEFLATE\n                                else:\n                                    zip_type = v4c.FLAG_DZ_TRANPOSED_DEFLATE\n                                if compression == 1:\n                                    param = 0\n                                else:\n                                    param = (\n                                        gp.channel_group.samples_byte_nr\n                                        + gp.channel_group.invalidation_bytes_nr\n                                    )\n                                kwargs = {\n                                    \"data\": data_,\n                                    \"zip_type\": zip_type,\n                                    \"param\": param,\n                                }\n                                block = DataZippedBlock(**kwargs)\n                            else:\n                                block = DataBlock(data=data_)\n                            address = tell()\n                            block.address = address\n\n                            write(bytes(block))\n\n                            align = block.block_len % 8\n                            if align:\n                                write(b\"\\0\" * (8 - align))\n                            dl_block[f\"data_block_addr{i}\"] = address\n\n                        address = tell()\n                        dl_block.address = address\n                        write(bytes(dl_block))\n\n                        if compression and self.version != \"4.00\":\n                            hl_block.first_dl_addr = address\n                            address = tell()\n                            hl_block.address = address\n                            write(bytes(hl_block))\n\n                        gp.data_group.data_block_addr = address\n                else:\n                    gp.data_group.data_block_addr = 0\n\n                if self._callback:\n                    self._callback(int(50 * (gp_nr + 1) / groups_nr), 100)\n                if self._terminate:\n                    dst_.close()\n                    self.close()\n                    return\n\n            address = tell()\n\n            blocks = []\n\n            # attachments\n            at_map = {}\n            if self.attachments:\n                for at_block in self.attachments:\n                    address = at_block.to_blocks(address, blocks, defined_texts)\n\n                for i in range(len(self.attachments) - 1):\n                    at_block = self.attachments[i]\n                    at_block.next_at_addr = self.attachments[i + 1].address\n                self.attachments[-1].next_at_addr = 0\n\n            # file history blocks\n            for fh in self.file_history:\n                address = fh.to_blocks(address, blocks, defined_texts)\n\n            for i, fh in enumerate(self.file_history[:-1]):\n                fh.next_fh_addr = self.file_history[i + 1].address\n            self.file_history[-1].next_fh_addr = 0\n\n            # data groups\n            gp_rec_ids = []\n            valid_data_groups = []\n            for gp in self.groups:\n                if gp.channel_group.flags & v4c.FLAG_CG_VLSD:\n                    continue\n\n                valid_data_groups.append(gp.data_group)\n                gp_rec_ids.append(gp.data_group.record_id_len)\n\n                address = gp.data_group.to_blocks(address, blocks, defined_texts)\n\n            if valid_data_groups:\n                for i, dg in enumerate(valid_data_groups[:-1]):\n                    addr_ = valid_data_groups[i + 1].address\n                    dg.next_dg_addr = addr_\n                valid_data_groups[-1].next_dg_addr = 0\n\n            # go through each data group and append the rest of the blocks\n            for i, gp in enumerate(self.groups):\n\n                channels = gp.channels\n\n                for j, channel in enumerate(channels):\n                    if channel.attachment is not None:\n                        channel.attachment_addr = self.attachments[channel.attachment].address\n\n                    address = channel.to_blocks(\n                        address, blocks, defined_texts, cc_map, si_map\n                    )\n\n                    if channel.channel_type == v4c.CHANNEL_TYPE_SYNC:\n                        idx = self._attachments_map[channel.data_block_addr]\n                        channel.data_block_addr = self.attachments[idx].address\n                    else:\n                        sdata = self._load_signal_data(group=gp, index=j)\n                        if sdata:\n                            split_size = self._write_fragment_size\n                            if self._write_fragment_size:\n                                chunks = float(len(sdata)) / split_size\n                                chunks = int(ceil(chunks))\n                            else:\n                                chunks = 1\n\n                            if chunks == 1:\n                                if compression and self.version > \"4.00\":\n                                    signal_data = DataZippedBlock(\n                                        data=sdata,\n                                        zip_type=v4c.FLAG_DZ_DEFLATE,\n                                        original_type=b\"SD\",\n                                    )\n                                    signal_data.address = address\n                                    address += signal_data.block_len\n                                    blocks.append(signal_data)\n                                    align = signal_data.block_len % 8\n                                    if align:\n                                        blocks.append(b\"\\0\" * (8 - align))\n                                        address += 8 - align\n                                else:\n                                    signal_data = DataBlock(data=sdata, type=\"SD\")\n                                    signal_data.address = address\n                                    address += signal_data.block_len\n                                    blocks.append(signal_data)\n                                    align = signal_data.block_len % 8\n                                    if align:\n                                        blocks.append(b\"\\0\" * (8 - align))\n                                        address += 8 - align\n\n                                channel.data_block_addr = signal_data.address\n                            else:\n                                kwargs = {\n                                    \"flags\": v4c.FLAG_DL_EQUAL_LENGHT,\n                                    \"links_nr\": chunks + 1,\n                                    \"data_block_nr\": chunks,\n                                    \"data_block_len\": self._write_fragment_size,\n                                }\n                                dl_block = DataList(**kwargs)\n\n                                for k in range(chunks):\n\n                                    data_ = sdata[k * split_size : (k + 1) * split_size]\n                                    if compression and self.version > \"4.00\":\n                                        zip_type = v4c.FLAG_DZ_DEFLATE\n                                        param = 0\n\n                                        kwargs = {\n                                            \"data\": data_,\n                                            \"zip_type\": zip_type,\n                                            \"param\": param,\n                                            \"original_type\": b\"SD\",\n                                        }\n                                        block = DataZippedBlock(**kwargs)\n                                    else:\n                                        block = DataBlock(data=data_, type=\"SD\")\n                                    blocks.append(block)\n                                    block.address = address\n                                    address += block.block_len\n\n                                    align = block.block_len % 8\n                                    if align:\n                                        blocks.append(b\"\\0\" * (8 - align))\n                                        address += 8 - align\n                                    dl_block[f\"data_block_addr{k}\"] = block.address\n\n                                dl_block.address = address\n                                blocks.append(dl_block)\n\n                                address += dl_block.block_len\n\n                                if compression and self.version > \"4.00\":\n                                    kwargs = {\n                                        \"flags\": v4c.FLAG_DL_EQUAL_LENGHT,\n                                        \"zip_type\": v4c.FLAG_DZ_DEFLATE,\n                                        \"first_dl_addr\": dl_block.address,\n                                    }\n                                    hl_block = HeaderList(**kwargs)\n                                    hl_block.address = address\n                                    address += hl_block.block_len\n\n                                    blocks.append(hl_block)\n\n                                    channel.data_block_addr = hl_block.address\n                                else:\n                                    channel.data_block_addr = dl_block.address\n\n                        else:\n                            channel.data_block_addr = 0\n\n                    dep_list = gp.channel_dependencies[j]\n                    if dep_list:\n                        if all(isinstance(dep, ChannelArrayBlock) for dep in dep_list):\n                            for dep in dep_list:\n                                dep.address = address\n                                address += dep.block_len\n                                blocks.append(dep)\n                            for k, dep in enumerate(dep_list[:-1]):\n                                dep.composition_addr = dep_list[k + 1].address\n                            dep_list[-1].composition_addr = 0\n\n                            channel.component_addr = dep_list[0].address\n\n                        else:\n                            index = dep_list[0][1]\n                            addr_ = gp.channels[index].address\n\n                for channel in gp.logging_channels:\n                    address = channel.to_blocks(\n                        address, blocks, defined_texts, cc_map, si_map\n                    )\n\n                group_channels = list(chain(gp.channels, gp.logging_channels))\n                if group_channels:\n                    for j, channel in enumerate(group_channels[:-1]):\n                        channel.next_ch_addr = group_channels[j + 1].address\n                    group_channels[-1].next_ch_addr = 0\n\n                # channel dependecies\n                j = len(channels) - 1\n                while j >= 0:\n                    dep_list = gp.channel_dependencies[j]\n                    if dep_list and all(isinstance(dep, tuple) for dep in dep_list):\n                        index = dep_list[0][1]\n                        channels[j].component_addr = channels[index].address\n                        index = dep_list[-1][1]\n                        channels[j].next_ch_addr = channels[index].next_ch_addr\n                        channels[index].next_ch_addr = 0\n\n                        for _, ch_nr in dep_list:\n                            channels[ch_nr].source_addr = 0\n                    j -= 1\n\n                # channel group\n                if gp.channel_group.flags & v4c.FLAG_CG_VLSD:\n                    continue\n\n                gp.channel_group.first_sample_reduction_addr = 0\n\n                if channels:\n                    gp.channel_group.first_ch_addr = gp.channels[0].address\n                else:\n                    gp.channel_group.first_ch_addr = 0\n                gp.channel_group.next_cg_addr = 0\n\n                address = gp.channel_group.to_blocks(\n                    address, blocks, defined_texts, si_map\n                )\n                gp.data_group.first_cg_addr = gp.channel_group.address\n\n                if self._callback:\n                    self._callback(int(50 * (i + 1) / groups_nr) + 25, 100)\n                if self._terminate:\n                    dst_.close()\n                    self.close()\n                    return\n\n            for gp in self.groups:\n                for dep_list in gp.channel_dependencies:\n                    if dep_list:\n                        if all(isinstance(dep, ChannelArrayBlock) for dep in dep_list):\n                            for dep in dep_list:\n                                if dep.ca_type != v4c.CA_TYPE_LOOKUP:\n                                    dep.referenced_channels = []\n                                    continue\n                                for i, (gp_nr, ch_nr) in enumerate(\n                                    dep.referenced_channels\n                                ):\n                                    grp = self.groups[gp_nr]\n                                    ch = grp.channels[ch_nr]\n                                    dep[\n                                        f\"scale_axis_{i}_dg_addr\"\n                                    ] = grp.data_group.address\n                                    dep[\n                                        f\"scale_axis_{i}_cg_addr\"\n                                    ] = grp.channel_group.address\n                                    dep[f\"scale_axis_{i}_ch_addr\"] = ch.address\n\n            for gp in self.groups:\n                gp.data_group.record_id_len = 0\n\n            ev_map = []\n\n            if self.events:\n                for event in self.events:\n                    for i, ref in enumerate(event.scopes):\n                        try:\n                            dg_cntr, ch_cntr = ref\n                            event[f\"scope_{i}_addr\"] = (\n                                self.groups[dg_cntr].channels[ch_cntr].address\n                            )\n                        except TypeError:\n                            dg_cntr = ref\n                            event[f\"scope_{i}_addr\"] = self.groups[\n                                dg_cntr\n                            ].channel_group.address\n                    for i in range(event.attachment_nr):\n                        key = f\"attachment_{i}_addr\"\n                        addr = event[key]\n                        event[key] = at_map[addr]\n\n                    blocks.append(event)\n                    ev_map.append(address)\n                    event.address = address\n                    address += event.block_len\n\n                    if event.name:\n                        tx_block = TextBlock(text=event.name)\n                        tx_block.address = address\n                        blocks.append(tx_block)\n                        address += tx_block.block_len\n                        event.name_addr = tx_block.address\n                    else:\n                        event.name_addr = 0\n\n                    if event.comment:\n                        meta = event.comment.startswith(\"<EVcomment\")\n                        tx_block = TextBlock(text=event.comment, meta=meta)\n                        tx_block.address = address\n                        blocks.append(tx_block)\n                        address += tx_block.block_len\n                        event.comment_addr = tx_block.address\n                    else:\n                        event.comment_addr = 0\n\n                    if event.parent is not None:\n                        event.parent_ev_addr = ev_map[event.parent]\n                    if event.range_start is not None:\n                        event.range_start_ev_addr = ev_map[event.range_start]\n\n                for i in range(len(self.events) - 1):\n                    self.events[i].next_ev_addr = self.events[i + 1].address\n                self.events[-1].next_ev_addr = 0\n\n                self.header.first_event_addr = self.events[0].address\n\n            if self._terminate:\n                dst_.close()\n                self.close()\n                return\n\n            if self._callback:\n                blocks_nr = len(blocks)\n                threshold = blocks_nr / 25\n                count = 1\n                for i, block in enumerate(blocks):\n                    write(bytes(block))\n                    if i >= threshold:\n                        self._callback(75 + count, 100)\n                        count += 1\n                        threshold += blocks_nr / 25\n            else:\n                for block in blocks:\n                    write(bytes(block))\n\n            for gp, rec_id in zip(self.groups, gp_rec_ids):\n                gp.data_group.record_id_len = rec_id\n\n            if valid_data_groups:\n                addr_ = valid_data_groups[0].address\n                self.header.first_dg_addr = addr_\n            else:\n                self.header.first_dg_addr = 0\n            self.header.file_history_addr = self.file_history[0].address\n            if self.attachments:\n                first_attachment = self.attachments[0]\n                addr_ = first_attachment.address\n                self.header.first_attachment_addr = addr_\n            else:\n                self.header.first_attachment_addr = 0\n\n            seek(v4c.IDENTIFICATION_BLOCK_SIZE)\n            write(bytes(self.header))\n\n            for orig_addr, gp in zip(original_data_addresses, self.groups):\n                gp.data_group.data_block_addr = orig_addr\n\n            at_map = {value: key for key, value in at_map.items()}\n\n            for event in self.events:\n                for i in range(event.attachment_nr):\n                    key = f\"attachment_{i}_addr\"\n                    addr = event[key]\n                    event[key] = at_map[addr]\n\n        except:\n            raise\n            if not file_like:\n                dst_.close()\n\n        if not file_like:\n            dst_.close()\n\n        if dst == self.name:\n            self.close()\n            Path.unlink(self.name)\n            Path.rename(destination, self.name)\n\n            self.groups.clear()\n            self.header = None\n            self.identification = None\n            self.file_history.clear()\n            self.channels_db.clear()\n            self.masters_db.clear()\n            self.attachments.clear()\n            self.file_comment = None\n\n            self._ch_map.clear()\n            self._master_channel_cache.clear()\n\n            self._tempfile = TemporaryFile()\n            self._file = open(self.name, \"rb\")\n            self._read()\n\n        if self._callback:\n            self._callback(100, 100)\n\n        return dst"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_channel_name(self, group, index):\n        gp_nr, ch_nr = self._validate_channel_selection(None, group, index)\n\n        return self.groups[gp_nr].channels[ch_nr].name", "response": "Gets the name of a channel in a group."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_channel_unit(self, name=None, group=None, index=None):\n        gp_nr, ch_nr = self._validate_channel_selection(name, group, index)\n\n        grp = self.groups[gp_nr]\n\n        channel = grp.channels[ch_nr]\n\n        conversion = channel.conversion\n\n        unit = conversion and conversion.unit or channel.unit or \"\"\n\n        return unit", "response": "Gets the channel unit from the specified name group and index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_channel_comment(self, name=None, group=None, index=None):\n        gp_nr, ch_nr = self._validate_channel_selection(name, group, index)\n\n        grp = self.groups[gp_nr]\n\n        channel = grp.channels[ch_nr]\n\n        return extract_cncomment_xml(channel.comment)", "response": "Gets the comment of a specific channel in a specific group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a command line parser. It is used when generating the documentation", "response": "def _cmd_line_parser():\n    '''\n    return a command line parser. It is used when generating the documentation\n    '''\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--path',\n                        help=('path to test files, '\n                              'if not provided the script folder is used'))\n    parser.add_argument('--text_output',\n                        action='store_true',\n                        help='option to save the results to text file')\n    parser.add_argument('--format',\n                        default='rst',\n                        nargs='?',\n                        choices=['rst', 'md'],\n                        help='text formatting')\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nappends item and print it to stdout", "response": "def append(self, item):\n        \"\"\" append item and print it to stdout \"\"\"\n        print(item)\n        super(MyList, self).append(item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extend(self, items):\n        print('\\n'.join(items))\n        super(MyList, self).extend(items)", "response": "extend items and print them to stdout using the new line separator\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot(self):\n        try:\n            import pyqtgraph\n            from .gui.utils import COLORS\n            from .gui.widgets.plot_standalone import StandalonePlot\n\n            from PyQt5.QtWidgets import QApplication\n\n            app = QApplication([])\n\n            plot = StandalonePlot([self], True, False)\n\n            name = self.name\n\n            if self.comment:\n                comment = self.comment.replace(\"$\", \"\")\n                comment = extract_cncomment_xml(comment)\n                comment = fill(comment, 120).replace(\"\\\\n\", \" \")\n\n                title = f\"{name}\\n({comment})\"\n                plot.plot.plot.plotItem.setTitle(title, color=COLORS[0])\n            else:\n                plot.plot.plot.plotItem.setTitle(name, color=COLORS[0])\n\n            plot.show()\n            plot.setWindowTitle(f\"{self.name} - asammdf{__version__}\")\n\n            app.exec_()\n            return\n\n        except:\n            try:\n                import matplotlib.pyplot as plt\n                from mpl_toolkits.mplot3d import axes3d\n                from matplotlib.widgets import Slider\n            except ImportError:\n                logging.warning(\"Signal plotting requires pyqtgraph or matplotlib\")\n                return\n\n        if len(self.samples.shape) <= 1 and self.samples.dtype.names is None:\n            fig = plt.figure()\n            fig.canvas.set_window_title(self.name)\n            fig.text(\n                0.95,\n                0.05,\n                f\"asammdf {__version__}\",\n                fontsize=8,\n                color=\"red\",\n                ha=\"right\",\n                va=\"top\",\n                alpha=0.5,\n            )\n\n            name = self.name\n\n            if self.comment:\n                comment = self.comment.replace(\"$\", \"\")\n                comment = extract_cncomment_xml(comment)\n                comment = fill(comment, 120).replace(\"\\\\n\", \" \")\n\n                title = f\"{name}\\n({comment})\"\n                plt.title(title)\n            else:\n                plt.title(name)\n            try:\n                if not self.master_metadata:\n                    plt.xlabel(\"Time [s]\")\n                    plt.ylabel(f\"[{self.unit}]\")\n                    plt.plot(self.timestamps, self.samples, \"b\")\n                    plt.plot(self.timestamps, self.samples, \"b.\")\n                    plt.grid(True)\n                    plt.show()\n                else:\n                    master_name, sync_type = self.master_metadata\n                    if sync_type in (0, 1):\n                        plt.xlabel(f\"{master_name} [s]\")\n                    elif sync_type == 2:\n                        plt.xlabel(f\"{master_name} [deg]\")\n                    elif sync_type == 3:\n                        plt.xlabel(f\"{master_name} [m]\")\n                    elif sync_type == 4:\n                        plt.xlabel(f\"{master_name} [index]\")\n                    plt.ylabel(f\"[{self.unit}]\")\n                    plt.plot(self.timestamps, self.samples, \"b\")\n                    plt.plot(self.timestamps, self.samples, \"b.\")\n                    plt.grid(True)\n                    plt.show()\n            except ValueError:\n                plt.close(fig)\n        else:\n            try:\n                names = self.samples.dtype.names\n                if self.samples.dtype.names is None or len(names) == 1:\n\n                    if names:\n                        samples = self.samples[names[0]]\n                    else:\n                        samples = self.samples\n\n                    shape = samples.shape[1:]\n\n                    fig = plt.figure()\n                    fig.canvas.set_window_title(self.name)\n                    fig.text(\n                        0.95,\n                        0.05,\n                        f\"asammdf {__version__}\",\n                        fontsize=8,\n                        color=\"red\",\n                        ha=\"right\",\n                        va=\"top\",\n                        alpha=0.5,\n                    )\n\n                    if self.comment:\n                        comment = self.comment.replace(\"$\", \"\")\n                        plt.title(f\"{self.name}\\n({comment})\")\n                    else:\n                        plt.title(self.name)\n\n                    ax = fig.add_subplot(111, projection=\"3d\")\n\n                    # Grab some test data.\n                    X = np.array(range(shape[1]))\n                    Y = np.array(range(shape[0]))\n                    X, Y = np.meshgrid(X, Y)\n\n                    Z = samples[0]\n\n                    # Plot a basic wireframe.\n                    self._plot_axis = ax.plot_wireframe(X, Y, Z, rstride=1, cstride=1)\n\n                    # Place Sliders on Graph\n                    ax_a = plt.axes([0.25, 0.1, 0.65, 0.03])\n\n                    # Create Sliders & Determine Range\n                    sa = Slider(\n                        ax_a,\n                        \"Time [s]\",\n                        self.timestamps[0],\n                        self.timestamps[-1],\n                        valinit=self.timestamps[0],\n                    )\n\n                    def update(val):\n                        self._plot_axis.remove()\n                        idx = np.searchsorted(self.timestamps, sa.val, side=\"right\")\n                        Z = samples[idx - 1]\n                        self._plot_axis = ax.plot_wireframe(\n                            X, Y, Z, rstride=1, cstride=1\n                        )\n                        fig.canvas.draw_idle()\n\n                    sa.on_changed(update)\n\n                    plt.show()\n\n                else:\n                    fig = plt.figure()\n                    fig.canvas.set_window_title(self.name)\n                    fig.text(\n                        0.95,\n                        0.05,\n                        f\"asammdf {__version__}\",\n                        fontsize=8,\n                        color=\"red\",\n                        ha=\"right\",\n                        va=\"top\",\n                        alpha=0.5,\n                    )\n\n                    if self.comment:\n                        comment = self.comment.replace(\"$\", \"\")\n                        plt.title(f\"{self.name}\\n({comment})\")\n                    else:\n                        plt.title(self.name)\n\n                    ax = fig.add_subplot(111, projection=\"3d\")\n\n                    samples = self.samples[names[0]]\n                    axis1 = self.samples[names[1]]\n                    axis2 = self.samples[names[2]]\n\n                    # Grab some test data.\n                    X, Y = np.meshgrid(axis2[0], axis1[0])\n\n                    Z = samples[0]\n\n                    # Plot a basic wireframe.\n                    self._plot_axis = ax.plot_wireframe(X, Y, Z, rstride=1, cstride=1)\n\n                    # Place Sliders on Graph\n                    ax_a = plt.axes([0.25, 0.1, 0.65, 0.03])\n\n                    # Create Sliders & Determine Range\n                    sa = Slider(\n                        ax_a,\n                        \"Time [s]\",\n                        self.timestamps[0],\n                        self.timestamps[-1],\n                        valinit=self.timestamps[0],\n                    )\n\n                    def update(val):\n                        self._plot_axis.remove()\n                        idx = np.searchsorted(self.timestamps, sa.val, side=\"right\")\n                        Z = samples[idx - 1]\n                        X, Y = np.meshgrid(axis2[idx - 1], axis1[idx - 1])\n                        self._plot_axis = ax.plot_wireframe(\n                            X, Y, Z, rstride=1, cstride=1\n                        )\n                        fig.canvas.draw_idle()\n\n                    sa.on_changed(update)\n\n                    plt.show()\n\n            except Exception as err:\n                print(err)", "response": "Plot the signal samples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new Signal object that is cut from the original signal.", "response": "def cut(self, start=None, stop=None, include_ends=True, interpolation_mode=0):\n        \"\"\"\n        Cuts the signal according to the *start* and *stop* values, by using\n        the insertion indexes in the signal's *time* axis.\n\n        Parameters\n        ----------\n        start : float\n            start timestamp for cutting\n        stop : float\n            stop timestamp for cutting\n        include_ends : bool\n            include the *start* and *stop* timestamps after cutting the signal.\n            If *start* and *stop* are found in the original timestamps, then\n            the new samples will be computed using interpolation. Default *True*\n        interpolation_mode : int\n            interpolation mode for integer signals; default 0\n\n                * 0 - repeat previous samples\n                * 1 - linear interpolation\n\n        Returns\n        -------\n        result : Signal\n            new *Signal* cut from the original\n\n        Examples\n        --------\n        >>> new_sig = old_sig.cut(1.0, 10.5)\n        >>> new_sig.timestamps[0], new_sig.timestamps[-1]\n        0.98, 10.48\n\n        \"\"\"\n        ends = (start, stop)\n        if len(self) == 0:\n            result = Signal(\n                np.array([], dtype=self.samples.dtype),\n                np.array([], dtype=self.timestamps.dtype),\n                self.unit,\n                self.name,\n                self.conversion,\n                self.comment,\n                self.raw,\n                self.master_metadata,\n                self.display_name,\n                self.attachment,\n                self.source,\n                self.bit_count,\n                self.stream_sync,\n                encoding=self.encoding,\n            )\n\n        elif start is None and stop is None:\n            # return the channel uncut\n            result = Signal(\n                self.samples.copy(),\n                self.timestamps.copy(),\n                self.unit,\n                self.name,\n                self.conversion,\n                self.comment,\n                self.raw,\n                self.master_metadata,\n                self.display_name,\n                self.attachment,\n                self.source,\n                self.bit_count,\n                self.stream_sync,\n                invalidation_bits=self.invalidation_bits.copy()\n                if self.invalidation_bits is not None\n                else None,\n                encoding=self.encoding,\n            )\n\n        else:\n            if start is None:\n                # cut from begining to stop\n                if stop < self.timestamps[0]:\n                    result = Signal(\n                        np.array([], dtype=self.samples.dtype),\n                        np.array([], dtype=self.timestamps.dtype),\n                        self.unit,\n                        self.name,\n                        self.conversion,\n                        self.comment,\n                        self.raw,\n                        self.master_metadata,\n                        self.display_name,\n                        self.attachment,\n                        self.source,\n                        self.bit_count,\n                        self.stream_sync,\n                        encoding=self.encoding,\n                    )\n\n                else:\n                    stop = np.searchsorted(self.timestamps, stop, side=\"right\")\n                    if (\n                        include_ends\n                        and ends[-1] not in self.timestamps\n                        and ends[-1] < self.timestamps[-1]\n                    ):\n                        interpolated = self.interp([ends[1]], interpolation_mode=interpolation_mode)\n                        samples = np.append(\n                            self.samples[:stop], interpolated.samples, axis=0\n                        )\n                        timestamps = np.append(self.timestamps[:stop], ends[1])\n                        if self.invalidation_bits is not None:\n                            invalidation_bits = np.append(\n                                self.invalidation_bits[:stop],\n                                interpolated.invalidation_bits,\n                            )\n                        else:\n                            invalidation_bits = None\n                    else:\n                        samples = self.samples[:stop].copy()\n                        timestamps = self.timestamps[:stop].copy()\n                        if self.invalidation_bits is not None:\n                            invalidation_bits = self.invalidation_bits[:stop].copy()\n                        else:\n                            invalidation_bits = None\n                    result = Signal(\n                        samples,\n                        timestamps,\n                        self.unit,\n                        self.name,\n                        self.conversion,\n                        self.comment,\n                        self.raw,\n                        self.master_metadata,\n                        self.display_name,\n                        self.attachment,\n                        self.source,\n                        self.bit_count,\n                        self.stream_sync,\n                        invalidation_bits=invalidation_bits,\n                        encoding=self.encoding,\n                    )\n\n            elif stop is None:\n                # cut from start to end\n                if start > self.timestamps[-1]:\n                    result = Signal(\n                        np.array([], dtype=self.samples.dtype),\n                        np.array([], dtype=self.timestamps.dtype),\n                        self.unit,\n                        self.name,\n                        self.conversion,\n                        self.comment,\n                        self.raw,\n                        self.master_metadata,\n                        self.display_name,\n                        self.attachment,\n                        self.source,\n                        self.bit_count,\n                        self.stream_sync,\n                        encoding=self.encoding,\n                    )\n\n                else:\n                    start = np.searchsorted(self.timestamps, start, side=\"left\")\n                    if (\n                        include_ends\n                        and ends[0] not in self.timestamps\n                        and ends[0] > self.timestamps[0]\n                    ):\n                        interpolated = self.interp([ends[0]], interpolation_mode=interpolation_mode)\n                        samples = np.append(\n                            interpolated.samples, self.samples[start:], axis=0\n                        )\n                        timestamps = np.append(ends[0], self.timestamps[start:])\n                        if self.invalidation_bits is not None:\n                            invalidation_bits = np.append(\n                                interpolated.invalidation_bits,\n                                self.invalidation_bits[start:],\n                            )\n                        else:\n                            invalidation_bits = None\n                    else:\n                        samples = self.samples[start:].copy()\n                        timestamps = self.timestamps[start:].copy()\n                        if self.invalidation_bits is not None:\n                            invalidation_bits = self.invalidation_bits[start:].copy()\n                        else:\n                            invalidation_bits = None\n                    result = Signal(\n                        samples,\n                        timestamps,\n                        self.unit,\n                        self.name,\n                        self.conversion,\n                        self.comment,\n                        self.raw,\n                        self.master_metadata,\n                        self.display_name,\n                        self.attachment,\n                        self.source,\n                        self.bit_count,\n                        self.stream_sync,\n                        invalidation_bits=invalidation_bits,\n                        encoding=self.encoding,\n                    )\n\n            else:\n                # cut between start and stop\n                if start > self.timestamps[-1] or stop < self.timestamps[0]:\n                    result = Signal(\n                        np.array([], dtype=self.samples.dtype),\n                        np.array([], dtype=self.timestamps.dtype),\n                        self.unit,\n                        self.name,\n                        self.conversion,\n                        self.comment,\n                        self.raw,\n                        self.master_metadata,\n                        self.display_name,\n                        self.attachment,\n                        self.source,\n                        self.bit_count,\n                        self.stream_sync,\n                        encoding=self.encoding,\n                    )\n                else:\n                    start = np.searchsorted(self.timestamps, start, side=\"left\")\n                    stop = np.searchsorted(self.timestamps, stop, side=\"right\")\n\n                    if start == stop:\n                        if include_ends:\n                            interpolated = self.interp(np.unique(ends), interpolation_mode=interpolation_mode)\n                            samples = interpolated.samples\n                            timestamps = np.array(\n                                np.unique(ends), dtype=self.timestamps.dtype\n                            )\n                            invalidation_bits = interpolated.invalidation_bits\n                        else:\n                            samples = np.array([], dtype=self.samples.dtype)\n                            timestamps = np.array([], dtype=self.timestamps.dtype)\n                            if self.invalidation_bits is not None:\n                                invalidation_bits = np.array([], dtype=bool)\n                            else:\n                                invalidation_bits = None\n                    else:\n                        samples = self.samples[start:stop].copy()\n                        timestamps = self.timestamps[start:stop].copy()\n                        if self.invalidation_bits:\n                            invalidation_bits = self.invalidation_bits[\n                                start:stop\n                            ].copy()\n                        else:\n                            invalidation_bits = None\n\n                        if (\n                            include_ends\n                            and ends[-1] not in self.timestamps\n                            and ends[-1] < self.timestamps[-1]\n                        ):\n                            interpolated = self.interp([ends[1]], interpolation_mode=interpolation_mode)\n                            samples = np.append(samples, interpolated.samples, axis=0)\n                            timestamps = np.append(timestamps, ends[1])\n                            if invalidation_bits is not None:\n                                invalidation_bits = np.append(\n                                    invalidation_bits, interpolated.invalidation_bits\n                                )\n\n                        if (\n                            include_ends\n                            and ends[0] not in self.timestamps\n                            and ends[0] > self.timestamps[0]\n                        ):\n                            interpolated = self.interp([ends[0]], interpolation_mode=interpolation_mode)\n                            samples = np.append(interpolated.samples, samples, axis=0)\n                            timestamps = np.append(ends[0], timestamps)\n\n                            if invalidation_bits is not None:\n                                invalidation_bits = np.append(\n                                    interpolated.invalidation_bits, invalidation_bits\n                                )\n\n                    result = Signal(\n                        samples,\n                        timestamps,\n                        self.unit,\n                        self.name,\n                        self.conversion,\n                        self.comment,\n                        self.raw,\n                        self.master_metadata,\n                        self.display_name,\n                        self.attachment,\n                        self.source,\n                        self.bit_count,\n                        self.stream_sync,\n                        invalidation_bits=invalidation_bits,\n                        encoding=self.encoding,\n                    )\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extend(self, other):\n        if len(self.timestamps):\n            last_stamp = self.timestamps[-1]\n        else:\n            last_stamp = 0\n        if len(other):\n            other_first_sample = other.timestamps[0]\n            if last_stamp >= other_first_sample:\n                timestamps = other.timestamps + last_stamp\n            else:\n                timestamps = other.timestamps\n\n            if self.invalidation_bits is None and other.invalidation_bits is None:\n                invalidation_bits = None\n            elif self.invalidation_bits is None and other.invalidation_bits is not None:\n                invalidation_bits = np.concatenate(\n                    (np.zeros(len(self), dtype=bool), other.invalidation_bits)\n                )\n            elif self.invalidation_bits is not None and other.invalidation_bits is None:\n                invalidation_bits = np.concatenate(\n                    (self.invalidation_bits, np.zeros(len(other), dtype=bool))\n                )\n            else:\n                invalidation_bits = np.append(\n                    self.invalidation_bits, other.invalidation_bits\n                )\n\n            result = Signal(\n                np.append(self.samples, other.samples, axis=0),\n                np.append(self.timestamps, timestamps),\n                self.unit,\n                self.name,\n                self.conversion,\n                self.comment,\n                self.raw,\n                self.master_metadata,\n                self.display_name,\n                self.attachment,\n                self.source,\n                self.bit_count,\n                self.stream_sync,\n                invalidation_bits=invalidation_bits,\n                encoding=self.encoding,\n            )\n        else:\n            result = self\n\n        return result", "response": "extend this signal with samples from another signal"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interp(self, new_timestamps, interpolation_mode=0):\n        if not len(self.samples) or not len(new_timestamps):\n            return Signal(\n                self.samples.copy(),\n                self.timestamps.copy(),\n                self.unit,\n                self.name,\n                comment=self.comment,\n                conversion=self.conversion,\n                raw=self.raw,\n                master_metadata=self.master_metadata,\n                display_name=self.display_name,\n                attachment=self.attachment,\n                stream_sync=self.stream_sync,\n                invalidation_bits=self.invalidation_bits.copy()\n                if self.invalidation_bits is not None\n                else None,\n                encoding=self.encoding,\n            )\n        else:\n\n            if len(self.samples.shape) > 1:\n                idx = np.searchsorted(self.timestamps, new_timestamps, side=\"right\")\n                idx -= 1\n                idx = np.clip(idx, 0, idx[-1])\n                s = self.samples[idx]\n\n                if self.invalidation_bits is not None:\n                    invalidation_bits = self.invalidation_bits[idx]\n                else:\n                    invalidation_bits = None\n            else:\n\n                kind = self.samples.dtype.kind\n                if kind == \"f\":\n                    s = np.interp(new_timestamps, self.timestamps, self.samples)\n\n                    if self.invalidation_bits is not None:\n                        idx = np.searchsorted(\n                            self.timestamps, new_timestamps, side=\"right\"\n                        )\n                        idx -= 1\n                        idx = np.clip(idx, 0, idx[-1])\n                        invalidation_bits = self.invalidation_bits[idx]\n                    else:\n                        invalidation_bits = None\n                elif kind in \"ui\":\n                    if interpolation_mode == 1:\n                        s = np.interp(\n                            new_timestamps, self.timestamps, self.samples\n                        ).astype(self.samples.dtype)\n                        if self.invalidation_bits is not None:\n                            idx = np.searchsorted(\n                                self.timestamps, new_timestamps, side=\"right\"\n                            )\n                            idx -= 1\n                            idx = np.clip(idx, 0, idx[-1])\n                            invalidation_bits = self.invalidation_bits[idx]\n                        else:\n                            invalidation_bits = None\n                    else:\n                        idx = np.searchsorted(\n                            self.timestamps, new_timestamps, side=\"right\"\n                        )\n                        idx -= 1\n                        idx = np.clip(idx, 0, idx[-1])\n                        s = self.samples[idx]\n\n                        if self.invalidation_bits is not None:\n                            invalidation_bits = self.invalidation_bits[idx]\n                        else:\n                            invalidation_bits = None\n                else:\n                    idx = np.searchsorted(self.timestamps, new_timestamps, side=\"right\")\n                    idx -= 1\n                    idx = np.clip(idx, 0, idx[-1])\n                    s = self.samples[idx]\n\n                    if self.invalidation_bits is not None:\n                        invalidation_bits = self.invalidation_bits[idx]\n                    else:\n                        invalidation_bits = None\n\n            return Signal(\n                s,\n                new_timestamps,\n                self.unit,\n                self.name,\n                comment=self.comment,\n                conversion=self.conversion,\n                source=self.source,\n                raw=self.raw,\n                master_metadata=self.master_metadata,\n                display_name=self.display_name,\n                attachment=self.attachment,\n                stream_sync=self.stream_sync,\n                invalidation_bits=invalidation_bits,\n                encoding=self.encoding,\n            )", "response": "Returns a new Signal with interpolated samples using the new_timestamps."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndelegates operations to the samples attribute but in a time correct manner by considering the timestamps*", "response": "def __apply_func(self, other, func_name):\n        \"\"\" delegate operations to the *samples* attribute, but in a time\n        correct manner by considering the *timestamps*\n\n        \"\"\"\n\n        if isinstance(other, Signal):\n            if len(self) and len(other):\n                start = max(self.timestamps[0], other.timestamps[0])\n                stop = min(self.timestamps[-1], other.timestamps[-1])\n                s1 = self.cut(start, stop)\n                s2 = other.cut(start, stop)\n            else:\n                s1 = self\n                s2 = other\n            time = np.union1d(s1.timestamps, s2.timestamps)\n            s = self.interp(time).samples\n            o = other.interp(time).samples\n            func = getattr(s, func_name)\n            s = func(o)\n        elif other is None:\n            s = self.samples\n            time = self.timestamps\n        else:\n            func = getattr(self.samples, func_name)\n            s = func(other)\n            time = self.timestamps\n        return Signal(\n            samples=s,\n            timestamps=time,\n            unit=self.unit,\n            name=self.name,\n            conversion=self.conversion,\n            raw=self.raw,\n            master_metadata=self.master_metadata,\n            display_name=self.display_name,\n            attachment=self.attachment,\n            stream_sync=self.stream_sync,\n            invalidation_bits=self.invalidation_bits,\n            source=self.source,\n            encoding=self.encoding,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns new Signal with samples of dtype np_type", "response": "def astype(self, np_type):\n        \"\"\" returns new *Signal* with samples of dtype *np_type*\n\n        Parameters\n        ----------\n        np_type : np.dtype\n            new numpy dtye\n\n        Returns\n        -------\n        signal : Signal\n            new *Signal* with the samples of *np_type* dtype\n\n        \"\"\"\n        return Signal(\n            self.samples.astype(np_type),\n            self.timestamps,\n            unit=self.unit,\n            name=self.name,\n            conversion=self.conversion,\n            raw=self.raw,\n            master_metadata=self.master_metadata,\n            display_name=self.display_name,\n            attachment=self.attachment,\n            stream_sync=self.stream_sync,\n            invalidation_bits=self.invalidation_bits,\n            source=self.source,\n            encoding=self.encoding,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new *Signal* with the physical samples values.", "response": "def physical(self):\n        \"\"\"\n        get the physical samples values\n\n        Returns\n        -------\n        phys : Signal\n            new *Signal* with physical values\n\n        \"\"\"\n\n        if not self.raw or self.conversion is None:\n            samples = self.samples.copy()\n        else:\n            samples = self.conversion.convert(self.samples)\n\n        return Signal(\n            samples,\n            self.timestamps.copy(),\n            unit=self.unit,\n            name=self.name,\n            conversion=self.conversion,\n            raw=False,\n            master_metadata=self.master_metadata,\n            display_name=self.display_name,\n            attachment=self.attachment,\n            stream_sync=self.stream_sync,\n            invalidation_bits=self.invalidation_bits,\n            source=self.source,\n            encoding=self.encoding,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a set of graphs from a result file.", "response": "def generate_graphs(result, topic, aspect, for_doc=False):\n    \"\"\" genrate graphs from result file\n\n    Parameters\n    ----------\n    result : str\n        path to result file\n    topic : str\n        benchmark topic; for example \"Open file\" or \"Save file\"\n    aspect : str\n        performance indiitemsor; can be \"ram\" (RAM memory usage) or \"time\" (elapsed time)\n    for_doc : bool\n        wether the source code is used inside the documentation\n\n    \"\"\"\n    result = result\n    topic = topic\n    aspect = aspect\n    for_doc = for_doc\n\n    with open(result, 'r') as f:\n        lines = f.readlines()\n\n    platform = 'x86' if '32 bit' in lines[2] else 'x64'\n\n    idx = [i for i, line in enumerate(lines) if line.startswith('==')]\n\n    table_spans = {'open': [idx[1] + 1, idx[2]],\n                   'save': [idx[4] + 1, idx[5]],\n                   'get': [idx[7] + 1, idx[8]],\n                   'convert' : [idx[10] + 1, idx[11]],\n                   'merge' : [idx[13] + 1, idx[14]]}\n\n\n    start, stop = table_spans[topic.lower()]\n\n    items = [l[:50].strip(' \\t\\r\\n\\0*') for l in lines[start: stop]]\n    time = np.array([int(l[50:61].strip(' \\t\\r\\n\\0*')) for l in lines[start: stop]])\n    ram = np.array([int(l[61:].strip(' \\t\\r\\n\\0*')) for l in lines[start: stop]])\n\n    if aspect == 'ram':\n        array = ram\n    else:\n        array = time\n\n    y_pos = list(range(len(items)))\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches(15, 3.8 / 12 * len(items) + 1.2)\n\n    asam_pos = [i for i, c in enumerate(items) if c.startswith('asam')]\n    mdfreader_pos = [i for i, c in enumerate(items) if c.startswith('mdfreader')]\n\n    ax.barh(asam_pos, array[asam_pos], color='green', ecolor='green')\n    ax.barh(mdfreader_pos, array[mdfreader_pos], color='blue', ecolor='black')\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(items)\n    ax.invert_yaxis()\n    ax.set_xlabel('Time [ms]' if aspect == 'time' else 'RAM [MB]')\n    if topic == 'Get':\n        ax.set_title('Get all channels (36424 calls) - {}'\n                     .format('time' if aspect == 'time' else 'ram usage'))\n    else:\n        ax.set_title('{} test file - {}'\n                     .format(topic, 'time' if aspect == 'time' else 'ram usage'))\n    ax.xaxis.grid()\n\n    fig.subplots_adjust(bottom=0.72/fig.get_figheight(),\n                        top=1-0.48/fig.get_figheight(),\n                        left=0.4,\n                        right=0.9)\n\n    if aspect == 'time':\n        if topic == 'Get':\n            name = '{}_get_all_channels.png'.format(platform)\n        else:\n            name = '{}_{}.png'.format(platform, topic.lower())\n    else:\n        if topic == 'Get':\n            name = '{}_get_all_channels_ram_usage.png'.format(platform)\n        else:\n            name = '{}_{}_ram_usage.png'.format(platform, topic.lower())\n\n    if for_doc:\n        plt.show()\n    else:\n        plt.savefig(name, dpi=300)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract(self):\n        if self.flags & v4c.FLAG_AT_EMBEDDED:\n            if self.flags & v4c.FLAG_AT_COMPRESSED_EMBEDDED:\n                data = decompress(self.embedded_data)\n            else:\n                data = self.embedded_data\n            if self.flags & v4c.FLAG_AT_MD5_VALID:\n                md5_worker = md5()\n                md5_worker.update(data)\n                md5_sum = md5_worker.digest()\n                if self.md5_sum == md5_sum:\n                    return data\n                else:\n                    message = f\"ATBLOCK md5sum={self.md5_sum} and embedded data md5sum={md5_sum}\"\n                    logger.warning(message)\n            else:\n                return data\n        else:\n            logger.warning(\"external attachments not supported\")", "response": "extract attachment data Returns ------- data : bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the data block of the current channel group into memory", "response": "def _load_data(self, group, record_offset=0, record_count=None):\n        \"\"\" get group's data block bytes\"\"\"\n        has_yielded = False\n        offset = 0\n        _count = record_count\n        channel_group = group.channel_group\n\n        if group.data_location == v23c.LOCATION_ORIGINAL_FILE:\n            # go to the first data block of the current data group\n            stream = self._file\n        else:\n            stream = self._tempfile\n\n        record_offset *= channel_group.samples_byte_nr\n\n        # go to the first data block of the current data group\n        if group.sorted:\n            samples_size = channel_group.samples_byte_nr\n            if not samples_size:\n                yield b\"\", 0, _count\n                has_yielded = True\n            else:\n                if self._read_fragment_size:\n                    split_size = self._read_fragment_size // samples_size\n                    split_size *= samples_size\n                else:\n                    channels_nr = len(group.channels)\n\n                    y_axis = CONVERT\n\n                    idx = searchsorted(CHANNEL_COUNT, channels_nr, side=\"right\") - 1\n                    if idx < 0:\n                        idx = 0\n                    split_size = y_axis[idx]\n\n                    split_size = split_size // samples_size\n                    split_size *= samples_size\n\n                if split_size == 0:\n                    split_size = samples_size\n\n                blocks = iter(group.data_blocks)\n\n                cur_size = 0\n                data = []\n\n                while True:\n                    try:\n                        info = next(blocks)\n                        address, size = info.address, info.size\n                        current_address = address\n                    except StopIteration:\n                        break\n\n                    if offset + size < record_offset + 1:\n                        offset += size\n                        continue\n\n                    stream.seek(address)\n\n                    if offset < record_offset:\n                        delta = record_offset - offset\n                        stream.read(delta)\n                        current_address += delta\n                        size -= delta\n                        offset = record_offset\n\n                    while size >= split_size - cur_size:\n                        stream.seek(current_address)\n                        if data:\n                            data.append(stream.read(split_size - cur_size))\n                            yield b\"\".join(data), offset, _count\n                            has_yielded = True\n                            current_address += split_size - cur_size\n                        else:\n                            yield stream.read(split_size), offset, _count\n                            has_yielded = True\n                            current_address += split_size\n                        offset += split_size\n\n                        size -= split_size - cur_size\n                        data = []\n                        cur_size = 0\n\n                    if size:\n                        stream.seek(current_address)\n                        data.append(stream.read(size))\n                        cur_size += size\n                        offset += size\n\n                if data:\n                    yield b\"\".join(data), offset, _count\n                    has_yielded = True\n                elif not offset:\n                    yield b\"\", 0, _count\n                    has_yielded = True\n                if not has_yielded:\n                    yield b\"\", 0, _count\n\n        else:\n            record_id = group.channel_group.record_id\n            cg_size = group.record_size\n            if group.data_group.record_id_len <= 2:\n                record_id_nr = group.data_group.record_id_len\n            else:\n                record_id_nr = 0\n            cg_data = []\n\n            blocks = group.data_blocks\n\n            for info in blocks:\n                address, size = info.address, info.size\n                stream.seek(address)\n                data = stream.read(size)\n\n                i = 0\n                while i < size:\n                    rec_id = data[i]\n                    # skip record id\n                    i += 1\n                    rec_size = cg_size[rec_id]\n                    if rec_id == record_id:\n                        rec_data = data[i : i + rec_size]\n                        cg_data.append(rec_data)\n                    # consider the second record ID if it exists\n                    if record_id_nr == 2:\n                        i += rec_size + 1\n                    else:\n                        i += rec_size\n                cg_data = b\"\".join(cg_data)\n                size = len(cg_data)\n\n                if size:\n                    if offset + size < record_offset + 1:\n                        offset += size\n                        continue\n\n                    if offset < record_offset:\n                        delta = record_offset - offset\n                        size -= delta\n                        offset = record_offset\n\n                    yield cg_data, offset, _count\n                    has_yielded = True\n                    offset += size\n        if not has_yielded:\n            yield b\"\", 0, _count"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the record dtype and parents dict for this MDF group and create the record structure for the MDF record.", "response": "def _prepare_record(self, group):\n        \"\"\" compute record dtype and parents dict for this group\n\n        Parameters\n        ----------\n        group : dict\n            MDF group dict\n\n        Returns\n        -------\n        parents, dtypes : dict, numpy.dtype\n            mapping of channels to records fields, records fiels dtype\n\n        \"\"\"\n\n        parents, dtypes = group.parents, group.types\n        if parents is None:\n            if group.data_location == v23c.LOCATION_ORIGINAL_FILE:\n                stream = self._file\n            else:\n                stream = self._tempfile\n            grp = group\n            record_size = grp.channel_group.samples_byte_nr << 3\n            next_byte_aligned_position = 0\n            types = []\n            current_parent = \"\"\n            parent_start_offset = 0\n            parents = {}\n            group_channels = UniqueDB()\n\n            # the channels are first sorted ascending (see __lt__ method of Channel\n            # class): a channel with lower start offset is smaller, when two\n            # channels havethe same start offset the one with higer bit size is\n            # considered smaller. The reason is that when the numpy record is built\n            # and there are overlapping channels, the parent fields mustbe bigger\n            # (bit size) than the embedded channels. For each channel the parent\n            # dict will have a (parent name, bit offset) pair: the channel value is\n            # computed using the values from the parent field, and the bit offset,\n            # which is the channel's bit offset within the parent bytes.\n            # This means all parents will have themselves as parent, and bit offset\n            # of 0. Gaps in the records are also considered. Non standard integers\n            # size is adjusted to the first higher standard integer size (eq. uint\n            # of 28bits will be adjusted to 32bits)\n\n            sortedchannels = sorted(enumerate(grp.channels), key=lambda i: i[1])\n            for original_index, new_ch in sortedchannels:\n                # skip channels with channel dependencies from the numpy record\n                if new_ch.component_addr:\n                    continue\n\n                start_offset = new_ch.start_offset\n                try:\n                    additional_byte_offset = new_ch.additional_byte_offset\n                    start_offset += 8 * additional_byte_offset\n                except AttributeError:\n                    pass\n\n                bit_offset = start_offset % 8\n                data_type = new_ch.data_type\n                bit_count = new_ch.bit_count\n                name = new_ch.name\n\n                # handle multiple occurance of same channel name\n                name = group_channels.get_unique_name(name)\n\n                if start_offset >= next_byte_aligned_position:\n                    parent_start_offset = (start_offset // 8) * 8\n\n                    # check if there are byte gaps in the record\n                    gap = (parent_start_offset - next_byte_aligned_position) // 8\n                    if gap:\n                        types.append((\"\", f\"V{gap}\"))\n\n                    # adjust size to 1, 2, 4 or 8 bytes for nonstandard integers\n                    size = bit_offset + bit_count\n                    if data_type == v23c.DATA_TYPE_STRING:\n                        next_byte_aligned_position = parent_start_offset + size\n                        if next_byte_aligned_position <= record_size:\n                            dtype_pair = (name, get_fmt_v3(data_type, size))\n                            types.append(dtype_pair)\n                            parents[original_index] = name, bit_offset\n                        else:\n                            next_byte_aligned_position = parent_start_offset\n\n                    elif data_type == v23c.DATA_TYPE_BYTEARRAY:\n                        next_byte_aligned_position = parent_start_offset + size\n                        if next_byte_aligned_position <= record_size:\n                            dtype_pair = (name, get_fmt_v3(data_type, size))\n                            types.append(dtype_pair)\n                            parents[original_index] = name, bit_offset\n                        else:\n                            next_byte_aligned_position = parent_start_offset\n\n                    else:\n                        if size > 32:\n                            next_byte_aligned_position = parent_start_offset + 64\n                        elif size > 16:\n                            next_byte_aligned_position = parent_start_offset + 32\n                        elif size > 8:\n                            next_byte_aligned_position = parent_start_offset + 16\n                        else:\n                            next_byte_aligned_position = parent_start_offset + 8\n\n                        if next_byte_aligned_position <= record_size:\n                            dtype_pair = (name, get_fmt_v3(data_type, size))\n                            types.append(dtype_pair)\n                            parents[original_index] = name, bit_offset\n                        else:\n                            next_byte_aligned_position = parent_start_offset\n\n                    current_parent = name\n                else:\n                    max_overlapping = next_byte_aligned_position - start_offset\n                    if max_overlapping >= bit_count:\n                        parents[original_index] = (\n                            current_parent,\n                            start_offset - parent_start_offset,\n                        )\n                if next_byte_aligned_position > record_size:\n                    break\n\n            gap = (record_size - next_byte_aligned_position) // 8\n            if gap:\n                dtype_pair = (\"\", f\"V{gap}\")\n                types.append(dtype_pair)\n\n            dtypes = dtype(types)\n\n            group.parents, group.types = parents, dtypes\n        return parents, dtypes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_trigger(self, group, timestamp, pre_time=0, post_time=0, comment=\"\"):\n        comment_template = \"\"\"<EVcomment>\n    <TX>{}</TX>\n</EVcomment>\"\"\"\n        try:\n            group = self.groups[group]\n        except IndexError:\n            return\n\n        trigger = group.trigger\n\n        if comment:\n            try:\n                comment = ET.fromstring(comment)\n                if comment.find(\".//TX\"):\n                    comment = comment.find(\".//TX\").text\n                else:\n                    comment = \"\"\n            except ET.ParseError:\n                pass\n\n        if trigger:\n            count = trigger[\"trigger_events_nr\"]\n            trigger[\"trigger_events_nr\"] += 1\n            trigger.block_len += 24\n            trigger[f\"trigger_{count}_time\"] = timestamp\n            trigger[f\"trigger_{count}_pretime\"] = pre_time\n            trigger[f\"trigger_{count}_posttime\"] = post_time\n            if comment:\n                if trigger.comment is None:\n                    comment = f\"{count + 1}. {comment}\"\n                    comment = comment_template.format(comment)\n                    trigger.comment = comment\n                else:\n                    current_comment = trigger.comment\n                    try:\n                        current_comment = ET.fromstring(current_comment)\n                        if current_comment.find(\".//TX\"):\n                            current_comment = current_comment.find(\".//TX\").text\n                        else:\n                            current_comment = \"\"\n                    except ET.ParseError:\n                        pass\n\n                    comment = f\"{current_comment}\\n{count + 1}. {comment}\"\n                    comment = comment_template.format(comment)\n                    trigger.comment = comment\n        else:\n            trigger = TriggerBlock(\n                trigger_event_nr=1,\n                trigger_0_time=timestamp,\n                trigger_0_pretime=pre_time,\n                trigger_0_posttime=post_time,\n            )\n            if comment:\n                comment = f\"1. {comment}\"\n                comment = comment_template.format(comment)\n                trigger.comment = comment\n\n            group.trigger = trigger", "response": "add a new trigger to the group"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend a new data group to the end of the current data group.", "response": "def append(\n        self, signals, acquisition_info=\"Python\", common_timebase=False, units=None\n    ):\n        \"\"\"Appends a new data group.\n\n        For channel dependencies type Signals, the *samples* attribute must be\n        a numpy.recarray\n\n        Parameters\n        ----------\n        signals : list | Signal | pandas.DataFrame\n            list of *Signal* objects, or a single *Signal* object, or a pandas\n            *DataFrame* object. All bytes columns in the pandas *DataFrame*\n            must be *latin-1* encoded\n        acquisition_info : str\n            acquisition information; default 'Python'\n        common_timebase : bool\n            flag to hint that the signals have the same timebase. Only set this\n            if you know for sure that all appended channels share the same\n            time base\n        units : dict\n            will contain the signal units mapped to the singal names when\n            appending a pandas DataFrame\n\n\n        Examples\n        --------\n        >>> # case 1 conversion type None\n        >>> s1 = np.array([1, 2, 3, 4, 5])\n        >>> s2 = np.array([-1, -2, -3, -4, -5])\n        >>> s3 = np.array([0.1, 0.04, 0.09, 0.16, 0.25])\n        >>> t = np.array([0.001, 0.002, 0.003, 0.004, 0.005])\n        >>> names = ['Positive', 'Negative', 'Float']\n        >>> units = ['+', '-', '.f']\n        >>> info = {}\n        >>> s1 = Signal(samples=s1, timstamps=t, unit='+', name='Positive')\n        >>> s2 = Signal(samples=s2, timstamps=t, unit='-', name='Negative')\n        >>> s3 = Signal(samples=s3, timstamps=t, unit='flts', name='Floats')\n        >>> mdf = MDF3('new.mdf')\n        >>> mdf.append([s1, s2, s3], 'created by asammdf v1.1.0')\n        >>> # case 2: VTAB conversions from channels inside another file\n        >>> mdf1 = MDF3('in.mdf')\n        >>> ch1 = mdf1.get(\"Channel1_VTAB\")\n        >>> ch2 = mdf1.get(\"Channel2_VTABR\")\n        >>> sigs = [ch1, ch2]\n        >>> mdf2 = MDF3('out.mdf')\n        >>> mdf2.append(sigs, 'created by asammdf v1.1.0')\n        >>> df = pd.DataFrame.from_dict({'s1': np.array([1, 2, 3, 4, 5]), 's2': np.array([-1, -2, -3, -4, -5])})\n        >>> units = {'s1': 'V', 's2': 'A'}\n        >>> mdf2.append(df, units=units)\n\n        \"\"\"\n        if isinstance(signals, Signal):\n            signals = [signals]\n        elif isinstance(signals, DataFrame):\n            self._append_dataframe(signals, acquisition_info, units=units)\n            return\n\n        version = self.version\n        interp_mode = self._integer_interpolation\n\n        # check if the signals have a common timebase\n        # if not interpolate the signals using the union of all timbases\n        if signals:\n            timestamps = signals[0].timestamps\n            if not common_timebase:\n                for signal in signals[1:]:\n                    if not array_equal(signal.timestamps, timestamps):\n                        different = True\n                        break\n                else:\n                    different = False\n\n                if different:\n                    times = [s.timestamps for s in signals]\n                    timestamps = reduce(union1d, times).flatten().astype(float64)\n                    signals = [s.interp(timestamps, interpolation_mode=interp_mode) for s in signals]\n                    times = None\n        else:\n            timestamps = array([])\n\n        if self.version < \"3.10\":\n            if timestamps.dtype.byteorder == \">\":\n                timestamps = timestamps.byteswap().newbyteorder()\n            for signal in signals:\n                if signal.samples.dtype.byteorder == \">\":\n                    signal.samples = signal.samples.byteswap().newbyteorder()\n\n        if self.version >= \"3.00\":\n            channel_size = v23c.CN_DISPLAYNAME_BLOCK_SIZE\n        elif self.version >= \"2.10\":\n            channel_size = v23c.CN_LONGNAME_BLOCK_SIZE\n        else:\n            channel_size = v23c.CN_SHORT_BLOCK_SIZE\n\n        file = self._tempfile\n        tell = file.tell\n\n        kargs = {\n            \"module_nr\": 0,\n            \"module_address\": 0,\n            \"type\": v23c.SOURCE_ECU,\n            \"description\": b\"Channel inserted by Python Script\",\n        }\n        ce_block = ChannelExtension(**kargs)\n\n        canopen_time_fields = (\"ms\", \"days\")\n        canopen_date_fields = (\n            \"ms\",\n            \"min\",\n            \"hour\",\n            \"day\",\n            \"month\",\n            \"year\",\n            \"summer_time\",\n            \"day_of_week\",\n        )\n\n        dg_cntr = len(self.groups)\n\n        gp = Group(None)\n        gp.channels = gp_channels = []\n        gp.channel_dependencies = gp_dep = []\n        gp.signal_types = gp_sig_types = []\n        gp.string_dtypes = []\n\n        self.groups.append(gp)\n\n        cycles_nr = len(timestamps)\n        fields = []\n        types = []\n        parents = {}\n        ch_cntr = 0\n        offset = 0\n        field_names = UniqueDB()\n\n        if signals:\n            master_metadata = signals[0].master_metadata\n        else:\n            master_metadata = None\n        if master_metadata:\n            time_name = master_metadata[0]\n        else:\n            time_name = \"time\"\n\n        if signals:\n            # conversion for time channel\n            kargs = {\n                \"conversion_type\": v23c.CONVERSION_TYPE_NONE,\n                \"unit\": b\"s\",\n                \"min_phy_value\": timestamps[0] if cycles_nr else 0,\n                \"max_phy_value\": timestamps[-1] if cycles_nr else 0,\n            }\n            conversion = ChannelConversion(**kargs)\n            conversion.unit = \"s\"\n            source = ce_block\n\n            # time channel\n            t_type, t_size = fmt_to_datatype_v3(timestamps.dtype, timestamps.shape)\n            kargs = {\n                \"short_name\": time_name.encode(\"latin-1\"),\n                \"channel_type\": v23c.CHANNEL_TYPE_MASTER,\n                \"data_type\": t_type,\n                \"start_offset\": 0,\n                \"min_raw_value\": timestamps[0] if cycles_nr else 0,\n                \"max_raw_value\": timestamps[-1] if cycles_nr else 0,\n                \"bit_count\": t_size,\n                \"block_len\": channel_size,\n                \"version\": version,\n            }\n            channel = Channel(**kargs)\n            channel.name = name = time_name\n            channel.conversion = conversion\n            channel.source = source\n\n            gp_channels.append(channel)\n\n            self.channels_db.add(name, (dg_cntr, ch_cntr))\n            self.masters_db[dg_cntr] = 0\n            # data group record parents\n            parents[ch_cntr] = name, 0\n\n            # time channel doesn't have channel dependencies\n            gp_dep.append(None)\n\n            fields.append(timestamps)\n            types.append((name, timestamps.dtype))\n\n            offset += t_size\n            ch_cntr += 1\n\n            gp_sig_types.append(0)\n\n        for signal in signals:\n            sig = signal\n            names = sig.samples.dtype.names\n            name = signal.name\n            if names is None:\n                sig_type = v23c.SIGNAL_TYPE_SCALAR\n            else:\n                if names in (canopen_time_fields, canopen_date_fields):\n                    sig_type = v23c.SIGNAL_TYPE_CANOPEN\n                elif names[0] != sig.name:\n                    sig_type = v23c.SIGNAL_TYPE_STRUCTURE_COMPOSITION\n                else:\n                    sig_type = v23c.SIGNAL_TYPE_ARRAY\n\n            gp_sig_types.append(sig_type)\n\n            # conversions for channel\n\n            conversion = conversion_transfer(signal.conversion)\n            conversion.unit = unit = signal.unit\n            israw = signal.raw\n\n            if not israw and not unit:\n                conversion = None\n\n            if sig_type == v23c.SIGNAL_TYPE_SCALAR:\n\n                # source for channel\n                if signal.source:\n                    source = signal.source\n                    if source.source_type != 2:\n                        kargs = {\n                            \"type\": v23c.SOURCE_ECU,\n                            \"description\": source.name.encode(\"latin-1\"),\n                            \"ECU_identification\": source.path.encode(\"latin-1\"),\n                        }\n                    else:\n                        kargs = {\n                            \"type\": v23c.SOURCE_VECTOR,\n                            \"message_name\": source.name.encode(\"latin-1\"),\n                            \"sender_name\": source.path.encode(\"latin-1\"),\n                        }\n\n                    new_source = ChannelExtension(**kargs)\n\n                else:\n                    new_source = ce_block\n\n                # compute additional byte offset for large records size\n                if offset > v23c.MAX_UINT16:\n                    additional_byte_offset = ceil((offset - v23c.MAX_UINT16) / 8)\n                    start_bit_offset = offset - additional_byte_offset * 8\n                else:\n                    start_bit_offset = offset\n                    additional_byte_offset = 0\n\n                s_type, s_size = fmt_to_datatype_v3(\n                    signal.samples.dtype, signal.samples.shape\n                )\n\n                name = signal.name\n                display_name = signal.display_name\n\n                if signal.samples.dtype.kind == \"u\" and signal.bit_count <= 4:\n                    s_size_ = signal.bit_count\n                else:\n                    s_size_ = s_size\n\n                kargs = {\n                    \"channel_type\": v23c.CHANNEL_TYPE_VALUE,\n                    \"data_type\": s_type,\n                    \"start_offset\": start_bit_offset,\n                    \"bit_count\": s_size_,\n                    \"additional_byte_offset\": additional_byte_offset,\n                    \"block_len\": channel_size,\n                    \"version\": version,\n                }\n\n                if s_size < 8:\n                    s_size = 8\n\n                channel = Channel(**kargs)\n                channel.name = signal.name\n                channel.comment = signal.comment\n                channel.source = new_source\n                channel.conversion = conversion\n                gp_channels.append(channel)\n\n                offset += s_size\n\n                entry = (dg_cntr, ch_cntr)\n                self.channels_db.add(name, entry)\n                self.channels_db.add(display_name, entry)\n\n                # update the parents as well\n                field_name = field_names.get_unique_name(name)\n                parents[ch_cntr] = field_name, 0\n\n                if signal.samples.dtype.kind == \"S\":\n                    gp.string_dtypes.append(signal.samples.dtype)\n\n                fields.append(signal.samples)\n                if s_type != v23c.DATA_TYPE_BYTEARRAY:\n                    types.append((field_name, signal.samples.dtype))\n                else:\n                    types.append(\n                        (field_name, signal.samples.dtype, signal.samples.shape[1:])\n                    )\n\n                ch_cntr += 1\n\n                # simple channels don't have channel dependencies\n                gp_dep.append(None)\n\n            # second, add the composed signals\n            elif sig_type in (\n                v23c.SIGNAL_TYPE_CANOPEN,\n                v23c.SIGNAL_TYPE_STRUCTURE_COMPOSITION,\n            ):\n                new_dg_cntr = len(self.groups)\n                new_gp = {}\n                new_gp.channels = new_gp_channels = []\n                new_gp.channel_dependencies = new_gp_dep = []\n                new_gp.signal_types = new_gp_sig_types = []\n                self.groups.append(new_gp)\n\n                new_fields = []\n                new_types = []\n                new_parents = {}\n                new_ch_cntr = 0\n                new_offset = 0\n                new_field_names = UniqueDB()\n\n                # conversion for time channel\n                kargs = {\n                    \"conversion_type\": v23c.CONVERSION_TYPE_NONE,\n                    \"unit\": b\"s\",\n                    \"min_phy_value\": timestamps[0] if cycles_nr else 0,\n                    \"max_phy_value\": timestamps[-1] if cycles_nr else 0,\n                }\n                conversion = ChannelConversion(**kargs)\n                conversion.unit = \"s\"\n\n                source = ce_block\n\n                # time channel\n                t_type, t_size = fmt_to_datatype_v3(timestamps.dtype, timestamps.shape)\n                kargs = {\n                    \"short_name\": time_name.encode(\"latin-1\"),\n                    \"channel_type\": v23c.CHANNEL_TYPE_MASTER,\n                    \"data_type\": t_type,\n                    \"start_offset\": 0,\n                    \"min_raw_value\": timestamps[0] if cycles_nr else 0,\n                    \"max_raw_value\": timestamps[-1] if cycles_nr else 0,\n                    \"bit_count\": t_size,\n                    \"block_len\": channel_size,\n                    \"version\": version,\n                }\n                channel = Channel(**kargs)\n                channel.name = name = time_name\n                channel.source = source\n                channel.conversion = conversion\n                gp_channels.append(channel)\n\n                self.channels_db.add(name, (new_dg_cntr, new_ch_cntr))\n\n                self.masters_db[new_dg_cntr] = 0\n                # data group record parents\n                new_parents[new_ch_cntr] = name, 0\n\n                # time channel doesn't have channel dependencies\n                new_gp_dep.append(None)\n\n                new_fields.append(timestamps)\n                new_types.append((name, timestamps.dtype))\n                new_field_names.get_unique_name(name)\n                new_gp_sig_types.append(0)\n\n                new_offset += t_size\n                new_ch_cntr += 1\n\n                names = signal.samples.dtype.names\n                if names == (\"ms\", \"days\"):\n                    channel_group_comment = \"From mdf v4 CANopen Time channel\"\n                elif names == (\n                    \"ms\",\n                    \"min\",\n                    \"hour\",\n                    \"day\",\n                    \"month\",\n                    \"year\",\n                    \"summer_time\",\n                    \"day_of_week\",\n                ):\n                    channel_group_comment = \"From mdf v4 CANopen Date channel\"\n                else:\n                    channel_group_comment = \"From mdf v4 structure channel composition\"\n\n                for name in names:\n\n                    samples = signal.samples[name]\n\n                    # conversions for channel\n\n                    kargs = {\n                        \"conversion_type\": v23c.CONVERSION_TYPE_NONE,\n                        \"unit\": signal.unit.encode(\"latin-1\"),\n                        \"min_phy_value\": 0,\n                        \"max_phy_value\": 0,\n                    }\n                    conversion = ChannelConversion(**kargs)\n\n                    # source for channel\n                    if signal.source:\n                        source = signal.source\n                        if source.source_type != 2:\n                            kargs = {\n                                \"type\": v23c.SOURCE_ECU,\n                                \"description\": source.name.encode(\"latin-1\"),\n                                \"ECU_identification\": source.path.encode(\"latin-1\"),\n                            }\n                        else:\n                            kargs = {\n                                \"type\": v23c.SOURCE_VECTOR,\n                                \"message_name\": source.name.encode(\"latin-1\"),\n                                \"sender_name\": source.path.encode(\"latin-1\"),\n                            }\n\n                        source = ChannelExtension(**kargs)\n\n                    else:\n                        source = ce_block\n\n                    # compute additional byte offset for large records size\n                    if new_offset > v23c.MAX_UINT16:\n                        additional_byte_offset = (new_offset - v23c.MAX_UINT16) // 8\n                        start_bit_offset = new_offset - additional_byte_offset << 3\n                    else:\n                        start_bit_offset = new_offset\n                        additional_byte_offset = 0\n                    s_type, s_size = fmt_to_datatype_v3(samples.dtype, samples.shape)\n\n                    kargs = {\n                        \"channel_type\": v23c.CHANNEL_TYPE_VALUE,\n                        \"data_type\": s_type,\n                        \"start_offset\": start_bit_offset,\n                        \"bit_count\": s_size,\n                        \"additional_byte_offset\": additional_byte_offset,\n                        \"block_len\": channel_size,\n                        \"version\": version,\n                    }\n\n                    if s_size < 8:\n                        s_size = 8\n\n                    channel = Channel(**kargs)\n                    channel.name = name\n                    channel.source = source\n                    channel.conversion = conversion\n\n                    new_gp_channels.append(channel)\n                    new_offset += s_size\n\n                    self.channels_db.add(name, (new_dg_cntr, new_ch_cntr))\n\n                    # update the parents as well\n                    field_name = new_field_names.get_unique_name(name)\n                    new_parents[new_ch_cntr] = field_name, 0\n\n                    new_fields.append(samples)\n                    new_types.append((field_name, samples.dtype))\n\n                    new_ch_cntr += 1\n\n                    # simple channels don't have channel dependencies\n                    new_gp_dep.append(None)\n\n                # channel group\n                kargs = {\n                    \"cycles_nr\": cycles_nr,\n                    \"samples_byte_nr\": new_offset // 8,\n                    \"ch_nr\": new_ch_cntr,\n                }\n                new_gp.channel_group = ChannelGroup(**kargs)\n                new_gp.channel_group.comment = channel_group_comment\n\n                # data group\n                if self.version >= \"3.20\":\n                    block_len = v23c.DG_POST_320_BLOCK_SIZE\n                else:\n                    block_len = v23c.DG_PRE_320_BLOCK_SIZE\n                new_gp.data_group = DataGroup(block_len=block_len)\n\n                # data block\n                new_types = dtype(new_types)\n\n                new_gp.types = new_types\n                new_gp.parents = new_parents\n                new_gp.sorted = True\n\n                samples = fromarrays(new_fields, dtype=new_types)\n\n                block = samples.tostring()\n\n                new_gp.data_location = v23c.LOCATION_TEMPORARY_FILE\n                if cycles_nr:\n                    data_address = tell()\n                    new_gp.data_group.data_block_addr = data_address\n                    self._tempfile.write(block)\n                    size = len(block)\n                    new_gp.data_blocks.append(\n                        DataBlockInfo(\n                            address=data_address,\n                            raw_size=size,\n                            size=size,\n                            block_type=0,\n                            param=0,\n                        )\n                    )\n                else:\n                    new_gp.data_group.data_block_addr = 0\n\n                # data group trigger\n                new_gp.trigger = None\n\n            else:\n                names = signal.samples.dtype.names\n                name = signal.name\n\n                component_names = []\n                component_samples = []\n                if names:\n                    samples = signal.samples[names[0]]\n                else:\n                    samples = signal.samples\n\n                shape = samples.shape[1:]\n                dims = [list(range(size)) for size in shape]\n\n                for indexes in product(*dims):\n                    subarray = samples\n                    for idx in indexes:\n                        subarray = subarray[:, idx]\n                    component_samples.append(subarray)\n\n                    indexes = \"\".join(f\"[{idx}]\" for idx in indexes)\n                    component_name = f\"{name}{indexes}\"\n                    component_names.append(component_name)\n\n                # add channel dependency block for composed parent channel\n                sd_nr = len(component_samples)\n                kargs = {\"sd_nr\": sd_nr}\n                for i, dim in enumerate(shape[::-1]):\n                    kargs[f\"dim_{i}\"] = dim\n                parent_dep = ChannelDependency(**kargs)\n                gp_dep.append(parent_dep)\n\n                # source for channel\n                if signal.source:\n                    source = signal.source\n                    if source.source_type != 2:\n                        kargs = {\n                            \"type\": v23c.SOURCE_ECU,\n                            \"description\": source.name.encode(\"latin-1\"),\n                            \"ECU_identification\": source.path.encode(\"latin-1\"),\n                        }\n                    else:\n                        kargs = {\n                            \"type\": v23c.SOURCE_VECTOR,\n                            \"message_name\": source.name.encode(\"latin-1\"),\n                            \"sender_name\": source.path.encode(\"latin-1\"),\n                        }\n\n                    source = ChannelExtension(**kargs)\n\n                else:\n                    source = ce_block\n\n                s_type, s_size = fmt_to_datatype_v3(samples.dtype, (), True)\n                # compute additional byte offset for large records size\n                if offset > v23c.MAX_UINT16:\n                    additional_byte_offset = (offset - v23c.MAX_UINT16) // 8\n                    start_bit_offset = offset - additional_byte_offset << 3\n                else:\n                    start_bit_offset = offset\n                    additional_byte_offset = 0\n\n                kargs = {\n                    \"channel_type\": v23c.CHANNEL_TYPE_VALUE,\n                    \"data_type\": s_type,\n                    \"start_offset\": start_bit_offset,\n                    \"bit_count\": s_size,\n                    \"additional_byte_offset\": additional_byte_offset,\n                    \"block_len\": channel_size,\n                    \"version\": version,\n                }\n\n                if s_size < 8:\n                    s_size = 8\n\n                channel = Channel(**kargs)\n                channel.comment = signal.comment\n                channel.display_name = signal.display_name\n\n                gp_channels.append(channel)\n\n                self.channels_db.add(name, (dg_cntr, ch_cntr))\n\n                ch_cntr += 1\n\n                for i, (name, samples) in enumerate(\n                    zip(component_names, component_samples)\n                ):\n\n                    if i < sd_nr:\n                        dep_pair = dg_cntr, ch_cntr\n                        parent_dep.referenced_channels.append(dep_pair)\n                        description = b\"\\0\"\n                    else:\n                        description = f\"{signal.name} - axis {name}\"\n                        description = description.encode(\"latin-1\")\n\n                    s_type, s_size = fmt_to_datatype_v3(samples.dtype, ())\n                    shape = samples.shape[1:]\n\n                    # source for channel\n                    if signal.source:\n                        source = signal.source\n                        if source.source_type != 2:\n                            kargs = {\n                                \"type\": v23c.SOURCE_ECU,\n                                \"description\": source.name.encode(\"latin-1\"),\n                                \"ECU_identification\": source.path.encode(\"latin-1\"),\n                            }\n                        else:\n                            kargs = {\n                                \"type\": v23c.SOURCE_VECTOR,\n                                \"message_name\": source.name.encode(\"latin-1\"),\n                                \"sender_name\": source.path.encode(\"latin-1\"),\n                            }\n\n                        source = ChannelExtension(**kargs)\n                    else:\n                        source = ce_block\n\n                    # compute additional byte offset for large records size\n                    if offset > v23c.MAX_UINT16:\n                        additional_byte_offset = (offset - v23c.MAX_UINT16) // 8\n                        start_bit_offset = offset - additional_byte_offset << 3\n                    else:\n                        start_bit_offset = offset\n                        additional_byte_offset = 0\n\n                    kargs = {\n                        \"channel_type\": v23c.CHANNEL_TYPE_VALUE,\n                        \"data_type\": s_type,\n                        \"start_offset\": start_bit_offset,\n                        \"bit_count\": s_size,\n                        \"additional_byte_offset\": additional_byte_offset,\n                        \"block_len\": channel_size,\n                        \"description\": description,\n                        \"version\": version,\n                    }\n\n                    if s_size < 8:\n                        s_size = 8\n\n                    channel = Channel(**kargs)\n                    channel.name = name\n                    channel.source = source\n                    gp_channels.append(channel)\n\n                    size = s_size\n                    for dim in shape:\n                        size *= dim\n                    offset += size\n\n                    self.channels_db.add(name, (dg_cntr, ch_cntr))\n\n                    # update the parents as well\n                    field_name = field_names.get_unique_name(name)\n                    parents[ch_cntr] = field_name, 0\n\n                    fields.append(samples)\n                    types.append((field_name, samples.dtype, shape))\n\n                    gp_dep.append(None)\n\n                    ch_cntr += 1\n\n                for name in names[1:]:\n                    samples = signal.samples[name]\n\n                    component_names = []\n                    component_samples = []\n\n                    shape = samples.shape[1:]\n                    dims = [list(range(size)) for size in shape]\n\n                    for indexes in product(*dims):\n                        subarray = samples\n                        for idx in indexes:\n                            subarray = subarray[:, idx]\n                        component_samples.append(subarray)\n\n                        indexes = \"\".join(f\"[{idx}]\" for idx in indexes)\n                        component_name = f\"{name}{indexes}\"\n                        component_names.append(component_name)\n\n                    # add channel dependency block for composed parent channel\n                    sd_nr = len(component_samples)\n                    kargs = {\"sd_nr\": sd_nr}\n                    for i, dim in enumerate(shape[::-1]):\n                        kargs[\"dim_{}\".format(i)] = dim\n                    parent_dep = ChannelDependency(**kargs)\n                    gp_dep.append(parent_dep)\n\n                    # source for channel\n                    if signal.source:\n                        source = signal.source\n                        if source.source_type != 2:\n                            kargs = {\n                                \"type\": v23c.SOURCE_ECU,\n                                \"description\": source.name.encode(\"latin-1\"),\n                                \"ECU_identification\": source.path.encode(\"latin-1\"),\n                            }\n                        else:\n                            kargs = {\n                                \"type\": v23c.SOURCE_VECTOR,\n                                \"message_name\": source.name.encode(\"latin-1\"),\n                                \"sender_name\": source.path.encode(\"latin-1\"),\n                            }\n\n                        source = ChannelExtension(**kargs)\n\n                    else:\n                        source = ce_block\n\n                    s_type, s_size = fmt_to_datatype_v3(samples.dtype, ())\n                    # compute additional byte offset for large records size\n                    if offset > v23c.MAX_UINT16:\n                        additional_byte_offset = (offset - v23c.MAX_UINT16) // 8\n                        start_bit_offset = offset - additional_byte_offset << 3\n                    else:\n                        start_bit_offset = offset\n                        additional_byte_offset = 0\n\n                    kargs = {\n                        \"channel_type\": v23c.CHANNEL_TYPE_VALUE,\n                        \"data_type\": s_type,\n                        \"start_offset\": start_bit_offset,\n                        \"bit_count\": s_size,\n                        \"additional_byte_offset\": additional_byte_offset,\n                        \"block_len\": channel_size,\n                        \"version\": version,\n                    }\n\n                    if s_size < 8:\n                        s_size = 8\n\n                    channel = Channel(**kargs)\n                    channel.name = name\n                    channel.comment = signal.comment\n                    channel.source = source\n                    gp_channels.append(channel)\n\n                    self.channels_db.add(name, (dg_cntr, ch_cntr))\n\n                    ch_cntr += 1\n\n                    for i, (name, samples) in enumerate(\n                        zip(component_names, component_samples)\n                    ):\n\n                        if i < sd_nr:\n                            dep_pair = dg_cntr, ch_cntr\n                            parent_dep.referenced_channels.append(dep_pair)\n                            description = b\"\\0\"\n                        else:\n                            description = f\"{signal.name} - axis {name}\"\n                            description = description.encode(\"latin-1\")\n\n                        s_type, s_size = fmt_to_datatype_v3(samples.dtype, ())\n                        shape = samples.shape[1:]\n\n                        # source for channel\n                        if signal.source:\n                            source = signal.source\n                            if source.source_type != 2:\n                                kargs = {\n                                    \"type\": v23c.SOURCE_ECU,\n                                    \"description\": source.name.encode(\"latin-1\"),\n                                    \"ECU_identification\": source.path.encode(\"latin-1\"),\n                                }\n                            else:\n                                kargs = {\n                                    \"type\": v23c.SOURCE_VECTOR,\n                                    \"message_name\": source.name.encode(\"latin-1\"),\n                                    \"sender_name\": source.path.encode(\"latin-1\"),\n                                }\n\n                            source = ChannelExtension(**kargs)\n\n                        else:\n                            source = ce_block\n\n                        # compute additional byte offset for large records size\n                        if offset > v23c.MAX_UINT16:\n                            additional_byte_offset = (offset - v23c.MAX_UINT16) // 8\n                            start_bit_offset = offset - additional_byte_offset << 3\n                        else:\n                            start_bit_offset = offset\n                            additional_byte_offset = 0\n\n                        kargs = {\n                            \"channel_type\": v23c.CHANNEL_TYPE_VALUE,\n                            \"data_type\": s_type,\n                            \"start_offset\": start_bit_offset,\n                            \"bit_count\": s_size,\n                            \"additional_byte_offset\": additional_byte_offset,\n                            \"block_len\": channel_size,\n                            \"description\": description,\n                            \"version\": version,\n                        }\n\n                        if s_size < 8:\n                            s_size = 8\n\n                        channel = Channel(**kargs)\n                        channel.name = name\n                        channel.source = source\n                        gp_channels.append(channel)\n\n                        size = s_size\n                        for dim in shape:\n                            size *= dim\n                        offset += size\n\n                        self.channels_db.add(name, (dg_cntr, ch_cntr))\n\n                        # update the parents as well\n                        field_name = field_names.get_unique_name(name)\n                        parents[ch_cntr] = field_name, 0\n\n                        fields.append(samples)\n                        types.append((field_name, samples.dtype, shape))\n\n                        gp_dep.append(None)\n\n                        ch_cntr += 1\n\n        # channel group\n        kargs = {\n            \"cycles_nr\": cycles_nr,\n            \"samples_byte_nr\": offset // 8,\n            \"ch_nr\": ch_cntr,\n        }\n        if self.version >= \"3.30\":\n            kargs[\"block_len\"] = v23c.CG_POST_330_BLOCK_SIZE\n        else:\n            kargs[\"block_len\"] = v23c.CG_PRE_330_BLOCK_SIZE\n        gp.channel_group = ChannelGroup(**kargs)\n        gp.channel_group.comment = acquisition_info\n\n        # data group\n        if self.version >= \"3.20\":\n            block_len = v23c.DG_POST_320_BLOCK_SIZE\n        else:\n            block_len = v23c.DG_PRE_320_BLOCK_SIZE\n        gp.data_group = DataGroup(block_len=block_len)\n\n        # data block\n        types = dtype(types)\n\n        gp.types = types\n        gp.parents = parents\n        gp.sorted = True\n\n        if signals:\n            samples = fromarrays(fields, dtype=types)\n        else:\n            samples = array([])\n\n        block = samples.tostring()\n\n        gp.data_location = v23c.LOCATION_TEMPORARY_FILE\n        if cycles_nr:\n            data_address = tell()\n            gp.data_group.data_block_addr = data_address\n            size = len(block)\n            self._tempfile.write(block)\n\n            gp.data_blocks.append(\n                DataBlockInfo(\n                    address=data_address,\n                    block_type=0,\n                    raw_size=size,\n                    size=size,\n                    param=0,\n                )\n            )\n\n        else:\n            gp.data_location = v23c.LOCATION_TEMPORARY_FILE\n\n        # data group trigger\n        gp.trigger = None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _append_dataframe(self, df, source_info=\"\", units=None):\n\n        units = units or {}\n\n        t = df.index\n        index_name = df.index.name\n        time_name = index_name or \"time\"\n\n        version = self.version\n\n        timestamps = t\n\n        # if self.version < '3.10':\n        #     if timestamps.dtype.byteorder == '>':\n        #         timestamps = timestamps.byteswap().newbyteorder()\n        #     for signal in signals:\n        #         if signal.samples.dtype.byteorder == '>':\n        #             signal.samples = signal.samples.byteswap().newbyteorder()\n\n        if self.version >= \"3.00\":\n            channel_size = v23c.CN_DISPLAYNAME_BLOCK_SIZE\n        elif self.version >= \"2.10\":\n            channel_size = v23c.CN_LONGNAME_BLOCK_SIZE\n        else:\n            channel_size = v23c.CN_SHORT_BLOCK_SIZE\n\n        file = self._tempfile\n        tell = file.tell\n\n        kargs = {\n            \"module_nr\": 0,\n            \"module_address\": 0,\n            \"type\": v23c.SOURCE_ECU,\n            \"description\": b\"Channel inserted by Python Script\",\n        }\n        ce_block = ChannelExtension(**kargs)\n\n        dg_cntr = len(self.groups)\n\n        gp = {}\n        gp.channels = gp_channels = []\n        gp.channel_dependencies = gp_dep = []\n        gp.signal_types = gp_sig_types = []\n        gp.string_dtypes = []\n\n        self.groups.append(gp)\n\n        cycles_nr = len(timestamps)\n        fields = []\n        types = []\n        parents = {}\n        ch_cntr = 0\n        offset = 0\n        field_names = UniqueDB()\n\n        if df.shape[0]:\n            # conversion for time channel\n            kargs = {\n                \"conversion_type\": v23c.CONVERSION_TYPE_NONE,\n                \"unit\": b\"s\",\n                \"min_phy_value\": timestamps[0] if cycles_nr else 0,\n                \"max_phy_value\": timestamps[-1] if cycles_nr else 0,\n            }\n            conversion = ChannelConversion(**kargs)\n            conversion.unit = \"s\"\n            source = ce_block\n\n            # time channel\n            t_type, t_size = fmt_to_datatype_v3(timestamps.dtype, timestamps.shape)\n            kargs = {\n                \"short_name\": time_name.encode(\"latin-1\"),\n                \"channel_type\": v23c.CHANNEL_TYPE_MASTER,\n                \"data_type\": t_type,\n                \"start_offset\": 0,\n                \"min_raw_value\": timestamps[0] if cycles_nr else 0,\n                \"max_raw_value\": timestamps[-1] if cycles_nr else 0,\n                \"bit_count\": t_size,\n                \"block_len\": channel_size,\n                \"version\": version,\n            }\n            channel = Channel(**kargs)\n            channel.name = name = time_name\n            channel.conversion = conversion\n            channel.source = source\n\n            gp_channels.append(channel)\n\n            self.channels_db.add(name, (dg_cntr, ch_cntr))\n            self.masters_db[dg_cntr] = 0\n            # data group record parents\n            parents[ch_cntr] = name, 0\n\n            # time channel doesn't have channel dependencies\n            gp_dep.append(None)\n\n            fields.append(timestamps)\n            types.append((name, timestamps.dtype))\n            field_names.add(name)\n\n            offset += t_size\n            ch_cntr += 1\n\n            gp_sig_types.append(0)\n\n        for signal in df:\n\n            sig = df[signal]\n            name = signal\n\n            sig_type = v23c.SIGNAL_TYPE_SCALAR\n\n            gp_sig_types.append(sig_type)\n\n            new_source = ce_block\n\n            # compute additional byte offset for large records size\n            if offset > v23c.MAX_UINT16:\n                additional_byte_offset = ceil((offset - v23c.MAX_UINT16) / 8)\n                start_bit_offset = offset - additional_byte_offset * 8\n            else:\n                start_bit_offset = offset\n                additional_byte_offset = 0\n\n            s_type, s_size = fmt_to_datatype_v3(sig.dtype, sig.shape)\n\n            kargs = {\n                \"channel_type\": v23c.CHANNEL_TYPE_VALUE,\n                \"data_type\": s_type,\n                \"min_raw_value\": 0,\n                \"max_raw_value\": 0,\n                \"start_offset\": start_bit_offset,\n                \"bit_count\": s_size,\n                \"additional_byte_offset\": additional_byte_offset,\n                \"block_len\": channel_size,\n                \"version\": version,\n            }\n\n            if s_size < 8:\n                s_size = 8\n\n            channel = Channel(**kargs)\n            channel.name = name\n            channel.source = new_source\n\n            unit = units.get(name, b\"\")\n            if unit:\n                if hasattr(unit, \"encode\"):\n                    unit = unit.encode(\"latin-1\")\n                # conversion for time channel\n                kargs = {\n                    \"conversion_type\": v23c.CONVERSION_TYPE_NONE,\n                    \"unit\": unit,\n                    \"min_phy_value\": 0,\n                    \"max_phy_value\": 0,\n                }\n                conversion = ChannelConversion(**kargs)\n                conversion.unit = unit\n\n            gp_channels.append(channel)\n\n            offset += s_size\n\n            self.channels_db.add(name, (dg_cntr, ch_cntr))\n\n            # update the parents as well\n            field_name = field_names.get_unique_name(name)\n            parents[ch_cntr] = field_name, 0\n\n            if sig.dtype.kind == \"S\":\n                gp.string_dtypes.append(sig.dtype)\n\n            fields.append(sig)\n            types.append((field_name, sig.dtype))\n\n            ch_cntr += 1\n\n            # simple channels don't have channel dependencies\n            gp_dep.append(None)\n\n        # channel group\n        kargs = {\n            \"cycles_nr\": cycles_nr,\n            \"samples_byte_nr\": offset // 8,\n            \"ch_nr\": ch_cntr,\n        }\n        if self.version >= \"3.30\":\n            kargs[\"block_len\"] = v23c.CG_POST_330_BLOCK_SIZE\n        else:\n            kargs[\"block_len\"] = v23c.CG_PRE_330_BLOCK_SIZE\n        gp.channel_group = ChannelGroup(**kargs)\n        gp.channel_group.comment = source_info\n\n        # data group\n        if self.version >= \"3.20\":\n            block_len = v23c.DG_POST_320_BLOCK_SIZE\n        else:\n            block_len = v23c.DG_PRE_320_BLOCK_SIZE\n        gp.data_group = DataGroup(block_len=block_len)\n\n        # data block\n        types = dtype(types)\n\n        gp.types = types\n        gp.parents = parents\n        gp.sorted = True\n\n        if df.shape[0]:\n            samples = fromarrays(fields, dtype=types)\n        else:\n            samples = array([])\n\n        block = samples.tostring()\n\n        gp.data_location = v23c.LOCATION_TEMPORARY_FILE\n        if cycles_nr:\n            data_address = tell()\n            gp.data_group.data_block_addr = data_address\n            size = len(block)\n            self._tempfile.write(block)\n\n            gp.data_blocks.append(\n                DataBlockInfo(\n                    address=data_address,\n                    block_type=0,\n                    raw_size=size,\n                    size=size,\n                    param=0,\n                )\n            )\n        else:\n            gp.data_location = v23c.LOCATION_TEMPORARY_FILE\n        # data group trigger\n        gp.trigger = None", "response": "Append a new data group from a Pandas data frame."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        if self._tempfile is not None:\n            self._tempfile.close()\n        if self._file is not None and not self._from_filelike:\n            self._file.close()", "response": "Closes the temporary file and the MDF object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextending the master channel with new samples.", "response": "def extend(self, index, signals):\n        \"\"\"\n        Extend a group with new samples. *signals* contains (values, invalidation_bits)\n        pairs for each extended signal. Since MDF3 does not support invalidation\n        bits, the second item of each pair must be None. The first pair is the master channel's pair, and the\n        next pairs must respect the same order in which the signals were appended. The samples must have raw\n        or physical values according to the *Signals* used for the initial append.\n\n        Parameters\n        ----------\n        index : int\n            group index\n        signals : list\n            list of (numpy.ndarray, None) objects\n\n        Examples\n        --------\n        >>> # case 1 conversion type None\n        >>> s1 = np.array([1, 2, 3, 4, 5])\n        >>> s2 = np.array([-1, -2, -3, -4, -5])\n        >>> s3 = np.array([0.1, 0.04, 0.09, 0.16, 0.25])\n        >>> t = np.array([0.001, 0.002, 0.003, 0.004, 0.005])\n        >>> names = ['Positive', 'Negative', 'Float']\n        >>> units = ['+', '-', '.f']\n        >>> s1 = Signal(samples=s1, timstamps=t, unit='+', name='Positive')\n        >>> s2 = Signal(samples=s2, timstamps=t, unit='-', name='Negative')\n        >>> s3 = Signal(samples=s3, timstamps=t, unit='flts', name='Floats')\n        >>> mdf = MDF3('new.mdf')\n        >>> mdf.append([s1, s2, s3], 'created by asammdf v1.1.0')\n        >>> t = np.array([0.006, 0.007, 0.008, 0.009, 0.010])\n        >>> mdf2.extend(0, [(t, None), (s1.samples, None), (s2.samples, None), (s3.samples, None)])\n\n        \"\"\"\n        new_group_offset = 0\n        gp = self.groups[index]\n        if not signals:\n            message = '\"append\" requires a non-empty list of Signal objects'\n            raise MdfException(message)\n\n        if gp.data_location == v23c.LOCATION_ORIGINAL_FILE:\n            stream = self._file\n        else:\n            stream = self._tempfile\n\n        canopen_time_fields = (\"ms\", \"days\")\n        canopen_date_fields = (\n            \"ms\",\n            \"min\",\n            \"hour\",\n            \"day\",\n            \"month\",\n            \"year\",\n            \"summer_time\",\n            \"day_of_week\",\n        )\n\n        fields = []\n        types = []\n\n        cycles_nr = len(signals[0][0])\n        string_counter = 0\n\n        for signal, _ in signals:\n            sig = signal\n            names = sig.dtype.names\n\n            if len(sig.shape) <= 1:\n                if names is None:\n                    sig_type = v23c.SIGNAL_TYPE_SCALAR\n                else:\n                    if names in (canopen_time_fields, canopen_date_fields):\n                        sig_type = v23c.SIGNAL_TYPE_CANOPEN\n                    elif names[0] != sig.name:\n                        sig_type = v23c.SIGNAL_TYPE_STRUCTURE_COMPOSITION\n                    else:\n                        sig_type = v23c.SIGNAL_TYPE_ARRAY\n            else:\n                sig_type = v23c.SIGNAL_TYPE_ARRAY\n\n            if sig_type == v23c.SIGNAL_TYPE_SCALAR:\n                if signal.dtype.kind == \"S\":\n                    str_dtype = gp.string_dtypes[string_counter]\n                    signal = signal.astype(str_dtype)\n                    string_counter += 1\n                fields.append(signal)\n\n                if signal.shape[1:]:\n                    types.append((\"\", signal.dtype, signal.shape[1:]))\n                else:\n                    types.append((\"\", signal.dtype))\n\n            # second, add the composed signals\n            elif sig_type in (\n                v23c.SIGNAL_TYPE_CANOPEN,\n                v23c.SIGNAL_TYPE_STRUCTURE_COMPOSITION,\n            ):\n                new_group_offset += 1\n                new_gp = self.groups[index + new_group_offset]\n\n                new_fields = []\n                new_types = []\n\n                names = signal.dtype.names\n                for name in names:\n                    new_fields.append(signal[name])\n                    new_types.append((\"\", signal.dtype))\n\n                # data block\n                new_types = dtype(new_types)\n\n                samples = fromarrays(new_fields, dtype=new_types)\n                samples = samples.tostring()\n\n                record_size = new_gp.channel_group.samples_byte_nr\n                extended_size = cycles_nr * record_size\n\n                if samples:\n                    stream.seek(0, 2)\n                    data_address = stream.tell()\n                    stream.write(samples)\n\n                    new_gp.data_blocks.append(\n                        DataBlockInfo(\n                            address=data_address,\n                            raw_size=extended_size,\n                            size=extended_size,\n                            block_type=0,\n                            param=0,\n                        )\n                    )\n\n            else:\n\n                names = signal.dtype.names\n\n                component_samples = []\n                if names:\n                    samples = signal[names[0]]\n                else:\n                    samples = signal\n\n                shape = samples.shape[1:]\n                dims = [list(range(size)) for size in shape]\n\n                for indexes in product(*dims):\n                    subarray = samples\n                    for idx in indexes:\n                        subarray = subarray[:, idx]\n                    component_samples.append(subarray)\n\n                if names:\n                    new_samples = [signal[fld] for fld in names[1:]]\n                    component_samples.extend(new_samples)\n\n                for samples in component_samples:\n                    shape = samples.shape[1:]\n\n                    fields.append(samples)\n                    types.append((\"\", samples.dtype, shape))\n\n        record_size = gp.channel_group.samples_byte_nr\n        extended_size = cycles_nr * record_size\n\n        # data block\n        types = dtype(types)\n\n        samples = fromarrays(fields, dtype=types)\n        samples = samples.tostring()\n\n        if cycles_nr:\n            stream.seek(0, 2)\n            data_address = stream.tell()\n            stream.write(samples)\n            gp.channel_group.cycles_nr += cycles_nr\n\n            gp.data_blocks.append(\n                DataBlockInfo(\n                    address=data_address,\n                    block_type=0,\n                    raw_size=extended_size,\n                    size=extended_size,\n                    param=0,\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_channel_comment(self, name=None, group=None, index=None):\n        gp_nr, ch_nr = self._validate_channel_selection(name, group, index)\n\n        grp = self.groups[gp_nr]\n        if grp.data_location == v23c.LOCATION_ORIGINAL_FILE:\n            stream = self._file\n        else:\n            stream = self._tempfile\n\n        channel = grp.channels[ch_nr]\n\n        return channel.comment", "response": "Gets the comment of a specific channel in a specific group."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the object containing the channel samples.", "response": "def get(\n        self,\n        name=None,\n        group=None,\n        index=None,\n        raster=None,\n        samples_only=False,\n        data=None,\n        raw=False,\n        ignore_invalidation_bits=False,\n        source=None,\n        record_offset=0,\n        record_count=None,\n        copy_master=True,\n    ):\n        \"\"\"Gets channel samples.\n        Channel can be specified in two ways:\n\n        * using the first positional argument *name*\n\n            * if *source* is given this will be first used to validate the\n              channel selection\n            * if there are multiple occurances for this channel then the\n              *group* and *index* arguments can be used to select a specific\n              group.\n            * if there are multiple occurances for this channel and either the\n              *group* or *index* arguments is None then a warning is issued\n\n        * using the group number (keyword argument *group*) and the channel\n          number (keyword argument *index*). Use *info* method for group and\n          channel numbers\n\n\n        If the *raster* keyword argument is not *None* the output is\n        interpolated accordingly.\n\n        Parameters\n        ----------\n        name : string\n            name of channel\n        group : int\n            0-based group index\n        index : int\n            0-based channel index\n        raster : float\n            time raster in seconds\n        samples_only : bool\n            if *True* return only the channel samples as numpy array; if\n            *False* return a *Signal* object\n        data : bytes\n            prevent redundant data read by providing the raw data group samples\n        raw : bool\n            return channel samples without appling the conversion rule; default\n            `False`\n        ignore_invalidation_bits : bool\n            only defined to have the same API with the MDF v4\n        source : str\n            source name used to select the channel\n        record_offset : int\n            if *data=None* use this to select the record offset from which the\n            group data should be loaded\n        copy_master : bool\n            make a copy of the timebase for this channel\n\n\n        Returns\n        -------\n        res : (numpy.array, None) | Signal\n            returns *Signal* if *samples_only*=*False* (default option),\n            otherwise returns a (numpy.array, None) tuple (for compatibility\n            with MDF v4 class.\n\n            The *Signal* samples are\n\n                * numpy recarray for channels that have CDBLOCK or BYTEARRAY\n                  type channels\n                * numpy array for all the rest\n\n        Raises\n        ------\n        MdfException :\n\n        * if the channel name is not found\n        * if the group index is out of range\n        * if the channel index is out of range\n\n        Examples\n        --------\n        >>> from asammdf import MDF, Signal\n        >>> import numpy as np\n        >>> t = np.arange(5)\n        >>> s = np.ones(5)\n        >>> mdf = MDF(version='3.30')\n        >>> for i in range(4):\n        ...     sigs = [Signal(s*(i*10+j), t, name='Sig') for j in range(1, 4)]\n        ...     mdf.append(sigs)\n        ...\n        >>> # first group and channel index of the specified channel name\n        ...\n        >>> mdf.get('Sig')\n        UserWarning: Multiple occurances for channel \"Sig\". Using first occurance from data group 4. Provide both \"group\" and \"index\" arguments to select another data group\n        <Signal Sig:\n                samples=[ 1.  1.  1.  1.  1.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        >>> # first channel index in the specified group\n        ...\n        >>> mdf.get('Sig', 1)\n        <Signal Sig:\n                samples=[ 11.  11.  11.  11.  11.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        >>> # channel named Sig from group 1 channel index 2\n        ...\n        >>> mdf.get('Sig', 1, 2)\n        <Signal Sig:\n                samples=[ 12.  12.  12.  12.  12.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        >>> # channel index 1 or group 2\n        ...\n        >>> mdf.get(None, 2, 1)\n        <Signal Sig:\n                samples=[ 21.  21.  21.  21.  21.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        >>> mdf.get(group=2, index=1)\n        <Signal Sig:\n                samples=[ 21.  21.  21.  21.  21.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n        >>> mdf.get('Sig', source='VN7060')\n        <Signal Sig:\n                samples=[ 12.  12.  12.  12.  12.]\n                timestamps=[0 1 2 3 4]\n                unit=\"\"\n                info=None\n                comment=\"\">\n\n        \"\"\"\n\n        gp_nr, ch_nr = self._validate_channel_selection(\n            name, group, index, source=source\n        )\n\n        original_data = data\n\n        grp = self.groups[gp_nr]\n\n        if grp.data_location == v23c.LOCATION_ORIGINAL_FILE:\n            stream = self._file\n        else:\n            stream = self._tempfile\n\n        channel = grp.channels[ch_nr]\n\n        conversion = channel.conversion\n        name = channel.name\n        display_name = channel.display_name\n\n        bit_count = channel.bit_count or 64\n\n        dep = grp.channel_dependencies[ch_nr]\n        cycles_nr = grp.channel_group.cycles_nr\n\n        encoding = None\n\n        # get data group record\n        if data is None:\n            data = self._load_data(\n                grp, record_offset=record_offset, record_count=record_count\n            )\n        else:\n            data = (data,)\n\n        # check if this is a channel array\n        if dep:\n            if dep.dependency_type == v23c.DEPENDENCY_TYPE_VECTOR:\n                shape = [dep.sd_nr]\n            elif dep.dependency_type >= v23c.DEPENDENCY_TYPE_NDIM:\n                shape = []\n                i = 0\n                while True:\n                    try:\n                        dim = dep[f\"dim_{i}\"]\n                        shape.append(dim)\n                        i += 1\n                    except KeyError:\n                        break\n                shape = shape[::-1]\n\n            record_shape = tuple(shape)\n\n            arrays = [\n                self.get(\n                    group=dg_nr,\n                    index=ch_nr,\n                    samples_only=True,\n                    raw=raw,\n                    data=original_data,\n                    record_offset=record_offset,\n                    record_count=record_count,\n                )[0]\n                for dg_nr, ch_nr in dep.referenced_channels\n            ]\n            shape.insert(0, cycles_nr)\n\n            vals = column_stack(arrays).flatten().reshape(tuple(shape))\n\n            arrays = [vals]\n            types = [(channel.name, vals.dtype, record_shape)]\n\n            types = dtype(types)\n            vals = fromarrays(arrays, dtype=types)\n\n            if not samples_only or raster:\n                timestamps = self.get_master(\n                    gp_nr,\n                    original_data,\n                    record_offset=record_offset,\n                    record_count=record_count,\n                    copy_master=copy_master,\n                )\n                if raster and len(timestamps) > 1:\n                    num = float(float32((timestamps[-1] - timestamps[0]) / raster))\n                    if num.is_integer():\n                        t = linspace(timestamps[0], timestamps[-1], int(num))\n                    else:\n                        t = arange(timestamps[0], timestamps[-1], raster)\n\n                    vals = (\n                        Signal(vals, timestamps, name=\"_\")\n                        .interp(t, interpolation_mode=self._integer_interpolation)\n                        .samples\n                    )\n\n                    timestamps = t\n\n        else:\n            # get channel values\n            channel_values = []\n            timestamps = []\n            count = 0\n            for fragment in data:\n                data_bytes, _offset, _count = fragment\n                parents, dtypes = self._prepare_record(grp)\n\n                try:\n                    parent, bit_offset = parents[ch_nr]\n                except KeyError:\n                    parent, bit_offset = None, None\n\n                bits = channel.bit_count\n\n                if parent is not None:\n                    if grp.record is None:\n                        if dtypes.itemsize:\n                            record = fromstring(data_bytes, dtype=dtypes)\n                        else:\n                            record = None\n                    else:\n                        record = grp.record\n\n                    record.setflags(write=False)\n\n                    vals = record[parent]\n                    data_type = channel.data_type\n                    size = vals.dtype.itemsize\n                    if data_type == v23c.DATA_TYPE_BYTEARRAY:\n                        size *= vals.shape[1]\n\n                    vals_dtype = vals.dtype.kind\n                    if vals_dtype not in \"ui\" and (bit_offset or not bits == size * 8):\n                        vals = self._get_not_byte_aligned_data(data_bytes, grp, ch_nr)\n                    else:\n                        dtype_ = vals.dtype\n                        kind_ = dtype_.kind\n\n                        if data_type in v23c.INT_TYPES:\n                            if kind_ == 'f':\n                                if bits != size * 8:\n                                    vals = self._get_not_byte_aligned_data(\n                                        data_bytes, grp, ch_nr\n                                    )\n                                else:\n                                    dtype_fmt = get_fmt_v3(data_type, bits)\n                                    channel_dtype = dtype(dtype_fmt.split(')')[-1])\n                                    vals = vals.view(channel_dtype)\n                            else:\n\n                                if dtype_.byteorder == \">\":\n                                    if bit_offset or bits != size << 3:\n                                        vals = self._get_not_byte_aligned_data(\n                                            data_bytes, grp, ch_nr\n                                        )\n                                else:\n                                    if bit_offset:\n                                        if dtype_.kind == \"i\":\n                                            vals = vals.astype(\n                                                dtype(f\"{dtype_.byteorder}u{size}\")\n                                            )\n                                            vals >>= bit_offset\n                                        else:\n                                            vals = vals >> bit_offset\n\n                                    if bits != size << 3:\n                                        if data_type in v23c.SIGNED_INT:\n                                            vals = as_non_byte_sized_signed_int(vals, bits)\n                                        else:\n                                            mask = (1 << bits) - 1\n                                            if vals.flags.writeable:\n                                                vals &= mask\n                                            else:\n                                                vals = vals & mask\n                        else:\n                            if bits != size * 8:\n                                vals = self._get_not_byte_aligned_data(\n                                    data_bytes, grp, ch_nr\n                                )\n                            else:\n                                if kind_ in \"ui\":\n                                    dtype_fmt = get_fmt_v3(data_type, bits)\n                                    channel_dtype = dtype(dtype_fmt.split(')')[-1])\n                                    vals = vals.view(channel_dtype)\n\n                else:\n                    vals = self._get_not_byte_aligned_data(data_bytes, grp, ch_nr)\n\n                if not samples_only or raster:\n                    timestamps.append(\n                        self.get_master(gp_nr, fragment, copy_master=copy_master)\n                    )\n\n                if bits == 1 and self._single_bit_uint_as_bool:\n                    vals = array(vals, dtype=bool)\n                else:\n                    data_type = channel.data_type\n                    channel_dtype = array([], dtype=get_fmt_v3(data_type, bits))\n                    if vals.dtype != channel_dtype.dtype:\n                        vals = vals.astype(channel_dtype.dtype)\n\n                channel_values.append(vals.copy())\n                count += 1\n\n            if count > 1:\n                vals = concatenate(channel_values)\n            elif count == 1:\n                vals = channel_values[0]\n            else:\n                vals = []\n\n            if not samples_only or raster:\n                if count > 1:\n                    timestamps = concatenate(timestamps)\n                else:\n                    timestamps = timestamps[0]\n\n                if raster and len(timestamps) > 1:\n                    num = float(float32((timestamps[-1] - timestamps[0]) / raster))\n                    if num.is_integer():\n                        t = linspace(timestamps[0], timestamps[-1], int(num))\n                    else:\n                        t = arange(timestamps[0], timestamps[-1], raster)\n\n                    vals = (\n                        Signal(vals, timestamps, name=\"_\")\n                        .interp(t, interpolation_mode=self._integer_interpolation)\n                        .samples\n                    )\n\n                    timestamps = t\n\n            if conversion is None:\n                conversion_type = v23c.CONVERSION_TYPE_NONE\n            else:\n                conversion_type = conversion.conversion_type\n\n            if conversion_type == v23c.CONVERSION_TYPE_NONE:\n                if vals.dtype.kind == \"S\":\n                    encoding = \"latin-1\"\n\n            if not raw:\n                if conversion:\n                    vals = conversion.convert(vals)\n                    conversion = None\n\n        if samples_only:\n            res = vals, None\n        else:\n            if conversion:\n                unit = conversion.unit\n            else:\n                unit = \"\"\n\n            comment = channel.comment\n\n            description = channel.description.decode(\"latin-1\").strip(\" \\t\\n\\0\")\n            if comment:\n                comment = f\"{comment}\\n{description}\"\n            else:\n                comment = description\n\n            source = channel.source\n\n            if source:\n                if source[\"type\"] == v23c.SOURCE_ECU:\n                    source = SignalSource(\n                        source.name,\n                        source.path,\n                        source.comment,\n                        0,  # source type other\n                        0,  # bus type none\n                    )\n                else:\n                    source = SignalSource(\n                        source.name,\n                        source.path,\n                        source.comment,\n                        2,  # source type bus\n                        2,  # bus type CAN\n                    )\n\n            master_metadata = self._master_channel_metadata.get(gp_nr, None)\n\n            res = Signal(\n                samples=vals,\n                timestamps=timestamps,\n                unit=unit,\n                name=channel.name,\n                comment=comment,\n                conversion=conversion,\n                raw=raw,\n                master_metadata=master_metadata,\n                display_name=display_name,\n                source=source,\n                bit_count=bit_count,\n                encoding=encoding,\n            )\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns master channel samples for given group index", "response": "def get_master(\n        self,\n        index,\n        data=None,\n        raster=None,\n        record_offset=0,\n        record_count=None,\n        copy_master=True,\n    ):\n        \"\"\" returns master channel samples for given group\n\n        Parameters\n        ----------\n        index : int\n            group index\n        data : (bytes, int)\n            (data block raw bytes, fragment offset); default None\n        raster : float\n            raster to be used for interpolation; default None\n        record_offset : int\n            if *data=None* use this to select the record offset from which the\n            group data should be loaded\n\n        Returns\n        -------\n        t : numpy.array\n            master channel samples\n\n        \"\"\"\n        original_data = data\n\n        fragment = data\n        if fragment:\n            data_bytes, offset, _count = fragment\n            try:\n                timestamps = self._master_channel_cache[(index, offset, _count)]\n                if raster and timestamps:\n                    timestamps = arange(timestamps[0], timestamps[-1], raster)\n                    return timestamps\n                else:\n                    if copy_master:\n                        return timestamps.copy()\n                    else:\n                        return timestamps\n            except KeyError:\n                pass\n        else:\n            try:\n                timestamps = self._master_channel_cache[index]\n                if raster and timestamps:\n                    timestamps = arange(timestamps[0], timestamps[-1], raster)\n                    return timestamps\n                else:\n                    if copy_master:\n                        return timestamps.copy()\n                    else:\n                        return timestamps\n            except KeyError:\n                pass\n\n        group = self.groups[index]\n\n        if group.data_location == v23c.LOCATION_ORIGINAL_FILE:\n            stream = self._file\n        else:\n            stream = self._tempfile\n\n        time_ch_nr = self.masters_db.get(index, None)\n        cycles_nr = group.channel_group.cycles_nr\n\n        if time_ch_nr is None:\n            t = arange(cycles_nr, dtype=float64)\n            metadata = (\"time\", 1)\n        else:\n            time_ch = group.channels[time_ch_nr]\n\n            metadata = (time_ch.name, 1)\n\n            if time_ch.bit_count == 0:\n                if time_ch.sampling_rate:\n                    sampling_rate = time_ch.sampling_rate\n                else:\n                    sampling_rate = 1\n                t = arange(cycles_nr, dtype=float64) * sampling_rate\n            else:\n                # get data group parents and dtypes\n                parents, dtypes = self._prepare_record(group)\n\n                # get data group record\n                if data is None:\n                    data = self._load_data(\n                        group, record_offset=record_offset, record_count=record_count\n                    )\n                    _count = record_count\n                else:\n                    data = (data,)\n\n                time_values = []\n                count = 0\n                for fragment in data:\n                    data_bytes, offset, _count = fragment\n                    parent, _ = parents.get(time_ch_nr, (None, None))\n                    if parent is not None:\n                        if group.record is None:\n                            if dtypes.itemsize:\n                                record = fromstring(data_bytes, dtype=dtypes)\n                            else:\n                                record = None\n                        else:\n                            record = group.record\n                        record.setflags(write=False)\n                        t = record[parent]\n                    else:\n                        t = self._get_not_byte_aligned_data(\n                            data_bytes, group, time_ch_nr\n                        )\n                    time_values.append(t.copy())\n                    count += 1\n\n                if count > 1:\n                    t = concatenate(time_values)\n                elif count == 1:\n                    t = time_values[0]\n                else:\n                    t = array([], dtype=float64)\n\n                # get timestamps\n                conversion = time_ch.conversion\n                if conversion is None:\n                    time_conv_type = v23c.CONVERSION_TYPE_NONE\n                else:\n                    time_conv_type = conversion.conversion_type\n                if time_conv_type == v23c.CONVERSION_TYPE_LINEAR:\n                    time_a = conversion.a\n                    time_b = conversion.b\n                    t = t * time_a\n                    if time_b:\n                        t += time_b\n\n        if not t.dtype == float64:\n            t = t.astype(float64)\n\n        self._master_channel_metadata[index] = metadata\n\n        if original_data is None:\n            self._master_channel_cache[index] = t\n        else:\n            data_bytes, offset, _count = original_data\n            self._master_channel_cache[(index, offset, _count)] = t\n\n        if raster:\n            timestamps = t\n            if len(t) > 1:\n                num = float(float32((timestamps[-1] - timestamps[0]) / raster))\n                if int(num) == num:\n                    timestamps = linspace(t[0], t[-1], int(num))\n                else:\n                    timestamps = arange(t[0], t[-1], raster)\n        else:\n            timestamps = t\n        if copy_master:\n            return timestamps.copy()\n        else:\n            return timestamps"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_get_triggers(self):\n        for i, gp in enumerate(self.groups):\n            trigger = gp.trigger\n            if trigger:\n\n                for j in range(trigger[\"trigger_events_nr\"]):\n                    trigger_info = {\n                        \"comment\": trigger.comment,\n                        \"index\": j,\n                        \"group\": i,\n                        \"time\": trigger[f\"trigger_{j}_time\"],\n                        \"pre_time\": trigger[f\"trigger_{j}_pretime\"],\n                        \"post_time\": trigger[f\"trigger_{j}_posttime\"],\n                    }\n                    yield trigger_info", "response": "generator that yields the dict containing the information of all triggers in the cluster"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget MDF information as a dict", "response": "def info(self):\n        \"\"\"get MDF information as a dict\n\n        Examples\n        --------\n        >>> mdf = MDF3('test.mdf')\n        >>> mdf.info()\n\n        \"\"\"\n        info = {}\n        for key in (\"author\", \"department\", \"project\", \"subject\"):\n            value = self.header[key].decode(\"latin-1\").strip(\" \\n\\t\\0\")\n            info[key] = value\n        info[\"version\"] = self.version\n        info[\"groups\"] = len(self.groups)\n        for i, gp in enumerate(self.groups):\n            if gp.data_location == v23c.LOCATION_ORIGINAL_FILE:\n                stream = self._file\n            elif gp.data_location == v23c.LOCATION_TEMPORARY_FILE:\n                stream = self._tempfile\n            inf = {}\n            info[f\"group {i}\"] = inf\n            inf[\"cycles\"] = gp.channel_group.cycles_nr\n            inf[\"comment\"] = gp.channel_group.comment\n            inf[\"channels count\"] = len(gp.channels)\n            for j, channel in enumerate(gp.channels):\n                name = channel.name\n\n                if channel.channel_type == v23c.CHANNEL_TYPE_MASTER:\n                    ch_type = \"master\"\n                else:\n                    ch_type = \"value\"\n                inf[f\"channel {j}\"] = f'name=\"{name}\" type={ch_type}'\n\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save(self, dst, overwrite=False, compression=0):\n\n        dst = Path(dst).with_suffix(\".mdf\")\n\n        destination_dir = dst.parent\n        destination_dir.mkdir(parents=True, exist_ok=True)\n\n        if overwrite is False:\n            if dst.is_file():\n                cntr = 0\n                while True:\n                    name = dst.with_suffix(f\".{cntr}.mdf\")\n                    if not name.exists():\n                        break\n                    else:\n                        cntr += 1\n                message = (\n                    f'Destination file \"{dst}\" already exists '\n                    f'and \"overwrite\" is False. Saving MDF file as \"{name}\"'\n                )\n                logger.warning(message)\n                dst = name\n\n        if not self.header.comment:\n            self.header.comment = \"\"\"<FHcomment>\n<TX>created</TX>\n<tool_id>asammdf</tool_id>\n<tool_vendor> </tool_vendor>\n<tool_version>{__version__}</tool_version>\n</FHcomment>\"\"\"\n        else:\n\n            old_history = self.header.comment\n            timestamp = time.asctime()\n\n            text = f\"{old_history}\\n{timestamp}: updated by asammdf {__version__}\"\n            self.header.comment = text\n\n        defined_texts, cc_map, si_map = {}, {}, {}\n\n        if dst == self.name:\n            destination = dst.with_suffix(\".savetemp\")\n        else:\n            destination = dst\n\n        with open(destination, \"wb+\") as dst_:\n\n            groups_nr = len(self.groups)\n\n            write = dst_.write\n            seek = dst_.seek\n            # list of all blocks\n            blocks = []\n\n            address = 0\n\n            write(bytes(self.identification))\n            address += v23c.ID_BLOCK_SIZE\n\n            write(bytes(self.header))\n            address += self.header.block_len\n\n            if self.header.program:\n                write(bytes(self.header.program))\n                self.header.program_addr = address\n                address += self.header.program.block_len\n            else:\n                self.header.program_addr = 0\n\n            comment = TextBlock(text=self.header.comment)\n            write(bytes(comment))\n            self.header.comment_addr = address\n            address += comment.block_len\n\n            # DataGroup\n            # put them first in the block list so they will be written first to\n            # disk this way, in case of memory=False, we can safely\n            # restore he original data block address\n            gp_rec_ids = []\n\n            original_data_block_addrs = [\n                group.data_group.data_block_addr for group in self.groups\n            ]\n\n            for idx, gp in enumerate(self.groups):\n                dg = gp.data_group\n                gp_rec_ids.append(dg.record_id_len)\n                dg.record_id_len = 0\n\n                # DataBlock\n                dim = 0\n                for (data_bytes, _, __) in self._load_data(gp):\n                    dim += len(data_bytes)\n                    write(data_bytes)\n\n                if gp.data_blocks:\n                    gp.data_group.data_block_addr = address\n                else:\n                    gp.data_group.data_block_addr = 0\n                address += dim\n\n                if self._callback:\n                    self._callback(int(33 * (idx + 1) / groups_nr), 100)\n                if self._terminate:\n                    dst_.close()\n                    self.close()\n                    return\n\n            for gp in self.groups:\n                dg = gp.data_group\n                blocks.append(dg)\n                dg.address = address\n                address += dg.block_len\n\n            if self.groups:\n                for i, dg in enumerate(self.groups[:-1]):\n                    addr = self.groups[i + 1].data_group.address\n                    dg.data_group.next_dg_addr = addr\n                self.groups[-1].data_group.next_dg_addr = 0\n\n            for idx, gp in enumerate(self.groups):\n                # Channel Dependency\n                cd = gp.channel_dependencies\n                for dep in cd:\n                    if dep:\n                        dep.address = address\n                        blocks.append(dep)\n                        address += dep.block_len\n\n                for channel, dep in zip(gp.channels, gp.channel_dependencies):\n                    if dep:\n                        channel.component_addr = dep.address = address\n                        blocks.append(dep)\n                        address += dep.block_len\n                    else:\n                        channel.component_addr = 0\n                    address = channel.to_blocks(\n                        address, blocks, defined_texts, cc_map, si_map\n                    )\n\n                count = len(gp.channels)\n                if count:\n                    for i in range(count - 1):\n                        gp.channels[i].next_ch_addr = gp.channels[i + 1].address\n                    gp.channels[-1].next_ch_addr = 0\n\n                # ChannelGroup\n                cg = gp.channel_group\n                if gp.channels:\n                    cg.first_ch_addr = gp.channels[0].address\n                else:\n                    cg.first_ch_addr = 0\n                cg.next_cg_addr = 0\n                address = cg.to_blocks(address, blocks, defined_texts, si_map)\n\n                # TriggerBLock\n                trigger = gp.trigger\n                if trigger:\n                    address = trigger.to_blocks(address, blocks)\n\n                if self._callback:\n                    self._callback(int(33 * (idx + 1) / groups_nr) + 33, 100)\n                if self._terminate:\n                    dst_.close()\n                    self.close()\n                    return\n\n            # update referenced channels addresses in the channel dependecies\n            for gp in self.groups:\n                for dep in gp.channel_dependencies:\n                    if not dep:\n                        continue\n\n                    for i, pair_ in enumerate(dep.referenced_channels):\n                        dg_nr, ch_nr = pair_\n                        grp = self.groups[dg_nr]\n                        ch = grp.channels[ch_nr]\n                        dep[f\"ch_{i}\"] = ch.address\n                        dep[f\"cg_{i}\"] = grp.channel_group.address\n                        dep[f\"dg_{i}\"] = grp.data_group.address\n\n            # DataGroup\n            for gp in self.groups:\n                gp.data_group.first_cg_addr = gp.channel_group.address\n                if gp.trigger:\n                    gp.data_group.trigger_addr = gp.trigger.address\n                else:\n                    gp.data_group.trigger_addr = 0\n\n            if self.groups:\n                address = self.groups[0].data_group.address\n                self.header.first_dg_addr = address\n                self.header.dg_nr = len(self.groups)\n\n            if self._terminate:\n                dst_.close()\n                self.close()\n                return\n\n            if self._callback:\n                blocks_nr = len(blocks)\n                threshold = blocks_nr / 33\n                count = 1\n                for i, block in enumerate(blocks):\n                    write(bytes(block))\n                    if i >= threshold:\n                        self._callback(66 + count, 100)\n                        count += 1\n                        threshold += blocks_nr / 33\n            else:\n                for block in blocks:\n                    write(bytes(block))\n\n            for gp, rec_id, original_address in zip(\n                self.groups, gp_rec_ids, original_data_block_addrs\n            ):\n                gp.data_group.record_id_len = rec_id\n                gp.data_group.data_block_addr = original_address\n\n            seek(0)\n            write(bytes(self.identification))\n            write(bytes(self.header))\n\n        if dst == self.name:\n            self.close()\n            Path.unlink(self.name)\n            Path.rename(destination, self.name)\n\n            self.groups.clear()\n            self.header = None\n            self.identification = None\n            self.channels_db.clear()\n            self.masters_db.clear()\n\n            self._master_channel_cache.clear()\n\n            self._tempfile = TemporaryFile()\n            self._file = open(self.name, \"rb\")\n            self._read()\n\n        if self._callback:\n            self._callback(100, 100)\n\n        return dst", "response": "Save the MDF to dst."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef excepthook(exc_type, exc_value, tracebackobj):\n    separator = \"-\" * 80\n    notice = \"The following error was triggered:\"\n\n    now = datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\")\n\n    info = StringIO()\n    traceback.print_tb(tracebackobj, None, info)\n    info.seek(0)\n    info = info.read()\n\n    errmsg = f\"{exc_type}\\t \\n{exc_value}\"\n    sections = [now, separator, errmsg, separator, info]\n    msg = \"\\n\".join(sections)\n\n    print(''.join(traceback.format_tb(tracebackobj)))\n    print('{0}: {1}'.format(exc_type, exc_value))\n\n    QtWidgets.QMessageBox.warning(None, notice, traceback.format_exc())", "response": "Function to catch unhandled exceptions and print a message to the console."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the TENANT_TOKEN from a particular string", "response": "def clean_tenant_url(url_string):\n    \"\"\"\n    Removes the TENANT_TOKEN from a particular string\n    \"\"\"\n    if hasattr(settings, 'PUBLIC_SCHEMA_URLCONF'):\n        if (settings.PUBLIC_SCHEMA_URLCONF and\n                url_string.startswith(settings.PUBLIC_SCHEMA_URLCONF)):\n            url_string = url_string[len(settings.PUBLIC_SCHEMA_URLCONF):]\n    return url_string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef app_labels(apps_list):\n    if AppConfig is None:\n        return [app.split('.')[-1] for app in apps_list]\n    return [AppConfig.create(app).label for app in apps_list]", "response": "Returns a list of app labels of the given apps_list now properly handles\n     new Django 1. 7 + application registry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef path(self, name):\n        if name is None:\n            name = ''\n        try:\n            location = safe_join(self.location, connection.tenant.domain_url)\n        except AttributeError:\n            location = self.location\n        try:\n            path = safe_join(location, name)\n        except ValueError:\n            raise SuspiciousOperation(\n                \"Attempted access to '%s' denied.\" % name)\n        return os.path.normpath(path)", "response": "Return the path to the named file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary of all relationships to the given table.", "response": "def get_relations(self, cursor, table_name):\n        \"\"\"\n        Returns a dictionary of {field_name: (field_name_other_table, other_table)}\n        representing all relationships to the given table.\n        \"\"\"\n        cursor.execute(self._get_relations_query, {\n            'schema': self.connection.schema_name,\n            'table': table_name\n        })\n        relations = {}\n        for row in cursor.fetchall():\n            relations[row[1]] = (row[2], row[0])\n\n        return relations"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges the option_list to use the options from the wrapped command. Adds schema parameter to specify which schema will be used when executing the wrapped command.", "response": "def run_from_argv(self, argv):\n        \"\"\"\n        Changes the option_list to use the options from the wrapped command.\n        Adds schema parameter to specify which schema will be used when\n        executing the wrapped command.\n        \"\"\"\n        # load the command object.\n        try:\n            app_name = get_commands()[argv[2]]\n        except KeyError:\n            raise CommandError(\"Unknown command: %r\" % argv[2])\n\n        if isinstance(app_name, BaseCommand):\n            # if the command is already loaded, use it directly.\n            klass = app_name\n        else:\n            klass = load_command_class(app_name, argv[2])\n\n        # Ugly, but works. Delete tenant_command from the argv, parse the schema manually\n        # and forward the rest of the arguments to the actual command being wrapped.\n        del argv[1]\n        schema_parser = argparse.ArgumentParser()\n        schema_parser.add_argument(\"-s\", \"--schema\", dest=\"schema_name\", help=\"specify tenant schema\")\n        schema_namespace, args = schema_parser.parse_known_args(argv)\n\n        tenant = self.get_tenant_from_options_or_interactive(schema_name=schema_namespace.schema_name)\n        connection.set_tenant(tenant)\n        klass.run_from_argv(args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate a command over all registered schemata.", "response": "def handle(self, *args, **options):\n        \"\"\"\n        Iterates a command over all registered schemata.\n        \"\"\"\n        if options['schema_name']:\n            # only run on a particular schema\n            connection.set_schema_to_public()\n            self.execute_command(get_tenant_model().objects.get(schema_name=options['schema_name']), self.COMMAND_NAME,\n                                 *args, **options)\n        else:\n            for tenant in get_tenant_model().objects.all():\n                if not (options['skip_public'] and tenant.schema_name == get_public_schema_name()):\n                    self.execute_command(tenant, self.COMMAND_NAME, *args, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_template_sources(template_name, template_dirs=None):\n        if not connection.tenant or isinstance(connection.tenant, FakeTenant):\n            return\n        if not template_dirs:\n            try:\n                template_dirs = settings.MULTITENANT_TEMPLATE_DIRS\n            except AttributeError:\n                raise ImproperlyConfigured('To use %s.%s you must define the MULTITENANT_TEMPLATE_DIRS' %\n                                           (__name__, FilesystemLoader.__name__))\n        for template_dir in template_dirs:\n            try:\n                if '%s' in template_dir:\n                    yield safe_join(template_dir % connection.tenant.domain_url, template_name)\n                else:\n                    yield safe_join(template_dir, connection.tenant.domain_url, template_name)\n            except UnicodeDecodeError:\n                # The template dir name was a bytestring that wasn't valid UTF-8.\n                raise\n            except ValueError:\n                # The joined path was located outside of this particular\n                # template_dir (it might be inside another one, so this isn't\n                # fatal).\n                pass", "response": "Yields the absolute paths to template_name when appended to each\n        directory in template_dirs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_schema(self, schema_name, include_public=True):\n        self.tenant = FakeTenant(schema_name=schema_name)\n        self.schema_name = schema_name\n        self.include_public_schema = include_public\n        self.set_settings_schema(schema_name)\n        self.search_path_set = False\n        # Content type can no longer be cached as public and tenant schemas\n        # have different models. If someone wants to change this, the cache\n        # needs to be separated between public and shared schemas. If this\n        # cache isn't cleared, this can cause permission problems. For example,\n        # on public, a particular model has id 14, but on the tenants it has\n        # the id 15. if 14 is cached instead of 15, the permissions for the\n        # wrong model will be fetched.\n        ContentType.objects.clear_cache()", "response": "This method sets the schema of the current database entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntests for configuration recommendations.", "response": "def best_practice(app_configs, **kwargs):\n    \"\"\"\n    Test for configuration recommendations. These are best practices, they\n    avoid hard to find bugs and unexpected behaviour.\n    \"\"\"\n    if app_configs is None:\n        app_configs = apps.get_app_configs()\n\n    # Take the app_configs and turn them into *old style* application names.\n    # This is what we expect in the SHARED_APPS and TENANT_APPS settings.\n    INSTALLED_APPS = [\n        config.name\n        for config in app_configs\n    ]\n\n    if not hasattr(settings, 'TENANT_APPS'):\n        return [Critical('TENANT_APPS setting not set')]\n\n    if not hasattr(settings, 'TENANT_MODEL'):\n        return [Critical('TENANT_MODEL setting not set')]\n\n    if not hasattr(settings, 'SHARED_APPS'):\n        return [Critical('SHARED_APPS setting not set')]\n\n    if 'tenant_schemas.routers.TenantSyncRouter' not in settings.DATABASE_ROUTERS:\n        return [\n            Critical(\"DATABASE_ROUTERS setting must contain \"\n                     \"'tenant_schemas.routers.TenantSyncRouter'.\")\n        ]\n\n    errors = []\n\n    django_index = next(i for i, s in enumerate(INSTALLED_APPS) if s.startswith('django.'))\n    if INSTALLED_APPS.index('tenant_schemas') > django_index:\n        errors.append(\n            Warning(\"You should put 'tenant_schemas' before any django \"\n                    \"core applications in INSTALLED_APPS.\",\n                    obj=\"django.conf.settings\",\n                    hint=\"This is necessary to overwrite built-in django \"\n                         \"management commands with their schema-aware \"\n                         \"implementations.\",\n                    id=\"tenant_schemas.W001\"))\n\n    if not settings.TENANT_APPS:\n        errors.append(\n            Error(\"TENANT_APPS is empty.\",\n                  hint=\"Maybe you don't need this app?\",\n                  id=\"tenant_schemas.E001\"))\n\n    if hasattr(settings, 'PG_EXTRA_SEARCH_PATHS'):\n        if get_public_schema_name() in settings.PG_EXTRA_SEARCH_PATHS:\n            errors.append(Critical(\n                \"%s can not be included on PG_EXTRA_SEARCH_PATHS.\"\n                % get_public_schema_name()))\n\n        # make sure no tenant schema is in settings.PG_EXTRA_SEARCH_PATHS\n        invalid_schemas = set(settings.PG_EXTRA_SEARCH_PATHS).intersection(\n            get_tenant_model().objects.all().values_list('schema_name', flat=True))\n        if invalid_schemas:\n            errors.append(Critical(\n                \"Do not include tenant schemas (%s) on PG_EXTRA_SEARCH_PATHS.\"\n                % \", \".join(sorted(invalid_schemas))))\n\n    if not settings.SHARED_APPS:\n        errors.append(\n            Warning(\"SHARED_APPS is empty.\",\n                    id=\"tenant_schemas.W002\"))\n\n    if not set(settings.TENANT_APPS).issubset(INSTALLED_APPS):\n        delta = set(settings.TENANT_APPS).difference(INSTALLED_APPS)\n        errors.append(\n            Error(\"You have TENANT_APPS that are not in INSTALLED_APPS\",\n                  hint=[a for a in settings.TENANT_APPS if a in delta],\n                  id=\"tenant_schemas.E002\"))\n\n    if not set(settings.SHARED_APPS).issubset(INSTALLED_APPS):\n        delta = set(settings.SHARED_APPS).difference(INSTALLED_APPS)\n        errors.append(\n            Error(\"You have SHARED_APPS that are not in INSTALLED_APPS\",\n                  hint=[a for a in settings.SHARED_APPS if a in delta],\n                  id=\"tenant_schemas.E003\"))\n\n    if not isinstance(default_storage, TenantStorageMixin):\n        errors.append(Warning(\n            \"Your default storage engine is not tenant aware.\",\n            hint=\"Set settings.DEFAULT_FILE_STORAGE to \"\n                 \"'tenant_schemas.storage.TenantFileSystemStorage'\",\n            id=\"tenant_schemas.W003\"\n        ))\n\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the items in the list and return the number of items deleted.", "response": "def delete(self):\n        \"\"\"\n        Make sure we call the delete method of each object in the queryset so\n        that safety checks and schema deletion (if requested) are executed\n        even when using bulk delete.\n        \"\"\"\n        counter, counter_dict = 0, {}\n        for obj in self:\n            result = obj.delete()\n            if result is not None:\n                current_counter, current_counter_dict = result\n                counter += current_counter\n                counter_dict.update(current_counter_dict)\n        if counter:\n            return counter, counter_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self, force_drop=False, *args, **kwargs):\n        if connection.schema_name not in (self.schema_name, get_public_schema_name()):\n            raise Exception(\"Can't delete tenant outside it's own schema or \"\n                            \"the public schema. Current schema is %s.\"\n                            % connection.schema_name)\n\n        if schema_exists(self.schema_name) and (self.auto_drop_schema or force_drop):\n            cursor = connection.cursor()\n            cursor.execute('DROP SCHEMA IF EXISTS %s CASCADE' % self.schema_name)\n\n        return super(TenantMixin, self).delete(*args, **kwargs)", "response": "Deletes this row. Drops the tenant's schema if the attribute\n        auto_drop_schema set to True."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a short string representation of the object.", "response": "def short_repr(obj, max_len=40):\n  \"\"\"Returns a short, term-friendly string representation of the object.\n\n  Args:\n    obj: An object for which to return a string representation.\n    max_len: Maximum length of the returned string. Longer reprs will be turned\n        into a brief descriptive string giving the type and length of obj.\n  \"\"\"\n  obj_repr = repr(obj)\n  if len(obj_repr) <= max_len:\n    return obj_repr\n  return '<{} of length {}>'.format(type(obj).__name__, len(obj_repr))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_notification_callback(self, notification_cb):\n    self._notification_cb = notification_cb\n    if not notification_cb and self.dimensions:\n      self.measured_value.notify_value_set = None\n    return self", "response": "Set the notifier we ll call when measurements are set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _maybe_make_unit_desc(self, unit_desc):\n    if isinstance(unit_desc, str) or unit_desc is None:\n      unit_desc = units.Unit(unit_desc)\n    if not isinstance(unit_desc, units.UnitDescriptor):\n      raise TypeError('Invalid units for measurement %s: %s' % (self.name,\n                                                                unit_desc))\n    return unit_desc", "response": "Return the UnitDescriptor or convert a string to one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a Dimension instance if it is not already.", "response": "def _maybe_make_dimension(self, dimension):\n    \"\"\"Return a `measurements.Dimension` instance.\"\"\"\n    # For backwards compatibility the argument can be either a Dimension, a\n    # string or a `units.UnitDescriptor`.\n    if isinstance(dimension, Dimension):\n      return dimension\n    if isinstance(dimension, units.UnitDescriptor):\n      return Dimension.from_unit_descriptor(dimension)\n    if isinstance(dimension, str):\n      return Dimension.from_string(dimension)\n\n    raise TypeError('Cannot convert %s to a dimension', dimension)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeclares dimensions for this Measurement returns self for chaining.", "response": "def with_dimensions(self, *dimensions):\n    \"\"\"Declare dimensions for this Measurement, returns self for chaining.\"\"\"\n    self.dimensions = tuple(\n        self._maybe_make_dimension(dim) for dim in dimensions)\n    self._cached = None\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef with_validator(self, validator):\n    if not callable(validator):\n      raise ValueError('Validator must be callable', validator)\n    self.validators.append(validator)\n    self._cached = None\n    return self", "response": "Add a validator callback to this Measurement chainable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef with_args(self, **kwargs):\n    validators = [\n        validator.with_args(**kwargs)\n        if hasattr(validator, 'with_args') else validator\n        for validator in self.validators\n    ]\n    return mutablerecords.CopyRecord(\n        self, name=util.format_string(self.name, kwargs),\n        docstring=util.format_string(self.docstring, kwargs),\n        validators=validators,\n        _cached=None,\n    )", "response": "String substitution for names and docstrings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self):\n    # PASS if all our validators return True, otherwise FAIL.\n    try:\n      if all(v(self.measured_value.value) for v in self.validators):\n        self.outcome = Outcome.PASS\n      else:\n        self.outcome = Outcome.FAIL\n      return self\n    except Exception as e:  # pylint: disable=bare-except\n      _LOG.error('Validation for measurement %s raised an exception %s.',\n                 self.name, e)\n      self.outcome = Outcome.FAIL\n      raise\n    finally:\n      if self._cached:\n        self._cached['outcome'] = self.outcome.name", "response": "Validate this measurement and update its outcome field."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_base_types(self):\n    if not self._cached:\n      # Create the single cache file the first time this is called.\n      self._cached = {\n          'name': self.name,\n          'outcome': self.outcome.name,\n      }\n      if self.validators:\n        self._cached['validators'] = data.convert_to_base_types(\n            tuple(str(v) for v in self.validators))\n      if self.dimensions:\n        self._cached['dimensions'] = data.convert_to_base_types(self.dimensions)\n      if self.units:\n        self._cached['units'] = data.convert_to_base_types(self.units)\n      if self.docstring:\n        self._cached['docstring'] = self.docstring\n    if self.measured_value.is_value_set:\n      self._cached['measured_value'] = self.measured_value.basetype_value()\n    return self._cached", "response": "Convert this measurement to a dict of basic types."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_dataframe(self, columns=None):\n    if not isinstance(self.measured_value, DimensionedMeasuredValue):\n      raise TypeError(\n        'Only a dimensioned measurement can be converted to a DataFrame')\n\n    if columns is None:\n      columns = [d.name for d in self.dimensions]\n      columns += [self.units.name if self.units else 'value']\n\n    dataframe = self.measured_value.to_dataframe(columns)\n\n    return dataframe", "response": "Convert a multi - dim measurement to a pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set(self, value):\n    if self.is_value_set:\n      # While we want to *allow* re-setting previously set measurements, we'd\n      # rather promote the use of multidimensional measurements instead of\n      # discarding data, so we make this somewhat chatty.\n      _LOG.warning(\n          'Overriding previous measurement %s value of %s with %s, the old '\n          'value will be lost.  Use a dimensioned measurement if you need to '\n          'save multiple values.', self.name, self.stored_value, value)\n    if value is None:\n      _LOG.warning('Measurement %s is set to None', self.name)\n    self.stored_value = value\n    self._cached_value = data.convert_to_base_types(value)\n    self.is_value_set = True", "response": "Set the value for this measurement with some sanity checks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a string into a Dimension", "response": "def from_string(cls, string):\n    \"\"\"Convert a string into a Dimension\"\"\"\n    # Note: There is some ambiguity as to whether the string passed is intended\n    # to become a unit looked up by name or suffix, or a Dimension descriptor.\n    if string in units.UNITS_BY_ALL:\n      return cls(description=string, unit=units.Unit(string))\n    else:\n      return cls(description=string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef value(self):\n    if not self.is_value_set:\n      raise MeasurementNotSetError('Measurement not yet set', self.name)\n    return [dimensions + (value,) for dimensions, value in\n            six.iteritems(self.value_dict)]", "response": "Returns the values stored in this record."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting to a pandas. DataFrame", "response": "def to_dataframe(self, columns=None):\n    \"\"\"Converts to a `pandas.DataFrame`\"\"\"\n    if not self.is_value_set:\n      raise ValueError('Value must be set before converting to a DataFrame.')\n    if not pandas:\n      raise RuntimeError('Install pandas to convert to pandas.DataFrame')\n    return pandas.DataFrame.from_records(self.value, columns=columns)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _assert_valid_key(self, name):\n    if name not in self._measurements:\n      raise NotAMeasurementError('Not a measurement', name, self._measurements)", "response": "Raises NotAMeasurementError if name is not a valid measurement."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninstall apk to device.", "response": "def install(self, apk_path, destination_dir=None, timeout_ms=None):\n    \"\"\"Install apk to device.\n\n    Doesn't support verifier file, instead allows destination directory to be\n    overridden.\n\n    Arguments:\n      apk_path: Local path to apk to install.\n      destination_dir: Optional destination directory. Use /system/app/ for\n        persistent applications.\n      timeout_ms: Expected timeout for pushing and installing.\n\n    Returns:\n      The pm install output.\n    \"\"\"\n    if not destination_dir:\n      destination_dir = '/data/local/tmp/'\n    basename = os.path.basename(apk_path)\n    destination_path = destination_dir + basename\n    self.push(apk_path, destination_path, timeout_ms=timeout_ms)\n    return self.Shell('pm install -r \"%s\"' % destination_path,\n                      timeout_ms=timeout_ms)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npush a file to the device.", "response": "def push(self, source_file, device_filename, timeout_ms=None):\n    \"\"\"Push source_file to file on device.\n\n    Arguments:\n      source_file: Either a filename or file-like object to push to the device.\n        If a filename, will set the remote mtime to match the local mtime,\n        otherwise will use the current time.\n      device_filename: The filename on the device to write to.\n      timeout_ms: Expected timeout for any part of the push.\n    \"\"\"\n    mtime = 0\n    if isinstance(source_file, six.string_types):\n      mtime = os.path.getmtime(source_file)\n      source_file = open(source_file)\n\n    self.filesync_service.send(\n        source_file, device_filename, mtime=mtime,\n        timeout=timeouts.PolledTimeout.from_millis(timeout_ms))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npulls a file from the device.", "response": "def pull(self, device_filename, dest_file=None, timeout_ms=None):\n    \"\"\"Pull file from device.\n\n    Arguments:\n      device_filename: The filename on the device to pull.\n      dest_file: If set, a filename or writable file-like object.\n      timeout_ms: Expected timeout for the pull.\n\n    Returns:\n      The file data if dest_file is not set, None otherwise.\n    \"\"\"\n    should_return_data = dest_file is None\n    if isinstance(dest_file, six.string_types):\n      dest_file = open(dest_file, 'w')\n    elif dest_file is None:\n      dest_file = six.StringIO()\n    self.filesync_service.recv(device_filename, dest_file,\n                               timeouts.PolledTimeout.from_millis(timeout_ms))\n    if should_return_data:\n      return dest_file.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, device_path, timeout_ms=None):\n    return self.filesync_service.list(\n        device_path, timeouts.PolledTimeout.from_millis(timeout_ms))", "response": "Yields a list of filesync_service. DeviceFileStat objects for the given directory contents."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef async_command(self, command, raw=False, timeout_ms=None):\n    return self.shell_service.async_command(\n        str(command), raw=raw, timeout_ms=timeout_ms)", "response": "See shell_service. ShellService. async_command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_remote_command(self, destination, timeout_ms, success_msgs=None):\n    timeout = timeouts.PolledTimeout.from_millis(timeout_ms)\n    stream = self._adb_connection.open_stream(destination, timeout)\n    if not stream:\n      raise usb_exceptions.AdbStreamUnavailableError(\n          'Service %s not supported', destination)\n    try:\n      message = stream.read(timeout_ms=timeout)\n      # Some commands report success messages, ignore them.\n      if any([m in message for m in success_msgs]):\n        return\n    except usb_exceptions.CommonUsbError:\n      if destination.startswith('reboot:'):\n        # We expect this if the device is rebooting.\n        return\n      raise\n    raise usb_exceptions.AdbRemoteError('Device message: %s', message)", "response": "Check for remote errors and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting to the device.", "response": "def connect(cls, usb_handle, **kwargs):\n    \"\"\"Connect to the device.\n\n    Args:\n      usb_handle: UsbHandle instance to use.\n      **kwargs: See AdbConnection.connect for kwargs. Includes rsa_keys, and\n        auth_timeout_ms.\n\n    Returns:\n      An instance of this class if the device connected successfully.\n    \"\"\"\n    adb_connection = adb_protocol.AdbConnection.connect(usb_handle, **kwargs)\n    return cls(adb_connection)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send(query,\n         address=DEFAULT_ADDRESS,\n         port=DEFAULT_PORT,\n         ttl=DEFAULT_TTL,\n         local_only=False,\n         timeout_s=2):\n  \"\"\"Sends a query to the given multicast socket and returns responses.\n\n  Args:\n    query: The string query to send.\n    address: Multicast IP address component of the socket to send to.\n    port: Multicast UDP port component of the socket to send to.\n    ttl: TTL for multicast messages. 1 to keep traffic in-network.\n    timeout_s: Seconds to wait for responses.\n\n  Returns: A set of all responses that arrived before the timeout expired.\n           Responses are tuples of (sender_address, message).\n  \"\"\"\n  # Set up the socket as a UDP Multicast socket with the given timeout.\n  sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n  sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, ttl)\n  if local_only:\n    # Set outgoing interface to localhost to ensure no packets leave this host.\n    sock.setsockopt(\n        socket.IPPROTO_IP,\n        socket.IP_MULTICAST_IF,\n        struct.pack('!L', LOCALHOST_ADDRESS))\n  sock.settimeout(timeout_s)\n  sock.sendto(query.encode('utf-8'), (address, port))\n\n  # Set up our thread-safe Queue for handling responses.\n  recv_queue = queue.Queue()\n  def _handle_responses():\n    while True:\n      try:\n        data, address = sock.recvfrom(MAX_MESSAGE_BYTES)\n        data = data.decode('utf-8')\n      except socket.timeout:\n        recv_queue.put(None)\n        break\n      else:\n        _LOG.debug('Multicast response to query \"%s\": %s:%s',\n                   query, address[0], data)\n        recv_queue.put((address[0], str(data)))\n\n  # Yield responses as they come in, giving up once timeout expires.\n  response_thread = threading.Thread(target=_handle_responses)\n  response_thread.start()\n  while response_thread.is_alive():\n    recv_tuple = recv_queue.get()\n    if not recv_tuple:\n      break\n    yield recv_tuple\n  response_thread.join()", "response": "Sends a query to the given multicast socket and returns responses."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstopping listening for messages.", "response": "def stop(self, timeout_s=None):\n    \"\"\"Stop listening for messages.\"\"\"\n    self._live = False\n    try:\n      self._sock.shutdown(socket.SHUT_RDWR)\n      self._sock.close()\n    except socket.error:\n      pass\n    self.join(timeout_s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self):\n    self._live = True\n    self._sock.settimeout(self.LISTEN_TIMEOUT_S)\n\n    # Passing in INADDR_ANY means the kernel will choose the default interface.\n    # The localhost address is used to receive messages sent in \"local_only\"\n    # mode and the default address is used to receive all other messages.\n    for interface_ip in (socket.INADDR_ANY, LOCALHOST_ADDRESS):\n      self._sock.setsockopt(\n          socket.IPPROTO_IP,\n          socket.IP_ADD_MEMBERSHIP,\n          # IP_ADD_MEMBERSHIP takes the 8-byte group address followed by the IP\n          # assigned to the interface on which to listen.\n          struct.pack('!4sL', socket.inet_aton(self.address), interface_ip))\n\n    if sys.platform == 'darwin':\n      self._sock.setsockopt(socket.SOL_SOCKET,\n                            socket.SO_REUSEPORT,\n                            1)  # Allow multiple listeners to bind.\n    else:\n      self._sock.setsockopt(socket.SOL_SOCKET,\n                            socket.SO_REUSEADDR,\n                            1)  # Allow multiple listeners to bind.\n    self._sock.bind((self.address, self.port))\n\n    while self._live:\n      try:\n        data, address = self._sock.recvfrom(MAX_MESSAGE_BYTES)\n        data = data.decode('utf-8')\n        log_line = 'Received multicast message from %s: %s' % (address, data)\n        response = self._callback(data)\n        if response is not None:\n          log_line += ', responding with %s bytes' % len(response)\n          # Send replies out-of-band instead of with the same multicast socket\n          # so that multiple processes on the same host can listen for\n          # requests and reply (if they all try to use the multicast socket\n          # to reply, they conflict and this sendto fails).\n          response = response.encode('utf-8')\n          socket.socket(socket.AF_INET, socket.SOCK_DGRAM).sendto(\n              response, address)\n        _LOG.debug(log_line)\n      except socket.timeout:\n        pass\n      except socket.error:\n        _LOG.debug('Error receiving multicast message', exc_info=True)", "response": "Listen for pings until stopped."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a command to the device.", "response": "def send_command(self, command, arg=None):\n    \"\"\"Sends a command to the device.\n\n    Args:\n      command: The command to send.\n      arg: Optional argument to the command.\n    \"\"\"\n    if arg is not None:\n      command = '%s:%s' % (command, arg)\n    self._write(six.StringIO(command), len(command))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle simple responses from the device.", "response": "def handle_simple_responses(\n      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):\n    \"\"\"Accepts normal responses from the device.\n\n    Args:\n      timeout_ms: Timeout in milliseconds to wait for each response.\n      info_cb: Optional callback for text sent from the bootloader.\n\n    Returns:\n      OKAY packet's message.\n    \"\"\"\n    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the protocol for sending data to the device.", "response": "def handle_data_sending(self, source_file, source_len,\n                          info_cb=DEFAULT_MESSAGE_CALLBACK,\n                          progress_callback=None, timeout_ms=None):\n    \"\"\"Handles the protocol for sending data to the device.\n\n    Arguments:\n      source_file: File-object to read from for the device.\n      source_len: Amount of data, in bytes, to send to the device.\n      info_cb: Optional callback for text sent from the bootloader.\n      progress_callback: Callback that takes the current and the total progress\n        of the current file.\n      timeout_ms: Timeout in milliseconds to wait for each response.\n\n    Raises:\n      FastbootTransferError: When fastboot can't handle this amount of data.\n      FastbootStateMismatch: Fastboot responded with the wrong packet type.\n      FastbootRemoteFailure: Fastboot reported failure.\n      FastbootInvalidResponse: Fastboot responded with an unknown packet type.\n\n    Returns:\n      OKAY packet's message.\n    \"\"\"\n    accepted_size = self._accept_responses(\n        'DATA', info_cb, timeout_ms=timeout_ms)\n\n    accepted_size = binascii.unhexlify(accepted_size[:8])\n    accepted_size, = struct.unpack('>I', accepted_size)\n    if accepted_size != source_len:\n      raise usb_exceptions.FastbootTransferError(\n          'Device refused to download %s bytes of data (accepts %s bytes)',\n          source_len, accepted_size)\n    self._write(source_file, accepted_size, progress_callback)\n    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _accept_responses(self, expected_header, info_cb, timeout_ms=None):\n    while True:\n      response = self.usb.read(64, timeout_ms=timeout_ms)\n      header = response[:4]\n      remaining = response[4:]\n\n      if header == 'INFO':\n        info_cb(FastbootMessage(remaining, header))\n      elif header in self.FINAL_HEADERS:\n        if header != expected_header:\n          raise usb_exceptions.FastbootStateMismatch(\n              'Expected %s, got %s', expected_header, header)\n        if header == 'OKAY':\n          info_cb(FastbootMessage(remaining, header))\n        return remaining\n      elif header == 'FAIL':\n        info_cb(FastbootMessage(remaining, header))\n        raise usb_exceptions.FastbootRemoteFailure('FAIL: %s', remaining)\n      else:\n        raise usb_exceptions.FastbootInvalidResponse(\n            'Got unknown header %s and response %s', header, remaining)", "response": "Returns a list of responses from the device until the expected header or a FAIL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _handle_progress(self, total, progress_callback):  # pylint: disable=no-self-use\n    current = 0\n    while True:\n      current += yield\n      try:\n        progress_callback(current, total)\n      except Exception:  # pylint: disable=broad-except\n        _LOG.exception('Progress callback raised an exception. %s',\n                       progress_callback)\n        continue", "response": "Handles the progress of the current page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend the data to the device.", "response": "def _write(self, data, length, progress_callback=None):\n    \"\"\"Sends the data to the device, tracking progress with the callback.\"\"\"\n    if progress_callback:\n      progress = self._handle_progress(length, progress_callback)\n      six.next(progress)\n    while length:\n      tmp = data.read(FASTBOOT_DOWNLOAD_CHUNK_SIZE_KB * 1024)\n      length -= len(tmp)\n      self.usb.write(tmp)\n\n      if progress_callback:\n        progress.send(len(tmp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _simple_command(self, command, arg=None, **kwargs):\n    self._protocol.send_command(command, arg)\n    return self._protocol.handle_simple_responses(**kwargs)", "response": "Send a simple command."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nflash a partition from a file on disk.", "response": "def flash_from_file(self, partition, source_file, source_len=0,\n                      info_cb=DEFAULT_MESSAGE_CALLBACK, progress_callback=None,\n                      timeout_ms=None):\n    \"\"\"Flashes a partition from the file on disk.\n\n    Args:\n      partition: Partition name to flash to.\n      source_file: Filename to download to the device.\n      source_len: Optional length of source_file, uses os.stat if not provided.\n      info_cb: See Download.\n      progress_callback: See Download.\n      timeout_ms: The amount of time to wait on okay after flashing.\n\n    Returns:\n      Download and flash responses, normally nothing.\n    \"\"\"\n    if source_len == 0:\n      # Fall back to stat.\n      source_len = os.stat(source_file).st_size\n    download_response = self.download(\n        source_file, source_len=source_len, info_cb=info_cb,\n        progress_callback=progress_callback)\n    flash_response = self.flash(partition, info_cb=info_cb,\n                                timeout_ms=timeout_ms)\n    return download_response + flash_response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download(self, source_file, source_len=0,\n               info_cb=DEFAULT_MESSAGE_CALLBACK, progress_callback=None):\n    \"\"\"Downloads a file to the device.\n\n    Args:\n      source_file: A filename or file-like object to download to the device.\n      source_len: Optional length of source_file. If source_file is a file-like\n          object and source_len is not provided, source_file is read into\n          memory.\n      info_cb: Optional callback accepting FastbootMessage for text sent from\n          the bootloader.\n      progress_callback: Optional callback called with the percent of the\n          source_file downloaded. Note, this doesn't include progress of the\n          actual flashing.\n\n    Returns:\n      Response to a download request, normally nothing.\n    \"\"\"\n    if isinstance(source_file, six.string_types):\n      source_len = os.stat(source_file).st_size\n      source_file = open(source_file)\n\n    if source_len == 0:\n      # Fall back to storing it all in memory :(\n      data = source_file.read()\n      source_file = six.StringIO(data)\n      source_len = len(data)\n\n    self._protocol.send_command('download', '%08x' % source_len)\n    return self._protocol.handle_data_sending(\n        source_file, source_len, info_cb, progress_callback=progress_callback)", "response": "Downloads a file to the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nflashes the last downloaded file to the given partition.", "response": "def flash(self, partition, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):\n    \"\"\"Flashes the last downloaded file to the given partition.\n\n    Args:\n      partition: Partition to flash.\n      timeout_ms: Optional timeout in milliseconds to wait for it to finish.\n      info_cb: See Download. Usually no messages.\n\n    Returns:\n      Response to a download request, normally nothing.\n    \"\"\"\n    return self._simple_command('flash', arg=partition, info_cb=info_cb,\n                                timeout_ms=timeout_ms)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nerase the given partition.", "response": "def erase(self, partition, timeout_ms=None):\n    \"\"\"Erases the given partition.\"\"\"\n    self._simple_command('erase', arg=partition, timeout_ms=timeout_ms)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_var(self, var, info_cb=DEFAULT_MESSAGE_CALLBACK):\n    return self._simple_command('getvar', arg=var, info_cb=info_cb)", "response": "Returns the value of a given variable according to the current bootloader."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef oem(self, command, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):\n    return self._simple_command(\n        'oem %s' % command, timeout_ms=timeout_ms, info_cb=info_cb)", "response": "Executes an OEM command on the device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reboot(self, target_mode=None, timeout_ms=None):\n    return self._simple_command('reboot', arg=target_mode,\n                                timeout_ms=timeout_ms)", "response": "Reboots the device.\n\n    Args:\n        target_mode: Normal reboot when unspecified (or None). Can specify\n            other target modes, such as 'recovery' or 'bootloader'.\n        timeout_ms: Optional timeout in milliseconds to wait for a response.\n    Returns:\n        Usually the empty string. Depends on the bootloader and the target_mode."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _discover(**kwargs):\n  query = station_server.MULTICAST_QUERY\n  for host, response in multicast.send(query, **kwargs):\n    try:\n      result = json.loads(response)\n    except ValueError:\n      _LOG.warn('Received bad JSON over multicast from %s: %s', host, response)\n    try:\n      yield StationInfo(result['cell'], host, result['port'],\n                        result['station_id'], 'ONLINE',\n                        result.get('test_description'),\n                        result['test_name'])\n    except KeyError:\n      if 'last_activity_time_millis' in result:\n        _LOG.debug('Received old station API response on multicast. Ignoring.')\n      else:\n        _LOG.warn('Received bad multicast response from %s: %s', host, response)", "response": "Yields info about station servers announcing themselves via multicast."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_stations(cls, station_info_list):\n    with cls.station_map_lock:\n\n      # By default, assume old stations are unreachable.\n      for host_port, station_info in six.iteritems(cls.station_map):\n        cls.station_map[host_port] = station_info._replace(status='UNREACHABLE')\n\n      for station_info in station_info_list:\n        host_port = '%s:%s' % (station_info.host, station_info.port)\n        cls.station_map[host_port] = station_info", "response": "Update the station map with the given list of stations."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef publish_if_new(cls):\n    message = cls.make_message()\n    if message != cls.last_message:\n      super(DashboardPubSub, cls).publish(message)\n      cls.last_message = message", "response": "If the station map has changed publish the new information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the basic phase group example.", "response": "def run_basic_group():\n  \"\"\"Run the basic phase group example.\n\n  In this example, there are no terminal phases; all phases are run.\n  \"\"\"\n  test = htf.Test(htf.PhaseGroup(\n      setup=[setup_phase],\n      main=[main_phase],\n      teardown=[teardown_phase],\n  ))\n  test.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_setup_error_group():\n  test = htf.Test(htf.PhaseGroup(\n      setup=[error_setup_phase],\n      main=[main_phase],\n      teardown=[teardown_phase],\n  ))\n  test.execute()", "response": "Run the phase group example where an error occurs in a setup phase."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_main_error_group():\n  test = htf.Test(htf.PhaseGroup(\n      setup=[setup_phase],\n      main=[error_main_phase, main_phase],\n      teardown=[teardown_phase],\n  ))\n  test.execute()", "response": "Run the phase group example where an error occurs in a main phase."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the nested groups example.", "response": "def run_nested_groups():\n  \"\"\"Run the nested groups example.\n\n  This example shows a PhaseGroup in a PhaseGroup.  No phase is terminal, so all\n  are run in the order;\n    main_phase\n    inner_main_phase\n    inner_teardown_phase\n    teardown_phase\n  \"\"\"\n  test = htf.Test(\n      htf.PhaseGroup(\n          main=[\n              main_phase,\n              htf.PhaseGroup.with_teardown(inner_teardown_phase)(\n                  inner_main_phase),\n          ],\n          teardown=[teardown_phase]\n      )\n  )\n  test.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_nested_error_groups():\n  test = htf.Test(\n      htf.PhaseGroup(\n          main=[\n              htf.PhaseGroup.with_teardown(inner_teardown_phase)(\n                  error_main_phase, main_phase),\n              main_phase,\n          ],\n          teardown=[teardown_phase],\n      )\n  )\n  test.execute()", "response": "Run nested groups example where an error occurs in nested main phase."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a legal python name for the given name for use as a unit key.", "response": "def unit_key_from_name(name):\n  \"\"\"Return a legal python name for the given name for use as a unit key.\"\"\"\n  result = name\n\n  for old, new in six.iteritems(UNIT_KEY_REPLACEMENTS):\n    result = result.replace(old, new)\n\n  # Collapse redundant underscores and convert to uppercase.\n  result = re.sub(r'_+', '_', result.upper())\n\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_adb_message(self, data):\n    message = AdbMessage(AdbMessage.WIRE_TO_CMD.get(self.cmd),\n                         self.arg0, self.arg1, data)\n    if (len(data) != self.data_length or\n        message.data_crc32 != self.data_checksum):\n      raise usb_exceptions.AdbDataIntegrityError(\n          '%s (%s) received invalid data: %s', message, self, repr(data))\n    return message", "response": "Turn the data into an ADB message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_message(self, message, timeout):\n    with self._writer_lock:\n      self._transport.write(message.header, timeout.remaining_ms)\n\n      # Use any remaining time to send the data.  Note that if we get this far,\n      # we always at least try to send the data (with a minimum of 10ms timeout)\n      # because we don't want the remote end to get out of sync because we sent\n      # a header but no data.\n      if timeout.has_expired():\n        _LOG.warning('Timed out between AdbMessage header and data, sending '\n                     'data anyway with 10ms timeout')\n        timeout = timeouts.PolledTimeout.from_millis(10)\n      self._transport.write(message.data, timeout.remaining_ms)", "response": "Send the given message over this transport."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading an AdbMessage from the device.", "response": "def read_message(self, timeout):\n    \"\"\"Read an AdbMessage from this transport.\n\n    Args:\n      timeout: Timeout for the entire read operation, in the form of a\n        timeouts.PolledTimeout instance.  Note that for packets with a data\n        payload, two USB reads are performed.\n\n    Returns:\n      The ADB message read from the device.\n\n    Raises:\n      UsbReadFailedError: There's an error during read, including timeout.\n      AdbProtocolError: A message is incorrectly formatted.\n      AdbTimeoutError: timeout is already expired, or expires before we read the\n        entire message, specifically between reading header and data packets.\n    \"\"\"\n    with self._reader_lock:\n      raw_header = self._transport.read(\n          struct.calcsize(AdbMessage.HEADER_STRUCT_FORMAT),\n          timeout.remaining_ms)\n      if not raw_header:\n        raise usb_exceptions.AdbProtocolError('Adb connection lost')\n\n      try:\n        raw_message = RawAdbMessage(*struct.unpack(\n            AdbMessage.HEADER_STRUCT_FORMAT, raw_header))\n      except struct.error as exception:\n        raise usb_exceptions.AdbProtocolError(\n            'Unable to unpack ADB command (%s): %s (%s)',\n            AdbMessage.HEADER_STRUCT_FORMAT, raw_header, exception)\n\n      if raw_message.data_length > 0:\n        if timeout.has_expired():\n          _LOG.warning('Timed out between AdbMessage header and data, reading '\n                       'data anyway with 10ms timeout')\n          timeout = timeouts.PolledTimeout.from_millis(10)\n        data = self._transport.read(raw_message.data_length,\n                                    timeout.remaining_ms)\n      else:\n        data = ''\n\n      return raw_message.to_adb_message(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_until(self, expected_commands, timeout):\n    msg = timeouts.loop_until_timeout_or_valid(\n        timeout, lambda: self.read_message(timeout),\n        lambda m: m.command in expected_commands, 0)\n    if msg.command not in expected_commands:\n      raise usb_exceptions.AdbTimeoutError(\n          'Timed out establishing connection, waiting for: %s',\n          expected_commands)\n    return msg", "response": "Read AdbMessages from this transport until one of the expected commands is received."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_message_type(name, attributes, has_data=True):\n\n  def assert_command_is(self, command):  # pylint: disable=invalid-name\n    \"\"\"Assert that a message's command matches the given command.\"\"\"\n    if self.command != command:\n      raise usb_exceptions.AdbProtocolError(\n          'Expected %s command, received %s', command, self)\n\n  return type(name, (collections.namedtuple(name, attributes),),\n              {\n                  'assert_command_is': assert_command_is,\n                  'has_data': has_data,\n                  # Struct format on the wire has an unsigned int for each attr.\n                  'struct_format': '<%sI' % len(attributes.split()),\n              })", "response": "Make a message type for the AdbTransport subclasses."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stat(self, filename, timeout=None):\n    transport = StatFilesyncTransport(self.stream)\n    transport.write_data('STAT', filename, timeout)\n    stat_msg = transport.read_message(timeout)\n    stat_msg.assert_command_is('STAT')\n    return DeviceFileStat(filename, stat_msg.mode, stat_msg.size, stat_msg.time)", "response": "Return device file stat."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list(self, path, timeout=None):\n    transport = DentFilesyncTransport(self.stream)\n    transport.write_data('LIST', path, timeout)\n    return (DeviceFileStat(dent_msg.name, dent_msg.mode,\n                           dent_msg.size, dent_msg.time) for dent_msg in\n            transport.read_until_done('DENT', timeout))", "response": "List the contents of a directory on the device."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef recv(self, filename, dest_file, timeout=None):\n    transport = DataFilesyncTransport(self.stream)\n    transport.write_data('RECV', filename, timeout)\n    for data_msg in transport.read_until_done('DATA', timeout):\n      dest_file.write(data_msg.data)", "response": "Retrieve a file from the device into the file - like dest_file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_for_fail_message(self, transport, exc_info, timeout):  # pylint: disable=no-self-use\n    try:\n      transport.read_message(timeout)\n    except usb_exceptions.CommonUsbError:\n      # If we got a remote error, raise that exception.\n      if sys.exc_info()[0] is usb_exceptions.AdbRemoteError:\n        raise\n    # Otherwise reraise the original exception.\n    raise_with_traceback(exc_info[0](exc_info[1]), traceback=exc_info[2])", "response": "Checks if a message from a transport is a FAIL message."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npushes a file - like object to the device.", "response": "def send(self, src_file, filename, st_mode=DEFAULT_PUSH_MODE, mtime=None,\n           timeout=None):\n    \"\"\"Push a file-like object to the device.\n\n    Args:\n      src_file: File-like object for reading from\n      filename: Filename to push to on the device\n      st_mode: stat mode for filename on the device\n      mtime: modification time to set for the file on the device\n      timeout: Timeout to use for the send operation.\n\n    Raises:\n      AdbProtocolError: If we get an unexpected response.\n      AdbRemoteError: If there's a remote error (but valid protocol).\n    \"\"\"\n    transport = DataFilesyncTransport(self.stream)\n    transport.write_data('SEND', '%s,%s' % (filename, st_mode), timeout)\n\n    try:\n      while True:\n        data = src_file.read(MAX_PUSH_DATA_BYTES)\n        if not data:\n          break\n        transport.write_data('DATA', data, timeout)\n\n      mtime = mtime or int(time.time())\n      transport.write_message(\n          FilesyncMessageTypes.DoneMessage('DONE', mtime), timeout)\n    except usb_exceptions.AdbStreamClosedError:\n      # Try to do one last read to see if we can get any more information,\n      # ignoring any errors for this Read attempt. Note that this always\n      # raises, either a new AdbRemoteError, or the AdbStreamClosedError.\n      self._check_for_fail_message(transport, sys.exc_info(), timeout)\n\n    data_msg = transport.read_message(timeout)\n    data_msg.assert_command_is('OKAY')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_data(self, command, data, timeout=None):\n    self.write_message(FilesyncMessageTypes.DataMessage(command, data), timeout)", "response": "Shortcut for writing specifically a DataMessage."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite an arbitrary message to the device.", "response": "def write_message(self, msg, timeout=None):\n    \"\"\"Write an arbitrary message (of one of the types above).\n\n    For the host side implementation, this will only ever be a DataMessage, but\n    it's implemented generically enough here that you could use\n    FilesyncTransport to implement the device side if you wanted.\n\n    Args:\n      msg:  The message to send, must be one of the types above.\n      timeout: timeouts.PolledTimeout to use for the operation.\n    \"\"\"\n    replace_dict = {'command': self.CMD_TO_WIRE[msg.command]}\n    if msg.has_data:\n      # Swap out data for the data length for the wire.\n      data = msg[-1]\n      replace_dict[msg._fields[-1]] = len(data)\n\n    self.stream.write(struct.pack(msg.struct_format,\n                                  *msg._replace(**replace_dict)), timeout)\n    if msg.has_data:\n      self.stream.write(data, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding messages read until we receive a given command.", "response": "def read_until_done(self, command, timeout=None):\n    \"\"\"Yield messages read until we receive a 'DONE' command.\n\n    Read messages of the given command until we receive a 'DONE' command.  If a\n    command different than the requested one is received, an AdbProtocolError\n    is raised.\n\n    Args:\n      command: The command to expect, like 'DENT' or 'DATA'.\n      timeout: The timeouts.PolledTimeout to use for this operation.\n\n    Yields:\n      Messages read, of type self.RECV_MSG_TYPE, see read_message().\n\n    Raises:\n      AdbProtocolError: If an unexpected command is read.\n      AdbRemoteError: If a 'FAIL' message is read.\n    \"\"\"\n    message = self.read_message(timeout)\n    while message.command != 'DONE':\n      message.assert_command_is(command)\n      yield message\n      message = self.read_message(timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a message from this transport and returns it.", "response": "def read_message(self, timeout=None):\n    \"\"\"Read a message from this transport and return it.\n\n    Reads a message of RECV_MSG_TYPE and returns it.  Note that this method\n    abstracts the data length and data read so that the caller simply gets the\n    data along with the header in the returned message.\n\n    Args:\n      timeout: timeouts.PolledTimeout to use for the operation.\n\n    Returns:\n      An instance of self.RECV_MSG_TYPE that was read from self.stream.\n\n    Raises:\n      AdbProtocolError: If an invalid response is received.\n      AdbRemoteError: If a FAIL response is received.\n    \"\"\"\n    raw_data = self.stream.read(\n        struct.calcsize(self.RECV_MSG_TYPE.struct_format), timeout)\n    try:\n      raw_message = struct.unpack(self.RECV_MSG_TYPE.struct_format, raw_data)\n    except struct.error:\n      raise usb_exceptions.AdbProtocolError(\n          '%s expected format \"%s\", got data %s', self,\n          self.RECV_MSG_TYPE.struct_format, raw_data)\n\n    if raw_message[0] not in self.WIRE_TO_CMD:\n      raise usb_exceptions.AdbProtocolError(\n          'Unrecognized command id: %s', raw_message)\n\n    # Swap out the wire command with the string equivalent.\n    raw_message = (self.WIRE_TO_CMD[raw_message[0]],) + raw_message[1:]\n\n    if self.RECV_MSG_TYPE.has_data and raw_message[-1]:\n      # For messages that have data, the length of the data is the last field\n      # in the struct.  We do another read and swap out that length for the\n      # actual data read before we create the namedtuple to return.\n      data_len = raw_message[-1]\n      raw_message = raw_message[:-1] + (self.stream.read(data_len, timeout),)\n\n    if raw_message[0] not in self.VALID_RESPONSES:\n      raise usb_exceptions.AdbProtocolError(\n          '%s not a valid response for %s', raw_message[0], self)\n    if raw_message[0] == 'FAIL':\n      raise usb_exceptions.AdbRemoteError(\n          'Remote ADB failure: %s', raw_message)\n    return self.RECV_MSG_TYPE(*raw_message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _lazy_load_units_by_code():\n  if UNITS_BY_CODE:\n    # already populated\n    return\n\n  for unit in units.UNITS_BY_NAME.values():\n    UNITS_BY_CODE[unit.code] = unit", "response": "Populate dict of units by code iff UNITS_BY_CODE is empty."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies data from the OpenHTF TestRecord to the MfgEvent proto.", "response": "def _populate_basic_data(mfg_event, record):\n  \"\"\"Copies data from the OpenHTF TestRecord to the MfgEvent proto.\"\"\"\n  # TODO:\n  #   * Missing in proto: set run name from metadata.\n  #   * `part_tags` field on proto is unused\n  #   * `timings` field on proto is unused.\n  #   * Handle arbitrary units as uom_code/uom_suffix.\n\n  # Populate non-repeated fields.\n  mfg_event.dut_serial = record.dut_id\n  mfg_event.start_time_ms = record.start_time_millis\n  mfg_event.end_time_ms = record.end_time_millis\n  mfg_event.tester_name = record.station_id\n  mfg_event.test_name = record.metadata.get('test_name') or record.station_id\n  mfg_event.test_status = test_runs_converter.OUTCOME_MAP[record.outcome]\n  mfg_event.operator_name = record.metadata.get('operator_name', '')\n  mfg_event.test_version = str(record.metadata.get('test_version', ''))\n  mfg_event.test_description = record.metadata.get('test_description', '')\n\n  # Populate part_tags.\n  mfg_event.part_tags.extend(record.metadata.get('part_tags', []))\n\n  # Populate phases.\n  for phase in record.phases:\n    mfg_phase = mfg_event.phases.add()\n    mfg_phase.name = phase.name\n    mfg_phase.description = phase.codeinfo.sourcecode\n    mfg_phase.timing.start_time_millis = phase.start_time_millis\n    mfg_phase.timing.end_time_millis = phase.end_time_millis\n\n  # Populate failure codes.\n  for details in record.outcome_details:\n    failure_code = mfg_event.failure_codes.add()\n    failure_code.code = details.code\n    failure_code.details = details.description\n\n  # Populate test logs.\n  for log_record in record.log_records:\n    test_log = mfg_event.test_logs.add()\n    test_log.timestamp_millis = log_record.timestamp_millis\n    test_log.log_message = log_record.message\n    test_log.logger_name = log_record.logger_name\n    test_log.levelno = log_record.level\n    if log_record.level <= logging.DEBUG:\n      test_log.level = test_runs_pb2.TestRunLogMessage.DEBUG\n    elif log_record.level <= logging.INFO:\n      test_log.level = test_runs_pb2.TestRunLogMessage.INFO\n    elif log_record.level <= logging.WARNING:\n      test_log.level = test_runs_pb2.TestRunLogMessage.WARNING\n    elif log_record.level <= logging.ERROR:\n      test_log.level = test_runs_pb2.TestRunLogMessage.ERROR\n    elif log_record.level <= logging.CRITICAL:\n      test_log.level = test_runs_pb2.TestRunLogMessage.CRITICAL\n    test_log.log_source = log_record.source\n    test_log.lineno = log_record.lineno"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _attach_record_as_json(mfg_event, record):\n  attachment = mfg_event.attachment.add()\n  attachment.name = TEST_RECORD_ATTACHMENT_NAME\n  test_record_dict = htf_data.convert_to_base_types(record)\n  attachment.value_binary = _convert_object_to_json(test_record_dict)\n  attachment.type = test_runs_pb2.TEXT_UTF8", "response": "Attach a copy of the record as JSON so we have an un - mangled copy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _attach_config(mfg_event, record):\n  if 'config' not in record.metadata:\n    return\n  attachment = mfg_event.attachment.add()\n  attachment.name = 'config'\n  attachment.value_binary = _convert_object_to_json(record.metadata['config'])\n  attachment.type = test_runs_pb2.TEXT_UTF8", "response": "Attaches the OpenHTF config file as JSON."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake the names of phase measurement and attachments unique. This function will make the names of measurements and attachments unique. It modifies the input all_phases. Args: all_phases: the phases to make unique Returns: the phases now modified.", "response": "def phase_uniquizer(all_phases):\n  \"\"\"Makes the names of phase measurement and attachments unique.\n\n  This function will make the names of measurements and attachments unique.\n  It modifies the input all_phases.\n\n  Args:\n    all_phases: the phases to make unique\n\n  Returns:\n    the phases now modified.\n  \"\"\"\n  measurement_name_maker = UniqueNameMaker(\n      itertools.chain.from_iterable(\n          phase.measurements.keys() for phase in all_phases\n          if phase.measurements))\n  attachment_names = list(itertools.chain.from_iterable(\n      phase.attachments.keys() for phase in all_phases))\n  attachment_names.extend(itertools.chain.from_iterable([\n      'multidim_' + name for name, meas in phase.measurements.items()\n      if meas.dimensions is not None\n  ] for phase in all_phases if phase.measurements))\n  attachment_name_maker = UniqueNameMaker(attachment_names)\n  for phase in all_phases:\n    # Make measurements unique.\n    for name, _ in sorted(phase.measurements.items()):\n      old_name = name\n      name = measurement_name_maker.make_unique(name)\n\n      phase.measurements[old_name].name = name\n      phase.measurements[name] = phase.measurements.pop(old_name)\n    # Make attachments unique.\n    for name, _ in sorted(phase.attachments.items()):\n      old_name = name\n      name = attachment_name_maker.make_unique(name)\n      phase.attachments[old_name].name = name\n      phase.attachments[name] = phase.attachments.pop(old_name)\n  return all_phases"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a multi - dim measurement to an openhtf. test_record. Attachment.", "response": "def multidim_measurement_to_attachment(name, measurement):\n  \"\"\"Convert a multi-dim measurement to an `openhtf.test_record.Attachment`.\"\"\"\n\n  dimensions = list(measurement.dimensions)\n  if measurement.units:\n    dimensions.append(\n        measurements.Dimension.from_unit_descriptor(measurement.units))\n\n  dims = []\n  for d in dimensions:\n    if d.suffix is None:\n      suffix = u''\n    # Ensure that the suffix is unicode. It's typically str/bytes because\n    # units.py looks them up against str/bytes.\n    elif isinstance(d.suffix, unicode):\n      suffix = d.suffix\n    else:\n      suffix = d.suffix.decode('utf8')\n    dims.append({\n        'uom_suffix': suffix,\n        'uom_code': d.code,\n        'name': d.name,\n    })\n  # Refer to the module docstring for the expected schema.\n  dimensioned_measured_value = measurement.measured_value\n  value = (sorted(dimensioned_measured_value.value, key=lambda x: x[0])\n           if dimensioned_measured_value.is_value_set else None)\n  outcome_str = MEASUREMENT_OUTCOME_TO_TEST_RUN_STATUS_NAME[measurement.outcome]\n  data = _convert_object_to_json({\n      'outcome': outcome_str,\n      'name': name,\n      'dimensions': dims,\n      'value': value,\n  })\n  attachment = htf_test_record.Attachment(data, test_runs_pb2.MULTIDIM_JSON)\n\n  return attachment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting each multidim measurement into unique attachments for all phases.", "response": "def convert_multidim_measurements(all_phases):\n  \"\"\"Converts each multidim measurements into attachments for all phases..\"\"\"\n  # Combine actual attachments with attachments we make from multi-dim\n  # measurements.\n  attachment_names = list(itertools.chain.from_iterable(\n      phase.attachments.keys() for phase in all_phases))\n  attachment_names.extend(itertools.chain.from_iterable([\n      'multidim_' + name for name, meas in phase.measurements.items()\n      if meas.dimensions is not None\n  ] for phase in all_phases if phase.measurements))\n  attachment_name_maker = UniqueNameMaker(attachment_names)\n\n  for phase in all_phases:\n    # Process multi-dim measurements into unique attachments.\n    for name, measurement in sorted(phase.measurements.items()):\n      if measurement.dimensions:\n        old_name = name\n        name = attachment_name_maker.make_unique('multidim_%s' % name)\n        attachment = multidim_measurement_to_attachment(name, measurement)\n        phase.attachments[name] = attachment\n        phase.measurements.pop(old_name)\n  return all_phases"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an OpenHTF test record attachment to a multi - dim measurement.", "response": "def attachment_to_multidim_measurement(attachment, name=None):\n  \"\"\"Convert an OpenHTF test record attachment to a multi-dim measurement.\n\n  This is a best effort attempt to reverse, as some data is lost in converting\n  from a multidim to an attachment.\n\n  Args:\n    attachment: an `openhtf.test_record.Attachment` from a multi-dim.\n    name: an optional name for the measurement.  If not provided will use the\n     name included in the attachment.\n\n  Returns:\n    An multi-dim `openhtf.Measurement`.\n  \"\"\"\n  data = json.loads(attachment.data)\n\n  name = name or data.get('name')\n  # attachment_dimn are a list of dicts with keys 'uom_suffix' and 'uom_code'\n  attachment_dims = data.get('dimensions', [])\n  # attachment_value is a list of lists [[t1, x1, y1, f1], [t2, x2, y2, f2]]\n  attachment_values = data.get('value')\n\n  attachment_outcome_str = data.get('outcome')\n  if attachment_outcome_str not in TEST_RUN_STATUS_NAME_TO_MEASUREMENT_OUTCOME:\n    # Fpr backward compatibility with saved data we'll convert integers to str\n    try:\n      attachment_outcome_str = test_runs_pb2.Status.Name(\n          int(attachment_outcome_str))\n    except ValueError:\n      attachment_outcome_str = None\n\n  # Convert test status outcome str to measurement outcome\n  outcome = TEST_RUN_STATUS_NAME_TO_MEASUREMENT_OUTCOME.get(\n      attachment_outcome_str)\n\n  # convert dimensions into htf.Dimensions\n  _lazy_load_units_by_code()\n  dims = []\n  for d in attachment_dims:\n    # Try to convert into htf.Dimension including backwards compatibility.\n    unit = UNITS_BY_CODE.get(d.get('uom_code'), units.NONE)\n    description = d.get('name', '')\n    dims.append(measurements.Dimension(description=description, unit=unit))\n\n  # Attempt to determine if units are included.\n  if attachment_values and len(dims) == len(attachment_values[0]):\n    # units provided\n    units_ = dims[-1].unit\n    dimensions = dims[:-1]\n  else:\n    units_ = None\n    dimensions = dims\n\n  # created dimensioned_measured_value and populate with values.\n  measured_value = measurements.DimensionedMeasuredValue(\n      name=name,\n      num_dimensions=len(dimensions)\n  )\n  for row in attachment_values:\n    coordinates = tuple(row[:-1])\n    val = row[-1]\n    measured_value[coordinates] = val\n\n  measurement = measurements.Measurement(\n      name=name,\n      units=units_,\n      dimensions=tuple(dimensions),\n      measured_value=measured_value,\n      outcome=outcome\n  )\n  return measurement"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _copy_unidimensional_measurement(\n      self, phase, name, measurement, mfg_event):\n    \"\"\"Copy uni-dimensional measurements to the MfgEvent.\"\"\"\n    mfg_measurement = mfg_event.measurement.add()\n\n    # Copy basic measurement fields.\n    mfg_measurement.name = name\n    if measurement.docstring:\n      mfg_measurement.description = measurement.docstring\n    mfg_measurement.parameter_tag.append(phase.name)\n    if (measurement.units and\n        measurement.units.code in test_runs_converter.UOM_CODE_MAP):\n      mfg_measurement.unit_code = (\n          test_runs_converter.UOM_CODE_MAP[measurement.units.code])\n\n    # Copy failed measurements as failure_codes. This happens early to include\n    # unset measurements.\n    if (measurement.outcome != measurements.Outcome.PASS and\n        phase.outcome != htf_test_record.PhaseOutcome.SKIP):\n      failure_code = mfg_event.failure_codes.add()\n      failure_code.code = name\n      failure_code.details = '\\n'.join(str(v) for v in measurement.validators)\n\n    # Copy measurement value.\n    measured_value = measurement.measured_value\n    status_str = MEASUREMENT_OUTCOME_TO_TEST_RUN_STATUS_NAME[\n        measurement.outcome]\n    mfg_measurement.status = test_runs_pb2.Status.Value(status_str)\n    if not measured_value.is_value_set:\n      return\n    value = measured_value.value\n\n    if isinstance(value, numbers.Number):\n      mfg_measurement.numeric_value = float(value)\n    elif isinstance(value, bytes):\n      # text_value expects unicode or ascii-compatible strings, so we must\n      # 'decode' it, even if it's actually just garbage bytestring data.\n      mfg_measurement.text_value = unicode(value, errors='replace')\n    elif isinstance(value, unicode):\n      # Don't waste time and potential errors decoding unicode.\n      mfg_measurement.text_value = value\n    else:\n      # Coercing to string.\n      mfg_measurement.text_value = str(value)\n\n    # Copy measurement validators.\n    for validator in measurement.validators:\n      if isinstance(validator, validators.RangeValidatorBase):\n        if validator.minimum is not None:\n          mfg_measurement.numeric_minimum = float(validator.minimum)\n        if validator.maximum is not None:\n          mfg_measurement.numeric_maximum = float(validator.maximum)\n      elif isinstance(validator, validators.RegexMatcher):\n        mfg_measurement.expected_text = validator.regex\n      else:\n        mfg_measurement.description += '\\nValidator: ' + str(validator)", "response": "Copies the unidimensional measurement to the MfgEvent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _copy_attachment(self, name, data, mimetype, mfg_event):\n    attachment = mfg_event.attachment.add()\n    attachment.name = name\n    if isinstance(data, unicode):\n      data = data.encode('utf8')\n    attachment.value_binary = data\n    if mimetype in test_runs_converter.MIMETYPE_MAP:\n      attachment.type = test_runs_converter.MIMETYPE_MAP[mimetype]\n    elif mimetype == test_runs_pb2.MULTIDIM_JSON:\n      attachment.type = mimetype\n    else:\n      attachment.type = test_runs_pb2.BINARY", "response": "Copies an attachment to mfg_event."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, data, timeout_ms=None):\n    timeout = timeouts.PolledTimeout.from_millis(timeout_ms)\n    # Break the data up into our transport's maxdata sized WRTE messages.\n    while data:\n      self._transport.write(\n          data[:self._transport.adb_connection.maxdata], timeout)\n      data = data[self._transport.adb_connection.maxdata:]", "response": "Writes data to this stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(self, length=0, timeout_ms=None):\n    return self._transport.read(\n        length, timeouts.PolledTimeout.from_millis(timeout_ms))", "response": "Reads data from the remote end of this stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_until_close(self, timeout_ms=None):\n    while True:\n      try:\n        yield self.read(timeout_ms=timeout_ms)\n      except usb_exceptions.AdbStreamClosedError:\n        break", "response": "Yields data from a single call to read until the stream is closed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_or_check_remote_id(self, remote_id):\n    if not self.remote_id:\n      assert self.closed_state == self.ClosedState.PENDING, 'Bad ClosedState!'\n      self.remote_id = remote_id\n      self.closed_state = self.ClosedState.OPEN\n    elif self.remote_id != remote_id:\n      raise usb_exceptions.AdbProtocolError(\n          '%s remote-id change to %s', self, remote_id)", "response": "Set or check the remote id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a command over this AdbConnection.", "response": "def _send_command(self, command, timeout, data=''):\n    \"\"\"Send the given command/data over this transport.\n\n    We do a couple sanity checks in here to be sure we can format a valid\n    AdbMessage for our underlying AdbConnection, and then send it.  This method\n    can be used to send any message type, and doesn't do any state tracking or\n    acknowledgement checking.\n\n    Args:\n      command: The command to send, should be one of 'OKAY', 'WRTE', or 'CLSE'\n      timeout: timeouts.PolledTimeout to use for this operation\n      data: If provided, data to send with the AdbMessage.\n    \"\"\"\n    if len(data) > self.adb_connection.maxdata:\n      raise usb_exceptions.AdbProtocolError('Message data too long (%s>%s): %s',\n                                            len(data),\n                                            self.adb_connection.maxdata, data)\n    if not self.remote_id:\n      # If we get here, we probably missed the OKAY response to our OPEN.  We\n      # should have failed earlier, but in case someone does something tricky\n      # with multiple threads, we sanity check this here.\n      raise usb_exceptions.AdbProtocolError('%s send before OKAY: %s',\n                                            self, data)\n    self.adb_connection.transport.write_message(\n        adb_message.AdbMessage(command, self.local_id, self.remote_id, data),\n        timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_message(self, message, handle_wrte=True):\n    if message.command == 'OKAY':\n      self._set_or_check_remote_id(message.arg0)\n      if not self._expecting_okay:\n        raise usb_exceptions.AdbProtocolError(\n            '%s received unexpected OKAY: %s', self, message)\n      self._expecting_okay = False\n    elif message.command == 'CLSE':\n      self.closed_state = self.ClosedState.CLOSED\n    elif not handle_wrte:\n      raise usb_exceptions.AdbProtocolError(\n          '%s received WRTE before OKAY/CLSE: %s', self, message)\n    else:\n      with self._read_buffer_lock:\n        self._read_buffer.append(message.data)\n        self._buffer_size += len(message.data)", "response": "Handle a message that was read for this stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_messages_until_true(self, predicate, timeout):\n    while not predicate():\n      # Hold the message_received Lock while we try to acquire the reader_lock\n      # and waiting on the message_received condition, to prevent another reader\n      # thread from notifying the condition between us failing to acquire the\n      # reader_lock and waiting on the condition.\n      self._message_received.acquire()\n      if self._reader_lock.acquire(False):\n        try:\n          # Release the message_received Lock while we do the read so other\n          # threads can wait() on the condition without having to block on\n          # acquiring the message_received Lock (we may have a longer timeout\n          # than them, so that would be bad).\n          self._message_received.release()\n\n          # We are now the thread responsible for reading a message.  Check\n          # predicate() to make sure nobody else read a message between our last\n          # check and acquiring the reader Lock.\n          if predicate():\n            return\n\n          # Read and handle a message, using our timeout.\n          self._handle_message(\n              self.adb_connection.read_for_stream(self, timeout))\n\n          # Notify anyone interested that we handled a message, causing them to\n          # check their predicate again.\n          with self._message_received:\n            self._message_received.notify_all()\n        finally:\n          self._reader_lock.release()\n      else:\n        # There is some other thread reading a message.  Since we are already\n        # holding the message_received Lock, we can immediately do the wait.\n        try:\n          self._message_received.wait(timeout.remaining)\n          if timeout.has_expired():\n            raise usb_exceptions.AdbTimeoutError(\n                '%s timed out reading messages.', self)\n        finally:\n          # Make sure we release this even if an exception occurred.\n          self._message_received.release()", "response": "Read a message from this stream and handle it."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures this stream transport was successfully opened.", "response": "def ensure_opened(self, timeout):\n    \"\"\"Ensure this stream transport was successfully opened.\n\n    Checks to make sure we receive our initial OKAY message.  This must be\n    called after creating this AdbStreamTransport and before calling read() or\n    write().\n\n    Args:\n      timeout: timeouts.PolledTimeout to use for this operation.\n\n    Returns:\n      True if this stream was successfully opened, False if the service was\n    not recognized by the remote endpoint.  If False is returned, then this\n    AdbStreamTransport will be already closed.\n\n    Raises:\n      AdbProtocolError: If we receive a WRTE message instead of OKAY/CLSE.\n    \"\"\"\n    self._handle_message(self.adb_connection.read_for_stream(self, timeout),\n                         handle_wrte=False)\n    return self.is_open()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the given message to this transport's queue. This method also handles ACKing any WRTE messages. Args: message: The AdbMessage to enqueue. timeout: The timeout to use for the operation. Specifically, WRTE messages cause an OKAY to be sent; timeout is used for that send.", "response": "def enqueue_message(self, message, timeout):\n    \"\"\"Add the given message to this transport's queue.\n\n    This method also handles ACKing any WRTE messages.\n\n    Args:\n      message: The AdbMessage to enqueue.\n      timeout: The timeout to use for the operation.  Specifically, WRTE\n        messages cause an OKAY to be sent; timeout is used for that send.\n    \"\"\"\n    # Ack WRTE messages immediately, handle our OPEN ack if it gets enqueued.\n    if message.command == 'WRTE':\n      self._send_command('OKAY', timeout=timeout)\n    elif message.command == 'OKAY':\n      self._set_or_check_remote_id(message.arg0)\n    self.message_queue.put(message)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite data to this stream using the given timeouts. PolledTimeout.", "response": "def write(self, data, timeout):\n    \"\"\"Write data to this stream, using the given timeouts.PolledTimeout.\"\"\"\n    if not self.remote_id:\n      raise usb_exceptions.AdbStreamClosedError(\n          'Cannot write() to half-opened %s', self)\n    if self.closed_state != self.ClosedState.OPEN:\n      raise usb_exceptions.AdbStreamClosedError(\n          'Cannot write() to closed %s', self)\n    elif self._expecting_okay:\n      raise usb_exceptions.AdbProtocolError(\n          'Previous WRTE failed, %s in unknown state', self)\n\n    # Make sure we only have one WRTE in flight at a time, because ADB doesn't\n    # identify which WRTE it is ACK'ing when it sends the OKAY message back.\n    with self._write_lock:\n      self._expecting_okay = True\n      self._send_command('WRTE', timeout, data)\n      self._read_messages_until_true(lambda: not self._expecting_okay, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, length, timeout):\n    self._read_messages_until_true(\n        lambda: self._buffer_size and self._buffer_size >= length, timeout)\n\n    with self._read_buffer_lock:\n      data, push_back = ''.join(self._read_buffer), ''\n      if length:\n        data, push_back = data[:length], data[length:]\n      self._read_buffer.clear()\n      self._buffer_size = len(push_back)\n      if push_back:\n        self._read_buffer.appendleft(push_back)\n    return data", "response": "Read length bytes from this stream transport."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclose this stream, future reads/writes will fail.", "response": "def close(self, timeout_ms):\n    \"\"\"Close this stream, future reads/writes will fail.\"\"\"\n    if self.closed_state == self.ClosedState.CLOSED:\n      return\n\n    self.closed_state = self.ClosedState.CLOSED\n    try:\n      if not self.adb_connection.close_stream_transport(\n          self, timeouts.PolledTimeout.from_millis(timeout_ms)):\n        _LOG.warning('Attempt to close mystery %s', self)\n    except usb_exceptions.AdbTimeoutError:\n      _LOG.warning('%s close() timed out, ignoring', self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an AdbStreamTransport with a newly allocated local_id.", "response": "def _make_stream_transport(self):\n    \"\"\"Create an AdbStreamTransport with a newly allocated local_id.\"\"\"\n    msg_queue = queue.Queue()\n    with self._stream_transport_map_lock:\n      # Start one past the last id we used, and grab the first available one.\n      # This mimics the ADB behavior of 'increment an unsigned and let it\n      # overflow', but with a check to ensure we don't reuse an id in use,\n      # even though that's unlikely with 2^32 - 1 of them available.  We try\n      # at most 64 id's, if we've wrapped around and there isn't one available\n      # in the first 64, there's a problem, better to fail fast than hang for\n      # a potentially very long time (STREAM_ID_LIMIT can be very large).\n      self._last_id_used = (self._last_id_used % STREAM_ID_LIMIT) + 1\n      for local_id in itertools.islice(\n          itertools.chain(\n              range(self._last_id_used, STREAM_ID_LIMIT),\n              range(1, self._last_id_used)), 64):\n        if local_id not in list(self._stream_transport_map.keys()):\n          self._last_id_used = local_id\n          break\n      else:\n        raise usb_exceptions.AdbStreamUnavailableError('Ran out of local ids!')\n      # Ignore this warning - the for loop will always have at least one\n      # iteration, so local_id will always be set.\n      # pylint: disable=undefined-loop-variable\n      stream_transport = AdbStreamTransport(self, local_id, msg_queue)\n      self._stream_transport_map[local_id] = stream_transport\n    return stream_transport"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle an incoming message for a given stream.", "response": "def _handle_message_for_stream(self, stream_transport, message, timeout):\n    \"\"\"Handle an incoming message, check if it's for the given stream.\n\n    If the message is not for the stream, then add it to the appropriate\n    message queue.\n\n    Args:\n      stream_transport: AdbStreamTransport currently waiting on a message.\n      message: Message to check and handle.\n      timeout: Timeout to use for the operation, should be an instance of\n        timeouts.PolledTimeout.\n\n    Returns:\n      The message read if it was for this stream, None otherwise.\n\n    Raises:\n      AdbProtocolError: If we receive an unexpected message type.\n    \"\"\"\n    if message.command not in ('OKAY', 'CLSE', 'WRTE'):\n      raise usb_exceptions.AdbProtocolError(\n          '%s received unexpected message: %s', self, message)\n\n    if message.arg1 == stream_transport.local_id:\n      # Ack writes immediately.\n      if message.command == 'WRTE':\n        # Make sure we don't get a WRTE before an OKAY/CLSE message.\n        if not stream_transport.remote_id:\n          raise usb_exceptions.AdbProtocolError(\n              '%s received WRTE before OKAY/CLSE: %s',\n              stream_transport, message)\n        self.transport.write_message(adb_message.AdbMessage(\n            'OKAY', stream_transport.local_id, stream_transport.remote_id),\n                                     timeout)\n      elif message.command == 'CLSE':\n        self.close_stream_transport(stream_transport, timeout)\n      return message\n    else:\n      # Message was not for this stream, add it to the right stream's queue.\n      with self._stream_transport_map_lock:\n        dest_transport = self._stream_transport_map.get(message.arg1)\n\n      if dest_transport:\n        if message.command == 'CLSE':\n          self.close_stream_transport(dest_transport, timeout)\n        dest_transport.enqueue_message(message, timeout)\n      else:\n        _LOG.warning('Received message for unknown local-id: %s', message)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open_stream(self, destination, timeout_ms=None):\n    timeout = timeouts.PolledTimeout.from_millis(timeout_ms)\n\n    stream_transport = self._make_stream_transport()\n    self.transport.write_message(\n        adb_message.AdbMessage(\n            command='OPEN',\n            arg0=stream_transport.local_id, arg1=0,\n            data=destination + '\\0'),\n        timeout)\n    if not stream_transport.ensure_opened(timeout):\n      return None\n    return AdbStream(destination, stream_transport)", "response": "Opens a new stream to a destination service on the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close_stream_transport(self, stream_transport, timeout):\n    with self._stream_transport_map_lock:\n      if stream_transport.local_id in self._stream_transport_map:\n        del self._stream_transport_map[stream_transport.local_id]\n        # If we never got a remote_id, there's no CLSE message to send.\n        if stream_transport.remote_id:\n          self.transport.write_message(adb_message.AdbMessage(\n              'CLSE', stream_transport.local_id, stream_transport.remote_id),\n                                       timeout)\n        return True\n    return False", "response": "Closes the given stream transport."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattempting to read a packet from the given stream transport.", "response": "def read_for_stream(self, stream_transport, timeout_ms=None):\n    \"\"\"Attempt to read a packet for the given stream transport.\n\n    Will read packets from self.transport until one intended for the given\n    AdbStream is found.  If another thread is already reading packets, this will\n    block until that thread reads a packet for this stream, or timeout expires.\n    Note that this method always returns None, but if a packet was read for the\n    given stream, it will have been added to that stream's message queue.\n\n    This is somewhat tricky to do - first we check if there's a message already\n    in our queue.  If not, then we try to use our AdbConnection to read a\n    message for this stream.\n\n    If some other thread is already doing reads, then read_for_stream() will sit\n    in a tight loop, with a short delay, checking our message queue for a\n    message from the other thread.\n\n    Note that we must pass the queue in from the AdbStream, rather than looking\n    it up in the AdbConnection's map, because the AdbConnection may have\n    removed the queue from its map (while it still had messages in it).  The\n    AdbStream itself maintains a reference to the queue to avoid dropping those\n    messages.\n\n    The AdbMessage read is guaranteed to be one of 'OKAY', 'WRTE', or 'CLSE'.\n    If it was a WRTE message, then it will have been automatically ACK'd with an\n    OKAY message, if it was a CLSE message it will have been ACK'd with a\n    corresponding CLSE message, and this AdbStream will be marked as closed.\n\n    Args:\n      stream_transport: The AdbStreamTransport for the stream that is reading\n        an AdbMessage from this AdbConnection.\n      timeout_ms: If provided, timeout, in milliseconds, to use.  Note this\n        timeout applies to this entire call, not for each individual Read, since\n        there may be multiple reads if messages for other streams are read.\n        This argument may be a timeouts.PolledTimeout.\n\n    Returns:\n      AdbMessage that was read, guaranteed to be one of 'OKAY', 'CLSE', or\n    'WRTE' command.\n\n    Raises:\n      AdbTimeoutError: If we don't get a packet for this stream before\n        timeout expires.\n      AdbStreamClosedError: If the given stream has been closed.\n    \"\"\"\n    timeout = timeouts.PolledTimeout.from_millis(timeout_ms)\n    # Bail when the timeout expires, or when we no longer have the given stream\n    # in our map (it may have been closed, we don't want to leave a thread\n    # hanging in this loop when that happens).\n    while (not timeout.has_expired() and\n           stream_transport.local_id in self._stream_transport_map):\n      try:\n        # Block for up to 10ms to rate-limit how fast we spin.\n        return stream_transport.message_queue.get(True, .01)\n      except queue.Empty:\n        pass\n\n      # If someone else has the Lock, just keep checking our queue.\n      if not self._reader_lock.acquire(False):\n        continue\n\n      try:\n        # Now that we've acquired the Lock, we have to check the queue again,\n        # just in case someone had the Lock but hadn't yet added our message\n        # to the queue when we checked the first time.  Now that we have the\n        # Lock ourselves, we're sure there are no potentially in-flight reads.\n        try:\n          return stream_transport.message_queue.get_nowait()\n        except queue.Empty:\n          pass\n\n        while not timeout.has_expired():\n          msg = self._handle_message_for_stream(\n              stream_transport, self.transport.read_message(timeout), timeout)\n          if msg:\n            return msg\n      finally:\n        self._reader_lock.release()\n\n    if timeout.has_expired():\n      raise usb_exceptions.AdbTimeoutError(\n          'Read timed out for %s', stream_transport)\n\n    # The stream is no longer in the map, so it's closed, but check for any\n    # queued messages.\n    try:\n      return stream_transport.message_queue.get_nowait()\n    except queue.Empty:\n      raise usb_exceptions.AdbStreamClosedError(\n          'Attempt to read from closed or unknown %s', stream_transport)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect(cls, transport, rsa_keys=None, timeout_ms=1000,\n              auth_timeout_ms=100):\n    \"\"\"Establish a new connection to a device, connected via transport.\n\n    Args:\n      transport: A transport to use for reads/writes from/to the device,\n        usually an instance of UsbHandle, but really it can be anything with\n        read() and write() methods.\n      rsa_keys: List of AuthSigner subclass instances to be used for\n        authentication. The device can either accept one of these via the sign\n        method, or we will send the result of get_public_key from the first one\n        if the device doesn't accept any of them.\n      timeout_ms: Timeout to wait for the device to respond to our CNXN\n        request.  Actual timeout may take longer if the transport object passed\n        has a longer default timeout than timeout_ms, or if auth_timeout_ms is\n        longer than timeout_ms and public key auth is used.  This argument may\n        be a PolledTimeout object.\n      auth_timeout_ms: Timeout to wait for when sending a new public key. This\n        is only relevant when we send a new public key. The device shows a\n        dialog and this timeout is how long to wait for that dialog. If used\n        in automation, this should be low to catch such a case as a failure\n        quickly; while in interactive settings it should be high to allow\n        users to accept the dialog. We default to automation here, so it's low\n        by default.  This argument may be a PolledTimeout object.\n\n    Returns:\n      An instance of AdbConnection that is connected to the device.\n\n    Raises:\n      usb_exceptions.DeviceAuthError: When the device expects authentication,\n        but we weren't given any valid keys.\n      usb_exceptions.AdbProtocolError: When the device does authentication in an\n        unexpected way, or fails to respond appropriately to our CNXN request.\n    \"\"\"\n    timeout = timeouts.PolledTimeout.from_millis(timeout_ms)\n    if ADB_MESSAGE_LOG:\n      adb_transport = adb_message.DebugAdbTransportAdapter(transport)\n    else:\n      adb_transport = adb_message.AdbTransportAdapter(transport)\n    adb_transport.write_message(\n        adb_message.AdbMessage(\n            command='CNXN', arg0=ADB_VERSION, arg1=MAX_ADB_DATA,\n            data='host::%s\\0' % ADB_BANNER),\n        timeout)\n\n    msg = adb_transport.read_until(('AUTH', 'CNXN'), timeout)\n    if msg.command == 'CNXN':\n      return cls(adb_transport, msg.arg1, msg.data)\n\n    # We got an AUTH response, so we have to try to authenticate.\n    if not rsa_keys:\n      raise usb_exceptions.DeviceAuthError(\n          'Device authentication required, no keys available.')\n\n    # Loop through our keys, signing the last 'banner' or token.\n    for rsa_key in rsa_keys:\n      if msg.arg0 != cls.AUTH_TOKEN:\n        raise usb_exceptions.AdbProtocolError('Bad AUTH response: %s', msg)\n\n      signed_token = rsa_key.sign(msg.data)\n      adb_transport.write_message(\n          adb_message.AdbMessage(\n              command='AUTH', arg0=cls.AUTH_SIGNATURE, arg1=0,\n              data=signed_token),\n          timeout)\n\n      msg = adb_transport.read_until(('AUTH', 'CNXN'), timeout)\n      if msg.command == 'CNXN':\n        return cls(adb_transport, msg.arg1, msg.data)\n\n    # None of the keys worked, so send a public key.\n    adb_transport.write_message(\n        adb_message.AdbMessage(\n            command='AUTH', arg0=cls.AUTH_RSAPUBLICKEY, arg1=0,\n            data=rsa_keys[0].get_public_key() + '\\0'),\n        timeout)\n    try:\n      msg = adb_transport.read_until(\n          ('CNXN',), timeouts.PolledTimeout.from_millis(auth_timeout_ms))\n    except usb_exceptions.UsbReadFailedError as exception:\n      if exception.is_timeout():\n        exceptions.reraise(usb_exceptions.DeviceAuthError,\n                           'Accept auth key on device, then retry.')\n      raise\n\n    # The read didn't time-out, so we got a CNXN response.\n    return cls(adb_transport, msg.arg1, msg.data)", "response": "Establish a new connection to a device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef increment(self):\n    self.value += self.increment_size\n    return self.value - self.increment_size", "response": "Increment our value return the previous value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plug(update_kwargs=True, **plugs_map):\n  for a_plug in plugs_map.values():\n    if not (isinstance(a_plug, PlugPlaceholder)\n            or issubclass(a_plug, BasePlug)):\n      raise InvalidPlugError(\n          'Plug %s is not a subclass of plugs.BasePlug nor a placeholder '\n          'for one' % a_plug)\n\n  def result(func):\n    \"\"\"Wrap the given function and return the wrapper.\n\n    Args:\n      func: The function to wrap.\n\n    Returns:\n      A PhaseDescriptor that, when called will invoke the wrapped function,\n        passing plugs as keyword args.\n\n    Raises:\n      DuplicatePlugError:  If a plug name is declared twice for the\n          same function.\n    \"\"\"\n    phase = openhtf.core.phase_descriptor.PhaseDescriptor.wrap_or_copy(func)\n    duplicates = (frozenset(p.name for p in phase.plugs) &\n                  frozenset(plugs_map))\n    if duplicates:\n      raise DuplicatePlugError(\n          'Plugs %s required multiple times on phase %s' % (duplicates, func))\n\n    phase.plugs.extend([\n        PhasePlug(name, a_plug, update_kwargs=update_kwargs)\n        for name, a_plug in six.iteritems(plugs_map)])\n    return phase\n  return result", "response": "Creates a decorator that will wrap the given function and return the resulting PhaseDescriptor that will replace positional arguments with the plugs specified."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uses_base_tear_down(cls):\n    this_tear_down = getattr(cls, 'tearDown')\n    base_tear_down = getattr(BasePlug, 'tearDown')\n    return this_tear_down.__code__ is base_tear_down.__code__", "response": "Checks whether the tearDown method is the BasePlug implementation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of names identifying the plug classes in the plug s MRO.", "response": "def get_plug_mro(self, plug_type):\n    \"\"\"Returns a list of names identifying the plug classes in the plug's MRO.\n\n    For example:\n        ['openhtf.plugs.user_input.UserInput']\n    Or:\n        ['openhtf.plugs.user_input.UserInput',\n         'my_module.advanced_user_input.AdvancedUserInput']\n    \"\"\"\n    ignored_classes = (BasePlug, FrontendAwareBasePlug)\n    return [\n        self.get_plug_name(base_class) for base_class in plug_type.mro()\n        if (issubclass(base_class, BasePlug) and\n            base_class not in ignored_classes)\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef initialize_plugs(self, plug_types=None):\n    types = plug_types if plug_types is not None else self._plug_types\n    for plug_type in types:\n      # Create a logger for this plug. All plug loggers go under the 'plug'\n      # sub-logger in the logger hierarchy.\n      plug_logger = self.logger.getChild(plug_type.__name__)\n      if plug_type in self._plugs_by_type:\n        continue\n      try:\n        if not issubclass(plug_type, BasePlug):\n          raise InvalidPlugError(\n              'Plug type \"%s\" is not an instance of BasePlug' % plug_type)\n        if plug_type.logger != _LOG:\n          # They put a logger attribute on the class itself, overriding ours.\n          raise InvalidPlugError(\n              'Do not override \"logger\" in your plugs.', plug_type)\n\n        # Override the logger so that __init__'s logging goes into the record.\n        plug_type.logger = plug_logger\n        try:\n          plug_instance = plug_type()\n        finally:\n          # Now set it back since we'll give the instance a logger in a moment.\n          plug_type.logger = _LOG\n        # Set the logger attribute directly (rather than in BasePlug) so we\n        # don't depend on subclasses' implementation of __init__ to have it\n        # set.\n        if plug_instance.logger != _LOG:\n          raise InvalidPlugError(\n              'Do not set \"self.logger\" in __init__ in your plugs', plug_type)\n        else:\n          # Now the instance has its own copy of the test logger.\n          plug_instance.logger = plug_logger\n      except Exception:  # pylint: disable=broad-except\n        plug_logger.exception('Exception instantiating plug type %s', plug_type)\n        self.tear_down_plugs()\n        raise\n      self.update_plug(plug_type, plug_instance)", "response": "Instantiate required plugs and saves the instances in self. _plug_types_by_type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_plug(self, plug_type, plug_value):\n    self._plug_types.add(plug_type)\n    if plug_type in self._plugs_by_type:\n      self._plugs_by_type[plug_type].tearDown()\n    plug_name = self.get_plug_name(plug_type)\n    self._plugs_by_type[plug_type] = plug_value\n    self._plugs_by_name[plug_name] = plug_value\n    self._plug_descriptors[plug_name] = self._make_plug_descriptor(plug_type)", "response": "Update internal data stores with the given plug value for plug type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef provide_plugs(self, plug_name_map):\n    return {name: self._plugs_by_type[cls] for name, cls in plug_name_map}", "response": "Provide the requested plugs as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntears down all instantiated plugs.", "response": "def tear_down_plugs(self):\n    \"\"\"Call tearDown() on all instantiated plugs.\n\n    Note that initialize_plugs must have been called before calling\n    this method, and initialize_plugs must be called again after calling\n    this method if you want to access the plugs attribute again.\n\n    Any exceptions in tearDown() methods are logged, but do not get raised\n    by this method.\n    \"\"\"\n    _LOG.debug('Tearing down all plugs.')\n    for plug_type, plug_instance in six.iteritems(self._plugs_by_type):\n      if plug_instance.uses_base_tear_down():\n        name = '<PlugTearDownThread: BasePlug No-Op for %s>' % plug_type\n      else:\n        name = '<PlugTearDownThread: %s>' % plug_type\n      thread = _PlugTearDownThread(plug_instance, name=name)\n      thread.start()\n      timeout_s = (conf.plug_teardown_timeout_s\n                   if conf.plug_teardown_timeout_s\n                   else None)\n      thread.join(timeout_s)\n      if thread.is_alive():\n        thread.kill()\n        _LOG.warning('Killed tearDown for plug %s after timeout.',\n                     plug_instance)\n    self._plugs_by_type.clear()\n    self._plugs_by_name.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_for_plug_update(self, plug_name, remote_state, timeout_s):\n    plug = self._plugs_by_name.get(plug_name)\n\n    if plug is None:\n      raise InvalidPlugError('Cannot wait on unknown plug \"%s\".' % plug_name)\n\n    if not isinstance(plug, FrontendAwareBasePlug):\n      raise InvalidPlugError('Cannot wait on a plug %s that is not an subclass '\n                             'of FrontendAwareBasePlug.' % plug_name)\n\n    state, update_event = plug.asdict_with_event()\n    if state != remote_state:\n      return state\n\n    if update_event.wait(timeout_s):\n      return plug._asdict()", "response": "Waits for a change in the state of a frontend - aware plug."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the names of frontend - aware plugs.", "response": "def get_frontend_aware_plug_names(self):\n    \"\"\"Returns the names of frontend-aware plugs.\"\"\"\n    return [name for name, plug in six.iteritems(self._plugs_by_name)\n            if isinstance(plug, FrontendAwareBasePlug)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _wait_for_any_event(events, timeout_s):\n  def any_event_set():\n    return any(event.is_set() for event in events)\n\n  result = timeouts.loop_until_timeout_or_true(\n      timeout_s, any_event_set, sleep_s=_WAIT_FOR_ANY_EVENT_POLL_S)\n\n  return result or any_event_set()", "response": "Wait for any in a list of threading. Event s to be set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the update loop.", "response": "def run(self):\n    \"\"\"Call self._poll_for_update() in a loop and handle errors.\"\"\"\n    while True:\n      try:\n        self._poll_for_update()\n      except RuntimeError as error:\n        # Note that because logging triggers a call to notify_update(), by\n        # logging a message, we automatically retry publishing the update\n        # after an error occurs.\n        if error.message == 'dictionary changed size during iteration':\n          # These errors occur occasionally and it is infeasible to get rid of\n          # them entirely unless data.convert_to_base_types() is made\n          # thread-safe. Ignore the error and retry quickly.\n          _LOG.debug('Ignoring (probably harmless) error in station watcher: '\n                     '`dictionary changed size during iteration`.')\n          time.sleep(0.1)\n        else:\n          _LOG.exception('Error in station watcher: %s', error)\n          time.sleep(1)\n      except Exception as error:  # pylint: disable=broad-except\n        _LOG.exception('Error in station watcher: %s', error)\n        time.sleep(1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _poll_for_update(self):\n    test, test_state = _get_executing_test()\n\n    if test is None:\n      time.sleep(_WAIT_FOR_EXECUTING_TEST_POLL_S)\n      return\n\n    state_dict, event = self._to_dict_with_event(test_state)\n    self._update_callback(state_dict)\n\n    plug_manager = test_state.plug_manager\n    plug_events = [\n        plug_manager.get_plug_by_class_path(plug_name).asdict_with_event()[1]\n        for plug_name in plug_manager.get_frontend_aware_plug_names()\n    ]\n    events = [event] + plug_events\n\n    # Wait for the test state or a plug state to change, or for the previously\n    # executing test to finish.\n    while not _wait_for_any_event(events, _CHECK_FOR_FINISHED_TEST_POLL_S):\n      new_test, _ = _get_executing_test()\n      if test != new_test:\n        break", "response": "Call the callback with the current test state then wait for a change."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _to_dict_with_event(cls, test_state):\n    original_dict, event = test_state.asdict_with_event()\n\n    # This line may produce a 'dictionary changed size during iteration' error.\n    test_state_dict = data.convert_to_base_types(original_dict)\n\n    test_state_dict['execution_uid'] = test_state.execution_uid\n    return test_state_dict, event", "response": "Process a test state into the format we want to send to the frontend."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _safe_lock_release_py2(rlock):\n  assert isinstance(rlock, threading._RLock)\n  ident = _thread.get_ident()\n  expected_count = 0\n  if rlock._RLock__owner == ident:\n    expected_count = rlock._RLock__count\n  try:\n    yield\n  except ThreadTerminationError:\n    # Check if the current thread still owns the lock by checking if we can\n    # acquire the underlying lock.\n    if rlock._RLock__block.acquire(0):\n      # Lock is clean, so unlock and we are done.\n      rlock._RLock__block.release()\n    elif rlock._RLock__owner == ident and expected_count > 0:\n      # The lock is still held up the stack, so make sure the count is accurate.\n      if rlock._RLock__count != expected_count:\n        rlock._RLock__count = expected_count\n    elif rlock._RLock__owner == ident or rlock._RLock__owner is None:\n      # The internal lock is still acquired, but either this thread or no thread\n      # owns it, which means it needs to be hard reset.\n      rlock._RLock__owner = None\n      rlock._RLock__count = 0\n      rlock._RLock__block.release()\n    raise", "response": "Ensures that a threading. RLock object is fully released for Python 2."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loop(_=None, force=False):   # pylint: disable=invalid-name\n  if not force:\n    raise AttributeError(\n        'threads.loop() is DEPRECATED.  If you really like this and want to '\n        'keep it, file an issue at https://github.com/google/openhtf/issues '\n        'and use it as @loop(force=True) for now.')\n\n  def real_loop(fn):\n    @functools.wraps(fn)\n    def _proc(*args, **kwargs):\n      \"\"\"Wrapper to return.\"\"\"\n      while True:\n        fn(*args, **kwargs)\n    _proc.once = fn  # way for tests to invoke the function once\n                     # you may need to pass in \"self\" since this may be unbound.\n    return _proc\n  return real_loop", "response": "A decorator that returns a function to loop indefinitely."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef synchronized(func):  # pylint: disable=invalid-name\n  @functools.wraps(func)\n  def synchronized_method(self, *args, **kwargs):\n    \"\"\"Wrapper to return.\"\"\"\n    if not hasattr(self, '_lock'):\n      if func.__name__ in type(self).__dict__:\n        hint = ''\n      else:\n        hint = (' Might be missing call to super in %s.__init__?' %\n                type(self).__name__)\n      raise RuntimeError('Can\\'t synchronize method `%s` of %s without '\n                         'attribute `_lock`.%s' %\n                         (func.__name__, type(self).__name__, hint))\n    with self._lock:  # pylint: disable=protected-access\n      return func(self, *args, **kwargs)\n  return synchronized_method", "response": "Decorator to synchronize a method."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kill(self):\n    self._killed.set()\n    if not self.is_alive():\n      logging.debug('Cannot kill thread that is no longer running.')\n      return\n    if not self._is_thread_proc_running():\n      logging.debug(\"Thread's _thread_proc function is no longer running, \"\n                    'will not kill; letting thread exit gracefully.')\n      return\n    self.async_raise(ThreadTerminationError)", "response": "Terminates the current thread by raising an error."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_record_logger_for(test_uid):\n  htf_logger = logging.getLogger(RECORD_LOGGER_PREFIX)\n  record_logger = HtfTestLogger('.'.join(((RECORD_LOGGER_PREFIX, test_uid))))\n  record_logger.parent = htf_logger\n  return record_logger", "response": "Return the child logger associated with the specified test UID."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef initialize_record_handler(test_uid, test_record, notify_update):\n  htf_logger = logging.getLogger(LOGGER_PREFIX)\n  htf_logger.addHandler(RecordHandler(test_uid, test_record, notify_update))", "response": "Initialize the record handler for a test."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef log_once(log_func, msg, *args, **kwargs):\n  if msg not in _LOG_ONCE_SEEN:\n    log_func(msg, *args, **kwargs)\n    # Key on the message, ignoring args. This should fit most use cases.\n    _LOG_ONCE_SEEN.add(msg)", "response": "Logs a message only once."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef configure_logging():\n\n  # Define the top-level logger.\n  htf_logger = logging.getLogger(LOGGER_PREFIX)\n  htf_logger.propagate = False\n  htf_logger.setLevel(logging.DEBUG)\n\n  # By default, don't print any logs to the CLI.\n  if CLI_LOGGING_VERBOSITY == 0:\n    htf_logger.addHandler(logging.NullHandler())\n    return\n\n  if CLI_LOGGING_VERBOSITY == 1:\n    logging_level = logging.INFO\n  else:\n    logging_level = logging.DEBUG\n\n  # Configure a handler to print to the CLI.\n  cli_handler = KillableThreadSafeStreamHandler(stream=sys.stdout)\n  cli_handler.setFormatter(CliFormatter())\n  cli_handler.setLevel(logging_level)\n  cli_handler.addFilter(MAC_FILTER)\n  htf_logger.addHandler(cli_handler)\n\n  # Suppress CLI logging if the --quiet flag is used, or while CLI_QUIET is set\n  # in the console_output module.\n  cli_handler.addFilter(console_output.CliQuietFilter())", "response": "One - time initialization of loggers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave a logging. LogRecord to our test record.", "response": "def emit(self, record):\n    \"\"\"Save a logging.LogRecord to our test record.\n\n    Logs carry useful metadata such as the logger name and level information.\n    We capture this in a structured format in the test record to enable\n    filtering by client applications.\n\n    Args:\n      record: A logging.LogRecord to record.\n    \"\"\"\n    try:\n      message = self.format(record)\n      log_record = LogRecord(\n          record.levelno, record.name, os.path.basename(record.pathname),\n          record.lineno, int(record.created * 1000), message,\n      )\n      self._test_record.add_log_record(log_record)\n      self._notify_update()\n    except Exception:  # pylint: disable=broad-except\n      self.handleError(record)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format(self, record):\n    super(CliFormatter, self).format(record)\n    localized_time = datetime.datetime.fromtimestamp(record.created)\n    terse_time = localized_time.strftime(u'%H:%M:%S')\n    terse_level = record.levelname[0]\n    terse_name = record.name.split('.')[-1]\n    match = RECORD_LOGGER_RE.match(record.name)\n    if match:\n      # Figure out which OpenHTF subsystem the record came from.\n      subsys_match = SUBSYSTEM_LOGGER_RE.match(record.name)\n      if subsys_match:\n        terse_name = '<{subsys}: {id}>'.format(\n            subsys=subsys_match.group('subsys'),\n            id=subsys_match.group('id'))\n      else:\n        # Fall back to using the last five characters of the test UUID.\n        terse_name = '<test %s>' % match.group('test_uid')[-5:]\n    return '{lvl} {time} {logger} - {msg}'.format(lvl=terse_level,\n                                                  time=terse_time,\n                                                  logger=terse_name,\n                                                  msg=record.message)", "response": "Format the record as tersely as possible but preserve info."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _send_mfg_inspector_request(envelope_data, credentials, destination_url):\n  logging.info('Uploading result...')\n  http = httplib2.Http()\n\n  if credentials.access_token_expired:\n    credentials.refresh(http)\n  credentials.authorize(http)\n\n  resp, content = http.request(destination_url, 'POST', envelope_data)\n\n  try:\n    result = json.loads(content)\n  except Exception:\n    logging.debug('Upload failed with response %s: %s', resp, content)\n    raise UploadFailedError(resp, content)\n\n  if resp.status != 200:\n    logging.debug('Upload failed: %s', result)\n    raise UploadFailedError(result['error'], result)\n\n  return result", "response": "Send Mfg Inspector request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_mfg_inspector_data(inspector_proto, credentials, destination_url):\n  envelope = guzzle_pb2.TestRunEnvelope()\n  envelope.payload = zlib.compress(inspector_proto.SerializeToString())\n  envelope.payload_type = guzzle_pb2.COMPRESSED_MFG_EVENT\n  envelope_data = envelope.SerializeToString()\n\n  for _ in xrange(5):\n    try:\n      result = _send_mfg_inspector_request(\n          envelope_data, credentials, destination_url)\n      return result\n    except UploadFailedError:\n      time.sleep(1)\n\n  logging.critical(\n      'Could not upload to mfg-inspector after 5 attempts. Giving up.')\n\n  return {}", "response": "Upload MfgEvent to steam_engine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_json(cls, json_data):\n    return cls(user=json_data['client_email'],\n               keydata=json_data['private_key'],\n               token_uri=json_data['token_uri'])", "response": "Create an uploader given a JSON - formatted MfgInspectorCallback object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a test record to a mfg - inspector proto.", "response": "def _convert(self, test_record_obj):\n    \"\"\"Convert and cache a test record to a mfg-inspector proto.\"\"\"\n\n    if self._cached_proto is None:\n      self._cached_proto = self._converter(test_record_obj)\n\n    return self._cached_proto"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_to_disk(self, filename_pattern=None):\n    if not self._converter:\n      raise RuntimeError(\n          'Must set _converter on subclass or via set_converter before calling '\n          'save_to_disk.')\n\n    pattern = filename_pattern or self._default_filename_pattern\n    if not pattern:\n      raise RuntimeError(\n          'Must specify provide a filename_pattern or set a '\n          '_default_filename_pattern on subclass.')\n\n    def save_to_disk_callback(test_record_obj):\n      proto = self._convert(test_record_obj)\n      output_to_file = callbacks.OutputToFile(pattern)\n\n      with output_to_file.open_output_file(test_record_obj) as outfile:\n        outfile.write(proto.SerializeToString())\n\n    return save_to_disk_callback", "response": "Returns a callback to convert test record to proto and save to disk."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upload(self):\n    if not self._converter:\n      raise RuntimeError(\n          'Must set _converter on subclass or via set_converter before calling '\n          'upload.')\n\n    if not self.credentials:\n      raise RuntimeError('Must provide credentials to use upload callback.')\n\n    def upload_callback(test_record_obj):\n      proto = self._convert(test_record_obj)\n      self.upload_result = send_mfg_inspector_data(\n          proto, self.credentials, self.destination_url)\n\n    return upload_callback", "response": "Returns a callback to convert a test record to a proto and upload."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting as long as the stream is not closed.", "response": "def _writer_thread_proc(self, is_raw):\n    \"\"\"Write as long as the stream is not closed.\"\"\"\n    # If we're not in raw mode, do line-buffered reads to play nicer with\n    # potential interactive uses, max of MAX_ADB_DATA, since anything we write\n    # to the stream will get packetized to that size anyway.\n    #\n    # Loop until our stream gets closed, which will cause one of these\n    # operations to raise.  Since we're in a separate thread, it'll just get\n    # ignored, which is what we want.\n    reader = self.stdin.read if is_raw else self.stdin.readline\n    while not self.stream.is_closed():\n      self.stream.write(reader(adb_protocol.MAX_ADB_DATA))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _reader_thread_proc(self, timeout):\n    for data in self.stream.read_until_close(timeout_ms=timeout):\n      if self.stdout is not None:\n        self.stdout.write(data)", "response": "Read until the stream is closed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nblocks until the command has completed.", "response": "def wait(self, timeout_ms=None):\n    \"\"\"Block until this command has completed.\n\n    Args:\n      timeout_ms: Timeout, in milliseconds, to wait.\n\n    Returns:\n      Output of the command if it complete and self.stdout is a StringIO\n    object or was passed in as None.  Returns True if the command completed but\n    stdout was provided (and was not a StringIO object).  Returns None if the\n    timeout expired before the command completed.  Be careful to check the\n    return value explicitly for None, as the output may be ''.\n    \"\"\"\n    closed = timeouts.loop_until_timeout_or_true(\n        timeouts.PolledTimeout.from_millis(timeout_ms),\n        self.stream.is_closed, .1)\n    if closed:\n      if hasattr(self.stdout, 'getvalue'):\n        return self.stdout.getvalue()\n      return True\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef command(self, command, raw=False, timeout_ms=None):\n    return ''.join(self.streaming_command(command, raw, timeout_ms))", "response": "Runs the given command and returns the output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef streaming_command(self, command, raw=False, timeout_ms=None):\n    if raw:\n      command = self._to_raw_command(command)\n    return self.adb_connection.streaming_command('shell', command, timeout_ms)", "response": "Run the given command and yield the output as we receive it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef async_command(self, command, stdin=None, stdout=None, raw=False,\n                    timeout_ms=None):\n    \"\"\"Run the given command on the device asynchronously.\n\n    Input will be read from stdin, output written to stdout.  ADB doesn't\n    distinguish between stdout and stdin on the device, so they get interleaved\n    into stdout here.  stdin and stdout should be file-like objects, so you\n    could use sys.stdin and sys.stdout to emulate the 'adb shell' commandline.\n\n    Args:\n      command: The command to run, will be run with /bin/sh -c 'command' on\n        the device.\n      stdin: File-like object to read from to pipe to the command's stdin.  Can\n        be None, in which case nothing will be written to the command's stdin.\n      stdout: File-like object to write the command's output to.  Can be None,\n        in which case the command's output will be buffered internally, and can\n        be access via the return value of wait().\n      raw: If True, run the command as per RawCommand (see above).\n      timeout_ms: Timeout for the command, in milliseconds.\n\n    Returns:\n      An AsyncCommandHandle instance that can be used to send/receive data to\n    and from the command or wait on the command to finish.\n\n    Raises:\n      AdbStreamUnavailableError: If the remote devices doesn't support the\n        shell: service.\n    \"\"\"\n    timeout = timeouts.PolledTimeout.from_millis(timeout_ms)\n    if raw:\n      command = self._to_raw_command(command)\n    stream = self.adb_connection.open_stream('shell:%s' % command, timeout)\n    if not stream:\n      raise usb_exceptions.AdbStreamUnavailableError(\n          '%s does not support service: shell', self)\n    if raw and stdin is not None:\n      # Short delay to make sure the ioctl to set raw mode happens before we do\n      # any writes to the stream, if we don't do this bad things happen...\n      time.sleep(.1)\n    return AsyncCommandHandle(stream, stdin, stdout, timeout, raw)", "response": "Run a command on the device asynchronously."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_flag_values(self, flags=None):\n    if flags is None:\n      flags = self._flags\n    for keyval in flags.config_value:\n      k, v = keyval.split('=', 1)\n      v = self._modules['yaml'].load(v) if isinstance(v, str) else v\n\n      # Force any command line keys and values that are bytes to unicode.\n      k = k.decode() if isinstance(k, bytes) else k\n      v = v.decode() if isinstance(v, bytes) else v\n\n      self._flag_values.setdefault(k, v)", "response": "Load flag values given from command line flags."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeclaring a configuration key with the given name.", "response": "def declare(self, name, description=None, **kwargs):\n    \"\"\"Declare a configuration key with the given name.\n\n    Args:\n      name: Configuration key to declare, must not have been already declared.\n      description: If provided, use this as the description for this key.\n      **kwargs: Other kwargs to pass to the Declaration, only default_value\n          is currently supported.\n    \"\"\"\n    if not self._is_valid_key(name):\n      raise self.InvalidKeyError(\n          'Invalid key name, must begin with a lowercase letter', name)\n    if name in self._declarations:\n      raise self.KeyAlreadyDeclaredError(\n          'Configuration key already declared', name)\n    self._declarations[name] = self.Declaration(\n        name, description=description, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset(self):\n    # Populate loaded_values with values from --config-file, if it was given.\n    self._loaded_values = {}\n    if self._flags.config_file is not None:\n      self.load_from_file(self._flags.config_file, _allow_undeclared=True)", "response": "Reset the loaded state of the configuration to what it was at import."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_from_file(self, yamlfile, _override=True, _allow_undeclared=False):\n    self._logger.info('Loading configuration from file: %s', yamlfile)\n\n    try:\n      parsed_yaml = self._modules['yaml'].safe_load(yamlfile.read())\n    except self._modules['yaml'].YAMLError:\n      self._logger.exception('Problem parsing YAML')\n      raise self.ConfigurationInvalidError(\n          'Failed to load from %s as YAML' % yamlfile)\n\n    if not isinstance(parsed_yaml, dict):\n      # Parsed YAML, but it's not a dict.\n      raise self.ConfigurationInvalidError(\n          'YAML parsed, but wrong type, should be dict', parsed_yaml)\n\n    self._logger.debug('Configuration loaded from file: %s', parsed_yaml)\n    self.load_from_dict(\n        parsed_yaml, _override=_override, _allow_undeclared=_allow_undeclared)", "response": "Loads the configuration from a YAML file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(self, _override=True, _allow_undeclared=False, **kwargs):\n    self.load_from_dict(\n        kwargs, _override=_override, _allow_undeclared=_allow_undeclared)", "response": "load configuration values from kwargs see load_from_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_from_dict(self, dictionary, _override=True, _allow_undeclared=False):\n    undeclared_keys = []\n    for key, value in self._modules['six'].iteritems(dictionary):\n      # Warn in this case.  We raise if you try to access a config key that\n      # hasn't been declared, but we don't raise here so that you can use\n      # configuration files that are supersets of required configuration for\n      # any particular test station.\n      if key not in self._declarations and not _allow_undeclared:\n        undeclared_keys.append(key)\n        continue\n      if key in self._loaded_values:\n        if _override:\n          self._logger.info(\n              'Overriding previously loaded value for %s (%s) with value: %s',\n              key, self._loaded_values[key], value)\n        else:\n          self._logger.info(\n              'Ignoring new value (%s), keeping previous value for %s: %s',\n              value, key, self._loaded_values[key])\n          continue\n\n      # Force any keys and values that are bytes to unicode.\n      key = key.decode() if isinstance(key, bytes) else key\n      value = value.decode() if isinstance(value, bytes) else value\n\n      self._loaded_values[key] = value\n    if undeclared_keys:\n      self._logger.warning('Ignoring undeclared configuration keys: %s',\n                           undeclared_keys)", "response": "Loads the config with values from a dictionary instead of a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _asdict(self):\n    # Start with any default values we have, and override with loaded values,\n    # and then override with flag values.\n    retval = {key: self._declarations[key].default_value for\n              key in self._declarations if self._declarations[key].has_default}\n    retval.update(self._loaded_values)\n    # Only update keys that are declared so we don't allow injecting\n    # un-declared keys via commandline flags.\n    for key, value in self._modules['six'].iteritems(self._flag_values):\n      if key in self._declarations:\n        retval[key] = value\n    return retval", "response": "Create a dictionary snapshot of the current config values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef help_text(self):\n    result = []\n    for name in sorted(self._declarations.keys()):\n      result.append(name)\n      result.append('-' * len(name))\n      decl = self._declarations[name]\n      if decl.description:\n        result.append(decl.description.strip())\n      else:\n        result.append('(no description found)')\n      if decl.has_default:\n        result.append('')\n        quotes = '\"' if type(decl.default_value) is str else ''\n        result.append('  default_value={quotes}{val}{quotes}'.format(\n            quotes=quotes, val=decl.default_value))\n      result.append('')\n      result.append('')\n    return '\\n'.join(result)", "response": "Return a string with all config keys and their descriptions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_and_restore(self, _func=None, **config_values):\n    functools = self._modules['functools']  # pylint: disable=redefined-outer-name\n\n    if not _func:\n      return functools.partial(self.save_and_restore, **config_values)\n\n    @functools.wraps(_func)\n    def _saving_wrapper(*args, **kwargs):\n      saved_config = dict(self._loaded_values)\n      try:\n        self.load_from_dict(config_values)\n        return _func(*args, **kwargs)\n      finally:\n        self._loaded_values = saved_config # pylint: disable=attribute-defined-outside-init\n    return _saving_wrapper", "response": "Decorator for saving and restoring conf state after a function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call_once(func):\n  argspec = inspect.getargspec(func)\n  if argspec.args or argspec.varargs or argspec.keywords:\n    raise ValueError('Can only decorate functions with no args', func, argspec)\n\n  @functools.wraps(func)\n  def _wrapper():\n    # If we haven't been called yet, actually invoke func and save the result.\n    if not _wrapper.HasRun():\n      _wrapper.MarkAsRun()\n      _wrapper.return_value = func()\n    return _wrapper.return_value\n\n  _wrapper.has_run = False\n  _wrapper.HasRun = lambda: _wrapper.has_run\n  _wrapper.MarkAsRun = lambda: setattr(_wrapper, 'has_run', True)\n  return _wrapper", "response": "Decorator to only allow a function to be called once."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling the decorated function at most count times every seconds seconds. The decorated function will sleep to ensure that at most count invocations occur within any 'seconds' second window.", "response": "def call_at_most_every(seconds, count=1):\n  \"\"\"Call the decorated function at most count times every seconds seconds.\n\n  The decorated function will sleep to ensure that at most count invocations\n  occur within any 'seconds' second window.\n  \"\"\"\n  def decorator(func):\n    try:\n      call_history = getattr(func, '_call_history')\n    except AttributeError:\n      call_history = collections.deque(maxlen=count)\n      setattr(func, '_call_history', call_history)\n\n    @functools.wraps(func)\n    def _wrapper(*args, **kwargs):\n      current_time = time.time()\n      window_count = sum(ts > current_time - seconds for ts in call_history)\n      if window_count >= count:\n        # We need to sleep until the relevant call is outside the window.  This\n        # should only ever be the the first entry in call_history, but if we\n        # somehow ended up with extra calls in the window, this recovers.\n        time.sleep(call_history[window_count - count] - current_time + seconds)\n      # Append this call, deque will automatically trim old calls using maxlen.\n      call_history.append(time.time())\n      return func(*args, **kwargs)\n    return _wrapper\n  return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening a USB handle based on configuration.", "response": "def _open_usb_handle(serial_number=None, **kwargs):\n  \"\"\"Open a UsbHandle subclass, based on configuration.\n\n  If configuration 'remote_usb' is set, use it to connect to remote usb,\n  otherwise attempt to connect locally.'remote_usb' is set to usb type,\n  EtherSync or other.\n\n  Example of Cambrionix unit in config:\n  remote_usb: ethersync\n  ethersync:\n       mac_addr: 78:a5:04:ca:91:66\n       plug_port: 5\n\n  Args:\n    serial_number: Optional serial number to connect to.\n    **kwargs: Arguments to pass to respective handle's Open() method.\n\n  Returns:\n    Instance of UsbHandle.\n  \"\"\"\n  init_dependent_flags()\n  remote_usb = conf.remote_usb\n  if remote_usb:\n    if remote_usb.strip() == 'ethersync':\n      device = conf.ethersync\n      try:\n        mac_addr = device['mac_addr']\n        port = device['plug_port']\n      except (KeyError, TypeError):\n        raise ValueError('Ethersync needs mac_addr and plug_port to be set')\n      else:\n        ethersync = cambrionix.EtherSync(mac_addr)\n        serial_number = ethersync.get_usb_serial(port)\n\n  return local_usb.LibUsbHandle.open(serial_number=serial_number, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to open a USB handle.", "response": "def _try_open(cls):\n    \"\"\"Try to open a USB handle.\"\"\"\n    handle = None\n    for usb_cls, subcls, protocol in [(adb_device.CLASS,\n                                       adb_device.SUBCLASS,\n                                       adb_device.PROTOCOL),\n                                      (fastboot_device.CLASS,\n                                       fastboot_device.SUBCLASS,\n                                       fastboot_device.PROTOCOL)]:\n      try:\n        handle = local_usb.LibUsbHandle.open(\n            serial_number=cls.serial_number,\n            interface_class=usb_cls,\n            interface_subclass=subcls,\n            interface_protocol=protocol)\n        cls.serial_number = handle.serial_number\n        return True\n      except usb_exceptions.DeviceNotFoundError:\n        pass\n      except usb_exceptions.MultipleInterfacesFoundError:\n        _LOG.warning('Multiple Android devices found, ignoring!')\n      finally:\n        if handle:\n          handle.close()\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _retry_usb_function(count, func, *args, **kwargs):\n  helper = timeouts.RetryHelper(count)\n  while True:\n    try:\n      return func(*args, **kwargs)\n    except usb_exceptions.CommonUsbError:\n      if not helper.retry_if_possible():\n        raise\n      time.sleep(0.1)\n    else:\n      break", "response": "Helper function to retry USB function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting bootconfig either as full dict or specific value for key.", "response": "def get_boot_config(self, name, info_cb=None):\n    \"\"\"Get bootconfig, either as full dict or specific value for key.\"\"\"\n    result = {}\n    def default_info_cb(msg):\n      \"\"\"Default Info CB.\"\"\"\n      if not msg.message:\n        return\n      key, value = msg.message.split(':', 1)\n      result[key.strip()] = value.strip()\n    info_cb = info_cb or default_info_cb\n    final_result = self.oem('bootconfig %s' % name, info_cb=info_cb)\n    # Return INFO messages before the final OKAY message.\n    if name in result:\n      return result[name]\n    return final_result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _device_to_sysfs_path(device):\n    return '%s-%s' % (\n        device.getBusNumber(),\n        '.'.join([str(item) for item in device.GetPortNumberList()]))", "response": "Convert a device to corresponding sysfs path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen a new device and return it.", "response": "def open(cls, **kwargs):\n    \"\"\"See iter_open, but raises if multiple or no matches found.\"\"\"\n    handle_iter = cls.iter_open(**kwargs)\n\n    try:\n      handle = six.next(handle_iter)\n    except StopIteration:\n      # No matching interface, raise.\n      raise usb_exceptions.DeviceNotFoundError(\n          'Open failed with args: %s', kwargs)\n\n    try:\n      multiple_handle = six.next(handle_iter)\n    except StopIteration:\n      # Exactly one matching device, return it.\n      return handle\n\n    # We have more than one device, close the ones we opened and bail.\n    handle.close()\n    multiple_handle.close()\n    raise usb_exceptions.MultipleInterfacesFoundError(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding USB handles that match the given arguments.", "response": "def iter_open(cls, name=None, interface_class=None, interface_subclass=None,\n                interface_protocol=None, serial_number=None, port_path=None,\n                default_timeout_ms=None):\n    \"\"\"Find and yield locally connected devices that match.\n\n    Note that devices are opened (and interfaces claimd) as they are yielded.\n    Any devices yielded must be Close()'d.\n\n    Args:\n      name: Name to give *all* returned handles, used for logging only.\n      interface_class: USB interface_class to match.\n      interface_subclass: USB interface_subclass to match.\n      interface_protocol: USB interface_protocol to match.\n      serial_number: USB serial_number to match.\n      port_path: USB Port path to match, like X-X.X.X\n      default_timeout_ms: Default timeout in milliseconds of reads/writes on\n        the handles yielded.\n\n    Yields:\n      UsbHandle instances that match any non-None args given.\n\n    Raises:\n      LibusbWrappingError: When a libusb call errors during open.\n    \"\"\"\n    ctx = usb1.USBContext()\n    try:\n      devices = ctx.getDeviceList(skip_on_error=True)\n    except libusb1.USBError as exception:\n      raise usb_exceptions.LibusbWrappingError(\n          exception, 'Open(name=%s, class=%s, subclass=%s, protocol=%s, '\n          'serial=%s, port=%s) failed', name, interface_class,\n          interface_subclass, interface_protocol, serial_number, port_path)\n\n    for device in devices:\n      try:\n        if (serial_number is not None and\n            device.getSerialNumber() != serial_number):\n          continue\n\n        if (port_path is not None and\n            cls._device_to_sysfs_path(device) != port_path):\n          continue\n\n        for setting in device.iterSettings():\n          if (interface_class is not None and\n              setting.getClass() != interface_class):\n            continue\n          if (interface_subclass is not None and\n              setting.getSubClass() != interface_subclass):\n            continue\n          if (interface_protocol is not None and\n              setting.getProtocol() != interface_protocol):\n            continue\n\n          yield cls(device, setting, name=name,\n                    default_timeout_ms=default_timeout_ms)\n      except libusb1.USBError as exception:\n        if (exception.value !=\n            libusb1.libusb_error.forward_dict['LIBUSB_ERROR_ACCESS']):\n          raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hello_world(test, example, prompts):\n  test.logger.info('Hello World!')\n  test.measurements.widget_type = prompts.prompt(\n      'What\\'s the widget type? (Hint: try `MyWidget` to PASS)',\n      text_input=True)\n  if test.measurements.widget_type == 'raise':\n    raise Exception()\n  test.measurements.widget_color = 'Black'\n  test.measurements.widget_size = 3\n  test.measurements.specified_as_args = 'Measurement args specified directly'\n  test.logger.info('Plug value: %s', example.increment())", "response": "A hello world test phase."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_measurements(test):\n  test.measurements.level_none = 0\n  time.sleep(1)\n  test.measurements.level_some = 8\n  time.sleep(1)\n  test.measurements.level_all = 9\n  time.sleep(1)\n  level_all = test.get_measurement('level_all')\n  assert level_all.value == 9", "response": "Test phase that sets a measurement."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompares the pprint representation of two objects and yield diff lines.", "response": "def pprint_diff(first, second, first_name='first', second_name='second'):\n  \"\"\"Compare the pprint representation of two objects and yield diff lines.\"\"\"\n  return difflib.unified_diff(\n      pprint.pformat(first).splitlines(),\n      pprint.pformat(second).splitlines(),\n      fromfile=first_name, tofile=second_name, lineterm='')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompare two string blobs error log diff", "response": "def equals_log_diff(expected, actual, level=logging.ERROR):\n  \"\"\"Compare two string blobs, error log diff if they don't match.\"\"\"\n  if expected == actual:\n    return True\n\n  # Output the diff first.\n  logging.log(level, '***** Data mismatch: *****')\n  for line in difflib.unified_diff(\n      expected.splitlines(), actual.splitlines(),\n      fromfile='expected', tofile='actual', lineterm=''):\n    logging.log(level, line)\n  logging.log(level, '^^^^^  Data diff  ^^^^^')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare two test_record tuples ignoring any volatile fields.", "response": "def assert_records_equal_nonvolatile(first, second, volatile_fields, indent=0):\n  \"\"\"Compare two test_record tuples, ignoring any volatile fields.\n\n  'Volatile' fields include any fields that are expected to differ between\n  successive runs of the same test, mainly timestamps.  All other fields\n  are recursively compared.\n  \"\"\"\n  if isinstance(first, dict) and isinstance(second, dict):\n    if set(first) != set(second):\n      logging.error('%sMismatching keys:', ' ' * indent)\n      logging.error('%s  %s', ' ' * indent, list(first.keys()))\n      logging.error('%s  %s', ' ' * indent, list(second.keys()))\n      assert set(first) == set(second)\n    for key in first:\n      if key in volatile_fields:\n        continue\n      try:\n        assert_records_equal_nonvolatile(first[key], second[key],\n                                         volatile_fields, indent + 2)\n      except AssertionError:\n        logging.error('%sKey: %s ^', ' ' * indent, key)\n        raise\n  elif hasattr(first, '_asdict') and hasattr(second, '_asdict'):\n    # Compare namedtuples as dicts so we get more useful output.\n    assert_records_equal_nonvolatile(first._asdict(), second._asdict(),\n                                     volatile_fields, indent)\n  elif hasattr(first, '__iter__') and hasattr(second, '__iter__'):\n    for idx, (fir, sec) in enumerate(itertools.izip(first, second)):\n      try:\n        assert_records_equal_nonvolatile(fir, sec, volatile_fields, indent + 2)\n      except AssertionError:\n        logging.error('%sIndex: %s ^', ' ' * indent, idx)\n        raise\n  elif (isinstance(first, records.RecordClass) and\n        isinstance(second, records.RecordClass)):\n    assert_records_equal_nonvolatile(\n        {slot: getattr(first, slot) for slot in first.__slots__},\n        {slot: getattr(second, slot) for slot in second.__slots__},\n        volatile_fields, indent)\n  elif first != second:\n    logging.error('%sRaw: \"%s\" != \"%s\"', ' ' * indent, first, second)\n    assert first == second"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_to_base_types(obj, ignore_keys=tuple(), tuple_type=tuple,\n                          json_safe=True):\n  \"\"\"Recursively convert objects into base types.\n\n  This is used to convert some special types of objects used internally into\n  base types for more friendly output via mechanisms such as JSON.  It is used\n  for sending internal objects via the network and outputting test records.\n  Specifically, the conversions that are performed:\n\n    - If an object has an as_base_types() method, immediately return the result\n      without any recursion; this can be used with caching in the object to\n      prevent unnecessary conversions.\n    - If an object has an _asdict() method, use that to convert it to a dict and\n      recursively converting its contents.\n    - mutablerecords Record instances are converted to dicts that map\n      attribute name to value.  Optional attributes with a value of None are\n      skipped.\n    - Enum instances are converted to strings via their .name attribute.\n    - Real and integral numbers are converted to built-in types.\n    - Byte and unicode strings are left alone (instances of six.string_types).\n    - Other non-None values are converted to strings via str().\n\n  The return value contains only the Python built-in types: dict, list, tuple,\n  str, unicode, int, float, long, bool, and NoneType (unless tuple_type is set\n  to something else).  If tuples should be converted to lists (e.g. for an\n  encoding that does not differentiate between the two), pass 'tuple_type=list'\n  as an argument.\n\n  If `json_safe` is True, then the float 'inf', '-inf', and 'nan' values will be\n  converted to strings. This ensures that the returned dictionary can be passed\n  to json.dumps to create valid JSON. Otherwise, json.dumps may return values\n  such as NaN which are not valid JSON.\n  \"\"\"\n  # Because it's *really* annoying to pass a single string accidentally.\n  assert not isinstance(ignore_keys, six.string_types), 'Pass a real iterable!'\n\n  if hasattr(obj, 'as_base_types'):\n    return obj.as_base_types()\n  if hasattr(obj, '_asdict'):\n    obj = obj._asdict()\n  elif isinstance(obj, records.RecordClass):\n    obj = {attr: getattr(obj, attr)\n           for attr in type(obj).all_attribute_names\n           if (getattr(obj, attr, None) is not None or\n               attr in type(obj).required_attributes)}\n  elif isinstance(obj, Enum):\n    obj = obj.name\n\n  if type(obj) in PASSTHROUGH_TYPES:\n    return obj\n\n  # Recursively convert values in dicts, lists, and tuples.\n  if isinstance(obj, dict):\n    return {convert_to_base_types(k, ignore_keys, tuple_type):\n               convert_to_base_types(v, ignore_keys, tuple_type)\n            for k, v in six.iteritems(obj) if k not in ignore_keys}\n  elif isinstance(obj, list):\n    return [convert_to_base_types(val, ignore_keys, tuple_type, json_safe)\n            for val in obj]\n  elif isinstance(obj, tuple):\n    return tuple_type(\n        convert_to_base_types(value, ignore_keys, tuple_type, json_safe)\n        for value in obj)\n\n  # Convert numeric types (e.g. numpy ints and floats) into built-in types.\n  elif isinstance(obj, numbers.Integral):\n    return long(obj)\n  elif isinstance(obj, numbers.Real):\n    as_float = float(obj)\n    if json_safe and (math.isinf(as_float) or math.isnan(as_float)):\n      return str(as_float)\n    return as_float\n\n  # Convert all other types to strings.\n  try:\n    return str(obj)\n  except:\n    logging.warning('Problem casting object of type %s to str.', type(obj))\n    raise", "response": "Recursively convert a Python object into base types."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the approximate total memory footprint an object.", "response": "def total_size(obj):\n  \"\"\"Returns the approximate total memory footprint an object.\"\"\"\n  seen = set()\n  def sizeof(current_obj):\n    try:\n      return _sizeof(current_obj)\n    except Exception:  # pylint: disable=broad-except\n      # Not sure what just happened, but let's assume it's a reference.\n      return struct.calcsize('P')\n\n  def _sizeof(current_obj):\n    \"\"\"Do a depth-first acyclic traversal of all reachable objects.\"\"\"\n    if id(current_obj) in seen:\n      # A rough approximation of the size cost of an additional reference.\n      return struct.calcsize('P')\n    seen.add(id(current_obj))\n    size = sys.getsizeof(current_obj)\n\n    if isinstance(current_obj, dict):\n      size += sum(map(sizeof, itertools.chain.from_iterable(\n          six.iteritems(current_obj))))\n    elif (isinstance(current_obj, collections.Iterable) and\n          not isinstance(current_obj, six.string_types)):\n      size += sum(sizeof(item) for item in current_obj)\n    elif isinstance(current_obj, records.RecordClass):\n      size += sum(sizeof(getattr(current_obj, attr))\n                  for attr in current_obj.__slots__)\n    return size\n\n  return sizeof(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening file based on pattern.", "response": "def open_output_file(self, test_record):\n    \"\"\"Open file based on pattern.\"\"\"\n    # Ignore keys for the log filename to not convert larger data structures.\n    record_dict = data.convert_to_base_types(\n        test_record, ignore_keys=('code_info', 'phases', 'log_records'))\n    pattern = self.filename_pattern\n    if isinstance(pattern, six.string_types) or callable(pattern):\n      output_file = self.open_file(util.format_string(pattern, record_dict))\n      try:\n        yield output_file\n      finally:\n        output_file.close()\n    elif hasattr(self.filename_pattern, 'write'):\n      yield self.filename_pattern\n    else:\n      raise ValueError(\n          'filename_pattern must be string, callable, or File-like object')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mutable_record_transform(cls):\n  if not (len(cls.bases) > 0\n          and isinstance(cls.bases[0], astroid.Call)\n          and cls.bases[0].func.as_string() == 'mutablerecords.Record'):\n    return\n\n  try:\n    # Add required attributes.\n    if len(cls.bases[0].args) >= 2:\n      for a in cls.bases[0].args[1].elts:\n        cls.locals[a] = [None]\n\n    # Add optional attributes.\n    if len(cls.bases[0].args) >= 3:\n      for a,b in cls.bases[0].args[2].items:\n        cls.locals[a.value] = [None]\n\n  except:\n    raise SyntaxError('Invalid mutablerecords syntax')", "response": "Transform mutable records usage by updating locals."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _log_every_n_to_logger(n, logger, level, message, *args):  # pylint: disable=invalid-name\n  logger = logger or logging.getLogger()\n  def _gen():  # pylint: disable=missing-docstring\n    while True:\n      for _ in range(n):\n        yield False\n      logger.log(level, message, *args)\n      yield True\n  gen = _gen()\n  return lambda: six.next(gen)", "response": "A function that logs the given message every n calls to a logger."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog a message every n calls.", "response": "def log_every_n(n, level, message, *args):  # pylint: disable=invalid-name\n  \"\"\"Logs a message every n calls. See _log_every_n_to_logger.\"\"\"\n  return _log_every_n_to_logger(n, None, level, message, *args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat a string without requiring all values to be present.", "response": "def partial_format(target, **kwargs):\n  \"\"\"Formats a string without requiring all values to be present.\n\n  This function allows substitutions to be gradually made in several steps\n  rather than all at once.  Similar to string.Template.safe_substitute.\n  \"\"\"\n  output = target[:]\n\n  for tag, var in re.findall(r'(\\{(.*?)\\})', output):\n    root = var.split('.')[0]  # dot notation\n    root = root.split('[')[0]  # dict notation\n    if root in kwargs:\n      output = output.replace(tag, tag.format(**{root: kwargs[root]}))\n\n  return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_string(target, kwargs):\n  if not target:\n    return target\n  if callable(target):\n    return target(**kwargs)\n  if not isinstance(target, six.string_types):\n    return target\n  if '{' in target:\n    return partial_format(target, **kwargs)\n  if '%' in target:\n    return target % kwargs\n  return target", "response": "Formats a string in any of three ways."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef asdict_with_event(self):\n    event = threading.Event()\n    with self._lock:\n      self._update_events.add(event)\n    return self._asdict(), event", "response": "Get a dict representation of this object and an update event."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnotifies any update events that there was an update.", "response": "def notify_update(self):\n    \"\"\"Notify any update events that there was an update.\"\"\"\n    with self._lock:\n      for event in self._update_events:\n        event.set()\n      self._update_events.clear()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbind sockets to an available port returning sockets and the bound port.", "response": "def bind_port(requested_port):\n  \"\"\"Bind sockets to an available port, returning sockets and the bound port.\"\"\"\n  sockets = tornado.netutil.bind_sockets(requested_port)\n\n  if requested_port != 0:\n    return sockets, requested_port\n\n  # Get the actual port number.\n  for s in sockets:\n    host, port = s.getsockname()[:2]\n    if host == '0.0.0.0':\n      return sockets, port\n\n  raise RuntimeError('Could not determine the bound port.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loop_until_timeout_or_valid(timeout_s, function, validation_fn, sleep_s=1):  # pylint: disable=invalid-name\n  if timeout_s is None or not hasattr(timeout_s, 'has_expired'):\n    timeout_s = PolledTimeout(timeout_s)\n\n  while True:\n    # Calls the function at least once\n    result = function()\n    if validation_fn(result) or timeout_s.has_expired():\n      return result\n    time.sleep(sleep_s)", "response": "Loops until the specified function returns valid or a timeout is reached."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef loop_until_timeout_or_true(timeout_s, function, sleep_s=1):  # pylint: disable=invalid-name\n  return loop_until_timeout_or_valid(timeout_s, function, lambda x: x, sleep_s)", "response": "Loops until the specified function returns True or a timeout is reached."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef loop_until_timeout_or_not_none(timeout_s, function, sleep_s=1):  # pylint: disable=invalid-name\n  return loop_until_timeout_or_valid(\n      timeout_s, function, lambda x: x is not None, sleep_s)", "response": "Loops until the specified function returns non - None or until a timeout is reached."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute a method forever at the specified interval.", "response": "def execute_forever(method, interval_s):  # pylint: disable=invalid-name\n  \"\"\"Executes a method forever at the specified interval.\n\n  Args:\n    method: The callable to execute.\n    interval_s: The number of seconds to start the execution after each method\n        finishes.\n  Returns:\n    An Interval object.\n  \"\"\"\n  interval = Interval(method)\n  interval.start(interval_s)\n  return interval"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting a method forever until the method returns a false value.", "response": "def execute_until_false(method, interval_s):  # pylint: disable=invalid-name\n  \"\"\"Executes a method forever until the method returns a false value.\n\n  Args:\n    method: The callable to execute.\n    interval_s: The number of seconds to start the execution after each method\n        finishes.\n  Returns:\n    An Interval object.\n  \"\"\"\n  interval = Interval(method, stop_if_false=True)\n  interval.start(interval_s)\n  return interval"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retry_until_true_or_limit_reached(method, limit, sleep_s=1,\n                                      catch_exceptions=()):\n  \"\"\"Executes a method until the retry limit is hit or True is returned.\"\"\"\n  return retry_until_valid_or_limit_reached(\n      method, limit, lambda x: x, sleep_s, catch_exceptions)", "response": "Executes a method until the retry limit is hit or True is returned."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef retry_until_not_none_or_limit_reached(method, limit, sleep_s=1,\n                                          catch_exceptions=()):\n  \"\"\"Executes a method until the retry limit is hit or not None is returned.\"\"\"\n  return retry_until_valid_or_limit_reached(\n      method, limit, lambda x: x is not None, sleep_s, catch_exceptions)", "response": "Executes a method until the retry limit is hit or None is returned."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retry_until_valid_or_limit_reached(method, limit, validation_fn, sleep_s=1,\n                                       catch_exceptions=()):\n  \"\"\"Executes a method until the retry limit or validation_fn returns True.\n\n  The method is always called once so the effective lower limit for 'limit' is\n  1.  Passing in a number less than 1 will still result it the method being\n  called once.\n\n  Args:\n    method: The method to execute should take no arguments.\n    limit: The number of times to try this method.  Must be >0.\n    validation_fn: The validation function called on the function result to\n        determine whether to keep looping.\n    sleep_s: The time to sleep in between invocations.\n    catch_exceptions: Tuple of exception types to catch and count as failures.\n  Returns:\n      Whatever the method last returned, implicit False would indicate the\n        method never succeeded.\n  \"\"\"\n  assert limit > 0, 'Limit must be greater than 0'\n\n  def _execute_method(helper):\n    try:\n      return method()\n    except catch_exceptions:\n      if not helper.remaining:\n        raise\n      return None\n\n  helper = RetryHelper(limit - 1)\n  result = _execute_method(helper)\n  while not validation_fn(result) and helper.retry_if_possible():\n    time.sleep(sleep_s)\n    result = _execute_method(helper)\n  return result", "response": "Executes a method until the retry limit or validation_fn returns True."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute_after_delay(time_s, func, *args, **kwargs):\n  timeout = PolledTimeout.from_seconds(time_s)\n\n  def target():\n    time.sleep(timeout.remaining)\n    try:\n      func(*args, **kwargs)\n    except Exception:  # pylint: disable=broad-except\n      _LOG.exception('Error executing %s after %s expires.', func, timeout)\n  if timeout.remaining is not None:\n    thread = threading.Thread(target=target)\n    thread.start()", "response": "A function that executes the given function after a delay."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_millis(cls, timeout_ms):\n    if hasattr(timeout_ms, 'has_expired'):\n      return timeout_ms\n    if timeout_ms is None:\n      return cls(None)\n    return cls(timeout_ms / 1000.0)", "response": "Create a new PolledTimeout object from a given number of milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start(self, interval_s):\n    if self.running:\n      return False\n\n    self.stopped.clear()\n\n    def _execute():\n      # Always execute immediately once\n      if not self.method() and self.stop_if_false:\n        return\n      while not self.stopped.wait(interval_s):\n        if not self.method() and self.stop_if_false:\n          return\n\n    self.thread = threading.Thread(target=_execute)\n    self.thread.daemon = True\n    self.thread.start()\n    return True", "response": "Starts executing the method at the specified interval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop the interval. If a timeout is provided and stop returns False then the thread is effectively abandoned in whatever state it was in (presumably dead-locked). Args: timeout_s: The time in seconds to wait on the thread to finish. By default it's forever. Returns: False if a timeout was provided and we timed out.", "response": "def stop(self, timeout_s=None):\n    \"\"\"Stops the interval.\n\n    If a timeout is provided and stop returns False then the thread is\n    effectively abandoned in whatever state it was in (presumably dead-locked).\n\n    Args:\n      timeout_s: The time in seconds to wait on the thread to finish.  By\n          default it's forever.\n    Returns:\n      False if a timeout was provided and we timed out.\n    \"\"\"\n    self.stopped.set()\n    if self.thread:\n      self.thread.join(timeout_s)\n      return not self.thread.isAlive()\n    else:\n      return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef join(self, timeout_s=None):\n    if not self.thread:\n      return False\n    self.thread.join(timeout_s)\n    return self.running", "response": "Joins blocking until the interval ends or until timeout is reached."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transform_declare(node):\n  global CURRENT_ROOT\n\n  if not (isinstance(node.func, astroid.Attribute)\n          and isinstance(node.func.expr, astroid.Name)\n          and node.func.expr.name == 'conf'\n          and node.func.attrname == 'declare'):\n    return\n\n  conf_key_name = None\n  if node.args:\n    conf_key_name = node.args[0].value\n  else:\n    for keyword in node.keywords:\n      if keyword.arg == 'name':\n        # Assume the name is an astroid.Const(str), so it has a str value.\n        conf_key_name = keyword.value.value\n        break\n    assert conf_key_name != None, \"Invalid conf.declare() syntax\"\n\n  if CONF_NODE:\n    # Keep track of the current root, refreshing the locals if it changes.\n    if not CURRENT_ROOT or CURRENT_ROOT != node.root():\n      CURRENT_ROOT = node.root()\n      CONF_NODE.locals = CONF_LOCALS\n      CONF_NODE.locals[conf_key_name] = [None]\n\n  else:\n    CONF_LOCALS[conf_key_name] = [None]", "response": "Transform conf. declare calls by stashing the declared names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntransforming usages of the conf module by updating locals.", "response": "def transform_conf_module(cls):\n  \"\"\"Transform usages of the conf module by updating locals.\"\"\"\n  global CONF_NODE\n\n  if cls.name == 'openhtf.conf':\n    # Put all the attributes in Configuration into the openhtf.conf node.\n    cls._locals.update(cls.locals['Configuration'][0].locals)\n\n    # Store reference to this node for future use.\n    CONF_NODE = cls\n    CONF_LOCALS.update(cls.locals)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering all transforms with the linter.", "response": "def register(linter):\n  \"\"\"Register all transforms with the linter.\"\"\"\n  MANAGER.register_transform(astroid.Call, transform_declare)\n  MANAGER.register_transform(astroid.Module, transform_conf_module)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n    if platform.system() == 'Windows':\n      # Windows doesn't support file-like objects for select(), so fall back\n      # to raw_input().\n      response = input(''.join((self._message,\n                                os.linesep,\n                                PROMPT)))\n      self._answered = True\n      self._callback(response)\n      return\n\n    # First, display the prompt to the console.\n    console_output.cli_print(self._message, color=self._color,\n                             end=os.linesep, logger=None)\n    console_output.cli_print(PROMPT, color=self._color, end='', logger=None)\n    sys.stdout.flush()\n\n    # Before reading, clear any lingering buffered terminal input.\n    termios.tcflush(sys.stdin, termios.TCIFLUSH)\n\n    line = ''\n    while not self._stop_event.is_set():\n      inputs, _, _ = select.select([sys.stdin], [], [], 0.001)\n      if sys.stdin in inputs:\n        new = os.read(sys.stdin.fileno(), 1024)\n        if not new:\n          # Hit EOF!\n          # They hit ^D (to insert EOF). Tell them to hit ^C if they\n          # want to actually quit.\n          print('Hit ^C (Ctrl+c) to exit.')\n          break\n        line += new.decode('utf-8')\n        if '\\n' in line:\n          response = line[:line.find('\\n')]\n          self._answered = True\n          self._callback(response)\n          return", "response": "Main logic for this thread to execute."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _asdict(self):\n    with self._cond:\n      if self._prompt is None:\n        return\n      return {'id': self._prompt.id,\n              'message': self._prompt.message,\n              'text-input': self._prompt.text_input}", "response": "Return a dictionary representation of the current prompt."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_prompt(self):\n    with self._cond:\n      self._prompt = None\n      if self._console_prompt:\n        self._console_prompt.Stop()\n        self._console_prompt = None\n      self.notify_update()", "response": "Removes the prompt from the list of active prompt objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prompt(self, message, text_input=False, timeout_s=None, cli_color=''):\n    self.start_prompt(message, text_input, cli_color)\n    return self.wait_for_prompt(timeout_s)", "response": "Display a prompt and wait for a response."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_prompt(self, message, text_input=False, cli_color=''):\n    with self._cond:\n      if self._prompt:\n        raise MultiplePromptsError\n      prompt_id = uuid.uuid4().hex\n      _LOG.debug('Displaying prompt (%s): \"%s\"%s', prompt_id, message,\n                 ', Expects text input.' if text_input else '')\n\n      self._response = None\n      self._prompt = Prompt(\n          id=prompt_id, message=message, text_input=text_input)\n      if sys.stdin.isatty():\n        self._console_prompt = ConsolePrompt(\n            message, functools.partial(self.respond, prompt_id), cli_color)\n        self._console_prompt.start()\n\n      self.notify_update()\n      return prompt_id", "response": "Display a prompt.\n\n    Args:\n      message: A string to be presented to the user.\n      text_input: A boolean indicating whether the user must respond with text.\n      cli_color: An ANSI color code, or the empty string.\n\n    Raises:\n      MultiplePromptsError: There was already an existing prompt.\n\n    Returns:\n      A string uniquely identifying the prompt."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wait_for_prompt(self, timeout_s=None):\n    with self._cond:\n      if self._prompt:\n        if timeout_s is None:\n          self._cond.wait(3600 * 24 * 365)\n        else:\n          self._cond.wait(timeout_s)\n      if self._response is None:\n        raise PromptUnansweredError\n      return self._response", "response": "Wait for the user to respond to the current prompt."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef respond(self, prompt_id, response):\n    _LOG.debug('Responding to prompt (%s): \"%s\"', prompt_id, response)\n    with self._cond:\n      if not (self._prompt and self._prompt.id == prompt_id):\n        return False\n      self._response = response\n      self.last_response = (prompt_id, response)\n      self.remove_prompt()\n      self._cond.notifyAll()\n    return True", "response": "Respond to the given prompt with the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_terminal(self):\n    return (self.raised_exception or self.is_timeout or\n            self.phase_result == openhtf.PhaseResult.STOP)", "response": "True if this result will stop the test."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute the encompassed phase and save the result.", "response": "def _thread_proc(self):\n    \"\"\"Execute the encompassed phase and save the result.\"\"\"\n    # Call the phase, save the return value, or default it to CONTINUE.\n    phase_return = self._phase_desc(self._test_state)\n    if phase_return is None:\n      phase_return = openhtf.PhaseResult.CONTINUE\n\n    # If phase_return is invalid, this will raise, and _phase_execution_outcome\n    # will get set to the InvalidPhaseResultError in _thread_exception instead.\n    self._phase_execution_outcome = PhaseExecutionOutcome(phase_return)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait for thread to finish returning a PhaseExecutionOutcome instance.", "response": "def join_or_die(self):\n    \"\"\"Wait for thread to finish, returning a PhaseExecutionOutcome instance.\"\"\"\n    if self._phase_desc.options.timeout_s is not None:\n      self.join(self._phase_desc.options.timeout_s)\n    else:\n      self.join(DEFAULT_PHASE_TIMEOUT_S)\n\n    # We got a return value or an exception and handled it.\n    if isinstance(self._phase_execution_outcome, PhaseExecutionOutcome):\n      return self._phase_execution_outcome\n\n    # Check for timeout, indicated by None for\n    # PhaseExecutionOutcome.phase_result.\n    if self.is_alive():\n      self.kill()\n      return PhaseExecutionOutcome(None)\n\n    # Phase was killed.\n    return PhaseExecutionOutcome(threads.ThreadTerminationError())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute_phase(self, phase):\n    repeat_count = 1\n    repeat_limit = phase.options.repeat_limit or sys.maxsize\n    while not self._stopping.is_set():\n      is_last_repeat = repeat_count >= repeat_limit\n      phase_execution_outcome = self._execute_phase_once(phase, is_last_repeat)\n\n      if phase_execution_outcome.is_repeat and not is_last_repeat:\n        repeat_count += 1\n        continue\n\n      return phase_execution_outcome\n    # We've been cancelled, so just 'timeout' the phase.\n    return PhaseExecutionOutcome(None)", "response": "Executes a phase or skips it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _execute_phase_once(self, phase_desc, is_last_repeat):\n    # Check this before we create a PhaseState and PhaseRecord.\n    if phase_desc.options.run_if and not phase_desc.options.run_if():\n      _LOG.debug('Phase %s skipped due to run_if returning falsey.',\n                 phase_desc.name)\n      return PhaseExecutionOutcome(openhtf.PhaseResult.SKIP)\n\n    override_result = None\n    with self.test_state.running_phase_context(phase_desc) as phase_state:\n      _LOG.debug('Executing phase %s', phase_desc.name)\n      with self._current_phase_thread_lock:\n        # Checking _stopping must be in the lock context, otherwise there is a\n        # race condition: this thread checks _stopping and then switches to\n        # another thread where stop() sets _stopping and checks\n        # _current_phase_thread (which would not be set yet).  In that case, the\n        # new phase thread will be still be started.\n        if self._stopping.is_set():\n          # PhaseRecord will be written at this point, so ensure that it has a\n          # Killed result.\n          result = PhaseExecutionOutcome(threads.ThreadTerminationError())\n          phase_state.result = result\n          return result\n        phase_thread = PhaseExecutorThread(phase_desc, self.test_state)\n        phase_thread.start()\n        self._current_phase_thread = phase_thread\n\n      phase_state.result = phase_thread.join_or_die()\n      if phase_state.result.is_repeat and is_last_repeat:\n        _LOG.error('Phase returned REPEAT, exceeding repeat_limit.')\n        phase_state.hit_repeat_limit = True\n        override_result = PhaseExecutionOutcome(openhtf.PhaseResult.STOP)\n      self._current_phase_thread = None\n\n    # Refresh the result in case a validation for a partially set measurement\n    # raised an exception.\n    result = override_result or phase_state.result\n    _LOG.debug('Phase %s finished with result %s', phase_desc.name,\n               result.phase_result)\n    return result", "response": "Executes the given phase and returns a PhaseExecutionOutcome."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstopping execution of the current phase.", "response": "def stop(self, timeout_s=None):\n    \"\"\"Stops execution of the current phase, if any.\n\n    It will raise a ThreadTerminationError, which will cause the test to stop\n    executing and terminate with an ERROR state.\n\n    Args:\n      timeout_s: int or None, timeout in seconds to wait for the phase to stop.\n    \"\"\"\n    self._stopping.set()\n    with self._current_phase_thread_lock:\n      phase_thread = self._current_phase_thread\n      if not phase_thread:\n        return\n\n    if phase_thread.is_alive():\n      phase_thread.kill()\n\n      _LOG.debug('Waiting for cancelled phase to exit: %s', phase_thread)\n      timeout = timeouts.PolledTimeout.from_seconds(timeout_s)\n      while phase_thread.is_alive() and not timeout.has_expired():\n        time.sleep(0.1)\n      _LOG.debug('Cancelled phase %s exit',\n                 \"didn't\" if phase_thread.is_alive() else 'did')\n    # Clear the currently running phase, whether it finished or timed out.\n    self.test_state.stop_running_phase()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_code_info(phases_or_groups):\n  if isinstance(phases_or_groups, PhaseGroup):\n    return phases_or_groups.load_code_info()\n  ret = []\n  for phase in phases_or_groups:\n    if isinstance(phase, PhaseGroup):\n      ret.append(phase.load_code_info())\n    else:\n      ret.append(\n          mutablerecords.CopyRecord(\n              phase, code_info=test_record.CodeInfo.for_function(phase.func)))\n  return ret", "response": "Recursively load code info for a PhaseGroup or list of phases or groups."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flatten_phases_and_groups(phases_or_groups):\n  if isinstance(phases_or_groups, PhaseGroup):\n    phases_or_groups = [phases_or_groups]\n  ret = []\n  for phase in phases_or_groups:\n    if isinstance(phase, PhaseGroup):\n      ret.append(phase.flatten())\n    elif isinstance(phase, collections.Iterable):\n      ret.extend(flatten_phases_and_groups(phase))\n    else:\n      ret.append(phase_descriptor.PhaseDescriptor.wrap_or_copy(phase))\n  return ret", "response": "Recursively flatten nested lists for the list of phases or groups."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies only the args that the phase knows.", "response": "def optionally_with_args(phase, **kwargs):\n  \"\"\"Apply only the args that the phase knows.\n\n  If the phase has a **kwargs-style argument, it counts as knowing all args.\n\n  Args:\n    phase: phase_descriptor.PhaseDescriptor or PhaseGroup or callable, or\n        iterable of those, the phase or phase group (or iterable) to apply\n        with_args to.\n    **kwargs: arguments to apply to the phase.\n\n  Returns:\n    phase_descriptor.PhaseDescriptor or PhaseGroup or iterable with the updated\n    args.\n  \"\"\"\n  if isinstance(phase, PhaseGroup):\n    return phase.with_args(**kwargs)\n  if isinstance(phase, collections.Iterable):\n    return [optionally_with_args(p, **kwargs) for p in phase]\n\n  if not isinstance(phase, phase_descriptor.PhaseDescriptor):\n    phase = phase_descriptor.PhaseDescriptor.wrap_or_copy(phase)\n  return phase.with_known_args(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef optionally_with_plugs(phase, **subplugs):\n  if isinstance(phase, PhaseGroup):\n    return phase.with_plugs(**subplugs)\n  if isinstance(phase, collections.Iterable):\n    return [optionally_with_plugs(p, **subplugs) for p in phase]\n\n  if not isinstance(phase, phase_descriptor.PhaseDescriptor):\n    phase = phase_descriptor.PhaseDescriptor.wrap_or_copy(phase)\n  return phase.with_known_plugs(**subplugs)", "response": "Applies only the with_plugs that the phase knows."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert list of phases or groups into a new PhaseGroup if not already.", "response": "def convert_if_not(cls, phases_or_groups):\n    \"\"\"Convert list of phases or groups into a new PhaseGroup if not already.\"\"\"\n    if isinstance(phases_or_groups, PhaseGroup):\n      return mutablerecords.CopyRecord(phases_or_groups)\n\n    flattened = flatten_phases_and_groups(phases_or_groups)\n    return cls(main=flattened)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a PhaseGroup creator function with setup and teardown phases.", "response": "def with_context(cls, setup_phases, teardown_phases):\n    \"\"\"Create PhaseGroup creator function with setup and teardown phases.\n\n    Args:\n      setup_phases: list of phase_descriptor.PhaseDescriptors/PhaseGroups/\n          callables/iterables, phases to run during the setup for the PhaseGroup\n          returned from the created function.\n      teardown_phases: list of phase_descriptor.PhaseDescriptors/PhaseGroups/\n          callables/iterables, phases to run during the teardown for the\n          PhaseGroup returned from the created function.\n\n    Returns:\n      Function that takes *phases and returns a PhaseGroup with the predefined\n      setup and teardown phases, with *phases as the main phases.\n    \"\"\"\n    setup = flatten_phases_and_groups(setup_phases)\n    teardown = flatten_phases_and_groups(teardown_phases)\n\n    def _context_wrapper(*phases):\n      return cls(setup=setup,\n                 main=flatten_phases_and_groups(phases),\n                 teardown=teardown)\n    return _context_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncombining with another PhaseGroup and return the result.", "response": "def combine(self, other, name=None):\n    \"\"\"Combine with another PhaseGroup and return the result.\"\"\"\n    return PhaseGroup(\n        setup=self.setup + other.setup,\n        main=self.main + other.main,\n        teardown=self.teardown + other.teardown,\n        name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wrap(self, main_phases, name=None):\n    new_main = list(self.main)\n    if isinstance(main_phases, collections.Iterable):\n      new_main.extend(main_phases)\n    else:\n      new_main.append(main_phases)\n    return PhaseGroup(\n        setup=self.setup,\n        main=new_main,\n        teardown=self.teardown,\n        name=name)", "response": "Returns a PhaseGroup with additional main phases."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload coded info for all contained phases.", "response": "def load_code_info(self):\n    \"\"\"Load coded info for all contained phases.\"\"\"\n    return PhaseGroup(\n        setup=load_code_info(self.setup),\n        main=load_code_info(self.main),\n        teardown=load_code_info(self.teardown),\n        name=self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npublishes a message to all the subscribers.", "response": "def publish(cls, message, client_filter=None):\n    \"\"\"Publish messages to subscribers.\n\n    Args:\n      message: The message to publish.\n      client_filter: A filter function to call passing in each client. Only\n                     clients for whom the function returns True will have the\n                     message sent to them.\n    \"\"\"\n    with cls._lock:\n      for client in cls.subscribers:\n        if (not client_filter) or client_filter(client):\n          client.send(message)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nincrementing counter and raises an exception for first two runs.", "response": "def run(self):\n    \"\"\"Increments counter and raises an exception for first two runs.\"\"\"\n    self.count += 1\n    print('FailTwicePlug: Run number %s' % (self.count))\n    if self.count < 3:\n      raise RuntimeError('Fails a couple times')\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstrings substitution of name.", "response": "def format_strings(self, **kwargs):\n    \"\"\"String substitution of name.\"\"\"\n    return mutablerecords.CopyRecord(\n        self, name=util.format_string(self.name, kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wrap_or_copy(cls, func, **options):\n    if isinstance(func, openhtf.PhaseGroup):\n      raise PhaseWrapError('Cannot wrap PhaseGroup <%s> as a phase.' % (\n          func.name or 'Unnamed'))\n    if isinstance(func, cls):\n      # We want to copy so that a phase can be reused with different options\n      # or kwargs.  See with_args() below for more details.\n      retval = mutablerecords.CopyRecord(func)\n    else:\n      retval = cls(func)\n    retval.options.update(**options)\n    return retval", "response": "Wrap a function or instance of a PhaseDescriptor class and return a new copy of it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef with_known_args(self, **kwargs):\n    argspec = inspect.getargspec(self.func)\n    stored = {}\n    for key, arg in six.iteritems(kwargs):\n      if key in argspec.args or argspec.keywords:\n        stored[key] = arg\n    if stored:\n      return self.with_args(**stored)\n    return self", "response": "Send only known keyword - arguments to the phase when called."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef with_args(self, **kwargs):\n    # Make a copy so we can have multiple of the same phase with different args\n    # in the same test.\n    new_info = mutablerecords.CopyRecord(self)\n    new_info.options = new_info.options.format_strings(**kwargs)\n    new_info.extra_kwargs.update(kwargs)\n    new_info.measurements = [m.with_args(**kwargs) for m in self.measurements]\n    return new_info", "response": "Send these keyword - arguments to the phase when called."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsubstitutes plugs for this phase.", "response": "def _apply_with_plugs(self, subplugs, error_on_unknown):\n    \"\"\"Substitute plugs for placeholders for this phase.\n\n    Args:\n      subplugs: dict of plug name to plug class, plug classes to replace.\n      error_on_unknown: bool, if True, then error when an unknown plug name is\n          provided.\n\n    Raises:\n      openhtf.plugs.InvalidPlugError if for one of the plug names one of the\n      following is true:\n        - error_on_unknown is True and the plug name is not registered.\n        - The new plug subclass is not a subclass of the original.\n        - The original plug class is not a placeholder or automatic placeholder.\n\n    Returns:\n      PhaseDescriptor with updated plugs.\n    \"\"\"\n    plugs_by_name = {plug.name: plug for plug in self.plugs}\n    new_plugs = dict(plugs_by_name)\n\n    for name, sub_class in six.iteritems(subplugs):\n      original_plug = plugs_by_name.get(name)\n      accept_substitute = True\n      if original_plug is None:\n        if not error_on_unknown:\n          continue\n        accept_substitute = False\n      elif isinstance(original_plug.cls, openhtf.plugs.PlugPlaceholder):\n        accept_substitute = issubclass(sub_class, original_plug.cls.base_class)\n      else:\n        # Check __dict__ to see if the attribute is explicitly defined in the\n        # class, rather than being defined in a parent class.\n        accept_substitute = ('auto_placeholder' in original_plug.cls.__dict__\n                             and original_plug.cls.auto_placeholder\n                             and issubclass(sub_class, original_plug.cls))\n\n      if not accept_substitute:\n        raise openhtf.plugs.InvalidPlugError(\n            'Could not find valid placeholder for substitute plug %s '\n            'required for phase %s' % (name, self.name))\n      new_plugs[name] = mutablerecords.CopyRecord(original_plug, cls=sub_class)\n\n    return mutablerecords.CopyRecord(\n        self,\n        plugs=list(new_plugs.values()),\n        options=self.options.format_strings(**subplugs),\n        measurements=[m.with_args(**subplugs) for m in self.measurements])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flush_buffers(self):\n    while True:\n      try:\n        self.read(FLUSH_READ_SIZE, timeout_ms=10)\n      except usb_exceptions.LibusbWrappingError as exception:\n        if exception.is_timeout():\n          break\n        raise", "response": "Default implementation calls Read until it blocks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _dotify(cls, data):\n    return ''.join(char if char in cls.PRINTABLE_DATA else '.' for char in data)", "response": "Add dots to the data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_usb_serial(self, port_num):\n\n    port = self.port_map[str(port_num)]\n    arg = ''.join(['DEVICE INFO,', self._addr, '.', port])\n    cmd = (['esuit64', '-t', arg])\n    info = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n    serial = None\n    if \"SERIAL\" in info:\n      serial_info = info.split('SERIAL:')[1]\n      serial = serial_info.split('\\n')[0].strip()\n      use_info = info.split('BY')[1].split(' ')[1]\n      if use_info == 'NO':\n        cmd = (['esuit64', '-t', 'AUTO USE ALL'])\n        subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n        time.sleep(50.0/1000.0)\n    else:\n      raise ValueError('No USB device detected')\n    return serial", "response": "Get the USB serial number of a given port number."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nopening usb port Args: port_num: port number on the Cambrionix unit Return: usb handle", "response": "def open_usb_handle(self, port_num):\n    \"\"\"open usb port\n\n    Args:\n      port_num: port number on the Cambrionix unit\n\n    Return:\n      usb handle\n    \"\"\"\n    serial = self.get_usb_serial(port_num)\n    return local_usb.LibUsbHandle.open(serial_number=serial)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the visible length of the string when printed.", "response": "def _printed_len(some_string):\n  \"\"\"Compute the visible length of the string when printed.\"\"\"\n  return len([x for  x in ANSI_ESC_RE.sub('', some_string)\n              if x in string.printable])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting a message as a banner with a fixed width.", "response": "def banner_print(msg, color='', width=60, file=sys.stdout, logger=_LOG):\n  \"\"\"Print the message as a banner with a fixed width.\n\n  Also logs the message (un-bannered) to the given logger at the debug level.\n\n  Args:\n    msg: The message to print.\n    color: Optional colorama color string to be applied to the message. You can\n        concatenate colorama color strings together in order to get any set of\n        effects you want.\n    width: Total width for the resulting banner.\n    file: A file object to which the banner text will be written. Intended for\n        use with CLI output file objects like sys.stdout.\n    logger: A logger to use, or None to disable logging.\n\n  Example:\n\n    >>> banner_print('Foo Bar Baz')\n\n    ======================== Foo Bar Baz =======================\n\n  \"\"\"\n  if logger:\n    logger.debug(ANSI_ESC_RE.sub('', msg))\n  if CLI_QUIET:\n    return\n  lpad = int(math.ceil((width - _printed_len(msg) - 2) / 2.0)) * '='\n  rpad = int(math.floor((width - _printed_len(msg) - 2) / 2.0)) * '='\n  file.write('{sep}{color}{lpad} {msg} {rpad}{reset}{sep}{sep}'.format(\n      sep=_linesep_for_file(file), color=color, lpad=lpad, msg=msg, rpad=rpad,\n      reset=colorama.Style.RESET_ALL))\n  file.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bracket_print(msg, color='', width=8, file=sys.stdout):\n  if CLI_QUIET:\n    return\n  lpad = int(math.ceil((width - 2 - _printed_len(msg)) / 2.0)) * ' '\n  rpad = int(math.floor((width - 2 - _printed_len(msg)) / 2.0)) * ' '\n  file.write('[{lpad}{bright}{color}{msg}{reset}{rpad}]'.format(\n      lpad=lpad, bright=colorama.Style.BRIGHT, color=color, msg=msg,\n      reset=colorama.Style.RESET_ALL, rpad=rpad))\n  file.write(colorama.Style.RESET_ALL)\n  file.write(_linesep_for_file(file))\n  file.flush()", "response": "Prints the message in brackets in the specified color and end the line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the message to file and also log it.", "response": "def cli_print(msg, color='', end=None, file=sys.stdout, logger=_LOG):\n  \"\"\"Print the message to file and also log it.\n\n  This function is intended as a 'tee' mechanism to enable the CLI interface as\n  a first-class citizen, while ensuring that everything the operator sees also\n  has an analogous logging entry in the test record for later inspection.\n\n  Args:\n    msg: The message to print/log.\n    color: Optional colorama color string to be applied to the message. You can\n        concatenate colorama color strings together in order to get any set of\n        effects you want.\n    end: A custom line-ending string to print instead of newline.\n    file: A file object to which the baracketed text will be written. Intended\n        for use with CLI output file objects like sys.stdout.\n    logger: A logger to use, or None to disable logging.\n  \"\"\"\n  if logger:\n    logger.debug('-> {}'.format(msg))\n  if CLI_QUIET:\n    return\n  if end is None:\n    end = _linesep_for_file(file)\n  file.write('{color}{msg}{reset}{end}'.format(\n      color=color, msg=msg, reset=colorama.Style.RESET_ALL, end=end))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the error message to the file in the specified color.", "response": "def error_print(msg, color=colorama.Fore.RED, file=sys.stderr):\n  \"\"\"Print the error message to the file in the specified color.\n\n  Args:\n    msg: The error message to be printed.\n    color: Optional colorama color string to be applied to the message. You can\n        concatenate colorama color strings together here, but note that style\n        strings will not be applied.\n    file: A file object to which the baracketed text will be written. Intended\n        for use with CLI output file objects, specifically sys.stderr.\n  \"\"\"\n  if CLI_QUIET:\n    return\n  file.write('{sep}{bright}{color}Error: {normal}{msg}{sep}{reset}'.format(\n      sep=_linesep_for_file(file), bright=colorama.Style.BRIGHT, color=color,\n      normal=colorama.Style.NORMAL, msg=msg, reset=colorama.Style.RESET_ALL))\n  file.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reraise(exc_type, message=None, *args, **kwargs):  # pylint: disable=invalid-name\n  last_lineno = inspect.currentframe().f_back.f_lineno\n  line_msg = 'line %s: ' % last_lineno\n  if message:\n    line_msg += str(message)\n  raise exc_type(line_msg, *args, **kwargs).raise_with_traceback(sys.exc_info()[2])", "response": "This function is used to reraise an exception in a library."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot1D_mat(a, b, M, title=''):\n    na, nb = M.shape\n\n    gs = gridspec.GridSpec(3, 3)\n\n    xa = np.arange(na)\n    xb = np.arange(nb)\n\n    ax1 = pl.subplot(gs[0, 1:])\n    pl.plot(xb, b, 'r', label='Target distribution')\n    pl.yticks(())\n    pl.title(title)\n\n    ax2 = pl.subplot(gs[1:, 0])\n    pl.plot(a, xa, 'b', label='Source distribution')\n    pl.gca().invert_xaxis()\n    pl.gca().invert_yaxis()\n    pl.xticks(())\n\n    pl.subplot(gs[1:, 1:], sharex=ax1, sharey=ax2)\n    pl.imshow(M, interpolation='nearest')\n    pl.axis('off')\n\n    pl.xlim((0, nb))\n    pl.tight_layout()\n    pl.subplots_adjust(wspace=0., hspace=0.2)", "response": "Plots the matrix M with the source and target 1D distribution a and b on the left and b on the tot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot2D_samples_mat(xs, xt, G, thr=1e-8, **kwargs):\n    if ('color' not in kwargs) and ('c' not in kwargs):\n        kwargs['color'] = 'k'\n    mx = G.max()\n    for i in range(xs.shape[0]):\n        for j in range(xt.shape[0]):\n            if G[i, j] / mx > thr:\n                pl.plot([xs[i, 0], xt[j, 0]], [xs[i, 1], xt[j, 1]],\n                        alpha=G[i, j] / mx, **kwargs)", "response": "Plot matrix M in 2D with lines using alpha values"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sinkhorn_lpl1_mm(a, labels_a, b, M, reg, eta=0.1, numItermax=10,\n                     numInnerItermax=200, stopInnerThr=1e-9, verbose=False,\n                     log=False, to_numpy=True):\n    \"\"\"\n    Solve the entropic regularization optimal transport problem with nonconvex\n    group lasso regularization on GPU\n\n    If the input matrix are in numpy format, they will be uploaded to the\n    GPU first which can incur significant time overhead.\n\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega_e(\\gamma)\n        + \\eta \\Omega_g(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega_e` is the entropic regularization term\n        :math:`\\Omega_e(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - :math:`\\Omega_g` is the group lasso  regulaization term\n      :math:`\\Omega_g(\\gamma)=\\sum_{i,c} \\|\\gamma_{i,\\mathcal{I}_c}\\|^{1/2}_1`\n      where  :math:`\\mathcal{I}_c` are the index of samples from class c\n      in the source domain.\n    - a and b are source and target weights (sum to 1)\n\n    The algorithm used for solving the problem is the generalised conditional\n    gradient as proposed in  [5]_ [7]_\n\n\n    Parameters\n    ----------\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    labels_a : np.ndarray (ns,)\n        labels of samples in the source domain\n    b : np.ndarray (nt,)\n        samples weights in the target domain\n    M : np.ndarray (ns,nt)\n        loss matrix\n    reg : float\n        Regularization term for entropic regularization >0\n    eta : float, optional\n        Regularization term  for group lasso regularization >0\n    numItermax : int, optional\n        Max number of iterations\n    numInnerItermax : int, optional\n        Max number of iterations (inner sinkhorn solver)\n    stopInnerThr : float, optional\n        Stop threshold on error (inner sinkhorn solver) (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n    to_numpy : boolean, optional (default True)\n        If true convert back the GPU array result to numpy format.\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n\n    References\n    ----------\n\n    .. [5] N. Courty; R. Flamary; D. Tuia; A. Rakotomamonjy,\n       \"Optimal Transport for Domain Adaptation,\" in IEEE\n       Transactions on Pattern Analysis and Machine Intelligence ,\n       vol.PP, no.99, pp.1-1\n    .. [7] Rakotomamonjy, A., Flamary, R., & Courty, N. (2015).\n       Generalized conditional gradient: analysis of convergence\n       and applications. arXiv preprint arXiv:1510.06567.\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized OT\n    ot.bregman.sinkhorn : Entropic regularized OT\n    ot.optim.cg : General regularized OT\n\n    \"\"\"\n\n    a, labels_a, b, M = utils.to_gpu(a, labels_a, b, M)\n\n    p = 0.5\n    epsilon = 1e-3\n\n    indices_labels = []\n    labels_a2 = cp.asnumpy(labels_a)\n    classes = npp.unique(labels_a2)\n    for c in classes:\n        idxc, = utils.to_gpu(npp.where(labels_a2 == c))\n        indices_labels.append(idxc)\n\n    W = np.zeros(M.shape)\n\n    for cpt in range(numItermax):\n        Mreg = M + eta * W\n        transp = sinkhorn(a, b, Mreg, reg, numItermax=numInnerItermax,\n                          stopThr=stopInnerThr, to_numpy=False)\n        # the transport has been computed. Check if classes are really\n        # separated\n        W = np.ones(M.shape)\n        for (i, c) in enumerate(classes):\n\n            majs = np.sum(transp[indices_labels[i]], axis=0)\n            majs = p * ((majs + epsilon)**(p - 1))\n            W[indices_labels[i]] = majs\n\n    if to_numpy:\n        return utils.to_np(transp)\n    else:\n        return transp", "response": "Solve the entropic regularization optimal transport problem with nonconvex group lasso regularization on GPU and the optimal transport problem with sinkhorn regularization on GPU."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_1D_gauss(n, m, s):\n    x = np.arange(n, dtype=np.float64)\n    h = np.exp(-(x - m)**2 / (2 * s**2))\n    return h / h.sum()", "response": "make a 1D histogram for a gaussian distribution"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake 2D samples from a 2D gaussian distribution", "response": "def make_2D_samples_gauss(n, m, sigma, random_state=None):\n    \"\"\"return n samples drawn from 2D gaussian N(m,sigma)\n\n    Parameters\n    ----------\n\n    n : int\n        number of samples to make\n    m : np.array (2,)\n        mean value of the gaussian distribution\n    sigma : np.array (2,2)\n        covariance matrix of the gaussian distribution\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : np.array (n,2)\n          n samples drawn from  N(m,sigma)\n\n    \"\"\"\n\n    generator = check_random_state(random_state)\n    if np.isscalar(sigma):\n        sigma = np.array([sigma, ])\n    if len(sigma) > 1:\n        P = sp.linalg.sqrtm(sigma)\n        res = generator.randn(n, 2).dot(P) + m\n    else:\n        res = generator.randn(n, 2) * np.sqrt(sigma) + m\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_2D_samples_gauss(n, m, sigma, random_state=None):\n    return make_2D_samples_gauss(n, m, sigma, random_state=None)", "response": "Returns a 2D array of samples from a gaussian distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the data classif dataset for classification problems.", "response": "def make_data_classif(dataset, n, nz=.5, theta=0, random_state=None, **kwargs):\n    \"\"\" dataset generation for classification problems\n\n    Parameters\n    ----------\n\n    dataset : str\n        type of classification problem (see code)\n    n : int\n        number of training samples\n    nz : float\n        noise level (>0)\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : np.array (n,d)\n          n observation of size d\n    y : np.array (n,)\n          labels of the samples\n\n    \"\"\"\n\n    generator = check_random_state(random_state)\n\n    if dataset.lower() == '3gauss':\n        y = np.floor((np.arange(n) * 1.0 / n * 3)) + 1\n        x = np.zeros((n, 2))\n        # class 1\n        x[y == 1, 0] = -1.\n        x[y == 1, 1] = -1.\n        x[y == 2, 0] = -1.\n        x[y == 2, 1] = 1.\n        x[y == 3, 0] = 1.\n        x[y == 3, 1] = 0\n\n        x[y != 3, :] += 1.5 * nz * generator.randn(sum(y != 3), 2)\n        x[y == 3, :] += 2 * nz * generator.randn(sum(y == 3), 2)\n\n    elif dataset.lower() == '3gauss2':\n        y = np.floor((np.arange(n) * 1.0 / n * 3)) + 1\n        x = np.zeros((n, 2))\n        y[y == 4] = 3\n        # class 1\n        x[y == 1, 0] = -2.\n        x[y == 1, 1] = -2.\n        x[y == 2, 0] = -2.\n        x[y == 2, 1] = 2.\n        x[y == 3, 0] = 2.\n        x[y == 3, 1] = 0\n\n        x[y != 3, :] += nz * generator.randn(sum(y != 3), 2)\n        x[y == 3, :] += 2 * nz * generator.randn(sum(y == 3), 2)\n\n    elif dataset.lower() == 'gaussrot':\n        rot = np.array(\n            [[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]])\n        m1 = np.array([-1, 1])\n        m2 = np.array([1, -1])\n        y = np.floor((np.arange(n) * 1.0 / n * 2)) + 1\n        n1 = np.sum(y == 1)\n        n2 = np.sum(y == 2)\n        x = np.zeros((n, 2))\n\n        x[y == 1, :] = get_2D_samples_gauss(n1, m1, nz, random_state=generator)\n        x[y == 2, :] = get_2D_samples_gauss(n2, m2, nz, random_state=generator)\n\n        x = x.dot(rot)\n\n    else:\n        x = np.array(0)\n        y = np.array(0)\n        print(\"unknown dataset\")\n\n    return x, y.astype(int)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dataset with classif randomized", "response": "def get_data_classif(dataset, n, nz=.5, theta=0, random_state=None, **kwargs):\n    \"\"\" Deprecated see  make_data_classif   \"\"\"\n    return make_data_classif(dataset, n, nz=.5, theta=0, random_state=None, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sinkhorn(a, b, M, reg, method='sinkhorn', numItermax=1000,\n             stopThr=1e-9, verbose=False, log=False, **kwargs):\n    u\"\"\"\n    Solve the entropic regularization optimal transport problem and return the OT matrix\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - a and b are source and target weights (sum to 1)\n\n    The algorithm used for solving the problem is the Sinkhorn-Knopp matrix scaling algorithm as proposed in [2]_\n\n\n    Parameters\n    ----------\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,) or np.ndarray (nt,nbb)\n        samples in the target domain, compute sinkhorn with multiple targets\n        and fixed M if b is a matrix (return OT loss + dual variables in log)\n    M : np.ndarray (ns,nt)\n        loss matrix\n    reg : float\n        Regularization term >0\n    method : str\n        method used for the solver either 'sinkhorn', 'greenkhorn', 'sinkhorn_stabilized' or\n        'sinkhorn_epsilon_scaling', see those function for specific parameters\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    Examples\n    --------\n\n    >>> import ot\n    >>> a=[.5,.5]\n    >>> b=[.5,.5]\n    >>> M=[[0.,1.],[1.,0.]]\n    >>> ot.sinkhorn(a,b,M,1)\n    array([[ 0.36552929,  0.13447071],\n           [ 0.13447071,  0.36552929]])\n\n\n    References\n    ----------\n\n    .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation of Optimal Transport, Advances in Neural Information Processing Systems (NIPS) 26, 2013\n\n    .. [9] Schmitzer, B. (2016). Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems. arXiv preprint arXiv:1610.06519.\n\n    .. [10] Chizat, L., Peyr\u00e9, G., Schmitzer, B., & Vialard, F. X. (2016). Scaling algorithms for unbalanced transport problems. arXiv preprint arXiv:1607.05816.\n\n\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized OT\n    ot.optim.cg : General regularized OT\n    ot.bregman.sinkhorn_knopp : Classic Sinkhorn [2]\n    ot.bregman.sinkhorn_stabilized: Stabilized sinkhorn [9][10]\n    ot.bregman.sinkhorn_epsilon_scaling: Sinkhorn with epslilon scaling [9][10]\n\n    \"\"\"\n\n    if method.lower() == 'sinkhorn':\n        def sink():\n            return sinkhorn_knopp(a, b, M, reg, numItermax=numItermax,\n                                  stopThr=stopThr, verbose=verbose, log=log, **kwargs)\n    elif method.lower() == 'greenkhorn':\n        def sink():\n            return greenkhorn(a, b, M, reg, numItermax=numItermax,\n                              stopThr=stopThr, verbose=verbose, log=log)\n    elif method.lower() == 'sinkhorn_stabilized':\n        def sink():\n            return sinkhorn_stabilized(a, b, M, reg, numItermax=numItermax,\n                                       stopThr=stopThr, verbose=verbose, log=log, **kwargs)\n    elif method.lower() == 'sinkhorn_epsilon_scaling':\n        def sink():\n            return sinkhorn_epsilon_scaling(\n                a, b, M, reg, numItermax=numItermax,\n                stopThr=stopThr, verbose=verbose, log=log, **kwargs)\n    else:\n        print('Warning : unknown method using classic Sinkhorn Knopp')\n\n        def sink():\n            return sinkhorn_knopp(a, b, M, reg, numItermax=numItermax,\n                                  stopThr=stopThr, verbose=verbose, log=log, **kwargs)\n\n    return sink()", "response": "u Solve the entropic regularization optimal transport problem and return the OT matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sinkhorn2(a, b, M, reg, method='sinkhorn', numItermax=1000,\n              stopThr=1e-9, verbose=False, log=False, **kwargs):\n    u\"\"\"\n    Solve the entropic regularization optimal transport problem and return the loss\n\n    The function solves the following optimization problem:\n\n    .. math::\n        W = \\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - a and b are source and target weights (sum to 1)\n\n    The algorithm used for solving the problem is the Sinkhorn-Knopp matrix scaling algorithm as proposed in [2]_\n\n\n    Parameters\n    ----------\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,) or np.ndarray (nt,nbb)\n        samples in the target domain, compute sinkhorn with multiple targets\n        and fixed M if b is a matrix (return OT loss + dual variables in log)\n    M : np.ndarray (ns,nt)\n        loss matrix\n    reg : float\n        Regularization term >0\n    method : str\n        method used for the solver either 'sinkhorn',  'sinkhorn_stabilized' or\n        'sinkhorn_epsilon_scaling', see those function for specific parameters\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    W : (nt) ndarray or float\n        Optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    Examples\n    --------\n\n    >>> import ot\n    >>> a=[.5,.5]\n    >>> b=[.5,.5]\n    >>> M=[[0.,1.],[1.,0.]]\n    >>> ot.sinkhorn2(a,b,M,1)\n    array([ 0.26894142])\n\n\n\n    References\n    ----------\n\n    .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation of Optimal Transport, Advances in Neural Information Processing Systems (NIPS) 26, 2013\n\n    .. [9] Schmitzer, B. (2016). Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems. arXiv preprint arXiv:1610.06519.\n\n    .. [10] Chizat, L., Peyr\u00e9, G., Schmitzer, B., & Vialard, F. X. (2016). Scaling algorithms for unbalanced transport problems. arXiv preprint arXiv:1607.05816.\n\n       [21] Altschuler J., Weed J., Rigollet P. : Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration, Advances in Neural Information Processing Systems (NIPS) 31, 2017\n\n\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized OT\n    ot.optim.cg : General regularized OT\n    ot.bregman.sinkhorn_knopp : Classic Sinkhorn [2]\n    ot.bregman.greenkhorn : Greenkhorn [21]\n    ot.bregman.sinkhorn_stabilized: Stabilized sinkhorn [9][10]\n    ot.bregman.sinkhorn_epsilon_scaling: Sinkhorn with epslilon scaling [9][10]\n\n    \"\"\"\n\n    if method.lower() == 'sinkhorn':\n        def sink():\n            return sinkhorn_knopp(a, b, M, reg, numItermax=numItermax,\n                                  stopThr=stopThr, verbose=verbose, log=log, **kwargs)\n    elif method.lower() == 'sinkhorn_stabilized':\n        def sink():\n            return sinkhorn_stabilized(a, b, M, reg, numItermax=numItermax,\n                                       stopThr=stopThr, verbose=verbose, log=log, **kwargs)\n    elif method.lower() == 'sinkhorn_epsilon_scaling':\n        def sink():\n            return sinkhorn_epsilon_scaling(\n                a, b, M, reg, numItermax=numItermax,\n                stopThr=stopThr, verbose=verbose, log=log, **kwargs)\n    else:\n        print('Warning : unknown method using classic Sinkhorn Knopp')\n\n        def sink():\n            return sinkhorn_knopp(a, b, M, reg, **kwargs)\n\n    b = np.asarray(b, dtype=np.float64)\n    if len(b.shape) < 2:\n        b = b.reshape((-1, 1))\n\n    return sink()", "response": "u Solve the entropic regularization optimal transport problem and return the loss matrix"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sinkhorn_knopp(a, b, M, reg, numItermax=1000,\n                   stopThr=1e-9, verbose=False, log=False, **kwargs):\n    \"\"\"\n    Solve the entropic regularization optimal transport problem and return the OT matrix\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - a and b are source and target weights (sum to 1)\n\n    The algorithm used for solving the problem is the Sinkhorn-Knopp matrix scaling algorithm as proposed in [2]_\n\n\n    Parameters\n    ----------\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,) or np.ndarray (nt,nbb)\n        samples in the target domain, compute sinkhorn with multiple targets\n        and fixed M if b is a matrix (return OT loss + dual variables in log)\n    M : np.ndarray (ns,nt)\n        loss matrix\n    reg : float\n        Regularization term >0\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    Examples\n    --------\n\n    >>> import ot\n    >>> a=[.5,.5]\n    >>> b=[.5,.5]\n    >>> M=[[0.,1.],[1.,0.]]\n    >>> ot.sinkhorn(a,b,M,1)\n    array([[ 0.36552929,  0.13447071],\n           [ 0.13447071,  0.36552929]])\n\n\n    References\n    ----------\n\n    .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation of Optimal Transport, Advances in Neural Information Processing Systems (NIPS) 26, 2013\n\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized OT\n    ot.optim.cg : General regularized OT\n\n    \"\"\"\n\n    a = np.asarray(a, dtype=np.float64)\n    b = np.asarray(b, dtype=np.float64)\n    M = np.asarray(M, dtype=np.float64)\n\n    if len(a) == 0:\n        a = np.ones((M.shape[0],), dtype=np.float64) / M.shape[0]\n    if len(b) == 0:\n        b = np.ones((M.shape[1],), dtype=np.float64) / M.shape[1]\n\n    # init data\n    Nini = len(a)\n    Nfin = len(b)\n\n    if len(b.shape) > 1:\n        nbb = b.shape[1]\n    else:\n        nbb = 0\n\n    if log:\n        log = {'err': []}\n\n    # we assume that no distances are null except those of the diagonal of\n    # distances\n    if nbb:\n        u = np.ones((Nini, nbb)) / Nini\n        v = np.ones((Nfin, nbb)) / Nfin\n    else:\n        u = np.ones(Nini) / Nini\n        v = np.ones(Nfin) / Nfin\n\n    # print(reg)\n\n    # Next 3 lines equivalent to K= np.exp(-M/reg), but faster to compute\n    K = np.empty(M.shape, dtype=M.dtype)\n    np.divide(M, -reg, out=K)\n    np.exp(K, out=K)\n\n    # print(np.min(K))\n    tmp2 = np.empty(b.shape, dtype=M.dtype)\n\n    Kp = (1 / a).reshape(-1, 1) * K\n    cpt = 0\n    err = 1\n    while (err > stopThr and cpt < numItermax):\n        uprev = u\n        vprev = v\n\n        KtransposeU = np.dot(K.T, u)\n        v = np.divide(b, KtransposeU)\n        u = 1. / np.dot(Kp, v)\n\n        if (np.any(KtransposeU == 0)\n                or np.any(np.isnan(u)) or np.any(np.isnan(v))\n                or np.any(np.isinf(u)) or np.any(np.isinf(v))):\n            # we have reached the machine precision\n            # come back to previous solution and quit loop\n            print('Warning: numerical errors at iteration', cpt)\n            u = uprev\n            v = vprev\n            break\n        if cpt % 10 == 0:\n            # we can speed up the process by checking for the error only all\n            # the 10th iterations\n            if nbb:\n                err = np.sum((u - uprev)**2) / np.sum((u)**2) + \\\n                    np.sum((v - vprev)**2) / np.sum((v)**2)\n            else:\n                # compute right marginal tmp2= (diag(u)Kdiag(v))^T1\n                np.einsum('i,ij,j->j', u, K, v, out=tmp2)\n                err = np.linalg.norm(tmp2 - b)**2  # violation of marginal\n            if log:\n                log['err'].append(err)\n\n            if verbose:\n                if cpt % 200 == 0:\n                    print(\n                        '{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n                print('{:5d}|{:8e}|'.format(cpt, err))\n        cpt = cpt + 1\n    if log:\n        log['u'] = u\n        log['v'] = v\n\n    if nbb:  # return only loss\n        res = np.einsum('ik,ij,jk,ij->k', u, K, v, M)\n        if log:\n            return res, log\n        else:\n            return res\n\n    else:  # return OT matrix\n\n        if log:\n            return u.reshape((-1, 1)) * K * v.reshape((1, -1)), log\n        else:\n            return u.reshape((-1, 1)) * K * v.reshape((1, -1))", "response": "Solve the entropic regularization optimal transport problem and return the OT matrix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsolving the entropic regularization optimal transport problem and return the OT matrix The algorithm used is based on the paper Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration by Jason Altschuler, Jonathan Weed, Philippe Rigollet appeared at NIPS 2017 which is a stochastic version of the Sinkhorn-Knopp algorithm [2]. The function solves the following optimization problem: .. math:: \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma) s.t. \\gamma 1 = a \\gamma^T 1= b \\gamma\\geq 0 where : - M is the (ns,nt) metric cost matrix - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})` - a and b are source and target weights (sum to 1) Parameters ---------- a : np.ndarray (ns,) samples weights in the source domain b : np.ndarray (nt,) or np.ndarray (nt,nbb) samples in the target domain, compute sinkhorn with multiple targets and fixed M if b is a matrix (return OT loss + dual variables in log) M : np.ndarray (ns,nt) loss matrix reg : float Regularization term >0 numItermax : int, optional Max number of iterations stopThr : float, optional Stop threshol on error (>0) log : bool, optional record log if True Returns ------- gamma : (ns x nt) ndarray Optimal transportation matrix for the given parameters log : dict log dictionary return only if log==True in parameters Examples -------- >>> import ot >>> a=[.5,.5] >>> b=[.5,.5] >>> M=[[0.,1.],[1.,0.]] >>> ot.bregman.greenkhorn(a,b,M,1) array([[ 0.36552929, 0.13447071], [ 0.13447071, 0.36552929]]) References ---------- .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation of Optimal Transport, Advances in Neural Information Processing Systems (NIPS) 26, 2013 [22] J. Altschuler, J.Weed, P. Rigollet : Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration, Advances in Neural Information Processing Systems (NIPS) 31, 2017 See Also -------- ot.lp.emd : Unregularized OT ot.optim.cg : General regularized OT", "response": "def greenkhorn(a, b, M, reg, numItermax=10000, stopThr=1e-9, verbose=False, log=False):\n    \"\"\"\n    Solve the entropic regularization optimal transport problem and return the OT matrix\n\n    The algorithm used is based on the paper\n\n    Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration\n        by Jason Altschuler, Jonathan Weed, Philippe Rigollet\n        appeared at NIPS 2017\n\n    which is a stochastic version of the Sinkhorn-Knopp algorithm [2].\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - a and b are source and target weights (sum to 1)\n\n\n\n    Parameters\n    ----------\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,) or np.ndarray (nt,nbb)\n        samples in the target domain, compute sinkhorn with multiple targets\n        and fixed M if b is a matrix (return OT loss + dual variables in log)\n    M : np.ndarray (ns,nt)\n        loss matrix\n    reg : float\n        Regularization term >0\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    Examples\n    --------\n\n    >>> import ot\n    >>> a=[.5,.5]\n    >>> b=[.5,.5]\n    >>> M=[[0.,1.],[1.,0.]]\n    >>> ot.bregman.greenkhorn(a,b,M,1)\n    array([[ 0.36552929,  0.13447071],\n           [ 0.13447071,  0.36552929]])\n\n\n    References\n    ----------\n\n    .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation of Optimal Transport, Advances in Neural Information Processing Systems (NIPS) 26, 2013\n       [22] J. Altschuler, J.Weed, P. Rigollet : Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration, Advances in Neural Information Processing Systems (NIPS) 31, 2017\n\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized OT\n    ot.optim.cg : General regularized OT\n\n    \"\"\"\n\n    a = np.asarray(a, dtype=np.float64)\n    b = np.asarray(b, dtype=np.float64)\n    M = np.asarray(M, dtype=np.float64)\n\n    if len(a) == 0:\n        a = np.ones((M.shape[0],), dtype=np.float64) / M.shape[0]\n    if len(b) == 0:\n        b = np.ones((M.shape[1],), dtype=np.float64) / M.shape[1]\n\n    n = a.shape[0]\n    m = b.shape[0]\n\n    # Next 3 lines equivalent to K= np.exp(-M/reg), but faster to compute\n    K = np.empty_like(M)\n    np.divide(M, -reg, out=K)\n    np.exp(K, out=K)\n\n    u = np.full(n, 1. / n)\n    v = np.full(m, 1. / m)\n    G = u[:, np.newaxis] * K * v[np.newaxis, :]\n\n    viol = G.sum(1) - a\n    viol_2 = G.sum(0) - b\n    stopThr_val = 1\n\n    if log:\n        log = dict()\n        log['u'] = u\n        log['v'] = v\n\n    for i in range(numItermax):\n        i_1 = np.argmax(np.abs(viol))\n        i_2 = np.argmax(np.abs(viol_2))\n        m_viol_1 = np.abs(viol[i_1])\n        m_viol_2 = np.abs(viol_2[i_2])\n        stopThr_val = np.maximum(m_viol_1, m_viol_2)\n\n        if m_viol_1 > m_viol_2:\n            old_u = u[i_1]\n            u[i_1] = a[i_1] / (K[i_1, :].dot(v))\n            G[i_1, :] = u[i_1] * K[i_1, :] * v\n\n            viol[i_1] = u[i_1] * K[i_1, :].dot(v) - a[i_1]\n            viol_2 += (K[i_1, :].T * (u[i_1] - old_u) * v)\n\n        else:\n            old_v = v[i_2]\n            v[i_2] = b[i_2] / (K[:, i_2].T.dot(u))\n            G[:, i_2] = u * K[:, i_2] * v[i_2]\n            #aviol = (G@one_m - a)\n            #aviol_2 = (G.T@one_n - b)\n            viol += (-old_v + v[i_2]) * K[:, i_2] * u\n            viol_2[i_2] = v[i_2] * K[:, i_2].dot(u) - b[i_2]\n\n            #print('b',np.max(abs(aviol -viol)),np.max(abs(aviol_2 - viol_2)))\n\n        if stopThr_val <= stopThr:\n            break\n    else:\n        print('Warning: Algorithm did not converge')\n\n    if log:\n        log['u'] = u\n        log['v'] = v\n\n    if log:\n        return G, log\n    else:\n        return G"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sinkhorn_epsilon_scaling(a, b, M, reg, numItermax=100, epsilon0=1e4, numInnerItermax=100,\n                             tau=1e3, stopThr=1e-9, warmstart=None, verbose=False, print_period=10, log=False, **kwargs):\n    \"\"\"\n    Solve the entropic regularization optimal transport problem with log\n    stabilization and epsilon scaling.\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - a and b are source and target weights (sum to 1)\n\n    The algorithm used for solving the problem is the Sinkhorn-Knopp matrix\n    scaling algorithm as proposed in [2]_ but with the log stabilization\n    proposed in [10]_ and the log scaling proposed in [9]_ algorithm 3.2\n\n\n    Parameters\n    ----------\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,)\n        samples in the target domain\n    M : np.ndarray (ns,nt)\n        loss matrix\n    reg : float\n        Regularization term >0\n    tau : float\n        thershold for max value in u or v for log scaling\n    tau : float\n        thershold for max value in u or v for log scaling\n    warmstart : tible of vectors\n        if given then sarting values for alpha an beta log scalings\n    numItermax : int, optional\n        Max number of iterations\n    numInnerItermax : int, optional\n        Max number of iterationsin the inner slog stabilized sinkhorn\n    epsilon0 : int, optional\n        first epsilon regularization value (then exponential decrease to reg)\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    Examples\n    --------\n\n    >>> import ot\n    >>> a=[.5,.5]\n    >>> b=[.5,.5]\n    >>> M=[[0.,1.],[1.,0.]]\n    >>> ot.bregman.sinkhorn_epsilon_scaling(a,b,M,1)\n    array([[ 0.36552929,  0.13447071],\n           [ 0.13447071,  0.36552929]])\n\n\n    References\n    ----------\n\n    .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation of Optimal Transport, Advances in Neural Information Processing Systems (NIPS) 26, 2013\n\n    .. [9] Schmitzer, B. (2016). Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems. arXiv preprint arXiv:1610.06519.\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized OT\n    ot.optim.cg : General regularized OT\n\n    \"\"\"\n\n    a = np.asarray(a, dtype=np.float64)\n    b = np.asarray(b, dtype=np.float64)\n    M = np.asarray(M, dtype=np.float64)\n\n    if len(a) == 0:\n        a = np.ones((M.shape[0],), dtype=np.float64) / M.shape[0]\n    if len(b) == 0:\n        b = np.ones((M.shape[1],), dtype=np.float64) / M.shape[1]\n\n    # init data\n    na = len(a)\n    nb = len(b)\n\n    # nrelative umerical precision with 64 bits\n    numItermin = 35\n    numItermax = max(numItermin, numItermax)  # ensure that last velue is exact\n\n    cpt = 0\n    if log:\n        log = {'err': []}\n\n    # we assume that no distances are null except those of the diagonal of\n    # distances\n    if warmstart is None:\n        alpha, beta = np.zeros(na), np.zeros(nb)\n    else:\n        alpha, beta = warmstart\n\n    def get_K(alpha, beta):\n        \"\"\"log space computation\"\"\"\n        return np.exp(-(M - alpha.reshape((na, 1))\n                        - beta.reshape((1, nb))) / reg)\n\n    # print(np.min(K))\n    def get_reg(n):  # exponential decreasing\n        return (epsilon0 - reg) * np.exp(-n) + reg\n\n    loop = 1\n    cpt = 0\n    err = 1\n    while loop:\n\n        regi = get_reg(cpt)\n\n        G, logi = sinkhorn_stabilized(a, b, M, regi, numItermax=numInnerItermax, stopThr=1e-9, warmstart=(\n            alpha, beta), verbose=False, print_period=20, tau=tau, log=True)\n\n        alpha = logi['alpha']\n        beta = logi['beta']\n\n        if cpt >= numItermax:\n            loop = False\n\n        if cpt % (print_period) == 0:  # spsion nearly converged\n            # we can speed up the process by checking for the error only all\n            # the 10th iterations\n            transp = G\n            err = np.linalg.norm(\n                (np.sum(transp, axis=0) - b))**2 + np.linalg.norm((np.sum(transp, axis=1) - a))**2\n            if log:\n                log['err'].append(err)\n\n            if verbose:\n                if cpt % (print_period * 10) == 0:\n                    print(\n                        '{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n                print('{:5d}|{:8e}|'.format(cpt, err))\n\n        if err <= stopThr and cpt > numItermin:\n            loop = False\n\n        cpt = cpt + 1\n    # print('err=',err,' cpt=',cpt)\n    if log:\n        log['alpha'] = alpha\n        log['beta'] = beta\n        log['warmstart'] = (log['alpha'], log['beta'])\n        return G, log\n    else:\n        return G", "response": "Solve the entropic regularization optimal transport problem with log - stabilization and epsilon scaling."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the weighted geometric mean of distributions", "response": "def geometricBar(weights, alldistribT):\n    \"\"\"return the weighted geometric mean of distributions\"\"\"\n    assert(len(weights) == alldistribT.shape[1])\n    return np.exp(np.dot(np.log(alldistribT), weights.T))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the geometric mean of distributions", "response": "def geometricMean(alldistribT):\n    \"\"\"return the  geometric mean of distributions\"\"\"\n    return np.exp(np.mean(np.log(alldistribT), axis=1))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef projR(gamma, p):\n    return np.multiply(gamma.T, p / np.maximum(np.sum(gamma, axis=1), 1e-10)).T", "response": "return the KL projection on the row constrints"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef projC(gamma, q):\n    return np.multiply(gamma, q / np.maximum(np.sum(gamma, axis=0), 1e-10))", "response": "return the KL projection on the column constrints"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef barycenter(A, M, reg, weights=None, numItermax=1000,\n               stopThr=1e-4, verbose=False, log=False):\n    \"\"\"Compute the entropic regularized wasserstein barycenter of distributions A\n\n     The function solves the following optimization problem:\n\n    .. math::\n       \\mathbf{a} = arg\\min_\\mathbf{a} \\sum_i W_{reg}(\\mathbf{a},\\mathbf{a}_i)\n\n    where :\n\n    - :math:`W_{reg}(\\cdot,\\cdot)` is the entropic regularized Wasserstein distance (see ot.bregman.sinkhorn)\n    - :math:`\\mathbf{a}_i` are training distributions in the columns of matrix :math:`\\mathbf{A}`\n    - reg and :math:`\\mathbf{M}` are respectively the regularization term and the cost matrix for OT\n\n    The algorithm used for solving the problem is the Sinkhorn-Knopp matrix scaling algorithm as proposed in [3]_\n\n    Parameters\n    ----------\n    A : np.ndarray (d,n)\n        n training distributions a_i of size d\n    M : np.ndarray (d,d)\n        loss matrix   for OT\n    reg : float\n        Regularization term >0\n    weights : np.ndarray (n,)\n        Weights of each histogram a_i on the simplex (barycentric coodinates)\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    a : (d,) ndarray\n        Wasserstein barycenter\n    log : dict\n        log dictionary return only if log==True in parameters\n\n\n    References\n    ----------\n\n    .. [3] Benamou, J. D., Carlier, G., Cuturi, M., Nenna, L., & Peyr\u00e9, G. (2015). Iterative Bregman projections for regularized transportation problems. SIAM Journal on Scientific Computing, 37(2), A1111-A1138.\n\n\n\n    \"\"\"\n\n    if weights is None:\n        weights = np.ones(A.shape[1]) / A.shape[1]\n    else:\n        assert(len(weights) == A.shape[1])\n\n    if log:\n        log = {'err': []}\n\n    # M = M/np.median(M) # suggested by G. Peyre\n    K = np.exp(-M / reg)\n\n    cpt = 0\n    err = 1\n\n    UKv = np.dot(K, np.divide(A.T, np.sum(K, axis=0)).T)\n    u = (geometricMean(UKv) / UKv.T).T\n\n    while (err > stopThr and cpt < numItermax):\n        cpt = cpt + 1\n        UKv = u * np.dot(K, np.divide(A, np.dot(K, u)))\n        u = (u.T * geometricBar(weights, UKv)).T / UKv\n\n        if cpt % 10 == 1:\n            err = np.sum(np.std(UKv, axis=1))\n\n            # log and verbose print\n            if log:\n                log['err'].append(err)\n\n            if verbose:\n                if cpt % 200 == 0:\n                    print(\n                        '{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n                print('{:5d}|{:8e}|'.format(cpt, err))\n\n    if log:\n        log['niter'] = cpt\n        return geometricBar(weights, UKv), log\n    else:\n        return geometricBar(weights, UKv)", "response": "Compute the entropic regularized wasserstein barycenter of distributions A and M."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the entropic regularized wasserstein barycenter of distributions A and A.", "response": "def convolutional_barycenter2d(A, reg, weights=None, numItermax=10000, stopThr=1e-9, stabThr=1e-30, verbose=False, log=False):\n    \"\"\"Compute the entropic regularized wasserstein barycenter of distributions A\n    where A is a collection of 2D images.\n\n     The function solves the following optimization problem:\n\n    .. math::\n       \\mathbf{a} = arg\\min_\\mathbf{a} \\sum_i W_{reg}(\\mathbf{a},\\mathbf{a}_i)\n\n    where :\n\n    - :math:`W_{reg}(\\cdot,\\cdot)` is the entropic regularized Wasserstein distance (see ot.bregman.sinkhorn)\n    - :math:`\\mathbf{a}_i` are training distributions (2D images) in the mast two dimensions of matrix :math:`\\mathbf{A}`\n    - reg is the regularization strength scalar value\n\n    The algorithm used for solving the problem is the Sinkhorn-Knopp matrix scaling algorithm as proposed in [21]_\n\n    Parameters\n    ----------\n    A : np.ndarray (n,w,h)\n        n distributions (2D images) of size w x h\n    reg : float\n        Regularization term >0\n    weights : np.ndarray (n,)\n        Weights of each image on the simplex (barycentric coodinates)\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    stabThr : float, optional\n        Stabilization threshold to avoid numerical precision issue\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    a : (w,h) ndarray\n        2D Wasserstein barycenter\n    log : dict\n        log dictionary return only if log==True in parameters\n\n\n    References\n    ----------\n\n    .. [21] Solomon, J., De Goes, F., Peyr\u00e9, G., Cuturi, M., Butscher, A., Nguyen, A. & Guibas, L. (2015).\n    Convolutional wasserstein distances: Efficient optimal transportation on geometric domains\n    ACM Transactions on Graphics (TOG), 34(4), 66\n\n\n    \"\"\"\n\n    if weights is None:\n        weights = np.ones(A.shape[0]) / A.shape[0]\n    else:\n        assert(len(weights) == A.shape[0])\n\n    if log:\n        log = {'err': []}\n\n    b = np.zeros_like(A[0, :, :])\n    U = np.ones_like(A)\n    KV = np.ones_like(A)\n\n    cpt = 0\n    err = 1\n\n    # build the convolution operator\n    t = np.linspace(0, 1, A.shape[1])\n    [Y, X] = np.meshgrid(t, t)\n    xi1 = np.exp(-(X - Y)**2 / reg)\n\n    def K(x):\n        return np.dot(np.dot(xi1, x), xi1)\n\n    while (err > stopThr and cpt < numItermax):\n\n        bold = b\n        cpt = cpt + 1\n\n        b = np.zeros_like(A[0, :, :])\n        for r in range(A.shape[0]):\n            KV[r, :, :] = K(A[r, :, :] / np.maximum(stabThr, K(U[r, :, :])))\n            b += weights[r] * np.log(np.maximum(stabThr, U[r, :, :] * KV[r, :, :]))\n        b = np.exp(b)\n        for r in range(A.shape[0]):\n            U[r, :, :] = b / np.maximum(stabThr, KV[r, :, :])\n\n        if cpt % 10 == 1:\n            err = np.sum(np.abs(bold - b))\n            # log and verbose print\n            if log:\n                log['err'].append(err)\n\n            if verbose:\n                if cpt % 200 == 0:\n                    print('{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n                print('{:5d}|{:8e}|'.format(cpt, err))\n\n    if log:\n        log['niter'] = cpt\n        log['U'] = U\n        return b, log\n    else:\n        return b"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the unmixing of an observation with a given dictionary using Wasserstein distance The function solve the following optimization problem: .. math:: \\mathbf{h} = arg\\min_\\mathbf{h} (1- \\\\alpha) W_{M,reg}(\\mathbf{a},\\mathbf{Dh})+\\\\alpha W_{M0,reg0}(\\mathbf{h}_0,\\mathbf{h}) where : - :math:`W_{M,reg}(\\cdot,\\cdot)` is the entropic regularized Wasserstein distance with M loss matrix (see ot.bregman.sinkhorn) - :math:`\\mathbf{a}` is an observed distribution, :math:`\\mathbf{h}_0` is aprior on unmixing - reg and :math:`\\mathbf{M}` are respectively the regularization term and the cost matrix for OT data fitting - reg0 and :math:`\\mathbf{M0}` are respectively the regularization term and the cost matrix for regularization - :math:`\\\\alpha`weight data fitting and regularization The optimization problem is solved suing the algorithm described in [4] Parameters ---------- a : np.ndarray (d) observed distribution D : np.ndarray (d,n) dictionary matrix M : np.ndarray (d,d) loss matrix M0 : np.ndarray (n,n) loss matrix h0 : np.ndarray (n,) prior on h reg : float Regularization term >0 (Wasserstein data fitting) reg0 : float Regularization term >0 (Wasserstein reg with h0) alpha : float How much should we trust the prior ([0,1]) numItermax : int, optional Max number of iterations stopThr : float, optional Stop threshol on error (>0) verbose : bool, optional Print information along iterations log : bool, optional record log if True Returns ------- a : (d,) ndarray Wasserstein barycenter log : dict log dictionary return only if log==True in parameters References ---------- .. [4] S. Nakhostin, N. Courty, R. Flamary, D. Tuia, T. Corpetti, Supervised planetary unmixing with optimal transport, Whorkshop on Hyperspectral Image and Signal Processing : Evolution in Remote Sensing (WHISPERS), 2016.", "response": "def unmix(a, D, M, M0, h0, reg, reg0, alpha, numItermax=1000,\n          stopThr=1e-3, verbose=False, log=False):\n    \"\"\"\n    Compute the unmixing of an observation with a given dictionary using Wasserstein distance\n\n    The function solve the following optimization problem:\n\n    .. math::\n       \\mathbf{h} = arg\\min_\\mathbf{h}  (1- \\\\alpha) W_{M,reg}(\\mathbf{a},\\mathbf{Dh})+\\\\alpha W_{M0,reg0}(\\mathbf{h}_0,\\mathbf{h})\n\n\n    where :\n\n    - :math:`W_{M,reg}(\\cdot,\\cdot)` is the entropic regularized Wasserstein distance with M loss matrix (see ot.bregman.sinkhorn)\n    - :math:`\\mathbf{a}` is an observed distribution,  :math:`\\mathbf{h}_0` is aprior on unmixing\n    - reg and :math:`\\mathbf{M}` are respectively the regularization term and the cost matrix for OT data fitting\n    - reg0 and :math:`\\mathbf{M0}` are respectively the regularization term and the cost matrix for regularization\n    - :math:`\\\\alpha`weight data fitting and regularization\n\n    The optimization problem is solved suing the algorithm described in [4]\n\n\n    Parameters\n    ----------\n    a : np.ndarray (d)\n        observed distribution\n    D : np.ndarray (d,n)\n        dictionary matrix\n    M : np.ndarray (d,d)\n        loss matrix\n    M0 : np.ndarray (n,n)\n        loss matrix\n    h0 : np.ndarray (n,)\n        prior on h\n    reg : float\n        Regularization term >0 (Wasserstein data fitting)\n    reg0 : float\n        Regularization term >0 (Wasserstein reg with h0)\n    alpha : float\n        How much should we trust the prior ([0,1])\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    a : (d,) ndarray\n        Wasserstein barycenter\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    References\n    ----------\n\n    .. [4] S. Nakhostin, N. Courty, R. Flamary, D. Tuia, T. Corpetti, Supervised planetary unmixing with optimal transport, Whorkshop on Hyperspectral Image and Signal Processing : Evolution in Remote Sensing (WHISPERS), 2016.\n\n    \"\"\"\n\n    # M = M/np.median(M)\n    K = np.exp(-M / reg)\n\n    # M0 = M0/np.median(M0)\n    K0 = np.exp(-M0 / reg0)\n    old = h0\n\n    err = 1\n    cpt = 0\n    # log = {'niter':0, 'all_err':[]}\n    if log:\n        log = {'err': []}\n\n    while (err > stopThr and cpt < numItermax):\n        K = projC(K, a)\n        K0 = projC(K0, h0)\n        new = np.sum(K0, axis=1)\n        # we recombine the current selection from dictionnary\n        inv_new = np.dot(D, new)\n        other = np.sum(K, axis=1)\n        # geometric interpolation\n        delta = np.exp(alpha * np.log(other) + (1 - alpha) * np.log(inv_new))\n        K = projR(K, delta)\n        K0 = np.dot(np.diag(np.dot(D.T, delta / inv_new)), K0)\n\n        err = np.linalg.norm(np.sum(K0, axis=1) - old)\n        old = new\n        if log:\n            log['err'].append(err)\n\n        if verbose:\n            if cpt % 200 == 0:\n                print('{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n            print('{:5d}|{:8e}|'.format(cpt, err))\n\n        cpt = cpt + 1\n\n    if log:\n        log['niter'] = cpt\n        return np.sum(K0, axis=1), log\n    else:\n        return np.sum(K0, axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef empirical_sinkhorn(X_s, X_t, reg, a=None, b=None, metric='sqeuclidean', numIterMax=10000, stopThr=1e-9, verbose=False, log=False, **kwargs):\n    '''\n    Solve the entropic regularization optimal transport problem and return the\n    OT matrix from empirical data\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - :math:`M` is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - :math:`a` and :math:`b` are source and target weights (sum to 1)\n\n\n    Parameters\n    ----------\n    X_s : np.ndarray (ns, d)\n        samples in the source domain\n    X_t : np.ndarray (nt, d)\n        samples in the target domain\n    reg : float\n        Regularization term >0\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,)\n        samples weights in the target domain\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Regularized optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    Examples\n    --------\n\n    >>> n_s = 2\n    >>> n_t = 2\n    >>> reg = 0.1\n    >>> X_s = np.reshape(np.arange(n_s), (n_s, 1))\n    >>> X_t = np.reshape(np.arange(0, n_t), (n_t, 1))\n    >>> emp_sinkhorn = empirical_sinkhorn(X_s, X_t, reg, verbose=False)\n    >>> print(emp_sinkhorn)\n    >>> [[4.99977301e-01 2.26989344e-05]\n        [2.26989344e-05 4.99977301e-01]]\n\n\n    References\n    ----------\n\n    .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation of Optimal Transport, Advances in Neural Information Processing Systems (NIPS) 26, 2013\n\n    .. [9] Schmitzer, B. (2016). Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems. arXiv preprint arXiv:1610.06519.\n\n    .. [10] Chizat, L., Peyr\u00e9, G., Schmitzer, B., & Vialard, F. X. (2016). Scaling algorithms for unbalanced transport problems. arXiv preprint arXiv:1607.05816.\n    '''\n\n    if a is None:\n        a = unif(np.shape(X_s)[0])\n    if b is None:\n        b = unif(np.shape(X_t)[0])\n\n    M = dist(X_s, X_t, metric=metric)\n\n    if log:\n        pi, log = sinkhorn(a, b, M, reg, numItermax=numIterMax, stopThr=stopThr, verbose=verbose, log=True, **kwargs)\n        return pi, log\n    else:\n        pi = sinkhorn(a, b, M, reg, numItermax=numIterMax, stopThr=stopThr, verbose=verbose, log=False, **kwargs)\n        return pi", "response": "This function solves the entropic regularization optimal transport problem and returns the OT matrix from empirical data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef empirical_sinkhorn2(X_s, X_t, reg, a=None, b=None, metric='sqeuclidean', numIterMax=10000, stopThr=1e-9, verbose=False, log=False, **kwargs):\n    '''\n    Solve the entropic regularization optimal transport problem from empirical\n    data and return the OT loss\n\n\n    The function solves the following optimization problem:\n\n    .. math::\n        W = \\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - :math:`M` is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - :math:`a` and :math:`b` are source and target weights (sum to 1)\n\n\n    Parameters\n    ----------\n    X_s : np.ndarray (ns, d)\n        samples in the source domain\n    X_t : np.ndarray (nt, d)\n        samples in the target domain\n    reg : float\n        Regularization term >0\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,)\n        samples weights in the target domain\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Regularized optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    Examples\n    --------\n\n    >>> n_s = 2\n    >>> n_t = 2\n    >>> reg = 0.1\n    >>> X_s = np.reshape(np.arange(n_s), (n_s, 1))\n    >>> X_t = np.reshape(np.arange(0, n_t), (n_t, 1))\n    >>> loss_sinkhorn = empirical_sinkhorn2(X_s, X_t, reg, verbose=False)\n    >>> print(loss_sinkhorn)\n    >>> [4.53978687e-05]\n\n\n    References\n    ----------\n\n    .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation of Optimal Transport, Advances in Neural Information Processing Systems (NIPS) 26, 2013\n\n    .. [9] Schmitzer, B. (2016). Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems. arXiv preprint arXiv:1610.06519.\n\n    .. [10] Chizat, L., Peyr\u00e9, G., Schmitzer, B., & Vialard, F. X. (2016). Scaling algorithms for unbalanced transport problems. arXiv preprint arXiv:1607.05816.\n    '''\n\n    if a is None:\n        a = unif(np.shape(X_s)[0])\n    if b is None:\n        b = unif(np.shape(X_t)[0])\n\n    M = dist(X_s, X_t, metric=metric)\n\n    if log:\n        sinkhorn_loss, log = sinkhorn2(a, b, M, reg, numItermax=numIterMax, stopThr=stopThr, verbose=verbose, log=log, **kwargs)\n        return sinkhorn_loss, log\n    else:\n        sinkhorn_loss = sinkhorn2(a, b, M, reg, numItermax=numIterMax, stopThr=stopThr, verbose=verbose, log=log, **kwargs)\n        return sinkhorn_loss", "response": "This function solves the entropic regularization optimal transport problem from empirical data and returns the OT loss of the entropic regularization problem from empirical data and the OT loss of the target domain."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef empirical_sinkhorn_divergence(X_s, X_t, reg, a=None, b=None, metric='sqeuclidean', numIterMax=10000, stopThr=1e-9, verbose=False, log=False, **kwargs):\n    '''\n    Compute the sinkhorn divergence loss from empirical data\n\n    The function solves the following optimization problems and return the\n    sinkhorn divergence :math:`S`:\n\n    .. math::\n\n        W &= \\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        W_a &= \\min_{\\gamma_a} <\\gamma_a,M_a>_F + reg\\cdot\\Omega(\\gamma_a)\n\n        W_b &= \\min_{\\gamma_b} <\\gamma_b,M_b>_F + reg\\cdot\\Omega(\\gamma_b)\n\n        S &= W - 1/2 * (W_a + W_b)\n\n    .. math::\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n\n             \\gamma_a 1 = a\n\n             \\gamma_a^T 1= a\n\n             \\gamma_a\\geq 0\n\n             \\gamma_b 1 = b\n\n             \\gamma_b^T 1= b\n\n             \\gamma_b\\geq 0\n    where :\n\n    - :math:`M` (resp. :math:`M_a, M_b`) is the (ns,nt) metric cost matrix (resp (ns, ns) and (nt, nt))\n    - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - :math:`a` and :math:`b` are source and target weights (sum to 1)\n\n\n    Parameters\n    ----------\n    X_s : np.ndarray (ns, d)\n        samples in the source domain\n    X_t : np.ndarray (nt, d)\n        samples in the target domain\n    reg : float\n        Regularization term >0\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,)\n        samples weights in the target domain\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Regularized optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    Examples\n    --------\n\n    >>> n_s = 2\n    >>> n_t = 4\n    >>> reg = 0.1\n    >>> X_s = np.reshape(np.arange(n_s), (n_s, 1))\n    >>> X_t = np.reshape(np.arange(0, n_t), (n_t, 1))\n    >>> emp_sinkhorn_div = empirical_sinkhorn_divergence(X_s, X_t, reg)\n    >>> print(emp_sinkhorn_div)\n    >>> [2.99977435]\n\n\n    References\n    ----------\n\n    .. [23] Aude Genevay, Gabriel Peyr\u00e9, Marco Cuturi, Learning Generative Models with Sinkhorn Divergences,  Proceedings of the Twenty-First International Conference on Artficial Intelligence and Statistics, (AISTATS) 21, 2018\n    '''\n    if log:\n        sinkhorn_loss_ab, log_ab = empirical_sinkhorn2(X_s, X_t, reg, a, b, metric=metric, numIterMax=numIterMax, stopThr=1e-9, verbose=verbose, log=log, **kwargs)\n\n        sinkhorn_loss_a, log_a = empirical_sinkhorn2(X_s, X_s, reg, a, b, metric=metric, numIterMax=numIterMax, stopThr=1e-9, verbose=verbose, log=log, **kwargs)\n\n        sinkhorn_loss_b, log_b = empirical_sinkhorn2(X_t, X_t, reg, a, b, metric=metric, numIterMax=numIterMax, stopThr=1e-9, verbose=verbose, log=log, **kwargs)\n\n        sinkhorn_div = sinkhorn_loss_ab - 1 / 2 * (sinkhorn_loss_a + sinkhorn_loss_b)\n\n        log = {}\n        log['sinkhorn_loss_ab'] = sinkhorn_loss_ab\n        log['sinkhorn_loss_a'] = sinkhorn_loss_a\n        log['sinkhorn_loss_b'] = sinkhorn_loss_b\n        log['log_sinkhorn_ab'] = log_ab\n        log['log_sinkhorn_a'] = log_a\n        log['log_sinkhorn_b'] = log_b\n\n        return max(0, sinkhorn_div), log\n\n    else:\n        sinkhorn_loss_ab = empirical_sinkhorn2(X_s, X_t, reg, a, b, metric=metric, numIterMax=numIterMax, stopThr=1e-9, verbose=verbose, log=log, **kwargs)\n\n        sinkhorn_loss_a = empirical_sinkhorn2(X_s, X_s, reg, a, b, metric=metric, numIterMax=numIterMax, stopThr=1e-9, verbose=verbose, log=log, **kwargs)\n\n        sinkhorn_loss_b = empirical_sinkhorn2(X_t, X_t, reg, a, b, metric=metric, numIterMax=numIterMax, stopThr=1e-9, verbose=verbose, log=log, **kwargs)\n\n        sinkhorn_div = sinkhorn_loss_ab - 1 / 2 * (sinkhorn_loss_a + sinkhorn_loss_b)\n        return max(0, sinkhorn_div)", "response": "Compute the sinkhorn divergence loss from empirical data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef signature(obj):\n    '''Get a signature object for the passed callable.'''\n\n    if not callable(obj):\n        raise TypeError('{0!r} is not a callable object'.format(obj))\n\n    if isinstance(obj, types.MethodType):\n        sig = signature(obj.__func__)\n        if obj.__self__ is None:\n            # Unbound method: the first parameter becomes positional-only\n            if sig.parameters:\n                first = sig.parameters.values()[0].replace(\n                    kind=_POSITIONAL_ONLY)\n                return sig.replace(\n                    parameters=(first,) + tuple(sig.parameters.values())[1:])\n            else:\n                return sig\n        else:\n            # In this case we skip the first parameter of the underlying\n            # function (usually `self` or `cls`).\n            return sig.replace(parameters=tuple(sig.parameters.values())[1:])\n\n    try:\n        sig = obj.__signature__\n    except AttributeError:\n        pass\n    else:\n        if sig is not None:\n            return sig\n\n    try:\n        # Was this function wrapped by a decorator?\n        wrapped = obj.__wrapped__\n    except AttributeError:\n        pass\n    else:\n        return signature(wrapped)\n\n    if isinstance(obj, types.FunctionType):\n        return Signature.from_function(obj)\n\n    if isinstance(obj, functools.partial):\n        sig = signature(obj.func)\n\n        new_params = OrderedDict(sig.parameters.items())\n\n        partial_args = obj.args or ()\n        partial_keywords = obj.keywords or {}\n        try:\n            ba = sig.bind_partial(*partial_args, **partial_keywords)\n        except TypeError:\n            msg = 'partial object {0!r} has incorrect arguments'.format(obj)\n            raise ValueError(msg)\n\n        for arg_name, arg_value in ba.arguments.items():\n            param = new_params[arg_name]\n            if arg_name in partial_keywords:\n                # We set a new default value, because the following code\n                # is correct:\n                #\n                #   >>> def foo(a): print(a)\n                #   >>> print(partial(partial(foo, a=10), a=20)())\n                #   20\n                #   >>> print(partial(partial(foo, a=10), a=20)(a=30))\n                #   30\n                #\n                # So, with 'partial' objects, passing a keyword argument is\n                # like setting a new default value for the corresponding\n                # parameter\n                #\n                # We also mark this parameter with '_partial_kwarg'\n                # flag.  Later, in '_bind', the 'default' value of this\n                # parameter will be added to 'kwargs', to simulate\n                # the 'functools.partial' real call.\n                new_params[arg_name] = param.replace(default=arg_value,\n                                                     _partial_kwarg=True)\n\n            elif (param.kind not in (_VAR_KEYWORD, _VAR_POSITIONAL)\n                  and not param._partial_kwarg):\n                new_params.pop(arg_name)\n\n        return sig.replace(parameters=new_params.values())\n\n    sig = None\n    if isinstance(obj, type):\n        # obj is a class or a metaclass\n\n        # First, let's see if it has an overloaded __call__ defined\n        # in its metaclass\n        call = _get_user_defined_method(type(obj), '__call__')\n        if call is not None:\n            sig = signature(call)\n        else:\n            # Now we check if the 'obj' class has a '__new__' method\n            new = _get_user_defined_method(obj, '__new__')\n            if new is not None:\n                sig = signature(new)\n            else:\n                # Finally, we should have at least __init__ implemented\n                init = _get_user_defined_method(obj, '__init__')\n                if init is not None:\n                    sig = signature(init)\n    elif not isinstance(obj, _NonUserDefinedCallables):\n        # An object with __call__\n        # We also check that the 'obj' is not an instance of\n        # _WrapperDescriptor or _MethodWrapper to avoid\n        # infinite recursion (and even potential segfault)\n        call = _get_user_defined_method(type(obj), '__call__', 'im_func')\n        if call is not None:\n            sig = signature(call)\n\n    if sig is not None:\n        # For classes and objects we skip the first parameter of their\n        # __call__, __new__, or __init__ methods\n        return sig.replace(parameters=tuple(sig.parameters.values())[1:])\n\n    if isinstance(obj, types.BuiltinFunctionType):\n        # Raise a nicer error message for builtins\n        msg = 'no signature found for builtin function {0!r}'.format(obj)\n        raise ValueError(msg)\n\n    raise ValueError(\n        'callable {0!r} is not supported by signature'.format(obj))", "response": "Get a signature object for the passed callable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef emd(a, b, M, numItermax=100000, log=False):\n\n    a = np.asarray(a, dtype=np.float64)\n    b = np.asarray(b, dtype=np.float64)\n    M = np.asarray(M, dtype=np.float64)\n\n    # if empty array given then use unifor distributions\n    if len(a) == 0:\n        a = np.ones((M.shape[0],), dtype=np.float64) / M.shape[0]\n    if len(b) == 0:\n        b = np.ones((M.shape[1],), dtype=np.float64) / M.shape[1]\n\n    G, cost, u, v, result_code = emd_c(a, b, M, numItermax)\n    result_code_string = check_result(result_code)\n    if log:\n        log = {}\n        log['cost'] = cost\n        log['u'] = u\n        log['v'] = v\n        log['warning'] = result_code_string\n        log['result_code'] = result_code\n        return G, log\n    return G", "response": "Solves the Earth Movers distance problem and returns the OT matrix of the optimal transportation matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef emd2(a, b, M, processes=multiprocessing.cpu_count(),\n         numItermax=100000, log=False, return_matrix=False):\n    \"\"\"Solves the Earth Movers distance problem and returns the loss\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F\n\n        s.t. \\gamma 1 = a\n             \\gamma^T 1= b\n             \\gamma\\geq 0\n    where :\n\n    - M is the metric cost matrix\n    - a and b are the sample weights\n\n    Uses the algorithm proposed in [1]_\n\n    Parameters\n    ----------\n    a : (ns,) ndarray, float64\n        Source histogram (uniform weigth if empty list)\n    b : (nt,) ndarray, float64\n        Target histogram (uniform weigth if empty list)\n    M : (ns,nt) ndarray, float64\n        loss matrix\n    numItermax : int, optional (default=100000)\n        The maximum number of iterations before stopping the optimization\n        algorithm if it has not converged.\n    log: boolean, optional (default=False)\n        If True, returns a dictionary containing the cost and dual\n        variables. Otherwise returns only the optimal transportation cost.\n    return_matrix: boolean, optional (default=False)\n        If True, returns the optimal transportation matrix in the log.\n\n    Returns\n    -------\n    gamma: (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    log: dict\n        If input log is true, a dictionary containing the cost and dual\n        variables and exit status\n\n\n    Examples\n    --------\n\n    Simple example with obvious solution. The function emd accepts lists and\n    perform automatic conversion to numpy arrays\n\n\n    >>> import ot\n    >>> a=[.5,.5]\n    >>> b=[.5,.5]\n    >>> M=[[0.,1.],[1.,0.]]\n    >>> ot.emd2(a,b,M)\n    0.0\n\n    References\n    ----------\n\n    .. [1] Bonneel, N., Van De Panne, M., Paris, S., & Heidrich, W.\n        (2011, December).  Displacement interpolation using Lagrangian mass\n        transport. In ACM Transactions on Graphics (TOG) (Vol. 30, No. 6, p.\n        158). ACM.\n\n    See Also\n    --------\n    ot.bregman.sinkhorn : Entropic regularized OT\n    ot.optim.cg : General regularized OT\"\"\"\n\n    a = np.asarray(a, dtype=np.float64)\n    b = np.asarray(b, dtype=np.float64)\n    M = np.asarray(M, dtype=np.float64)\n\n    # if empty array given then use unifor distributions\n    if len(a) == 0:\n        a = np.ones((M.shape[0],), dtype=np.float64) / M.shape[0]\n    if len(b) == 0:\n        b = np.ones((M.shape[1],), dtype=np.float64) / M.shape[1]\n\n    if log or return_matrix:\n        def f(b):\n            G, cost, u, v, resultCode = emd_c(a, b, M, numItermax)\n            result_code_string = check_result(resultCode)\n            log = {}\n            if return_matrix:\n                log['G'] = G\n            log['u'] = u\n            log['v'] = v\n            log['warning'] = result_code_string\n            log['result_code'] = resultCode\n            return [cost, log]\n    else:\n        def f(b):\n            G, cost, u, v, result_code = emd_c(a, b, M, numItermax)\n            check_result(result_code)\n            return cost\n\n    if len(b.shape) == 1:\n        return f(b)\n    nb = b.shape[1]\n\n    res = parmap(f, [b[:, i] for i in range(nb)], processes)\n    return res", "response": "Solves the Earth Movers distance problem and returns the loss matrix."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsolve the free support (locations of the barycenters are optimized, not the weights) Wasserstein barycenter problem (i.e. the weighted Frechet mean for the 2-Wasserstein distance) The function solves the Wasserstein barycenter problem when the barycenter measure is constrained to be supported on k atoms. This problem is considered in [1] (Algorithm 2). There are two differences with the following codes: - we do not optimize over the weights - we do not do line search for the locations updates, we use i.e. theta = 1 in [1] (Algorithm 2). This can be seen as a discrete implementation of the fixed-point algorithm of [2] proposed in the continuous setting. Parameters ---------- measures_locations : list of (k_i,d) np.ndarray The discrete support of a measure supported on k_i locations of a d-dimensional space (k_i can be different for each element of the list) measures_weights : list of (k_i,) np.ndarray Numpy arrays where each numpy array has k_i non-negatives values summing to one representing the weights of each discrete input measure X_init : (k,d) np.ndarray Initialization of the support locations (on k atoms) of the barycenter b : (k,) np.ndarray Initialization of the weights of the barycenter (non-negatives, sum to 1) weights : (k,) np.ndarray Initialization of the coefficients of the barycenter (non-negatives, sum to 1) numItermax : int, optional Max number of iterations stopThr : float, optional Stop threshol on error (>0) verbose : bool, optional Print information along iterations log : bool, optional record log if True Returns ------- X : (k,d) np.ndarray Support locations (on k atoms) of the barycenter References ---------- .. [1] Cuturi, Marco, and Arnaud Doucet. \"Fast computation of Wasserstein barycenters.\" International Conference on Machine Learning. 2014. .. [2] \u00c1lvarez-Esteban, Pedro C., et al. \"A fixed-point approach to barycenters in Wasserstein space.\" Journal of Mathematical Analysis and Applications 441.2 (2016): 744-762.", "response": "def free_support_barycenter(measures_locations, measures_weights, X_init, b=None, weights=None, numItermax=100, stopThr=1e-7, verbose=False, log=None):\n    \"\"\"\n    Solves the free support (locations of the barycenters are optimized, not the weights) Wasserstein barycenter problem (i.e. the weighted Frechet mean for the 2-Wasserstein distance)\n\n    The function solves the Wasserstein barycenter problem when the barycenter measure is constrained to be supported on k atoms.\n    This problem is considered in [1] (Algorithm 2). There are two differences with the following codes:\n    - we do not optimize over the weights\n    - we do not do line search for the locations updates, we use i.e. theta = 1 in [1] (Algorithm 2). This can be seen as a discrete implementation of the fixed-point algorithm of [2] proposed in the continuous setting.\n\n    Parameters\n    ----------\n    measures_locations : list of (k_i,d) np.ndarray\n        The discrete support of a measure supported on k_i locations of a d-dimensional space (k_i can be different for each element of the list)\n    measures_weights : list of (k_i,) np.ndarray\n        Numpy arrays where each numpy array has k_i non-negatives values summing to one representing the weights of each discrete input measure\n\n    X_init : (k,d) np.ndarray\n        Initialization of the support locations (on k atoms) of the barycenter\n    b : (k,) np.ndarray\n        Initialization of the weights of the barycenter (non-negatives, sum to 1)\n    weights : (k,) np.ndarray\n        Initialization of the coefficients of the barycenter (non-negatives, sum to 1)\n\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n    Returns\n    -------\n    X : (k,d) np.ndarray\n        Support locations (on k atoms) of the barycenter\n\n    References\n    ----------\n\n    .. [1] Cuturi, Marco, and Arnaud Doucet. \"Fast computation of Wasserstein barycenters.\" International Conference on Machine Learning. 2014.\n\n    .. [2]  \u00c1lvarez-Esteban, Pedro C., et al. \"A fixed-point approach to barycenters in Wasserstein space.\" Journal of Mathematical Analysis and Applications 441.2 (2016): 744-762.\n\n    \"\"\"\n\n    iter_count = 0\n\n    N = len(measures_locations)\n    k = X_init.shape[0]\n    d = X_init.shape[1]\n    if b is None:\n        b = np.ones((k,))/k\n    if weights is None:\n        weights = np.ones((N,)) / N\n\n    X = X_init\n\n    log_dict = {}\n    displacement_square_norms = []\n\n    displacement_square_norm = stopThr + 1.\n\n    while ( displacement_square_norm > stopThr and iter_count < numItermax ):\n\n        T_sum = np.zeros((k, d))\n\n        for (measure_locations_i, measure_weights_i, weight_i) in zip(measures_locations, measures_weights, weights.tolist()):\n\n            M_i = dist(X, measure_locations_i)\n            T_i = emd(b, measure_weights_i, M_i)\n            T_sum = T_sum + weight_i * np.reshape(1. / b, (-1, 1)) * np.matmul(T_i, measure_locations_i)\n\n        displacement_square_norm = np.sum(np.square(T_sum-X))\n        if log:\n            displacement_square_norms.append(displacement_square_norm)\n\n        X = T_sum\n\n        if verbose:\n            print('iteration %d, displacement_square_norm=%f\\n', iter_count, displacement_square_norm)\n\n        iter_count += 1\n\n    if log:\n        log_dict['displacement_square_norms'] = displacement_square_norms\n        return X, log_dict\n    else:\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef joint_OT_mapping_linear(xs, xt, mu=1, eta=0.001, bias=False, verbose=False,\n                            verbose2=False, numItermax=100, numInnerItermax=10,\n                            stopInnerThr=1e-6, stopThr=1e-5, log=False,\n                            **kwargs):\n    \"\"\"Joint OT and linear mapping estimation as proposed in [8]\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\min_{\\gamma,L}\\quad \\|L(X_s) -n_s\\gamma X_t\\|^2_F +\n          \\mu<\\gamma,M>_F + \\eta  \\|L -I\\|^2_F\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - M is the (ns,nt) squared euclidean cost matrix between samples in\n       Xs and Xt (scaled by ns)\n    - :math:`L` is a dxd linear operator that approximates the barycentric\n      mapping\n    - :math:`I` is the identity matrix (neutral linear mapping)\n    - a and b are uniform source and target weights\n\n    The problem consist in solving jointly an optimal transport matrix\n    :math:`\\gamma` and a linear mapping that fits the barycentric mapping\n    :math:`n_s\\gamma X_t`.\n\n    One can also estimate a mapping with constant bias (see supplementary\n    material of [8]) using the bias optional argument.\n\n    The algorithm used for solving the problem is the block coordinate\n    descent that alternates between updates of G (using conditionnal gradient)\n    and the update of L using a classical least square solver.\n\n\n    Parameters\n    ----------\n    xs : np.ndarray (ns,d)\n        samples in the source domain\n    xt : np.ndarray (nt,d)\n        samples in the target domain\n    mu : float,optional\n        Weight for the linear OT loss (>0)\n    eta : float, optional\n        Regularization term  for the linear mapping L (>0)\n    bias : bool,optional\n        Estimate linear mapping with constant bias\n    numItermax : int, optional\n        Max number of BCD iterations\n    stopThr : float, optional\n        Stop threshold on relative loss decrease (>0)\n    numInnerItermax : int, optional\n        Max number of iterations (inner CG solver)\n    stopInnerThr : float, optional\n        Stop threshold on error (inner CG solver) (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    L : (d x d) ndarray\n        Linear mapping matrix (d+1 x d if bias)\n    log : dict\n        log dictionary return only if log==True in parameters\n\n\n    References\n    ----------\n\n    .. [8] M. Perrot, N. Courty, R. Flamary, A. Habrard,\n        \"Mapping estimation for discrete optimal transport\",\n        Neural Information Processing Systems (NIPS), 2016.\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized OT\n    ot.optim.cg : General regularized OT\n\n    \"\"\"\n\n    ns, nt, d = xs.shape[0], xt.shape[0], xt.shape[1]\n\n    if bias:\n        xs1 = np.hstack((xs, np.ones((ns, 1))))\n        xstxs = xs1.T.dot(xs1)\n        Id = np.eye(d + 1)\n        Id[-1] = 0\n        I0 = Id[:, :-1]\n\n        def sel(x):\n            return x[:-1, :]\n    else:\n        xs1 = xs\n        xstxs = xs1.T.dot(xs1)\n        Id = np.eye(d)\n        I0 = Id\n\n        def sel(x):\n            return x\n\n    if log:\n        log = {'err': []}\n\n    a, b = unif(ns), unif(nt)\n    M = dist(xs, xt) * ns\n    G = emd(a, b, M)\n\n    vloss = []\n\n    def loss(L, G):\n        \"\"\"Compute full loss\"\"\"\n        return np.sum((xs1.dot(L) - ns * G.dot(xt))**2) + mu * \\\n            np.sum(G * M) + eta * np.sum(sel(L - I0)**2)\n\n    def solve_L(G):\n        \"\"\" solve L problem with fixed G (least square)\"\"\"\n        xst = ns * G.dot(xt)\n        return np.linalg.solve(xstxs + eta * Id, xs1.T.dot(xst) + eta * I0)\n\n    def solve_G(L, G0):\n        \"\"\"Update G with CG algorithm\"\"\"\n        xsi = xs1.dot(L)\n\n        def f(G):\n            return np.sum((xsi - ns * G.dot(xt))**2)\n\n        def df(G):\n            return -2 * ns * (xsi - ns * G.dot(xt)).dot(xt.T)\n        G = cg(a, b, M, 1.0 / mu, f, df, G0=G0,\n               numItermax=numInnerItermax, stopThr=stopInnerThr)\n        return G\n\n    L = solve_L(G)\n\n    vloss.append(loss(L, G))\n\n    if verbose:\n        print('{:5s}|{:12s}|{:8s}'.format(\n            'It.', 'Loss', 'Delta loss') + '\\n' + '-' * 32)\n        print('{:5d}|{:8e}|{:8e}'.format(0, vloss[-1], 0))\n\n    # init loop\n    if numItermax > 0:\n        loop = 1\n    else:\n        loop = 0\n    it = 0\n\n    while loop:\n\n        it += 1\n\n        # update G\n        G = solve_G(L, G)\n\n        # update L\n        L = solve_L(G)\n\n        vloss.append(loss(L, G))\n\n        if it >= numItermax:\n            loop = 0\n\n        if abs(vloss[-1] - vloss[-2]) / abs(vloss[-2]) < stopThr:\n            loop = 0\n\n        if verbose:\n            if it % 20 == 0:\n                print('{:5s}|{:12s}|{:8s}'.format(\n                    'It.', 'Loss', 'Delta loss') + '\\n' + '-' * 32)\n            print('{:5d}|{:8e}|{:8e}'.format(\n                it, vloss[-1], (vloss[-1] - vloss[-2]) / abs(vloss[-2])))\n    if log:\n        log['loss'] = vloss\n        return G, L, log\n    else:\n        return G, L", "response": "Jointly a linear mapping estimation as proposed in [ 8 ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\njointing OT and nonlinear mapping estimation with kernels as proposed in [8] The function solves the following optimization problem: .. math:: \\min_{\\gamma,L\\in\\mathcal{H}}\\quad \\|L(X_s) - n_s\\gamma X_t\\|^2_F + \\mu<\\gamma,M>_F + \\eta \\|L\\|^2_\\mathcal{H} s.t. \\gamma 1 = a \\gamma^T 1= b \\gamma\\geq 0 where : - M is the (ns,nt) squared euclidean cost matrix between samples in Xs and Xt (scaled by ns) - :math:`L` is a ns x d linear operator on a kernel matrix that approximates the barycentric mapping - a and b are uniform source and target weights The problem consist in solving jointly an optimal transport matrix :math:`\\gamma` and the nonlinear mapping that fits the barycentric mapping :math:`n_s\\gamma X_t`. One can also estimate a mapping with constant bias (see supplementary material of [8]) using the bias optional argument. The algorithm used for solving the problem is the block coordinate descent that alternates between updates of G (using conditionnal gradient) and the update of L using a classical kernel least square solver. Parameters ---------- xs : np.ndarray (ns,d) samples in the source domain xt : np.ndarray (nt,d) samples in the target domain mu : float,optional Weight for the linear OT loss (>0) eta : float, optional Regularization term for the linear mapping L (>0) bias : bool,optional Estimate linear mapping with constant bias kerneltype : str,optional kernel used by calling function ot.utils.kernel (gaussian by default) sigma : float, optional Gaussian kernel bandwidth. numItermax : int, optional Max number of BCD iterations stopThr : float, optional Stop threshold on relative loss decrease (>0) numInnerItermax : int, optional Max number of iterations (inner CG solver) stopInnerThr : float, optional Stop threshold on error (inner CG solver) (>0) verbose : bool, optional Print information along iterations log : bool, optional record log if True Returns ------- gamma : (ns x nt) ndarray Optimal transportation matrix for the given parameters L : (ns x d) ndarray Nonlinear mapping matrix (ns+1 x d if bias) log : dict log dictionary return only if log==True in parameters References ---------- .. [8] M. Perrot, N. Courty, R. Flamary, A. Habrard, \"Mapping estimation for discrete optimal transport\", Neural Information Processing Systems (NIPS), 2016. See Also -------- ot.lp.emd : Unregularized OT ot.optim.cg : General regularized OT", "response": "def joint_OT_mapping_kernel(xs, xt, mu=1, eta=0.001, kerneltype='gaussian',\n                            sigma=1, bias=False, verbose=False, verbose2=False,\n                            numItermax=100, numInnerItermax=10,\n                            stopInnerThr=1e-6, stopThr=1e-5, log=False,\n                            **kwargs):\n    \"\"\"Joint OT and nonlinear mapping estimation with kernels as proposed in [8]\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\min_{\\gamma,L\\in\\mathcal{H}}\\quad \\|L(X_s) -\n        n_s\\gamma X_t\\|^2_F + \\mu<\\gamma,M>_F + \\eta  \\|L\\|^2_\\mathcal{H}\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - M is the (ns,nt) squared euclidean cost matrix between samples in\n      Xs and Xt (scaled by ns)\n    - :math:`L` is a ns x d linear operator on a kernel matrix that\n      approximates the barycentric mapping\n    - a and b are uniform source and target weights\n\n    The problem consist in solving jointly an optimal transport matrix\n    :math:`\\gamma` and the nonlinear mapping that fits the barycentric mapping\n    :math:`n_s\\gamma X_t`.\n\n    One can also estimate a mapping with constant bias (see supplementary\n    material of [8]) using the bias optional argument.\n\n    The algorithm used for solving the problem is the block coordinate\n    descent that alternates between updates of G (using conditionnal gradient)\n    and the update of L using a classical kernel least square solver.\n\n\n    Parameters\n    ----------\n    xs : np.ndarray (ns,d)\n        samples in the source domain\n    xt : np.ndarray (nt,d)\n        samples in the target domain\n    mu : float,optional\n        Weight for the linear OT loss (>0)\n    eta : float, optional\n        Regularization term  for the linear mapping L (>0)\n    bias : bool,optional\n        Estimate linear mapping with constant bias\n    kerneltype : str,optional\n        kernel used by calling function ot.utils.kernel (gaussian by default)\n    sigma : float, optional\n        Gaussian kernel bandwidth.\n    numItermax : int, optional\n        Max number of BCD iterations\n    stopThr : float, optional\n        Stop threshold on relative loss decrease (>0)\n    numInnerItermax : int, optional\n        Max number of iterations (inner CG solver)\n    stopInnerThr : float, optional\n        Stop threshold on error (inner CG solver) (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    L : (ns x d) ndarray\n        Nonlinear mapping matrix (ns+1 x d if bias)\n    log : dict\n        log dictionary return only if log==True in parameters\n\n\n    References\n    ----------\n\n    .. [8] M. Perrot, N. Courty, R. Flamary, A. Habrard,\n       \"Mapping estimation for discrete optimal transport\",\n       Neural Information Processing Systems (NIPS), 2016.\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized OT\n    ot.optim.cg : General regularized OT\n\n    \"\"\"\n\n    ns, nt = xs.shape[0], xt.shape[0]\n\n    K = kernel(xs, xs, method=kerneltype, sigma=sigma)\n    if bias:\n        K1 = np.hstack((K, np.ones((ns, 1))))\n        Id = np.eye(ns + 1)\n        Id[-1] = 0\n        Kp = np.eye(ns + 1)\n        Kp[:ns, :ns] = K\n\n        # ls regu\n        # K0 = K1.T.dot(K1)+eta*I\n        # Kreg=I\n\n        # RKHS regul\n        K0 = K1.T.dot(K1) + eta * Kp\n        Kreg = Kp\n\n    else:\n        K1 = K\n        Id = np.eye(ns)\n\n        # ls regul\n        # K0 = K1.T.dot(K1)+eta*I\n        # Kreg=I\n\n        # proper kernel ridge\n        K0 = K + eta * Id\n        Kreg = K\n\n    if log:\n        log = {'err': []}\n\n    a, b = unif(ns), unif(nt)\n    M = dist(xs, xt) * ns\n    G = emd(a, b, M)\n\n    vloss = []\n\n    def loss(L, G):\n        \"\"\"Compute full loss\"\"\"\n        return np.sum((K1.dot(L) - ns * G.dot(xt))**2) + mu * \\\n            np.sum(G * M) + eta * np.trace(L.T.dot(Kreg).dot(L))\n\n    def solve_L_nobias(G):\n        \"\"\" solve L problem with fixed G (least square)\"\"\"\n        xst = ns * G.dot(xt)\n        return np.linalg.solve(K0, xst)\n\n    def solve_L_bias(G):\n        \"\"\" solve L problem with fixed G (least square)\"\"\"\n        xst = ns * G.dot(xt)\n        return np.linalg.solve(K0, K1.T.dot(xst))\n\n    def solve_G(L, G0):\n        \"\"\"Update G with CG algorithm\"\"\"\n        xsi = K1.dot(L)\n\n        def f(G):\n            return np.sum((xsi - ns * G.dot(xt))**2)\n\n        def df(G):\n            return -2 * ns * (xsi - ns * G.dot(xt)).dot(xt.T)\n        G = cg(a, b, M, 1.0 / mu, f, df, G0=G0,\n               numItermax=numInnerItermax, stopThr=stopInnerThr)\n        return G\n\n    if bias:\n        solve_L = solve_L_bias\n    else:\n        solve_L = solve_L_nobias\n\n    L = solve_L(G)\n\n    vloss.append(loss(L, G))\n\n    if verbose:\n        print('{:5s}|{:12s}|{:8s}'.format(\n            'It.', 'Loss', 'Delta loss') + '\\n' + '-' * 32)\n        print('{:5d}|{:8e}|{:8e}'.format(0, vloss[-1], 0))\n\n    # init loop\n    if numItermax > 0:\n        loop = 1\n    else:\n        loop = 0\n    it = 0\n\n    while loop:\n\n        it += 1\n\n        # update G\n        G = solve_G(L, G)\n\n        # update L\n        L = solve_L(G)\n\n        vloss.append(loss(L, G))\n\n        if it >= numItermax:\n            loop = 0\n\n        if abs(vloss[-1] - vloss[-2]) / abs(vloss[-2]) < stopThr:\n            loop = 0\n\n        if verbose:\n            if it % 20 == 0:\n                print('{:5s}|{:12s}|{:8s}'.format(\n                    'It.', 'Loss', 'Delta loss') + '\\n' + '-' * 32)\n            print('{:5d}|{:8e}|{:8e}'.format(\n                it, vloss[-1], (vloss[-1] - vloss[-2]) / abs(vloss[-2])))\n    if log:\n        log['loss'] = vloss\n        return G, L, log\n    else:\n        return G, L"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn OT linear operator between samples The function estimates the optimal linear operator that aligns the two empirical distributions. This is equivalent to estimating the closed form mapping between two Gaussian distributions :math:`N(\\mu_s,\\Sigma_s)` and :math:`N(\\mu_t,\\Sigma_t)` as proposed in [14] and discussed in remark 2.29 in [15]. The linear operator from source to target :math:`M` .. math:: M(x)=Ax+b where : .. math:: A=\\Sigma_s^{-1/2}(\\Sigma_s^{1/2}\\Sigma_t\\Sigma_s^{1/2})^{1/2} \\Sigma_s^{-1/2} .. math:: b=\\mu_t-A\\mu_s Parameters ---------- xs : np.ndarray (ns,d) samples in the source domain xt : np.ndarray (nt,d) samples in the target domain reg : float,optional regularization added to the diagonals of convariances (>0) ws : np.ndarray (ns,1), optional weights for the source samples wt : np.ndarray (ns,1), optional weights for the target samples bias: boolean, optional estimate bias b else b=0 (default:True) log : bool, optional record log if True Returns ------- A : (d x d) ndarray Linear operator b : (1 x d) ndarray bias log : dict log dictionary return only if log==True in parameters References ---------- .. [14] Knott, M. and Smith, C. S. \"On the optimal mapping of distributions\", Journal of Optimization Theory and Applications Vol 43, 1984 .. [15] Peyr\u00e9, G., & Cuturi, M. (2017). \"Computational Optimal Transport\", 2018.", "response": "def OT_mapping_linear(xs, xt, reg=1e-6, ws=None,\n                      wt=None, bias=True, log=False):\n    \"\"\" return OT linear operator between samples\n\n    The function estimates the optimal linear operator that aligns the two\n    empirical distributions. This is equivalent to estimating the closed\n    form mapping between two Gaussian distributions :math:`N(\\mu_s,\\Sigma_s)`\n    and :math:`N(\\mu_t,\\Sigma_t)` as proposed in [14] and discussed in remark 2.29 in [15].\n\n    The linear operator from source to target :math:`M`\n\n    .. math::\n        M(x)=Ax+b\n\n    where :\n\n    .. math::\n        A=\\Sigma_s^{-1/2}(\\Sigma_s^{1/2}\\Sigma_t\\Sigma_s^{1/2})^{1/2}\n        \\Sigma_s^{-1/2}\n    .. math::\n        b=\\mu_t-A\\mu_s\n\n    Parameters\n    ----------\n    xs : np.ndarray (ns,d)\n        samples in the source domain\n    xt : np.ndarray (nt,d)\n        samples in the target domain\n    reg : float,optional\n        regularization added to the diagonals of convariances (>0)\n    ws : np.ndarray (ns,1), optional\n        weights for the source samples\n    wt : np.ndarray (ns,1), optional\n        weights for the target samples\n    bias: boolean, optional\n        estimate bias b else b=0 (default:True)\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    A : (d x d) ndarray\n        Linear operator\n    b : (1 x d) ndarray\n        bias\n    log : dict\n        log dictionary return only if log==True in parameters\n\n\n    References\n    ----------\n\n    .. [14] Knott, M. and Smith, C. S. \"On the optimal mapping of\n        distributions\", Journal of Optimization Theory and Applications\n        Vol 43, 1984\n\n    .. [15]  Peyr\u00e9, G., & Cuturi, M. (2017). \"Computational Optimal\n        Transport\", 2018.\n\n\n    \"\"\"\n\n    d = xs.shape[1]\n\n    if bias:\n        mxs = xs.mean(0, keepdims=True)\n        mxt = xt.mean(0, keepdims=True)\n\n        xs = xs - mxs\n        xt = xt - mxt\n    else:\n        mxs = np.zeros((1, d))\n        mxt = np.zeros((1, d))\n\n    if ws is None:\n        ws = np.ones((xs.shape[0], 1)) / xs.shape[0]\n\n    if wt is None:\n        wt = np.ones((xt.shape[0], 1)) / xt.shape[0]\n\n    Cs = (xs * ws).T.dot(xs) / ws.sum() + reg * np.eye(d)\n    Ct = (xt * wt).T.dot(xt) / wt.sum() + reg * np.eye(d)\n\n    Cs12 = linalg.sqrtm(Cs)\n    Cs_12 = linalg.inv(Cs12)\n\n    M0 = linalg.sqrtm(Cs12.dot(Ct.dot(Cs12)))\n\n    A = Cs_12.dot(M0.dot(Cs_12))\n\n    b = mxt - mxs.dot(A)\n\n    if log:\n        log = {}\n        log['Cs'] = Cs\n        log['Ct'] = Ct\n        log['Cs12'] = Cs12\n        log['Cs_12'] = Cs_12\n        return A, b, log\n    else:\n        return A, b"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfit the coupling matrix from source and target sets of samples Xs and ys and Xt and yt.", "response": "def fit(self, Xs=None, ys=None, Xt=None, yt=None):\n        \"\"\"Build a coupling matrix from source and target sets of samples\n        (Xs, ys) and (Xt, yt)\n\n        Parameters\n        ----------\n        Xs : array-like, shape (n_source_samples, n_features)\n            The training input samples.\n        ys : array-like, shape (n_source_samples,)\n            The class labels\n        Xt : array-like, shape (n_target_samples, n_features)\n            The training input samples.\n        yt : array-like, shape (n_target_samples,)\n            The class labels. If some target samples are unlabeled, fill the\n            yt's elements with -1.\n\n            Warning: Note that, due to this convention -1 cannot be used as a\n            class label\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n\n        # check the necessary inputs parameters are here\n        if check_params(Xs=Xs, Xt=Xt):\n\n            # pairwise distance\n            self.cost_ = dist(Xs, Xt, metric=self.metric)\n            self.cost_ = cost_normalization(self.cost_, self.norm)\n\n            if (ys is not None) and (yt is not None):\n\n                if self.limit_max != np.infty:\n                    self.limit_max = self.limit_max * np.max(self.cost_)\n\n                # assumes labeled source samples occupy the first rows\n                # and labeled target samples occupy the first columns\n                classes = [c for c in np.unique(ys) if c != -1]\n                for c in classes:\n                    idx_s = np.where((ys != c) & (ys != -1))\n                    idx_t = np.where(yt == c)\n\n                    # all the coefficients corresponding to a source sample\n                    # and a target sample :\n                    # with different labels get a infinite\n                    for j in idx_t[0]:\n                        self.cost_[idx_s[0], j] = self.limit_max\n\n            # distribution estimation\n            self.mu_s = self.distribution_estimation(Xs)\n            self.mu_t = self.distribution_estimation(Xt)\n\n            # store arrays of samples\n            self.xs_ = Xs\n            self.xt_ = Xt\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transform(self, Xs=None, ys=None, Xt=None, yt=None, batch_size=128):\n\n        # check the necessary inputs parameters are here\n        if check_params(Xs=Xs):\n\n            if np.array_equal(self.xs_, Xs):\n\n                # perform standard barycentric mapping\n                transp = self.coupling_ / np.sum(self.coupling_, 1)[:, None]\n\n                # set nans to 0\n                transp[~ np.isfinite(transp)] = 0\n\n                # compute transported samples\n                transp_Xs = np.dot(transp, self.xt_)\n            else:\n                # perform out of sample mapping\n                indices = np.arange(Xs.shape[0])\n                batch_ind = [\n                    indices[i:i + batch_size]\n                    for i in range(0, len(indices), batch_size)]\n\n                transp_Xs = []\n                for bi in batch_ind:\n\n                    # get the nearest neighbor in the source domain\n                    D0 = dist(Xs[bi], self.xs_)\n                    idx = np.argmin(D0, axis=1)\n\n                    # transport the source samples\n                    transp = self.coupling_ / np.sum(\n                        self.coupling_, 1)[:, None]\n                    transp[~ np.isfinite(transp)] = 0\n                    transp_Xs_ = np.dot(transp, self.xt_)\n\n                    # define the transported points\n                    transp_Xs_ = transp_Xs_[idx, :] + Xs[bi] - self.xs_[idx, :]\n\n                    transp_Xs.append(transp_Xs_)\n\n                transp_Xs = np.concatenate(transp_Xs, axis=0)\n\n            return transp_Xs", "response": "This function transforms source samples Xs onto target ones Xt and yt."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inverse_transform(self, Xs=None, ys=None, Xt=None, yt=None,\n                          batch_size=128):\n        \"\"\"Transports target samples Xt onto target samples Xs\n\n        Parameters\n        ----------\n        Xs : array-like, shape (n_source_samples, n_features)\n            The training input samples.\n        ys : array-like, shape (n_source_samples,)\n            The class labels\n        Xt : array-like, shape (n_target_samples, n_features)\n            The training input samples.\n        yt : array-like, shape (n_target_samples,)\n            The class labels. If some target samples are unlabeled, fill the\n            yt's elements with -1.\n\n            Warning: Note that, due to this convention -1 cannot be used as a\n            class label\n        batch_size : int, optional (default=128)\n            The batch size for out of sample inverse transform\n\n        Returns\n        -------\n        transp_Xt : array-like, shape (n_source_samples, n_features)\n            The transported target samples.\n        \"\"\"\n\n        # check the necessary inputs parameters are here\n        if check_params(Xt=Xt):\n\n            if np.array_equal(self.xt_, Xt):\n\n                # perform standard barycentric mapping\n                transp_ = self.coupling_.T / np.sum(self.coupling_, 0)[:, None]\n\n                # set nans to 0\n                transp_[~ np.isfinite(transp_)] = 0\n\n                # compute transported samples\n                transp_Xt = np.dot(transp_, self.xs_)\n            else:\n                # perform out of sample mapping\n                indices = np.arange(Xt.shape[0])\n                batch_ind = [\n                    indices[i:i + batch_size]\n                    for i in range(0, len(indices), batch_size)]\n\n                transp_Xt = []\n                for bi in batch_ind:\n\n                    D0 = dist(Xt[bi], self.xt_)\n                    idx = np.argmin(D0, axis=1)\n\n                    # transport the target samples\n                    transp_ = self.coupling_.T / np.sum(\n                        self.coupling_, 0)[:, None]\n                    transp_[~ np.isfinite(transp_)] = 0\n                    transp_Xt_ = np.dot(transp_, self.xs_)\n\n                    # define the transported points\n                    transp_Xt_ = transp_Xt_[idx, :] + Xt[bi] - self.xt_[idx, :]\n\n                    transp_Xt.append(transp_Xt_)\n\n                transp_Xt = np.concatenate(transp_Xt, axis=0)\n\n            return transp_Xt", "response": "This function transforms target samples Xs onto target samples Xt onto target samples yt."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfit the coupling matrix from source set of samples ys and Xt and yt.", "response": "def fit(self, Xs=None, ys=None, Xt=None, yt=None):\n        \"\"\"Build a coupling matrix from source and target sets of samples\n        (Xs, ys) and (Xt, yt)\n\n        Parameters\n        ----------\n        Xs : array-like, shape (n_source_samples, n_features)\n            The training input samples.\n        ys : array-like, shape (n_source_samples,)\n            The class labels\n        Xt : array-like, shape (n_target_samples, n_features)\n            The training input samples.\n        yt : array-like, shape (n_target_samples,)\n            The class labels. If some target samples are unlabeled, fill the\n            yt's elements with -1.\n\n            Warning: Note that, due to this convention -1 cannot be used as a\n            class label\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n\n        self.mu_s = self.distribution_estimation(Xs)\n        self.mu_t = self.distribution_estimation(Xt)\n\n        # coupling estimation\n        returned_ = OT_mapping_linear(Xs, Xt, reg=self.reg,\n                                      ws=self.mu_s.reshape((-1, 1)),\n                                      wt=self.mu_t.reshape((-1, 1)),\n                                      bias=self.bias, log=self.log)\n\n        # deal with the value of log\n        if self.log:\n            self.A_, self.B_, self.log_ = returned_\n        else:\n            self.A_, self.B_, = returned_\n            self.log_ = dict()\n\n        # re compute inverse mapping\n        self.A1_ = linalg.inv(self.A_)\n        self.B1_ = -self.B_.dot(self.A1_)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninverse transform of target samples Xs onto target samples Xt onto target samples yt.", "response": "def inverse_transform(self, Xs=None, ys=None, Xt=None, yt=None,\n                          batch_size=128):\n        \"\"\"Transports target samples Xt onto target samples Xs\n\n        Parameters\n        ----------\n        Xs : array-like, shape (n_source_samples, n_features)\n            The training input samples.\n        ys : array-like, shape (n_source_samples,)\n            The class labels\n        Xt : array-like, shape (n_target_samples, n_features)\n            The training input samples.\n        yt : array-like, shape (n_target_samples,)\n            The class labels. If some target samples are unlabeled, fill the\n            yt's elements with -1.\n\n            Warning: Note that, due to this convention -1 cannot be used as a\n            class label\n        batch_size : int, optional (default=128)\n            The batch size for out of sample inverse transform\n\n        Returns\n        -------\n        transp_Xt : array-like, shape (n_source_samples, n_features)\n            The transported target samples.\n        \"\"\"\n\n        # check the necessary inputs parameters are here\n        if check_params(Xt=Xt):\n\n            transp_Xt = Xt.dot(self.A1_) + self.B1_\n\n            return transp_Xt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self, Xs=None, ys=None, Xt=None, yt=None):\n\n        super(SinkhornTransport, self).fit(Xs, ys, Xt, yt)\n\n        # coupling estimation\n        returned_ = sinkhorn(\n            a=self.mu_s, b=self.mu_t, M=self.cost_, reg=self.reg_e,\n            numItermax=self.max_iter, stopThr=self.tol,\n            verbose=self.verbose, log=self.log)\n\n        # deal with the value of log\n        if self.log:\n            self.coupling_, self.log_ = returned_\n        else:\n            self.coupling_ = returned_\n            self.log_ = dict()\n\n        return self", "response": "Fit the sinkhorn algorithm to the set of samples and compute the coupling matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfit the EMD algorithm to the set of samples and compute the coupling matrix.", "response": "def fit(self, Xs, ys=None, Xt=None, yt=None):\n        \"\"\"Build a coupling matrix from source and target sets of samples\n        (Xs, ys) and (Xt, yt)\n\n        Parameters\n        ----------\n        Xs : array-like, shape (n_source_samples, n_features)\n            The training input samples.\n        ys : array-like, shape (n_source_samples,)\n            The class labels\n        Xt : array-like, shape (n_target_samples, n_features)\n            The training input samples.\n        yt : array-like, shape (n_target_samples,)\n            The class labels. If some target samples are unlabeled, fill the\n            yt's elements with -1.\n\n            Warning: Note that, due to this convention -1 cannot be used as a\n            class label\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n\n        super(EMDTransport, self).fit(Xs, ys, Xt, yt)\n\n        returned_ = emd(\n            a=self.mu_s, b=self.mu_t, M=self.cost_, numItermax=self.max_iter,\n            log=self.log)\n\n        # coupling estimation\n        if self.log:\n            self.coupling_, self.log_ = returned_\n        else:\n            self.coupling_ = returned_\n            self.log_ = dict()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit(self, Xs, ys=None, Xt=None, yt=None):\n\n        # check the necessary inputs parameters are here\n        if check_params(Xs=Xs, Xt=Xt, ys=ys):\n\n            super(SinkhornL1l2Transport, self).fit(Xs, ys, Xt, yt)\n\n            returned_ = sinkhorn_l1l2_gl(\n                a=self.mu_s, labels_a=ys, b=self.mu_t, M=self.cost_,\n                reg=self.reg_e, eta=self.reg_cl, numItermax=self.max_iter,\n                numInnerItermax=self.max_inner_iter, stopInnerThr=self.tol,\n                verbose=self.verbose, log=self.log)\n\n            # deal with the value of log\n            if self.log:\n                self.coupling_, self.log_ = returned_\n            else:\n                self.coupling_ = returned_\n                self.log_ = dict()\n\n        return self", "response": "Fits the sinkhorn_l1l2_gl method to build a coupling matrix from source and target sets of samples Xs and ys and Xt and yt."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fit(self, Xs=None, ys=None, Xt=None, yt=None):\n\n        # check the necessary inputs parameters are here\n        if check_params(Xs=Xs, Xt=Xt):\n\n            self.xs_ = Xs\n            self.xt_ = Xt\n\n            if self.kernel == \"linear\":\n                returned_ = joint_OT_mapping_linear(\n                    Xs, Xt, mu=self.mu, eta=self.eta, bias=self.bias,\n                    verbose=self.verbose, verbose2=self.verbose2,\n                    numItermax=self.max_iter,\n                    numInnerItermax=self.max_inner_iter, stopThr=self.tol,\n                    stopInnerThr=self.inner_tol, log=self.log)\n\n            elif self.kernel == \"gaussian\":\n                returned_ = joint_OT_mapping_kernel(\n                    Xs, Xt, mu=self.mu, eta=self.eta, bias=self.bias,\n                    sigma=self.sigma, verbose=self.verbose,\n                    verbose2=self.verbose, numItermax=self.max_iter,\n                    numInnerItermax=self.max_inner_iter,\n                    stopInnerThr=self.inner_tol, stopThr=self.tol,\n                    log=self.log)\n\n            # deal with the value of log\n            if self.log:\n                self.coupling_, self.mapping_, self.log_ = returned_\n            else:\n                self.coupling_, self.mapping_ = returned_\n                self.log_ = dict()\n\n        return self", "response": "Fits the optimal coupling and estimates the associated mapping\n            to the source set of samples ys and Xt."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform(self, Xs):\n\n        # check the necessary inputs parameters are here\n        if check_params(Xs=Xs):\n\n            if np.array_equal(self.xs_, Xs):\n                # perform standard barycentric mapping\n                transp = self.coupling_ / np.sum(self.coupling_, 1)[:, None]\n\n                # set nans to 0\n                transp[~ np.isfinite(transp)] = 0\n\n                # compute transported samples\n                transp_Xs = np.dot(transp, self.xt_)\n            else:\n                if self.kernel == \"gaussian\":\n                    K = kernel(Xs, self.xs_, method=self.kernel,\n                               sigma=self.sigma)\n                elif self.kernel == \"linear\":\n                    K = Xs\n                if self.bias:\n                    K = np.hstack((K, np.ones((Xs.shape[0], 1))))\n                transp_Xs = K.dot(self.mapping_)\n\n            return transp_Xs", "response": "Transports source samples Xs onto target ones Xt\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef euclidean_distances(a, b, squared=False, to_numpy=True):\n\n    a, b = to_gpu(a, b)\n\n    a2 = np.sum(np.square(a), 1)\n    b2 = np.sum(np.square(b), 1)\n\n    c = -2 * np.dot(a, b.T)\n    c += a2[:, None]\n    c += b2[None, :]\n\n    if not squared:\n        np.sqrt(c, out=c)\n    if to_numpy:\n        return to_np(c)\n    else:\n        return c", "response": "Compute the pairwise euclidean distance between two matrices a and b."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dist(x1, x2=None, metric='sqeuclidean', to_numpy=True):\n    if x2 is None:\n        x2 = x1\n    if metric == \"sqeuclidean\":\n        return euclidean_distances(x1, x2, squared=True, to_numpy=to_numpy)\n    elif metric == \"euclidean\":\n        return euclidean_distances(x1, x2, squared=False, to_numpy=to_numpy)\n    else:\n        raise NotImplementedError", "response": "Compute distance between samples in x1 and x2 on gpu\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading numpy arrays to GPU and return them", "response": "def to_gpu(*args):\n    \"\"\" Upload numpy arrays to GPU and return them\"\"\"\n    if len(args) > 1:\n        return (cp.asarray(x) for x in args)\n    else:\n        return cp.asarray(args[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_np(*args):\n    if len(args) > 1:\n        return (cp.asnumpy(x) for x in args)\n    else:\n        return cp.asnumpy(args[0])", "response": "convert GPU arras to numpy and return them"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef barycenter(A, M, weights=None, verbose=False, log=False, solver='interior-point'):\n\n    if weights is None:\n        weights = np.ones(A.shape[1]) / A.shape[1]\n    else:\n        assert(len(weights) == A.shape[1])\n\n    n_distributions = A.shape[1]\n    n = A.shape[0]\n\n    n2 = n * n\n    c = np.zeros((0))\n    b_eq1 = np.zeros((0))\n    for i in range(n_distributions):\n        c = np.concatenate((c, M.ravel() * weights[i]))\n        b_eq1 = np.concatenate((b_eq1, A[:, i]))\n    c = np.concatenate((c, np.zeros(n)))\n\n    lst_idiag1 = [sps.kron(sps.eye(n), np.ones((1, n))) for i in range(n_distributions)]\n    #  row constraints\n    A_eq1 = sps.hstack((sps.block_diag(lst_idiag1), sps.coo_matrix((n_distributions * n, n))))\n\n    # columns constraints\n    lst_idiag2 = []\n    lst_eye = []\n    for i in range(n_distributions):\n        if i == 0:\n            lst_idiag2.append(sps.kron(np.ones((1, n)), sps.eye(n)))\n            lst_eye.append(-sps.eye(n))\n        else:\n            lst_idiag2.append(sps.kron(np.ones((1, n)), sps.eye(n - 1, n)))\n            lst_eye.append(-sps.eye(n - 1, n))\n\n    A_eq2 = sps.hstack((sps.block_diag(lst_idiag2), sps.vstack(lst_eye)))\n    b_eq2 = np.zeros((A_eq2.shape[0]))\n\n    # full problem\n    A_eq = sps.vstack((A_eq1, A_eq2))\n    b_eq = np.concatenate((b_eq1, b_eq2))\n\n    if not cvxopt or solver in ['interior-point']:\n        # cvxopt not installed or interior point\n\n        if solver is None:\n            solver = 'interior-point'\n\n        options = {'sparse': True, 'disp': verbose}\n        sol = sp.optimize.linprog(c, A_eq=A_eq, b_eq=b_eq, method=solver,\n                                  options=options)\n        x = sol.x\n        b = x[-n:]\n\n    else:\n\n        h = np.zeros((n_distributions * n2 + n))\n        G = -sps.eye(n_distributions * n2 + n)\n\n        sol = solvers.lp(matrix(c), scipy_sparse_to_spmatrix(G), matrix(h),\n                         A=scipy_sparse_to_spmatrix(A_eq), b=matrix(b_eq),\n                         solver=solver)\n\n        x = np.array(sol['x'])\n        b = x[-n:].ravel()\n\n    if log:\n        return b, sol\n    else:\n        return b", "response": "Compute the Wasserstein barycenter of distributions A and M."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cg(a, b, M, reg, f, df, G0=None, numItermax=200,\n       stopThr=1e-9, verbose=False, log=False):\n    \"\"\"\n    Solve the general regularized OT problem with conditional gradient\n\n        The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg*f(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`f` is the regularization term ( and df is its gradient)\n    - a and b are source and target weights (sum to 1)\n\n    The algorithm used for solving the problem is conditional gradient as discussed in  [1]_\n\n\n    Parameters\n    ----------\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,)\n        samples in the target domain\n    M : np.ndarray (ns,nt)\n        loss matrix\n    reg : float\n        Regularization term >0\n    G0 :  np.ndarray (ns,nt), optional\n        initial guess (default is indep joint density)\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n\n    References\n    ----------\n\n    .. [1] Ferradans, S., Papadakis, N., Peyr\u00e9, G., & Aujol, J. F. (2014). Regularized discrete optimal transport. SIAM Journal on Imaging Sciences, 7(3), 1853-1882.\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized optimal ransport\n    ot.bregman.sinkhorn : Entropic regularized optimal transport\n\n    \"\"\"\n\n    loop = 1\n\n    if log:\n        log = {'loss': []}\n\n    if G0 is None:\n        G = np.outer(a, b)\n    else:\n        G = G0\n\n    def cost(G):\n        return np.sum(M * G) + reg * f(G)\n\n    f_val = cost(G)\n    if log:\n        log['loss'].append(f_val)\n\n    it = 0\n\n    if verbose:\n        print('{:5s}|{:12s}|{:8s}'.format(\n            'It.', 'Loss', 'Delta loss') + '\\n' + '-' * 32)\n        print('{:5d}|{:8e}|{:8e}'.format(it, f_val, 0))\n\n    while loop:\n\n        it += 1\n        old_fval = f_val\n\n        # problem linearization\n        Mi = M + reg * df(G)\n        # set M positive\n        Mi += Mi.min()\n\n        # solve linear program\n        Gc = emd(a, b, Mi)\n\n        deltaG = Gc - G\n\n        # line search\n        alpha, fc, f_val = line_search_armijo(cost, G, deltaG, Mi, f_val)\n\n        G = G + alpha * deltaG\n\n        # test convergence\n        if it >= numItermax:\n            loop = 0\n\n        delta_fval = (f_val - old_fval) / abs(f_val)\n        if abs(delta_fval) < stopThr:\n            loop = 0\n\n        if log:\n            log['loss'].append(f_val)\n\n        if verbose:\n            if it % 20 == 0:\n                print('{:5s}|{:12s}|{:8s}'.format(\n                    'It.', 'Loss', 'Delta loss') + '\\n' + '-' * 32)\n            print('{:5d}|{:8e}|{:8e}'.format(it, f_val, delta_fval))\n\n    if log:\n        return G, log\n    else:\n        return G", "response": "Solve the general regularized OT problem with conditional gradient problem with conditional gradient problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsolves the general regularized OT problem with the generalized conditional gradient The function solves the following optimization problem: .. math:: \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg1\\cdot\\Omega(\\gamma) + reg2\\cdot f(\\gamma) s.t. \\gamma 1 = a \\gamma^T 1= b \\gamma\\geq 0 where : - M is the (ns,nt) metric cost matrix - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})` - :math:`f` is the regularization term ( and df is its gradient) - a and b are source and target weights (sum to 1) The algorithm used for solving the problem is the generalized conditional gradient as discussed in [5,7]_ Parameters ---------- a : np.ndarray (ns,) samples weights in the source domain b : np.ndarray (nt,) samples in the target domain M : np.ndarray (ns,nt) loss matrix reg1 : float Entropic Regularization term >0 reg2 : float Second Regularization term >0 G0 : np.ndarray (ns,nt), optional initial guess (default is indep joint density) numItermax : int, optional Max number of iterations numInnerItermax : int, optional Max number of iterations of Sinkhorn stopThr : float, optional Stop threshol on error (>0) verbose : bool, optional Print information along iterations log : bool, optional record log if True Returns ------- gamma : (ns x nt) ndarray Optimal transportation matrix for the given parameters log : dict log dictionary return only if log==True in parameters References ---------- .. [5] N. Courty; R. Flamary; D. Tuia; A. Rakotomamonjy, \"Optimal Transport for Domain Adaptation,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence , vol.PP, no.99, pp.1-1 .. [7] Rakotomamonjy, A., Flamary, R., & Courty, N. (2015). Generalized conditional gradient: analysis of convergence and applications. arXiv preprint arXiv:1510.06567. See Also -------- ot.optim.cg : conditional gradient", "response": "def gcg(a, b, M, reg1, reg2, f, df, G0=None, numItermax=10,\n        numInnerItermax=200, stopThr=1e-9, verbose=False, log=False):\n    \"\"\"\n    Solve the general regularized OT problem with the generalized conditional gradient\n\n        The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg1\\cdot\\Omega(\\gamma) + reg2\\cdot f(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma\\geq 0\n    where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - :math:`f` is the regularization term ( and df is its gradient)\n    - a and b are source and target weights (sum to 1)\n\n    The algorithm used for solving the problem is the generalized conditional gradient as discussed in  [5,7]_\n\n\n    Parameters\n    ----------\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,)\n        samples in the target domain\n    M : np.ndarray (ns,nt)\n        loss matrix\n    reg1 : float\n        Entropic Regularization term >0\n    reg2 : float\n        Second Regularization term >0\n    G0 :  np.ndarray (ns,nt), optional\n        initial guess (default is indep joint density)\n    numItermax : int, optional\n        Max number of iterations\n    numInnerItermax : int, optional\n        Max number of iterations of Sinkhorn\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n\n    References\n    ----------\n\n    .. [5] N. Courty; R. Flamary; D. Tuia; A. Rakotomamonjy, \"Optimal Transport for Domain Adaptation,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence , vol.PP, no.99, pp.1-1\n    .. [7] Rakotomamonjy, A., Flamary, R., & Courty, N. (2015). Generalized conditional gradient: analysis of convergence and applications. arXiv preprint arXiv:1510.06567.\n\n    See Also\n    --------\n    ot.optim.cg : conditional gradient\n\n    \"\"\"\n\n    loop = 1\n\n    if log:\n        log = {'loss': []}\n\n    if G0 is None:\n        G = np.outer(a, b)\n    else:\n        G = G0\n\n    def cost(G):\n        return np.sum(M * G) + reg1 * np.sum(G * np.log(G)) + reg2 * f(G)\n\n    f_val = cost(G)\n    if log:\n        log['loss'].append(f_val)\n\n    it = 0\n\n    if verbose:\n        print('{:5s}|{:12s}|{:8s}'.format(\n            'It.', 'Loss', 'Delta loss') + '\\n' + '-' * 32)\n        print('{:5d}|{:8e}|{:8e}'.format(it, f_val, 0))\n\n    while loop:\n\n        it += 1\n        old_fval = f_val\n\n        # problem linearization\n        Mi = M + reg2 * df(G)\n\n        # solve linear program with Sinkhorn\n        # Gc = sinkhorn_stabilized(a,b, Mi, reg1, numItermax = numInnerItermax)\n        Gc = sinkhorn(a, b, Mi, reg1, numItermax=numInnerItermax)\n\n        deltaG = Gc - G\n\n        # line search\n        dcost = Mi + reg1 * (1 + np.log(G))  # ??\n        alpha, fc, f_val = line_search_armijo(cost, G, deltaG, dcost, f_val)\n\n        G = G + alpha * deltaG\n\n        # test convergence\n        if it >= numItermax:\n            loop = 0\n\n        delta_fval = (f_val - old_fval) / abs(f_val)\n        if abs(delta_fval) < stopThr:\n            loop = 0\n\n        if log:\n            log['loss'].append(f_val)\n\n        if verbose:\n            if it % 20 == 0:\n                print('{:5s}|{:12s}|{:8s}'.format(\n                    'It.', 'Loss', 'Delta loss') + '\\n' + '-' * 32)\n            print('{:5d}|{:8e}|{:8e}'.format(it, f_val, delta_fval))\n\n    if log:\n        return G, log\n    else:\n        return G"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef im2mat(I):\n    return I.reshape((I.shape[0] * I.shape[1], I.shape[2]))", "response": "Converts and image to matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef projection_simplex(V, z=1, axis=None):\n    if axis == 1:\n        n_features = V.shape[1]\n        U = np.sort(V, axis=1)[:, ::-1]\n        z = np.ones(len(V)) * z\n        cssv = np.cumsum(U, axis=1) - z[:, np.newaxis]\n        ind = np.arange(n_features) + 1\n        cond = U - cssv / ind > 0\n        rho = np.count_nonzero(cond, axis=1)\n        theta = cssv[np.arange(len(V)), rho - 1] / rho\n        return np.maximum(V - theta[:, np.newaxis], 0)\n\n    elif axis == 0:\n        return projection_simplex(V.T, z, axis=1).T\n\n    else:\n        V = V.ravel().reshape(1, -1)\n        return projection_simplex(V, z, axis=1).ravel()", "response": "Projection of x onto the simplex scaled by z"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes objective value and gradients of dual object.", "response": "def dual_obj_grad(alpha, beta, a, b, C, regul):\n    \"\"\"\n    Compute objective value and gradients of dual objective.\n\n    Parameters\n    ----------\n    alpha: array, shape = len(a)\n    beta: array, shape = len(b)\n        Current iterate of dual potentials.\n    a: array, shape = len(a)\n    b: array, shape = len(b)\n        Input histograms (should be non-negative and sum to 1).\n    C: array, shape = len(a) x len(b)\n        Ground cost matrix.\n    regul: Regularization object\n        Should implement a delta_Omega(X) method.\n\n    Returns\n    -------\n    obj: float\n        Objective value (higher is better).\n    grad_alpha: array, shape = len(a)\n        Gradient w.r.t. alpha.\n    grad_beta: array, shape = len(b)\n        Gradient w.r.t. beta.\n    \"\"\"\n    obj = np.dot(alpha, a) + np.dot(beta, b)\n    grad_alpha = a.copy()\n    grad_beta = b.copy()\n\n    # X[:, j] = alpha + beta[j] - C[:, j]\n    X = alpha[:, np.newaxis] + beta - C\n\n    # val.shape = len(b)\n    # G.shape = len(a) x len(b)\n    val, G = regul.delta_Omega(X)\n\n    obj -= np.sum(val)\n    grad_alpha -= G.sum(axis=1)\n    grad_beta -= G.sum(axis=0)\n\n    return obj, grad_alpha, grad_beta"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef solve_dual(a, b, C, regul, method=\"L-BFGS-B\", tol=1e-3, max_iter=500,\n               verbose=False):\n    \"\"\"\n    Solve the \"smoothed\" dual objective.\n\n    Parameters\n    ----------\n    a: array, shape = len(a)\n    b: array, shape = len(b)\n        Input histograms (should be non-negative and sum to 1).\n    C: array, shape = len(a) x len(b)\n        Ground cost matrix.\n    regul: Regularization object\n        Should implement a delta_Omega(X) method.\n    method: str\n        Solver to be used (passed to `scipy.optimize.minimize`).\n    tol: float\n        Tolerance parameter.\n    max_iter: int\n        Maximum number of iterations.\n\n    Returns\n    -------\n    alpha: array, shape = len(a)\n    beta: array, shape = len(b)\n        Dual potentials.\n    \"\"\"\n\n    def _func(params):\n        # Unpack alpha and beta.\n        alpha = params[:len(a)]\n        beta = params[len(a):]\n\n        obj, grad_alpha, grad_beta = dual_obj_grad(alpha, beta, a, b, C, regul)\n\n        # Pack grad_alpha and grad_beta.\n        grad = np.concatenate((grad_alpha, grad_beta))\n\n        # We need to maximize the dual.\n        return -obj, -grad\n\n    # Unfortunately, `minimize` only supports functions whose argument is a\n    # vector. So, we need to concatenate alpha and beta.\n    alpha_init = np.zeros(len(a))\n    beta_init = np.zeros(len(b))\n    params_init = np.concatenate((alpha_init, beta_init))\n\n    res = minimize(_func, params_init, method=method, jac=True,\n                   tol=tol, options=dict(maxiter=max_iter, disp=verbose))\n\n    alpha = res.x[:len(a)]\n    beta = res.x[len(a):]\n\n    return alpha, beta, res", "response": "Solve the smoothed dual objective."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef semi_dual_obj_grad(alpha, a, b, C, regul):\n    obj = np.dot(alpha, a)\n    grad = a.copy()\n\n    # X[:, j] = alpha - C[:, j]\n    X = alpha[:, np.newaxis] - C\n\n    # val.shape = len(b)\n    # G.shape = len(a) x len(b)\n    val, G = regul.max_Omega(X, b)\n\n    obj -= np.dot(b, val)\n    grad -= np.dot(G, b)\n\n    return obj, grad", "response": "Compute objective value and gradient of semi - dual object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsolving the smoothed semi - dual objective.", "response": "def solve_semi_dual(a, b, C, regul, method=\"L-BFGS-B\", tol=1e-3, max_iter=500,\n                    verbose=False):\n    \"\"\"\n    Solve the \"smoothed\" semi-dual objective.\n\n    Parameters\n    ----------\n    a: array, shape = len(a)\n    b: array, shape = len(b)\n        Input histograms (should be non-negative and sum to 1).\n    C: array, shape = len(a) x len(b)\n        Ground cost matrix.\n    regul: Regularization object\n        Should implement a max_Omega(X) method.\n    method: str\n        Solver to be used (passed to `scipy.optimize.minimize`).\n    tol: float\n        Tolerance parameter.\n    max_iter: int\n        Maximum number of iterations.\n\n    Returns\n    -------\n    alpha: array, shape = len(a)\n        Semi-dual potentials.\n    \"\"\"\n\n    def _func(alpha):\n        obj, grad = semi_dual_obj_grad(alpha, a, b, C, regul)\n        # We need to maximize the semi-dual.\n        return -obj, -grad\n\n    alpha_init = np.zeros(len(a))\n\n    res = minimize(_func, alpha_init, method=method, jac=True,\n                   tol=tol, options=dict(maxiter=max_iter, disp=verbose))\n\n    return res.x, res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving optimal transportation plan from dual potentials.", "response": "def get_plan_from_dual(alpha, beta, C, regul):\n    \"\"\"\n    Retrieve optimal transportation plan from optimal dual potentials.\n\n    Parameters\n    ----------\n    alpha: array, shape = len(a)\n    beta: array, shape = len(b)\n        Optimal dual potentials.\n    C: array, shape = len(a) x len(b)\n        Ground cost matrix.\n    regul: Regularization object\n        Should implement a delta_Omega(X) method.\n\n    Returns\n    -------\n    T: array, shape = len(a) x len(b)\n        Optimal transportation plan.\n    \"\"\"\n    X = alpha[:, np.newaxis] + beta - C\n    return regul.delta_Omega(X)[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves optimal transportation plan from optimal semi - dual potentials.", "response": "def get_plan_from_semi_dual(alpha, b, C, regul):\n    \"\"\"\n    Retrieve optimal transportation plan from optimal semi-dual potentials.\n\n    Parameters\n    ----------\n    alpha: array, shape = len(a)\n        Optimal semi-dual potentials.\n    b: array, shape = len(b)\n        Second input histogram (should be non-negative and sum to 1).\n    C: array, shape = len(a) x len(b)\n        Ground cost matrix.\n    regul: Regularization object\n        Should implement a delta_Omega(X) method.\n\n    Returns\n    -------\n    T: array, shape = len(a) x len(b)\n        Optimal transportation plan.\n    \"\"\"\n    X = alpha[:, np.newaxis] - C\n    return regul.max_Omega(X, b)[1] * b"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef smooth_ot_dual(a, b, M, reg, reg_type='l2', method=\"L-BFGS-B\", stopThr=1e-9,\n                   numItermax=500, verbose=False, log=False):\n    r\"\"\"\n    Solve the regularized OT problem in the dual and return the OT matrix\n\n    The function solves the smooth relaxed dual formulation (7) in [17]_ :\n\n    .. math::\n        \\max_{\\alpha,\\beta}\\quad a^T\\alpha+b^T\\beta-\\sum_j\\delta_\\Omega(\\alpha+\\beta_j-\\mathbf{m}_j)\n\n    where :\n\n    - :math:`\\mathbf{m}_j` is the jth column of the cost matrix\n    - :math:`\\delta_\\Omega` is the convex conjugate of the regularization term :math:`\\Omega`\n    - a and b are source and target weights (sum to 1)\n\n    The OT matrix can is reconstructed from the gradient of :math:`\\delta_\\Omega`\n    (See [17]_ Proposition 1).\n    The optimization algorithm is using gradient decent (L-BFGS by default).\n\n\n    Parameters\n    ----------\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,) or np.ndarray (nt,nbb)\n        samples in the target domain, compute sinkhorn with multiple targets\n        and fixed M if b is a matrix (return OT loss + dual variables in log)\n    M : np.ndarray (ns,nt)\n        loss matrix\n    reg : float\n        Regularization term >0\n    reg_type : str\n        Regularization type,  can be the following (default ='l2'):\n        - 'kl' : Kullback Leibler (~ Neg-entropy used in sinkhorn [2]_)\n        - 'l2' : Squared Euclidean regularization\n    method : str\n        Solver to use for scipy.optimize.minimize\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n\n    References\n    ----------\n\n    .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation of Optimal Transport, Advances in Neural Information Processing Systems (NIPS) 26, 2013\n\n    .. [17] Blondel, M., Seguy, V., & Rolet, A. (2018). Smooth and Sparse Optimal Transport. Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics (AISTATS).\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized OT\n    ot.sinhorn : Entropic regularized OT\n    ot.optim.cg : General regularized OT\n\n    \"\"\"\n\n    if reg_type.lower() in ['l2', 'squaredl2']:\n        regul = SquaredL2(gamma=reg)\n    elif reg_type.lower() in ['entropic', 'negentropy', 'kl']:\n        regul = NegEntropy(gamma=reg)\n    else:\n        raise NotImplementedError('Unknown regularization')\n\n    # solve dual\n    alpha, beta, res = solve_dual(a, b, M, regul, max_iter=numItermax,\n                                  tol=stopThr, verbose=verbose)\n\n    # reconstruct transport matrix\n    G = get_plan_from_dual(alpha, beta, M, regul)\n\n    if log:\n        log = {'alpha': alpha, 'beta': beta, 'res': res}\n        return G, log\n    else:\n        return G", "response": "r Solve the regularized OT problem in the dual and return the OT matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef smooth_ot_semi_dual(a, b, M, reg, reg_type='l2', method=\"L-BFGS-B\", stopThr=1e-9,\n                        numItermax=500, verbose=False, log=False):\n    r\"\"\"\n    Solve the regularized OT problem in the semi-dual and return the OT matrix\n\n    The function solves the smooth relaxed dual formulation (10) in [17]_ :\n\n    .. math::\n        \\max_{\\alpha}\\quad a^T\\alpha-OT_\\Omega^*(\\alpha,b)\n\n    where :\n\n    .. math::\n        OT_\\Omega^*(\\alpha,b)=\\sum_j b_j\n\n    - :math:`\\mathbf{m}_j` is the jth column of the cost matrix\n    - :math:`OT_\\Omega^*(\\alpha,b)` is defined in Eq. (9) in [17]\n    - a and b are source and target weights (sum to 1)\n\n    The OT matrix can is reconstructed using [17]_ Proposition 2.\n    The optimization algorithm is using gradient decent (L-BFGS by default).\n\n\n    Parameters\n    ----------\n    a : np.ndarray (ns,)\n        samples weights in the source domain\n    b : np.ndarray (nt,) or np.ndarray (nt,nbb)\n        samples in the target domain, compute sinkhorn with multiple targets\n        and fixed M if b is a matrix (return OT loss + dual variables in log)\n    M : np.ndarray (ns,nt)\n        loss matrix\n    reg : float\n        Regularization term >0\n    reg_type : str\n        Regularization type,  can be the following (default ='l2'):\n        - 'kl' : Kullback Leibler (~ Neg-entropy used in sinkhorn [2]_)\n        - 'l2' : Squared Euclidean regularization\n    method : str\n        Solver to use for scipy.optimize.minimize\n    numItermax : int, optional\n        Max number of iterations\n    stopThr : float, optional\n        Stop threshol on error (>0)\n    verbose : bool, optional\n        Print information along iterations\n    log : bool, optional\n        record log if True\n\n\n    Returns\n    -------\n    gamma : (ns x nt) ndarray\n        Optimal transportation matrix for the given parameters\n    log : dict\n        log dictionary return only if log==True in parameters\n\n\n    References\n    ----------\n\n    .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation of Optimal Transport, Advances in Neural Information Processing Systems (NIPS) 26, 2013\n\n    .. [17] Blondel, M., Seguy, V., & Rolet, A. (2018). Smooth and Sparse Optimal Transport. Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics (AISTATS).\n\n    See Also\n    --------\n    ot.lp.emd : Unregularized OT\n    ot.sinhorn : Entropic regularized OT\n    ot.optim.cg : General regularized OT\n\n    \"\"\"\n    if reg_type.lower() in ['l2', 'squaredl2']:\n        regul = SquaredL2(gamma=reg)\n    elif reg_type.lower() in ['entropic', 'negentropy', 'kl']:\n        regul = NegEntropy(gamma=reg)\n    else:\n        raise NotImplementedError('Unknown regularization')\n\n    # solve dual\n    alpha, res = solve_semi_dual(a, b, M, regul, max_iter=numItermax,\n                                 tol=stopThr, verbose=verbose)\n\n    # reconstruct transport matrix\n    G = get_plan_from_semi_dual(alpha, b, M, regul)\n\n    if log:\n        log = {'alpha': alpha, 'res': res}\n        return G, log\n    else:\n        return G", "response": "r Solve the regularized OT problem in the semi - dual form and return the OT matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef toc(message='Elapsed time : {} s'):\n    t = time.time()\n    print(message.format(t - __time_tic_toc))\n    return t - __time_tic_toc", "response": "Python implementation of Matlab toc function"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean_zeros(a, b, M):\n    M2 = M[a > 0, :][:, b > 0].copy()  # copy force c style matrix (froemd)\n    a2 = a[a > 0]\n    b2 = b[b > 0]\n    return a2, b2, M2", "response": "Remove all components with zeros weights in a and b"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the euclidean distance matrix between each row of X and Y.", "response": "def euclidean_distances(X, Y, squared=False):\n    \"\"\"\n    Considering the rows of X (and Y=X) as vectors, compute the\n    distance matrix between each pair of vectors.\n    Parameters\n    ----------\n    X : {array-like}, shape (n_samples_1, n_features)\n    Y : {array-like}, shape (n_samples_2, n_features)\n    squared : boolean, optional\n        Return squared Euclidean distances.\n    Returns\n    -------\n    distances : {array}, shape (n_samples_1, n_samples_2)\n    \"\"\"\n    XX = np.einsum('ij,ij->i', X, X)[:, np.newaxis]\n    YY = np.einsum('ij,ij->i', Y, Y)[np.newaxis, :]\n    distances = np.dot(X, Y.T)\n    distances *= -2\n    distances += XX\n    distances += YY\n    np.maximum(distances, 0, out=distances)\n    if X is Y:\n        # Ensure that distances between vectors and themselves are set to 0.0.\n        # This may not be the case due to floating point rounding errors.\n        distances.flat[::distances.shape[0] + 1] = 0.0\n    return distances if squared else np.sqrt(distances, out=distances)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes distance between samples in x1 and x2 using function scipy. spatial. distance. cdist", "response": "def dist(x1, x2=None, metric='sqeuclidean'):\n    \"\"\"Compute distance between samples in x1 and x2 using function scipy.spatial.distance.cdist\n\n    Parameters\n    ----------\n\n    x1 : np.array (n1,d)\n        matrix with n1 samples of size d\n    x2 : np.array (n2,d), optional\n        matrix with n2 samples of size d (if None then x2=x1)\n    metric : str, fun, optional\n        name of the metric to be computed (full list in the doc of scipy),  If a string,\n        the distance function can be 'braycurtis', 'canberra', 'chebyshev', 'cityblock',\n        'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski',\n        'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n        'sokalmichener', 'sokalsneath', 'sqeuclidean', 'wminkowski', 'yule'.\n\n\n    Returns\n    -------\n\n    M : np.array (n1,n2)\n        distance matrix computed with given metric\n\n    \"\"\"\n    if x2 is None:\n        x2 = x1\n    if metric == \"sqeuclidean\":\n        return euclidean_distances(x1, x2, squared=True)\n    return cdist(x1, x2, metric=metric)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dist0(n, method='lin_square'):\n    res = 0\n    if method == 'lin_square':\n        x = np.arange(n, dtype=np.float64).reshape((n, 1))\n        res = dist(x, x)\n    return res", "response": "Compute standard cost matrix of size n for OT problems\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cost_normalization(C, norm=None):\n\n    if norm == \"median\":\n        C /= float(np.median(C))\n    elif norm == \"max\":\n        C /= float(np.max(C))\n    elif norm == \"log\":\n        C = np.log(1 + C)\n    elif norm == \"loglog\":\n        C = np.log(1 + np.log(1 + C))\n\n    return C", "response": "Apply normalization to the loss matrix C."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parmap(f, X, nprocs=multiprocessing.cpu_count()):\n    q_in = multiprocessing.Queue(1)\n    q_out = multiprocessing.Queue()\n\n    proc = [multiprocessing.Process(target=fun, args=(f, q_in, q_out))\n            for _ in range(nprocs)]\n    for p in proc:\n        p.daemon = True\n        p.start()\n\n    sent = [q_in.put((i, x)) for i, x in enumerate(X)]\n    [q_in.put((None, None)) for _ in range(nprocs)]\n    res = [q_out.get() for _ in range(len(sent))]\n\n    [p.join() for p in proc]\n\n    return [x for i, x in sorted(res)]", "response": "paralell map for multiprocessing"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_params(**kwargs):\n\n    missing_params = []\n    check = True\n\n    for param in kwargs:\n        if kwargs[param] is None:\n            missing_params.append(param)\n\n    if len(missing_params) > 0:\n        print(\"POT - Warning: following necessary parameters are missing\")\n        for p in missing_params:\n            print(\"\\n\", p)\n\n        check = False\n\n    return check", "response": "check whether some parameters are missing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nturning seed into a np. random. RandomState instance", "response": "def check_random_state(seed):\n    \"\"\"Turn seed into a np.random.RandomState instance\n    Parameters\n    ----------\n    seed : None | int | instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, (int, np.integer)):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError('{} cannot be used to seed a numpy.random.RandomState'\n                     ' instance'.format(seed))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_deprecated(func):\n    if sys.version_info < (3, 5):\n        raise NotImplementedError(\"This is only available for python3.5 \"\n                                  \"or above\")\n    closures = getattr(func, '__closure__', [])\n    if closures is None:\n        closures = []\n    is_deprecated = ('deprecated' in ''.join([c.cell_contents\n                                              for c in closures\n                                              if isinstance(c.cell_contents, str)]))\n    return is_deprecated", "response": "Helper to check if a function is wrapped by our deprecated decorator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the parameter names for the estimator.", "response": "def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [p for p in init_signature.parameters.values()\n                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n        for p in parameters:\n            if p.kind == p.VAR_POSITIONAL:\n                raise RuntimeError(\"POT estimators should always \"\n                                   \"specify their parameters in the signature\"\n                                   \" of their __init__ (no varargs).\"\n                                   \" %s with constructor %s doesn't \"\n                                   \" follow this convention.\"\n                                   % (cls, init_signature))\n        # Extract and sort argument names excluding 'self'\n        return sorted([p.name for p in parameters])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes squared euclidean distance between samples x1 and x2", "response": "def dist(x1, x2):\n    \"\"\" Compute squared euclidean distance between samples (autograd)\n    \"\"\"\n    x1p2 = np.sum(np.square(x1), 1)\n    x2p2 = np.sum(np.square(x2), 1)\n    return x1p2.reshape((-1, 1)) + x2p2.reshape((1, -1)) - 2 * np.dot(x1, x2.T)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sinkhorn(w1, w2, M, reg, k):\n    K = np.exp(-M / reg)\n    ui = np.ones((M.shape[0],))\n    vi = np.ones((M.shape[1],))\n    for i in range(k):\n        vi = w2 / (np.dot(K.T, ui))\n        ui = w1 / (np.dot(K, vi))\n    G = ui.reshape((M.shape[0], 1)) * K * vi.reshape((1, M.shape[1]))\n    return G", "response": "Sinkhorn algorithm with fixed number of iteration"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting samples in X by classes in y", "response": "def split_classes(X, y):\n    \"\"\"split samples in X by classes in y\n    \"\"\"\n    lstsclass = np.unique(y)\n    return [X[y == i, :].astype(np.float32) for i in lstsclass]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wda(X, y, p=2, reg=1, k=10, solver=None, maxiter=100, verbose=0, P0=None):\n\n    mx = np.mean(X)\n    X -= mx.reshape((1, -1))\n\n    # data split between classes\n    d = X.shape[1]\n    xc = split_classes(X, y)\n    # compute uniform weighs\n    wc = [np.ones((x.shape[0]), dtype=np.float32) / x.shape[0] for x in xc]\n\n    def cost(P):\n        # wda loss\n        loss_b = 0\n        loss_w = 0\n\n        for i, xi in enumerate(xc):\n            xi = np.dot(xi, P)\n            for j, xj in enumerate(xc[i:]):\n                xj = np.dot(xj, P)\n                M = dist(xi, xj)\n                G = sinkhorn(wc[i], wc[j + i], M, reg, k)\n                if j == 0:\n                    loss_w += np.sum(G * M)\n                else:\n                    loss_b += np.sum(G * M)\n\n        # loss inversed because minimization\n        return loss_w / loss_b\n\n    # declare manifold and problem\n    manifold = Stiefel(d, p)\n    problem = Problem(manifold=manifold, cost=cost)\n\n    # declare solver and solve\n    if solver is None:\n        solver = SteepestDescent(maxiter=maxiter, logverbosity=verbose)\n    elif solver in ['tr', 'TrustRegions']:\n        solver = TrustRegions(maxiter=maxiter, logverbosity=verbose)\n\n    Popt = solver.solve(problem, x=P0)\n\n    def proj(X):\n        return (X - mx.reshape((1, -1))).dot(Popt)\n\n    return Popt, proj", "response": "Function solves the Wasserstein Discriminant Analysis problem for a given set of training samples X and y."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an interpolated point cloud following the dissimilarity matrix C with SMACOF multidimensional scaling.", "response": "def smacof_mds(C, dim, max_iter=3000, eps=1e-9):\r\n    \"\"\"\r\n    Returns an interpolated point cloud following the dissimilarity matrix C\r\n    using SMACOF multidimensional scaling (MDS) in specific dimensionned\r\n    target space\r\n\r\n    Parameters\r\n    ----------\r\n    C : ndarray, shape (ns, ns)\r\n        dissimilarity matrix\r\n    dim : int\r\n          dimension of the targeted space\r\n    max_iter :  int\r\n        Maximum number of iterations of the SMACOF algorithm for a single run\r\n    eps : float\r\n        relative tolerance w.r.t stress to declare converge\r\n\r\n    Returns\r\n    -------\r\n    npos : ndarray, shape (R, dim)\r\n           Embedded coordinates of the interpolated point cloud (defined with\r\n           one isometry)\r\n    \"\"\"\r\n\r\n    rng = np.random.RandomState(seed=3)\r\n\r\n    mds = manifold.MDS(\r\n        dim,\r\n        max_iter=max_iter,\r\n        eps=1e-9,\r\n        dissimilarity='precomputed',\r\n        n_init=1)\r\n    pos = mds.fit(C).embedding_\r\n\r\n    nmds = manifold.MDS(\r\n        2,\r\n        max_iter=max_iter,\r\n        eps=1e-9,\r\n        dissimilarity=\"precomputed\",\r\n        random_state=rng,\r\n        n_init=1)\r\n    npos = nmds.fit_transform(C, init=pos)\r\n\r\n    return npos"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the coordinate gradient update for regularized discrete distributions for (i, :) The function computes the gradient of the semi dual problem: .. math:: \\max_v \\sum_i (\\sum_j v_j * b_j - reg * log(\\sum_j exp((v_j - M_{i,j})/reg) * b_j)) * a_i Where : - M is the (ns,nt) metric cost matrix - v is a dual variable in R^J - reg is the regularization term - a and b are source and target weights (sum to 1) The algorithm used for solving the problem is the ASGD & SAG algorithms as proposed in [18]_ [alg.1 & alg.2] Parameters ---------- b : np.ndarray(nt,) target measure M : np.ndarray(ns, nt) cost matrix reg : float nu Regularization term > 0 v : np.ndarray(nt,) dual variable i : number int picked number i Returns ------- coordinate gradient : np.ndarray(nt,) Examples -------- >>> n_source = 7 >>> n_target = 4 >>> reg = 1 >>> numItermax = 300000 >>> a = ot.utils.unif(n_source) >>> b = ot.utils.unif(n_target) >>> rng = np.random.RandomState(0) >>> X_source = rng.randn(n_source, 2) >>> Y_target = rng.randn(n_target, 2) >>> M = ot.dist(X_source, Y_target) >>> method = \"ASGD\" >>> asgd_pi = stochastic.solve_semi_dual_entropic(a, b, M, reg, method, numItermax) >>> print(asgd_pi) References ---------- [Genevay et al., 2016] : Stochastic Optimization for Large-scale Optimal Transport, Advances in Neural Information Processing Systems (2016), arXiv preprint arxiv:1605.08527.", "response": "def coordinate_grad_semi_dual(b, M, reg, beta, i):\n    '''\n    Compute the coordinate gradient update for regularized discrete distributions for (i, :)\n\n    The function computes the gradient of the semi dual problem:\n\n    .. math::\n        \\max_v \\sum_i (\\sum_j v_j * b_j - reg * log(\\sum_j exp((v_j - M_{i,j})/reg) * b_j)) * a_i\n\n    Where :\n\n    - M is the (ns,nt) metric cost matrix\n    - v is a dual variable in R^J\n    - reg is the regularization term\n    - a and b are source and target weights (sum to 1)\n\n    The algorithm used for solving the problem is the ASGD & SAG algorithms\n    as proposed in [18]_ [alg.1 & alg.2]\n\n\n    Parameters\n    ----------\n\n    b : np.ndarray(nt,)\n        target measure\n    M : np.ndarray(ns, nt)\n        cost matrix\n    reg : float nu\n        Regularization term > 0\n    v : np.ndarray(nt,)\n        dual variable\n    i : number int\n        picked number i\n\n    Returns\n    -------\n\n    coordinate gradient : np.ndarray(nt,)\n\n    Examples\n    --------\n\n    >>> n_source = 7\n    >>> n_target = 4\n    >>> reg = 1\n    >>> numItermax = 300000\n    >>> a = ot.utils.unif(n_source)\n    >>> b = ot.utils.unif(n_target)\n    >>> rng = np.random.RandomState(0)\n    >>> X_source = rng.randn(n_source, 2)\n    >>> Y_target = rng.randn(n_target, 2)\n    >>> M = ot.dist(X_source, Y_target)\n    >>> method = \"ASGD\"\n    >>> asgd_pi = stochastic.solve_semi_dual_entropic(a, b, M, reg,\n                                                      method, numItermax)\n    >>> print(asgd_pi)\n\n    References\n    ----------\n\n    [Genevay et al., 2016] :\n                    Stochastic Optimization for Large-scale Optimal Transport,\n                     Advances in Neural Information Processing Systems (2016),\n                      arXiv preprint arxiv:1605.08527.\n\n    '''\n\n    r = M[i, :] - beta\n    exp_beta = np.exp(-r / reg) * b\n    khi = exp_beta / (np.sum(exp_beta))\n    return b - khi"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sag_entropic_transport(a, b, M, reg, numItermax=10000, lr=None):\n    '''\n    Compute the SAG algorithm to solve the regularized discrete measures\n        optimal transport max problem\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1 = b\n\n             \\gamma \\geq 0\n\n    Where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term with :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - a and b are source and target weights (sum to 1)\n\n    The algorithm used for solving the problem is the SAG algorithm\n    as proposed in [18]_ [alg.1]\n\n\n    Parameters\n    ----------\n\n    a : np.ndarray(ns,),\n        source measure\n    b : np.ndarray(nt,),\n        target measure\n    M : np.ndarray(ns, nt),\n        cost matrix\n    reg : float number,\n        Regularization term > 0\n    numItermax : int number\n        number of iteration\n    lr : float number\n        learning rate\n\n    Returns\n    -------\n\n    v : np.ndarray(nt,)\n        dual variable\n\n    Examples\n    --------\n\n    >>> n_source = 7\n    >>> n_target = 4\n    >>> reg = 1\n    >>> numItermax = 300000\n    >>> a = ot.utils.unif(n_source)\n    >>> b = ot.utils.unif(n_target)\n    >>> rng = np.random.RandomState(0)\n    >>> X_source = rng.randn(n_source, 2)\n    >>> Y_target = rng.randn(n_target, 2)\n    >>> M = ot.dist(X_source, Y_target)\n    >>> method = \"ASGD\"\n    >>> asgd_pi = stochastic.solve_semi_dual_entropic(a, b, M, reg,\n                                                      method, numItermax)\n    >>> print(asgd_pi)\n\n    References\n    ----------\n\n    [Genevay et al., 2016] :\n                    Stochastic Optimization for Large-scale Optimal Transport,\n                     Advances in Neural Information Processing Systems (2016),\n                      arXiv preprint arxiv:1605.08527.\n    '''\n\n    if lr is None:\n        lr = 1. / max(a / reg)\n    n_source = np.shape(M)[0]\n    n_target = np.shape(M)[1]\n    cur_beta = np.zeros(n_target)\n    stored_gradient = np.zeros((n_source, n_target))\n    sum_stored_gradient = np.zeros(n_target)\n    for _ in range(numItermax):\n        i = np.random.randint(n_source)\n        cur_coord_grad = a[i] * coordinate_grad_semi_dual(b, M, reg,\n                                                          cur_beta, i)\n        sum_stored_gradient += (cur_coord_grad - stored_gradient[i])\n        stored_gradient[i] = cur_coord_grad\n        cur_beta += lr * (1. / n_source) * sum_stored_gradient\n    return cur_beta", "response": "Compute the SAG algorithm to solve the regularized discrete measures\n    optimal transport max problem"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef averaged_sgd_entropic_transport(a, b, M, reg, numItermax=300000, lr=None):\n    '''\n    Compute the ASGD algorithm to solve the regularized semi continous measures optimal transport max problem\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma \\geq 0\n\n    Where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term with :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - a and b are source and target weights (sum to 1)\n\n    The algorithm used for solving the problem is the ASGD algorithm\n    as proposed in [18]_ [alg.2]\n\n\n    Parameters\n    ----------\n\n    b : np.ndarray(nt,)\n        target measure\n    M : np.ndarray(ns, nt)\n        cost matrix\n    reg : float number\n        Regularization term > 0\n    numItermax : int number\n        number of iteration\n    lr : float number\n        learning rate\n\n\n    Returns\n    -------\n\n    ave_v : np.ndarray(nt,)\n        dual variable\n\n    Examples\n    --------\n\n    >>> n_source = 7\n    >>> n_target = 4\n    >>> reg = 1\n    >>> numItermax = 300000\n    >>> a = ot.utils.unif(n_source)\n    >>> b = ot.utils.unif(n_target)\n    >>> rng = np.random.RandomState(0)\n    >>> X_source = rng.randn(n_source, 2)\n    >>> Y_target = rng.randn(n_target, 2)\n    >>> M = ot.dist(X_source, Y_target)\n    >>> method = \"ASGD\"\n    >>> asgd_pi = stochastic.solve_semi_dual_entropic(a, b, M, reg,\n                                                      method, numItermax)\n    >>> print(asgd_pi)\n\n    References\n    ----------\n\n    [Genevay et al., 2016] :\n                    Stochastic Optimization for Large-scale Optimal Transport,\n                     Advances in Neural Information Processing Systems (2016),\n                      arXiv preprint arxiv:1605.08527.\n    '''\n\n    if lr is None:\n        lr = 1. / max(a / reg)\n    n_source = np.shape(M)[0]\n    n_target = np.shape(M)[1]\n    cur_beta = np.zeros(n_target)\n    ave_beta = np.zeros(n_target)\n    for cur_iter in range(numItermax):\n        k = cur_iter + 1\n        i = np.random.randint(n_source)\n        cur_coord_grad = coordinate_grad_semi_dual(b, M, reg, cur_beta, i)\n        cur_beta += (lr / np.sqrt(k)) * cur_coord_grad\n        ave_beta = (1. / k) * cur_beta + (1 - 1. / k) * ave_beta\n    return ave_beta", "response": "Compute the ASGD algorithm to solve the regularized semi continous measures optimal transport max problem"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the transportation matrix to solve the regularized discrete measures optimal transport max problem The function solves the following optimization problem: .. math:: \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma) s.t. \\gamma 1 = a \\gamma^T 1= b \\gamma \\geq 0 Where : - M is the (ns,nt) metric cost matrix - :math:`\\Omega` is the entropic regularization term with :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})` - a and b are source and target weights (sum to 1) The algorithm used for solving the problem is the SAG or ASGD algorithms as proposed in [18]_ Parameters ---------- a : np.ndarray(ns,) source measure b : np.ndarray(nt,) target measure M : np.ndarray(ns, nt) cost matrix reg : float number Regularization term > 0 methode : str used method (SAG or ASGD) numItermax : int number number of iteration lr : float number learning rate n_source : int number size of the source measure n_target : int number size of the target measure log : bool, optional record log if True Returns ------- pi : np.ndarray(ns, nt) transportation matrix log : dict log dictionary return only if log==True in parameters Examples -------- >>> n_source = 7 >>> n_target = 4 >>> reg = 1 >>> numItermax = 300000 >>> a = ot.utils.unif(n_source) >>> b = ot.utils.unif(n_target) >>> rng = np.random.RandomState(0) >>> X_source = rng.randn(n_source, 2) >>> Y_target = rng.randn(n_target, 2) >>> M = ot.dist(X_source, Y_target) >>> method = \"ASGD\" >>> asgd_pi = stochastic.solve_semi_dual_entropic(a, b, M, reg, method, numItermax) >>> print(asgd_pi) References ---------- [Genevay et al., 2016] : Stochastic Optimization for Large-scale Optimal Transport, Advances in Neural Information Processing Systems (2016), arXiv preprint arxiv:1605.08527.", "response": "def solve_semi_dual_entropic(a, b, M, reg, method, numItermax=10000, lr=None,\n                                log=False):\n    '''\n    Compute the transportation matrix to solve the regularized discrete\n        measures optimal transport max problem\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma \\geq 0\n\n    Where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term with :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - a and b are source and target weights (sum to 1)\n    The algorithm used for solving the problem is the SAG or ASGD algorithms\n    as proposed in [18]_\n\n\n    Parameters\n    ----------\n\n    a : np.ndarray(ns,)\n        source measure\n    b : np.ndarray(nt,)\n        target measure\n    M : np.ndarray(ns, nt)\n        cost matrix\n    reg : float number\n        Regularization term > 0\n    methode : str\n        used method (SAG or ASGD)\n    numItermax : int number\n        number of iteration\n    lr : float number\n        learning rate\n    n_source : int number\n        size of the source measure\n    n_target : int number\n        size of the target measure\n    log : bool, optional\n        record log if True\n\n    Returns\n    -------\n\n    pi : np.ndarray(ns, nt)\n        transportation matrix\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    Examples\n    --------\n\n    >>> n_source = 7\n    >>> n_target = 4\n    >>> reg = 1\n    >>> numItermax = 300000\n    >>> a = ot.utils.unif(n_source)\n    >>> b = ot.utils.unif(n_target)\n    >>> rng = np.random.RandomState(0)\n    >>> X_source = rng.randn(n_source, 2)\n    >>> Y_target = rng.randn(n_target, 2)\n    >>> M = ot.dist(X_source, Y_target)\n    >>> method = \"ASGD\"\n    >>> asgd_pi = stochastic.solve_semi_dual_entropic(a, b, M, reg,\n                                                      method, numItermax)\n    >>> print(asgd_pi)\n\n    References\n    ----------\n\n    [Genevay et al., 2016] :\n                    Stochastic Optimization for Large-scale Optimal Transport,\n                     Advances in Neural Information Processing Systems (2016),\n                      arXiv preprint arxiv:1605.08527.\n    '''\n\n    if method.lower() == \"sag\":\n        opt_beta = sag_entropic_transport(a, b, M, reg, numItermax, lr)\n    elif method.lower() == \"asgd\":\n        opt_beta = averaged_sgd_entropic_transport(a, b, M, reg, numItermax, lr)\n    else:\n        print(\"Please, select your method between SAG and ASGD\")\n        return None\n\n    opt_alpha = c_transform_entropic(b, M, reg, opt_beta)\n    pi = (np.exp((opt_alpha[:, None] + opt_beta[None, :] - M[:, :]) / reg) *\n          a[:, None] * b[None, :])\n\n    if log:\n        log = {}\n        log['alpha'] = opt_alpha\n        log['beta'] = opt_beta\n        return pi, log\n    else:\n        return pi"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef batch_grad_dual(a, b, M, reg, alpha, beta, batch_size, batch_alpha,\n                    batch_beta):\n    '''\n    Computes the partial gradient of the dual optimal transport problem.\n\n    For each (i,j) in a batch of coordinates, the partial gradients are :\n\n    .. math::\n        \\partial_{u_i} F = u_i * b_s/l_{v} - \\sum_{j \\in B_v} exp((u_i + v_j - M_{i,j})/reg) * a_i * b_j\n\n        \\partial_{v_j} F = v_j * b_s/l_{u} - \\sum_{i \\in B_u} exp((u_i + v_j - M_{i,j})/reg) * a_i * b_j\n\n    Where :\n\n    - M is the (ns,nt) metric cost matrix\n    - u, v are dual variables in R^ixR^J\n    - reg is the regularization term\n    - :math:`B_u` and :math:`B_v` are lists of index\n    - :math:`b_s` is the size of the batchs :math:`B_u` and :math:`B_v`\n    - :math:`l_u` and :math:`l_v` are the lenghts of :math:`B_u` and :math:`B_v`\n    - a and b are source and target weights (sum to 1)\n\n\n    The algorithm used for solving the dual problem is the SGD algorithm\n    as proposed in [19]_ [alg.1]\n\n\n    Parameters\n    ----------\n\n    a : np.ndarray(ns,)\n        source measure\n    b : np.ndarray(nt,)\n        target measure\n    M : np.ndarray(ns, nt)\n        cost matrix\n    reg : float number\n        Regularization term > 0\n    alpha : np.ndarray(ns,)\n        dual variable\n    beta : np.ndarray(nt,)\n        dual variable\n    batch_size : int number\n        size of the batch\n    batch_alpha : np.ndarray(bs,)\n        batch of index of alpha\n    batch_beta : np.ndarray(bs,)\n        batch of index of beta\n\n    Returns\n    -------\n\n    grad : np.ndarray(ns,)\n        partial grad F\n\n    Examples\n    --------\n\n    >>> n_source = 7\n    >>> n_target = 4\n    >>> reg = 1\n    >>> numItermax = 20000\n    >>> lr = 0.1\n    >>> batch_size = 3\n    >>> log = True\n    >>> a = ot.utils.unif(n_source)\n    >>> b = ot.utils.unif(n_target)\n    >>> rng = np.random.RandomState(0)\n    >>> X_source = rng.randn(n_source, 2)\n    >>> Y_target = rng.randn(n_target, 2)\n    >>> M = ot.dist(X_source, Y_target)\n    >>> sgd_dual_pi, log = stochastic.solve_dual_entropic(a, b, M, reg,\n                                                            batch_size,\n                                                            numItermax, lr, log)\n    >>> print(log['alpha'], log['beta'])\n    >>> print(sgd_dual_pi)\n\n    References\n    ----------\n\n    [Seguy et al., 2018] :\n                    International Conference on Learning Representation (2018),\n                      arXiv preprint arxiv:1711.02283.\n    '''\n\n    G = - (np.exp((alpha[batch_alpha, None] + beta[None, batch_beta] -\n                   M[batch_alpha, :][:, batch_beta]) / reg) *\n           a[batch_alpha, None] * b[None, batch_beta])\n    grad_beta = np.zeros(np.shape(M)[1])\n    grad_alpha = np.zeros(np.shape(M)[0])\n    grad_beta[batch_beta] = (b[batch_beta] * len(batch_alpha) / np.shape(M)[0]\n                             + G.sum(0))\n    grad_alpha[batch_alpha] = (a[batch_alpha] * len(batch_beta)\n                               / np.shape(M)[1] + G.sum(1))\n\n    return grad_alpha, grad_beta", "response": "This function computes the partial gradient of the dual optimal transport problem."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sgd_entropic_regularization(a, b, M, reg, batch_size, numItermax, lr):\n    '''\n    Compute the sgd algorithm to solve the regularized discrete measures\n        optimal transport dual problem\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma \\geq 0\n\n    Where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term with :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - a and b are source and target weights (sum to 1)\n\n    Parameters\n    ----------\n\n    a : np.ndarray(ns,)\n        source measure\n    b : np.ndarray(nt,)\n        target measure\n    M : np.ndarray(ns, nt)\n        cost matrix\n    reg : float number\n        Regularization term > 0\n    batch_size : int number\n        size of the batch\n    numItermax : int number\n        number of iteration\n    lr : float number\n        learning rate\n\n    Returns\n    -------\n\n    alpha : np.ndarray(ns,)\n        dual variable\n    beta : np.ndarray(nt,)\n        dual variable\n\n    Examples\n    --------\n\n    >>> n_source = 7\n    >>> n_target = 4\n    >>> reg = 1\n    >>> numItermax = 20000\n    >>> lr = 0.1\n    >>> batch_size = 3\n    >>> log = True\n    >>> a = ot.utils.unif(n_source)\n    >>> b = ot.utils.unif(n_target)\n    >>> rng = np.random.RandomState(0)\n    >>> X_source = rng.randn(n_source, 2)\n    >>> Y_target = rng.randn(n_target, 2)\n    >>> M = ot.dist(X_source, Y_target)\n    >>> sgd_dual_pi, log = stochastic.solve_dual_entropic(a, b, M, reg,\n                                                          batch_size,\n                                                          numItermax, lr, log)\n    >>> print(log['alpha'], log['beta'])\n    >>> print(sgd_dual_pi)\n\n    References\n    ----------\n\n    [Seguy et al., 2018] :\n                    International Conference on Learning Representation (2018),\n                      arXiv preprint arxiv:1711.02283.\n    '''\n\n    n_source = np.shape(M)[0]\n    n_target = np.shape(M)[1]\n    cur_alpha = np.zeros(n_source)\n    cur_beta = np.zeros(n_target)\n    for cur_iter in range(numItermax):\n        k = np.sqrt(cur_iter + 1)\n        batch_alpha = np.random.choice(n_source, batch_size, replace=False)\n        batch_beta = np.random.choice(n_target, batch_size, replace=False)\n        update_alpha, update_beta = batch_grad_dual(a, b, M, reg, cur_alpha,\n                                                    cur_beta, batch_size,\n                                                    batch_alpha, batch_beta)\n        cur_alpha[batch_alpha] += (lr / k) * update_alpha[batch_alpha]\n        cur_beta[batch_beta] += (lr / k) * update_beta[batch_beta]\n\n    return cur_alpha, cur_beta", "response": "This function computes the sgd algorithm to solve the regularized discrete measures and optimal transport dual problem."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef solve_dual_entropic(a, b, M, reg, batch_size, numItermax=10000, lr=1,\n                        log=False):\n    '''\n    Compute the transportation matrix to solve the regularized discrete measures\n        optimal transport dual problem\n\n    The function solves the following optimization problem:\n\n    .. math::\n        \\gamma = arg\\min_\\gamma <\\gamma,M>_F + reg\\cdot\\Omega(\\gamma)\n\n        s.t. \\gamma 1 = a\n\n             \\gamma^T 1= b\n\n             \\gamma \\geq 0\n\n    Where :\n\n    - M is the (ns,nt) metric cost matrix\n    - :math:`\\Omega` is the entropic regularization term :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n    - a and b are source and target weights (sum to 1)\n\n    Parameters\n    ----------\n\n    a : np.ndarray(ns,)\n        source measure\n    b : np.ndarray(nt,)\n        target measure\n    M : np.ndarray(ns, nt)\n        cost matrix\n    reg : float number\n        Regularization term > 0\n    batch_size : int number\n        size of the batch\n    numItermax : int number\n        number of iteration\n    lr : float number\n        learning rate\n    log : bool, optional\n        record log if True\n\n    Returns\n    -------\n\n    pi : np.ndarray(ns, nt)\n        transportation matrix\n    log : dict\n        log dictionary return only if log==True in parameters\n\n    Examples\n    --------\n\n    >>> n_source = 7\n    >>> n_target = 4\n    >>> reg = 1\n    >>> numItermax = 20000\n    >>> lr = 0.1\n    >>> batch_size = 3\n    >>> log = True\n    >>> a = ot.utils.unif(n_source)\n    >>> b = ot.utils.unif(n_target)\n    >>> rng = np.random.RandomState(0)\n    >>> X_source = rng.randn(n_source, 2)\n    >>> Y_target = rng.randn(n_target, 2)\n    >>> M = ot.dist(X_source, Y_target)\n    >>> sgd_dual_pi, log = stochastic.solve_dual_entropic(a, b, M, reg,\n                                                            batch_size,\n                                                            numItermax, lr, log)\n    >>> print(log['alpha'], log['beta'])\n    >>> print(sgd_dual_pi)\n\n    References\n    ----------\n\n    [Seguy et al., 2018] :\n                    International Conference on Learning Representation (2018),\n                      arXiv preprint arxiv:1711.02283.\n    '''\n\n    opt_alpha, opt_beta = sgd_entropic_regularization(a, b, M, reg, batch_size,\n                                                      numItermax, lr)\n    pi = (np.exp((opt_alpha[:, None] + opt_beta[None, :] - M[:, :]) / reg) *\n          a[:, None] * b[None, :])\n    if log:\n        log = {}\n        log['alpha'] = opt_alpha\n        log['beta'] = opt_beta\n        return pi, log\n    else:\n        return pi", "response": "This function solves the regularized discrete measures\n    optimal transport dual problem."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef syntaxError(self, filename, msg, lineno, offset, text):\n        line = text.splitlines()[-1]\n        if offset is not None:\n            if sys.version_info < (3, 8):\n                offset = offset - (len(text) - len(line)) + 1\n            self._stderr.write('%s:%d:%d: %s\\n' %\n                               (filename, lineno, offset, msg))\n        else:\n            self._stderr.write('%s:%d: %s\\n' % (filename, lineno, msg))\n        self._stderr.write(line)\n        self._stderr.write('\\n')\n        if offset is not None:\n            self._stderr.write(re.sub(r'\\S', ' ', line[:offset - 1]) +\n                               \"^\\n\")", "response": "A method to print out a syntax error in the file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting a message to the stdout.", "response": "def flake(self, message):\n        \"\"\"\n        pyflakes found something wrong with the code.\n\n        @param: A L{pyflakes.messages.Message}.\n        \"\"\"\n        self._stdout.write(str(message))\n        self._stdout.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef counter(items):\n    results = {}\n    for item in items:\n        results[item] = results.get(item, 0) + 1\n    return results", "response": "Returns a list of items with a Counter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over all direct child nodes of node.", "response": "def iter_child_nodes(node, omit=None, _fields_order=_FieldsOrder()):\n    \"\"\"\n    Yield all direct child nodes of *node*, that is, all fields that\n    are nodes and all items of fields that are lists of nodes.\n\n    :param node:          AST node to be iterated upon\n    :param omit:          String or tuple of strings denoting the\n                          attributes of the node to be omitted from\n                          further parsing\n    :param _fields_order: Order of AST node fields\n    \"\"\"\n    for name in _fields_order[node.__class__]:\n        if omit and name in omit:\n            continue\n        field = getattr(node, name, None)\n        if isinstance(field, ast.AST):\n            yield field\n        elif isinstance(field, list):\n            for item in field:\n                yield item"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef source_statement(self):\n        if self._has_alias():\n            return 'import %s as %s' % (self.fullName, self.name)\n        else:\n            return 'import %s' % self.fullName", "response": "Generate a source statement equivalent to the import."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nschedule a function handler to be called just before completion.", "response": "def deferFunction(self, callable):\n        \"\"\"\n        Schedule a function handler to be called just before completion.\n\n        This is used for handling function bodies, which must be deferred\n        because code later in the file might modify the global scope. When\n        `callable` is called, the scope at the time this is called will be\n        restored, however it will contain any new bindings added to it.\n        \"\"\"\n        self._deferredFunctions.append((callable, self.scopeStack[:], self.offset))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deferAssignment(self, callable):\n        self._deferredAssignments.append((callable, self.scopeStack[:], self.offset))", "response": "Schedule an assignment handler to be called just after deferred\n        function handlers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the deferred callables in C { deferred }.", "response": "def runDeferred(self, deferred):\n        \"\"\"\n        Run the callables in C{deferred} using their associated scope stack.\n        \"\"\"\n        for handler, scope, offset in deferred:\n            self.scopeStack = scope\n            self.offset = offset\n            handler()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if dead scopes are available and report names in them.", "response": "def checkDeadScopes(self):\n        \"\"\"\n        Look at scopes which have been fully examined and report names in them\n        which were imported but unused.\n        \"\"\"\n        for scope in self.deadScopes:\n            # imports in classes are public members\n            if isinstance(scope, ClassScope):\n                continue\n\n            all_binding = scope.get('__all__')\n            if all_binding and not isinstance(all_binding, ExportBinding):\n                all_binding = None\n\n            if all_binding:\n                all_names = set(all_binding.names)\n                undefined = all_names.difference(scope)\n            else:\n                all_names = undefined = []\n\n            if undefined:\n                if not scope.importStarred and \\\n                   os.path.basename(self.filename) != '__init__.py':\n                    # Look for possible mistakes in the export list\n                    for name in undefined:\n                        self.report(messages.UndefinedExport,\n                                    scope['__all__'].source, name)\n\n                # mark all import '*' as used by the undefined in __all__\n                if scope.importStarred:\n                    from_list = []\n                    for binding in scope.values():\n                        if isinstance(binding, StarImportation):\n                            binding.used = all_binding\n                            from_list.append(binding.fullName)\n                    # report * usage, with a list of possible sources\n                    from_list = ', '.join(sorted(from_list))\n                    for name in undefined:\n                        self.report(messages.ImportStarUsage,\n                                    scope['__all__'].source, name, from_list)\n\n            # Look for imported names that aren't used.\n            for value in scope.values():\n                if isinstance(value, Importation):\n                    used = value.used or value.name in all_names\n                    if not used:\n                        messg = messages.UnusedImport\n                        self.report(messg, value.source, str(value))\n                    for node in value.redefined:\n                        if isinstance(self.getParent(node), FOR_TYPES):\n                            messg = messages.ImportShadowedByLoopVar\n                        elif used:\n                            continue\n                        else:\n                            messg = messages.RedefinedWhileUnused\n                        self.report(messg, node, value.name, value.source)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef differentForks(self, lnode, rnode):\n        ancestor = self.getCommonAncestor(lnode, rnode, self.root)\n        parts = getAlternatives(ancestor)\n        if parts:\n            for items in parts:\n                if self.descendantOf(lnode, items, ancestor) ^ \\\n                   self.descendantOf(rnode, items, ancestor):\n                    return True\n        return False", "response": "True if lnode and rnode are located on different forks of IF orTRY"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef addBinding(self, node, value):\n        # assert value.source in (node, node.parent):\n        for scope in self.scopeStack[::-1]:\n            if value.name in scope:\n                break\n        existing = scope.get(value.name)\n\n        if (existing and not isinstance(existing, Builtin) and\n                not self.differentForks(node, existing.source)):\n\n            parent_stmt = self.getParent(value.source)\n            if isinstance(existing, Importation) and isinstance(parent_stmt, FOR_TYPES):\n                self.report(messages.ImportShadowedByLoopVar,\n                            node, value.name, existing.source)\n\n            elif scope is self.scope:\n                if (isinstance(parent_stmt, ast.comprehension) and\n                        not isinstance(self.getParent(existing.source),\n                                       (FOR_TYPES, ast.comprehension))):\n                    self.report(messages.RedefinedInListComp,\n                                node, value.name, existing.source)\n                elif not existing.used and value.redefines(existing):\n                    if value.name != '_' or isinstance(existing, Importation):\n                        if not is_typing_overload(existing, self.scopeStack):\n                            self.report(messages.RedefinedWhileUnused,\n                                        node, value.name, existing.source)\n\n            elif isinstance(existing, Importation) and value.redefines(existing):\n                existing.redefined.append(node)\n\n        if value.name in self.scope:\n            # then assume the rebound name is used as a global or within a loop\n            value.used = self.scope[value.name].used\n\n        self.scope[value.name] = value", "response": "Called when a binding is added to the current scope."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines if the given node is a docstring.", "response": "def isDocstring(self, node):\n        \"\"\"\n        Determine if the given node is a docstring, as long as it is at the\n        correct place in the node tree.\n        \"\"\"\n        return isinstance(node, ast.Str) or (isinstance(node, ast.Expr) and\n                                             isinstance(node.value, ast.Str))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbind a node to the global scope.", "response": "def GLOBAL(self, node):\n        \"\"\"\n        Keep track of globals declarations.\n        \"\"\"\n        global_scope_index = 1 if self._in_doctest() else 0\n        global_scope = self.scopeStack[global_scope_index]\n\n        # Ignore 'global' statement in global scope.\n        if self.scope is not global_scope:\n\n            # One 'global' statement can bind multiple (comma-delimited) names.\n            for node_name in node.names:\n                node_value = Assignment(node_name, node)\n\n                # Remove UndefinedName messages already reported for this name.\n                # TODO: if the global is not used in this scope, it does not\n                # become a globally defined name.  See test_unused_global.\n                self.messages = [\n                    m for m in self.messages if not\n                    isinstance(m, messages.UndefinedName) or\n                    m.message_args[0] != node_name]\n\n                # Bind name to global scope if it doesn't exist already.\n                global_scope.setdefault(node_name, node_value)\n\n                # Bind name to non-global scopes, but as already \"used\".\n                node_value.used = (global_scope, node)\n                for scope in self.scopeStack[global_scope_index + 1:]:\n                    scope[node_name] = node_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef NAME(self, node):\n        # Locate the name in locals / function / globals scopes.\n        if isinstance(node.ctx, (ast.Load, ast.AugLoad)):\n            self.handleNodeLoad(node)\n            if (node.id == 'locals' and isinstance(self.scope, FunctionScope) and\n                    isinstance(node.parent, ast.Call)):\n                # we are doing locals() call in current scope\n                self.scope.usesLocals = True\n        elif isinstance(node.ctx, (ast.Store, ast.AugStore, ast.Param)):\n            self.handleNodeStore(node)\n        elif isinstance(node.ctx, ast.Del):\n            self.handleNodeDelete(node)\n        else:\n            # Unknown context\n            raise RuntimeError(\"Got impossible expression context: %r\" % (node.ctx,))", "response": "Handle occurrence of Name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CLASSDEF(self, node):\n        for deco in node.decorator_list:\n            self.handleNode(deco, node)\n        for baseNode in node.bases:\n            self.handleNode(baseNode, node)\n        if not PY2:\n            for keywordNode in node.keywords:\n                self.handleNode(keywordNode, node)\n        self.pushScope(ClassScope)\n        # doctest does not process doctest within a doctest\n        # classes within classes are processed.\n        if (self.withDoctest and\n                not self._in_doctest() and\n                not isinstance(self.scope, FunctionScope)):\n            self.deferFunction(lambda: self.handleDoctests(node))\n        for stmt in node.body:\n            self.handleNode(stmt, node)\n        self.popScope()\n        self.addBinding(node, ClassDefinition(node.name, node))", "response": "Check names used in a class definition, including its decorators, base\n        classes, and the body of its definition.  Additionally, add its name to\n        the current scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check(codeString, filename, reporter=None):\n    if reporter is None:\n        reporter = modReporter._makeDefaultReporter()\n    # First, compile into an AST and handle syntax errors.\n    try:\n        tree = ast.parse(codeString, filename=filename)\n    except SyntaxError:\n        value = sys.exc_info()[1]\n        msg = value.args[0]\n\n        (lineno, offset, text) = value.lineno, value.offset, value.text\n\n        if checker.PYPY:\n            if text is None:\n                lines = codeString.splitlines()\n                if len(lines) >= lineno:\n                    text = lines[lineno - 1]\n                    if sys.version_info >= (3, ) and isinstance(text, bytes):\n                        try:\n                            text = text.decode('ascii')\n                        except UnicodeDecodeError:\n                            text = None\n            offset -= 1\n\n        # If there's an encoding problem with the file, the text is None.\n        if text is None:\n            # Avoid using msg, since for the only known case, it contains a\n            # bogus message that claims the encoding the file declared was\n            # unknown.\n            reporter.unexpectedError(filename, 'problem decoding source')\n        else:\n            reporter.syntaxError(filename, msg, lineno, offset, text)\n        return 1\n    except Exception:\n        reporter.unexpectedError(filename, 'problem decoding source')\n        return 1\n    # Okay, it's syntactically valid.  Now check it.\n    file_tokens = checker.make_tokens(codeString)\n    w = checker.Checker(tree, file_tokens=file_tokens, filename=filename)\n    w.messages.sort(key=lambda m: m.lineno)\n    for warning in w.messages:\n        reporter.flake(warning)\n    return len(w.messages)", "response": "Check the given Python source for flakes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the given path for any errors and warnings detected.", "response": "def checkPath(filename, reporter=None):\n    \"\"\"\n    Check the given path, printing out any warnings detected.\n\n    @param reporter: A L{Reporter} instance, where errors and warnings will be\n        reported.\n\n    @return: the number of warnings printed\n    \"\"\"\n    if reporter is None:\n        reporter = modReporter._makeDefaultReporter()\n    try:\n        with open(filename, 'rb') as f:\n            codestr = f.read()\n    except IOError:\n        msg = sys.exc_info()[1]\n        reporter.unexpectedError(filename, msg.args[1])\n        return 1\n    return check(codestr, filename, reporter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isPythonFile(filename):\n    if filename.endswith('.py'):\n        return True\n\n    # Avoid obvious Emacs backup files\n    if filename.endswith(\"~\"):\n        return False\n\n    max_bytes = 128\n\n    try:\n        with open(filename, 'rb') as f:\n            text = f.read(max_bytes)\n            if not text:\n                return False\n    except IOError:\n        return False\n\n    first_line = text.splitlines()[0]\n    return PYTHON_SHEBANG_REGEX.match(first_line)", "response": "Return True if filename points to a Python file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over all Python source files in C { paths.", "response": "def iterSourceCode(paths):\n    \"\"\"\n    Iterate over all Python source files in C{paths}.\n\n    @param paths: A list of paths.  Directories will be recursed into and\n        any .py files found will be yielded.  Any non-directories will be\n        yielded as-is.\n    \"\"\"\n    for path in paths:\n        if os.path.isdir(path):\n            for dirpath, dirnames, filenames in os.walk(path):\n                for filename in filenames:\n                    full_path = os.path.join(dirpath, filename)\n                    if isPythonFile(full_path):\n                        yield full_path\n        else:\n            yield path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _exitOnSignal(sigName, message):\n    import signal\n\n    try:\n        sigNumber = getattr(signal, sigName)\n    except AttributeError:\n        # the signal constants defined in the signal module are defined by\n        # whether the C library supports them or not. So, SIGPIPE might not\n        # even be defined.\n        return\n\n    def handler(sig, f):\n        sys.exit(message)\n\n    try:\n        signal.signal(sigNumber, handler)\n    except ValueError:\n        # It's also possible the signal is defined, but then it's invalid. In\n        # this case, signal.signal raises ValueError.\n        pass", "response": "Handles a signal with sys. exit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a Model class find all related objects exploring children recursively returning a list of strings representing the related objects for select_related", "response": "def _get_subclasses_recurse(self, model, levels=None):\n        \"\"\"\n        Given a Model class, find all related objects, exploring children\n        recursively, returning a `list` of strings representing the\n        relations for select_related\n        \"\"\"\n        related_objects = [\n            f for f in model._meta.get_fields()\n            if isinstance(f, OneToOneRel)]\n\n        rels = [\n            rel for rel in related_objects\n            if isinstance(rel.field, OneToOneField)\n            and issubclass(rel.field.model, model)\n            and model is not rel.field.model\n            and rel.parent_link\n        ]\n\n        subclasses = []\n        if levels:\n            levels -= 1\n        for rel in rels:\n            if levels or levels is None:\n                for subclass in self._get_subclasses_recurse(\n                        rel.field.model, levels=levels):\n                    subclasses.append(\n                        rel.get_accessor_name() + LOOKUP_SEP + subclass)\n            subclasses.append(rel.get_accessor_name())\n        return subclasses"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_ancestors_path(self, model, levels=None):\n        if not issubclass(model, self.model):\n            raise ValueError(\n                \"%r is not a subclass of %r\" % (model, self.model))\n\n        ancestry = []\n        # should be a OneToOneField or None\n        parent_link = model._meta.get_ancestor_link(self.model)\n        if levels:\n            levels -= 1\n        while parent_link is not None:\n            related = parent_link.remote_field\n            ancestry.insert(0, related.get_accessor_name())\n            if levels or levels is None:\n                parent_model = related.model\n                parent_link = parent_model._meta.get_ancestor_link(\n                    self.model)\n            else:\n                parent_link = None\n        return LOOKUP_SEP.join(ancestry)", "response": "Returns the path to the ancestry of the given model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a queryset limited to not removed entries.", "response": "def get_queryset(self):\n        \"\"\"\n        Return queryset limited to not removed entries.\n        \"\"\"\n        kwargs = {'model': self.model, 'using': self._db}\n        if hasattr(self, '_hints'):\n            kwargs['hints'] = self._hints\n\n        return self._queryset_class(**kwargs).filter(is_removed=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\njoining one queryset together with another using a temporary table.", "response": "def join(self, qs=None):\n        '''\n        Join one queryset together with another using a temporary table. If\n        no queryset is used, it will use the current queryset and join that\n        to itself.\n\n        `Join` either uses the current queryset and effectively does a self-join to\n        create a new limited queryset OR it uses a querset given by the user.\n\n        The model of a given queryset needs to contain a valid foreign key to\n        the current queryset to perform a join. A new queryset is then created.\n        '''\n        to_field = 'id'\n\n        if qs:\n            fk = [\n                fk for fk in qs.model._meta.fields\n                if getattr(fk, 'related_model', None) == self.model\n            ]\n            fk = fk[0] if fk else None\n            model_set = '{}_set'.format(self.model.__name__.lower())\n            key = fk or getattr(qs.model, model_set, None)\n\n            if not key:\n                raise ValueError('QuerySet is not related to current model')\n\n            try:\n                fk_column = key.column\n            except AttributeError:\n                fk_column = 'id'\n                to_field = key.field.column\n\n            qs = qs.only(fk_column)\n            # if we give a qs we need to keep the model qs to not lose anything\n            new_qs = self\n        else:\n            fk_column = 'id'\n            qs = self.only(fk_column)\n            new_qs = self.model.objects.all()\n\n        TABLE_NAME = 'temp_stuff'\n        query = self.get_quoted_query(qs.query)\n        sql = '''\n            DROP TABLE IF EXISTS {table_name};\n            DROP INDEX IF EXISTS {table_name}_id;\n            CREATE TEMPORARY TABLE {table_name} AS {query};\n            CREATE INDEX {table_name}_{fk_column} ON {table_name} ({fk_column});\n        '''.format(table_name=TABLE_NAME, fk_column=fk_column, query=str(query))\n\n        with connection.cursor() as cursor:\n            cursor.execute(sql)\n\n        class TempModel(models.Model):\n            temp_key = models.ForeignKey(\n                self.model,\n                on_delete=models.DO_NOTHING,\n                db_column=fk_column,\n                to_field=to_field\n            )\n\n            class Meta:\n                managed = False\n                db_table = TABLE_NAME\n\n        conn = Join(\n            table_name=TempModel._meta.db_table,\n            parent_alias=new_qs.query.get_initial_alias(),\n            table_alias=None,\n            join_type='INNER JOIN',\n            join_field=self.model.tempmodel_set.rel,\n            nullable=False\n        )\n        new_qs.query.join(conn, reuse=None)\n        return new_qs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef current(self, fields=None):\n        if fields is None:\n            deferred_fields = self.deferred_fields\n            if deferred_fields:\n                fields = [\n                    field for field in self.fields\n                    if field not in deferred_fields\n                ]\n            else:\n                fields = self.fields\n\n        return dict((f, self.get_field_value(f)) for f in fields)", "response": "Returns dict of current values for all tracked fields"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_changed(self, field):\n        if field in self.fields:\n            # deferred fields haven't changed\n            if field in self.deferred_fields and field not in self.instance.__dict__:\n                return False\n            return self.previous(field) != self.get_field_value(field)\n        else:\n            raise FieldError('field \"%s\" not tracked' % field)", "response": "Returns True if the value of the given field has changed from currently saved value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns currently saved value of given field", "response": "def previous(self, field):\n        \"\"\"Returns currently saved value of given field\"\"\"\n\n        # handle deferred fields that have not yet been loaded from the database\n        if self.instance.pk and field in self.deferred_fields and field not in self.saved_data:\n\n            # if the field has not been assigned locally, simply fetch and un-defer the value\n            if field not in self.instance.__dict__:\n                self.get_field_value(field)\n\n            # if the field has been assigned locally, store the local value, fetch the database value,\n            # store database value to saved_data, and restore the local value\n            else:\n                current_value = self.get_field_value(field)\n                self.instance.refresh_from_db(fields=[field])\n                self.saved_data[field] = deepcopy(self.get_field_value(field))\n                setattr(self.instance, self.field_map[field], current_value)\n\n        return self.saved_data.get(field)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns dict of fields that changed since save", "response": "def changed(self):\n        \"\"\"Returns dict of fields that changed since save (with old values)\"\"\"\n        return dict(\n            (field, self.previous(field))\n            for field in self.fields\n            if self.has_changed(field)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns dict mapping fields names to model attribute names", "response": "def get_field_map(self, cls):\n        \"\"\"Returns dict mapping fields names to model attribute names\"\"\"\n        field_map = dict((field, field) for field in self.fields)\n        all_fields = dict((f.name, f.attname) for f in cls._meta.fields)\n        field_map.update(**dict((k, v) for (k, v) in all_fields.items()\n                                if k in field_map))\n        return field_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_changed(self, field):\n        if not self.instance.pk:\n            return True\n        elif field in self.saved_data:\n            return self.previous(field) != self.get_field_value(field)\n        else:\n            raise FieldError('field \"%s\" not tracked' % field)", "response": "Returns True if the value of the given field has changed from currently saved value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef changed(self):\n        if not self.instance.pk:\n            return {}\n        saved = self.saved_data.items()\n        current = self.current()\n        return dict((k, v) for k, v in saved if v != current[k])", "response": "Returns dict of fields that changed since save"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_status_query_managers(sender, **kwargs):\n    if not issubclass(sender, StatusModel):\n        return\n\n    if django.VERSION >= (1, 10):\n        # First, get current manager name...\n        default_manager = sender._meta.default_manager\n\n    for value, display in getattr(sender, 'STATUS', ()):\n        if _field_exists(sender, value):\n            raise ImproperlyConfigured(\n                \"StatusModel: Model '%s' has a field named '%s' which \"\n                \"conflicts with a status of the same name.\"\n                % (sender.__name__, value)\n            )\n        sender.add_to_class(value, QueryManager(status=value))\n\n    if django.VERSION >= (1, 10):\n        # ...then, put it back, as add_to_class is modifying the default manager!\n        sender._meta.default_manager_name = default_manager.name", "response": "Add a Querymanager for each status item dynamically."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a QueryManager for a specific timeframe.", "response": "def add_timeframed_query_manager(sender, **kwargs):\n    \"\"\"\n    Add a QueryManager for a specific timeframe.\n\n    \"\"\"\n    if not issubclass(sender, TimeFramedModel):\n        return\n    if _field_exists(sender, 'timeframed'):\n        raise ImproperlyConfigured(\n            \"Model '%s' has a field named 'timeframed' \"\n            \"which conflicts with the TimeFramedModel manager.\"\n            % sender.__name__\n        )\n    sender.add_to_class('timeframed', QueryManager(\n        (models.Q(start__lte=now) | models.Q(start__isnull=True)) &\n        (models.Q(end__gte=now) | models.Q(end__isnull=True))\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, using=None, soft=True, *args, **kwargs):\n        if soft:\n            self.is_removed = True\n            self.save(using=using)\n        else:\n            return super(SoftDeletableModel, self).delete(using=using, *args, **kwargs)", "response": "Delete the related object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_text(path, language='fra'):\n    import subprocess\n    from distutils import spawn\n    import tempfile\n    import time\n\n    # Check for dependencies. Needs Tesseract and Imagemagick installed.\n    if not spawn.find_executable('tesseract'):\n        raise EnvironmentError('tesseract not installed.')\n    if not spawn.find_executable('convert'):\n        raise EnvironmentError('imagemagick not installed.')\n    if not spawn.find_executable('gs'):\n        raise EnvironmentError('ghostscript not installed.')\n\n    with tempfile.NamedTemporaryFile(suffix='.tiff') as tf:\n        # Step 1: Convert to TIFF\n        gs_cmd = [\n            'gs',\n            '-q',\n            '-dNOPAUSE',\n            '-r600x600',\n            '-sDEVICE=tiff24nc',\n            '-sOutputFile=' + tf.name,\n            path,\n            '-c',\n            'quit',\n        ]\n        subprocess.Popen(gs_cmd)\n        time.sleep(3)\n\n        # Step 2: Enhance TIFF\n        magick_cmd = [\n            'convert',\n            tf.name,\n            '-colorspace',\n            'gray',\n            '-type',\n            'grayscale',\n            '-contrast-stretch',\n            '0',\n            '-sharpen',\n            '0x1',\n            'tiff:-',\n        ]\n\n        p1 = subprocess.Popen(magick_cmd, stdout=subprocess.PIPE)\n\n        tess_cmd = ['tesseract', '-l', language, '--oem', '1', '--psm', '3', 'stdin', 'stdout']\n        p2 = subprocess.Popen(tess_cmd, stdin=p1.stdout, stdout=subprocess.PIPE)\n\n        out, err = p2.communicate()\n\n        extracted_str = out\n\n        return extracted_str", "response": "Wraps Tesseract 4 OCR with custom language model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_text(path, bucket_name='cloud-vision-84893', language='fr'):\n\n    \"\"\"OCR with PDF/TIFF as source files on GCS\"\"\"\n    import os\n    from google.cloud import vision\n    from google.cloud import storage\n    from google.protobuf import json_format\n\n    # Supported mime_types are: 'application/pdf' and 'image/tiff'\n    mime_type = 'application/pdf'\n\n    path_dir, filename = os.path.split(path)\n    result_blob_basename = filename.replace('.pdf', '').replace('.PDF', '')\n    result_blob_name = result_blob_basename + '/output-1-to-1.json'\n    result_blob_uri = 'gs://{}/{}/'.format(bucket_name, result_blob_basename)\n    input_blob_uri = 'gs://{}/{}'.format(bucket_name, filename)\n\n    # Upload file to gcloud if it doesn't exist yet\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    if bucket.get_blob(filename) is None:\n        blob = bucket.blob(filename)\n        blob.upload_from_filename(path)\n\n    # See if result already exists\n    # TODO: upload as hash, not filename\n    result_blob = bucket.get_blob(result_blob_name)\n    if result_blob is None:\n\n        # How many pages should be grouped into each json output file.\n        batch_size = 10\n\n        client = vision.ImageAnnotatorClient()\n\n        feature = vision.types.Feature(type=vision.enums.Feature.Type.DOCUMENT_TEXT_DETECTION)\n\n        gcs_source = vision.types.GcsSource(uri=input_blob_uri)\n        input_config = vision.types.InputConfig(gcs_source=gcs_source, mime_type=mime_type)\n\n        gcs_destination = vision.types.GcsDestination(uri=result_blob_uri)\n        output_config = vision.types.OutputConfig(\n            gcs_destination=gcs_destination, batch_size=batch_size\n        )\n\n        async_request = vision.types.AsyncAnnotateFileRequest(\n            features=[feature], input_config=input_config, output_config=output_config\n        )\n\n        operation = client.async_batch_annotate_files(requests=[async_request])\n\n        print('Waiting for the operation to finish.')\n        operation.result(timeout=180)\n\n    # Get result after OCR is completed\n    result_blob = bucket.get_blob(result_blob_name)\n\n    json_string = result_blob.download_as_string()\n    response = json_format.Parse(json_string, vision.types.AnnotateFileResponse())\n\n    # The actual response for the first page of the input file.\n    first_page_response = response.responses[0]\n    annotation = first_page_response.full_text_annotation\n\n    return annotation.text.encode('utf-8')", "response": "Sends PDF files to Google Cloud Vision for OCR."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting extracted fields to csv file", "response": "def write_to_file(data, path):\n    \"\"\"Export extracted fields to csv\n\n    Appends .csv to path if missing and generates csv file in specified directory, if not then in root\n\n    Parameters\n    ----------\n    data : dict\n        Dictionary of extracted fields\n    path : str\n        directory to save generated csv file\n\n    Notes\n    ----\n    Do give file name to the function parameter path.\n\n    Examples\n    --------\n        >>> from invoice2data.output import to_csv\n        >>> to_csv.write_to_file(data, \"/exported_csv/invoice.csv\")\n        >>> to_csv.write_to_file(data, \"invoice.csv\")\n\n    \"\"\"\n    if path.endswith('.csv'):\n        filename = path\n    else:\n        filename = path + '.csv'\n\n    if sys.version_info[0] < 3:\n        openfile = open(filename, \"wb\")\n    else:\n        openfile = open(filename, \"w\", newline='')\n\n    with openfile as csv_file:\n        writer = csv.writer(csv_file, delimiter=',')\n\n        for line in data:\n            first_row = []\n            for k, v in line.items():\n                first_row.append(k)\n\n        writer.writerow(first_row)\n        for line in data:\n            csv_items = []\n            for k, v in line.items():\n                # first_row.append(k)\n                if k == 'date':\n                    v = v.strftime('%d/%m/%Y')\n                csv_items.append(v)\n            writer.writerow(csv_items)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_text(path):\n    import subprocess\n    from distutils import spawn\n\n    # Check for dependencies. Needs Tesseract and Imagemagick installed.\n    if not spawn.find_executable('tesseract'):\n        raise EnvironmentError('tesseract not installed.')\n    if not spawn.find_executable('convert'):\n        raise EnvironmentError('imagemagick not installed.')\n\n    # convert = \"convert -density 350 %s -depth 8 tiff:-\" % (path)\n    convert = ['convert', '-density', '350', path, '-depth', '8', 'png:-']\n    p1 = subprocess.Popen(convert, stdout=subprocess.PIPE)\n\n    tess = ['tesseract', 'stdin', 'stdout']\n    p2 = subprocess.Popen(tess, stdin=p1.stdout, stdout=subprocess.PIPE)\n\n    out, err = p2.communicate()\n\n    extracted_str = out\n\n    return extracted_str", "response": "Wraps Tesseract OCR to text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntrying to extract tables from an invoice", "response": "def extract(self, content, output):\n    \"\"\"Try to extract tables from an invoice\"\"\"\n\n    for table in self['tables']:\n\n        # First apply default options.\n        plugin_settings = DEFAULT_OPTIONS.copy()\n        plugin_settings.update(table)\n        table = plugin_settings\n\n        # Validate settings\n        assert 'start' in table, 'Table start regex missing'\n        assert 'end' in table, 'Table end regex missing'\n        assert 'body' in table, 'Table body regex missing'\n\n        start = re.search(table['start'], content)\n        end = re.search(table['end'], content)\n\n        if not start or not end:\n            logger.warning('no table body found - start %s, end %s', start, end)\n            continue\n\n        table_body = content[start.end(): end.start()]\n\n        for line in re.split(table['line_separator'], table_body):\n            # if the line has empty lines in it , skip them\n            if not line.strip('').strip('\\n') or not line:\n                continue\n\n            match = re.search(table['body'], line)\n            if match:\n                for field, value in match.groupdict().items():\n                    # If a field name already exists, do not overwrite it\n                    if field in output:\n                        continue\n\n                    if field.startswith('date') or field.endswith('date'):\n                        output[field] = self.parse_date(value)\n                        if not output[field]:\n                            logger.error(\"Date parsing failed on date '%s'\", value)\n                            return None\n                    elif field.startswith('amount'):\n                        output[field] = self.parse_number(value)\n                    else:\n                        output[field] = value\n            logger.debug('ignoring *%s* because it doesn\\'t match anything', line)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_text(path):\n    import subprocess\n    from distutils import spawn  # py2 compat\n\n    if spawn.find_executable(\"pdftotext\"):  # shutil.which('pdftotext'):\n        out, err = subprocess.Popen(\n            [\"pdftotext\", '-layout', '-enc', 'UTF-8', path, '-'], stdout=subprocess.PIPE\n        ).communicate()\n        return out\n    else:\n        raise EnvironmentError(\n            'pdftotext not installed. Can be downloaded from https://poppler.freedesktop.org/'\n        )", "response": "Wrapper around Poppler pdftotext.\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare the raw string and do transformations.", "response": "def prepare_input(self, extracted_str):\n        \"\"\"\n        Input raw string and do transformations, as set in template file.\n        \"\"\"\n\n        # Remove withspace\n        if self.options['remove_whitespace']:\n            optimized_str = re.sub(' +', '', extracted_str)\n        else:\n            optimized_str = extracted_str\n\n        # Remove accents\n        if self.options['remove_accents']:\n            optimized_str = unidecode(optimized_str)\n\n        # convert to lower case\n        if self.options['lowercase']:\n            optimized_str = optimized_str.lower()\n\n        # specific replace\n        for replace in self.options['replace']:\n            assert len(replace) == 2, 'A replace should be a list of 2 items'\n            optimized_str = optimized_str.replace(replace[0], replace[1])\n\n        return optimized_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsees if string matches keywords set in template file", "response": "def matches_input(self, optimized_str):\n        \"\"\"See if string matches keywords set in template file\"\"\"\n\n        if all([keyword in optimized_str for keyword in self['keywords']]):\n            logger.debug('Matched template %s', self['template_name'])\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing date and returns date after parsing", "response": "def parse_date(self, value):\n        \"\"\"Parses date and returns date after parsing\"\"\"\n        res = dateparser.parse(\n            value, date_formats=self.options['date_formats'], languages=self.options['languages']\n        )\n        logger.debug(\"result of date parsing=%s\", res)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract(self, optimized_str):\n\n        logger.debug('START optimized_str ========================')\n        logger.debug(optimized_str)\n        logger.debug('END optimized_str ==========================')\n        logger.debug(\n            'Date parsing: languages=%s date_formats=%s',\n            self.options['languages'],\n            self.options['date_formats'],\n        )\n        logger.debug('Float parsing: decimal separator=%s', self.options['decimal_separator'])\n        logger.debug(\"keywords=%s\", self['keywords'])\n        logger.debug(self.options)\n\n        # Try to find data for each field.\n        output = {}\n        output['issuer'] = self['issuer']\n\n        for k, v in self['fields'].items():\n            if k.startswith('static_'):\n                logger.debug(\"field=%s | static value=%s\", k, v)\n                output[k.replace('static_', '')] = v\n            else:\n                logger.debug(\"field=%s | regexp=%s\", k, v)\n\n                sum_field = False\n                if k.startswith('sum_amount') and type(v) is list:\n                    k = k[4:]  # remove 'sum_' prefix\n                    sum_field = True\n                # Fields can have multiple expressions\n                if type(v) is list:\n                    res_find = []\n                    for v_option in v:\n                        res_val = re.findall(v_option, optimized_str)\n                        if res_val:\n                            if sum_field:\n                                res_find += res_val\n                            else:\n                                res_find.extend(res_val)\n                else:\n                    res_find = re.findall(v, optimized_str)\n                if res_find:\n                    logger.debug(\"res_find=%s\", res_find)\n                    if k.startswith('date') or k.endswith('date'):\n                        output[k] = self.parse_date(res_find[0])\n                        if not output[k]:\n                            logger.error(\"Date parsing failed on date '%s'\", res_find[0])\n                            return None\n                    elif k.startswith('amount'):\n                        if sum_field:\n                            output[k] = 0\n                            for amount_to_parse in res_find:\n                                output[k] += self.parse_number(amount_to_parse)\n                        else:\n                            output[k] = self.parse_number(res_find[0])\n                    else:\n                        res_find = list(set(res_find))\n                        if len(res_find) == 1:\n                            output[k] = res_find[0]\n                        else:\n                            output[k] = res_find\n                else:\n                    logger.warning(\"regexp for field %s didn't match\", k)\n\n        output['currency'] = self.options['currency']\n\n        # Run plugins:\n        for plugin_keyword, plugin_func in PLUGIN_MAPPING.items():\n            if plugin_keyword in self.keys():\n                plugin_func.extract(self, optimized_str, output)\n\n        # If required fields were found, return output, else log error.\n        if 'required_fields' not in self.keys():\n            required_fields = ['date', 'amount', 'invoice_number', 'issuer']\n        else:\n            required_fields = []\n            for v in self['required_fields']:\n                required_fields.append(v)\n\n        if set(required_fields).issubset(output.keys()):\n            output['desc'] = 'Invoice from %s' % (self['issuer'])\n            logger.debug(output)\n            return output\n        else:\n            fields = list(set(output.keys()))\n            logger.error(\n                'Unable to match all required fields. '\n                'The required fields are: {0}. '\n                'Output contains the following fields: {1}.'.format(required_fields, fields)\n            )\n            return None", "response": "Given a template file and a string extract matching data fields."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_to_file(data, path):\n    if path.endswith('.json'):\n        filename = path\n    else:\n        filename = path + '.json'\n\n    with codecs.open(filename, \"w\", encoding='utf-8') as json_file:\n        for line in data:\n            line['date'] = line['date'].strftime('%d/%m/%Y')\n        print(type(json))\n        print(json)\n        json.dump(\n            data, json_file, indent=4, sort_keys=True, default=myconverter, ensure_ascii=False\n        )", "response": "Export extracted fields to json file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_data(invoicefile, templates=None, input_module=pdftotext):\n    if templates is None:\n        templates = read_templates()\n\n    # print(templates[0])\n    extracted_str = input_module.to_text(invoicefile).decode('utf-8')\n\n    logger.debug('START pdftotext result ===========================')\n    logger.debug(extracted_str)\n    logger.debug('END pdftotext result =============================')\n\n    logger.debug('Testing {} template files'.format(len(templates)))\n    for t in templates:\n        optimized_str = t.prepare_input(extracted_str)\n\n        if t.matches_input(optimized_str):\n            return t.extract(optimized_str)\n\n    logger.error('No template for %s', invoicefile)\n    return False", "response": "Extracts structured data from a PDF file or image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_parser():\n\n    parser = argparse.ArgumentParser(\n        description='Extract structured data from PDF files and save to CSV or JSON.'\n    )\n\n    parser.add_argument(\n        '--input-reader',\n        choices=input_mapping.keys(),\n        default='pdftotext',\n        help='Choose text extraction function. Default: pdftotext',\n    )\n\n    parser.add_argument(\n        '--output-format',\n        choices=output_mapping.keys(),\n        default='none',\n        help='Choose output format. Default: none',\n    )\n\n    parser.add_argument(\n        '--output-name',\n        '-o',\n        dest='output_name',\n        default='invoices-output',\n        help='Custom name for output file. Extension is added based on chosen format.',\n    )\n\n    parser.add_argument(\n        '--debug', dest='debug', action='store_true', help='Enable debug information.'\n    )\n\n    parser.add_argument(\n        '--copy', '-c', dest='copy', help='Copy and rename processed PDFs to specified folder.'\n    )\n\n    parser.add_argument(\n        '--move', '-m', dest='move', help='Move and rename processed PDFs to specified folder.'\n    )\n\n    parser.add_argument(\n        '--filename-format',\n        dest='filename',\n        default=\"{date} {invoice_number} {desc}.pdf\",\n        help='Filename format to use when moving or copying processed PDFs.'\n             'Default: \"{date} {invoice_number} {desc}.pdf\"',\n    )\n\n    parser.add_argument(\n        '--template-folder',\n        '-t',\n        dest='template_folder',\n        help='Folder containing invoice templates in yml file. Always adds built-in templates.',\n    )\n\n    parser.add_argument(\n        '--exclude-built-in-templates',\n        dest='exclude_built_in_templates',\n        default=False,\n        help='Ignore built-in templates.',\n        action=\"store_true\",\n    )\n\n    parser.add_argument(\n        'input_files', type=argparse.FileType('r'), nargs='+', help='File or directory to analyze.'\n    )\n\n    return parser", "response": "Creates argument parser for the invoice command line"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(args=None):\n    if args is None:\n        parser = create_parser()\n        args = parser.parse_args()\n\n    if args.debug:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)\n\n    input_module = input_mapping[args.input_reader]\n    output_module = output_mapping[args.output_format]\n\n    templates = []\n    # Load templates from external folder if set.\n    if args.template_folder:\n        templates += read_templates(os.path.abspath(args.template_folder))\n\n    # Load internal templates, if not disabled.\n    if not args.exclude_built_in_templates:\n        templates += read_templates()\n    output = []\n    for f in args.input_files:\n        res = extract_data(f.name, templates=templates, input_module=input_module)\n        if res:\n            logger.info(res)\n            output.append(res)\n            if args.copy:\n                filename = args.filename.format(\n                    date=res['date'].strftime('%Y-%m-%d'),\n                    invoice_number=res['invoice_number'],\n                    desc=res['desc'],\n                )\n                shutil.copyfile(f.name, join(args.copy, filename))\n            if args.move:\n                filename = args.filename.format(\n                    date=res['date'].strftime('%Y-%m-%d'),\n                    invoice_number=res['invoice_number'],\n                    desc=res['desc'],\n                )\n                shutil.move(f.name, join(args.move, filename))\n        f.close()\n\n    if output_module is not None:\n        output_module.write_to_file(output, args.output_name)", "response": "Take folder or single file and analyze each."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_to_file(data, path):\n\n    if path.endswith('.xml'):\n        filename = path\n    else:\n        filename = path + '.xml'\n\n    tag_data = ET.Element('data')\n    xml_file = open(filename, \"w\")\n    i = 0\n    for line in data:\n        i += 1\n        tag_item = ET.SubElement(tag_data, 'item')\n        tag_date = ET.SubElement(tag_item, 'date')\n        tag_desc = ET.SubElement(tag_item, 'desc')\n        tag_currency = ET.SubElement(tag_item, 'currency')\n        tag_amount = ET.SubElement(tag_item, 'amount')\n        tag_item.set('id', str(i))\n        tag_date.text = line['date'].strftime('%d/%m/%Y')\n        tag_desc.text = line['desc']\n        tag_currency.text = line['currency']\n        tag_amount.text = str(line['amount'])\n\n    xml_file.write(prettify(tag_data))\n    xml_file.close()", "response": "Export extracted fields to xml file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads yaml templates from folder and return list of dicts.", "response": "def read_templates(folder=None):\n    \"\"\"\n    Load yaml templates from template folder. Return list of dicts.\n\n    Use built-in templates if no folder is set.\n\n    Parameters\n    ----------\n    folder : str\n        user defined folder where they stores their files, if None uses built-in templates\n\n    Returns\n    -------\n    output : Instance of `InvoiceTemplate`\n        template which match based on keywords\n\n    Examples\n    --------\n\n    >>> read_template(\"home/duskybomb/invoice-templates/\")\n    InvoiceTemplate([('issuer', 'OYO'), ('fields', OrderedDict([('amount', 'GrandTotalRs(\\\\d+)'),\n    ('date', 'Date:(\\\\d{1,2}\\\\/\\\\d{1,2}\\\\/\\\\d{1,4})'), ('invoice_number', '([A-Z0-9]+)CashatHotel')])),\n    ('keywords', ['OYO', 'Oravel', 'Stays']), ('options', OrderedDict([('currency', 'INR'), ('decimal_separator', '.'),\n    ('remove_whitespace', True)])), ('template_name', 'com.oyo.invoice.yml')])\n\n    After reading the template you can use the result as an instance of `InvoiceTemplate` to extract fields from\n    `extract_data()`\n\n    >>> my_template = InvoiceTemplate([('issuer', 'OYO'), ('fields', OrderedDict([('amount', 'GrandTotalRs(\\\\d+)'),\n    ('date', 'Date:(\\\\d{1,2}\\\\/\\\\d{1,2}\\\\/\\\\d{1,4})'), ('invoice_number', '([A-Z0-9]+)CashatHotel')])),\n    ('keywords', ['OYO', 'Oravel', 'Stays']), ('options', OrderedDict([('currency', 'INR'), ('decimal_separator', '.'),\n    ('remove_whitespace', True)])), ('template_name', 'com.oyo.invoice.yml')])\n    >>> extract_data(\"invoice2data/test/pdfs/oyo.pdf\", my_template, pdftotext)\n    {'issuer': 'OYO', 'amount': 1939.0, 'date': datetime.datetime(2017, 12, 31, 0, 0), 'invoice_number': 'IBZY2087',\n     'currency': 'INR', 'desc': 'Invoice IBZY2087 from OYO'}\n\n    \"\"\"\n\n    output = []\n\n    if folder is None:\n        folder = pkg_resources.resource_filename(__name__, 'templates')\n\n    for path, subdirs, files in os.walk(folder):\n        for name in sorted(files):\n            if name.endswith('.yml'):\n                with open(os.path.join(path, name), 'rb') as f:\n                    encoding = chardet.detect(f.read())['encoding']\n                with codecs.open(os.path.join(path, name), encoding=encoding) as template_file:\n                    tpl = ordered_load(template_file.read())\n                tpl['template_name'] = name\n\n                # Test if all required fields are in template:\n                assert 'keywords' in tpl.keys(), 'Missing keywords field.'\n\n                # Keywords as list, if only one.\n                if type(tpl['keywords']) is not list:\n                    tpl['keywords'] = [tpl['keywords']]\n\n                output.append(InvoiceTemplate(tpl))\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_text(path):\n\n    try:\n        # python 2\n        from StringIO import StringIO\n        import sys\n\n        reload(sys)  # noqa: F821\n        sys.setdefaultencoding('utf8')\n    except ImportError:\n        from io import StringIO\n\n    from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n    from pdfminer.converter import TextConverter\n    from pdfminer.layout import LAParams\n    from pdfminer.pdfpage import PDFPage\n\n    rsrcmgr = PDFResourceManager()\n    retstr = StringIO()\n    codec = 'utf-8'\n    laparams = LAParams()\n    laparams.all_texts = True\n    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n    with open(path, 'rb') as fp:\n        interpreter = PDFPageInterpreter(rsrcmgr, device)\n        password = \"\"\n        maxpages = 0\n        caching = True\n        pagenos = set()\n        pages = PDFPage.get_pages(\n            fp,\n            pagenos,\n            maxpages=maxpages,\n            password=password,\n            caching=caching,\n            check_extractable=True,\n        )\n        for page in pages:\n            interpreter.process_page(page)\n    device.close()\n    str = retstr.getvalue()\n    retstr.close()\n    return str.encode('utf-8')", "response": "Wrapper around pdfminer. pdfpage. PDFPage. to_text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract(self, content, output):\n\n    # First apply default options.\n    plugin_settings = DEFAULT_OPTIONS.copy()\n    plugin_settings.update(self['lines'])\n    self['lines'] = plugin_settings\n\n    # Validate settings\n    assert 'start' in self['lines'], 'Lines start regex missing'\n    assert 'end' in self['lines'], 'Lines end regex missing'\n    assert 'line' in self['lines'], 'Line regex missing'\n\n    start = re.search(self['lines']['start'], content)\n    end = re.search(self['lines']['end'], content)\n    if not start or not end:\n        logger.warning('no lines found - start %s, end %s', start, end)\n        return\n    content = content[start.end(): end.start()]\n    lines = []\n    current_row = {}\n    if 'first_line' not in self['lines'] and 'last_line' not in self['lines']:\n        self['lines']['first_line'] = self['lines']['line']\n    for line in re.split(self['lines']['line_separator'], content):\n        # if the line has empty lines in it , skip them\n        if not line.strip('').strip('\\n') or not line:\n            continue\n        if 'first_line' in self['lines']:\n            match = re.search(self['lines']['first_line'], line)\n            if match:\n                if 'last_line' not in self['lines']:\n                    if current_row:\n                        lines.append(current_row)\n                    current_row = {}\n                if current_row:\n                    lines.append(current_row)\n                current_row = {\n                    field: value.strip() if value else ''\n                    for field, value in match.groupdict().items()\n                }\n                continue\n        if 'last_line' in self['lines']:\n            match = re.search(self['lines']['last_line'], line)\n            if match:\n                for field, value in match.groupdict().items():\n                    current_row[field] = '%s%s%s' % (\n                        current_row.get(field, ''),\n                        current_row.get(field, '') and '\\n' or '',\n                        value.strip() if value else '',\n                    )\n                if current_row:\n                    lines.append(current_row)\n                current_row = {}\n                continue\n        match = re.search(self['lines']['line'], line)\n        if match:\n            for field, value in match.groupdict().items():\n                current_row[field] = '%s%s%s' % (\n                    current_row.get(field, ''),\n                    current_row.get(field, '') and '\\n' or '',\n                    value.strip() if value else '',\n                )\n            continue\n        logger.debug('ignoring *%s* because it doesn\\'t match anything', line)\n    if current_row:\n        lines.append(current_row)\n\n    types = self['lines'].get('types', [])\n    for row in lines:\n        for name in row.keys():\n            if name in types:\n                row[name] = self.coerce_type(row[name], types[name])\n\n    if lines:\n        output['lines'] = lines", "response": "Try to extract lines from the invoice"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef analyze(problem, X, Y, second_order=False, print_to_console=False,\n            seed=None):\n    \"\"\"Perform a fractional factorial analysis\n\n    Returns a dictionary with keys 'ME' (main effect) and 'IE' (interaction\n    effect). The techniques bulks out the number of parameters with dummy\n    parameters to the nearest 2**n.  Any results involving dummy parameters\n    could indicate a problem with the model runs.\n\n    Arguments\n    ---------\n    problem: dict\n        The problem definition\n    X: numpy.matrix\n        The NumPy matrix containing the model inputs\n    Y: numpy.array\n        The NumPy array containing the model outputs\n    second_order: bool, default=False\n        Include interaction effects\n    print_to_console: bool, default=False\n        Print results directly to console\n\n    Returns\n    -------\n    Si: dict\n        A dictionary of sensitivity indices, including main effects ``ME``,\n        and interaction effects ``IE`` (if ``second_order`` is True)\n\n    Examples\n    --------\n    >>> X = sample(problem)\n    >>> Y = X[:, 0] + (0.1 * X[:, 1]) + ((1.2 * X[:, 2]) * (0.2 + X[:, 0]))\n    >>> analyze(problem, X, Y, second_order=True, print_to_console=True)\n    \"\"\"\n    if seed:\n        np.random.seed(seed)\n\n    problem = extend_bounds(problem)\n    num_vars = problem['num_vars']\n\n    X = generate_contrast(problem)\n\n    main_effect = (1. / (2 * num_vars)) * np.dot(Y, X)\n\n    Si = ResultDict((k, [None] * num_vars)\n                    for k in ['names', 'ME'])\n    Si['ME'] = main_effect\n    Si['names'] = problem['names']\n\n    if print_to_console:\n        print(\"Parameter ME\")\n        for j in range(num_vars):\n            print(\"%s %f\" % (problem['names'][j], Si['ME'][j]))\n\n    if second_order:\n        interaction_names, interaction_effects = interactions(problem,\n                                                              Y,\n                                                              print_to_console)\n        Si['interaction_names'] = interaction_names\n        Si['IE'] = interaction_effects\n\n    Si.to_df = MethodType(to_df, Si)\n\n    return Si", "response": "Perform a fractional factorial analysis of a problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interactions(problem, Y, print_to_console=False):\n\n    names = problem['names']\n    num_vars = problem['num_vars']\n\n    X = generate_contrast(problem)\n\n    ie_names = []\n    IE = []\n\n    for col in range(X.shape[1]):\n        for col_2 in range(col):\n            x = X[:, col] * X[:, col_2]\n            var_names = (names[col_2], names[col])\n            ie_names.append(var_names)\n            IE.append((1. / (2 * num_vars)) * np.dot(Y, x))\n    if print_to_console:\n        [print('%s %f' % (n, i)) for (n, i) in zip(ie_names, IE)]\n\n    return ie_names, IE", "response": "Computes the second order effects between two sets of input factors and the output factors of the two sets of input factors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform the Random Balanced Design - Fourier Amplitude Sensitivity Test", "response": "def analyze(problem, Y, X, M=10, print_to_console=False, seed=None):\r\n    \"\"\"Performs the Random Balanced Design - Fourier Amplitude Sensitivity Test\r\n    (RBD-FAST) on model outputs.\r\n\r\n    Returns a dictionary with keys 'S1', where each entry is a list of\r\n    size D (the number of parameters) containing the indices in the same order\r\n    as the parameter file.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    Y : numpy.array\r\n        A NumPy array containing the model outputs\r\n    X : numpy.array\r\n        A NumPy array containing the model inputs\r\n    M : int\r\n        The interference parameter, i.e., the number of harmonics to sum in\r\n        the Fourier series decomposition (default 10)\r\n    print_to_console : bool\r\n        Print results directly to console (default False)\r\n\r\n    References\r\n    ----------\r\n    .. [1] S. Tarantola, D. Gatelli and T. Mara (2006) \"Random Balance Designs\r\n          for the Estimation of First Order Global Sensitivity Indices\",\r\n          Reliability Engineering and System Safety, 91:6, 717-727\r\n\r\n    .. [2] Elmar Plischke (2010) \"An effective algorithm for computing global\r\n          sensitivity indices (EASI) Reliability Engineering & System Safety\",\r\n          95:4, 354-360. doi:10.1016/j.ress.2009.11.005\r\n\r\n    .. [3] Jean-Yves Tissot, Cl\u00e9mentine Prieur (2012) \"Bias correction for the\r\n          estimation of sensitivity indices based on random balance designs.\",\r\n          Reliability Engineering and System Safety, Elsevier, 107, 205-213.\r\n          doi:10.1016/j.ress.2012.06.010\r\n\r\n    .. [4] Jeanne Goffart, Mickael Rabouille & Nathan Mendes (2015)\r\n          \"Uncertainty and sensitivity analysis applied to hygrothermal\r\n          simulation of a brick building in a hot and humid climate\",\r\n          Journal of Building Performance Simulation.\r\n          doi:10.1080/19401493.2015.1112430\r\n\r\n    Examples\r\n    --------\r\n    >>> X = latin.sample(problem, 1000)\r\n    >>> Y = Ishigami.evaluate(X)\r\n    >>> Si = rbd_fast.analyze(problem, Y, X, print_to_console=False)\r\n    \"\"\"\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem['num_vars']\r\n    N = Y.size\r\n\r\n    # Calculate and Output the First Order Value\r\n    if print_to_console:\r\n        print(\"Parameter First\")\r\n    Si = ResultDict((k, [None] * D) for k in ['S1'])\r\n    Si['names'] = problem['names']\r\n\r\n    for i in range(D):\r\n        S1 = compute_first_order(permute_outputs(Y, X[:, i]), M)\r\n        S1 = unskew_S1(S1, M, N)\r\n        Si['S1'][i] = S1\r\n        if print_to_console:\r\n            print(\"%s %g\" %\r\n                  (problem['names'][i].ljust(9), Si['S1'][i]))\r\n    return Si"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npermuting the output according to one of the inputs as in [ _2 ]", "response": "def permute_outputs(Y, X):\r\n    \"\"\"\r\n    Permute the output according to one of the inputs as in [_2]\r\n\r\n    References\r\n    ----------\r\n    .. [2] Elmar Plischke (2010) \"An effective algorithm for computing global\r\n          sensitivity indices (EASI) Reliability Engineering & System Safety\",\r\n          95:4, 354-360. doi:10.1016/j.ress.2009.11.005\r\n\r\n    \"\"\"\r\n    permutation_index = np.argsort(X)\r\n    permutation_index = np.concatenate([permutation_index[::2],\r\n                                        permutation_index[1::2][::-1]])\r\n    return Y[permutation_index]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unskew_S1(S1, M, N):\r\n    lamb = (2 * M) / N\r\n    return S1 - lamb / (1 - lamb) * (1 - S1)", "response": "Unskew the sensivity indice based on random balance designs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef avail_approaches(pkg):\n    '''Create list of available modules.\n\n    Arguments\n    ---------\n    pkg : module\n        module to inspect\n\n    Returns\n    ---------\n    method : list\n        A list of available submodules\n    '''\n    methods = [modname for importer, modname, ispkg in\n               pkgutil.walk_packages(path=pkg.__path__)\n               if modname not in\n               ['common_args', 'directions', 'sobol_sequence']]\n    return methods", "response": "Create list of available modules."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scale_samples(params, bounds):\n    '''Rescale samples in 0-to-1 range to arbitrary bounds\n\n    Arguments\n    ---------\n    bounds : list\n        list of lists of dimensions `num_params`-by-2\n    params : numpy.ndarray\n        numpy array of dimensions `num_params`-by-:math:`N`,\n        where :math:`N` is the number of samples\n    '''\n    # Check bounds are legal (upper bound is greater than lower bound)\n    b = np.array(bounds)\n    lower_bounds = b[:, 0]\n    upper_bounds = b[:, 1]\n\n    if np.any(lower_bounds >= upper_bounds):\n        raise ValueError(\"Bounds are not legal\")\n\n    # This scales the samples in-place, by using the optional output\n    # argument for the numpy ufunctions\n    # The calculation is equivalent to:\n    #   sample * (upper_bound - lower_bound) + lower_bound\n    np.add(np.multiply(params,\n                       (upper_bounds - lower_bounds),\n                       out=params),\n           lower_bounds,\n           out=params)", "response": "Rescale samples in 0 - to - 1 range to arbitrary bounds\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unscale_samples(params, bounds):\n    # Check bounds are legal (upper bound is greater than lower bound)\n    b = np.array(bounds)\n    lower_bounds = b[:, 0]\n    upper_bounds = b[:, 1]\n\n    if np.any(lower_bounds >= upper_bounds):\n        raise ValueError(\"Bounds are not legal\")\n\n    # This scales the samples in-place, by using the optional output\n    # argument for the numpy ufunctions\n    # The calculation is equivalent to:\n    #   (sample - lower_bound) / (upper_bound - lower_bound)\n    np.divide(np.subtract(params, lower_bounds, out=params),\n              np.subtract(upper_bounds, lower_bounds),\n              out=params)", "response": "Rescale samples from arbitrary bounds back to [ 0 1 ) range\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nonuniform_scale_samples(params, bounds, dists):\n    b = np.array(bounds)\n\n    # initializing matrix for converted values\n    conv_params = np.zeros_like(params)\n\n    # loop over the parameters\n    for i in range(conv_params.shape[1]):\n        # setting first and second arguments for distributions\n        b1 = b[i][0]\n        b2 = b[i][1]\n\n        if dists[i] == 'triang':\n            # checking for correct parameters\n            if b1 <= 0 or b2 <= 0 or b2 >= 1:\n                raise ValueError('''Triangular distribution: Scale must be\n                    greater than zero; peak on interval [0,1]''')\n            else:\n                conv_params[:, i] = sp.stats.triang.ppf(\n                    params[:, i], c=b2, scale=b1, loc=0)\n\n        elif dists[i] == 'unif':\n            if b1 >= b2:\n                raise ValueError('''Uniform distribution: lower bound\n                    must be less than upper bound''')\n            else:\n                conv_params[:, i] = params[:, i] * (b2 - b1) + b1\n\n        elif dists[i] == 'norm':\n            if b2 <= 0:\n                raise ValueError('''Normal distribution: stdev must be > 0''')\n            else:\n                conv_params[:, i] = sp.stats.norm.ppf(\n                    params[:, i], loc=b1, scale=b2)\n\n        # lognormal distribution (ln-space, not base-10)\n        # paramters are ln-space mean and standard deviation\n        elif dists[i] == 'lognorm':\n            # checking for valid parameters\n            if b2 <= 0:\n                raise ValueError(\n                    '''Lognormal distribution: stdev must be > 0''')\n            else:\n                conv_params[:, i] = np.exp(\n                    sp.stats.norm.ppf(params[:, i], loc=b1, scale=b2))\n\n        else:\n            valid_dists = ['unif', 'triang', 'norm', 'lognorm']\n            raise ValueError('Distributions: choose one of %s' %\n                             \", \".join(valid_dists))\n\n    return conv_params", "response": "Rescale samples in 0 - to - 1 range to other distributions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_param_file(filename, delimiter=None):\n    names = []\n    bounds = []\n    groups = []\n    dists = []\n    num_vars = 0\n    fieldnames = ['name', 'lower_bound', 'upper_bound', 'group', 'dist']\n\n    with open(filename, 'rU') as csvfile:\n        dialect = csv.Sniffer().sniff(csvfile.read(1024), delimiters=delimiter)\n        csvfile.seek(0)\n        reader = csv.DictReader(\n            csvfile, fieldnames=fieldnames, dialect=dialect)\n        for row in reader:\n            if row['name'].strip().startswith('#'):\n                pass\n            else:\n                num_vars += 1\n                names.append(row['name'])\n                bounds.append(\n                    [float(row['lower_bound']), float(row['upper_bound'])])\n\n                # If the fourth column does not contain a group name, use\n                # the parameter name\n                if row['group'] is None:\n                    groups.append(row['name'])\n                elif row['group'] is 'NA':\n                    groups.append(row['name'])\n                else:\n                    groups.append(row['group'])\n\n                # If the fifth column does not contain a distribution\n                # use uniform\n                if row['dist'] is None:\n                    dists.append('unif')\n                else:\n                    dists.append(row['dist'])\n\n    if groups == names:\n        groups = None\n    elif len(set(groups)) == 1:\n        raise ValueError('''Only one group defined, results will not be\n            meaningful''')\n\n    # setting dists to none if all are uniform\n    # because non-uniform scaling is not needed\n    if all([d == 'unif' for d in dists]):\n        dists = None\n\n    return {'names': names, 'bounds': bounds, 'num_vars': num_vars,\n            'groups': groups, 'dists': dists}", "response": "Unpacks a parameter file into a dictionary containing the names bounds groups and distributions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a matrix which notes factor membership of groups is the number of variables and the number of unique group names.", "response": "def compute_groups_matrix(groups):\n    \"\"\"Generate matrix which notes factor membership of groups\n\n    Computes a k-by-g matrix which notes factor membership of groups\n    where:\n        k is the number of variables (factors)\n        g is the number of groups\n    Also returns a g-length list of unique group_names whose positions\n    correspond to the order of groups in the k-by-g matrix\n\n    Arguments\n    ---------\n    groups : list\n        Group names corresponding to each variable\n\n    Returns\n    -------\n    tuple\n        containing group matrix assigning parameters to\n        groups and a list of unique group names\n    \"\"\"\n    if not groups:\n        return None\n\n    num_vars = len(groups)\n\n    # Get a unique set of the group names\n    unique_group_names = list(OrderedDict.fromkeys(groups))\n    number_of_groups = len(unique_group_names)\n\n    indices = dict([(x, i) for (i, x) in enumerate(unique_group_names)])\n\n    output = np.zeros((num_vars, number_of_groups), dtype=np.int)\n\n    for parameter_row, group_membership in enumerate(groups):\n        group_index = indices[group_membership]\n        output[parameter_row, group_index] = 1\n\n    return output, unique_group_names"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates Derivative - based Global Sensitivity Measure on model outputs.", "response": "def analyze(problem, X, Y, num_resamples=1000,\r\n            conf_level=0.95, print_to_console=False, seed=None):\r\n    \"\"\"Calculates Derivative-based Global Sensitivity Measure on model outputs.\r\n\r\n    Returns a dictionary with keys 'vi', 'vi_std', 'dgsm', and 'dgsm_conf',\r\n    where each entry is a list of size D (the number of parameters) containing\r\n    the indices in the same order as the parameter file.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    X : numpy.matrix\r\n        The NumPy matrix containing the model inputs\r\n    Y : numpy.array\r\n        The NumPy array containing the model outputs\r\n    num_resamples : int\r\n        The number of resamples used to compute the confidence\r\n        intervals (default 1000)\r\n    conf_level : float\r\n        The confidence interval level (default 0.95)\r\n    print_to_console : bool\r\n        Print results directly to console (default False)\r\n\r\n    References\r\n    ----------\r\n    .. [1] Sobol, I. M. and S. Kucherenko (2009). \"Derivative based global\r\n           sensitivity measures and their link with global sensitivity\r\n           indices.\" Mathematics and Computers in Simulation, 79(10):3009-3017,\r\n           doi:10.1016/j.matcom.2009.01.023.\r\n    \"\"\"\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem['num_vars']\r\n\r\n    if Y.size % (D + 1) == 0:\r\n        N = int(Y.size / (D + 1))\r\n    else:\r\n        raise RuntimeError(\"Incorrect number of samples in model output file.\")\r\n\r\n    if not 0 < conf_level < 1:\r\n        raise RuntimeError(\"Confidence level must be between 0-1.\")\r\n\r\n    base = np.zeros(N)\r\n    X_base = np.zeros((N, D))\r\n    perturbed = np.zeros((N, D))\r\n    X_perturbed = np.zeros((N, D))\r\n    step = D + 1\r\n\r\n    base = Y[0:Y.size:step]\r\n    X_base = X[0:Y.size:step, :]\r\n    for j in range(D):\r\n        perturbed[:, j] = Y[(j + 1):Y.size:step]\r\n        X_perturbed[:, j] = X[(j + 1):Y.size:step, j]\r\n\r\n    # First order (+conf.) and Total order (+conf.)\r\n    keys = ('vi', 'vi_std', 'dgsm', 'dgsm_conf')\r\n    S = ResultDict((k, np.zeros(D)) for k in keys)\r\n    S['names'] = problem['names']\r\n\r\n    if print_to_console:\r\n        print(\"Parameter %s %s %s %s\" % keys)\r\n\r\n    for j in range(D):\r\n        S['vi'][j], S['vi_std'][j] = calc_vi(\r\n            base, perturbed[:, j], X_perturbed[:, j] - X_base[:, j])\r\n        S['dgsm'][j], S['dgsm_conf'][j] = calc_dgsm(base, perturbed[:, j], X_perturbed[\r\n                                                    :, j] - X_base[:, j], problem['bounds'][j], num_resamples, conf_level)\r\n\r\n        if print_to_console:\r\n            print(\"%s %f %f %f %f\" % (\r\n                problem['names'][j], S['vi'][j], S['vi_std'][j], S['dgsm'][j], S['dgsm_conf'][j]))\r\n\r\n    return S"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform Morris Analysis on model outputs.", "response": "def analyze(problem, X, Y,\n            num_resamples=1000,\n            conf_level=0.95,\n            print_to_console=False,\n            num_levels=4,\n            seed=None):\n    \"\"\"Perform Morris Analysis on model outputs.\n\n    Returns a dictionary with keys 'mu', 'mu_star', 'sigma', and\n    'mu_star_conf', where each entry is a list of parameters containing\n    the indices in the same order as the parameter file.\n\n    Arguments\n    ---------\n    problem : dict\n        The problem definition\n    X : numpy.matrix\n        The NumPy matrix containing the model inputs of dtype=float\n    Y : numpy.array\n        The NumPy array containing the model outputs of dtype=float\n    num_resamples : int\n        The number of resamples used to compute the confidence\n        intervals (default 1000)\n    conf_level : float\n        The confidence interval level (default 0.95)\n    print_to_console : bool\n        Print results directly to console (default False)\n    num_levels : int\n        The number of grid levels, must be identical to the value\n        passed to SALib.sample.morris (default 4)\n\n    Returns\n    -------\n    Si : dict\n        A dictionary of sensitivity indices containing the following entries.\n\n        - `mu` - the mean elementary effect\n        - `mu_star` - the absolute of the mean elementary effect\n        - `sigma` - the standard deviation of the elementary effect\n        - `mu_star_conf` - the bootstrapped confidence interval\n        - `names` - the names of the parameters\n\n    References\n    ----------\n    .. [1] Morris, M. (1991).  \"Factorial Sampling Plans for Preliminary\n           Computational Experiments.\"  Technometrics, 33(2):161-174,\n           doi:10.1080/00401706.1991.10484804.\n    .. [2] Campolongo, F., J. Cariboni, and A. Saltelli (2007).  \"An effective\n           screening design for sensitivity analysis of large models.\"\n           Environmental Modelling & Software, 22(10):1509-1518,\n           doi:10.1016/j.envsoft.2006.10.004.\n\n    Examples\n    --------\n    >>> X = morris.sample(problem, 1000, num_levels=4)\n    >>> Y = Ishigami.evaluate(X)\n    >>> Si = morris.analyze(problem, X, Y, conf_level=0.95,\n    >>>                     print_to_console=True, num_levels=4)\n\n    \"\"\"\n    if seed:\n        np.random.seed(seed)\n\n    msg = (\"dtype of {} array must be 'float', float32 or float64\")\n    if X.dtype not in ['float', 'float32', 'float64']:\n        raise ValueError(msg.format('X'))\n    if Y.dtype not in ['float', 'float32', 'float64']:\n        raise ValueError(msg.format('Y'))\n\n    # Assume that there are no groups\n    groups = None\n    delta = compute_delta(num_levels)\n\n    num_vars = problem['num_vars']\n\n    if (problem.get('groups') is None) & (Y.size % (num_vars + 1) == 0):\n        num_trajectories = int(Y.size / (num_vars + 1))\n    elif problem.get('groups') is not None:\n        groups, unique_group_names = compute_groups_matrix(\n            problem['groups'])\n        number_of_groups = len(unique_group_names)\n        num_trajectories = int(Y.size / (number_of_groups + 1))\n    else:\n        raise ValueError(\"Number of samples in model output file must be\"\n                         \"a multiple of (D+1), where D is the number of\"\n                         \"parameters (or groups) in your parameter file.\")\n    ee = np.zeros((num_vars, num_trajectories))\n    ee = compute_elementary_effects(\n        X, Y, int(Y.size / num_trajectories), delta)\n\n    # Output the Mu, Mu*, and Sigma Values. Also return them in case this is\n    # being called from Python\n    Si = ResultDict((k, [None] * num_vars)\n                    for k in ['names', 'mu', 'mu_star', 'sigma', 'mu_star_conf'])\n    Si['mu'] = np.average(ee, 1)\n    Si['mu_star'] = np.average(np.abs(ee), 1)\n    Si['sigma'] = np.std(ee, axis=1, ddof=1)\n    Si['names'] = problem['names']\n\n    for j in range(num_vars):\n        Si['mu_star_conf'][j] = compute_mu_star_confidence(\n            ee[j, :], num_trajectories, num_resamples, conf_level)\n\n    if groups is None:\n        if print_to_console:\n            print(\"{0:<30} {1:>10} {2:>10} {3:>15} {4:>10}\".format(\n                \"Parameter\",\n                \"Mu_Star\",\n                \"Mu\",\n                \"Mu_Star_Conf\",\n                \"Sigma\")\n            )\n            for j in list(range(num_vars)):\n                print(\"{0:30} {1:10.3f} {2:10.3f} {3:15.3f} {4:10.3f}\".format(\n                    Si['names'][j],\n                    Si['mu_star'][j],\n                    Si['mu'][j],\n                    Si['mu_star_conf'][j],\n                    Si['sigma'][j])\n                )\n        return Si\n    elif groups is not None:\n        # if there are groups, then the elementary effects returned need to be\n        # computed over the groups of variables,\n        # rather than the individual variables\n        Si_grouped = dict((k, [None] * num_vars)\n                          for k in ['mu_star', 'mu_star_conf'])\n        Si_grouped['mu_star'] = compute_grouped_metric(Si['mu_star'], groups)\n        Si_grouped['mu_star_conf'] = compute_grouped_metric(Si['mu_star_conf'],\n                                                            groups)\n        Si_grouped['names'] = unique_group_names\n        Si_grouped['sigma'] = compute_grouped_sigma(Si['sigma'], groups)\n        Si_grouped['mu'] = compute_grouped_sigma(Si['mu'], groups)\n\n        if print_to_console:\n            print(\"{0:<30} {1:>10} {2:>10} {3:>15} {4:>10}\".format(\n                \"Parameter\",\n                \"Mu_Star\",\n                \"Mu\",\n                \"Mu_Star_Conf\",\n                \"Sigma\")\n            )\n            for j in list(range(number_of_groups)):\n                print(\"{0:30} {1:10.3f} {2:10.3f} {3:15.3f} {4:10.3f}\".format(\n                    Si_grouped['names'][j],\n                    Si_grouped['mu_star'][j],\n                    Si_grouped['mu'][j],\n                    Si_grouped['mu_star_conf'][j],\n                    Si_grouped['sigma'][j])\n                )\n\n        return Si_grouped\n    else:\n        raise RuntimeError(\n            \"Could not determine which parameters should be returned\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_grouped_metric(ungrouped_metric, group_matrix):\n    '''\n    Computes the mean value for the groups of parameter values in the\n    argument ungrouped_metric\n    '''\n\n    group_matrix = np.array(group_matrix, dtype=np.bool)\n\n    mu_star_masked = np.ma.masked_array(ungrouped_metric * group_matrix.T,\n                                        mask=(group_matrix ^ 1).T)\n    mean_of_mu_star = np.ma.mean(mu_star_masked, axis=1)\n\n    return mean_of_mu_star", "response": "Computes the mean value for the groups of parameter values in the ungrouped_metric parameter values in the\n    argument ungrouped_metric"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the elementary effects of the given model.", "response": "def compute_elementary_effects(model_inputs, model_outputs, trajectory_size,\n                               delta):\n    '''\n    Arguments\n    ---------\n    model_inputs : matrix of inputs to the model under analysis.\n        x-by-r where x is the number of variables and\n        r is the number of rows (a function of x and num_trajectories)\n    model_outputs\n        an r-length vector of model outputs\n    trajectory_size\n        a scalar indicating the number of rows in a trajectory\n    delta : float\n        scaling factor computed from `num_levels`\n    '''\n    num_vars = model_inputs.shape[1]\n    num_rows = model_inputs.shape[0]\n    num_trajectories = int(num_rows / trajectory_size)\n\n    ee = np.zeros((num_trajectories, num_vars), dtype=np.float)\n\n    ip_vec = model_inputs.reshape(num_trajectories, trajectory_size, num_vars)\n    ip_cha = np.subtract(ip_vec[:, 1:, :], ip_vec[:, 0:-1, :])\n    up = (ip_cha > 0)\n    lo = (ip_cha < 0)\n\n    op_vec = model_outputs.reshape(num_trajectories, trajectory_size)\n\n    result_up = get_increased_values(op_vec, up, lo)\n    result_lo = get_decreased_values(op_vec, up, lo)\n\n    ee = np.subtract(result_up, result_lo)\n    np.divide(ee, delta, out=ee)\n\n    return ee"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_mu_star_confidence(ee, num_trajectories, num_resamples,\n                               conf_level):\n    '''\n    Uses bootstrapping where the elementary effects are resampled with\n    replacement to produce a histogram of resampled mu_star metrics.\n    This resample is used to produce a confidence interval.\n    '''\n    ee_resampled = np.zeros([num_trajectories])\n    mu_star_resampled = np.zeros([num_resamples])\n\n    if not 0 < conf_level < 1:\n        raise ValueError(\"Confidence level must be between 0-1.\")\n\n    resample_index = np.random.randint(\n        len(ee), size=(num_resamples, num_trajectories))\n    ee_resampled = ee[resample_index]\n    # Compute average of the absolute values over each of the resamples\n    mu_star_resampled = np.average(np.abs(ee_resampled), axis=1)\n\n    return norm.ppf(0.5 + conf_level / 2) * mu_star_resampled.std(ddof=1)", "response": "Compute the confidence interval of the elementary effects of a single resource set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef analyze(problem, Y, M=4, print_to_console=False, seed=None):\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem['num_vars']\r\n\r\n    if Y.size % (D) == 0:\r\n        N = int(Y.size / D)\r\n    else:\r\n        print(\"\"\"\r\n            Error: Number of samples in model output file must be a multiple of D,\r\n            where D is the number of parameters in your parameter file.\r\n          \"\"\")\r\n        exit()\r\n\r\n    # Recreate the vector omega used in the sampling\r\n    omega = np.zeros([D])\r\n    omega[0] = math.floor((N - 1) / (2 * M))\r\n    m = math.floor(omega[0] / (2 * M))\r\n\r\n    if m >= (D - 1):\r\n        omega[1:] = np.floor(np.linspace(1, m, D - 1))\r\n    else:\r\n        omega[1:] = np.arange(D - 1) % m + 1\r\n\r\n    # Calculate and Output the First and Total Order Values\r\n    if print_to_console:\r\n        print(\"Parameter First Total\")\r\n    Si = ResultDict((k, [None] * D) for k in ['S1', 'ST'])\r\n    Si['names'] = problem['names']\r\n\r\n    for i in range(D):\r\n        l = np.arange(i * N, (i + 1) * N)\r\n        Si['S1'][i] = compute_first_order(Y[l], N, M, omega[0])\r\n        Si['ST'][i] = compute_total_order(Y[l], N, omega[0])\r\n        if print_to_console:\r\n            print(\"%s %f %f\" %\r\n                  (problem['names'][i], Si['S1'][i], Si['ST'][i]))\r\n\r\n    return Si", "response": "Performs the Fourier Amplitude Sensitivity Test on the model outputs and returns a dictionary with keys S1 and ST."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform Sobol Analysis on model outputs.", "response": "def analyze(problem, Y, calc_second_order=True, num_resamples=100,\r\n            conf_level=0.95, print_to_console=False, parallel=False,\r\n            n_processors=None, seed=None):\r\n    \"\"\"Perform Sobol Analysis on model outputs.\r\n\r\n    Returns a dictionary with keys 'S1', 'S1_conf', 'ST', and 'ST_conf', where\r\n    each entry is a list of size D (the number of parameters) containing the\r\n    indices in the same order as the parameter file.  If calc_second_order is\r\n    True, the dictionary also contains keys 'S2' and 'S2_conf'.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    Y : numpy.array\r\n        A NumPy array containing the model outputs\r\n    calc_second_order : bool\r\n        Calculate second-order sensitivities (default True)\r\n    num_resamples : int\r\n        The number of resamples (default 100)\r\n    conf_level : float\r\n        The confidence interval level (default 0.95)\r\n    print_to_console : bool\r\n        Print results directly to console (default False)\r\n\r\n    References\r\n    ----------\r\n    .. [1] Sobol, I. M. (2001).  \"Global sensitivity indices for nonlinear\r\n           mathematical models and their Monte Carlo estimates.\"  Mathematics\r\n           and Computers in Simulation, 55(1-3):271-280,\r\n           doi:10.1016/S0378-4754(00)00270-6.\r\n    .. [2] Saltelli, A. (2002).  \"Making best use of model evaluations to\r\n           compute sensitivity indices.\"  Computer Physics Communications,\r\n           145(2):280-297, doi:10.1016/S0010-4655(02)00280-1.\r\n    .. [3] Saltelli, A., P. Annoni, I. Azzini, F. Campolongo, M. Ratto, and\r\n           S. Tarantola (2010).  \"Variance based sensitivity analysis of model\r\n           output.  Design and estimator for the total sensitivity index.\"\r\n           Computer Physics Communications, 181(2):259-270,\r\n           doi:10.1016/j.cpc.2009.09.018.\r\n\r\n    Examples\r\n    --------\r\n    >>> X = saltelli.sample(problem, 1000)\r\n    >>> Y = Ishigami.evaluate(X)\r\n    >>> Si = sobol.analyze(problem, Y, print_to_console=True)\r\n\r\n    \"\"\"\r\n    if seed:\r\n        np.random.seed(seed)\r\n    # determining if groups are defined and adjusting the number\r\n    # of rows in the cross-sampled matrix accordingly\r\n    if not problem.get('groups'):\r\n        D = problem['num_vars']\r\n    else:\r\n        D = len(set(problem['groups']))\r\n\r\n    if calc_second_order and Y.size % (2 * D + 2) == 0:\r\n        N = int(Y.size / (2 * D + 2))\r\n    elif not calc_second_order and Y.size % (D + 2) == 0:\r\n        N = int(Y.size / (D + 2))\r\n    else:\r\n        raise RuntimeError(\"\"\"\r\n        Incorrect number of samples in model output file.\r\n        Confirm that calc_second_order matches option used during sampling.\"\"\")\r\n\r\n    if conf_level < 0 or conf_level > 1:\r\n        raise RuntimeError(\"Confidence level must be between 0-1.\")\r\n\r\n    # normalize the model output\r\n    Y = (Y - Y.mean()) / Y.std()\r\n\r\n    A, B, AB, BA = separate_output_values(Y, D, N, calc_second_order)\r\n    r = np.random.randint(N, size=(N, num_resamples))\r\n    Z = norm.ppf(0.5 + conf_level / 2)\r\n\r\n    if not parallel:\r\n        S = create_Si_dict(D, calc_second_order)\r\n\r\n        for j in range(D):\r\n            S['S1'][j] = first_order(A, AB[:, j], B)\r\n            S['S1_conf'][j] = Z * first_order(A[r], AB[r, j], B[r]).std(ddof=1)\r\n            S['ST'][j] = total_order(A, AB[:, j], B)\r\n            S['ST_conf'][j] = Z * total_order(A[r], AB[r, j], B[r]).std(ddof=1)\r\n\r\n        # Second order (+conf.)\r\n        if calc_second_order:\r\n            for j in range(D):\r\n                for k in range(j + 1, D):\r\n                    S['S2'][j, k] = second_order(\r\n                        A, AB[:, j], AB[:, k], BA[:, j], B)\r\n                    S['S2_conf'][j, k] = Z * second_order(A[r], AB[r, j],\r\n                                                          AB[r, k], BA[r, j], B[r]).std(ddof=1)\r\n\r\n    else:\r\n        tasks, n_processors = create_task_list(\r\n            D, calc_second_order, n_processors)\r\n\r\n        func = partial(sobol_parallel, Z, A, AB, BA, B, r)\r\n        pool = Pool(n_processors)\r\n        S_list = pool.map_async(func, tasks)\r\n        pool.close()\r\n        pool.join()\r\n\r\n        S = Si_list_to_dict(S_list.get(), D, calc_second_order)\r\n\r\n    # Print results to console\r\n    if print_to_console:\r\n        print_indices(S, problem, calc_second_order)\r\n\r\n    # Add problem context and override conversion method for special case\r\n    S.problem = problem\r\n    S.to_df = MethodType(to_df, S)\r\n    return S"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Si_to_pandas_dict(S_dict):\r\n    problem = S_dict.problem\r\n    total_order = {\r\n        'ST': S_dict['ST'],\r\n        'ST_conf': S_dict['ST_conf']\r\n    }\r\n    first_order = {\r\n        'S1': S_dict['S1'],\r\n        'S1_conf': S_dict['S1_conf']\r\n    }\r\n\r\n    idx = None\r\n    second_order = None\r\n    if 'S2' in S_dict:\r\n        names = problem['names']\r\n        idx = list(combinations(names, 2))\r\n        second_order = {\r\n            'S2': [S_dict['S2'][names.index(i[0]), names.index(i[1])]\r\n                   for i in idx],\r\n            'S2_conf': [S_dict['S2_conf'][names.index(i[0]), names.index(i[1])]\r\n                        for i in idx]\r\n        }\r\n    return total_order, first_order, (idx, second_order)", "response": "Convert Si information into Pandas DataFrame compatible dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a uniform timestamp with parameter values for file identification", "response": "def timestamp(num_params, p_levels, k_choices, N):\n    \"\"\"\n    Returns a uniform timestamp with parameter values for file identification\n    \"\"\"\n    string = \"_v%s_l%s_gs%s_k%s_N%s_%s.txt\" % (num_params,\n                                               p_levels,\n                                               k_choices,\n                                               N,\n                                               dt.strftime(dt.now(),\n                                                           \"%d%m%y%H%M%S\"))\n    return string"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates model inputs using Latin hypercube sampling.", "response": "def sample(problem, N, seed=None):\r\n    \"\"\"Generate model inputs using Latin hypercube sampling (LHS).\r\n\r\n    Returns a NumPy matrix containing the model inputs generated by Latin\r\n    hypercube sampling.  The resulting matrix contains N rows and D columns,\r\n    where D is the number of parameters.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    N : int\r\n        The number of samples to generate\r\n    \"\"\"\r\n    if seed:\r\n        np.random.seed(seed)\r\n    D = problem['num_vars']\r\n\r\n    result = np.zeros([N, D])\r\n    temp = np.zeros([N])\r\n    d = 1.0 / N\r\n\r\n    for i in range(D):\r\n\r\n        for j in range(N):\r\n            temp[j] = np.random.uniform(\r\n                low=j * d, high=(j + 1) * d, size=1)[0]\r\n\r\n        np.random.shuffle(temp)\r\n\r\n        for j in range(N):\r\n            result[j, i] = temp[j]\r\n\r\n    scale_samples(result, problem['bounds'])\r\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds method specific options to CLI parser", "response": "def cli_parse(parser):\r\n    \"\"\"Add method specific options to CLI parser.\r\n\r\n    Parameters\r\n    ----------\r\n    parser : argparse object\r\n\r\n    Returns\r\n    ----------\r\n    Updated argparse object\r\n    \"\"\"\r\n    parser.add_argument('-n', '--samples', type=int, required=True,\r\n                        help='Number of Samples')\r\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef analyze(problem, X, Y, num_resamples=10,\r\n            conf_level=0.95, print_to_console=False, seed=None):\r\n    \"\"\"Perform Delta Moment-Independent Analysis on model outputs.\r\n\r\n    Returns a dictionary with keys 'delta', 'delta_conf', 'S1', and 'S1_conf',\r\n    where each entry is a list of size D (the number of parameters) containing\r\n    the indices in the same order as the parameter file.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    X: numpy.matrix\r\n        A NumPy matrix containing the model inputs\r\n    Y : numpy.array\r\n        A NumPy array containing the model outputs\r\n    num_resamples : int\r\n        The number of resamples when computing confidence intervals (default 10)\r\n    conf_level : float\r\n        The confidence interval level (default 0.95)\r\n    print_to_console : bool\r\n        Print results directly to console (default False)\r\n\r\n    References\r\n    ----------\r\n    .. [1] Borgonovo, E. (2007). \"A new uncertainty importance measure.\"\r\n           Reliability Engineering & System Safety, 92(6):771-784,\r\n           doi:10.1016/j.ress.2006.04.015.\r\n\r\n    .. [2] Plischke, E., E. Borgonovo, and C. L. Smith (2013). \"Global\r\n           sensitivity measures from given data.\" European Journal of\r\n           Operational Research, 226(3):536-550, doi:10.1016/j.ejor.2012.11.047.\r\n\r\n    Examples\r\n    --------\r\n    >>> X = latin.sample(problem, 1000)\r\n    >>> Y = Ishigami.evaluate(X)\r\n    >>> Si = delta.analyze(problem, X, Y, print_to_console=True)\r\n    \"\"\"\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem['num_vars']\r\n    N = Y.size\r\n\r\n    if not 0 < conf_level < 1:\r\n        raise RuntimeError(\"Confidence level must be between 0-1.\")\r\n\r\n    # equal frequency partition\r\n    M = min(np.ceil(N ** (2 / (7 + np.tanh((1500 - N) / 500)))), 48)\r\n    m = np.linspace(0, N, M + 1)\r\n    Ygrid = np.linspace(np.min(Y), np.max(Y), 100)\r\n\r\n    keys = ('delta', 'delta_conf', 'S1', 'S1_conf')\r\n    S = ResultDict((k, np.zeros(D)) for k in keys)\r\n    S['names'] = problem['names']\r\n\r\n    if print_to_console:\r\n        print(\"Parameter %s %s %s %s\" % keys)\r\n\r\n    try:\r\n        for i in range(D):\r\n            S['delta'][i], S['delta_conf'][i] = bias_reduced_delta(\r\n                Y, Ygrid, X[:, i], m, num_resamples, conf_level)\r\n            S['S1'][i] = sobol_first(Y, X[:, i], m)\r\n            S['S1_conf'][i] = sobol_first_conf(\r\n                Y, X[:, i], m, num_resamples, conf_level)\r\n            if print_to_console:\r\n                print(\"%s %f %f %f %f\" % (problem['names'][i], S['delta'][\r\n                    i], S['delta_conf'][i], S['S1'][i], S['S1_conf'][i]))\r\n    except np.linalg.LinAlgError as e:\r\n        msg = \"Singular matrix detected\\n\\\r\n             Sample size of {} may be too small\".format(Y.size)\r\n        raise np.linalg.LinAlgError(msg)\r\n\r\n    return S", "response": "Perform Delta Moment - Independent Analysis on model outputs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing brute force method to find the most distant trajectories of a specific entry in the log file.", "response": "def brute_force_most_distant(self, input_sample, num_samples,\n                                 num_params, k_choices,\n                                 num_groups=None):\n        \"\"\"Use brute force method to find most distant trajectories\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n            The number of samples to generate\n        num_params : int\n            The number of parameters\n        k_choices : int\n            The number of optimal trajectories\n        num_groups : int, default=None\n            The number of groups\n\n        Returns\n        -------\n        list\n        \"\"\"\n        scores = self.find_most_distant(input_sample,\n                                        num_samples,\n                                        num_params,\n                                        k_choices,\n                                        num_groups)\n\n        maximum_combo = self.find_maximum(scores, num_samples, k_choices)\n\n        return maximum_combo"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the 'k_choices' most distant choices from the 'num_samples' trajectories contained in 'input_sample' Arguments --------- input_sample : numpy.ndarray num_samples : int The number of samples to generate num_params : int The number of parameters k_choices : int The number of optimal trajectories num_groups : int, default=None The number of groups Returns ------- numpy.ndarray", "response": "def find_most_distant(self, input_sample, num_samples,\n                          num_params, k_choices, num_groups=None):\n        \"\"\"\n        Finds the 'k_choices' most distant choices from the\n        'num_samples' trajectories contained in 'input_sample'\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n            The number of samples to generate\n        num_params : int\n            The number of parameters\n        k_choices : int\n            The number of optimal trajectories\n        num_groups : int, default=None\n            The number of groups\n\n        Returns\n        -------\n        numpy.ndarray\n        \"\"\"\n        # Now evaluate the (N choose k_choices) possible combinations\n        if nchoosek(num_samples, k_choices) >= sys.maxsize:\n            raise ValueError(\"Number of combinations is too large\")\n        number_of_combinations = int(nchoosek(num_samples, k_choices))\n\n        # First compute the distance matrix for each possible pairing\n        # of trajectories and store in a shared-memory array\n        distance_matrix = self.compute_distance_matrix(input_sample,\n                                                       num_samples,\n                                                       num_params,\n                                                       num_groups)\n\n        # Initialise the output array\n        chunk = int(1e6)\n        if chunk > number_of_combinations:\n            chunk = number_of_combinations\n\n        counter = 0\n        # Generate a list of all the possible combinations\n        combo_gen = combinations(list(range(num_samples)), k_choices)\n        scores = np.zeros(number_of_combinations, dtype=np.float32)\n        # Generate the pairwise indices once\n        pairwise = np.array(\n            [y for y in combinations(list(range(k_choices)), 2)])\n\n        for combos in self.grouper(chunk, combo_gen):\n            scores[(counter * chunk):((counter + 1) * chunk)] \\\n                = self.mappable(combos, pairwise, distance_matrix)\n            counter += 1\n        return scores"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mappable(combos, pairwise, distance_matrix):\n        '''\n        Obtains scores from the distance_matrix for each pairwise combination\n        held in the combos array\n\n        Arguments\n        ----------\n        combos : numpy.ndarray\n        pairwise : numpy.ndarray\n        distance_matrix : numpy.ndarray\n        '''\n        combos = np.array(combos)\n        # Create a list of all pairwise combination for each combo in combos\n        combo_list = combos[:, pairwise[:, ]]\n\n        addresses = tuple([combo_list[:, :, 1], combo_list[:, :, 0]])\n\n        all_distances = distance_matrix[addresses]\n        new_scores = np.sqrt(\n            np.einsum('ij,ij->i', all_distances, all_distances))\n        return new_scores", "response": "Returns the scores of the distance_matrix for each pairwise combination in the combos array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_maximum(self, scores, N, k_choices):\n        if not isinstance(scores, np.ndarray):\n            raise TypeError(\"Scores input is not a numpy array\")\n\n        index_of_maximum = int(scores.argmax())\n        maximum_combo = self.nth(combinations(\n            list(range(N)), k_choices), index_of_maximum, None)\n        return sorted(maximum_combo)", "response": "Finds the k_choices maximum scores from scores"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nth(iterable, n, default=None):\n\n        if type(n) != int:\n            raise TypeError(\"n is not an integer\")\n\n        return next(islice(iterable, n, None), default)", "response": "Returns the nth item or a default value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample(problem, N, num_levels=4, optimal_trajectories=None,\n           local_optimization=True):\n    \"\"\"Generate model inputs using the Method of Morris\n\n    Returns a NumPy matrix containing the model inputs required for Method of\n    Morris.  The resulting matrix has :math:`(G+1)*T` rows and :math:`D`\n    columns, where :math:`D` is the number of parameters, :math:`G` is the\n    number of groups (if no groups are selected, the number of parameters).\n    :math:`T` is the number of trajectories :math:`N`,\n    or `optimal_trajectories` if selected.\n    These model inputs  are intended to be used with\n    :func:`SALib.analyze.morris.analyze`.\n\n    Parameters\n    ----------\n    problem : dict\n        The problem definition\n    N : int\n        The number of trajectories to generate\n    num_levels : int, default=4\n        The number of grid levels\n    optimal_trajectories : int\n        The number of optimal trajectories to sample (between 2 and N)\n    local_optimization : bool, default=True\n        Flag whether to use local optimization according to Ruano et al. (2012)\n        Speeds up the process tremendously for bigger N and num_levels.\n        If set to ``False`` brute force method is used, unless ``gurobipy`` is\n        available\n\n    Returns\n    -------\n    sample : numpy.ndarray\n        Returns a numpy.ndarray containing the model inputs required for Method\n        of Morris. The resulting matrix has :math:`(G/D+1)*N/T` rows and\n        :math:`D` columns, where :math:`D` is the number of parameters.\n    \"\"\"\n    if problem.get('groups'):\n        sample = _sample_groups(problem, N, num_levels)\n    else:\n        sample = _sample_oat(problem, N, num_levels)\n\n    if optimal_trajectories:\n\n        sample = _compute_optimised_trajectories(problem,\n                                                 sample,\n                                                 N,\n                                                 optimal_trajectories,\n                                                 local_optimization)\n\n    scale_samples(sample, problem['bounds'])\n    return sample", "response": "Generate model inputs using the Method of Morris\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate trajectories with groups", "response": "def _sample_oat(problem, N, num_levels=4):\n    \"\"\"Generate trajectories without groups\n\n    Arguments\n    ---------\n    problem : dict\n        The problem definition\n    N : int\n        The number of samples to generate\n    num_levels : int, default=4\n        The number of grid levels\n    \"\"\"\n    group_membership = np.asmatrix(np.identity(problem['num_vars'],\n                                               dtype=int))\n\n    num_params = group_membership.shape[0]\n    sample = np.zeros((N * (num_params + 1), num_params))\n    sample = np.array([generate_trajectory(group_membership,\n                                           num_levels)\n                       for n in range(N)])\n    return sample.reshape((N * (num_params + 1), num_params))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate trajectories for groups", "response": "def _sample_groups(problem, N, num_levels=4):\n    \"\"\"Generate trajectories for groups\n\n    Returns an :math:`N(g+1)`-by-:math:`k` array of `N` trajectories,\n    where :math:`g` is the number of groups and :math:`k` is the number\n    of factors\n\n    Arguments\n    ---------\n    problem : dict\n        The problem definition\n    N : int\n        The number of trajectories to generate\n    num_levels : int, default=4\n        The number of grid levels\n\n    Returns\n    -------\n    numpy.ndarray\n    \"\"\"\n    if len(problem['groups']) != problem['num_vars']:\n        raise ValueError(\"Groups do not match to number of variables\")\n\n    group_membership, _ = compute_groups_matrix(problem['groups'])\n\n    if group_membership is None:\n        raise ValueError(\"Please define the 'group_membership' matrix\")\n    if not isinstance(group_membership, np.ndarray):\n        raise TypeError(\"Argument 'group_membership' should be formatted \\\n                         as a numpy ndarray\")\n\n    num_params = group_membership.shape[0]\n    num_groups = group_membership.shape[1]\n    sample = np.zeros((N * (num_groups + 1), num_params))\n    sample = np.array([generate_trajectory(group_membership,\n                                           num_levels)\n                       for n in range(N)])\n    return sample.reshape((N * (num_groups + 1), num_params))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a single trajectory of size g + 1", "response": "def generate_trajectory(group_membership, num_levels=4):\n    \"\"\"Return a single trajectory\n\n    Return a single trajectory of size :math:`(g+1)`-by-:math:`k`\n    where :math:`g` is the number of groups,\n    and :math:`k` is the number of factors,\n    both implied by the dimensions of `group_membership`\n\n    Arguments\n    ---------\n    group_membership : np.ndarray\n        a k-by-g matrix which notes factor membership of groups\n    num_levels : int, default=4\n        The number of levels in the grid\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n\n    delta = compute_delta(num_levels)\n\n    # Infer number of groups `g` and number of params `k` from\n    # `group_membership` matrix\n    num_params = group_membership.shape[0]\n    num_groups = group_membership.shape[1]\n\n    # Matrix B - size (g + 1) * g -  lower triangular matrix\n    B = np.tril(np.ones([num_groups + 1, num_groups],\n                        dtype=int), -1)\n\n    P_star = generate_p_star(num_groups)\n\n    # Matrix J - a (g+1)-by-num_params matrix of ones\n    J = np.ones((num_groups + 1, num_params))\n\n    # Matrix D* - num_params-by-num_params matrix which decribes whether\n    # factors move up or down\n    D_star = np.diag([rd.choice([-1, 1]) for _ in range(num_params)])\n\n    x_star = generate_x_star(num_params, num_levels)\n\n    # Matrix B* - size (num_groups + 1) * num_params\n    B_star = compute_b_star(J, x_star, delta, B,\n                            group_membership, P_star, D_star)\n\n    return B_star"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the order in which groups move", "response": "def generate_p_star(num_groups):\n    \"\"\"Describe the order in which groups move\n\n    Arguments\n    ---------\n    num_groups : int\n\n    Returns\n    -------\n    np.ndarray\n        Matrix P* - size (g-by-g)\n    \"\"\"\n    p_star = np.eye(num_groups, num_groups)\n    rd.shuffle(p_star)\n    return p_star"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses command line arguments for a specific module.", "response": "def parse_subargs(module, parser, method, opts):\n    '''Attach argument parser for action specific options.\n\n    Arguments\n    ---------\n    module : module\n        name of module to extract action from\n    parser : argparser\n        argparser object to attach additional arguments to\n    method : str\n        name of method (morris, sobol, etc).\n        Must match one of the available submodules\n    opts : list\n        A list of argument options to parse\n\n    Returns\n    ---------\n    subargs : argparser namespace object\n    '''\n    module.cli_args(parser)\n    subargs = parser.parse_args(opts)\n    return subargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sample(problem, N, M=4, seed=None):\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    if N <= 4 * M**2:\r\n        raise ValueError(\"\"\"\r\n        Sample size N > 4M^2 is required. M=4 by default.\"\"\")\r\n\r\n    D = problem['num_vars']\r\n\r\n    omega = np.zeros([D])\r\n    omega[0] = math.floor((N - 1) / (2 * M))\r\n    m = math.floor(omega[0] / (2 * M))\r\n\r\n    if m >= (D - 1):\r\n        omega[1:] = np.floor(np.linspace(1, m, D - 1))\r\n    else:\r\n        omega[1:] = np.arange(D - 1) % m + 1\r\n\r\n    # Discretization of the frequency space, s\r\n    s = (2 * math.pi / N) * np.arange(N)\r\n\r\n    # Transformation to get points in the X space\r\n    X = np.zeros([N * D, D])\r\n    omega2 = np.zeros([D])\r\n\r\n    for i in range(D):\r\n        omega2[i] = omega[0]\r\n        idx = list(range(i)) + list(range(i + 1, D))\r\n        omega2[idx] = omega[1:]\r\n        l = range(i * N, (i + 1) * N)\r\n\r\n        # random phase shift on [0, 2pi) following Saltelli et al.\r\n        # Technometrics 1999\r\n        phi = 2 * math.pi * np.random.rand()\r\n\r\n        for j in range(D):\r\n            g = 0.5 + (1 / math.pi) * np.arcsin(np.sin(omega2[j] * s + phi))\r\n            X[l, j] = g\r\n\r\n    scale_samples(X, problem['bounds'])\r\n    return X", "response": "Generate a random sample of the given problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_local_maximum(self, input_sample, N, num_params,\n                           k_choices, num_groups=None):\n        \"\"\"Find the most different trajectories in the input sample using a\n        local approach\n\n        An alternative by Ruano et al. (2012) for the brute force approach as\n        originally proposed by Campolongo et al. (2007). The method should\n        improve the speed with which an optimal set of trajectories is\n        found tremendously for larger sample sizes.\n\n        Arguments\n        ---------\n        input_sample : np.ndarray\n        N : int\n            The number of trajectories\n        num_params : int\n            The number of factors\n        k_choices : int\n            The number of optimal trajectories to return\n        num_groups : int, default=None\n            The number of groups\n        Returns\n        -------\n        list\n        \"\"\"\n        distance_matrix = self.compute_distance_matrix(input_sample, N,\n                                                       num_params, num_groups,\n                                                       local_optimization=True)\n\n        tot_indices_list = []\n        tot_max_array = np.zeros(k_choices - 1)\n\n        # Loop over `k_choices`, i starts at 1\n        for i in range(1, k_choices):\n            indices_list = []\n            row_maxima_i = np.zeros(len(distance_matrix))\n\n            row_nr = 0\n            for row in distance_matrix:\n                indices = tuple(row.argsort()[-i:][::-1]) + (row_nr,)\n                row_maxima_i[row_nr] = self.sum_distances(\n                    indices, distance_matrix)\n                indices_list.append(indices)\n                row_nr += 1\n\n            # Find the indices belonging to the maximum distance\n            i_max_ind = self.get_max_sum_ind(indices_list, row_maxima_i, i, 0)\n\n            # Loop 'm' (called loop 'k' in Ruano)\n            m_max_ind = i_max_ind\n            # m starts at 1\n            m = 1\n\n            while m <= k_choices - i - 1:\n                m_ind = self.add_indices(m_max_ind, distance_matrix)\n\n                m_maxima = np.zeros(len(m_ind))\n\n                for n in range(0, len(m_ind)):\n                    m_maxima[n] = self.sum_distances(m_ind[n], distance_matrix)\n\n                m_max_ind = self.get_max_sum_ind(m_ind, m_maxima, i, m)\n\n                m += 1\n\n            tot_indices_list.append(m_max_ind)\n            tot_max_array[i -\n                          1] = self.sum_distances(m_max_ind, distance_matrix)\n\n        tot_max = self.get_max_sum_ind(\n            tot_indices_list, tot_max_array, \"tot\", \"tot\")\n        return sorted(list(tot_max))", "response": "This method finds the most different trajectories in the input sample using a local approach."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sum_distances(self, indices, distance_matrix):\n        combs_tup = np.array(tuple(combinations(indices, 2)))\n\n        # Put indices from tuples into two-dimensional array.\n        combs = np.array([[i[0] for i in combs_tup],\n                          [i[1] for i in combs_tup]])\n\n        # Calculate distance (vectorized)\n        dist = np.sqrt(\n            np.sum(np.square(distance_matrix[combs[0], combs[1]]), axis=0))\n\n        return dist", "response": "Calculates the sum of the combinatorial distances between a select group of trajectories indicated by indices and a distance matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the indices that belong to the maximum distance in distances in indices_list.", "response": "def get_max_sum_ind(self, indices_list, distances, i, m):\n        '''Get the indices that belong to the maximum distance in `distances`\n\n        Arguments\n        ---------\n        indices_list : list\n            list of tuples\n        distances : numpy.ndarray\n            size M\n        i : int\n        m : int\n\n        Returns\n        -------\n        list\n        '''\n        if len(indices_list) != len(distances):\n            msg = \"Indices and distances are lists of different length.\" + \\\n                \"Length indices_list = {} and length distances = {}.\" + \\\n                \"In loop i = {}  and m =  {}\"\n            raise ValueError(msg.format(\n                len(indices_list), len(distances), i, m))\n\n        max_index = tuple(distances.argsort()[-1:][::-1])\n        return indices_list[max_index[0]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding extra indices for the combinatorial problem.", "response": "def add_indices(self, indices, distance_matrix):\n        '''Adds extra indices for the combinatorial problem.\n\n        Arguments\n        ---------\n        indices : tuple\n        distance_matrix : numpy.ndarray (M,M)\n\n        Example\n        -------\n        >>> add_indices((1,2), numpy.array((5,5)))\n        [(1, 2, 3), (1, 2, 4), (1, 2, 5)]\n\n        '''\n        list_new_indices = []\n        for i in range(0, len(distance_matrix)):\n            if i not in indices:\n                list_new_indices.append(indices + (i,))\n        return list_new_indices"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating model inputs using Saltelli s sampling scheme.", "response": "def sample(problem, N, calc_second_order=True, seed=None):\r\n    \"\"\"Generates model inputs using Saltelli's extension of the Sobol sequence.\r\n\r\n    Returns a NumPy matrix containing the model inputs using Saltelli's sampling\r\n    scheme.  Saltelli's scheme extends the Sobol sequence in a way to reduce\r\n    the error rates in the resulting sensitivity index calculations.  If\r\n    calc_second_order is False, the resulting matrix has N * (D + 2)\r\n    rows, where D is the number of parameters.  If calc_second_order is True,\r\n    the resulting matrix has N * (2D + 2) rows.  These model inputs are\r\n    intended to be used with :func:`SALib.analyze.sobol.analyze`.\r\n\r\n    Parameters\r\n    ----------\r\n    problem : dict\r\n        The problem definition\r\n    N : int\r\n        The number of samples to generate\r\n    calc_second_order : bool\r\n        Calculate second-order sensitivities (default True)\r\n    \"\"\"\r\n    if seed:\r\n        np.random.seed(seed)\r\n\r\n    D = problem['num_vars']\r\n    groups = problem.get('groups')\r\n\r\n    if not groups:\r\n        Dg = problem['num_vars']\r\n    else:\r\n        Dg = len(set(groups))\r\n        G, group_names = compute_groups_matrix(groups)\r\n\r\n    # How many values of the Sobol sequence to skip\r\n    skip_values = 1000\r\n\r\n    # Create base sequence - could be any type of sampling\r\n    base_sequence = sobol_sequence.sample(N + skip_values, 2 * D)\r\n\r\n    if calc_second_order:\r\n        saltelli_sequence = np.zeros([(2 * Dg + 2) * N, D])\r\n    else:\r\n        saltelli_sequence = np.zeros([(Dg + 2) * N, D])\r\n    index = 0\r\n\r\n    for i in range(skip_values, N + skip_values):\r\n\r\n        # Copy matrix \"A\"\r\n        for j in range(D):\r\n            saltelli_sequence[index, j] = base_sequence[i, j]\r\n\r\n        index += 1\r\n\r\n        # Cross-sample elements of \"B\" into \"A\"\r\n        for k in range(Dg):\r\n            for j in range(D):\r\n                if (not groups and j == k) or (groups and group_names[k] == groups[j]):\r\n                    saltelli_sequence[index, j] = base_sequence[i, j + D]\r\n                else:\r\n                    saltelli_sequence[index, j] = base_sequence[i, j]\r\n\r\n            index += 1\r\n\r\n        # Cross-sample elements of \"A\" into \"B\"\r\n        # Only needed if you're doing second-order indices (true by default)\r\n        if calc_second_order:\r\n            for k in range(Dg):\r\n                for j in range(D):\r\n                    if (not groups and j == k) or (groups and group_names[k] == groups[j]):\r\n                        saltelli_sequence[index, j] = base_sequence[i, j]\r\n                    else:\r\n                        saltelli_sequence[index, j] = base_sequence[i, j + D]\r\n\r\n                index += 1\r\n\r\n        # Copy matrix \"B\"\r\n        for j in range(D):\r\n            saltelli_sequence[index, j] = base_sequence[i, j + D]\r\n\r\n        index += 1\r\n    if not problem.get('dists'):\r\n        # scaling values out of 0-1 range with uniform distributions\r\n        scale_samples(saltelli_sequence, problem['bounds'])\r\n        return saltelli_sequence\r\n    else:\r\n        # scaling values to other distributions based on inverse CDFs\r\n        scaled_saltelli = nonuniform_scale_samples(\r\n            saltelli_sequence, problem['bounds'], problem['dists'])\r\n        return scaled_saltelli"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert dict structure into Pandas DataFrame.", "response": "def to_df(self):\n        '''Convert dict structure into Pandas DataFrame.'''\n        return pd.DataFrame({k: v for k, v in self.items() if k is not 'names'},\n                            index=self['names'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample(N, D):\r\n    scale = 31\r\n    result = np.zeros([N, D])\r\n    \r\n    if D > len(directions) + 1:\r\n        raise ValueError(\"Error in Sobol sequence: not enough dimensions\")\r\n\r\n    L = int(math.ceil(math.log(N) / math.log(2)))\r\n\r\n    if L > scale:\r\n        raise ValueError(\"Error in Sobol sequence: not enough bits\")\r\n\r\n    for i in range(D):\r\n        V = np.zeros(L + 1, dtype=long)\r\n\r\n        if i == 0:\r\n            for j in range(1, L + 1):\r\n                V[j] = 1 << (scale - j)  # all m's = 1\r\n        else:\r\n            m = np.array(directions[i - 1], dtype=int)\r\n            a = m[0]\r\n            s = len(m) - 1\r\n\r\n            # The following code discards the first row of the ``m`` array\r\n            # Because it has floating point errors, e.g. values of 2.24e-314\r\n            if L <= s:\r\n                for j in range(1, L + 1):\r\n                    V[j] = m[j] << (scale - j)\r\n            else:\r\n                for j in range(1, s + 1):\r\n                    V[j] = m[j] << (scale - j)\r\n                for j in range(s + 1, L + 1):\r\n                    V[j] = V[j - s] ^ (V[j - s] >> s)\r\n                    for k in range(1, s):\r\n                        V[j] ^= ((a >> (s - 1 - k)) & 1) * V[j - k]\r\n\r\n        X = long(0)\r\n        for j in range(1, N):\r\n            X ^= V[index_of_least_significant_zero_bit(j - 1)]\r\n            result[j][i] = float(X / math.pow(2, scale))\r\n\r\n    return result", "response": "Generate N x D numpy array of Sobol sequence samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef horizontal_bar_plot(ax, Si, param_dict, sortby='mu_star', unit=''):\n    '''Updates a matplotlib axes instance with a horizontal bar plot\n\n    of mu_star, with error bars representing mu_star_conf\n    '''\n\n    assert sortby in ['mu_star', 'mu_star_conf', 'sigma', 'mu']\n\n    # Sort all the plotted elements by mu_star (or optionally another\n    # metric)\n    names_sorted = _sort_Si(Si, 'names', sortby)\n    mu_star_sorted = _sort_Si(Si, 'mu_star', sortby)\n    mu_star_conf_sorted = _sort_Si(Si, 'mu_star_conf', sortby)\n\n    # Plot horizontal barchart\n    y_pos = np.arange(len(mu_star_sorted))\n    plot_names = names_sorted\n\n    out = ax.barh(y_pos,\n                  mu_star_sorted,\n                  xerr=mu_star_conf_sorted,\n                  align='center',\n                  ecolor='black',\n                  **param_dict)\n\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(plot_names)\n    ax.set_xlabel(r'$\\mu^\\star$' + unit)\n\n    ax.set_ylim(min(y_pos)-1, max(y_pos)+1)\n\n    return out", "response": "Plots a horizontal bar plot of mu_star and mu_star_conf."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef covariance_plot(ax, Si, param_dict, unit=\"\"):\n    '''Plots mu* against sigma or the 95% confidence interval\n\n    '''\n\n    if Si['sigma'] is not None:\n        # sigma is not present if using morris groups\n        y = Si['sigma']\n        out = ax.scatter(Si['mu_star'], y, c=u'k', marker=u'o',\n                         **param_dict)\n        ax.set_ylabel(r'$\\sigma$')\n\n        ax.set_xlim(0,)\n        ax.set_ylim(0,)\n\n        x_axis_bounds = np.array(ax.get_xlim())\n\n        line1, = ax.plot(x_axis_bounds, x_axis_bounds, 'k-')\n        line2, = ax.plot(x_axis_bounds, 0.5 * x_axis_bounds, 'k--')\n        line3, = ax.plot(x_axis_bounds, 0.1 * x_axis_bounds, 'k-.')\n\n        ax.legend((line1, line2, line3), (r'$\\sigma / \\mu^{\\star} = 1.0$',\n                                          r'$\\sigma / \\mu^{\\star} = 0.5$',\n                                          r'$\\sigma / \\mu^{\\star} = 0.1$'),\n                  loc='best')\n\n    else:\n        y = Si['mu_star_conf']\n        out = ax.scatter(Si['mu_star'], y, c=u'k', marker=u'o',\n                         **param_dict)\n        ax.set_ylabel(r'$95\\% CI$')\n\n    ax.set_xlabel(r'$\\mu^\\star$ ' + unit)\n    ax.set_ylim(0-(0.01 * np.array(ax.get_ylim()[1])), )\n\n    return out", "response": "Plots mu^star against sigma or 95% confidence interval"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting a set of subplots of histograms of the input sample", "response": "def sample_histograms(fig, input_sample, problem, param_dict):\n    '''Plots a set of subplots of histograms of the input sample\n\n    '''\n    num_vars = problem['num_vars']\n    names = problem['names']\n\n    framing = 101 + (num_vars * 10)\n\n    # Find number of levels\n    num_levels = len(set(input_sample[:, 1]))\n\n    out = []\n\n    for variable in range(num_vars):\n        ax = fig.add_subplot(framing + variable)\n        out.append(ax.hist(input_sample[:, variable],\n                           bins=num_levels,\n                           normed=False,\n                           label=None,\n                           **param_dict))\n\n        ax.set_title('%s' % (names[variable]))\n        ax.tick_params(axis='x',  # changes apply to the x-axis\n                       which='both',  # both major and minor ticks are affected\n                       bottom='off',  # ticks along the bottom edge are off\n                       top='off',  # ticks along the top edge are off\n                       labelbottom='off')  # labels along the bottom edge off)\n        if variable > 0:\n            ax.tick_params(axis='y',  # changes apply to the y-axis\n                           which='both',  # both major and minor ticks affected\n                           labelleft='off')  # labels along the left edge off)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extend_bounds(problem):\n\n    num_vars = problem['num_vars']\n    num_ff_vars = 2 ** find_smallest(num_vars)\n    num_dummy_variables = num_ff_vars - num_vars\n\n    bounds = list(problem['bounds'])\n    names = problem['names']\n    if num_dummy_variables > 0:\n        bounds.extend([[0, 1] for x in range(num_dummy_variables)])\n        names.extend([\"dummy_\" + str(var)\n                      for var in range(num_dummy_variables)])\n        problem['bounds'] = bounds\n        problem['names'] = names\n        problem['num_vars'] = num_ff_vars\n\n    return problem", "response": "Extends the problem bounds to the nearest power of two\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the raw sample from the problem file", "response": "def generate_contrast(problem):\n    \"\"\"Generates the raw sample from the problem file\n\n    Arguments\n    =========\n    problem : dict\n        The problem definition\n    \"\"\"\n\n    num_vars = problem['num_vars']\n\n    # Find the smallest n, such that num_vars < k\n    k = [2 ** n for n in range(16)]\n    k_chosen = 2 ** find_smallest(num_vars)\n\n    # Generate the fractional factorial contrast\n    contrast = np.vstack([hadamard(k_chosen), -hadamard(k_chosen)])\n\n    return contrast"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample(problem, seed=None):\n    if seed:\n        np.random.seed(seed)\n    contrast = generate_contrast(problem)\n    sample = np.array((contrast + 1.) / 2, dtype=np.float)\n    problem = extend_bounds(problem)\n    scale_samples(sample, problem['bounds'])\n    return sample", "response": "Generates model inputs using a fractional factorial sample."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli_action(args):\n    problem = read_param_file(args.paramfile)\n    param_values = sample(problem, seed=args.seed)\n    np.savetxt(args.output, param_values, delimiter=args.delimiter,\n               fmt='%.' + str(args.precision) + 'e')", "response": "Run sampling method\n    Parameters ---------- args : argparse namespace\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding common sampling options to CLI parser.", "response": "def setup(parser):\n    \"\"\"Add common sampling options to CLI parser.\n\n    Parameters\n    ----------\n    parser : argparse object\n\n    Returns\n    ----------\n    Updated argparse object\n    \"\"\"\n    parser.add_argument(\n        '-p', '--paramfile', type=str, required=True,\n        help='Parameter Range File')\n    parser.add_argument(\n        '-o', '--output', type=str, required=True, help='Output File')\n    parser.add_argument(\n        '-s', '--seed', type=int, required=False, default=None,\n        help='Random Seed')\n    parser.add_argument(\n        '--delimiter', type=str, required=False, default=' ',\n        help='Column delimiter')\n    parser.add_argument('--precision', type=int, required=False,\n                        default=8, help='Output floating-point precision')\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_cli(cli_parser, run_sample, known_args=None):\n    parser = create(cli_parser)\n    args = parser.parse_args(known_args)\n\n    run_sample(args)", "response": "Run sampling with CLI arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the optimum k_choices of trajectories from the input_sample.", "response": "def sample(self, input_sample, num_samples, num_params, k_choices,\n               num_groups):\n        \"\"\"Computes the optimum k_choices of trajectories\n        from the input_sample.\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n            The number of samples to generate\n        num_params : int\n            The number of parameters\n        k_choices : int\n            The number of optimal trajectories\n        num_groups : int\n            The number of groups\n\n        Returns\n        -------\n        numpy.ndarray\n            An array of optimal trajectories\n        \"\"\"\n        return self._strategy.sample(input_sample, num_samples, num_params,\n                                     k_choices, num_groups)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the optimum k_choices of trajectories from the input_sample.", "response": "def sample(self, input_sample, num_samples, num_params,\n               k_choices, num_groups=None):\n        \"\"\"Computes the optimum k_choices of trajectories\n        from the input_sample.\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n            The number of samples to generate\n        num_params : int\n            The number of parameters\n        k_choices : int\n            The number of optimal trajectories\n        num_groups : int, default=None\n            The number of groups\n\n        Returns\n        -------\n        numpy.ndarray\n        \"\"\"\n        self.run_checks(num_samples, k_choices)\n        maximum_combo = self._sample(input_sample, num_samples,\n                                     num_params, k_choices, num_groups)\n\n        assert isinstance(maximum_combo, list)\n\n        output = self.compile_output(input_sample,\n                                     num_samples,\n                                     num_params,\n                                     maximum_combo,\n                                     num_groups)\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_checks(number_samples, k_choices):\n        assert isinstance(k_choices, int), \\\n            \"Number of optimal trajectories should be an integer\"\n\n        if k_choices < 2:\n            raise ValueError(\n                \"The number of optimal trajectories must be set to 2 or more.\")\n        if k_choices >= number_samples:\n            msg = \"The number of optimal trajectories should be less than the \\\n                    number of samples\"\n            raise ValueError(msg)", "response": "Runs checks on k_choices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_index_list(num_samples, num_params, num_groups=None):\n        if num_groups is None:\n            num_groups = num_params\n\n        index_list = []\n        for j in range(num_samples):\n            index_list.append(np.arange(num_groups + 1) + j * (num_groups + 1))\n        return index_list", "response": "Create a list of indices of input sample associated with each trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the output of the input_sample.", "response": "def compile_output(self, input_sample, num_samples, num_params,\n                       maximum_combo, num_groups=None):\n        \"\"\"Picks the trajectories from the input\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n        num_params : int\n        maximum_combo : list\n        num_groups : int\n\n        \"\"\"\n\n        if num_groups is None:\n            num_groups = num_params\n\n        self.check_input_sample(input_sample, num_groups, num_samples)\n\n        index_list = self._make_index_list(num_samples, num_params, num_groups)\n\n        output = np.zeros(\n            (np.size(maximum_combo) * (num_groups + 1), num_params))\n        for counter, combo in enumerate(maximum_combo):\n            output[index_list[counter]] = np.array(\n                input_sample[index_list[combo]])\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_input_sample(input_sample, num_params, num_samples):\n        assert type(input_sample) == np.ndarray, \\\n            \"Input sample is not an numpy array\"\n        assert input_sample.shape[0] == (num_params + 1) * num_samples, \\\n            \"Input sample does not match number of parameters or groups\"\n        assert np.any((input_sample >= 0) | (input_sample <= 1)), \\\n            \"Input sample must be scaled between 0 and 1\"", "response": "Checks the input sample is valid for the current version of the entry class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_distance(m, l):\n        '''Compute distance between two trajectories\n\n        Returns\n        -------\n        numpy.ndarray\n        '''\n\n        if np.shape(m) != np.shape(l):\n            raise ValueError(\"Input matrices are different sizes\")\n        if np.array_equal(m, l):\n            # print(\"Trajectory %s and %s are equal\" % (m, l))\n            distance = 0\n        else:\n            distance = np.array(np.sum(cdist(m, l)), dtype=np.float32)\n\n        return distance", "response": "Compute distance between two trajectories\n        returns numpy. ndarray"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the distance between each trajectory and every entry in the matrix.", "response": "def compute_distance_matrix(self, input_sample, num_samples, num_params,\n                                num_groups=None,\n                                local_optimization=False):\n        \"\"\"Computes the distance between each and every trajectory\n\n        Each entry in the matrix represents the sum of the geometric distances\n        between all the pairs of points of the two trajectories\n\n        If the `groups` argument is filled, then the distances are still\n        calculated for each trajectory,\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n            The input sample of trajectories for which to compute\n            the distance matrix\n        num_samples : int\n            The number of trajectories\n        num_params : int\n            The number of factors\n        num_groups : int, default=None\n            The number of groups\n        local_optimization : bool, default=False\n            If True, fills the lower triangle of the distance matrix\n\n        Returns\n        -------\n        distance_matrix : numpy.ndarray\n\n        \"\"\"\n        if num_groups:\n            self.check_input_sample(input_sample, num_groups, num_samples)\n        else:\n            self.check_input_sample(input_sample, num_params, num_samples)\n\n        index_list = self._make_index_list(num_samples, num_params, num_groups)\n        distance_matrix = np.zeros(\n            (num_samples, num_samples), dtype=np.float32)\n\n        for j in range(num_samples):\n            input_1 = input_sample[index_list[j]]\n            for k in range(j + 1, num_samples):\n                input_2 = input_sample[index_list[k]]\n\n                # Fills the lower triangle of the matrix\n                if local_optimization is True:\n                    distance_matrix[j, k] = self.compute_distance(\n                        input_1, input_2)\n\n                distance_matrix[k, j] = self.compute_distance(input_1, input_2)\n        return distance_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the centroids according to Huang [ 1997 ].", "response": "def init_huang(X, n_clusters, dissim, random_state):\n    \"\"\"Initialize centroids according to method by Huang [1997].\"\"\"\n    n_attrs = X.shape[1]\n    centroids = np.empty((n_clusters, n_attrs), dtype='object')\n    # determine frequencies of attributes\n    for iattr in range(n_attrs):\n        freq = defaultdict(int)\n        for curattr in X[:, iattr]:\n            freq[curattr] += 1\n        # Sample centroids using the probabilities of attributes.\n        # (I assume that's what's meant in the Huang [1998] paper; it works,\n        # at least)\n        # Note: sampling using population in static list with as many choices\n        # as frequency counts. Since the counts are small integers,\n        # memory consumption is low.\n        choices = [chc for chc, wght in freq.items() for _ in range(wght)]\n        # So that we are consistent between Python versions,\n        # each with different dict ordering.\n        choices = sorted(choices)\n        centroids[:, iattr] = random_state.choice(choices, n_clusters)\n    # The previously chosen centroids could result in empty clusters,\n    # so set centroid to closest point in X.\n    for ik in range(n_clusters):\n        ndx = np.argsort(dissim(X, centroids[ik]))\n        # We want the centroid to be unique, if possible.\n        while np.all(X[ndx[0]] == centroids, axis=1).any() and ndx.shape[0] > 1:\n            ndx = np.delete(ndx, 0)\n        centroids[ik] = X[ndx[0]]\n\n    return centroids"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_cao(X, n_clusters, dissim):\n    n_points, n_attrs = X.shape\n    centroids = np.empty((n_clusters, n_attrs), dtype='object')\n    # Method is based on determining density of points.\n    dens = np.zeros(n_points)\n    for iattr in range(n_attrs):\n        freq = defaultdict(int)\n        for val in X[:, iattr]:\n            freq[val] += 1\n        for ipoint in range(n_points):\n            dens[ipoint] += freq[X[ipoint, iattr]] / float(n_points) / float(n_attrs)\n\n    # Choose initial centroids based on distance and density.\n    centroids[0] = X[np.argmax(dens)]\n    if n_clusters > 1:\n        # For the remaining centroids, choose maximum dens * dissim to the\n        # (already assigned) centroid with the lowest dens * dissim.\n        for ik in range(1, n_clusters):\n            dd = np.empty((ik, n_points))\n            for ikk in range(ik):\n                dd[ikk] = dissim(X, centroids[ikk]) * dens\n            centroids[ik] = X[np.argmax(np.min(dd, axis=0))]\n\n    return centroids", "response": "Initialize the centroids according to method by Cao et al. [ 2009 ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef move_point_cat(point, ipoint, to_clust, from_clust, cl_attr_freq,\n                   membship, centroids):\n    \"\"\"Move point between clusters, categorical attributes.\"\"\"\n    membship[to_clust, ipoint] = 1\n    membship[from_clust, ipoint] = 0\n    # Update frequencies of attributes in cluster.\n    for iattr, curattr in enumerate(point):\n        to_attr_counts = cl_attr_freq[to_clust][iattr]\n        from_attr_counts = cl_attr_freq[from_clust][iattr]\n\n        # Increment the attribute count for the new \"to\" cluster\n        to_attr_counts[curattr] += 1\n\n        current_attribute_value_freq = to_attr_counts[curattr]\n        current_centroid_value = centroids[to_clust][iattr]\n        current_centroid_freq = to_attr_counts[current_centroid_value]\n        if current_centroid_freq < current_attribute_value_freq:\n            # We have incremented this value to the new mode. Update the centroid.\n            centroids[to_clust][iattr] = curattr\n\n        # Decrement the attribute count for the old \"from\" cluster\n        from_attr_counts[curattr] -= 1\n\n        old_centroid_value = centroids[from_clust][iattr]\n        if old_centroid_value == curattr:\n            # We have just removed a count from the old centroid value. We need to\n            # recalculate the centroid as it may no longer be the maximum\n            centroids[from_clust][iattr] = get_max_value_key(from_attr_counts)\n\n    return cl_attr_freq, membship, centroids", "response": "Move point between clusters categorical attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates labels and cost function given a matrix of points and a list of centroids for the k - modes algorithm.", "response": "def _labels_cost(X, centroids, dissim, membship=None):\n    \"\"\"Calculate labels and cost function given a matrix of points and\n    a list of centroids for the k-modes algorithm.\n    \"\"\"\n\n    X = check_array(X)\n\n    n_points = X.shape[0]\n    cost = 0.\n    labels = np.empty(n_points, dtype=np.uint16)\n    for ipoint, curpoint in enumerate(X):\n        diss = dissim(centroids, curpoint, X=X, membship=membship)\n        clust = np.argmin(diss)\n        labels[ipoint] = clust\n        cost += diss[clust]\n\n    return labels, cost"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _k_modes_iter(X, centroids, cl_attr_freq, membship, dissim, random_state):\n    moves = 0\n    for ipoint, curpoint in enumerate(X):\n        clust = np.argmin(dissim(centroids, curpoint, X=X, membship=membship))\n        if membship[clust, ipoint]:\n            # Point is already in its right place.\n            continue\n\n        # Move point, and update old/new cluster frequencies and centroids.\n        moves += 1\n        old_clust = np.argwhere(membship[:, ipoint])[0][0]\n\n        cl_attr_freq, membship, centroids = move_point_cat(\n            curpoint, ipoint, clust, old_clust, cl_attr_freq, membship, centroids\n        )\n\n        # In case of an empty cluster, reinitialize with a random point\n        # from the largest cluster.\n        if not membship[old_clust, :].any():\n            from_clust = membship.sum(axis=1).argmax()\n            choices = [ii for ii, ch in enumerate(membship[from_clust, :]) if ch]\n            rindx = random_state.choice(choices)\n\n            cl_attr_freq, membship, centroids = move_point_cat(\n                X[rindx], rindx, old_clust, from_clust, cl_attr_freq, membship, centroids\n            )\n\n    return centroids, moves", "response": "K - modes clustering algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute k - modes clustering.", "response": "def fit(self, X, y=None, **kwargs):\n        \"\"\"Compute k-modes clustering.\n\n        Parameters\n        ----------\n        X : array-like, shape=[n_samples, n_features]\n        \"\"\"\n        X = pandas_to_numpy(X)\n\n        random_state = check_random_state(self.random_state)\n        self._enc_cluster_centroids, self._enc_map, self.labels_,\\\n            self.cost_, self.n_iter_ = k_modes(X,\n                                               self.n_clusters,\n                                               self.max_iter,\n                                               self.cat_dissim,\n                                               self.init,\n                                               self.n_init,\n                                               self.verbose,\n                                               random_state,\n                                               self.n_jobs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fit_predict(self, X, y=None, **kwargs):\n        return self.fit(X, **kwargs).predict(X, **kwargs)", "response": "Compute cluster centroids and predict cluster index for each sample."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict(self, X, **kwargs):\n\n        assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\"\n\n        if self.verbose and self.cat_dissim == ng_dissim:\n            print(\"Ng's dissimilarity measure was used to train this model, \"\n                  \"but now that it is predicting the model will fall back to \"\n                  \"using simple matching dissimilarity.\")\n\n        X = pandas_to_numpy(X)\n        X = check_array(X, dtype=None)\n        X, _ = encode_features(X, enc_map=self._enc_map)\n        return _labels_cost(X, self._enc_cluster_centroids, self.cat_dissim)[0]", "response": "Predict the closest cluster each sample in X belongs to."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_max_value_key(dic):\n    v = np.array(list(dic.values()))\n    k = np.array(list(dic.keys()))\n\n    maxima = np.where(v == np.max(v))[0]\n    if len(maxima) == 1:\n        return k[maxima[0]]\n    else:\n        # In order to be consistent, always selects the minimum key\n        # (guaranteed to be unique) when there are multiple maximum values.\n        return k[maxima[np.argmin(k[maxima])]]", "response": "Gets the key for the maximum value in a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode_features(X, enc_map=None):\n    if enc_map is None:\n        fit = True\n        # We will calculate enc_map, so initialize the list of column mappings.\n        enc_map = []\n    else:\n        fit = False\n\n    Xenc = np.zeros(X.shape).astype('int')\n    for ii in range(X.shape[1]):\n        if fit:\n            col_enc = {val: jj for jj, val in enumerate(np.unique(X[:, ii]))\n                       if not (isinstance(val, float) and np.isnan(val))}\n            enc_map.append(col_enc)\n        # Unknown categories (including np.NaNs) all get a value of -1.\n        Xenc[:, ii] = np.array([enc_map[ii].get(x, -1) for x in X[:, ii]])\n\n    return Xenc, enc_map", "response": "Converts categorical values in each column of X to integers in the range\n    [ 0 n_unique_values_in_column - 1 ]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_centroids(encoded, mapping):\n    decoded = []\n    for ii in range(encoded.shape[1]):\n        # Invert the mapping so that we can decode.\n        inv_mapping = {v: k for k, v in mapping[ii].items()}\n        decoded.append(np.vectorize(inv_mapping.__getitem__)(encoded[:, ii]))\n    return np.atleast_2d(np.array(decoded)).T", "response": "Decodes the encoded centroids array back to the original data\n    labels using a list of mappings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmoving point between clusters numerical attributes.", "response": "def move_point_num(point, to_clust, from_clust, cl_attr_sum, cl_memb_sum):\n    \"\"\"Move point between clusters, numerical attributes.\"\"\"\n    # Update sum of attributes in cluster.\n    for iattr, curattr in enumerate(point):\n        cl_attr_sum[to_clust][iattr] += curattr\n        cl_attr_sum[from_clust][iattr] -= curattr\n    # Update sums of memberships in cluster\n    cl_memb_sum[to_clust] += 1\n    cl_memb_sum[from_clust] -= 1\n    return cl_attr_sum, cl_memb_sum"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _split_num_cat(X, categorical):\n    Xnum = np.asanyarray(X[:, [ii for ii in range(X.shape[1])\n                               if ii not in categorical]]).astype(np.float64)\n    Xcat = np.asanyarray(X[:, categorical])\n    return Xnum, Xcat", "response": "Extract numerical and categorical columns from feature matrix X."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate labels and cost function given a matrix of points Xnum and Xcat and k - prototypes algorithm.", "response": "def _labels_cost(Xnum, Xcat, centroids, num_dissim, cat_dissim, gamma, membship=None):\n    \"\"\"Calculate labels and cost function given a matrix of points and\n    a list of centroids for the k-prototypes algorithm.\n    \"\"\"\n\n    n_points = Xnum.shape[0]\n    Xnum = check_array(Xnum)\n\n    cost = 0.\n    labels = np.empty(n_points, dtype=np.uint16)\n    for ipoint in range(n_points):\n        # Numerical cost = sum of Euclidean distances\n        num_costs = num_dissim(centroids[0], Xnum[ipoint])\n        cat_costs = cat_dissim(centroids[1], Xcat[ipoint], X=Xcat, membship=membship)\n        # Gamma relates the categorical cost to the numerical cost.\n        tot_costs = num_costs + gamma * cat_costs\n        clust = np.argmin(tot_costs)\n        labels[ipoint] = clust\n        cost += tot_costs[clust]\n\n    return labels, cost"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsingling iteration of the k-prototypes algorithm", "response": "def _k_prototypes_iter(Xnum, Xcat, centroids, cl_attr_sum, cl_memb_sum, cl_attr_freq,\n                       membship, num_dissim, cat_dissim, gamma, random_state):\n    \"\"\"Single iteration of the k-prototypes algorithm\"\"\"\n    moves = 0\n    for ipoint in range(Xnum.shape[0]):\n        clust = np.argmin(\n            num_dissim(centroids[0], Xnum[ipoint]) +\n            gamma * cat_dissim(centroids[1], Xcat[ipoint], X=Xcat, membship=membship)\n        )\n        if membship[clust, ipoint]:\n            # Point is already in its right place.\n            continue\n\n        # Move point, and update old/new cluster frequencies and centroids.\n        moves += 1\n        old_clust = np.argwhere(membship[:, ipoint])[0][0]\n\n        # Note that membship gets updated by kmodes.move_point_cat.\n        # move_point_num only updates things specific to the k-means part.\n        cl_attr_sum, cl_memb_sum = move_point_num(\n            Xnum[ipoint], clust, old_clust, cl_attr_sum, cl_memb_sum\n        )\n        cl_attr_freq, membship, centroids[1] = kmodes.move_point_cat(\n            Xcat[ipoint], ipoint, clust, old_clust,\n            cl_attr_freq, membship, centroids[1]\n        )\n\n        # Update old and new centroids for numerical attributes using\n        # the means and sums of all values\n        for iattr in range(len(Xnum[ipoint])):\n            for curc in (clust, old_clust):\n                if cl_memb_sum[curc]:\n                    centroids[0][curc, iattr] = cl_attr_sum[curc, iattr] / cl_memb_sum[curc]\n                else:\n                    centroids[0][curc, iattr] = 0.\n\n        # In case of an empty cluster, reinitialize with a random point\n        # from largest cluster.\n        if not cl_memb_sum[old_clust]:\n            from_clust = membship.sum(axis=1).argmax()\n            choices = [ii for ii, ch in enumerate(membship[from_clust, :]) if ch]\n            rindx = random_state.choice(choices)\n\n            cl_attr_sum, cl_memb_sum = move_point_num(\n                Xnum[rindx], old_clust, from_clust, cl_attr_sum, cl_memb_sum\n            )\n            cl_attr_freq, membship, centroids[1] = kmodes.move_point_cat(\n                Xcat[rindx], rindx, old_clust, from_clust,\n                cl_attr_freq, membship, centroids[1]\n            )\n\n    return centroids, moves"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef k_prototypes(X, categorical, n_clusters, max_iter, num_dissim, cat_dissim,\n                 gamma, init, n_init, verbose, random_state, n_jobs):\n    \"\"\"k-prototypes algorithm\"\"\"\n    random_state = check_random_state(random_state)\n    if sparse.issparse(X):\n        raise TypeError(\"k-prototypes does not support sparse data.\")\n\n    if categorical is None or not categorical:\n        raise NotImplementedError(\n            \"No categorical data selected, effectively doing k-means. \"\n            \"Present a list of categorical columns, or use scikit-learn's \"\n            \"KMeans instead.\"\n        )\n    if isinstance(categorical, int):\n        categorical = [categorical]\n    assert len(categorical) != X.shape[1], \\\n        \"All columns are categorical, use k-modes instead of k-prototypes.\"\n    assert max(categorical) < X.shape[1], \\\n        \"Categorical index larger than number of columns.\"\n\n    ncatattrs = len(categorical)\n    nnumattrs = X.shape[1] - ncatattrs\n    n_points = X.shape[0]\n    assert n_clusters <= n_points, \"Cannot have more clusters ({}) \" \\\n                                   \"than data points ({}).\".format(n_clusters, n_points)\n\n    Xnum, Xcat = _split_num_cat(X, categorical)\n    Xnum, Xcat = check_array(Xnum), check_array(Xcat, dtype=None)\n\n    # Convert the categorical values in Xcat to integers for speed.\n    # Based on the unique values in Xcat, we can make a mapping to achieve this.\n    Xcat, enc_map = encode_features(Xcat)\n\n    # Are there more n_clusters than unique rows? Then set the unique\n    # rows as initial values and skip iteration.\n    unique = get_unique_rows(X)\n    n_unique = unique.shape[0]\n    if n_unique <= n_clusters:\n        max_iter = 0\n        n_init = 1\n        n_clusters = n_unique\n        init = list(_split_num_cat(unique, categorical))\n        init[1], _ = encode_features(init[1], enc_map)\n\n    # Estimate a good value for gamma, which determines the weighing of\n    # categorical values in clusters (see Huang [1997]).\n    if gamma is None:\n        gamma = 0.5 * Xnum.std()\n\n    results = []\n    seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n    if n_jobs == 1:\n        for init_no in range(n_init):\n            results.append(k_prototypes_single(Xnum, Xcat, nnumattrs, ncatattrs,\n                                               n_clusters, n_points, max_iter,\n                                               num_dissim, cat_dissim, gamma,\n                                               init, init_no, verbose, seeds[init_no]))\n    else:\n        results = Parallel(n_jobs=n_jobs, verbose=0)(\n            delayed(k_prototypes_single)(Xnum, Xcat, nnumattrs, ncatattrs,\n                                         n_clusters, n_points, max_iter,\n                                         num_dissim, cat_dissim, gamma,\n                                         init, init_no, verbose, seed)\n            for init_no, seed in enumerate(seeds))\n    all_centroids, all_labels, all_costs, all_n_iters = zip(*results)\n\n    best = np.argmin(all_costs)\n    if n_init > 1 and verbose:\n        print(\"Best run was number {}\".format(best + 1))\n\n    # Note: return gamma in case it was automatically determined.\n    return all_centroids[best], enc_map, all_labels[best], \\\n        all_costs[best], all_n_iters[best], gamma", "response": "k - prototypes algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self, X, y=None, categorical=None):\n        if categorical is not None:\n            assert isinstance(categorical, (int, list, tuple)), \"The 'categorical' \\\n                argument needs to be an integer with the index of the categorical \\\n                column in your data, or a list or tuple of several of them, \\\n                but it is a {}.\".format(type(categorical))\n\n        X = pandas_to_numpy(X)\n\n        random_state = check_random_state(self.random_state)\n        # If self.gamma is None, gamma will be automatically determined from\n        # the data. The function below returns its value.\n        self._enc_cluster_centroids, self._enc_map, self.labels_, self.cost_,\\\n            self.n_iter_, self.gamma = k_prototypes(X,\n                                                    categorical,\n                                                    self.n_clusters,\n                                                    self.max_iter,\n                                                    self.num_dissim,\n                                                    self.cat_dissim,\n                                                    self.gamma,\n                                                    self.init,\n                                                    self.n_init,\n                                                    self.verbose,\n                                                    random_state,\n                                                    self.n_jobs)\n        return self", "response": "Compute k - prototypes clustering."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict(self, X, categorical=None):\n        assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\"\n\n        if categorical is not None:\n            assert isinstance(categorical, (int, list, tuple)), \"The 'categorical' \\\n                argument needs to be an integer with the index of the categorical \\\n                column in your data, or a list or tuple of several of them, \\\n                but it is a {}.\".format(type(categorical))\n\n        X = pandas_to_numpy(X)\n        Xnum, Xcat = _split_num_cat(X, categorical)\n        Xnum, Xcat = check_array(Xnum), check_array(Xcat, dtype=None)\n        Xcat, _ = encode_features(Xcat, enc_map=self._enc_map)\n        return _labels_cost(Xnum, Xcat, self._enc_cluster_centroids,\n                            self.num_dissim, self.cat_dissim, self.gamma)[0]", "response": "Predict the closest cluster each sample in X belongs to."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef euclidean_dissim(a, b, **_):\n    if np.isnan(a).any() or np.isnan(b).any():\n        raise ValueError(\"Missing values detected in numerical columns.\")\n    return np.sum((a - b) ** 2, axis=1)", "response": "Euclidean distance dissimilarity function"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new consumer that retrieves messages from Redis.", "response": "def consume(self, queue_name, prefetch=1, timeout=5000):\n        \"\"\"Create a new consumer for a queue.\n\n        Parameters:\n          queue_name(str): The queue to consume.\n          prefetch(int): The number of messages to prefetch.\n          timeout(int): The idle timeout in milliseconds.\n\n        Returns:\n          Consumer: A consumer that retrieves messages from Redis.\n        \"\"\"\n        return _RedisConsumer(self, queue_name, prefetch, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flush(self, queue_name):\n        for name in (queue_name, dq_name(queue_name)):\n            self.do_purge(name)", "response": "Drop all the messages from a queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait for all the messages on the given queue to be processed.", "response": "def join(self, queue_name, *, interval=100, timeout=None):\n        \"\"\"Wait for all the messages on the given queue to be\n        processed.  This method is only meant to be used in tests to\n        wait for all the messages in a queue to be processed.\n\n        Raises:\n          QueueJoinTimeout: When the timeout elapses.\n\n        Parameters:\n          queue_name(str): The queue to wait on.\n          interval(Optional[int]): The interval, in milliseconds, at\n            which to check the queues.\n          timeout(Optional[int]): The max amount of time, in\n            milliseconds, to wait on this queue.\n        \"\"\"\n        deadline = timeout and time.monotonic() + timeout / 1000\n        while True:\n            if deadline and time.monotonic() >= deadline:\n                raise QueueJoinTimeout(queue_name)\n\n            size = 0\n            for name in (queue_name, dq_name(queue_name)):\n                size += self.do_qsize(name)\n\n            if size == 0:\n                return\n\n            time.sleep(interval / 1000)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a result from the backend.", "response": "def get_result(self, message, *, block: bool = False, timeout: int = None) -> Result:\n        \"\"\"Get a result from the backend.\n\n        Parameters:\n          message(Message)\n          block(bool): Whether or not to block until a result is set.\n          timeout(int): The maximum amount of time, in ms, to wait for\n            a result when block is True.  Defaults to 10 seconds.\n\n        Raises:\n          ResultMissing: When block is False and the result isn't set.\n          ResultTimeout: When waiting for a result times out.\n\n        Returns:\n          object: The result.\n        \"\"\"\n        if timeout is None:\n            timeout = DEFAULT_TIMEOUT\n\n        end_time = time.monotonic() + timeout / 1000\n        message_key = self.build_message_key(message)\n\n        attempts = 0\n        while True:\n            result = self._get(message_key)\n            if result is Missing and block:\n                attempts, delay = compute_backoff(attempts, factor=BACKOFF_FACTOR)\n                delay /= 1000\n                if time.monotonic() + delay > end_time:\n                    raise ResultTimeout(message)\n\n                time.sleep(delay)\n                continue\n\n            elif result is Missing:\n                raise ResultMissing(message)\n\n            else:\n                return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstores a result in the backend.", "response": "def store_result(self, message, result: Result, ttl: int) -> None:\n        \"\"\"Store a result in the backend.\n\n        Parameters:\n          message(Message)\n          result(object): Must be serializable.\n          ttl(int): The maximum amount of time the result may be\n            stored in the backend for.\n        \"\"\"\n        message_key = self.build_message_key(message)\n        return self._store(message_key, result, ttl)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a globally - unique key for a message.", "response": "def build_message_key(self, message) -> str:\n        \"\"\"Given a message, return its globally-unique key.\n\n        Parameters:\n          message(Message)\n\n        Returns:\n          str\n        \"\"\"\n        message_key = \"%(namespace)s:%(queue_name)s:%(actor_name)s:%(message_id)s\" % {\n            \"namespace\": self.namespace,\n            \"queue_name\": q_name(message.queue_name),\n            \"actor_name\": message.actor_name,\n            \"message_id\": message.message_id,\n        }\n        return hashlib.md5(message_key.encode(\"utf-8\")).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _store(self, message_key: str, result: Result, ttl: int) -> None:  # pragma: no cover\n        raise NotImplementedError(\"%(classname)r does not implement _store()\" % {\n            \"classname\": type(self).__name__,\n        })", "response": "Store a result in the backend."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef acquire(self, *, raise_on_failure=True):\n        acquired = False\n\n        try:\n            acquired = self._acquire()\n            if raise_on_failure and not acquired:\n                raise RateLimitExceeded(\"rate limit exceeded for key %(key)r\" % vars(self))\n\n            yield acquired\n        finally:\n            if acquired:\n                self._release()", "response": "A context manager function that acquires a new item under this rate limiter."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nattempts to acquire a POSIX file lock.", "response": "def flock(path):\n    \"\"\"Attempt to acquire a POSIX file lock.\n    \"\"\"\n    with open(path, \"w+\") as lf:\n        try:\n            fcntl.flock(lf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n            acquired = True\n            yield acquired\n\n        except OSError:\n            acquired = False\n            yield acquired\n\n        finally:\n            if acquired:\n                fcntl.flock(lf, fcntl.LOCK_UN)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self, **attributes):\n        updated_options = attributes.pop(\"options\", {})\n        options = self.options.copy()\n        options.update(updated_options)\n        return self._replace(**attributes, options=options)", "response": "Create a copy of this message."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the result associated with this message from a result backend.", "response": "def get_result(self, *, backend=None, block=False, timeout=None):\n        \"\"\"Get the result associated with this message from a result\n        backend.\n\n        Warning:\n          If you use multiple result backends or brokers you should\n          always pass the backend parameter.  This method is only able\n          to infer the result backend off of the default broker.\n\n        Parameters:\n          backend(ResultBackend): The result backend to use to get the\n            result.  If omitted, this method will try to find and use\n            the result backend on the default broker instance.\n          block(bool): Whether or not to block while waiting for a\n            result.\n          timeout(int): The maximum amount of time, in ms, to block\n            while waiting for a result.\n\n        Raises:\n          RuntimeError: If there is no result backend on the default\n            broker.\n          ResultMissing: When block is False and the result isn't set.\n          ResultTimeout: When waiting for a result times out.\n\n        Returns:\n          object: The result.\n        \"\"\"\n        if not backend:\n            broker = get_broker()\n            for middleware in broker.middleware:\n                if isinstance(middleware, Results):\n                    backend = middleware.backend\n                    break\n            else:\n                raise RuntimeError(\"The default broker doesn't have a results backend.\")\n\n        return backend.get_result(self, block=block, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef file_or_stderr(filename, *, mode=\"a\", encoding=\"utf-8\"):\n    if filename is not None:\n        return open(filename, mode, encoding=encoding)\n\n    @contextmanager\n    def stderr_wrapper():\n        yield sys.stderr\n\n    return stderr_wrapper()", "response": "Returns a context manager that opens either the given file or the given stderr."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_backoff(attempts, *, factor=5, jitter=True, max_backoff=2000, max_exponent=32):\n    exponent = min(attempts, max_exponent)\n    backoff = min(factor * 2 ** exponent, max_backoff)\n    if jitter:\n        backoff /= 2\n        backoff = int(backoff + uniform(0, backoff))\n    return attempts + 1, backoff", "response": "Compute an exponential backoff value based on some number of attempts."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwaiting on a list of objects that can be joined with a total timeout represented by timeout.", "response": "def join_all(joinables, timeout):\n    \"\"\"Wait on a list of objects that can be joined with a total\n    timeout represented by ``timeout``.\n\n    Parameters:\n      joinables(object): Objects with a join method.\n      timeout(int): The total timeout in milliseconds.\n    \"\"\"\n    started, elapsed = current_millis(), 0\n    for ob in joinables:\n        ob.join(timeout=timeout / 1000)\n        elapsed = current_millis() - started\n        timeout = max(0, timeout - elapsed)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dq_name(queue_name):\n    if queue_name.endswith(\".DQ\"):\n        return queue_name\n\n    if queue_name.endswith(\".XQ\"):\n        queue_name = queue_name[:-3]\n    return queue_name + \".DQ\"", "response": "Returns the delayed queue name for a given queue name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the dead letter queue name for a given queue name.", "response": "def xq_name(queue_name):\n    \"\"\"Returns the dead letter queue name for a given queue.  If the\n    given queue name belongs to a delayed queue, the dead letter queue\n    name for the original queue is generated.\n    \"\"\"\n    if queue_name.endswith(\".XQ\"):\n        return queue_name\n\n    if queue_name.endswith(\".DQ\"):\n        queue_name = queue_name[:-3]\n    return queue_name + \".XQ\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_broker() -> \"Broker\":\n    global global_broker\n    if global_broker is None:\n        from .brokers.rabbitmq import RabbitmqBroker\n\n        set_broker(RabbitmqBroker(\n            host=\"127.0.0.1\",\n            port=5672,\n            heartbeat=5,\n            connection_attempts=5,\n            blocked_connection_timeout=30,\n        ))\n    return global_broker", "response": "Get the global broker instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a middleware object to the broker.", "response": "def add_middleware(self, middleware, *, before=None, after=None):\n        \"\"\"Add a middleware object to this broker.  The middleware is\n        appended to the end of the middleware list by default.\n\n        You can specify another middleware (by class) as a reference\n        point for where the new middleware should be added.\n\n        Parameters:\n          middleware(Middleware): The middleware.\n          before(type): Add this middleware before a specific one.\n          after(type): Add this middleware after a specific one.\n\n        Raises:\n          ValueError: When either ``before`` or ``after`` refer to a\n            middleware that hasn't been registered yet.\n        \"\"\"\n        assert not (before and after), \\\n            \"provide either 'before' or 'after', but not both\"\n\n        if before or after:\n            for i, m in enumerate(self.middleware):  # noqa\n                if isinstance(m, before or after):\n                    break\n            else:\n                raise ValueError(\"Middleware %r not found\" % (before or after))\n\n            if before:\n                self.middleware.insert(i, middleware)\n            else:\n                self.middleware.insert(i + 1, middleware)\n        else:\n            self.middleware.append(middleware)\n\n        self.actor_options |= middleware.actor_options\n\n        for actor_name in self.get_declared_actors():\n            middleware.after_declare_actor(self, actor_name)\n\n        for queue_name in self.get_declared_queues():\n            middleware.after_declare_queue(self, queue_name)\n\n        for queue_name in self.get_declared_delay_queues():\n            middleware.after_declare_delay_queue(self, queue_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef declare_actor(self, actor):  # pragma: no cover\n        self.emit_before(\"declare_actor\", actor)\n        self.declare_queue(actor.queue_name)\n        self.actors[actor.actor_name] = actor\n        self.emit_after(\"declare_actor\", actor)", "response": "Declare an actor on this broker."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef URLRabbitmqBroker(url, *, middleware=None):\n    warnings.warn(\n        \"Use RabbitmqBroker with the 'url' parameter instead of URLRabbitmqBroker.\",\n        DeprecationWarning, stacklevel=2,\n    )\n    return RabbitmqBroker(url=url, middleware=middleware)", "response": "Alias for the RabbitMQ broker that takes a connection URL as a\n    positional argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef channel(self):\n        channel = getattr(self.state, \"channel\", None)\n        if channel is None:\n            channel = self.state.channel = self.connection.channel()\n            if self.confirm_delivery:\n                channel.confirm_delivery()\n\n            self.channels.add(channel)\n        return channel", "response": "The channel for the current thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n        # The main thread may keep connections open for a long time\n        # w/o publishing heartbeats, which means that they'll end up\n        # being closed by the time the broker is closed.  When that\n        # happens, pika logs a bunch of scary stuff so we want to\n        # filter that out.\n        logging_filter = _IgnoreScaryLogs()\n        logging.getLogger(\"pika.adapters.base_connection\").addFilter(logging_filter)\n        logging.getLogger(\"pika.adapters.blocking_connection\").addFilter(logging_filter)\n\n        self.logger.debug(\"Closing channels and connections...\")\n        for channel_or_conn in chain(self.channels, self.connections):\n            try:\n                channel_or_conn.close()\n            except pika.exceptions.AMQPError:\n                pass\n\n            except Exception:  # pragma: no cover\n                self.logger.debug(\"Encountered an error while closing %r.\", channel_or_conn, exc_info=True)\n        self.logger.debug(\"Channels and connections closed.\")", "response": "Close all RabbitMQ connections and channels."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef consume(self, queue_name, prefetch=1, timeout=5000):\n        return _RabbitmqConsumer(self.parameters, queue_name, prefetch, timeout)", "response": "Create a new consumer for a queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef declare_queue(self, queue_name):\n        attempts = 1\n        while True:\n            try:\n                if queue_name not in self.queues:\n                    self.emit_before(\"declare_queue\", queue_name)\n                    self._declare_queue(queue_name)\n                    self.queues.add(queue_name)\n                    self.emit_after(\"declare_queue\", queue_name)\n\n                    delayed_name = dq_name(queue_name)\n                    self._declare_dq_queue(queue_name)\n                    self.delay_queues.add(delayed_name)\n                    self.emit_after(\"declare_delay_queue\", delayed_name)\n\n                    self._declare_xq_queue(queue_name)\n                break\n            except (pika.exceptions.AMQPConnectionError,\n                    pika.exceptions.AMQPChannelError) as e:  # pragma: no cover\n                # Delete the channel and the connection so that the next\n                # caller may initiate new ones of each.\n                del self.channel\n                del self.connection\n\n                attempts += 1\n                if attempts > MAX_DECLARE_ATTEMPTS:\n                    raise ConnectionClosed(e) from None\n\n                self.logger.debug(\n                    \"Retrying declare due to closed connection. [%d/%d]\",\n                    attempts, MAX_DECLARE_ATTEMPTS,\n                )", "response": "Declare a queue.  Has no effect if a queue with the given\n        name already exists.\n\n        Parameters:\n          queue_name(str): The name of the new queue.\n\n        Raises:\n          ConnectionClosed: If the underlying channel or connection\n            has been closed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_queue_message_counts(self, queue_name):\n        queue_response = self._declare_queue(queue_name)\n        dq_queue_response = self._declare_dq_queue(queue_name)\n        xq_queue_response = self._declare_xq_queue(queue_name)\n        return (\n            queue_response.method.message_count,\n            dq_queue_response.method.message_count,\n            xq_queue_response.method.message_count,\n        )", "response": "This method returns the number of messages in a queue. It returns the number of delayed messages in the queue and dead letter queues."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flush(self, queue_name):\n        for name in (queue_name, dq_name(queue_name), xq_name(queue_name)):\n            self.channel.queue_purge(name)", "response": "Drop all the messages from a queue."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef join(self, queue_name, min_successes=10, idle_time=100, *, timeout=None):\n        deadline = timeout and time.monotonic() + timeout / 1000\n        successes = 0\n        while successes < min_successes:\n            if deadline and time.monotonic() >= deadline:\n                raise QueueJoinTimeout(queue_name)\n\n            total_messages = sum(self.get_queue_message_counts(queue_name)[:-1])\n            if total_messages == 0:\n                successes += 1\n            else:\n                successes = 0\n\n            self.connection.sleep(idle_time / 1000)", "response": "Wait for all the messages in the given queue to be processed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the barrier for the given number of parties.", "response": "def create(self, parties):\n        \"\"\"Create the barrier for the given number of parties.\n\n        Parameters:\n          parties(int): The number of parties to wait for.\n\n        Returns:\n          bool: Whether or not the new barrier was successfully created.\n        \"\"\"\n        assert parties > 0, \"parties must be a positive integer.\"\n        return self.backend.add(self.key, parties, self.ttl)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsignaling that a party has reached the barrier.", "response": "def wait(self, *, block=True, timeout=None):\n        \"\"\"Signal that a party has reached the barrier.\n\n        Warning:\n          Barrier blocking is currently only supported by the stub and\n          Redis backends.\n\n        Warning:\n          Re-using keys between blocking calls may lead to undefined\n          behaviour.  Make sure your barrier keys are always unique\n          (use a UUID).\n\n        Parameters:\n          block(bool): Whether or not to block while waiting for the\n            other parties.\n          timeout(int): The maximum number of milliseconds to wait for\n            the barrier to be cleared.\n\n        Returns:\n          bool: Whether or not the barrier has been reached by all parties.\n        \"\"\"\n        cleared = not self.backend.decr(self.key, 1, 1, self.ttl)\n        if cleared:\n            self.backend.wait_notify(self.key_events, self.ttl)\n            return True\n\n        if block:\n            return self.backend.wait(self.key_events, timeout)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nraises an exception in a thread.", "response": "def raise_thread_exception(thread_id, exception):\n    \"\"\"Raise an exception in a thread.\n\n    Currently, this is only available on CPython.\n\n    Note:\n      This works by setting an async exception in the thread.  This means\n      that the exception will only get called the next time that thread\n      acquires the GIL.  Concretely, this means that this middleware can't\n      cancel system calls.\n    \"\"\"\n    if current_platform == \"CPython\":\n        _raise_thread_exception_cpython(thread_id, exception)\n    else:\n        message = \"Setting thread exceptions (%s) is not supported for your current platform (%r).\"\n        exctype = (exception if inspect.isclass(exception) else type(exception)).__name__\n        logger.critical(message, exctype, current_platform)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting up a background thread that watches for source changes and sends SIGHUP to the current process whenever a file changes.", "response": "def setup_file_watcher(path, use_polling=False):\n    \"\"\"Sets up a background thread that watches for source changes and\n    automatically sends SIGHUP to the current process whenever a file\n    changes.\n    \"\"\"\n    if use_polling:\n        observer_class = watchdog.observers.polling.PollingObserver\n    else:\n        observer_class = EVENTED_OBSERVER\n\n    file_event_handler = _SourceChangesHandler(patterns=[\"*.py\"])\n    file_watcher = observer_class()\n    file_watcher.schedule(file_event_handler, path, recursive=True)\n    file_watcher.start()\n    return file_watcher"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef consume(self, queue_name, prefetch=1, timeout=100):\n        try:\n            return _StubConsumer(\n                self.queues[queue_name],\n                self.dead_letters_by_queue[queue_name],\n                timeout,\n            )\n        except KeyError:\n            raise QueueNotFound(queue_name)", "response": "Create a new consumer for a queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeclares a queue. Has no effect if a queue with the given name has already been declared.", "response": "def declare_queue(self, queue_name):\n        \"\"\"Declare a queue.  Has no effect if a queue with the given\n        name has already been declared.\n\n        Parameters:\n          queue_name(str): The name of the new queue.\n        \"\"\"\n        if queue_name not in self.queues:\n            self.emit_before(\"declare_queue\", queue_name)\n            self.queues[queue_name] = Queue()\n            self.emit_after(\"declare_queue\", queue_name)\n\n            delayed_name = dq_name(queue_name)\n            self.queues[delayed_name] = Queue()\n            self.delay_queues.add(delayed_name)\n            self.emit_after(\"declare_delay_queue\", delayed_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enqueue(self, message, *, delay=None):\n        queue_name = message.queue_name\n        if delay is not None:\n            queue_name = dq_name(queue_name)\n            message_eta = current_millis() + delay\n            message = message.copy(\n                queue_name=queue_name,\n                options={\n                    \"eta\": message_eta,\n                },\n            )\n\n        if queue_name not in self.queues:\n            raise QueueNotFound(queue_name)\n\n        self.emit_before(\"enqueue\", message, delay)\n        self.queues[queue_name].put(message.encode())\n        self.emit_after(\"enqueue\", message, delay)\n        return message", "response": "Enqueue a message on the specified queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndrops all the messages from a queue.", "response": "def flush(self, queue_name):\n        \"\"\"Drop all the messages from a queue.\n\n        Parameters:\n          queue_name(str): The queue to flush.\n        \"\"\"\n        for _ in iter_queue(self.queues[queue_name]):\n            self.queues[queue_name].task_done()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flush_all(self):\n        for queue_name in chain(self.queues, self.delay_queues):\n            self.flush(queue_name)", "response": "Drop all messages from all declared queues."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef join(self, queue_name, *, fail_fast=False, timeout=None):\n        try:\n            queues = [\n                self.queues[queue_name],\n                self.queues[dq_name(queue_name)],\n            ]\n        except KeyError:\n            raise QueueNotFound(queue_name)\n\n        deadline = timeout and time.monotonic() + timeout / 1000\n        while True:\n            for queue in queues:\n                timeout = deadline and deadline - time.monotonic()\n                join_queue(queue, timeout=timeout)\n\n            # We cycle through $queue then $queue.DQ then $queue\n            # again in case the messages that were on the DQ got\n            # moved back on $queue.\n            for queue in queues:\n                if queue.unfinished_tasks:\n                    break\n            else:\n                if fail_fast:\n                    for message in self.dead_letters_by_queue[queue_name]:\n                        raise message._exception from None\n\n                return", "response": "This method is used to wait for all the messages on a given queue to be processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_result(self, message, *, block=False, timeout=None):\n        if timeout is None:\n            timeout = DEFAULT_TIMEOUT\n\n        message_key = self.build_message_key(message)\n        if block:\n            timeout = int(timeout / 1000)\n            if timeout == 0:\n                data = self.client.rpoplpush(message_key, message_key)\n            else:\n                data = self.client.brpoplpush(message_key, message_key, timeout)\n\n            if data is None:\n                raise ResultTimeout(message)\n\n        else:\n            data = self.client.lindex(message_key, 0)\n            if data is None:\n                raise ResultMissing(message)\n\n        return self.encoder.decode(data)", "response": "Get a result from the backend."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef completed_count(self):\n        for count, message in enumerate(self.messages, start=1):\n            try:\n                message.get_result()\n            except ResultMissing:\n                return count - 1\n\n        return count", "response": "Returns the total number of jobs that have been completed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun this pipeline. Parameters: delay(int): The minimum amount of time, in milliseconds, the pipeline should be delayed by. Returns: pipeline: Itself.", "response": "def run(self, *, delay=None):\n        \"\"\"Run this pipeline.\n\n        Parameters:\n          delay(int): The minimum amount of time, in milliseconds, the\n            pipeline should be delayed by.\n\n        Returns:\n          pipeline: Itself.\n        \"\"\"\n        self.broker.enqueue(self.messages[0], delay=delay)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the result of this pipeline.", "response": "def get_result(self, *, block=False, timeout=None):\n        \"\"\"Get the result of this pipeline.\n\n        Pipeline results are represented by the result of the last\n        message in the chain.\n\n        Parameters:\n          block(bool): Whether or not to block until a result is set.\n          timeout(int): The maximum amount of time, in ms, to wait for\n            a result when block is True.  Defaults to 10 seconds.\n\n        Raises:\n          ResultMissing: When block is False and the result isn't set.\n          ResultTimeout: When waiting for a result times out.\n\n        Returns:\n          object: The result.\n        \"\"\"\n        return self.messages[-1].get_result(block=block, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_results(self, *, block=False, timeout=None):\n        deadline = None\n        if timeout:\n            deadline = time.monotonic() + timeout / 1000\n\n        for message in self.messages:\n            if deadline:\n                timeout = max(0, int((deadline - time.monotonic()) * 1000))\n\n            yield message.get_result(block=block, timeout=timeout)", "response": "Get the results of each job in the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef completed_count(self):\n        for count, child in enumerate(self.children, start=1):\n            try:\n                if isinstance(child, group):\n                    child.get_results()\n                else:\n                    child.get_result()\n            except ResultMissing:\n                return count - 1\n\n        return count", "response": "Returns the total number of jobs that have been completed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, *, delay=None):\n        for child in self.children:\n            if isinstance(child, (group, pipeline)):\n                child.run(delay=delay)\n            else:\n                self.broker.enqueue(child, delay=delay)\n\n        return self", "response": "Runs the actors in this group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the results of each job in the group.", "response": "def get_results(self, *, block=False, timeout=None):\n        \"\"\"Get the results of each job in the group.\n\n        Parameters:\n          block(bool): Whether or not to block until the results are stored.\n          timeout(int): The maximum amount of time, in milliseconds,\n            to wait for results when block is True.  Defaults to 10\n            seconds.\n\n        Raises:\n          ResultMissing: When block is False and the results aren't set.\n          ResultTimeout: When waiting for results times out.\n\n        Returns:\n          A result generator.\n        \"\"\"\n        deadline = None\n        if timeout:\n            deadline = time.monotonic() + timeout / 1000\n\n        for child in self.children:\n            if deadline:\n                timeout = max(0, int((deadline - time.monotonic()) * 1000))\n\n            if isinstance(child, group):\n                yield list(child.get_results(block=block, timeout=timeout))\n            else:\n                yield child.get_result(block=block, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nblocks until all the jobs in the group have finished or until the timeout expires.", "response": "def wait(self, *, timeout=None):\n        \"\"\"Block until all the jobs in the group have finished or\n        until the timeout expires.\n\n        Parameters:\n          timeout(int): The maximum amount of time, in ms, to wait.\n            Defaults to 10 seconds.\n        \"\"\"\n        for _ in self.get_results(block=True, timeout=timeout):  # pragma: no cover\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef actor(fn=None, *, actor_class=Actor, actor_name=None, queue_name=\"default\", priority=0, broker=None, **options):\n    def decorator(fn):\n        nonlocal actor_name, broker\n        actor_name = actor_name or fn.__name__\n        if not _queue_name_re.fullmatch(queue_name):\n            raise ValueError(\n                \"Queue names must start with a letter or an underscore followed \"\n                \"by any number of letters, digits, dashes or underscores.\"\n            )\n\n        broker = broker or get_broker()\n        invalid_options = set(options) - broker.actor_options\n        if invalid_options:\n            invalid_options_list = \", \".join(invalid_options)\n            raise ValueError((\n                \"The following actor options are undefined: %s. \"\n                \"Did you forget to add a middleware to your Broker?\"\n            ) % invalid_options_list)\n\n        return actor_class(\n            fn, actor_name=actor_name, queue_name=queue_name,\n            priority=priority, broker=broker, options=options,\n        )\n\n    if fn is None:\n        return decorator\n    return decorator(fn)", "response": "Declare an actor.\n\n    Examples:\n\n      >>> import dramatiq\n\n      >>> @dramatiq.actor\n      ... def add(x, y):\n      ...     print(x + y)\n      ...\n      >>> add\n      Actor(<function add at 0x106c6d488>, queue_name='default', actor_name='add')\n\n      >>> add(1, 2)\n      3\n\n      >>> add.send(1, 2)\n      Message(\n        queue_name='default',\n        actor_name='add',\n        args=(1, 2), kwargs={}, options={},\n        message_id='e0d27b45-7900-41da-bb97-553b8a081206',\n        message_timestamp=1497862448685)\n\n    Parameters:\n      fn(callable): The function to wrap.\n      actor_class(type): Type created by the decorator.  Defaults to\n        :class:`Actor` but can be any callable as long as it returns an\n        actor and takes the same arguments as the :class:`Actor` class.\n      actor_name(str): The name of the actor.\n      queue_name(str): The name of the queue to use.\n      priority(int): The actor's global priority.  If two tasks have\n        been pulled on a worker concurrently and one has a higher\n        priority than the other then it will be processed first.\n        Lower numbers represent higher priorities.\n      broker(Broker): The broker to use with this actor.\n      **options(dict): Arbitrary options that vary with the set of\n        middleware that you use.  See ``get_broker().actor_options``.\n\n    Returns:\n      Actor: The decorated function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef message(self, *args, **kwargs):\n        return self.message_with_options(args=args, kwargs=kwargs)", "response": "Build a message that can be enqueued on a broker."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef message_with_options(self, *, args=None, kwargs=None, **options):\n        for name in [\"on_failure\", \"on_success\"]:\n            callback = options.get(name)\n            if isinstance(callback, Actor):\n                options[name] = callback.actor_name\n\n            elif not isinstance(callback, (type(None), str)):\n                raise TypeError(name + \" value must be an Actor\")\n\n        return Message(\n            queue_name=self.queue_name,\n            actor_name=self.actor_name,\n            args=args or (), kwargs=kwargs or {},\n            options=options,\n        )", "response": "Build a message with arbitray set of processing options."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self, *args, **kwargs):\n        return self.send_with_options(args=args, kwargs=kwargs)", "response": "Asynchronously send a message to this actor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_with_options(self, *, args=None, kwargs=None, delay=None, **options):\n        message = self.message_with_options(args=args, kwargs=kwargs, **options)\n        return self.broker.enqueue(message, delay=delay)", "response": "Asynchronously sends a message to this actor along with an arbitrary set of processing options for the broker and the middleware."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start(self):\n        self.broker.emit_before(\"worker_boot\", self)\n\n        worker_middleware = _WorkerMiddleware(self)\n        self.broker.add_middleware(worker_middleware)\n        for _ in range(self.worker_threads):\n            self._add_worker()\n\n        self.broker.emit_after(\"worker_boot\", self)", "response": "Initialize the worker boot sequence and start up all the the\n        worker threads."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npause all the worker threads.", "response": "def pause(self):\n        \"\"\"Pauses all the worker threads.\n        \"\"\"\n        for child in chain(self.consumers.values(), self.workers):\n            child.pause()\n\n        for child in chain(self.consumers.values(), self.workers):\n            child.paused_event.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resume(self):\n        for child in chain(self.consumers.values(), self.workers):\n            child.resume()", "response": "Resumes all the worker threads."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef join(self):\n        while True:\n            for consumer in self.consumers.values():\n                consumer.delay_queue.join()\n\n            self.work_queue.join()\n\n            # If nothing got put on the delay queues while we were\n            # joining on the work queue then it shoud be safe to exit.\n            # This could still miss stuff but the chances are slim.\n            for consumer in self.consumers.values():\n                if consumer.delay_queue.unfinished_tasks:\n                    break\n            else:\n                if self.work_queue.unfinished_tasks:\n                    continue\n                return", "response": "Wait for this worker to complete its work in progress."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_message(self, message):\n        try:\n            if \"eta\" in message.options:\n                self.logger.debug(\"Pushing message %r onto delay queue.\", message.message_id)\n                self.broker.emit_before(\"delay_message\", message)\n                self.delay_queue.put((message.options.get(\"eta\", 0), message))\n\n            else:\n                actor = self.broker.get_actor(message.actor_name)\n                self.logger.debug(\"Pushing message %r onto work queue.\", message.message_id)\n                self.work_queue.put((actor.priority, message))\n        except ActorNotFound:\n            self.logger.error(\n                \"Received message for undefined actor %r. Moving it to the DLQ.\",\n                message.actor_name, exc_info=True,\n            )\n            message.fail()\n            self.post_process_message(message)", "response": "Handle a message received off of the underlying consumer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling by worker threads when a message is received from the broker.", "response": "def post_process_message(self, message):\n        \"\"\"Called by worker threads whenever they're done processing\n        individual messages, signaling that each message is ready to\n        be acked or rejected.\n        \"\"\"\n        while True:\n            try:\n                if message.failed:\n                    self.logger.debug(\"Rejecting message %r.\", message.message_id)\n                    self.broker.emit_before(\"nack\", message)\n                    self.consumer.nack(message)\n                    self.broker.emit_after(\"nack\", message)\n\n                else:\n                    self.logger.debug(\"Acknowledging message %r.\", message.message_id)\n                    self.broker.emit_before(\"ack\", message)\n                    self.consumer.ack(message)\n                    self.broker.emit_after(\"ack\", message)\n\n                return\n\n            # This applies to the Redis broker.  The alternative to\n            # constantly retrying would be to give up here and let the\n            # message be re-processed after the worker is eventually\n            # stopped or restarted, but we'd be doing the same work\n            # twice in that case and the behaviour would surprise\n            # users who don't deploy frequently.\n            except ConnectionError as e:\n                self.logger.warning(\n                    \"Failed to post_process_message(%s) due to a connection error: %s\\n\"\n                    \"The operation will be retried in %s seconds until the connection recovers.\\n\"\n                    \"If you restart this worker before this operation succeeds, the message will be re-processed later.\",\n                    message, e, POST_PROCESS_MESSAGE_RETRY_DELAY_SECS\n                )\n\n                time.sleep(POST_PROCESS_MESSAGE_RETRY_DELAY_SECS)\n                continue\n\n            # Not much point retrying here so we bail.  Most likely,\n            # the message will be re-run after the worker is stopped\n            # or restarted (because its ack lease will have expired).\n            except Exception:  # pragma: no cover\n                self.logger.exception(\n                    \"Unhandled error during post_process_message(%s).  You've found a bug in Dramatiq.  Please report it!\\n\"\n                    \"Although your message has been processed, it will be processed again once this worker is restarted.\",\n                    message,\n                )\n\n                return"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose this consumer thread and its underlying connection.", "response": "def close(self):\n        \"\"\"Close this consumer thread and its underlying connection.\n        \"\"\"\n        try:\n            if self.consumer:\n                self.requeue_messages(m for _, m in iter_queue(self.delay_queue))\n                self.consumer.close()\n        except ConnectionError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_message(self, message):\n        try:\n            self.logger.debug(\"Received message %s with id %r.\", message, message.message_id)\n            self.broker.emit_before(\"process_message\", message)\n\n            res = None\n            if not message.failed:\n                actor = self.broker.get_actor(message.actor_name)\n                res = actor(*message.args, **message.kwargs)\n\n            self.broker.emit_after(\"process_message\", message, result=res)\n\n        except SkipMessage:\n            self.logger.warning(\"Message %s was skipped.\", message)\n            self.broker.emit_after(\"skip_message\", message)\n\n        except BaseException as e:\n            # Stuff the exception into the message [proxy] so that it\n            # may be used by the stub broker to provide a nicer\n            # testing experience.\n            message.stuff_exception(e)\n\n            if isinstance(e, RateLimitExceeded):\n                self.logger.warning(\"Rate limit exceeded in message %s: %s.\", message, e)\n            else:\n                self.logger.warning(\"Failed to process message %s with unhandled exception.\", message, exc_info=True)\n\n            self.broker.emit_after(\"process_message\", message, exception=e)\n\n        finally:\n            # NOTE: There is no race here as any message that was\n            # processed must have come off of a consumer.  Therefore,\n            # there has to be a consumer for that message's queue so\n            # this is safe.  Probably.\n            self.consumers[message.queue_name].post_process_message(message)\n            self.work_queue.task_done()", "response": "Process a message pulled off of the work queue then push it back to the associated consumer for post processing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare the data between two sources.", "response": "def compare(self, a, b, zone):\n        '''\n        Compare zone data between 2 sources.\n\n        Note: only things supported by both sources will be considered\n        '''\n        self.log.info('compare: a=%s, b=%s, zone=%s', a, b, zone)\n\n        try:\n            a = [self.providers[source] for source in a]\n            b = [self.providers[source] for source in b]\n        except KeyError as e:\n            raise Exception('Unknown source: {}'.format(e.args[0]))\n\n        sub_zones = self.configured_sub_zones(zone)\n        za = Zone(zone, sub_zones)\n        for source in a:\n            source.populate(za)\n\n        zb = Zone(zone, sub_zones)\n        for source in b:\n            source.populate(zb)\n\n        return zb.changes(za, _AggregateTarget(a + b))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndumps the zone data from the specified source to the specified output_dir.", "response": "def dump(self, zone, output_dir, lenient, split, source, *sources):\n        '''\n        Dump zone data from the specified source\n        '''\n        self.log.info('dump: zone=%s, sources=%s', zone, sources)\n\n        # We broke out source to force at least one to be passed, add it to any\n        # others we got.\n        sources = [source] + list(sources)\n\n        try:\n            sources = [self.providers[s] for s in sources]\n        except KeyError as e:\n            raise Exception('Unknown source: {}'.format(e.args[0]))\n\n        clz = YamlProvider\n        if split:\n            clz = SplitYamlProvider\n        target = clz('dump', output_dir)\n\n        zone = Zone(zone, self.configured_sub_zones(zone))\n        for source in sources:\n            source.populate(zone, lenient=lenient)\n\n        plan = target.plan(zone)\n        if plan is None:\n            plan = Plan(zone, zone, [], False)\n        target.apply(plan)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nflush the zone cache if there is one", "response": "def flush_zone(cls, zone_name):\n        '''Flushes the zone cache, if there is one'''\n        cls.log.debug('flush_zone: zone_name=%s', zone_name)\n        try:\n            del cls._cache[zone_name]\n        except KeyError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _equals(self, b):\n        '''Checks whether two records are equal by comparing all fields.\n            :param b: Another _AzureRecord object\n            :type  b: _AzureRecord\n\n            :type return: bool\n        '''\n        def parse_dict(params):\n            vals = []\n            for char in params:\n                if char != 'ttl':\n                    list_records = params[char]\n                    try:\n                        for record in list_records:\n                            vals.append(record.__dict__)\n                    except:\n                        vals.append(list_records.__dict__)\n            vals.sort()\n            return vals\n\n        return (self.resource_group == b.resource_group) & \\\n               (self.zone_name == b.zone_name) & \\\n               (self.record_type == b.record_type) & \\\n               (self.params['ttl'] == b.params['ttl']) & \\\n               (parse_dict(self.params) == parse_dict(b.params)) & \\\n               (self.relative_record_set_name == b.relative_record_set_name)", "response": "Checks whether two records are equal by comparing all fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_zone(self, name, create=False):\n        '''Checks whether a zone specified in a source exist in Azure server.\n\n            Note that Azure zones omit end '.' eg: contoso.com vs contoso.com.\n            Returns the name if it exists.\n\n            :param name: Name of a zone to checks\n            :type  name: str\n            :param create: If True, creates the zone of that name.\n            :type  create: bool\n\n            :type return: str or None\n        '''\n        self.log.debug('_check_zone: name=%s', name)\n        try:\n            if name in self._azure_zones:\n                return name\n            self._dns_client.zones.get(self._resource_group, name)\n            self._azure_zones.add(name)\n            return name\n        except CloudError as err:\n            msg = 'The Resource \\'Microsoft.Network/dnszones/{}\\''.format(name)\n            msg += ' under resource group \\'{}\\''.format(self._resource_group)\n            msg += ' was not found.'\n            if msg == err.message:\n                # Then the only error is that the zone doesn't currently exist\n                if create:\n                    self.log.debug('_check_zone:no matching zone; creating %s',\n                                   name)\n                    create_zone = self._dns_client.zones.create_or_update\n                    create_zone(self._resource_group, name,\n                                Zone(location='global'))\n                    return name\n                else:\n                    return\n            raise", "response": "Checks whether a zone specified in a source exist in Azure server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopulating the object with the records from the given zone.", "response": "def populate(self, zone, target=False, lenient=False):\n        '''Required function of manager.py to collect records from zone.\n\n            Special notes for Azure.\n            Azure zone names omit final '.'\n            Azure root records names are represented by '@'. OctoDNS uses ''\n            Azure records created through online interface may have null values\n            (eg, no IP address for A record).\n            Azure online interface allows constructing records with null values\n            which are destroyed by _apply.\n\n            Specific quirks such as these are responsible for any non-obvious\n            parsing in this function and the functions '_params_for_*'.\n\n            :param zone: A dns zone\n            :type  zone: octodns.zone.Zone\n            :param target: Checks if Azure is source or target of config.\n                           Currently only supports as a target. Unused.\n            :type  target: bool\n            :param lenient: Unused. Check octodns.manager for usage.\n            :type  lenient: bool\n\n            :type return: void\n        '''\n        self.log.debug('populate: name=%s', zone.name)\n\n        exists = False\n        before = len(zone.records)\n\n        zone_name = zone.name[:len(zone.name) - 1]\n        self._populate_zones()\n        self._check_zone(zone_name)\n\n        _records = set()\n        records = self._dns_client.record_sets.list_by_dns_zone\n        if self._check_zone(zone_name):\n            exists = True\n            for azrecord in records(self._resource_group, zone_name):\n                if _parse_azure_type(azrecord.type) in self.SUPPORTS:\n                    _records.add(azrecord)\n            for azrecord in _records:\n                record_name = azrecord.name if azrecord.name != '@' else ''\n                typ = _parse_azure_type(azrecord.type)\n                data = getattr(self, '_data_for_{}'.format(typ))\n                data = data(azrecord)\n                data['type'] = typ\n                data['ttl'] = azrecord.ttl\n                record = Record.new(zone, record_name, data, source=self)\n                zone.add_record(record, lenient=lenient)\n\n        self.log.info('populate: found %s records, exists=%s',\n                      len(zone.records) - before, exists)\n        return exists"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies a record change.", "response": "def _apply(self, plan):\n        '''Required function of manager.py to actually apply a record change.\n\n            :param plan: Contains the zones and changes to be made\n            :type  plan: octodns.provider.base.Plan\n\n            :type return: void\n        '''\n        desired = plan.desired\n        changes = plan.changes\n        self.log.debug('_apply: zone=%s, len(changes)=%d', desired.name,\n                       len(changes))\n\n        azure_zone_name = desired.name[:len(desired.name) - 1]\n        self._check_zone(azure_zone_name, create=True)\n\n        for change in changes:\n            class_name = change.__class__.__name__\n            getattr(self, '_apply_{}'.format(class_name))(change)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying the changes to the provider. Returns the number of changes made.", "response": "def apply(self, plan):\n        '''\n        Submits actual planned changes to the provider. Returns the number of\n        changes made\n        '''\n        if self.apply_disabled:\n            self.log.info('apply: disabled')\n            return 0\n\n        self.log.info('apply: making changes')\n        self._apply(plan)\n        return len(plan.changes)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the mod list for the given action.", "response": "def _gen_mods(self, action, records, existing_rrsets):\n        '''\n        Turns `_Route53*`s in to `change_resource_record_sets` `Changes`\n        '''\n        return [r.mod(action, existing_rrsets) for r in records]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _gen_records(self, record, zone_id, creating=False):\n        '''\n        Turns an octodns.Record into one or more `_Route53*`s\n        '''\n        return _Route53Record.new(self, record, zone_id, creating)", "response": "Turns an octodns. Record into one or more _Route53Records"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if value is a valid DKIM", "response": "def _is_valid_dkim(self, value):\n        \"\"\"Check if value is a valid DKIM\"\"\"\n        validator_dict = {'h': lambda val: val in ['sha1', 'sha256'],\n                          's': lambda val: val in ['*', 'email'],\n                          't': lambda val: val in ['y', 's'],\n                          'v': lambda val: val == 'DKIM1',\n                          'k': lambda val: val == 'rsa',\n                          'n': lambda _: True,\n                          'g': lambda _: True}\n\n        splitted = value.split('\\\\;')\n        found_key = False\n        for splitted_value in splitted:\n            sub_split = map(lambda x: x.strip(), splitted_value.split(\"=\", 1))\n            if len(sub_split) < 2:\n                return False\n            key, value = sub_split[0], sub_split[1]\n            if key == \"p\":\n                is_valid_key = self._is_valid_dkim_key(value)\n                if not is_valid_key:\n                    return False\n                found_key = True\n            else:\n                is_valid_key = validator_dict.get(key, lambda _: False)(value)\n                if not is_valid_key:\n                    return False\n        return found_key"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all records of a DNS zone.", "response": "def get_records(self, zone_name):\n        \"\"\"\n        List all records of a DNS zone\n        :param zone_name: Name of zone\n        :return: list of id's records\n        \"\"\"\n        records = self._client.get('/domain/zone/{}/record'.format(zone_name))\n        return [self.get_record(zone_name, record_id) for record_id in records]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_record(self, zone_name, record_id):\n        return self._client.get(\n            '/domain/zone/{}/record/{}'.format(zone_name, record_id))", "response": "Get a record with given id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting records from have fieldType = type and subDomain = subdomain", "response": "def delete_records(self, zone_name, record_type, subdomain):\n        \"\"\"\n        Delete record from have fieldType=type and subDomain=subdomain\n        :param zone_name: Name of the zone\n        :param record_type: fieldType\n        :param subdomain: subDomain\n        \"\"\"\n        records = self._client.get('/domain/zone/{}/record'.format(zone_name),\n                                   fieldType=record_type, subDomain=subdomain)\n        for record in records:\n            self.delete_record(zone_name, record)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a record with a given id", "response": "def delete_record(self, zone_name, record_id):\n        \"\"\"\n        Delete record with a given id\n        :param zone_name: Name of the zone\n        :param record_id: Id of the record\n        \"\"\"\n        self.log.debug('Delete record: zone: %s, id %s', zone_name,\n                       record_id)\n        self._client.delete(\n            '/domain/zone/{}/record/{}'.format(zone_name, record_id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a record in a zone", "response": "def create_record(self, zone_name, params):\n        \"\"\"\n        Create a record\n        :param zone_name: Name of the zone\n        :param params: {'fieldType': 'A', 'ttl': 60, 'subDomain': 'www',\n        'target': '1.2.3.4'\n        \"\"\"\n        self.log.debug('Create record: zone: %s, id %s', zone_name,\n                       params)\n        return self._client.post('/domain/zone/{}/record'.format(zone_name),\n                                 **params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying a record change to the current record.", "response": "def _apply(self, plan):\n        \"\"\"Required function of manager.py to actually apply a record change.\n\n            :param plan: Contains the zones and changes to be made\n            :type  plan: octodns.provider.base.Plan\n\n            :type return: void\n        \"\"\"\n        desired = plan.desired\n        changes = plan.changes\n\n        self.log.debug('_apply: zone=%s, len(changes)=%d', desired.name,\n                       len(changes))\n\n        # Get gcloud zone, or create one if none existed before.\n        if desired.name not in self.gcloud_zones:\n            gcloud_zone = self._create_gcloud_zone(desired.name)\n        else:\n            gcloud_zone = self.gcloud_zones.get(desired.name)\n\n        gcloud_changes = gcloud_zone.changes()\n\n        for change in changes:\n            class_name = change.__class__.__name__\n            _rrset_func = getattr(\n                self, '_rrset_for_{}'.format(change.record._type))\n\n            if class_name == 'Create':\n                gcloud_changes.add_record_set(\n                    _rrset_func(gcloud_zone, change.record))\n            elif class_name == 'Delete':\n                gcloud_changes.delete_record_set(\n                    _rrset_func(gcloud_zone, change.record))\n            elif class_name == 'Update':\n                gcloud_changes.delete_record_set(\n                    _rrset_func(gcloud_zone, change.existing))\n                gcloud_changes.add_record_set(\n                    _rrset_func(gcloud_zone, change.new))\n            else:\n                raise RuntimeError('Change type \"{}\" for change \"{!s}\" '\n                                   'is none of \"Create\", \"Delete\" or \"Update'\n                                   .format(class_name, change))\n\n        gcloud_changes.create()\n\n        for i in range(120):\n            gcloud_changes.reload()\n            # https://cloud.google.com/dns/api/v1/changes#resource\n            # status can be one of either \"pending\" or \"done\"\n            if gcloud_changes.status != 'pending':\n                break\n            self.log.debug(\"Waiting for changes to complete\")\n            time.sleep(self.CHANGE_LOOP_WAIT)\n\n        if gcloud_changes.status != 'done':\n            raise RuntimeError(\"Timeout reached after {} seconds\".format(\n                i * self.CHANGE_LOOP_WAIT))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a google cloud ManagedZone with dns_name and zone named derived from it.", "response": "def _create_gcloud_zone(self, dns_name):\n        \"\"\"Creates a google cloud ManagedZone with dns_name, and zone named\n            derived from it. calls .create() method and returns it.\n\n            :param dns_name: fqdn of zone to create\n            :type  dns_name: str\n\n            :type return: new google.cloud.dns.ManagedZone\n        \"\"\"\n        # Zone name must begin with a letter, end with a letter or digit,\n        # and only contain lowercase letters, digits or dashes,\n        # and be 63 characters or less\n        zone_name = 'zone-{}-{}'.format(\n            dns_name.replace('.', '-'), uuid4().hex)[:63]\n\n        gcloud_zone = self.gcloud_client.zone(\n            name=zone_name,\n            dns_name=dns_name\n        )\n        gcloud_zone.create(client=self.gcloud_client)\n\n        # add this new zone to the list of zones.\n        self._gcloud_zones[gcloud_zone.dns_name] = gcloud_zone\n\n        self.log.info(\"Created zone {}. Fqdn {}.\".format(zone_name, dns_name))\n\n        return gcloud_zone"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload all ManagedZones into self. _gcloud_zones dict which is mapped with the dns_name as key.", "response": "def _get_cloud_zones(self, page_token=None):\n        \"\"\"Load all ManagedZones into the self._gcloud_zones dict which is\n        mapped with the dns_name as key.\n\n        :return: void\n        \"\"\"\n\n        gcloud_zones = self.gcloud_client.list_zones(page_token=page_token)\n        for gcloud_zone in gcloud_zones:\n            self._gcloud_zones[gcloud_zone.dns_name] = gcloud_zone\n\n        if gcloud_zones.next_page_token:\n            self._get_cloud_zones(gcloud_zones.next_page_token)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npopulate the record list for the given zone.", "response": "def populate(self, zone, target=False, lenient=False):\n        \"\"\"Required function of manager.py to collect records from zone.\n\n            :param zone: A dns zone\n            :type  zone: octodns.zone.Zone\n            :param target: Unused.\n            :type  target: bool\n            :param lenient: Unused. Check octodns.manager for usage.\n            :type  lenient: bool\n\n            :type return: void\n        \"\"\"\n\n        self.log.debug('populate: name=%s, target=%s, lenient=%s', zone.name,\n                       target, lenient)\n\n        exists = False\n        before = len(zone.records)\n\n        gcloud_zone = self.gcloud_zones.get(zone.name)\n\n        if gcloud_zone:\n            exists = True\n            for gcloud_record in self._get_gcloud_records(gcloud_zone):\n                if gcloud_record.record_type.upper() not in self.SUPPORTS:\n                    continue\n\n                record_name = gcloud_record.name\n                if record_name.endswith(zone.name):\n                    # google cloud always return fqdn. Make relative record\n                    # here. \"root\" records will then get the '' record_name,\n                    # which is also the way octodns likes it.\n                    record_name = record_name[:-(len(zone.name) + 1)]\n                typ = gcloud_record.record_type.upper()\n                data = getattr(self, '_data_for_{}'.format(typ))\n                data = data(gcloud_record)\n                data['type'] = typ\n                data['ttl'] = gcloud_record.ttl\n                self.log.debug('populate: adding record {} records: {!s}'\n                               .format(record_name, data))\n                record = Record.new(zone, record_name, data, source=self)\n                zone.add_record(record, lenient=lenient)\n\n        self.log.info('populate: found %s records, exists=%s',\n                      len(zone.records) - before, exists)\n        return exists"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate(cls, code, prefix):\n        '''\n        Validates an octoDNS geo code making sure that it is a valid and\n        corresponding:\n            * continent\n            * continent & country\n            * continent, country, & province\n        '''\n        reasons = []\n\n        pieces = code.split('-')\n        n = len(pieces)\n        if n > 3:\n            reasons.append('{}invalid geo code \"{}\"'.format(prefix, code))\n        elif n > 0 and pieces[0] not in geo_data:\n            reasons.append('{}unknown continent code \"{}\"'\n                           .format(prefix, code))\n        elif n > 1 and pieces[1] not in geo_data[pieces[0]]:\n            reasons.append('{}unknown country code \"{}\"'.format(prefix, code))\n        elif n > 2 and \\\n                pieces[2] not in geo_data[pieces[0]][pieces[1]]['provinces']:\n            reasons.append('{}unknown province code \"{}\"'.format(prefix, code))\n\n        return reasons", "response": "Validates an octoDNS geo code making sure that it is a valid and\n            corresponding."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_handshake_with_robot(self):  # type: ignore\n    try:\n        # Start the handshake using nassl - will throw WantReadError right away\n        self._ssl.do_handshake()\n    except WantReadError:\n        # Send the Client Hello\n        len_to_read = self._network_bio.pending()\n        while len_to_read:\n            # Get the data from the SSL engine\n            handshake_data_out = self._network_bio.read(len_to_read)\n            # Send it to the peer\n            self._sock.send(handshake_data_out)\n            len_to_read = self._network_bio.pending()\n\n    # Retrieve the server's response - directly read the underlying network socket\n    # Retrieve data until we get to the ServerHelloDone\n    # The server may send back a ServerHello, an Alert or a CertificateRequest first\n    did_receive_hello_done = False\n    remaining_bytes = b''\n    while not did_receive_hello_done:\n        try:\n            tls_record, len_consumed = TlsRecordParser.parse_bytes(remaining_bytes)\n            remaining_bytes = remaining_bytes[len_consumed::]\n        except NotEnoughData:\n            # Try to get more data\n            raw_ssl_bytes = self._sock.recv(16381)\n            if not raw_ssl_bytes:\n                # No data?\n                break\n\n            remaining_bytes = remaining_bytes + raw_ssl_bytes\n            continue\n\n        if isinstance(tls_record, TlsHandshakeRecord):\n            # Does the record contain a ServerDone message?\n            for handshake_message in tls_record.subprotocol_messages:\n                if handshake_message.handshake_type == TlsHandshakeTypeByte.SERVER_DONE:\n                    did_receive_hello_done = True\n                    break\n            # If not, it could be a ServerHello, Certificate or a CertificateRequest if the server requires client auth\n        elif isinstance(tls_record, TlsAlertRecord):\n            # Server returned a TLS alert\n            break\n        else:\n            raise ValueError('Unknown record? Type {}'.format(tls_record.header.type))\n\n    if did_receive_hello_done:\n        # Send a special Client Key Exchange Record as the payload\n        self._sock.send(self._robot_cke_record.to_bytes())\n\n        if self._robot_should_finish_handshake:\n            # Then send a CCS record\n            ccs_record = TlsChangeCipherSpecRecord.from_parameters(\n                tls_version=TlsVersionEnum[self._ssl_version.name]\n            )\n            self._sock.send(ccs_record.to_bytes())\n\n            # Lastly send a Finished record\n            finished_record_bytes = RobotTlsRecordPayloads.get_finished_record_bytes(self._ssl_version)\n            self._sock.send(finished_record_bytes)\n\n        # Return whatever the server sent back by raising an exception\n        # The goal is to detect similar/different responses\n        while True:\n            try:\n                tls_record, len_consumed = TlsRecordParser.parse_bytes(remaining_bytes)\n                remaining_bytes = remaining_bytes[len_consumed::]\n            except NotEnoughData:\n                # Try to get more data\n                try:\n                    raw_ssl_bytes = self._sock.recv(16381)\n                    if not raw_ssl_bytes:\n                        # No data?\n                        raise ServerResponseToRobot('No data')\n                except socket.error as e:\n                    # Server closed the connection after receiving the CCS payload\n                    raise ServerResponseToRobot('socket.error {}'.format(str(e)))\n\n                remaining_bytes = remaining_bytes + raw_ssl_bytes\n                continue\n\n            if isinstance(tls_record, TlsAlertRecord):\n                raise ServerResponseToRobot('TLS Alert {} {}'.format(tls_record.alert_description,\n                                                                     tls_record.alert_severity))\n            else:\n                break\n\n        raise ServerResponseToRobot('Ok')", "response": "Send a ROBOT payload and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a client key exchange record.", "response": "def get_client_key_exchange_record(\n            cls,\n            robot_payload_enum: RobotPmsPaddingPayloadEnum,\n            tls_version: TlsVersionEnum,\n            modulus: int,\n            exponent: int\n    ) -> TlsRsaClientKeyExchangeRecord:\n        \"\"\"A client key exchange record with a hardcoded pre_master_secret, and a valid or invalid padding.\n        \"\"\"\n        pms_padding = cls._compute_pms_padding(modulus)\n        tls_version_hex = binascii.b2a_hex(TlsRecordTlsVersionBytes[tls_version.name].value).decode('ascii')\n\n        pms_with_padding_payload = cls._CKE_PAYLOADS_HEX[robot_payload_enum]\n        final_pms = pms_with_padding_payload.format(\n            pms_padding=pms_padding, tls_version=tls_version_hex, pms=cls._PMS_HEX\n        )\n        cke_robot_record = TlsRsaClientKeyExchangeRecord.from_parameters(\n            tls_version, exponent, modulus, int(final_pms, 16)\n        )\n        return cke_robot_record"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_finished_record_bytes(cls, tls_version: TlsVersionEnum) -> bytes:\n        # TODO(AD): The ROBOT poc script uses the same Finished record for all possible client hello (default, GCM,\n        # etc.); as the Finished record contains a hashes of all previous records, it will be wrong and will cause\n        # servers to send a TLS Alert 20\n        # Here just like in the poc script, the Finished message does not match the Client Hello we sent\n        return b'\\x16' + TlsRecordTlsVersionBytes[tls_version.name].value + cls._FINISHED_RECORD", "response": "Returns the Finished record for the given TLS version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the result enum for the current state of the analysis.", "response": "def compute_result_enum(self) -> RobotScanResultEnum:\n        \"\"\"Look at the server's response to each ROBOT payload and return the conclusion of the analysis.\n        \"\"\"\n        # Ensure the results were consistent\n        for payload_enum, server_responses in self._payload_responses.items():\n            # We ran the check twice per payload and the two responses should be the same\n            if server_responses[0] != server_responses[1]:\n                return RobotScanResultEnum.UNKNOWN_INCONSISTENT_RESULTS\n\n        # Check if the server acts as an oracle by checking if the server replied differently to the payloads\n        if len(set([server_responses[0] for server_responses in self._payload_responses.values()])) == 1:\n            # All server responses were identical - no oracle\n            return RobotScanResultEnum.NOT_VULNERABLE_NO_ORACLE\n\n        # All server responses were NOT identical, server is vulnerable\n        # Check to see if it is a weak oracle\n        response_1 = self._payload_responses[RobotPmsPaddingPayloadEnum.WRONG_FIRST_TWO_BYTES][0]\n        response_2 = self._payload_responses[RobotPmsPaddingPayloadEnum.WRONG_POSITION_00][0]\n        response_3 = self._payload_responses[RobotPmsPaddingPayloadEnum.NO_00_IN_THE_MIDDLE][0]\n\n        # From the original script:\n        # If the response to the invalid PKCS#1 request (oracle_bad1) is equal to both\n        # requests starting with 0002, we have a weak oracle. This is because the only\n        # case where we can distinguish valid from invalid requests is when we send\n        # correctly formatted PKCS#1 message with 0x00 on a correct position. This\n        # makes our oracle weak\n        if response_1 == response_2 == response_3:\n            return RobotScanResultEnum.VULNERABLE_WEAK_ORACLE\n        else:\n            return RobotScanResultEnum.VULNERABLE_STRONG_ORACLE"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbeing the supplied server certificate EV?", "response": "def is_extended_validation(self, certificate: Certificate) -> bool:\n        \"\"\"Is the supplied server certificate EV?\n        \"\"\"\n        if not self.ev_oids:\n            raise ValueError('No EV OIDs supplied for {} store - cannot detect EV certificates'.format(self.name))\n\n        try:\n            cert_policies_ext = certificate.extensions.get_extension_for_oid(ExtensionOID.CERTIFICATE_POLICIES)\n        except ExtensionNotFound:\n            return False\n\n        for policy in cert_policies_ext.value:\n            if policy.policy_identifier in self.ev_oids:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_verified_certificate_chain(self, received_certificate_chain: List[Certificate]) -> List[Certificate]:\n        # The certificates must have been sent in the correct order or we give up\n        if not self._is_certificate_chain_order_valid(received_certificate_chain):\n            raise InvalidCertificateChainOrderError()\n\n        # TODO: OpenSSL 1.1.0 has SSL_get0_verified_chain() to do this directly\n        verified_certificate_chain = []\n        anchor_cert = None\n        # Assume that the certificates were sent in the correct order or give up\n        for cert in received_certificate_chain:\n            anchor_cert = self._get_certificate_with_subject(cert.issuer)\n            verified_certificate_chain.append(cert)\n            if anchor_cert:\n                verified_certificate_chain.append(anchor_cert)\n                break\n\n        if anchor_cert is None:\n            # Could not build the verified chain\n            raise AnchorCertificateNotInTrustStoreError()\n\n        return verified_certificate_chain", "response": "Build the verified certificate chain from the list of certificates sent in the trust store."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_scan_command(\n            self,\n            server_info: ServerConnectivityInfo,\n            scan_command: PluginScanCommand\n    ) -> PluginScanResult:\n        \"\"\"Run a single scan command against a server; will block until the scan command has been completed.\n\n        Args:\n            server_info: The server's connectivity information. The test_connectivity_to_server() method must have been\n                called first to ensure that the server is online and accessible.\n            scan_command: The scan command to run against this server.\n\n        Returns:\n            The result of the scan command, which will be an instance of the scan command's\n            corresponding PluginScanResult subclass.\n        \"\"\"\n        plugin_class = self._plugins_repository.get_plugin_class_for_command(scan_command)\n        plugin = plugin_class()\n        return plugin.process_task(server_info, scan_command)", "response": "Runs a single scan command against a server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_default(cls) -> 'TrustStoresRepository':\n        temp_path = mkdtemp()\n        try:\n            # Download the latest trust stores\n            archive_path = join(temp_path, 'trust_stores_as_pem.tar.gz')\n            urlretrieve(cls._UPDATE_URL, archive_path)\n\n            # Extract the archive\n            extract_path = join(temp_path, 'extracted')\n            tarfile.open(archive_path).extractall(extract_path)\n\n            # Copy the files to SSLyze and overwrite the existing stores\n            shutil.rmtree(cls._DEFAULT_TRUST_STORES_PATH)\n            shutil.copytree(extract_path, cls._DEFAULT_TRUST_STORES_PATH)\n        finally:\n            shutil.rmtree(temp_path)\n\n        # Re-generate the default repo - not thread-safe\n        cls._DEFAULT_REPOSITORY = cls(cls._DEFAULT_TRUST_STORES_PATH)\n        return cls._DEFAULT_REPOSITORY", "response": "Update the default trust store store"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_preferred_cipher_suite(\n            cls,\n            server_connectivity_info: ServerConnectivityInfo,\n            ssl_version: OpenSslVersionEnum,\n            accepted_cipher_list: List['AcceptedCipherSuite']\n    ) -> Optional['AcceptedCipherSuite']:\n        \"\"\"Try to detect the server's preferred cipher suite among all cipher suites supported by SSLyze.\n        \"\"\"\n        if len(accepted_cipher_list) < 2:\n            return None\n\n        accepted_cipher_names = [cipher.openssl_name for cipher in accepted_cipher_list]\n        should_use_legacy_openssl = None\n\n        # For TLS 1.2, we need to figure whether the modern or legacy OpenSSL should be used to connect\n        if ssl_version == OpenSslVersionEnum.TLSV1_2:\n            should_use_legacy_openssl = True\n            # If there are more than two modern-supported cipher suites, use the modern OpenSSL\n            for cipher_name in accepted_cipher_names:\n                modern_supported_cipher_count = 0\n                if not WorkaroundForTls12ForCipherSuites.requires_legacy_openssl(cipher_name):\n                    modern_supported_cipher_count += 1\n\n                if modern_supported_cipher_count > 1:\n                    should_use_legacy_openssl = False\n                    break\n\n        first_cipher_str = ', '.join(accepted_cipher_names)\n        # Swap the first two ciphers in the list to see if the server always picks the client's first cipher\n        second_cipher_str = ', '.join([accepted_cipher_names[1], accepted_cipher_names[0]] + accepted_cipher_names[2:])\n\n        try:\n            first_cipher = cls._get_selected_cipher_suite(\n                server_connectivity_info, ssl_version, first_cipher_str, should_use_legacy_openssl\n            )\n            second_cipher = cls._get_selected_cipher_suite(\n                server_connectivity_info, ssl_version, second_cipher_str, should_use_legacy_openssl\n            )\n        except (SslHandshakeRejected, ConnectionError):\n            # Could not complete a handshake\n            return None\n\n        if first_cipher.name == second_cipher.name:\n            # The server has its own preference for picking a cipher suite\n            return first_cipher\n        else:\n            # The server has no preferred cipher suite as it follows the client's preference for picking a cipher suite\n            return None", "response": "Try to detect the preferred cipher suite among all cipher suites supported by SSLyze."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the cipher suite that was selected by the server during the SSL handshake.", "response": "def _get_selected_cipher_suite(\n            server_connectivity: ServerConnectivityInfo,\n            ssl_version: OpenSslVersionEnum,\n            openssl_cipher_str: str,\n            should_use_legacy_openssl: Optional[bool]\n    ) -> 'AcceptedCipherSuite':\n        \"\"\"Given an OpenSSL cipher string (which may specify multiple cipher suites), return the cipher suite that was\n        selected by the server during the SSL handshake.\n        \"\"\"\n        ssl_connection = server_connectivity.get_preconfigured_ssl_connection(\n            override_ssl_version=ssl_version, should_use_legacy_openssl=should_use_legacy_openssl\n        )\n        ssl_connection.ssl_client.set_cipher_list(openssl_cipher_str)\n\n        # Perform the SSL handshake\n        try:\n            ssl_connection.connect()\n            selected_cipher = AcceptedCipherSuite.from_ongoing_ssl_connection(ssl_connection, ssl_version)\n        except ClientCertificateRequested:\n            selected_cipher = AcceptedCipherSuite.from_ongoing_ssl_connection(ssl_connection, ssl_version)\n        finally:\n            ssl_connection.close()\n        return selected_cipher"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the name of the certificate.", "response": "def name(self) -> str:\n        \"\"\"OpenSSL uses a different naming convention than the corresponding RFCs.\n        \"\"\"\n        return OPENSSL_TO_RFC_NAMES_MAPPING[self.ssl_version].get(self.openssl_name, self.openssl_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef queue_scan_command(self, server_info: ServerConnectivityInfo, scan_command: PluginScanCommand) -> None:\n        # Ensure we have the right processes and queues in place for this hostname\n        self._check_and_create_process(server_info.hostname)\n\n        # Add the task to the right queue\n        self._queued_tasks_nb += 1\n        if scan_command.is_aggressive:\n            # Aggressive commands should not be run in parallel against\n            # a given server so we use the priority queues to prevent this\n            self._hostname_queues_dict[server_info.hostname].put((server_info, scan_command))\n        else:\n            # Normal commands get put in the standard/shared queue\n            self._task_queue.put((server_info, scan_command))", "response": "Queues a scan command for the given server."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the results of all the scan commands in the queue.", "response": "def get_results(self) -> Iterable[PluginScanResult]:\n        \"\"\"Return the result of previously queued scan commands; new commands cannot be queued once this is called.\n\n        Returns:\n            The results of all the scan commands previously queued. Each result will be an instance of the scan\n            corresponding command's PluginScanResult subclass. If there was an unexpected error while running the scan\n            command, it will be a 'PluginRaisedExceptionScanResult' instance instead.\n        \"\"\"\n        # Put a 'None' sentinel in the queue to let the each process know when every task has been completed\n        for _ in range(self._get_current_processes_nb()):\n            self._task_queue.put(None)\n\n        for hostname, hostname_queue in self._hostname_queues_dict.items():\n            for i in range(len(self._processes_dict[hostname])):\n                hostname_queue.put(None)\n\n        received_task_results = 0\n        # Go on until all the tasks have been completed and all processes are done\n        expected_task_results = self._queued_tasks_nb + self._get_current_processes_nb()\n        while received_task_results != expected_task_results:\n            result = self._result_queue.get()\n            self._result_queue.task_done()\n            received_task_results += 1\n            if result is None:\n                # Getting None means that one process was done\n                pass\n            else:\n                # Getting an actual result\n                yield result\n\n        # Ensure all the queues and processes are done\n        self._task_queue.join()\n        self._result_queue.join()\n        for hostname_queue in self._hostname_queues_dict.values():\n            hostname_queue.join()\n        for process_list in self._processes_dict.values():\n            for process in process_list:\n                process.join()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self) -> None:\n        from sslyze.concurrent_scanner import PluginRaisedExceptionScanResult\n\n        # Start processing task in the priority queue first\n        current_queue_in = self.priority_queue_in\n        while True:\n\n            task = current_queue_in.get()  # Grab a task from queue_in\n            if task is None:  # All tasks have been completed\n                current_queue_in.task_done()\n\n                if current_queue_in == self.priority_queue_in:\n                    # All high priority tasks have been completed; switch to low priority tasks\n                    current_queue_in = self.queue_in\n                    continue\n                else:\n                    # All the tasks have been completed; pass on the sentinel to result_queue and exit\n                    self.queue_out.put(None)\n                    break\n\n            server_info, scan_command = task\n            try:\n                result = self._synchronous_scanner.run_scan_command(server_info, scan_command)\n            except Exception as e:\n                # raise\n                result = PluginRaisedExceptionScanResult(server_info, scan_command, e)\n\n            # Send the result to queue_out\n            self.queue_out.put(result)\n            current_queue_in.task_done()", "response": "This method is called by the main thread when the thread is started."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect_socket(self, sock: socket.socket) -> None:\n        # Setup HTTP tunneling\n        try:\n            sock.connect((self._tunnel_host, self._tunnel_port))\n        except socket.timeout as e:\n            raise ProxyError(self.ERR_PROXY_OFFLINE.format(str(e)))\n        except socket.error as e:\n            raise ProxyError(self.ERR_PROXY_OFFLINE.format(str(e)))\n\n        # Send a CONNECT request with the host we want to tunnel to\n        if self._tunnel_basic_auth_token is None:\n            sock.send(self.HTTP_CONNECT_REQ.format(self._server_host, self._server_port).encode('utf-8'))\n        else:\n            sock.send(self.HTTP_CONNECT_REQ_PROXY_AUTH_BASIC.format(\n                self._server_host, self._server_port, self._tunnel_basic_auth_token\n            ).encode('utf-8'))\n        http_response = HttpResponseParser.parse_from_socket(sock)\n\n        # Check if the proxy was able to connect to the host\n        if http_response.status != 200:\n            raise ProxyError(self.ERR_CONNECT_REJECTED)", "response": "Connects to the specified socket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an SSLConnection instance with the right SSL configuration for successfully connecting to the server.", "response": "def get_preconfigured_ssl_connection(\n            self,\n            override_ssl_version: Optional[OpenSslVersionEnum] = None,\n            ssl_verify_locations: Optional[str] = None,\n            should_use_legacy_openssl: Optional[bool] = None,\n    ) -> SslConnection:\n        \"\"\"Get an SSLConnection instance with the right SSL configuration for successfully connecting to the server.\n\n        Used by all plugins to connect to the server and run scans.\n        \"\"\"\n        if override_ssl_version is not None:\n            # Caller wants to override the ssl version to use for this connection\n            final_ssl_version = override_ssl_version\n            # Then we don't know which cipher suite is supported by the server for this ssl version\n            openssl_cipher_string = None\n        else:\n            # Use the ssl version and cipher suite that were successful during connectivity testing\n            final_ssl_version = self.highest_ssl_version_supported\n            openssl_cipher_string = self.openssl_cipher_string_supported\n\n        if should_use_legacy_openssl is not None:\n            # Caller wants to override which version of OpenSSL to use\n            # Then we don't know which cipher suite is supported by this version of OpenSSL\n            openssl_cipher_string = None\n\n        if self.client_auth_credentials is not None:\n            # If we have creds for client authentication, go ahead and use them\n            should_ignore_client_auth = False\n        else:\n            # Ignore client auth requests if the server allows optional TLS client authentication\n            should_ignore_client_auth = True\n            # But do not ignore them is client authentication is required so that the right exceptions get thrown\n            # within the plugins, providing a better output\n            if self.client_auth_requirement == ClientAuthenticationServerConfigurationEnum.REQUIRED:\n                should_ignore_client_auth = False\n\n        ssl_connection = SslConnectionConfigurator.get_connection(\n            ssl_version=final_ssl_version,\n            server_info=self,\n            openssl_cipher_string=openssl_cipher_string,\n            ssl_verify_locations=ssl_verify_locations,\n            should_use_legacy_openssl=should_use_legacy_openssl,\n            should_ignore_client_auth=should_ignore_client_auth,\n        )\n        return ssl_connection"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a CCS injection payload and return the result.", "response": "def do_handshake_with_ccs_injection(self):  # type: ignore\n    \"\"\"Modified do_handshake() to send a CCS injection payload and return the result.\n    \"\"\"\n    try:\n        # Start the handshake using nassl - will throw WantReadError right away\n        self._ssl.do_handshake()\n    except WantReadError:\n        # Send the Client Hello\n        len_to_read = self._network_bio.pending()\n        while len_to_read:\n            # Get the data from the SSL engine\n            handshake_data_out = self._network_bio.read(len_to_read)\n            # Send it to the peer\n            self._sock.send(handshake_data_out)\n            len_to_read = self._network_bio.pending()\n\n    # Retrieve the server's response - directly read the underlying network socket\n    # Retrieve data until we get to the ServerHelloDone\n    # The server may send back a ServerHello, an Alert or a CertificateRequest first\n    did_receive_hello_done = False\n    remaining_bytes = b''\n    while not did_receive_hello_done:\n        try:\n            tls_record, len_consumed = TlsRecordParser.parse_bytes(remaining_bytes)\n            remaining_bytes = remaining_bytes[len_consumed::]\n        except NotEnoughData:\n            # Try to get more data\n            raw_ssl_bytes = self._sock.recv(16381)\n            if not raw_ssl_bytes:\n                # No data?\n                break\n\n            remaining_bytes = remaining_bytes + raw_ssl_bytes\n            continue\n\n        if isinstance(tls_record, TlsHandshakeRecord):\n            # Does the record contain a ServerDone message?\n            for handshake_message in tls_record.subprotocol_messages:\n                if handshake_message.handshake_type == TlsHandshakeTypeByte.SERVER_DONE:\n                    did_receive_hello_done = True\n                    break\n            # If not, it could be a ServerHello, Certificate or a CertificateRequest if the server requires client auth\n        elif isinstance(tls_record, TlsAlertRecord):\n            # Server returned a TLS alert\n            break\n        else:\n            raise ValueError('Unknown record? Type {}'.format(tls_record.header.type))\n\n    if did_receive_hello_done:\n        # Send an early CCS record - this should be rejected by the server\n        payload = TlsChangeCipherSpecRecord.from_parameters(\n            tls_version=TlsVersionEnum[self._ssl_version.name]).to_bytes()\n        self._sock.send(payload)\n\n        # Send an early application data record which should be ignored by the server\n        app_data_record = TlsApplicationDataRecord.from_parameters(tls_version=TlsVersionEnum[self._ssl_version.name],\n                                                                   application_data=b'\\x00\\x00')\n        self._sock.send(app_data_record.to_bytes())\n\n        # Check if an alert was sent back\n        while True:\n            try:\n                tls_record, len_consumed = TlsRecordParser.parse_bytes(remaining_bytes)\n                remaining_bytes = remaining_bytes[len_consumed::]\n            except NotEnoughData:\n                # Try to get more data\n                try:\n                    raw_ssl_bytes = self._sock.recv(16381)\n                    if not raw_ssl_bytes:\n                        # No data?\n                        raise NotVulnerableToCcsInjection()\n                except socket.error:\n                    # Server closed the connection after receiving the CCS payload\n                    raise NotVulnerableToCcsInjection()\n\n                remaining_bytes = remaining_bytes + raw_ssl_bytes\n                continue\n\n            if isinstance(tls_record, TlsAlertRecord):\n                # Server returned a TLS alert but which one?\n                if tls_record.alert_description == 0x14:\n                    # BAD_RECORD_MAC: This means that the server actually tried to decrypt our early application data\n                    # record instead of ignoring it; server is vulnerable\n                    raise VulnerableToCcsInjection()\n\n                # Any other alert means that the server rejected the early CCS record\n                raise NotVulnerableToCcsInjection()\n            else:\n                break\n\n        raise NotVulnerableToCcsInjection()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a heartbleed payload and return the result.", "response": "def do_handshake_with_heartbleed(self):  # type: ignore\n    \"\"\"Modified do_handshake() to send a heartbleed payload and return the result.\n    \"\"\"\n    try:\n        # Start the handshake using nassl - will throw WantReadError right away\n        self._ssl.do_handshake()\n    except WantReadError:\n        # Send the Client Hello\n        len_to_read = self._network_bio.pending()\n        while len_to_read:\n            # Get the data from the SSL engine\n            handshake_data_out = self._network_bio.read(len_to_read)\n            # Send it to the peer\n            self._sock.send(handshake_data_out)\n            len_to_read = self._network_bio.pending()\n\n    # Build the heartbleed payload - based on\n    # https://blog.mozilla.org/security/2014/04/12/testing-for-heartbleed-vulnerability-without-exploiting-the-server/\n    payload = TlsHeartbeatRequestRecord.from_parameters(\n        tls_version=TlsVersionEnum[self._ssl_version.name],\n        heartbeat_data=b'\\x01' * 16381\n    ).to_bytes()\n\n    payload += TlsHeartbeatRequestRecord.from_parameters(\n        TlsVersionEnum[self._ssl_version.name],\n        heartbeat_data=b'\\x01\\x00\\x00'\n    ).to_bytes()\n\n    # Send the payload\n    self._sock.send(payload)\n\n    # Retrieve the server's response - directly read the underlying network socket\n    # Retrieve data until we get to the ServerHelloDone\n    # The server may send back a ServerHello, an Alert, a CertificateRequest or may just close the connection\n    did_receive_hello_done = False\n    remaining_bytes = b''\n    while not did_receive_hello_done:\n        try:\n            tls_record, len_consumed = TlsRecordParser.parse_bytes(remaining_bytes)\n            remaining_bytes = remaining_bytes[len_consumed::]\n        except NotEnoughData:\n            # Try to get more data\n            try:\n                raw_ssl_bytes = self._sock.recv(16381)\n            except socket.error:\n                # Server closed the connection as soon as it received the Heartbleed payload\n                raise NotVulnerableToHeartbleed()\n\n            if not raw_ssl_bytes:\n                # No data?\n                raise NotVulnerableToHeartbleed()\n\n            remaining_bytes = remaining_bytes + raw_ssl_bytes\n            continue\n\n        if isinstance(tls_record, TlsHandshakeRecord):\n            # Does the record contain a ServerDone message?\n            for handshake_message in tls_record.subprotocol_messages:\n                if handshake_message.handshake_type == TlsHandshakeTypeByte.SERVER_DONE:\n                    did_receive_hello_done = True\n                    break\n            # If not, it could be a ServerHello, Certificate or a CertificateRequest if the server requires client auth\n        elif isinstance(tls_record, TlsAlertRecord):\n            # Server returned a TLS alert\n            break\n        else:\n            raise ValueError('Unknown record? Type {}'.format(tls_record.header.type))\n\n    is_vulnerable_to_heartbleed = False\n    if did_receive_hello_done:\n        expected_heartbleed_payload = b'\\x01' * 10\n        if expected_heartbleed_payload in remaining_bytes:\n            # Server replied with our hearbeat payload\n            is_vulnerable_to_heartbleed = True\n        else:\n            try:\n                raw_ssl_bytes = self._sock.recv(16381)\n            except socket.error:\n                # Server closed the connection after receiving the heartbleed payload\n                raise NotVulnerableToHeartbleed()\n\n            if expected_heartbleed_payload in raw_ssl_bytes:\n                # Server replied with our hearbeat payload\n                is_vulnerable_to_heartbleed = True\n\n    if is_vulnerable_to_heartbleed:\n        raise VulnerableToHeartbleed()\n    else:\n        raise NotVulnerableToHeartbleed()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the command line and returns a list of server connectivity testers and server string parsing errors.", "response": "def parse_command_line(self) -> Tuple[List[ServerConnectivityTester], List[ServerStringParsingError], Any]:\n        \"\"\"Parses the command line used to launch SSLyze.\n        \"\"\"\n        (args_command_list, args_target_list) = self._parser.parse_args()\n\n        if args_command_list.update_trust_stores:\n            # Just update the trust stores and do nothing\n            TrustStoresRepository.update_default()\n            raise TrustStoresUpdateCompleted()\n\n        # Handle the --targets_in command line and fill args_target_list\n        if args_command_list.targets_in:\n            if args_target_list:\n                raise CommandLineParsingError('Cannot use --targets_list and specify targets within the command line.')\n\n            try:  # Read targets from a file\n                with open(args_command_list.targets_in) as f:\n                    for target in f.readlines():\n                        if target.strip():  # Ignore empty lines\n                            if not target.startswith('#'):  # Ignore comment lines\n                                args_target_list.append(target.strip())\n            except IOError:\n                raise CommandLineParsingError('Can\\'t read targets from input file \\'{}.'.format(\n                    args_command_list.targets_in))\n\n        if not args_target_list:\n            raise CommandLineParsingError('No targets to scan.')\n\n        # Handle the --regular command line parameter as a shortcut\n        if self._parser.has_option('--regular'):\n            if getattr(args_command_list, 'regular'):\n                setattr(args_command_list, 'regular', False)\n                for cmd in self.REGULAR_CMD:\n                    setattr(args_command_list, cmd, True)\n\n        # Sanity checks on the command line options\n        # Prevent --quiet and --xml_out -\n        if args_command_list.xml_file and args_command_list.xml_file == '-' and args_command_list.quiet:\n            raise CommandLineParsingError('Cannot use --quiet with --xml_out -.')\n\n        # Prevent --quiet and --json_out -\n        if args_command_list.json_file and args_command_list.json_file == '-' and args_command_list.quiet:\n            raise CommandLineParsingError('Cannot use --quiet with --json_out -.')\n\n        # Prevent --xml_out - and --json_out -\n        if args_command_list.json_file and args_command_list.json_file == '-' \\\n                and args_command_list.xml_file and args_command_list.xml_file == '-':\n            raise CommandLineParsingError('Cannot use --xml_out - with --json_out -.')\n\n        # Sanity checks on the client cert options\n        client_auth_creds = None\n        if bool(args_command_list.cert) ^ bool(args_command_list.key):\n            raise CommandLineParsingError('No private key or certificate file were given. See --cert and --key.')\n\n        elif args_command_list.cert:\n            # Private key formats\n            if args_command_list.keyform == 'DER':\n                key_type = OpenSslFileTypeEnum.ASN1\n            elif args_command_list.keyform == 'PEM':\n                key_type = OpenSslFileTypeEnum.PEM\n            else:\n                raise CommandLineParsingError('--keyform should be DER or PEM.')\n\n            # Let's try to open the cert and key files\n            try:\n                client_auth_creds = ClientAuthenticationCredentials(args_command_list.cert,\n                                                                    args_command_list.key,\n                                                                    key_type,\n                                                                    args_command_list.keypass)\n            except ValueError as e:\n                raise CommandLineParsingError('Invalid client authentication settings: {}.'.format(e.args[0]))\n\n        # HTTP CONNECT proxy\n        http_tunneling_settings = None\n        if args_command_list.https_tunnel:\n            try:\n                http_tunneling_settings = HttpConnectTunnelingSettings.from_url(args_command_list.https_tunnel)\n            except ValueError as e:\n                raise CommandLineParsingError('Invalid proxy URL for --https_tunnel: {}.'.format(e.args[0]))\n\n        # STARTTLS\n        tls_wrapped_protocol = TlsWrappedProtocolEnum.PLAIN_TLS\n        if args_command_list.starttls:\n            if args_command_list.starttls not in self.START_TLS_PROTOCOLS:\n                raise CommandLineParsingError(self.START_TLS_USAGE)\n            else:\n                # StartTLS was specified\n                if args_command_list.starttls in self.STARTTLS_PROTOCOL_DICT.keys():\n                    # Protocol was given in the command line\n                    tls_wrapped_protocol = self.STARTTLS_PROTOCOL_DICT[args_command_list.starttls]\n\n        # Create the server connectivity tester for each specified servers\n        # A limitation when using the command line is that only one client_auth_credentials and http_tunneling_settings\n        # can be specified, for all the servers to scan\n        good_server_list = []\n        bad_server_list = []\n        for server_string in args_target_list:\n            try:\n                hostname, ip_address, port = CommandLineServerStringParser.parse_server_string(server_string)\n            except ServerStringParsingError as e:\n                # Will happen if the server string is malformed\n                bad_server_list.append(e)\n                continue\n\n            try:\n                # TODO(AD): Unicode hostnames may fail on Python2\n                # hostname = hostname.decode('utf-8')\n                server_info = ServerConnectivityTester(\n                    hostname=hostname,\n                    port=port,\n                    ip_address=ip_address,\n                    tls_wrapped_protocol=tls_wrapped_protocol,\n                    tls_server_name_indication=args_command_list.sni,\n                    xmpp_to_hostname=args_command_list.xmpp_to,\n                    client_auth_credentials=client_auth_creds,\n                    http_tunneling_settings=http_tunneling_settings\n                )\n                good_server_list.append(server_info)\n            except ValueError as e:\n                # Will happen for example if xmpp_to is specified for a non-XMPP connection\n                raise CommandLineParsingError(e.args[0])\n\n        # Command line hacks\n        # Handle --starttls=auto now that we parsed the server strings\n        if args_command_list.starttls == 'auto':\n            for server_info in good_server_list:\n                # We use the port number to deduce the protocol\n                if server_info.port in self.STARTTLS_PROTOCOL_DICT.keys():\n                    server_info.tls_wrapped_protocol = self.STARTTLS_PROTOCOL_DICT[server_info.port]\n\n        # Handle --http_get now that we parsed the server strings\n        # Doing it here is hacky as the option is defined within PluginOpenSSLCipherSuites\n        if args_command_list.http_get:\n            for server_info in good_server_list:\n                if server_info.port == 443:\n                    server_info.tls_wrapped_protocol = TlsWrappedProtocolEnum.HTTPS\n\n        return good_server_list, bad_server_list, args_command_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_default_options(self) -> None:\n        # Updating the trust stores\n        update_stores_group = OptionGroup(self._parser, 'Trust stores options', '')\n        update_stores_group.add_option(\n            '--update_trust_stores',\n            help='Update the default trust stores used by SSLyze. The latest stores will be downloaded from '\n                 'https://github.com/nabla-c0d3/trust_stores_observatory. This option is meant to be used separately, '\n                 'and will silence any other command line option supplied to SSLyze.',\n            dest='update_trust_stores',\n            action='store_true',\n        )\n        self._parser.add_option_group(update_stores_group)\n\n        # Client certificate options\n        clientcert_group = OptionGroup(self._parser, 'Client certificate options', '')\n        clientcert_group.add_option(\n            '--cert',\n            help='Client certificate chain filename. The certificates must be in PEM format and must be sorted '\n                 'starting with the subject\\'s client certificate, followed by intermediate CA certificates if '\n                 'applicable.',\n            dest='cert'\n        )\n        clientcert_group.add_option(\n            '--key',\n            help='Client private key filename.',\n            dest='key'\n        )\n        clientcert_group.add_option(\n            '--keyform',\n            help='Client private key format. DER or PEM (default).',\n            dest='keyform',\n            default='PEM'\n        )\n        clientcert_group.add_option(\n            '--pass',\n            help='Client private key passphrase.',\n            dest='keypass',\n            default=''\n        )\n        self._parser.add_option_group(clientcert_group)\n\n        # Input / output\n        output_group = OptionGroup(self._parser, 'Input and output options', '')\n        # XML output\n        output_group.add_option(\n            '--xml_out',\n            help='Write the scan results as an XML document to the file XML_FILE. If XML_FILE is set to \"-\", the XML '\n                 'output will instead be printed to stdout. The corresponding XML Schema Definition is available at '\n                 './docs/xml_out.xsd',\n            dest='xml_file',\n            default=None\n        )\n        # JSON output\n        output_group.add_option(\n            '--json_out',\n            help='Write the scan results as a JSON document to the file JSON_FILE. If JSON_FILE is set to \"-\", the '\n                 'JSON output will instead be printed to stdout. The resulting JSON file is a serialized version of '\n                 'the ScanResult objects described in SSLyze\\'s Python API: the nodes and attributes will be the same. '\n                 'See https://nabla-c0d3.github.io/sslyze/documentation/available-scan-commands.html for more details.',\n            dest='json_file',\n            default=None\n        )\n        # Read targets from input file\n        output_group.add_option(\n            '--targets_in',\n            help='Read the list of targets to scan from the file TARGETS_IN. It should contain one host:port per '\n                 'line.',\n            dest='targets_in',\n            default=None\n        )\n        # No text output\n        output_group.add_option(\n            '--quiet',\n            action='store_true',\n            dest='quiet',\n            help='Do not output anything to stdout; useful when using --xml_out or --json_out.'\n        )\n        self._parser.add_option_group(output_group)\n\n        # Connectivity option group\n        connect_group = OptionGroup(self._parser, 'Connectivity options', '')\n        # Connection speed\n        connect_group.add_option(\n            '--slow_connection',\n            help='Greatly reduce the number of concurrent connections initiated by SSLyze. This will make the scans '\n                 'slower but more reliable if the connection between your host and the server is slow, or if the '\n                 'server cannot handle many concurrent connections. Enable this option if you are getting a lot of '\n                 'timeouts or errors.',\n            action='store_true',\n            dest='slow_connection',\n        )\n        # HTTP CONNECT Proxy\n        connect_group.add_option(\n            '--https_tunnel',\n            help='Tunnel all traffic to the target server(s) through an HTTP CONNECT proxy. HTTP_TUNNEL should be the '\n                 'proxy\\'s URL: \\'http://USER:PW@HOST:PORT/\\'. For proxies requiring authentication, only Basic '\n                 'Authentication is supported.',\n            dest='https_tunnel',\n            default=None\n        )\n        # STARTTLS\n        connect_group.add_option(\n            '--starttls',\n            help='Perform a StartTLS handshake when connecting to the target server(s). '\n                 '{}'.format(self.START_TLS_USAGE),\n            dest='starttls',\n            default=None\n        )\n        connect_group.add_option(\n            '--xmpp_to',\n            help='Optional setting for STARTTLS XMPP. XMPP_TO should be the hostname to be put in the \\'to\\' '\n                 'attribute of the XMPP stream. Default is the server\\'s hostname.',\n            dest='xmpp_to',\n            default=None\n        )\n        # Server Name Indication\n        connect_group.add_option(\n            '--sni',\n            help='Use Server Name Indication to specify the hostname to connect to.  Will only affect TLS 1.0+ '\n                 'connections.',\n            dest='sni',\n            default=None\n        )\n        self._parser.add_option_group(connect_group)", "response": "Add default command line options to the parser."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_plugin_options(self, available_plugins: Set[Type[Plugin]]) -> None:\n        for plugin_class in available_plugins:\n            # Add the current plugin's commands to the parser\n            group = OptionGroup(self._parser, plugin_class.get_title(), plugin_class.get_description())\n            for option in plugin_class.get_cli_option_group():\n                group.add_option(option)\n            self._parser.add_option_group(group)", "response": "Recovers the list of command line options implemented by the available plugins and adds them to the command\n            line parser."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the README file into the long description.", "response": "def get_long_description():\n    \"\"\"Convert the README file into the long description.\n    \"\"\"\n    with open(path.join(root_path, 'README.md'), encoding='utf-8') as f:\n        long_description = f.read()\n    return long_description"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the list of trust stores so they properly packaged when doing a cx_freeze build.", "response": "def get_include_files():\n    \"\"\"\"Get the list of trust stores so they properly packaged when doing a cx_freeze build.\n    \"\"\"\n    plugin_data_files = []\n    trust_stores_pem_path = path.join(root_path, 'sslyze', 'plugins', 'utils', 'trust_store', 'pem_files')\n    for file in listdir(trust_stores_pem_path):\n        file = path.join(trust_stores_pem_path, file)\n        if path.isfile(file):  # skip directories\n            filename = path.basename(file)\n            plugin_data_files.append((file, path.join('pem_files', filename)))\n    return plugin_data_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dns_subject_alternative_names(certificate: cryptography.x509.Certificate) -> List[str]:\n        subj_alt_names: List[str] = []\n        try:\n            san_ext = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n            subj_alt_names = san_ext.value.get_values_for_type(DNSName)\n        except ExtensionNotFound:\n            pass\n        return subj_alt_names", "response": "Retrieve all the DNS entries of the Subject Alternative Name extension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify that the certificate was issued for the given hostname.", "response": "def matches_hostname(cls, certificate: cryptography.x509.Certificate, hostname: str) -> None:\n        \"\"\"Verify that the certificate was issued for the given hostname.\n\n        Raises:\n            CertificateError: If the certificate was not issued for the supplied hostname.\n        \"\"\"\n        # Extract the names from the certificate to create the properly-formatted dictionary\n        certificate_names = {\n            'subject': (tuple([('commonName', name) for name in cls.get_common_names(certificate.subject)]),),\n            'subjectAltName': tuple([('DNS', name) for name in cls.get_dns_subject_alternative_names(certificate)]),\n        }\n        # CertificateError is raised on failure\n        ssl.match_hostname(certificate_names, hostname)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_name_as_short_text(cls, name_field: cryptography.x509.Name) -> str:\n        # Name_field is supposed to be a Subject or an Issuer; print the CN if there is one\n        common_names = cls.get_common_names(name_field)\n        if common_names:\n            # We don't support certs with multiple CNs\n            return common_names[0]\n        else:\n            # Otherwise show the whole field\n            return cls.get_name_as_text(name_field)", "response": "Convert a cryptography. x509. Name field returned by the cryptography module to a string suitable for displaying it to the user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the HTTP Public Key Pinning hash for the given certificate.", "response": "def get_hpkp_pin(cls, certificate: cryptography.x509.Certificate) -> str:\n        \"\"\"Generate the HTTP Public Key Pinning hash (RFC 7469) for the given certificate.\n        \"\"\"\n        return b64encode(cls.get_public_key_sha256(certificate)).decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_ocsp_must_staple_extension(certificate: cryptography.x509.Certificate) -> bool:\n        has_ocsp_must_staple = False\n        try:\n            tls_feature_ext = certificate.extensions.get_extension_for_oid(ExtensionOID.TLS_FEATURE)\n            for feature_type in tls_feature_ext.value:\n                if feature_type == cryptography.x509.TLSFeatureType.status_request:\n                    has_ocsp_must_staple = True\n                    break\n        except ExtensionNotFound:\n            pass\n\n        return has_ocsp_must_staple", "response": "Return True if the certificate has the OCSP Must - Staple extension defined in RFC 6066."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef count_scts_in_sct_extension(certificate: cryptography.x509.Certificate) -> Optional[int]:\n        scts_count = 0\n        try:\n            # Look for the x509 extension\n            sct_ext = certificate.extensions.get_extension_for_oid(\n                ExtensionOID.PRECERT_SIGNED_CERTIFICATE_TIMESTAMPS\n            )\n\n            if isinstance(sct_ext.value, cryptography.x509.UnrecognizedExtension):\n                # The version of OpenSSL on the system is too old and can't parse the SCT extension\n                return None\n\n            # Count the number of entries in the extension\n            scts_count = len(sct_ext.value)\n        except ExtensionNotFound:\n            pass\n\n        return scts_count", "response": "Return the number of Signed Certificate Timestamps embedded in the certificate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend an HTTP GET request to the server and return the HTTP status code.", "response": "def send_request(self, ssl_client: SslClient) -> str:\n        \"\"\"Send an HTTP GET to the server and return the HTTP status code.\n        \"\"\"\n        try:\n            ssl_client.write(HttpRequestGenerator.get_request(self._hostname))\n\n            # Parse the response and print the Location header\n            http_response = HttpResponseParser.parse_from_ssl_connection(ssl_client)\n            if http_response.version == 9:\n                # HTTP 0.9 => Probably not an HTTP response\n                result = self.ERR_NOT_HTTP\n            else:\n                redirect = ''\n                if 300 <= http_response.status < 400:\n                    redirect_location = http_response.getheader('Location')\n                    if redirect_location:\n                        # Add redirection URL to the result\n                        redirect = f' - {redirect_location}'\n\n                result = self.GET_RESULT_FORMAT.format(http_response.status, http_response.reason, redirect)\n        except socket.timeout:\n            result = self.ERR_HTTP_TIMEOUT\n        except IOError:\n            result = self.ERR_GENERIC\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_plugin_class_for_command(self, scan_command: PluginScanCommand) -> Type[Plugin]:\n        return self._scan_command_classes_to_plugin_classes[scan_command.__class__]", "response": "Get the class of the plugin implementing the supplied scan command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntricks to standardize the API between sockets and SSLConnection objects.", "response": "def _parse(read_method: Callable) -> HTTPResponse:\n        \"\"\"Trick to standardize the API between sockets and SSLConnection objects.\n        \"\"\"\n        response = read_method(4096)\n        while b'HTTP/' not in response or b'\\r\\n\\r\\n' not in response:\n            # Parse until the end of the headers\n            response += read_method(4096)\n\n        fake_sock = _FakeSocket(response)\n        response = HTTPResponse(fake_sock)  # type: ignore\n        response.begin()\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _work_function(job_q: Queue, result_q: Queue, error_q: Queue) -> None:\n    while True:\n        job = job_q.get()\n\n        if isinstance(job, _ThreadPoolSentinel):\n            # All the work is done, get out\n            result_q.put(_ThreadPoolSentinel())\n            error_q.put(_ThreadPoolSentinel())\n            job_q.task_done()\n            break\n\n        work_function = job[0]\n        args = job[1]\n        try:\n            result = work_function(*args)\n        except Exception as e:\n            error_q.put((job, e))\n        else:\n            result_q.put((job, result))\n        finally:\n            job_q.task_done()", "response": "This function runs the work function in a thread pool."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _resume_with_session_id(\n            self,\n            server_info: ServerConnectivityInfo,\n            ssl_version_to_use: OpenSslVersionEnum\n    ) -> bool:\n        \"\"\"Perform one session resumption using Session IDs.\n        \"\"\"\n        session1 = self._resume_ssl_session(server_info, ssl_version_to_use)\n        try:\n            # Recover the session ID\n            session1_id = self._extract_session_id(session1)\n        except IndexError:\n            # Session ID not assigned\n            return False\n\n        if session1_id == '':\n            # Session ID empty\n            return False\n\n        # Try to resume that SSL session\n        session2 = self._resume_ssl_session(server_info, ssl_version_to_use, session1)\n        try:\n            # Recover the session ID\n            session2_id = self._extract_session_id(session2)\n        except IndexError:\n            # Session ID not assigned\n            return False\n\n        # Finally, compare the two Session IDs\n        if session1_id != session2_id:\n            # Session ID assigned but not accepted\n            return False\n\n        return True", "response": "Perform one session resumption using Session IDs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming one session resumption using TLS Session Tickets.", "response": "def _resume_with_session_ticket(\n            self,\n            server_info: ServerConnectivityInfo,\n            ssl_version_to_use: OpenSslVersionEnum,\n    ) -> TslSessionTicketSupportEnum:\n        \"\"\"Perform one session resumption using TLS Session Tickets.\n        \"\"\"\n        # Connect to the server and keep the SSL session\n        try:\n            session1 = self._resume_ssl_session(server_info, ssl_version_to_use, should_enable_tls_ticket=True)\n        except SslHandshakeRejected:\n            if server_info.highest_ssl_version_supported >= OpenSslVersionEnum.TLSV1_3:\n                return TslSessionTicketSupportEnum.FAILED_ONLY_TLS_1_3_SUPPORTED\n            else:\n                raise\n\n        try:\n            # Recover the TLS ticket\n            session1_tls_ticket = self._extract_tls_session_ticket(session1)\n        except IndexError:\n            return TslSessionTicketSupportEnum.FAILED_TICKET_NOT_ASSIGNED\n\n        # Try to resume that session using the TLS ticket\n        session2 = self._resume_ssl_session(server_info, ssl_version_to_use, session1, should_enable_tls_ticket=True)\n        try:\n            # Recover the TLS ticket\n            session2_tls_ticket = self._extract_tls_session_ticket(session2)\n        except IndexError:\n            return TslSessionTicketSupportEnum.FAILED_TICKET_NOT_ASSIGNED\n\n        # Finally, compare the two TLS Tickets\n        if session1_tls_ticket != session2_tls_ticket:\n            return TslSessionTicketSupportEnum.FAILED_TICKED_IGNORED\n\n        return TslSessionTicketSupportEnum.SUCCEEDED"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting the SSL session ID from a SSL session object or raises IndexError", "response": "def _extract_session_id(ssl_session: nassl._nassl.SSL_SESSION) -> str:\n        \"\"\"Extract the SSL session ID from a SSL session object or raises IndexError if the session ID was not set.\n        \"\"\"\n        session_string = ((ssl_session.as_text()).split('Session-ID:'))[1]\n        session_id = (session_string.split('Session-ID-ctx:'))[0].strip()\n        return session_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the TLS session ticket from an SSL session object or raises IndexError", "response": "def _extract_tls_session_ticket(ssl_session: nassl._nassl.SSL_SESSION) -> str:\n        \"\"\"Extract the TLS session ticket from a SSL session object or raises IndexError if the ticket was not set.\n        \"\"\"\n        session_string = ((ssl_session.as_text()).split('TLS session ticket:'))[1]\n        session_tls_ticket = (session_string.split('Compression:'))[0]\n        return session_tls_ticket"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _resume_ssl_session(\n            server_info: ServerConnectivityInfo,\n            ssl_version_to_use: OpenSslVersionEnum,\n            ssl_session: Optional[nassl._nassl.SSL_SESSION] = None,\n            should_enable_tls_ticket: bool = False\n    ) -> nassl._nassl.SSL_SESSION:\n        \"\"\"Connect to the server and returns the session object that was assigned for that connection.\n        If ssl_session is given, tries to resume that session.\n        \"\"\"\n        ssl_connection = server_info.get_preconfigured_ssl_connection(override_ssl_version=ssl_version_to_use)\n        if not should_enable_tls_ticket:\n            # Need to disable TLS tickets to test session IDs, according to rfc5077:\n            # If a ticket is presented by the client, the server MUST NOT attempt\n            # to use the Session ID in the ClientHello for stateful session resumption\n            ssl_connection.ssl_client.disable_stateless_session_resumption()  # Turning off TLS tickets.\n\n        if ssl_session:\n            ssl_connection.ssl_client.set_session(ssl_session)\n\n        try:\n            # Perform the SSL handshake\n            ssl_connection.connect()\n            new_session = ssl_connection.ssl_client.get_session()  # Get session data\n        finally:\n            ssl_connection.close()\n\n        return new_session", "response": "Resumes a previously established SSL session."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _object_to_json_dict(obj: Any) -> Union[bool, int, float, str, Dict[str, Any]]:\n    if isinstance(obj, Enum):\n        # Properly serialize Enums (such as OpenSslVersionEnum)\n        result = obj.name\n\n    elif isinstance(obj, ObjectIdentifier):\n        # Use dotted string representation for OIDs\n        result = obj.dotted_string\n\n    elif isinstance(obj, x509._Certificate):\n        # Properly serialize certificates\n        certificate = obj\n        result = {  # type: ignore\n            # Add general info\n            'as_pem': obj.public_bytes(Encoding.PEM).decode('ascii'),\n            'hpkp_pin': CertificateUtils.get_hpkp_pin(obj),\n\n            # Add some of the fields of the cert\n            'subject': CertificateUtils.get_name_as_text(certificate.subject),\n            'issuer': CertificateUtils.get_name_as_text(certificate.issuer),\n            'serialNumber': str(certificate.serial_number),\n            'notBefore': certificate.not_valid_before.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            'notAfter': certificate.not_valid_after.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            'signatureAlgorithm': certificate.signature_hash_algorithm.name,\n            'publicKey': {\n                'algorithm': CertificateUtils.get_public_key_type(certificate)\n            },\n        }\n\n        dns_alt_names = CertificateUtils.get_dns_subject_alternative_names(certificate)\n        if dns_alt_names:\n            result['subjectAlternativeName'] = {'DNS': dns_alt_names}  # type: ignore\n\n        # Add some info about the public key\n        public_key = certificate.public_key()\n        if isinstance(public_key, EllipticCurvePublicKey):\n            result['publicKey']['size'] = str(public_key.curve.key_size)  # type: ignore\n            result['publicKey']['curve'] = public_key.curve.name  # type: ignore\n        else:\n            result['publicKey']['size'] = str(public_key.key_size)\n            result['publicKey']['exponent'] = str(public_key.public_numbers().e)\n\n    elif isinstance(obj, object):\n        # Some objects (like str) don't have a __dict__\n        if hasattr(obj, '__dict__'):\n            result = {}\n            for key, value in obj.__dict__.items():\n                # Remove private attributes\n                if key.startswith('_'):\n                    continue\n\n                result[key] = _object_to_json_dict(value)\n        else:\n            # Simple object like a bool\n            result = obj\n\n    else:\n        raise TypeError('Unknown type: {}'.format(repr(obj)))\n\n    return result", "response": "Convert an object to a dictionary suitable for the JSON output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_and_verify_certificate_chain(\n            server_info: ServerConnectivityInfo,\n            trust_store: TrustStore\n    ) -> Tuple[List[Certificate], str, Optional[OcspResponse]]:\n        \"\"\"Connects to the target server and uses the supplied trust store to validate the server's certificate.\n        Returns the server's certificate and OCSP response.\n        \"\"\"\n        ssl_connection = server_info.get_preconfigured_ssl_connection(ssl_verify_locations=trust_store.path)\n\n        # Enable OCSP stapling\n        ssl_connection.ssl_client.set_tlsext_status_ocsp()\n\n        try:  # Perform the SSL handshake\n            ssl_connection.connect()\n\n            ocsp_response = ssl_connection.ssl_client.get_tlsext_status_ocsp_resp()\n            x509_cert_chain = ssl_connection.ssl_client.get_peer_cert_chain()\n            (_, verify_str) = ssl_connection.ssl_client.get_certificate_chain_verify_result()\n\n        except ClientCertificateRequested:  # The server asked for a client cert\n            # We can get the server cert anyway\n            ocsp_response = ssl_connection.ssl_client.get_tlsext_status_ocsp_resp()\n            x509_cert_chain = ssl_connection.ssl_client.get_peer_cert_chain()\n            (_, verify_str) = ssl_connection.ssl_client.get_certificate_chain_verify_result()\n\n        finally:\n            ssl_connection.close()\n\n        # Parse the certificates using the cryptography module\n        parsed_x509_chain = [load_pem_x509_certificate(x509_cert.as_pem().encode('ascii'), backend=default_backend())\n                             for x509_cert in x509_cert_chain]\n        return parsed_x509_chain, verify_str, ocsp_response", "response": "Connects to the target server and uses the supplied trust store to validate the server s certificate. Returns the server s certificate and OCSP response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_description(cls) -> str:\n        if cls.__doc__ is None:\n            raise ValueError('No docstring found for {}'.format(cls.__name__))\n        return cls.__doc__.strip()", "response": "Returns the description of the command class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef command_line_parsed(\n            self,\n            available_plugins: Set[Type[Plugin]],\n            args_command_list: Any,\n            malformed_servers: List[ServerStringParsingError]\n    ) -> None:\n        \"\"\"The CLI was just started and successfully parsed the command line.\n        \"\"\"", "response": "The CLI was just started and successfully parsed the command line."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nserving the SMTP server on the given port and address.", "response": "def serve(self, port=None, address=None):\n        \"\"\"Serves the SMTP server on the given port and address.\"\"\"\n        port = port or self.port\n        address = address or self.address\n\n        log.info('Starting SMTP server at {0}:{1}'.format(address, port))\n\n        server = InboxServer(self.collator, (address, port), None)\n\n        try:\n            asyncore.loop()\n        except KeyboardInterrupt:\n            log.info('Cleaning up')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncommand - line dispatch.", "response": "def dispatch(self):\n        \"\"\"Command-line dispatch.\"\"\"\n        parser = argparse.ArgumentParser(description='Run an Inbox server.')\n\n        parser.add_argument('addr', metavar='addr', type=str, help='addr to bind to')\n        parser.add_argument('port', metavar='port', type=int, help='port to bind to')\n\n        args = parser.parse_args()\n\n        self.serve(port=args.port, address=args.addr)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_ltime(endianess, tm_format, data):\n\tif tm_format == 0:\n\t\treturn ''\n\telif tm_format == 1 or tm_format == 2:  # I can't find solid documentation on the BCD data-type\n\t\ty = data[0]\n\t\tyear = '????'\n\t\tif 90 <= y <= 99:\n\t\t\tyear = '19' + str(y)\n\t\telif 0 <= y <= 9:\n\t\t\tyear = '200' + str(y)\n\t\telif 10 <= y <= 89:\n\t\t\tyear = '20' + str(y)\n\t\tmonth = data[1]\n\t\tday = data[2]\n\t\thour = data[3]\n\t\tminute = data[4]\n\t\tsecond = data[5]\n\telif tm_format == 3 or tm_format == 4:\n\t\tif tm_format == 3:\n\t\t\tu_time = float(struct.unpack(endianess + 'I', data[0:4])[0])\n\t\t\tsecond = float(data[4])\n\t\t\tfinal_time = time.gmtime((u_time * 60) + second)\n\t\telif tm_format == 4:\n\t\t\tfinal_time = time.gmtime(float(struct.unpack(endianess + 'I', data[0:4])[0]))\n\t\tyear = str(final_time.tm_year)\n\t\tmonth = str(final_time.tm_mon)\n\t\tday = str(final_time.tm_mday)\n\t\thour = str(final_time.tm_hour)\n\t\tminute = str(final_time.tm_min)\n\t\tsecond = str(final_time.tm_sec)\n\n\treturn \"{0} {1} {2} {3}:{4}:{5}\".format((MONTHS.get(month) or 'UNKNOWN'), day, year, hour, minute, second)", "response": "This function formats the time in a human readable time stamp."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning data formatted into a log entry. :param str endianess: The endianess to use when packing values ('>' or '<') :param bool hist_date_time_flag: Whether or not a time stamp is included. :param int tm_format: The format that the data is packed in, this typically corresponds with the value in the GEN_CONFIG_TBL (table #0) (1 <= tm_format <= 4) :param bool event_number_flag: Whether or not an event number is included. :param bool hist_seq_nbr_flag: Whether or not an history sequence number is included. :param str data: The packed and machine-formatted data to parse :rtype: dict", "response": "def get_history_entry_record(endianess, hist_date_time_flag, tm_format, event_number_flag, hist_seq_nbr_flag, data):\n\t\"\"\"\n\tReturn data formatted into a log entry.\n\n\t:param str endianess: The endianess to use when packing values ('>' or '<')\n\t:param bool hist_date_time_flag: Whether or not a time stamp is included.\n\t:param int tm_format: The format that the data is packed in, this typically\n\t  corresponds with the value in the GEN_CONFIG_TBL (table #0) (1 <= tm_format <= 4)\n\t:param bool event_number_flag: Whether or not an event number is included.\n\t:param bool hist_seq_nbr_flag: Whether or not an history sequence number\n\t  is included.\n\t:param str data: The packed and machine-formatted data to parse\n\t:rtype: dict\n\t\"\"\"\n\trcd = {}\n\tif hist_date_time_flag:\n\t\ttmstmp = format_ltime(endianess, tm_format, data[0:LTIME_LENGTH.get(tm_format)])\n\t\tif tmstmp:\n\t\t\trcd['Time'] = tmstmp\n\t\tdata = data[LTIME_LENGTH.get(tm_format):]\n\tif event_number_flag:\n\t\trcd['Event Number'] = struct.unpack(endianess + 'H', data[:2])[0]\n\t\tdata = data[2:]\n\tif hist_seq_nbr_flag:\n\t\trcd['History Sequence Number'] = struct.unpack(endianess + 'H', data[:2])[0]\n\t\tdata = data[2:]\n\trcd['User ID'] = struct.unpack(endianess + 'H', data[:2])[0]\n\trcd['Procedure Number'], rcd['Std vs Mfg'] = get_table_idbb_field(endianess, data[2:4])[:2]\n\trcd['Arguments'] = data[4:]\n\treturn rcd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_table_idbb_field(endianess, data):\n\tbfld = struct.unpack(endianess + 'H', data[:2])[0]\n\tproc_nbr = bfld & 0x7ff\n\tstd_vs_mfg = bool(bfld & 0x800)\n\tselector = (bfld & 0xf000) >> 12\n\treturn (proc_nbr, std_vs_mfg, selector)", "response": "This function parses the packed TABLE_IDB_BFLD bit - field and returns the corresponding tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn data from a packed TABLE_IDC_BFLD bit-field. :param str endianess: The endianess to use when packing values ('>' or '<') :param str data: The packed and machine-formatted data to parse :rtype: tuple :return: Tuple of (proc_nbr, std_vs_mfg, proc_flag, flag1, flag2, flag3)", "response": "def get_table_idcb_field(endianess, data):\n\t\"\"\"\n\tReturn data from a packed TABLE_IDC_BFLD bit-field.\n\n\t:param str endianess: The endianess to use when packing values ('>' or '<')\n\t:param str data: The packed and machine-formatted data to parse\n\t:rtype: tuple\n\t:return: Tuple of (proc_nbr, std_vs_mfg, proc_flag, flag1, flag2, flag3)\n\t\"\"\"\n\tbfld = struct.unpack(endianess + 'H', data[:2])[0]\n\tproc_nbr = bfld & 2047\n\tstd_vs_mfg = bool(bfld & 2048)\n\tproc_flag = bool(bfld & 4096)\n\tflag1 = bool(bfld & 8192)\n\tflag2 = bool(bfld & 16384)\n\tflag3 = bool(bfld & 32768)\n\treturn (proc_nbr, std_vs_mfg, proc_flag, flag1, flag2, flag3)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_ipy(self, args):\n\t\timport c1218.data\n\t\timport c1219.data\n\t\tfrom c1219.access.general import C1219GeneralAccess\n\t\tfrom c1219.access.security import C1219SecurityAccess\n\t\tfrom c1219.access.log import C1219LogAccess\n\t\tfrom c1219.access.telephone import C1219TelephoneAccess\n\t\tvars = {\n\t\t\t'termineter.__version__': termineter.__version__,\n\t\t\t'C1218Packet': c1218.data.C1218Packet,\n\t\t\t'C1218ReadRequest': c1218.data.C1218ReadRequest,\n\t\t\t'C1218WriteRequest': c1218.data.C1218WriteRequest,\n\t\t\t'C1219ProcedureInit': c1219.data.C1219ProcedureInit,\n\t\t\t'C1219GeneralAccess': C1219GeneralAccess,\n\t\t\t'C1219SecurityAccess': C1219SecurityAccess,\n\t\t\t'C1219LogAccess': C1219LogAccess,\n\t\t\t'C1219TelephoneAccess': C1219TelephoneAccess,\n\t\t\t'frmwk': self.frmwk,\n\t\t\t'os': os,\n\t\t\t'sys': sys\n\t\t}\n\t\tbanner = 'Python ' + sys.version + ' on ' + sys.platform + os.linesep\n\t\tbanner += os.linesep\n\t\tbanner += 'The framework instance is in the \\'frmwk\\' variable.'\n\t\tif self.frmwk.is_serial_connected():\n\t\t\tvars['conn'] = self.frmwk.serial_connection\n\t\t\tbanner += os.linesep\n\t\t\tbanner += 'The connection instance is in the \\'conn\\' variable.'\n\t\ttry:\n\t\t\timport IPython.terminal.embed\n\t\texcept ImportError:\n\t\t\tpyconsole = code.InteractiveConsole(vars)\n\t\t\tsavestdin = os.dup(sys.stdin.fileno())\n\t\t\tsavestdout = os.dup(sys.stdout.fileno())\n\t\t\tsavestderr = os.dup(sys.stderr.fileno())\n\t\t\ttry:\n\t\t\t\tpyconsole.interact(banner)\n\t\t\texcept SystemExit:\n\t\t\t\tsys.stdin = os.fdopen(savestdin, 'r', 0)\n\t\t\t\tsys.stdout = os.fdopen(savestdout, 'w', 0)\n\t\t\t\tsys.stderr = os.fdopen(savestderr, 'w', 0)\n\t\telse:\n\t\t\tself.print_line(banner)\n\t\t\tpyconsole = IPython.terminal.embed.InteractiveShellEmbed(\n\t\t\t\tipython_dir=os.path.join(self.frmwk.directories.user_data, 'ipython')\n\t\t\t)\n\t\t\tpyconsole.mainloop(vars)", "response": "Start an interactive Python interpreter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreloading a module in to the framework", "response": "def do_reload(self, args):\n\t\t\"\"\"Reload a module in to the framework\"\"\"\n\t\tif args.module is not None:\n\t\t\tif args.module not in self.frmwk.modules:\n\t\t\t\tself.print_error('Invalid Module Selected.')\n\t\t\t\treturn\n\t\t\tmodule = self.frmwk.modules[args.module]\n\t\telif self.frmwk.current_module:\n\t\t\tmodule = self.frmwk.current_module\n\t\telse:\n\t\t\tself.print_error('Must \\'use\\' module first')\n\t\t\treturn\n\t\tself.reload_module(module)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_show(self, args):\n\t\tself.print_line('')\n\t\tif args.thing == 'modules':\n\t\t\tself.print_line('Modules' + os.linesep + '=======')\n\t\t\theaders = ('Name', 'Description')\n\t\t\trows = [(module.path, module.description) for module in self.frmwk.modules.values()]\n\t\telse:\n\t\t\tif self.frmwk.current_module and args.thing == 'options':\n\t\t\t\toptions = self.frmwk.current_module.options\n\t\t\t\tself.print_line('Module Options' + os.linesep + '==============')\n\t\t\tif self.frmwk.current_module and args.thing == 'advanced':\n\t\t\t\toptions = self.frmwk.current_module.advanced_options\n\t\t\t\tself.print_line('Advanced Module Options' + os.linesep + '=======================')\n\t\t\telif self.frmwk.current_module is None and args.thing == 'options':\n\t\t\t\toptions = self.frmwk.options\n\t\t\t\tself.print_line('Framework Options' + os.linesep + '=================')\n\t\t\telif self.frmwk.current_module is None and args.thing == 'advanced':\n\t\t\t\toptions = self.frmwk.advanced_options\n\t\t\t\tself.print_line('Advanced Framework Options' + os.linesep + '==========================')\n\t\t\theaders = ('Name', 'Value', 'Description')\n\t\t\traw_options = [options.get_option(name) for name in options]\n\t\t\trows = [(option.name, str(option.value), option.help) for option in raw_options]\n\t\trows = sorted(rows, key=lambda row: row[0])\n\t\tself.print_line('')\n\t\tself.frmwk.print_table(rows, headers=headers, line_prefix='  ')\n\t\tself.print_line('')\n\t\treturn", "response": "This function is called by the show command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send(self, data):\n\t\tif not isinstance(data, C1218Packet):\n\t\t\tdata = C1218Packet(data)\n\t\tif self.toggle_control:  # bit wise, fuck yeah\n\t\t\tif self._toggle_bit:\n\t\t\t\tdata.set_control(ord(data.control) | 0x20)\n\t\t\t\tself._toggle_bit = False\n\t\t\telif not self._toggle_bit:\n\t\t\t\tif ord(data.control) & 0x20:\n\t\t\t\t\tdata.set_control(ord(data.control) ^ 0x20)\n\t\t\t\tself._toggle_bit = True\n\t\telif self.toggle_control and not isinstance(data, C1218Packet):\n\t\t\tself.loggerio.warning('toggle bit is on but the data is not a C1218Packet instance')\n\t\tdata = data.build()\n\t\tself.loggerio.debug(\"sending frame,  length: {0:<3} data: {1}\".format(len(data), binascii.b2a_hex(data).decode('utf-8')))\n\t\tfor pktcount in range(0, 3):\n\t\t\tself.write(data)\n\t\t\tresponse = self.serial_h.read(1)\n\t\t\tif response == NACK:\n\t\t\t\tself.loggerio.warning('received a NACK after writing data')\n\t\t\t\ttime.sleep(0.10)\n\t\t\telif len(response) == 0:\n\t\t\t\tself.loggerio.error('received empty response after writing data')\n\t\t\t\ttime.sleep(0.10)\n\t\t\telif response != ACK:\n\t\t\t\tself.loggerio.error('received unknown response: ' + hex(ord(response)) + ' after writing data')\n\t\t\telse:\n\t\t\t\treturn\n\t\tself.loggerio.critical('failed 3 times to correctly send a frame')\n\t\traise C1218IOError('failed 3 times to correctly send a frame')", "response": "This function sends a raw C12. 18 frame and waits for an ACK response."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreceives a C1218Packet, the payload data is returned. :param bool full_frame: If set to True, the entire C1218 frame is returned instead of just the payload.", "response": "def recv(self, full_frame=False):\n\t\t\"\"\"\n\t\tReceive a C1218Packet, the payload data is returned.\n\n\t\t:param bool full_frame: If set to True, the entire C1218 frame is\n\t\t  returned instead of just the payload.\n\t\t\"\"\"\n\t\tpayloadbuffer = b''\n\t\ttries = 3\n\t\twhile tries:\n\t\t\ttmpbuffer = self.serial_h.read(1)\n\t\t\tif tmpbuffer != b'\\xee':\n\t\t\t\tself.loggerio.error('did not receive \\\\xee as the first byte of the frame')\n\t\t\t\tself.loggerio.debug('received \\\\x' + binascii.b2a_hex(tmpbuffer).decode('utf-8') + ' instead')\n\t\t\t\ttries -= 1\n\t\t\t\tcontinue\n\t\t\ttmpbuffer += self.serial_h.read(5)\n\t\t\tsequence, length = struct.unpack('>xxxBH', tmpbuffer)\n\t\t\tpayload = self.serial_h.read(length)\n\t\t\ttmpbuffer += payload\n\t\t\tchksum = self.serial_h.read(2)\n\t\t\tif chksum == packet_checksum(tmpbuffer):\n\t\t\t\tself.serial_h.write(ACK)\n\t\t\t\tdata = tmpbuffer + chksum\n\t\t\t\tself.loggerio.debug(\"received frame, length: {0:<3} data: {1}\".format(len(data), binascii.b2a_hex(data).decode('utf-8')))\n\t\t\t\tpayloadbuffer += payload\n\t\t\t\tif sequence == 0:\n\t\t\t\t\tif full_frame:\n\t\t\t\t\t\tpayloadbuffer = data\n\t\t\t\t\tif sys.version_info[0] == 2:\n\t\t\t\t\t\tpayloadbuffer = bytearray(payloadbuffer)\n\t\t\t\t\treturn payloadbuffer\n\t\t\t\telse:\n\t\t\t\t\ttries = 3\n\t\t\telse:\n\t\t\t\tself.serial_h.write(NACK)\n\t\t\t\tself.loggerio.warning('crc does not match on received frame')\n\t\t\t\ttries -= 1\n\t\tself.loggerio.critical('failed 3 times to correctly receive a frame')\n\t\traise C1218IOError('failed 3 times to correctly receive a frame')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, size):\n\t\tdata = self.serial_h.read(size)\n\t\tself.logger.debug('read data, length: ' + str(len(data)) + ' data: ' + binascii.b2a_hex(data).decode('utf-8'))\n\t\tself.serial_h.write(ACK)\n\t\tif sys.version_info[0] == 2:\n\t\t\tdata = bytearray(data)\n\t\treturn data", "response": "This function is noton. This function is noton. This function is noton. This function is noton. This function isnoton. This function isnoton. This function isnoton. This function isnoton."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n\t\tif self._initialized:\n\t\t\tself.stop()\n\t\tself.logged_in = False\n\t\treturn self.serial_h.close()", "response": "Close the connection to the serial device."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start(self):\n\t\tself.serial_h.flushOutput()\n\t\tself.serial_h.flushInput()\n\t\tself.send(C1218IdentRequest())\n\t\tdata = self.recv()\n\t\tif data[0] != 0x00:\n\t\t\tself.logger.error('received incorrect response to identification service request')\n\t\t\treturn False\n\n\t\tself._initialized = True\n\t\tself.send(C1218NegotiateRequest(self.c1218_pktsize, self.c1218_nbrpkts, baudrate=9600))\n\t\tdata = self.recv()\n\t\tif data[0] != 0x00:\n\t\t\tself.logger.error('received incorrect response to negotiate service request')\n\t\t\tself.stop()\n\t\t\traise C1218NegotiateError('received incorrect response to negotiate service request', data[0])\n\t\treturn True", "response": "Send an identity request and then a negotiation request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a terminate request.", "response": "def stop(self, force=False):\n\t\t\"\"\"\n\t\tSend a terminate request.\n\n\t\t:param bool force: ignore the remote devices response\n\t\t\"\"\"\n\t\tif self._initialized:\n\t\t\tself.send(C1218TerminateRequest())\n\t\t\tdata = self.recv()\n\t\t\tif data == b'\\x00' or force:\n\t\t\t\tself._initialized = False\n\t\t\t\tself._toggle_bit = False\n\t\t\t\treturn True\n\t\treturn False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef login(self, username='0000', userid=0, password=None):\n\t\tif password and len(password) > 20:\n\t\t\tself.logger.error('password longer than 20 characters received')\n\t\t\traise Exception('password longer than 20 characters, login failed')\n\n\t\tself.send(C1218LogonRequest(username, userid))\n\t\tdata = self.recv()\n\t\tif data != b'\\x00':\n\t\t\tself.logger.warning('login failed, username and user id rejected')\n\t\t\treturn False\n\n\t\tif password is not None:\n\t\t\tself.send(C1218SecurityRequest(password))\n\t\t\tdata = self.recv()\n\t\t\tif data != b'\\x00':\n\t\t\t\tself.logger.warning('login failed, password rejected')\n\t\t\t\treturn False\n\n\t\tself.logged_in = True\n\t\treturn True", "response": "login into the connected device."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef logoff(self):\n\t\tself.send(C1218LogoffRequest())\n\t\tdata = self.recv()\n\t\tif data == b'\\x00':\n\t\t\tself._initialized = False\n\t\t\treturn True\n\t\treturn False", "response": "Send a logoff request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading data from a table.", "response": "def get_table_data(self, tableid, octetcount=None, offset=None):\n\t\t\"\"\"\n\t\tRead data from a table. If successful, all of the data from the\n\t\trequested table will be returned.\n\n\t\t:param int tableid: The table number to read from (0x0000 <= tableid <= 0xffff)\n\t\t:param int octetcount: Limit the amount of data read, only works if\n\t\t  the meter supports this type of reading.\n\t\t:param int offset: The offset at which to start to read the data from.\n\t\t\"\"\"\n\t\tif self.caching_enabled and tableid in self._cacheable_tables and tableid in self._table_cache.keys():\n\t\t\tself.logger.info('returning cached table #' + str(tableid))\n\t\t\treturn self._table_cache[tableid]\n\t\tself.send(C1218ReadRequest(tableid, offset, octetcount))\n\t\tdata = self.recv()\n\t\tstatus = data[0]\n\t\tif status != 0x00:\n\t\t\tstatus = status\n\t\t\tdetails = (C1218_RESPONSE_CODES.get(status) or 'unknown response code')\n\t\t\tself.logger.error('could not read table id: ' + str(tableid) + ', error: ' + details)\n\t\t\traise C1218ReadTableError('could not read table id: ' + str(tableid) + ', error: ' + details, status)\n\t\tif len(data) < 4:\n\t\t\tif len(data) == 0:\n\t\t\t\tself.logger.error('could not read table id: ' + str(tableid) + ', error: no data was returned')\n\t\t\t\traise C1218ReadTableError('could not read table id: ' + str(tableid) + ', error: no data was returned')\n\t\t\tself.logger.error('could not read table id: ' + str(tableid) + ', error: data read was corrupt, invalid length (less than 4)')\n\t\t\traise C1218ReadTableError('could not read table id: ' + str(tableid) + ', error: data read was corrupt, invalid length (less than 4)')\n\t\tlength = struct.unpack('>H', data[1:3])[0]\n\t\tchksum = data[-1]\n\t\tdata = data[3:-1]\n\t\tif len(data) != length:\n\t\t\tself.logger.error('could not read table id: ' + str(tableid) + ', error: data read was corrupt, invalid length')\n\t\t\traise C1218ReadTableError('could not read table id: ' + str(tableid) + ', error: data read was corrupt, invalid length')\n\t\tif not check_data_checksum(data, chksum):\n\t\t\tself.logger.error('could not read table id: ' + str(tableid) + ', error: data read was corrupt, invalid check sum')\n\t\t\traise C1218ReadTableError('could not read table id: ' + str(tableid) + ', error: data read was corrupt, invalid checksum')\n\n\t\tif self.caching_enabled and tableid in self._cacheable_tables and not tableid in self._table_cache.keys():\n\t\t\tself.logger.info('caching table #' + str(tableid))\n\t\t\tself._table_cache[tableid] = data\n\t\treturn data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites data to a table.", "response": "def set_table_data(self, tableid, data, offset=None):\n\t\t\"\"\"\n\t\tWrite data to a table.\n\n\t\t:param int tableid: The table number to write to (0x0000 <= tableid <= 0xffff)\n\t\t:param str data: The data to write into the table.\n\t\t:param int offset: The offset at which to start to write the data (0x000000 <= octetcount <= 0xffffff).\n\t\t\"\"\"\n\t\tself.send(C1218WriteRequest(tableid, data, offset))\n\t\tdata = self.recv()\n\t\tif data[0] != 0x00:\n\t\t\tstatus = data[0]\n\t\t\tdetails = (C1218_RESPONSE_CODES.get(status) or 'unknown response code')\n\t\t\tself.logger.error('could not write data to the table, error: ' + details)\n\t\t\traise C1218WriteTableError('could not write data to the table, error: ' + details, status)\n\t\treturn"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitiating a C1219 procedure, the request is written to table 7 and the response is read from table 8. :param int process_number: The numeric procedure identifier (0 <= process_number <= 2047). :param bool std_vs_mfg: Whether the procedure is manufacturer specified or not. True is manufacturer specified. :param bytes params: The parameters to pass to the procedure initiation request. :return: A tuple of the result code and the response data. :rtype: tuple", "response": "def run_procedure(self, process_number, std_vs_mfg, params=''):\n\t\t\"\"\"\n\t\tInitiate a C1219 procedure, the request is written to table 7 and\n\t\tthe response is read from table 8.\n\n\t\t:param int process_number: The numeric procedure identifier (0 <= process_number <= 2047).\n\t\t:param bool std_vs_mfg: Whether the procedure is manufacturer specified\n\t\t  or not. True is manufacturer specified.\n\t\t:param bytes params: The parameters to pass to the procedure initiation request.\n\t\t:return: A tuple of the result code and the response data.\n\t\t:rtype: tuple\n\t\t\"\"\"\n\t\tseqnum = random.randint(2, 254)\n\t\tself.logger.info('starting procedure: ' + str(process_number) + ' (' + hex(process_number) + ') sequence number: ' + str(seqnum) + ' (' + hex(seqnum) + ')')\n\t\tprocedure_request = C1219ProcedureInit(self.c1219_endian, process_number, std_vs_mfg, 0, seqnum, params).build()\n\t\tself.set_table_data(7, procedure_request)\n\n\t\tresponse = self.get_table_data(8)\n\t\tif response[:3] == procedure_request[:3]:\n\t\t\treturn response[3], response[4:]\n\t\telse:\n\t\t\tself.logger.error('invalid response from procedure response table (table #8)')\n\t\t\traise C1219ProcedureError('invalid response from procedure response table (table #8)')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new option with a type of String.", "response": "def add_string(self, name, help, required=True, default=None):\n\t\t\"\"\"\n\t\tAdd a new option with a type of String.\n\n\t\t:param str name: The name of the option, how it will be referenced.\n\t\t:param str help: The string returned as help to describe how the option is used.\n\t\t:param bool required: Whether to require that this option be set or not.\n\t\t:param str default: The default value for this option. If required is True and the user must specify it, set to anything but None.\n\t\t\"\"\"\n\t\tself._options[name] = Option(name, 'str', help, required, default=default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new option with a type of a readable file. This is the same as the add_file method except that it will create a new option with a type of a readable file.", "response": "def add_rfile(self, name, help, required=True, default=None):\n\t\t\"\"\"\n\t\tAdd a new option with a type of a readable file. This is the same\n\t\tas the string option with the exception that the default value\n\t\twill have the following variables replaced within it:\n\t\t\t$USER_DATA The path to the users data directory\n\t\t\t$DATA_PATH The path to the framework's data directory\n\t\tThis will NOT check that the file exists or is readable.\n\n\t\t:param str name: The name of the option, how it will be referenced.\n\t\t:param str help: The string returned as help to describe how the option is used.\n\t\t:param bool required: Whether to require that this option be set or not.\n\t\t:param str default: The default value for this option. If required is True and the user must specify it, set to anything but None.\n\t\t\"\"\"\n\t\tif isinstance(default, str):\n\t\t\tdefault = default.replace('$DATA_PATH ', self.directories.data_path + os.path.sep)\n\t\t\tdefault = default.replace('$USER_DATA ', self.directories.user_data + os.path.sep)\n\t\tself._options[name] = Option(name, 'rfile', help, required, default=default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_option_value(self, name, value):\n\t\toption = self.get_option(name)\n\t\told_value = option.value\n\t\tif option.type in ('str', 'rfile'):\n\t\t\toption.value = value\n\t\telif option.type == 'int':\n\t\t\tvalue = value.lower()\n\t\t\tif not value.isdigit():\n\t\t\t\tif value.startswith('0x') and string_is_hex(value[2:]):\n\t\t\t\t\tvalue = int(value[2:], 16)\n\t\t\t\telse:\n\t\t\t\t\traise TypeError('invalid value type')\n\t\t\toption.value = int(value)\n\t\telif option.type == 'flt':\n\t\t\tif value.count('.') > 1:\n\t\t\t\traise TypeError('invalid value type')\n\t\t\tif not value.replace('.', '').isdigit():\n\t\t\t\traise TypeError('invalid value type')\n\t\t\toption.value = float(value)\n\t\telif option.type == 'bool':\n\t\t\tif value.lower() in ['true', '1', 'on']:\n\t\t\t\toption.value = True\n\t\t\telif value.lower() in ['false', '0', 'off']:\n\t\t\t\toption.value = False\n\t\t\telse:\n\t\t\t\traise TypeError('invalid value type')\n\t\telse:\n\t\t\traise Exception('unknown value type')\n\t\tif option.callback and not option.callback(value, old_value):\n\t\t\toption.value = old_value\n\t\t\treturn False\n\t\treturn True", "response": "Set the value of an option."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a list of options that are required but with default values of None.", "response": "def get_missing_options(self):\n\t\t\"\"\"\n\t\tGet a list of options that are required, but with default values\n\t\tof None.\n\t\t\"\"\"\n\t\treturn [option.name for option in self._options.values() if option.required and option.value is None]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_revision():\n\tgit_bin = smoke_zephyr.utilities.which('git')\n\tif not git_bin:\n\t\treturn None\n\tproc_h = subprocess.Popen(\n\t\t(git_bin, 'rev-parse', 'HEAD'),\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tclose_fds=True,\n\t\tcwd=os.path.dirname(os.path.abspath(__file__))\n\t)\n\trev = proc_h.stdout.read().strip()\n\tproc_h.wait()\n\tif not len(rev):\n\t\treturn None\n\treturn rev.decode('utf-8')", "response": "Retrieve the current git revision identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reload_module(self, module_path=None):\n\t\tif module_path is None:\n\t\t\tif self.current_module is not None:\n\t\t\t\tmodule_path = self.current_module.name\n\t\t\telse:\n\t\t\t\tself.logger.warning('must specify module if not module is currently being used')\n\t\t\t\treturn False\n\t\tif module_path not in self.module:\n\t\t\tself.logger.error('invalid module requested for reload')\n\t\t\traise termineter.errors.FrameworkRuntimeError('invalid module requested for reload')\n\n\t\tself.logger.info('reloading module: ' + module_path)\n\t\tmodule_instance = self.import_module(module_path, reload_module=True)\n\t\tif not isinstance(module_instance, termineter.module.TermineterModule):\n\t\t\tself.logger.error('module: ' + module_path + ' is not derived from the TermineterModule class')\n\t\t\traise termineter.errors.FrameworkRuntimeError('module: ' + module_path + ' is not derived from the TermineterModule class')\n\t\tif not hasattr(module_instance, 'run'):\n\t\t\tself.logger.error('module: ' + module_path + ' has no run() method')\n\t\t\traise termineter.errors.FrameworkRuntimeError('module: ' + module_path + ' has no run() method')\n\t\tif not isinstance(module_instance.options, termineter.options.Options) or not isinstance(module_instance.advanced_options, termineter.options.Options):\n\t\t\tself.logger.error('module: ' + module_path + ' options and advanced_options must be termineter.options.Options instances')\n\t\t\traise termineter.errors.FrameworkRuntimeError('options and advanced_options must be termineter.options.Options instances')\n\t\tmodule_instance.name = module_path.split('/')[-1]\n\t\tmodule_instance.path = module_path\n\t\tself.modules[module_path] = module_instance\n\t\tif self.current_module is not None:\n\t\t\tif self.current_module.path == module_instance.path:\n\t\t\t\tself.current_module = module_instance\n\t\treturn True", "response": "Reloads a module into the framework."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef serial_disconnect(self):\n\t\tif self._serial_connected:\n\t\t\ttry:\n\t\t\t\tself.serial_connection.close()\n\t\t\texcept c1218.errors.C1218IOError as error:\n\t\t\t\tself.logger.error('caught C1218IOError: ' + str(error))\n\t\t\texcept serial.serialutil.SerialException as error:\n\t\t\t\tself.logger.error('caught SerialException: ' + str(error))\n\t\t\tself._serial_connected = False\n\t\t\tself.logger.warning('the serial interface has been disconnected')\n\t\treturn True", "response": "Closes the serial connection to the meter and disconnects from the serial interface."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the serial connection from the framework settings and return it setting the framework instance in the process.", "response": "def serial_get(self):\n\t\t\"\"\"\n\t\tCreate the serial connection from the framework settings and return\n\t\tit, setting the framework instance in the process.\n\t\t\"\"\"\n\t\tfrmwk_c1218_settings = {\n\t\t\t'nbrpkts': self.advanced_options['C1218_MAX_PACKETS'],\n\t\t\t'pktsize': self.advanced_options['C1218_PACKET_SIZE']\n\t\t}\n\n\t\tfrmwk_serial_settings = termineter.utilities.get_default_serial_settings()\n\t\tfrmwk_serial_settings['baudrate'] = self.advanced_options['SERIAL_BAUD_RATE']\n\t\tfrmwk_serial_settings['bytesize'] = self.advanced_options['SERIAL_BYTE_SIZE']\n\t\tfrmwk_serial_settings['stopbits'] = self.advanced_options['SERIAL_STOP_BITS']\n\n\t\tself.logger.info('opening serial device: ' + self.options['SERIAL_CONNECTION'])\n\t\ttry:\n\t\t\tself.serial_connection = c1218.connection.Connection(self.options['SERIAL_CONNECTION'], c1218_settings=frmwk_c1218_settings, serial_settings=frmwk_serial_settings, enable_cache=self.advanced_options['CACHE_TABLES'])\n\t\texcept Exception as error:\n\t\t\tself.logger.error('could not open the serial device')\n\t\t\traise error\n\t\treturn self.serial_connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serial_connect(self):\n\t\tself.serial_get()\n\t\ttry:\n\t\t\tself.serial_connection.start()\n\t\texcept c1218.errors.C1218IOError as error:\n\t\t\tself.logger.error('serial connection has been opened but the meter is unresponsive')\n\t\t\traise error\n\t\tself._serial_connected = True\n\t\treturn True", "response": "Connect to the serial device."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to log into the meter over the C12.18 protocol. Returns True on success, False on a failure. This can be called by modules in order to login with a username and password configured within the framework instance.", "response": "def serial_login(self):\n\t\t\"\"\"\n\t\tAttempt to log into the meter over the C12.18 protocol. Returns True on success, False on a failure. This can be\n\t\tcalled by modules in order to login with a username and password configured within the framework instance.\n\t\t\"\"\"\n\t\tif not self._serial_connected:\n\t\t\traise termineter.errors.FrameworkRuntimeError('the serial interface is disconnected')\n\n\t\tusername = self.options['USERNAME']\n\t\tuser_id = self.options['USER_ID']\n\t\tpassword = self.options['PASSWORD']\n\t\tif self.options['PASSWORD_HEX']:\n\t\t\thex_regex = re.compile('^([0-9a-fA-F]{2})+$')\n\t\t\tif hex_regex.match(password) is None:\n\t\t\t\tself.print_error('Invalid characters in password')\n\t\t\t\traise termineter.errors.FrameworkConfigurationError('invalid characters in password')\n\t\t\tpassword = binascii.a2b_hex(password)\n\t\tif len(username) > 10:\n\t\t\tself.print_error('Username cannot be longer than 10 characters')\n\t\t\traise termineter.errors.FrameworkConfigurationError('username cannot be longer than 10 characters')\n\t\tif not (0 <= user_id <= 0xffff):\n\t\t\tself.print_error('User id must be between 0 and 0xffff')\n\t\t\traise termineter.errors.FrameworkConfigurationError('user id must be between 0 and 0xffff')\n\t\tif len(password) > 20:\n\t\t\tself.print_error('Password cannot be longer than 20 characters')\n\t\t\traise termineter.errors.FrameworkConfigurationError('password cannot be longer than 20 characters')\n\n\t\tif not self.serial_connection.login(username, user_id, password):\n\t\t\treturn False\n\t\treturn True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the database seeds.", "response": "def run(self):\n        \"\"\"\n        Run the database seeds.\n        \"\"\"\n        self.factory.register(User, self.users_factory)\n\n        self.factory(User, 50).create()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef app(environ, start_response):\n    from wsgi import container\n\n    \"\"\"Add Environ To Service Container\n    Add the environ to the service container. The environ is generated by the\n    the WSGI server above and used by a service provider to manipulate the\n    incoming requests\n    \"\"\"\n\n    container.bind('Environ', environ)\n\n    \"\"\"Execute All Service Providers That Require The WSGI Server\n    Run all service provider boot methods if the wsgi attribute is true.\n    \"\"\"\n\n    try:\n        for provider in container.make('WSGIProviders'):\n            container.resolve(provider.boot)\n    except Exception as e:\n        container.make('ExceptionHandler').load_exception(e)\n\n    \"\"\"We Are Ready For Launch\n    If we have a solid response and not redirecting then we need to return\n    a 200 status code along with the data. If we don't, then we'll have\n    to return a 302 redirection to where ever the user would like go\n    to next.\n    \"\"\"\n\n    start_response(container.make('Request').get_status_code(),\n                   container.make('Request').get_and_reset_headers())\n\n    \"\"\"Final Step\n    This will take the data variable from the Service Container and return\n    it to the WSGI server.\n    \"\"\"\n\n    return iter([bytes(container.make('Response'), 'utf-8')])", "response": "The WSGI application server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef before(self):\n        user = self.request.user()\n\n        if user and user.verified_at is None:\n            self.request.redirect('/email/verify')", "response": "Run This Middleware Before The Route Executes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef show(self, view: View, request: Request):\n        return view.render('welcome', {\n            'app': request.app().make('Application')\n        })", "response": "Show the welcome page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake a bluetooth address and returns a tuple with the corresponding char values.", "response": "def btaddrtochars(addr):\n    \"\"\"\n    Takes a bluetooth address and returns a tuple with the corresponding\n    char values. This can then be used to construct a\n    IOBluetoothDevice object, providing the signature of the withAddress:\n    selector has been set (as in _setpyobjcsignatures() in this module).\n\n    For example:\n        >>> chars = btaddrtochars(\"00:0e:0a:00:a2:00\")\n        >>> chars\n        (0, 14, 10, 0, 162, 0)\n        >>> device = _IOBluetooth.IOBluetoothDevice.withAddress_(chars)\n        >>> type(device)\n        <objective-c class IOBluetoothDevice at 0xa4024988>\n        >>> device.getAddressString()\n        u'00-0e-0a-00-a2-00'\n    \"\"\"\n    if not _lightbluecommon._isbtaddr(addr):\n        raise TypeError(\"address %s not valid bluetooth address\" % str(addr))\n    if addr.find(\":\") == -1:\n        addr = addr.replace(\"-\", \":\")   # consider alternative addr separator\n\n    # unhexlify gives binary value like '\\x0e', then ord to get the char value.\n    # unhexlify throws TypeError if value is not a hex pair.\n    import binascii\n    chars = [ord(binascii.unhexlify(part)) for part in addr.split(\":\")]\n    return tuple(chars)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwait until conditionfunc returns True or timeout seconds have passed.", "response": "def waituntil(conditionfunc, timeout=None):\n    \"\"\"\n    Waits until conditionfunc() returns true, or <timeout> seconds have passed.\n    (If timeout=None, this waits indefinitely until conditionfunc() returns\n    true.) Returns false if the process timed out, otherwise returns true.\n\n    Note!! You must call interruptwait() when you know that conditionfunc()\n    should be checked (e.g. if you are waiting for data and you know some data\n    has arrived) so that this can check conditionfunc(); otherwise it will just\n    continue to wait. (This allows the function to wait for an event that is\n    sent by interruptwait() instead of polling conditionfunc().)\n\n    This allows the caller to wait while the main event loop processes its\n    events. This must be done for certain situations, e.g. to receive socket\n    data or to accept client connections on a server socket, since IOBluetooth\n    requires the presence of an event loop to run these operations.\n\n    This function doesn't need to be called if there is something else that is\n    already processing the main event loop, e.g. if called from within a Cocoa\n    application.\n    \"\"\"\n    app = NSApplication.sharedApplication()\n    starttime = time.time()\n    if timeout is None:\n        timeout = NSDate.distantFuture().timeIntervalSinceNow()\n    if not isinstance(timeout, (int, float)):\n        raise TypeError(\"timeout must be int or float, was %s\" % \\\n                type(timeout))\n    endtime = starttime + timeout\n    while True:\n        currtime = time.time()\n        if currtime >= endtime:\n            return False\n        # use WAIT_MAX_TIMEOUT, don't wait forever in case of KeyboardInterrupt\n        e = app.nextEventMatchingMask_untilDate_inMode_dequeue_(NSAnyEventMask, NSDate.dateWithTimeIntervalSinceNow_(min(endtime - currtime, WAIT_MAX_TIMEOUT)), NSDefaultRunLoopMode, True)\n        if e is not None:\n            if (e.type() == NSApplicationDefined and e.subtype() == LIGHTBLUE_NOTIFY_ID):\n                if conditionfunc():\n                    return True\n            else:\n                app.postEvent_atStart_(e, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interruptwait():\n    evt = NSEvent.otherEventWithType_location_modifierFlags_timestamp_windowNumber_context_subtype_data1_data2_(NSApplicationDefined, NSPoint(), NSApplicationDefined, 0, 1, None, LIGHTBLUE_NOTIFY_ID, 0, 0)\n    NSApplication.sharedApplication().postEvent_atStart_(evt, True)", "response": "Interrupt the waiting process so that it can check whether it should stop waiting."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the given string is a valid Bluetooth address", "response": "def is_valid_address (s):\n    \"\"\"\n    returns True if address is a valid Bluetooth address\n\n    valid address are always strings of the form XX:XX:XX:XX:XX:XX\n    where X is a hexadecimal character.  For example,\n        01:23:45:67:89:AB is a valid address, but\n        IN:VA:LI:DA:DD:RE is not\n    \"\"\"\n    try:\n        pairs = s.split (\":\")\n        if len (pairs) != 6: return False\n        if not all(0 <= int(b, 16) <= 255 for b in pairs): return False\n    except:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_valid_uuid (uuid):\n    try:\n        if len (uuid) == 4:\n            if int (uuid, 16) < 0: return False\n        elif len (uuid) == 8:\n            if int (uuid, 16) < 0: return False\n        elif len (uuid) == 36:\n            pieces = uuid.split (\"-\")\n            if len (pieces) != 5 or \\\n                    len (pieces[0]) != 8 or \\\n                    len (pieces[1]) != 4 or \\\n                    len (pieces[2]) != 4 or \\\n                    len (pieces[3]) != 4 or \\\n                    len (pieces[4]) != 12:\n                return False\n            [ int (p, 16) for p in pieces ]\n        else:\n            return False\n    except ValueError: \n        return False\n    except TypeError:\n        return False\n    return True", "response": "Checks if a given UUID is a valid 128 - bit UUID."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_full_uuid (uuid):\n    if not is_valid_uuid (uuid): raise ValueError (\"invalid UUID\")\n    if len (uuid) == 4:\n        return \"0000%s-0000-1000-8000-00805F9B34FB\" % uuid\n    elif len (uuid) == 8:\n        return \"%s-0000-1000-8000-00805F9B34FB\" % uuid\n    else:\n        return uuid", "response": "Converts a short 16 - bit or 32 - bit reserved UUID to a full 128 - bit Bluetooth\n    UUID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_l2cap_options (sock):\n    # TODO this should be in the C module, because it depends\n    # directly on struct l2cap_options layout.\n    s = sock.getsockopt (SOL_L2CAP, L2CAP_OPTIONS, 12)\n    options = list( struct.unpack (\"HHHBBBH\", s))\n    return options", "response": "Get the options for the specified L2CAP socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset L2CAP options for the specified L2CAP socket.", "response": "def set_l2cap_options (sock, options):\n    \"\"\"set_l2cap_options (sock, options)\n\n    Sets L2CAP options for the specified L2CAP socket.\n    The option list must be in the same format supplied by\n    get_l2cap_options().\n    \"\"\"\n    # TODO this should be in the C module, because it depends\n    # directly on struct l2cap_options layout.\n    s = struct.pack (\"HHHBBBH\", *options)\n    sock.setsockopt (SOL_L2CAP, L2CAP_OPTIONS, s)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_l2cap_mtu (sock, mtu):\n    options = get_l2cap_options (sock)\n    options[0] = options[1] = mtu\n    set_l2cap_options (sock, options)", "response": "Sets the MTU for the specified L2CAP socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncancel an inquiry in process.", "response": "def cancel_inquiry (self):\n        \"\"\"\n        Call this method to cancel an inquiry in process.  inquiry_complete\n        will still be called.\n        \"\"\"\n        self.names_to_find = {}\n\n        if self.is_inquiring:\n            try:\n                _bt.hci_send_cmd (self.sock, _bt.OGF_LINK_CTL, \\\n                        _bt.OCF_INQUIRY_CANCEL)\n            except _bt.error as e:\n                self.sock.close ()\n                self.sock = None\n                raise BluetoothError (e.args[0],\n                                      \"error canceling inquiry: \" +\n                                      e.args[1])\n            self.is_inquiring = False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall when a bluetooth device is discovered.", "response": "def device_discovered (self, address, device_class, rssi, name):\n        \"\"\"\n        Called when a bluetooth device is discovered.\n\n        address is the bluetooth address of the device\n\n        device_class is the Class of Device, as specified in [1]\n                     passed in as a 3-byte string\n\n        name is the user-friendly name of the device if lookup_names was\n        set when the inquiry was started.  otherwise None\n\n        This method exists to be overriden.\n\n        [1] https://www.bluetooth.org/foundry/assignnumb/document/baseband\n        \"\"\"\n        if name:\n            print((\"found: %s - %s (class 0x%X, rssi %s)\" % \\\n                    (address, name, device_class, rssi)))\n        else:\n            print((\"found: %s (class 0x%X)\" % (address, device_class)))\n            print((\"found: %s (class 0x%X, rssi %s)\" % \\\n                    (address, device_class, rssi)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_inquiry_scan_activity(sock):\n    # save current filter\n    old_filter = sock.getsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, 14)\n\n    # Setup socket filter to receive only events related to the\n    # read_inquiry_mode command\n    flt = bluez.hci_filter_new()\n    opcode = bluez.cmd_opcode_pack(bluez.OGF_HOST_CTL, \n            bluez.OCF_READ_INQ_ACTIVITY)\n    bluez.hci_filter_set_ptype(flt, bluez.HCI_EVENT_PKT)\n    bluez.hci_filter_set_event(flt, bluez.EVT_CMD_COMPLETE);\n    bluez.hci_filter_set_opcode(flt, opcode)\n    sock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, flt )\n\n    # first read the current inquiry mode.\n    bluez.hci_send_cmd(sock, bluez.OGF_HOST_CTL, \n            bluez.OCF_READ_INQ_ACTIVITY )\n\n    pkt = sock.recv(255)\n\n    status,interval,window = struct.unpack(\"!xxxxxxBHH\", pkt)\n    interval = bluez.btohs(interval)\n    interval = (interval >> 8) | ( (interval & 0xFF) << 8 )\n    window = (window >> 8) | ( (window & 0xFF) << 8 )\n    if status != 0: mode = -1\n\n    # restore old filter\n    sock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, old_filter )\n    return interval, window", "response": "reads the current inquiry scan interval and window from the given socket"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_inquiry_scan_activity(sock, interval, window):\n    # save current filter\n    old_filter = sock.getsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, 14)\n\n    # Setup socket filter to receive only events related to the\n    # write_inquiry_mode command\n    flt = bluez.hci_filter_new()\n    opcode = bluez.cmd_opcode_pack(bluez.OGF_HOST_CTL, \n            bluez.OCF_WRITE_INQ_ACTIVITY)\n    bluez.hci_filter_set_ptype(flt, bluez.HCI_EVENT_PKT)\n    bluez.hci_filter_set_event(flt, bluez.EVT_CMD_COMPLETE);\n    bluez.hci_filter_set_opcode(flt, opcode)\n    sock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, flt )\n\n    # send the command!\n    bluez.hci_send_cmd(sock, bluez.OGF_HOST_CTL, \n            bluez.OCF_WRITE_INQ_ACTIVITY, struct.pack(\"HH\", \n                interval, window) )\n\n    pkt = sock.recv(255)\n\n    status = struct.unpack(\"xxxxxxB\", pkt)[0]\n\n    # restore old filter\n    sock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, old_filter )\n    if status != 0: return -1\n    return 0", "response": "write an inquiry scan activity command to the specified socket"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __waituntil(self, stopwaiting, timeoutmsg):\n        if not stopwaiting():\n            if self.__timeout == 0:\n                # in non-blocking mode (immediate timeout)\n                # push event loop to really be sure there is no data available\n                _macutil.looponce()\n                if not stopwaiting():\n                    # trying to perform operation now would block\n                    raise _socket.error(errno.EAGAIN, os.strerror(errno.EAGAIN))\n            else:\n                # block and wait until we get data, or time out\n                if not _macutil.waituntil(stopwaiting, self.__timeout):\n                    raise _socket.timeout(timeoutmsg)", "response": "Waits until stopwaiting returns True or until timeout is reached."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the object with the given callback object.", "response": "def initWithDelegate_(self, cb_obj):\n        \"\"\"\n        Arguments:\n        - cb_obj: An object that receives callbacks when events occur. This\n          object should have:\n            - a method '_handle_channeldata' which takes the related channel (a\n              IOBluetoothRFCOMMChannel or IOBluetoothL2CAPChannel) and the new\n              data (a string) as the arguments.\n            - a method '_handle_channelclosed' which takes the related channel\n              as the argument.\n\n        If this listener's delegate is passed to the openRFCOMMChannel... or\n        openL2CAPChannel... selectors as the delegate, the delegate (and\n        therefore this listener) will automatically start receiving events.\n\n        Otherwise, call setDelegate_() on the channel with this listener's\n        delegate as the argument to allow this listener to start receiving\n        channel events. (This is the only option for server-spawned sockets.)\n        \"\"\"\n        self = super(_ChannelEventListener, self).init()\n        if cb_obj is None:\n            raise TypeError(\"callback object is None\")\n        self.__cb_obj = cb_obj\n        self.__closenotif = None\n        self.__channelDelegate = \\\n                BBBluetoothChannelDelegate.alloc().initWithDelegate_(self)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the object with the given port and protocol.", "response": "def initWithDelegate_port_protocol_(self, cb_obj, port, proto):\n        \"\"\"\n        Arguments:\n        - cb_obj: to receive callbacks when a client connects to\n          to the channel, the callback object should have a method\n          '_handle_channelopened' which takes the newly opened\n          IOBluetoothRFCOMMChannel or IOBluetoothL2CAPChannel as its argument.\n        - port: the channel or PSM that the server is listening on\n        - proto: L2CAP or RFCOMM.\n        \"\"\"\n        self = super(_ChannelServerEventListener, self).init()\n        if cb_obj is None:\n            raise TypeError(\"callback object is None\")\n        self.__cb_obj = cb_obj\n        self.__usernotif = None\n\n        if proto == _lightbluecommon.RFCOMM:\n            usernotif = _IOBluetooth.IOBluetoothRFCOMMChannel.registerForChannelOpenNotifications_selector_withChannelID_direction_(self, \"newChannelOpened:channel:\", port, _macutil.kIOBluetoothUserNotificationChannelDirectionIncoming)\n        elif proto == _lightbluecommon.L2CAP:\n            usernotif = _IOBluetooth.IOBluetoothL2CAPChannel.registerForChannelOpenNotifications_selector_withPSM_direction_(self, \"newChannelOpened:channel:\", port, _macutil.kIOBluetoothUserNotificationChannelDirectionIncoming)\n\n        if usernotif is None:\n            raise _socket.error(\"Unable to register for channel-\" + \\\n                \"opened notifications on server socket on channel/PSM %d\" % \\\n                port)\n        self.__usernotif = usernotif\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef newChannelOpened_channel_(self, notif, newChannel):\n        if newChannel is not None and newChannel.isIncoming():\n            # not sure if delegate really needs to be set\n            newChannel.setDelegate_(self)\n\n            if hasattr(self.__cb_obj, '_handle_channelopened'):\n                self.__cb_obj._handle_channelopened(newChannel)", "response": "Handle when a new channel is opened."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getheader(self, header, default=None):\n        '''\n        Returns the response header value for the given header, which may\n        either be a string (not case-sensitive) or the raw byte\n        value of the header ID.\n\n        Returns the specified default value if the header is not present.\n        '''\n        if isinstance(header, str):\n            return self.headers.get(header.lower(), default)\n        return self.__rawheaders.get(header, default)", "response": "Returns the value of the given header."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_inquiry_mode(sock):\n    # save current filter\n    old_filter = sock.getsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, 14)\n\n    # Setup socket filter to receive only events related to the\n    # read_inquiry_mode command\n    flt = bluez.hci_filter_new()\n    opcode = bluez.cmd_opcode_pack(bluez.OGF_HOST_CTL, \n            bluez.OCF_READ_INQUIRY_MODE)\n    bluez.hci_filter_set_ptype(flt, bluez.HCI_EVENT_PKT)\n    bluez.hci_filter_set_event(flt, bluez.EVT_CMD_COMPLETE);\n    bluez.hci_filter_set_opcode(flt, opcode)\n    sock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, flt )\n\n    # first read the current inquiry mode.\n    bluez.hci_send_cmd(sock, bluez.OGF_HOST_CTL, \n            bluez.OCF_READ_INQUIRY_MODE )\n\n    pkt = sock.recv(255)\n\n    status,mode = struct.unpack(\"xxxxxxBB\", pkt)\n    if status != 0: mode = -1\n\n    # restore old filter\n    sock.setsockopt( bluez.SOL_HCI, bluez.HCI_FILTER, old_filter )\n    return mode", "response": "reads the current inquiry mode from the given socket"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit the given class of device into a 3 - item tuple with the major service class major device class and minor device class values.", "response": "def splitclass(classofdevice):\n    \"\"\"\n    Splits the given class of device to return a 3-item tuple with the\n    major service class, major device class and minor device class values.\n\n    These values indicate the device's major services and the type of the\n    device (e.g. mobile phone, laptop, etc.). If you google for\n    \"assigned numbers bluetooth baseband\" you might find some documents\n    that discuss how to extract this information from the class of device.\n\n    Example:\n        >>> splitclass(1057036)\n        (129, 1, 3)\n        >>>\n    \"\"\"\n    if not isinstance(classofdevice, int):\n        try:\n            classofdevice = int(classofdevice)\n        except (TypeError, ValueError):\n            raise TypeError(\"Given device class '%s' cannot be split\" % \\\n                str(classofdevice))\n\n    data = classofdevice >> 2   # skip over the 2 \"format\" bits\n    service = data >> 11\n    major = (data >> 6) & 0x1F\n    minor = data & 0x3F\n    return (service, major, minor)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _isbtaddr(address):\n    # Define validity regex. Accept either \":\" or \"-\" as separators.\n    global _validbtaddr\n    if _validbtaddr is None:\n        import re\n        _validbtaddr = re.compile(\"((\\d|[a-f]){2}(:|-)){5}(\\d|[a-f]){2}\",\n                re.IGNORECASE)\n    import types\n    if not isinstance(address, str):\n        return False\n    return _validbtaddr.match(address) is not None", "response": "Returns True if the given string is a valid bluetooth address."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch the given device for services with the specified parameters.", "response": "def _searchservices(device, name=None, uuid=None, uuidbad=None):\n    \"\"\"\n    Searches the given IOBluetoothDevice using the specified parameters.\n    Returns an empty list if the device has no services.\n\n    uuid should be IOBluetoothSDPUUID object.\n    \"\"\"\n    if not isinstance(device, _IOBluetooth.IOBluetoothDevice):\n        raise ValueError(\"device must be IOBluetoothDevice, was %s\" % \\\n            type(device))\n\n    services = []\n    allservices = device.getServices()\n    if uuid:\n        gooduuids = (uuid, )\n    else:\n        gooduuids = ()\n    if uuidbad:\n        baduuids = (uuidbad, )\n    else:\n        baduuids = ()\n\n    if allservices is not None:\n        for s in allservices:\n            if gooduuids and not s.hasServiceFromArray_(gooduuids):\n                continue\n            if baduuids and s.hasServiceFromArray_(baduuids):\n                continue\n            if name is None or s.getServiceName() == name:\n                services.append(s)\n    return services"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _getdevicetuple(iobtdevice):\n    addr = _macutil.formatdevaddr(iobtdevice.getAddressString())\n    name = iobtdevice.getName()\n    cod = iobtdevice.getClassOfDevice()\n    return (addr, name, cod)", "response": "Returns an addr name and COD tuple from a IOBluetoothDevice object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a tuple from the given IOBluetoothSDPServiceRecord.", "response": "def _getservicetuple(servicerecord):\n    \"\"\"\n        Returns a (device-addr, service-channel, service-name) tuple from the given\n        IOBluetoothSDPServiceRecord.\n        \"\"\"\n    addr = _macutil.formatdevaddr(servicerecord.getDevice().getAddressString())\n    name = servicerecord.getServiceName()\n    try:\n        result, channel = servicerecord.getRFCOMMChannelID_(None) # pyobjc 2.0\n    except TypeError:\n        result, channel = servicerecord.getRFCOMMChannelID_()\n    if result != _macutil.kIOReturnSuccess:\n        try:\n            result, channel = servicerecord.getL2CAPPSM_(None) # pyobjc 2.0\n        except:\n            result, channel = servicerecord.getL2CAPPSM_()\n        if result != _macutil.kIOReturnSuccess:\n            channel = None\n    return (addr, channel, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset(self):\n        '''\n        Reset PyMC3 model and all tracked distributions and parameters.\n        '''\n        self.model = pm.Model()\n        self.mu = None\n        self.par_groups = {}", "response": "Reset PyMC3 model and all tracked distributions and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_dist(self, spec, label, dist, **kwargs):\n        ''' Build and return a PyMC3 Distribution. '''\n        if isinstance(dist, string_types):\n            if hasattr(pm, dist):\n                dist = getattr(pm, dist)\n            elif dist in self.dists:\n                dist = self.dists[dist]\n            else:\n                raise ValueError(\"The Distribution class '%s' was not \"\n                                 \"found in PyMC3 or the PyMC3BackEnd.\" % dist)\n\n        # Inspect all args in case we have hyperparameters\n        def _expand_args(k, v, label):\n            if isinstance(v, Prior):\n                label = '%s_%s' % (label, k)\n                return self._build_dist(spec, label, v.name, **v.args)\n            return v\n\n        kwargs = {k: _expand_args(k, v, label) for (k, v) in kwargs.items()}\n\n        # Non-centered parameterization for hyperpriors\n        if spec.noncentered and 'sd' in kwargs and 'observed' not in kwargs \\\n                and isinstance(kwargs['sd'], pm.model.TransformedRV):\n            old_sd = kwargs['sd']\n            _offset = pm.Normal(label + '_offset', mu=0, sd=1,\n                                shape=kwargs['shape'])\n            return pm.Deterministic(label, _offset * old_sd)\n\n        return dist(label, **kwargs)", "response": "Build and return a PyMC3 Distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the PyMC3 model from an abstract model specification.", "response": "def build(self, spec, reset=True):\n        '''\n        Compile the PyMC3 model from an abstract model specification.\n        Args:\n            spec (Model): A bambi Model instance containing the abstract\n                specification of the model to compile.\n            reset (bool): if True (default), resets the PyMC3BackEnd instance\n                before compiling.\n        '''\n        if reset:\n            self.reset()\n\n        with self.model:\n\n            self.mu = 0.\n\n            for t in spec.terms.values():\n\n                data = t.data\n                label = t.name\n                dist_name = t.prior.name\n                dist_args = t.prior.args\n\n                n_cols = t.data.shape[1]\n\n                coef = self._build_dist(spec, label, dist_name,\n                                        shape=n_cols, **dist_args)\n\n                if t.random:\n                    self.mu += coef[t.group_index][:, None] * t.predictor\n                else:\n                    self.mu += pm.math.dot(data, coef)[:, None]\n\n            y = spec.y.data\n            y_prior = spec.family.prior\n            link_f = spec.family.link\n            if isinstance(link_f, string_types):\n                link_f = self.links[link_f]\n            else:\n                link_f = link_f\n            y_prior.args[spec.family.parent] = link_f(self.mu)\n            y_prior.args['observed'] = y\n            y_like = self._build_dist(spec, spec.y.name, y_prior.name,\n                                      **y_prior.args)\n\n            self.spec = spec"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the PyMC3 MCMC sampler.", "response": "def run(self, start=None, method='mcmc', init='auto', n_init=50000, **kwargs):\n        '''\n        Run the PyMC3 MCMC sampler.\n        Args:\n            start: Starting parameter values to pass to sampler; see\n                pm.sample() documentation for details.\n            method: The method to use for fitting the model. By default,\n                'mcmc', in which case the PyMC3 sampler will be used.\n                Alternatively, 'advi', in which case the model will be fitted\n                using  automatic differentiation variational inference as\n                implemented in PyMC3.\n            init: Initialization method (see PyMC3 sampler documentation). Currently, this is\n                 `'jitter+adapt_diag'`, but this can change in the future.\n            n_init: Number of initialization iterations if init = 'advi' or\n                'nuts'. Default is kind of in PyMC3 for the kinds of models\n                we expect to see run with bambi, so we lower it considerably.\n            kwargs (dict): Optional keyword arguments passed onto the sampler.\n        Returns: A PyMC3ModelResults instance.\n        '''\n\n        if method == 'mcmc':\n            samples = kwargs.pop('samples', 1000)\n            cores = kwargs.pop('chains', 1)\n            with self.model:\n                self.trace = pm.sample(samples, start=start, init=init,\n                                       n_init=n_init, cores=cores, **kwargs)\n            return self._convert_to_results()\n\n        elif method == 'advi':\n            with self.model:\n                self.advi_params = pm.variational.advi(start, **kwargs)\n            return PyMC3ADVIResults(self.spec, self.advi_params,\n                                    transformed_vars=self._get_transformed_vars())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename_columns(columns, name_lists):\n    ''' Renames numerical indices in column names returned by patsy dmatrix /\n    dmatrices calls based on the corresponding string levels.\n    Args:\n        columns (list): List of cols from dmatrix's .design_info.column_names\n        name_lists (list): List of lists, where the i'th list gives the set of\n            level names for the i'th index that needs to be replaced.\n    Example:\n        ts = ['threecats[1]:subjects[4]', 'threecats[0]:subjects[3]']\n        rename_columns(ts, (['a', 'b', 'c'], ['d1', 'd2', 'd3', 'd4', 'd5']))\n        # Returns ['threecats[b]:subjects[d5]', 'threecats[a]:subjects[d4]']\n    '''\n    def _replace(c, args):\n        grps = re.findall('([^\\]]*)(\\[(\\d+)\\])', c)\n        for i, (prefix, box, ind) in enumerate(grps):\n            c = c.replace(prefix + box, prefix + '[%s]' % args[i][int(ind)])\n        return c\n    return [_replace(c, name_lists) for c in columns]", "response": "Renames numerical indices in column names returned by patsy dmatrix calls based on corresponding string levels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot(self, varnames=None, ranefs=True, transformed=False,\n             combined=False, hist=False, bins=20, kind='trace'):\n        '''Plots posterior distributions and sample traces.\n\n        Basically a wrapperfor pm.traceplot() plus some niceties, based partly on code\n        from: https://pymc-devs.github.io/pymc3/notebooks/GLM-model-selection.html.\n\n        Args:\n            varnames (list): List of variable names to plot. If None, all\n                eligible variables are plotted.\n            ranefs (bool): If True (default), shows trace plots for\n                individual random effects.\n            transformed (bool): If False (default), excludes internally\n                transformed variables from plotting.\n            combined (bool): If True, concatenates all chains into one before\n                plotting. If False (default), plots separately lines for\n                each chain (on the same axes).\n            hist (bool): If True, plots a histogram for each fixed effect,\n                in addition to the kde plot. To prevent visual clutter,\n                histograms are never plotted for random effects.\n            bins (int): If hist is True, the number of bins in the histogram.\n                Ignored if hist is False.\n            kind (str): Either 'trace' (default) or 'priors'. If 'priors',\n                this just internally calls Model.plot()\n        '''\n\n        def _plot_row(data, row, title, hist=True):\n            # density plot\n            axes[row, 0].set_title(title)\n            if pma is not None:\n                arr = np.atleast_2d(data.values.T).T\n                pma.kdeplot_op(axes[row, 0], arr, bw = 4.5)\n            else:\n                data.plot(kind='kde', ax=axes[row, 0], legend=False)\n\n            # trace plot\n            axes[row, 1].set_title(title)\n            data.plot(kind='line', ax=axes[row, 1], legend=False)\n            # histogram\n            if hist:\n                if pma is not None:\n                    pma.histplot_op(axes[row, 0], arr)\n                else:\n                    data.plot(kind='hist', ax=axes[row, 0], legend=False,\n                              normed=True, bins=bins)\n\n        if kind == 'priors':\n            return self.model.plot(varnames)\n\n        # count the total number of rows in the plot\n        names = self._filter_names(varnames, ranefs, transformed)\n        random = [re.sub(r'_offset$', '', x) in self.model.random_terms\n                  for x in names]\n        rows = sum([len(self.level_dict[p]) if not r else 1\n                    for p, r in zip(names, random)])\n\n        # make the plot!\n        fig, axes = plt.subplots(rows, 2, figsize=(12, 2*rows))\n        if rows == 1:\n            axes = np.array([axes])  # For consistent 2D indexing\n\n        _select_args = {'ranefs': ranefs, 'transformed': transformed}\n\n        # Create list of chains (for combined, just one list w/ all chains)\n        chains = list(range(self.n_chains))\n        if combined:\n            chains = [chains]\n\n        for c in chains:\n\n            row = 0\n\n            for p in names:\n\n                df = self[p].to_df(chains=c, **_select_args)\n                # if p == 'floor|county_sd':\n\n                # fixed effects\n                if re.sub(r'_offset$', '', p) not in self.model.random_terms:\n                    for lev in self.level_dict[p]:\n                        lev_df = df[lev]\n                        _plot_row(lev_df, row, lev, hist)\n                        row += 1\n\n                # random effects\n                else:\n                    _plot_row(df, row, p, hist=False)  # too much clutter\n                    row += 1\n\n        fig.tight_layout()\n\n        # For bernoulli models, tell user which event is being modeled\n        if self.model.family.name == 'bernoulli':\n            event = next(i for i, x in enumerate(self.model.y.data.flatten())\n                         if x > .99)\n            warnings.warn('Modeling the probability that {}==\\'{}\\''.format(\n                self.model.y.name,\n                str(self.model.clean_data[self.model.y.name][event])))\n\n        return axes", "response": "Plots posterior distributions and sample traces for the specified variable names."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the HPD interval for the given interval.", "response": "def _hpd_interval(self, x, width):\n        \"\"\"\n        Code adapted from pymc3.stats.calc_min_interval:\n        https://github.com/pymc-devs/pymc3/blob/master/pymc3/stats.py\n        \"\"\"\n\n        x = np.sort(x)\n        n = len(x)\n\n        interval_idx_inc = int(np.floor(width * n))\n        n_intervals = n - interval_idx_inc\n        interval_width = x[interval_idx_inc:] - x[:n_intervals]\n\n        if len(interval_width) == 0:\n            raise ValueError('Too few elements for interval calculation')\n\n        min_idx = np.argmin(interval_width)\n        hdi_min = x[min_idx]\n        hdi_max = x[min_idx + interval_idx_inc]\n\n        index = ['hpd{}_{}'.format(width, x) for x in ['lower', 'upper']]\n        return pd.Series([hdi_min, hdi_max], index=index)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a DataFrame of summary statistics for the parameters.", "response": "def summary(self, varnames=None, ranefs=False, transformed=False, hpd=.95,\n                quantiles=None, diagnostics=['effective_n', 'gelman_rubin']):\n        '''Returns a DataFrame of summary/diagnostic statistics for the parameters.\n\n        Args:\n            varnames (list): List of variable names to include; if None\n                (default), all eligible variables are included.\n            ranefs (bool): Whether or not to include random effects in the\n                summary. Default is False.\n            transformed (bool): Whether or not to include internally\n                transformed variables in the summary. Default is False.\n            hpd (float, between 0 and 1): Show Highest Posterior Density (HPD)\n                intervals with specified width/proportion for all parameters.\n                If None, HPD intervals are suppressed.\n            quantiles (float, list): Show\n                specified quantiles of the marginal posterior distributions for\n                all parameters. If list, must be a list of floats between 0 and 1. If\n                None (default), no quantiles are shown.\n            diagnostics (list): List of functions to use to compute convergence\n                diagnostics for all parameters. Each element can be either a\n                callable or a string giving the name of a function in the\n                diagnostics module. Valid strings are 'gelman_rubin' and\n                'effective_n'. Functions must accept a MCMCResults object as\n                the sole input, and return a DataFrame with one labeled row per\n                parameter. If None, no convergence diagnostics are computed.\n        '''\n\n        samples = self.to_df(varnames, ranefs, transformed)\n\n        # build the basic DataFrame\n        df = pd.DataFrame({'mean': samples.mean(0), 'sd': samples.std(0)})\n\n        # add user-specified quantiles\n        if quantiles is not None:\n            if not isinstance(quantiles, (list, tuple)):\n                quantiles = [quantiles]\n            qnames = ['q' + str(q) for q in quantiles]\n            df = df.merge(samples.quantile(quantiles).set_index([qnames]).T,\n                          left_index=True, right_index=True)\n\n        # add HPD intervals\n        if hpd is not None:\n            df = df.merge(samples.apply(self._hpd_interval, axis=0,\n                          width=hpd).T, left_index=True, right_index=True)\n\n        # add convergence diagnostics\n        if diagnostics is not None:\n            _names = self._filter_names(ranefs=ranefs, transformed=transformed)\n            _self = self[_names]\n            if self.n_chains > 1:\n                for diag in diagnostics:\n                    if isinstance(diag, string_types):\n                        diag = getattr(bmd, diag)\n                    df = df.merge(diag(_self), left_index=True,\n                                  right_index=True)\n            else:\n                warnings.warn('Multiple MCMC chains are required in order '\n                              'to compute convergence diagnostics.')\n\n        # For bernoulli models, tell user which event is being modeled\n        if self.model.family.name == 'bernoulli':\n            event = next(i for i, x in enumerate(self.model.y.data.flatten())\n                         if x > .99)\n            warnings.warn('Modeling the probability that {}==\\'{}\\''.format(\n                self.model.y.name,\n                str(self.model.clean_data[self.model.y.name][event])))\n\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the MCMC samples in a nice neat pandas DataFrame with all MCMC samples in a nice neat pandas DataFrame with all eligible variables included.", "response": "def to_df(self, varnames=None, ranefs=False, transformed=False,\n              chains=None):\n        '''\n        Returns the MCMC samples in a nice, neat pandas DataFrame with all MCMC chains\n        concatenated.\n\n        Args:\n            varnames (list): List of variable names to include; if None\n                (default), all eligible variables are included.\n            ranefs (bool): Whether or not to include random effects in the\n                returned DataFrame. Default is True.\n            transformed (bool): Whether or not to include internally\n                transformed variables in the result. Default is False.\n            chains (int, list): Index, or list of indexes, of chains to\n                concatenate. E.g., [1, 3] would concatenate the first and\n                third chains, and ignore any others. If None (default),\n                concatenates all available chains.\n        '''\n\n        # filter out unwanted variables\n        names = self._filter_names(varnames, ranefs, transformed)\n\n        # concatenate the (pre-sliced) chains\n        if chains is None:\n            chains = list(range(self.n_chains))\n        chains = listify(chains)\n        data = [self.data[:, i, :] for i in chains]\n        data = np.concatenate(data, axis=0)\n\n        # construct the trace DataFrame\n        df = sum([self.level_dict[x] for x in names], [])\n        df = pd.DataFrame({x: data[:, self.levels.index(x)] for x in df})\n\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef autocorr(x):\n\n    y = x - x.mean()\n    n = len(y)\n    result = fftconvolve(y, y[::-1])\n    acorr = result[len(result) // 2:]\n    acorr /= np.arange(n, 0, -1)\n    acorr /= acorr[0]\n    return acorr", "response": "Compute autocorrelation using FFT for every lag for the input array\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef autocov(x):\n\n    acorr = autocorr(x)\n    varx = np.var(x, ddof=1) * (len(x) - 1) / len(x)\n    acov = acorr * varx\n    return acov", "response": "Compute autocovariance estimates for every lag for the input array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gelman_rubin(mcmc):\n\n    if mcmc.n_chains < 2:\n        raise ValueError(\n            'Gelman-Rubin diagnostic requires multiple chains '\n            'of the same length.')\n\n    # get Vhat and within-chain variance\n    Vhat, W = _vhat_w(mcmc)\n\n    # compute and return Gelman-Rubin statistic\n    Rhat = np.sqrt(Vhat / W)\n    return pd.DataFrame({'gelman_rubin': Rhat}, index=mcmc.levels)", "response": "Compute Gelman - Rubin diagnostic for a single entry in the MCMC."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the effective sample size for a MCMC result set.", "response": "def effective_n(mcmc):\n    \"\"\"\n    Args:\n        mcmc (MCMCResults): Pre-sliced MCMC samples to compute diagnostics for.\n    \"\"\"\n\n    if mcmc.n_chains < 2:\n        raise ValueError(\n            'Calculation of effective sample size requires multiple chains '\n            'of the same length.')\n\n    def get_neff(x):\n        \"\"\"Compute the effective sample size for a 2D array.\"\"\"\n        trace_value = x.T\n        nchain, n_samples = trace_value.shape\n\n        acov = np.asarray([autocov(trace_value[chain]) for chain in range(nchain)])\n\n        chain_mean = trace_value.mean(axis=1)\n        chain_var = acov[:, 0] * n_samples / (n_samples - 1.)\n        acov_t = acov[:, 1] * n_samples / (n_samples - 1.)\n        mean_var = np.mean(chain_var)\n        var_plus = mean_var * (n_samples - 1.) / n_samples\n        var_plus += np.var(chain_mean, ddof=1)\n\n        rho_hat_t = np.zeros(n_samples)\n        rho_hat_even = 1.\n        rho_hat_t[0] = rho_hat_even\n        rho_hat_odd = 1. - (mean_var - np.mean(acov_t)) / var_plus\n        rho_hat_t[1] = rho_hat_odd\n        # Geyer's initial positive sequence\n        max_t = 1\n        t = 1\n        while t < (n_samples - 2) and (rho_hat_even + rho_hat_odd) >= 0.:\n            rho_hat_even = 1. - (mean_var - np.mean(acov[:, t + 1])) / var_plus\n            rho_hat_odd = 1. - (mean_var - np.mean(acov[:, t + 2])) / var_plus\n            if (rho_hat_even + rho_hat_odd) >= 0:\n                rho_hat_t[t + 1] = rho_hat_even\n                rho_hat_t[t + 2] = rho_hat_odd\n            max_t = t + 2\n            t += 2\n\n        # Geyer's initial monotone sequence\n        t = 3\n        while t <= max_t - 2:\n            if (rho_hat_t[t + 1] + rho_hat_t[t + 2]) > (rho_hat_t[t - 1] + rho_hat_t[t]):\n                rho_hat_t[t + 1] = (rho_hat_t[t - 1] + rho_hat_t[t]) / 2.\n                rho_hat_t[t + 2] = rho_hat_t[t + 1]\n            t += 2\n        ess = nchain * n_samples\n        ess = ess / (-1. + 2. * np.sum(rho_hat_t))\n        return ess\n\n    nvar = mcmc.data.shape[-1]\n    n_eff = [get_neff(mcmc.data[:, :, i]) for i in range(nvar)]\n\n    return pd.DataFrame({'effective_n': n_eff}, index=mcmc.levels)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset the list of terms and y - variable.", "response": "def reset(self):\n        '''\n        Reset list of terms and y-variable.\n        '''\n        self.terms = OrderedDict()\n        self.y = None\n        self.backend = None\n        self.added_terms = []\n        self._added_priors = {}\n        self.completes = []\n        self.clean_data = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the model for sampling and fitting.", "response": "def build(self, backend=None):\n        '''Set up the model for sampling/fitting.\n\n        Performs any steps that require access to all model terms (e.g., scaling priors\n        on each term), then calls the BackEnd's build() method.\n\n        Args:\n            backend (str): The name of the backend to use for model fitting.\n                Currently, 'pymc' and 'stan' are supported. If None, assume\n                that fit() has already been called (possibly without building),\n                and look in self._backend_name.\n        '''\n\n        # retain only the complete cases\n        n_total = len(self.data.index)\n        if len(self.completes):\n            completes = [set(x) for x in sum(self.completes, [])]\n            completes = set.intersection(*completes)\n        else:\n            completes = [x for x in range(len(self.data.index))]\n        self.clean_data = self.data.iloc[list(completes), :]\n        # warn the user about any dropped rows\n        if len(completes) < n_total:\n            msg = \"Automatically removing {}/{} rows from the dataset.\"\n            msg = msg.format(n_total - len(completes), n_total)\n            warnings.warn(msg)\n\n        # loop over the added terms and actually _add() them\n        for term_args in self.added_terms:\n            self._add(**term_args)\n\n        # set custom priors\n        self._set_priors(**self._added_priors)\n\n        # prepare all priors\n        for name, term in self.terms.items():\n            type_ = 'intercept' if name == 'Intercept' else \\\n                'random' if self.terms[name].random else 'fixed'\n            term.prior = self._prepare_prior(term.prior, type_)\n\n        # check for backend\n        if backend is None:\n            if self._backend_name is None:\n                raise ValueError(\"Error: no backend was passed or set in the \"\n                                 \"Model; did you forget to call fit()?\")\n            backend = self._backend_name\n\n        # check for outcome\n        if self.y is None:\n            raise ValueError(\"No outcome (y) variable is set! Please specify \"\n                             \"an outcome variable using the formula interface \"\n                             \"before build() or fit().\")\n\n        # X = fixed effects design matrix (excluding intercept/constant term)\n        # r2_x = 1 - 1/VIF, i.e., R2 for predicting each x from all other x's.\n        # only compute these stats if there are multiple terms in the model\n        terms = [t for t in self.fixed_terms.values() if t.name != 'Intercept']\n\n        if len(self.fixed_terms) > 1:\n\n            X = [pd.DataFrame(x.data, columns=x.levels) for x in terms]\n            X = pd.concat(X, axis=1)\n\n            self.dm_statistics = {\n                'r2_x': pd.Series({\n                    x: sm.OLS(endog=X[x],\n                              exog=sm.add_constant(X.drop(x, axis=1))\n                              if 'Intercept' in self.term_names\n                              else X.drop(x, axis=1)).fit().rsquared\n                    for x in list(X.columns)}),\n                'sd_x': X.std(),\n                'mean_x': X.mean(axis=0)\n            }\n\n            # save potentially useful info for diagnostics, send to\n            # ModelResults.\n            # mat = correlation matrix of X, w/ diagonal replaced by X means\n            mat = X.corr()\n            for x in list(mat.columns):\n                mat.loc[x, x] = self.dm_statistics['mean_x'][x]\n            self._diagnostics = {\n                # the Variance Inflation Factors (VIF), which is possibly\n                # useful for diagnostics\n                'VIF': 1/(1 - self.dm_statistics['r2_x']),\n                'corr_mean_X': mat\n            }\n\n            # throw informative error if perfect collinearity among fixed fx\n            if any(self.dm_statistics['r2_x'] > .999):\n                raise ValueError(\n                    \"There is perfect collinearity among the fixed effects!\\n\"\n                    \"Printing some design matrix statistics:\\n\" +\n                    str(self.dm_statistics) + '\\n' +\n                    str(self._diagnostics))\n\n        # throw informative error message if any categorical predictors have 1\n        # category\n        num_cats = [x.data.size for x in self.fixed_terms.values()]\n        if any(np.array(num_cats) == 0):\n            raise ValueError(\n                \"At least one categorical predictor contains only 1 category!\")\n\n        # only set priors if there is at least one term in the model\n        if len(self.terms) > 0:\n            # Get and scale default priors if none are defined yet\n            if self.taylor is not None:\n                taylor = self.taylor\n            else:\n                taylor = 5 if self.family.name == 'gaussian' else 1\n            scaler = PriorScaler(self, taylor=taylor)\n            scaler.scale()\n\n        # For bernoulli models with n_trials = 1 (most common use case),\n        # tell user which event is being modeled\n        if self.family.name == 'bernoulli' and np.max(self.y.data) < 1.01:\n            event = next(\n                i for i, x in enumerate(self.y.data.flatten()) if x > .99)\n            warnings.warn('Modeling the probability that {}==\\'{}\\''.format(\n                self.y.name, str(self.clean_data[self.y.name].iloc[event])))\n\n        self._set_backend(backend)\n        self.backend.build(self)\n        self.built = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfits the model using the specified BackEnd.", "response": "def fit(self, fixed=None, random=None, priors=None, family='gaussian',\n            link=None, run=True, categorical=None, backend=None, **kwargs):\n        '''Fit the model using the specified BackEnd.\n\n        Args:\n            fixed (str): Optional formula specification of fixed effects.\n            random (list): Optional list-based specification of random effects.\n            priors (dict): Optional specification of priors for one or more\n                terms. A dict where the keys are the names of terms in the\n                model, and the values are either instances of class Prior or\n                ints, floats, or strings that specify the width of the priors\n                on a standardized scale.\n            family (str, Family): A specification of the model family\n                (analogous to the family object in R). Either a string, or an\n                instance of class priors.Family. If a string is passed, a\n                family with the corresponding name must be defined in the\n                defaults loaded at Model initialization. Valid pre-defined\n                families are 'gaussian', 'bernoulli', 'poisson', and 't'.\n            link (str): The model link function to use. Can be either a string\n                (must be one of the options defined in the current backend;\n                typically this will include at least 'identity', 'logit',\n                'inverse', and 'log'), or a callable that takes a 1D ndarray\n                or theano tensor as the sole argument and returns one with\n                the same shape.\n            run (bool): Whether or not to immediately begin fitting the model\n                once any set up of passed arguments is complete.\n            categorical (str, list): The names of any variables to treat as\n                categorical. Can be either a single variable name, or a list\n                of names. If categorical is None, the data type of the columns\n                in the DataFrame will be used to infer handling. In cases where\n                numeric columns are to be treated as categoricals (e.g., random\n                factors coded as numerical IDs), explicitly passing variable\n                names via this argument is recommended.\n            backend (str): The name of the BackEnd to use. Currently only\n                'pymc' and 'stan' backends are supported. Defaults to PyMC3.\n        '''\n\n        if fixed is not None or random is not None:\n            self.add(fixed=fixed, random=random, priors=priors, family=family,\n                     link=link, categorical=categorical, append=False)\n\n        ''' Run the BackEnd to fit the model. '''\n        if backend is None:\n            backend = 'pymc' if self._backend_name is None else self._backend_name\n\n        if run:\n            if not self.built or backend != self._backend_name:\n                self.build(backend)\n            return self.backend.run(**kwargs)\n\n        self._backend_name = backend"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd one or more terms to the model.", "response": "def add(self, fixed=None, random=None, priors=None, family='gaussian',\n            link=None, categorical=None, append=True):\n        '''Adds one or more terms to the model via an R-like formula syntax.\n\n        Args:\n            fixed (str): Optional formula specification of fixed effects.\n            random (list): Optional list-based specification of random effects.\n            priors (dict): Optional specification of priors for one or more\n                terms. A dict where the keys are the names of terms in the\n                model, and the values are either instances of class Prior or\n                ints, floats, or strings that specify the width of the priors\n                on a standardized scale.\n            family (str, Family): A specification of the model family\n                (analogous to the family object in R). Either a string, or an\n                instance of class priors.Family. If a string is passed, a\n                family with the corresponding name must be defined in the\n                defaults loaded at Model initialization. Valid pre-defined\n                families are 'gaussian', 'bernoulli', 'poisson', and 't'.\n            link (str): The model link function to use. Can be either a string\n                (must be one of the options defined in the current backend;\n                typically this will include at least 'identity', 'logit',\n                'inverse', and 'log'), or a callable that takes a 1D ndarray\n                or theano tensor as the sole argument and returns one with\n                the same shape.\n            categorical (str, list): The names of any variables to treat as\n                categorical. Can be either a single variable name, or a list\n                of names. If categorical is None, the data type of the columns\n                in the DataFrame will be used to infer handling. In cases where\n                numeric columns are to be treated as categoricals (e.g., random\n                factors coded as numerical IDs), explicitly passing variable\n                names via this argument is recommended.\n            append (bool): If True, terms are appended to the existing model\n                rather than replacing any existing terms. This allows\n                formula-based specification of the model in stages.\n        '''\n\n        data = self.data\n\n        # Primitive values (floats, strs) can be overwritten with Prior objects\n        # so we need to make sure to copy first to avoid bad things happening\n        # if user is re-using same prior dict in multiple models.\n        if priors is None:\n            priors = {}\n        else:\n            priors = deepcopy(priors)\n\n        if not append:\n            self.reset()\n\n        # Explicitly convert columns to category if desired--though this\n        # can also be done within the formula using C().\n        if categorical is not None:\n            data = data.copy()\n            cats = listify(categorical)\n            data[cats] = data[cats].apply(lambda x: x.astype('category'))\n\n        # Custom patsy.missing.NAAction class. Similar to patsy drop/raise\n        # defaults, but changes the raised message and logs any dropped rows\n        NA_handler = Custom_NA(dropna=self.dropna)\n\n        # screen fixed terms\n        if fixed is not None:\n            if '~' in fixed:\n                clean_fix = re.sub(r'\\[.+\\]', '', fixed)\n                dmatrices(clean_fix, data=data, NA_action=NA_handler)\n            else:\n                dmatrix(fixed, data=data, NA_action=NA_handler)\n\n        # screen random terms\n        if random is not None:\n            for term in listify(random):\n                for side in term.split('|'):\n                    dmatrix(side, data=data, NA_action=NA_handler)\n\n        # update the running list of complete cases\n        if len(NA_handler.completes):\n            self.completes.append(NA_handler.completes)\n\n        # save arguments to pass to _add()\n        args = dict(zip(\n            ['fixed', 'random', 'priors', 'family', 'link', 'categorical'],\n            [fixed, random, priors, family, link, categorical]))\n        self.added_terms.append(args)\n\n        self.built = False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add(self, fixed=None, random=None, priors=None, family='gaussian',\n             link=None, categorical=None, append=True):\n        '''Internal version of add(), with the same arguments.\n\n        Runs during Model.build()\n        '''\n\n        # use cleaned data with NAs removed (if user requested)\n        data = self.clean_data\n        # alter this pandas flag to avoid false positive SettingWithCopyWarnings\n        data._is_copy = False\n\n        # Explicitly convert columns to category if desired--though this\n        # can also be done within the formula using C().\n        if categorical is not None:\n            data = data.copy()\n            cats = listify(categorical)\n            data[cats] = data[cats].apply(lambda x: x.astype('category'))\n\n        if fixed is not None:\n            if '~' in fixed:\n                # check to see if formula is using the 'y[event] ~ x' syntax\n                # (for bernoulli models). If so, chop it into groups:\n                # 1 = 'y[event]', 2 = 'y', 3 = 'event', 4 = 'x'\n                # If this syntax is not being used, event = None\n                event = re.match(r'^((\\S+)\\[(\\S+)\\])\\s*~(.*)$', fixed)\n                if event is not None:\n                    fixed = '{}~{}'.format(event.group(2), event.group(4))\n                y, X = dmatrices(fixed, data=data, NA_action='raise')\n                y_label = y.design_info.term_names[0]\n                if event is not None:\n                    # pass in new Y data that has 1 if y=event and 0 otherwise\n                    y_data = y[:, y.design_info.column_names.index(event.group(1))]\n                    y_data = pd.DataFrame({event.group(3): y_data})\n                    self._add_y(y_label, family=family, link=link, data=y_data)\n                else:\n                    # use Y as-is\n                    self._add_y(y_label, family=family, link=link)\n            else:\n                X = dmatrix(fixed, data=data, NA_action='raise')\n\n            # Loop over predictor terms\n            for _name, _slice in X.design_info.term_name_slices.items():\n                cols = X.design_info.column_names[_slice]\n                term_data = pd.DataFrame(X[:, _slice], columns=cols)\n                prior = priors.pop(_name, priors.get('fixed', None))\n                self.terms[_name] = Term(_name, term_data, prior=prior)\n\n        # Random effects\n        if random is not None:\n\n            random = listify(random)\n\n            for f in random:\n\n                f = f.strip()\n\n                # Split specification into intercept, predictor, and grouper\n                patt = r'^([01]+)*[\\s\\+]*([^\\|]+)*\\|(.*)'\n\n                intcpt, pred, grpr = re.search(patt, f).groups()\n                label = '{}|{}'.format(pred, grpr) if pred else grpr\n                prior = priors.pop(label, priors.get('random', None))\n\n                # Treat all grouping variables as categoricals, regardless of\n                # their dtype and what the user may have specified in the\n                # 'categorical' argument.\n                var_names = re.findall('(\\w+)', grpr)\n                for v in var_names:\n                    if v in data.columns:\n                        data.loc[:, v] = data.loc[:, v].astype('category')\n                        self.clean_data.loc[:, v] = data.loc[:, v]\n\n                # Default to including random intercepts\n                intcpt = 1 if intcpt is None else int(intcpt)\n\n                grpr_df = dmatrix('0+%s' % grpr, data, return_type='dataframe',\n                                  NA_action='raise')\n\n                # If there's no predictor, we must be adding random intercepts\n                if not pred and grpr not in self.terms:\n                    name = '1|' + grpr\n                    pred = np.ones((len(grpr_df), 1))\n                    term = RandomTerm(name, grpr_df, pred, grpr_df.values,\n                                      categorical=True, prior=prior)\n                    self.terms[name] = term\n                else:\n                    pred_df = dmatrix('%s+%s' % (intcpt, pred), data,\n                                      return_type='dataframe',\n                                      NA_action='raise')\n                    # determine value of the 'constant' attribute\n                    const = np.atleast_2d(pred_df.T).T.sum(1).var() == 0\n\n                    for col, i in pred_df.design_info.column_name_indexes.items():\n                        pred_data = pred_df.iloc[:, i]\n                        lev_data = grpr_df.multiply(pred_data, axis=0)\n\n                        # Also rename intercepts and skip if already added.\n                        # This can happen if user specifies something like\n                        # random=['1|school', 'student|school'].\n                        if col == 'Intercept':\n                            if grpr in self.terms:\n                                continue\n                            label = '1|%s' % grpr\n                        else:\n                            label = col + '|' + grpr\n\n                        prior = priors.pop(label, priors.get('random', None))\n\n                        # Categorical or continuous is determined from data\n                        ld = lev_data.values\n                        if ((ld == 0) | (ld == 1)).all():\n                            lev_data = lev_data.astype(int)\n                            cat = True\n                        else:\n                            cat = False\n\n                        pred_data = pred_data[:, None]  # Must be 2D later\n                        term = RandomTerm(label, lev_data, pred_data,\n                                          grpr_df.values, categorical=cat,\n                                          constant=const if const else None,\n                                          prior=prior)\n                        self.terms[label] = term", "response": "Internal version of add."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a dependent variable to the model.", "response": "def _add_y(self, variable, prior=None, family='gaussian', link=None, *args,\n               **kwargs):\n        '''Add a dependent (or outcome) variable to the model.\n\n        Args:\n            variable (str): the name of the dataset column containing the\n                y values.\n            prior (Prior, int, float, str): Optional specification of prior.\n                Can be an instance of class Prior, a numeric value, or a string\n                describing the width. In the numeric case, the distribution\n                specified in the defaults will be used, and the passed value\n                will be used to scale the appropriate variance parameter. For\n                strings (e.g., 'wide', 'narrow', 'medium', or 'superwide'),\n                predefined values will be used.\n            family (str, Family): A specification of the model family\n                (analogous to the family object in R). Either a string, or an\n                instance of class priors.Family. If a string is passed, a\n                family with the corresponding name must be defined in the\n                defaults loaded at Model initialization. Valid pre-defined\n                families are 'gaussian', 'bernoulli', 'poisson', and 't'.\n            link (str): The model link function to use. Can be either a string\n                (must be one of the options defined in the current backend;\n                typically this will include at least 'identity', 'logit',\n                'inverse', and 'log'), or a callable that takes a 1D ndarray\n                or theano tensor as the sole argument and returns one with\n                the same shape.\n            args, kwargs: Optional positional and keyword arguments to pass\n                onto Term initializer.\n        '''\n        if isinstance(family, string_types):\n            family = self.default_priors.get(family=family)\n        self.family = family\n\n        # Override family's link if another is explicitly passed\n        if link is not None:\n            self.family.link = link\n\n        if prior is None:\n            prior = self.family.prior\n\n        # implement default Uniform [0, sd(Y)] prior for residual SD\n        if self.family.name == 'gaussian':\n            prior.update(sd=Prior('Uniform', lower=0,\n                                  upper=self.clean_data[variable].std()))\n\n        data = kwargs.pop('data', self.clean_data[variable])\n        term = Term(variable, data, prior=prior, *args, **kwargs)\n        self.y = term\n        self.built = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all derived terms whose name are derived from the specified string.", "response": "def _match_derived_terms(self, name):\n        '''\n        Returns all (random) terms whose named are derived from the\n        specified string. For example, 'condition|subject' should match the\n        terms with names '1|subject', 'condition[T.1]|subject', and so on.\n        Only works for strings with grouping operator ('|').\n        '''\n\n        if '|' not in name:\n            return None\n\n        patt = r'^([01]+)*[\\s\\+]*([^\\|]+)*\\|(.*)'\n        intcpt, pred, grpr = re.search(patt, name).groups()\n\n        intcpt = '1|%s' % grpr\n        if not pred:\n            return [self.terms[intcpt]] if intcpt in self.terms else None\n\n        source = '%s|%s' % (pred, grpr)\n        found = [t for (n, t) in self.terms.items() if n == intcpt or\n                 re.sub('(\\[.*?\\])', '', n) == source]\n        # If only the intercept matches, return None, because we want to err\n        # on the side of caution and not consider '1|subject' to be a match for\n        # 'condition|subject' if no slopes are found (e.g., the intercept could\n        # have been set by some other specification like 'gender|subject').\n        return found if found and (len(found) > 1 or found[0].name != intcpt) \\\n            else None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting priors for one or more existing terms.", "response": "def set_priors(self, priors=None, fixed=None, random=None,\n                   match_derived_names=True):\n        '''Set priors for one or more existing terms.\n\n        Args:\n            priors (dict): Dict of priors to update. Keys are names of terms\n                to update; values are the new priors (either a Prior instance,\n                or an int or float that scales the default priors). Note that\n                a tuple can be passed as the key, in which case the same prior\n                will be applied to all terms named in the tuple.\n            fixed (Prior, int, float, str): a prior specification to apply to\n                all fixed terms currently included in the model.\n            random (Prior, int, float, str): a prior specification to apply to\n                all random terms currently included in the model.\n            match_derived_names (bool): if True, the specified prior(s) will be\n                applied not only to terms that match the keyword exactly,\n                but to the levels of random effects that were derived from\n                the original specification with the passed name. For example,\n                `priors={'condition|subject':0.5}` would apply the prior\n                to the terms with names '1|subject', 'condition[T.1]|subject',\n                and so on. If False, an exact match is required for the\n                prior to be applied.\n        '''\n        # save arguments to pass to _set_priors() at build time\n        kwargs = dict(zip(\n            ['priors', 'fixed', 'random', 'match_derived_names'],\n            [priors, fixed, random, match_derived_names]))\n        self._added_priors.update(kwargs)\n\n        self.built = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing a new prior object.", "response": "def _prepare_prior(self, prior, _type):\n        '''\n        Args:\n            prior: Prior object, or float, or None.\n            type (string): 'intercept, 'fixed', or 'random'.\n        '''\n\n        if prior is None and not self.auto_scale:\n            prior = self.default_priors.get(term=_type + '_flat')\n\n        if isinstance(prior, Prior):\n            prior._auto_scale = False\n        else:\n            _scale = prior\n            prior = self.default_priors.get(term=_type)\n            prior.scale = _scale\n            if prior.scale is not None:\n                prior._auto_scale = False\n        return prior"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns dict of all and only fixed effects in model.", "response": "def fixed_terms(self):\n        '''Return dict of all and only fixed effects in model.'''\n        return {k: v for (k, v) in self.terms.items() if not v.random}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef random_terms(self):\n        '''Return dict of all and only random effects in model.'''\n        return {k: v for (k, v) in self.terms.items() if v.random}", "response": "Return dict of all and only random effects in model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninverting the dummy - coding process and represent full - rank dummies as a vector of indices into the coefficients.", "response": "def _invert_dummies(self, dummies):\n        '''\n        For the sake of computational efficiency (i.e., to avoid lots of\n        large matrix multiplications in the backends), invert the dummy-coding\n        process and represent full-rank dummies as a vector of indices into the\n        coefficients.\n        '''\n        vec = np.zeros(len(dummies), dtype=int)\n        for i in range(1, dummies.shape[1]):\n            vec[dummies[:, i] == 1] = i\n        return vec"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the model arguments with additional arguments.", "response": "def update(self, **kwargs):\n        '''Update the model arguments with additional arguments.\n\n        Args:\n            kwargs (dict): Optional keyword arguments to add to prior args.\n        '''\n\n        # Backends expect numpy arrays, so make sure all numeric values are\n        # represented as such.\n        kwargs = {k: (np.array(v) if isinstance(v, (int, float)) else v)\n                  for k, v in kwargs.items()}\n        self.args.update(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves default prior for a named distribution term type or family.", "response": "def get(self, dist=None, term=None, family=None):\n        '''Retrieve default prior for a named distribution, term type, or family.\n\n        Args:\n            dist (str): Name of desired distribution. Note that the name is\n                the key in the defaults dictionary, not the name of the\n                Distribution object used to construct the prior.\n            term (str): The type of term family to retrieve defaults for.\n                Must be one of 'intercept', 'fixed', or 'random'.\n            family (str): The name of the Family to retrieve. Must be a value\n                defined internally. In the default config, this is one of\n                'gaussian', 'bernoulli', 'poisson', or 't'.\n        '''\n\n        if dist is not None:\n            if dist not in self.dists:\n                raise ValueError(\n                    \"'%s' is not a valid distribution name.\" % dist)\n            return self._get_prior(self.dists[dist])\n        elif term is not None:\n            if term not in self.terms:\n                raise ValueError(\"'%s' is not a valid term type.\" % term)\n            return self._get_prior(self.terms[term])\n        elif family is not None:\n            if family not in self.families:\n                raise ValueError(\"'%s' is not a valid family name.\" % family)\n            _f = self.families[family]\n            prior = self._get_prior(_f['dist'])\n            return Family(family, prior, _f['link'], _f['parent'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_slope_stats(self, exog, predictor, sd_corr, full_mod=None,\n                         points=4):\n        '''\n        Args:\n            full_mod: statsmodels GLM to replace MLE model. For when 'predictor'\n                is not in the fixed part of the model.\n            points: number of points to use for LL approximation.\n        '''\n\n        if full_mod is None:\n            full_mod = self.mle\n\n        # figure out which column of exog to drop for the null model\n        keeps = [i for i, x in enumerate(list(exog.columns))\n                 if not np.array_equal(predictor, exog[x].values.flatten())]\n        i = [x for x in range(exog.shape[1]) if x not in keeps][0]\n\n        # get log-likelihood values from beta=0 to beta=MLE\n        values = np.linspace(0., full_mod.params[i], points)\n        # if there are multiple predictors, use statsmodels to optimize the LL\n        if keeps:\n            null = [GLM(endog=self.model.y.data, exog=exog,\n                           family=self.model.family.smfamily()).fit_constrained(\n                                str(exog.columns[i])+'='+str(val),\n                                start_params=full_mod.params.values)\n                    for val in values[:-1]]\n            null = np.append(null, full_mod)\n            ll = np.array([x.llf for x in null])\n        # if just a single predictor, use statsmodels to evaluate the LL\n        else:\n            null = [self.model.family.smfamily().loglike(\n                np.squeeze(self.model.y.data), val*predictor)\n                for val in values[:-1]]\n            ll = np.append(null, full_mod.llf)\n\n        # compute params of quartic approximatino to log-likelihood\n        # c: intercept, d: shift parameter\n        # a: quartic coefficient, b: quadratic coefficient\n\n        c, d = ll[-1], -(full_mod.params[i].item())\n        X = np.array([(values+d)**4,\n                       (values+d)**2]).T\n        a, b = np.squeeze(\n            np.linalg.multi_dot([\n                np.linalg.inv(np.dot(X.T, X)),\n                X.T,\n                (ll[:, None] - c)\n            ])\n        )\n\n        # m, v: mean and variance of beta distribution of correlations\n        # p, q: corresponding shape parameters of beta distribution\n        m = .5\n        v = sd_corr**2/4\n        p = m*(m*(1-m)/v - 1)\n        q = (1-m)*(m*(1-m)/v - 1)\n\n        # function to return central moments of rescaled beta distribution\n        def moment(k): return (2*p/(p+q))**k * hyp2f1(p, -k, p+q, (p+q)/p)\n\n        # evaluate the derivatives of beta = f(correlation).\n        # dict 'point' gives points about which to Taylor expand. We want to\n        # expand about the mean (generally 0), but some of the derivatives\n        # do not exist at 0. Evaluating at a point very close to 0 (e.g., .001)\n        # generally gives good results, but the higher order the expansion, the\n        # further from 0 we need to evaluate the derivatives, or they blow up.\n        point = dict(zip(range(1, 14), 2**np.linspace(-1, 5, 13)/100))\n        vals = dict(a=a, b=b, n=len(self.model.y.data), r=point[self.taylor])\n        _deriv = [eval(x, globals(), vals) for x in self.deriv]\n\n        # compute and return the approximate SD\n        def term(i, j):\n            return 1/np.math.factorial(i) * 1/np.math.factorial(j) \\\n                * _deriv[i] * _deriv[j] \\\n                * (moment(i+j) - moment(i)*moment(j))\n        terms = [term(i, j) for i in range(1, self.taylor+1)\n                 for j in range(1, self.taylor+1)]\n        return np.array(terms).sum()**.5", "response": "This function computes the slope stats for the given model and predictor."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset the model and all tracked distributions and parameters.", "response": "def reset(self):\n        '''\n        Reset Stan model and all tracked distributions and parameters.\n        '''\n        self.parameters = []\n        self.transformed_parameters = []\n        self.expressions = []\n        self.data = []\n        self.transformed_data = []\n        self.X = {}\n        self.model = []\n        self.mu_cont = []\n        self.mu_cat = []\n        self._original_names = {}\n        # variables to suppress in output. Stan uses limited set for variable\n        # names, so track variable names we may need to simplify for the model\n        # code and then sub back later.\n        self._suppress_vars = ['yhat', 'lp__']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the base class instance from an abstract model specification.", "response": "def build(self, spec, reset=True):\n        '''\n        Compile the Stan model from an abstract model specification.\n        Args:\n            spec (Model): A bambi Model instance containing the abstract\n                specification of the model to compile.\n            reset (bool): if True (default), resets the StanBackEnd instance\n                before compiling.\n        '''\n        if reset:\n            self.reset()\n\n        n_cases = len(spec.y.data)\n        self.data.append('int<lower=1> N;')\n        self.X['N'] = n_cases\n\n        def _sanitize_name(name):\n            ''' Stan only allows alphanumeric chars and underscore, and\n            variable names must begin with a letter. Additionally, Stan\n            reserves a few hundred strings that can't be used as variable\n            names. So to play it safe, we replace all invalid chars with '_',\n            and prepend all variables with 'b_'. We substitute the original\n            names back in later. '''\n            if name in self._original_names:\n                return name\n            clean = 'b_' + re.sub('[^a-zA-Z0-9\\_]+', '_', name)\n            self._original_names[clean] = name\n            return clean\n\n        def _map_dist(dist, **kwargs):\n            ''' Maps PyMC3 distribution names and attrs in the Prior object\n            to the corresponding Stan names and argument order. '''\n            if dist not in self.dists:\n                raise ValueError(\"There is no distribution named '%s' \"\n                                 \"in Stan.\" % dist)\n\n            stan_dist = self.dists[dist]\n            dist_name = stan_dist['name']\n            dist_args = stan_dist['args']\n            dist_bounds = stan_dist.get('bounds', '')\n\n            # Flat/HalfFlat/undefined priors are handled separately\n            if dist_name is None:\n                return None, dist_bounds\n\n            lookup_args = [a[1:] for a in dist_args if a.startswith('#')]\n            missing = set(lookup_args) - set(list(kwargs.keys()))\n            if missing:\n                raise ValueError(\"The following mandatory parameters of \"\n                                 \"the %s distribution are missing: %s.\"\n                                 % (dist, missing))\n\n            # Named arguments to take from the Prior object are denoted with\n            # a '#'; otherwise we take the value in the self.dists dict as-is.\n            dp = [kwargs[p[1:]] if p.startswith('#') else p for p in dist_args]\n\n            # Sometimes we get numpy arrays at this stage, so convert to float\n            dp = [float(p.ravel()[0]) if isinstance(p, np.ndarray) else p\n                  for p in dp]\n\n            dist_term = '%s(%s)' % (dist_name, ', '.join([str(p) for p in dp]))\n\n            # handle Uniform variables, for which the bounds are the parameters\n            if dist_name=='uniform':\n                dist_bounds = dist_bounds.format(*dp)\n\n            return dist_term, dist_bounds\n\n        def _add_data(name, data, term):\n            ''' Add all model components that directly touch or relate to data.\n            '''\n            if data.shape[1] == 1:\n                # For random effects, index into grouping variable\n                if n_cols > 1:\n                    index_name = _sanitize_name('%s_grp_ind' % name)\n                    self.data.append('int %s[N];' % index_name)\n                    self.X[index_name] = t.group_index + 1  # 1-based indexing\n                predictor = 'vector[N] %s;'\n            else:\n                predictor = ('matrix[N, %d]' % (n_cols)) + ' %s;'\n\n            data_name = _sanitize_name('%s_data' % name)\n            var_name = _sanitize_name(name)\n            self.data.append(predictor % data_name)\n            self.X[data_name] = data.squeeze()\n\n            if data.shape[1] == 1 and n_cols > 1:\n                code = '%s[%s[n]] * %s[n]' % (var_name, index_name, data_name)\n                self.mu_cat.append(code)\n            else:\n                self.mu_cont.append('%s * %s' % (data_name, var_name))\n\n        def _add_parameters(name, dist_name, n_cols, **dist_args):\n            ''' Add all model components related to latent parameters. We\n            handle these separately from the data components, as the parameters\n            can have nested specifications (in the case of random effects). '''\n\n            def _expand_args(k, v, name):\n                if isinstance(v, Prior):\n                    name = _sanitize_name('%s_%s' % (name, k))\n                    return _add_parameters(name, v.name, 1, **v.args)\n                return v\n\n            kwargs = {k: _expand_args(k, v, name)\n                      for (k, v) in dist_args.items()}\n\n            _dist, _bounds = _map_dist(dist_name, **kwargs)\n\n            if n_cols == 1:\n                stan_par = 'real'\n            else:\n                stan_par = 'vector[%d]' % n_cols\n\n            var_name = _sanitize_name(name)\n\n            # non-centered parameterization\n            if spec.noncentered and 'sd' in kwargs and \\\n                    isinstance(kwargs['sd'], string_types):\n                offset_name = _sanitize_name(name + '_offset')\n                offset = 'vector[%d] %s;' % (n_cols, offset_name)\n                self.parameters.append(offset)\n                self.model.append('%s ~ normal(0, 1);' % offset_name)\n                self.transformed_parameters.append('%s%s %s;' % (stan_par,\n                                                                 _bounds,\n                                                                 var_name))\n                trans = '%s = multiply(%s, %s);' % (var_name, offset_name,\n                                                    kwargs['sd'])\n                self.expressions.append(trans)\n\n            else:\n                self.parameters.append('%s%s %s;' % (stan_par, _bounds,\n                                                     var_name))\n                if _dist is not None:\n                    self.model.append('%s ~ %s;' % (var_name, _dist))\n\n            return name\n\n        for t in spec.terms.values():\n\n            data = t.data\n            label = t.name\n            dist_name = t.prior.name\n            dist_args = t.prior.args\n\n            n_cols = data.shape[1]\n\n            if t.random:\n                data = t.predictor\n\n            # Add to Stan model\n            _add_data(label, data, t)\n            _add_parameters(label, dist_name, n_cols, **dist_args)\n\n        # yhat\n        self.transformed_parameters.append('vector[N] yhat;')\n        if self.mu_cont:\n            yhat_cont = 'yhat = %s;' % ' + '.join(self.mu_cont)\n            self.expressions.append(yhat_cont)\n        else:\n            self.mu_cat.insert(0, '0')\n\n        if self.mu_cat:\n            loops = ('for (n in 1:N)\\n\\t\\tyhat[n] = yhat[n] + %s'\n                     % ' + '.join(self.mu_cat) + ';\\n\\t')\n            self.expressions.append(loops)\n\n        # Add expressions that go in transformed parameter block (they have\n        # to come after variable definitions)\n        self.transformed_parameters += self.expressions\n\n        # add response variable (y)\n        _response_format = self.families[spec.family.name]['format']\n        self.data.append('{} y{};'.format(*_response_format))\n\n        # add response distribution parameters other than the location\n        # parameter\n        for k, v in spec.family.prior.args.items():\n            if k != spec.family.parent and isinstance(v, Prior):\n                _bounds = _map_dist(v.name, **v.args)[1]\n                _param = 'real{} {}_{};'.format(_bounds, spec.y.name, k)\n                self.parameters.append(_param)\n\n        # specify the response distribution\n        _response_dist = self.families[spec.family.name]['name']\n        _response_args = '{}(yhat)'.format(self.links[spec.family.link])\n        _response_args = {spec.family.parent: _response_args}\n        for k, v in spec.family.prior.args.items():\n            if k != spec.family.parent:\n                _response_args[k] = '{}_{}'.format(spec.y.name, k) \\\n                    if isinstance(v, Prior) else str(v)\n        _dist = _map_dist(_response_dist, **_response_args)[0]\n        self.model.append('y ~ {};'.format(_dist))\n\n        # add the data\n        _response_type = self.families[spec.family.name]['type']\n        self.X['y'] = spec.y.data.astype(_response_type).squeeze()\n\n        # Construct the stan script\n        def format_block(name):\n            key = name.replace(' ', '_')\n            els = ''.join(['\\t%s\\n' % e for e in getattr(self, key)])\n            return '%s {\\n%s}\\n' % (name, els)\n\n        blocks = ['data', 'transformed data', 'parameters',\n                  'transformed parameters', 'model']\n        self.model_code = ''.join([format_block(bl) for bl in blocks])\n        self.spec = spec\n        self.stan_model = ps.StanModel(model_code=self.model_code)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the Stan sampler.", "response": "def run(self, samples=1000, chains=1, **kwargs):\n        '''\n        Run the Stan sampler.\n        Args:\n            samples (int): Number of samples to obtain (in each chain).\n            chains (int): Number of chains to use.\n            kwargs (dict): Optional keyword arguments passed onto the PyStan\n                StanModel.sampling() call.\n        Returns: A PyMC3ModelResults instance.\n        '''\n        self.fit = self.stan_model.sampling(data=self.X, iter=samples,\n                                            chains=chains, **kwargs)\n        return self._convert_to_results()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef err(self):\n        if self.__err is not None:\n            return self.__err\n\n        if self._uses_subprocess:\n            self.__err = self.std_err.read()\n        else:\n            self.__err = self._pexpect_out\n\n        return self.__err", "response": "Return the error output of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_attachment(self, upload_stream, project, wiki_identifier, name, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if wiki_identifier is not None:\n            route_values['wikiIdentifier'] = self._serialize.url('wiki_identifier', wiki_identifier, 'str')\n        query_parameters = {}\n        if name is not None:\n            query_parameters['name'] = self._serialize.query('name', name, 'str')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        content = self._client.stream_upload(upload_stream, callback=callback)\n        response = self._send(http_method='PUT',\n                              location_id='c4382d8d-fefc-40e0-92c5-49852e9e17c0',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content,\n                              media_type='application/octet-stream')\n        response_object = models.WikiAttachmentResponse()\n        response_object.attachment = self._deserialize('WikiAttachment', response)\n        response_object.eTag = response.headers.get('ETag')\n        return response_object", "response": "Creates an attachment in the wiki."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a page move operation that updates the path and order of the page.", "response": "def create_page_move(self, page_move_parameters, project, wiki_identifier, comment=None):\n        \"\"\"CreatePageMove.\n        Creates a page move operation that updates the path and order of the page as provided in the parameters.\n        :param :class:`<WikiPageMoveParameters> <azure.devops.v5_0.wiki.models.WikiPageMoveParameters>` page_move_parameters: Page more operation parameters.\n        :param str project: Project ID or project name\n        :param str wiki_identifier: Wiki Id or name.\n        :param str comment: Comment that is to be associated with this page move.\n        :rtype: :class:`<WikiPageMoveResponse> <azure.devops.v5_0.wiki.models.WikiPageMoveResponse>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if wiki_identifier is not None:\n            route_values['wikiIdentifier'] = self._serialize.url('wiki_identifier', wiki_identifier, 'str')\n        query_parameters = {}\n        if comment is not None:\n            query_parameters['comment'] = self._serialize.query('comment', comment, 'str')\n        content = self._serialize.body(page_move_parameters, 'WikiPageMoveParameters')\n        response = self._send(http_method='POST',\n                              location_id='e37bbe71-cbae-49e5-9a4e-949143b9d910',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        response_object = models.WikiPageMoveResponse()\n        response_object.page_move = self._deserialize('WikiPageMove', response)\n        response_object.eTag = response.headers.get('ETag')\n        return response_object"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates or updates a wiki page.", "response": "def create_or_update_page(self, parameters, project, wiki_identifier, path, version, comment=None):\n        \"\"\"CreateOrUpdatePage.\n        Creates or edits a wiki page.\n        :param :class:`<WikiPageCreateOrUpdateParameters> <azure.devops.v5_0.wiki.models.WikiPageCreateOrUpdateParameters>` parameters: Wiki create or update operation parameters.\n        :param str project: Project ID or project name\n        :param str wiki_identifier: Wiki Id or name.\n        :param str path: Wiki page path.\n        :param String version: Version of the page on which the change is to be made. Mandatory for `Edit` scenario. To be populated in the If-Match header of the request.\n        :param str comment: Comment to be associated with the page operation.\n        :rtype: :class:`<WikiPageResponse> <azure.devops.v5_0.wiki.models.WikiPageResponse>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if wiki_identifier is not None:\n            route_values['wikiIdentifier'] = self._serialize.url('wiki_identifier', wiki_identifier, 'str')\n        query_parameters = {}\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        if comment is not None:\n            query_parameters['comment'] = self._serialize.query('comment', comment, 'str')\n        content = self._serialize.body(parameters, 'WikiPageCreateOrUpdateParameters')\n        response = self._send(http_method='PUT',\n                              location_id='25d3fbc7-fe3d-46cb-b5a5-0b6f79caf27b',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        response_object = models.WikiPageResponse()\n        response_object.page = self._deserialize('WikiPage', response)\n        response_object.eTag = response.headers.get('ETag')\n        return response_object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a page for the provided path.", "response": "def get_page(self, project, wiki_identifier, path=None, recursion_level=None, version_descriptor=None, include_content=None):\n        \"\"\"GetPage.\n        Gets metadata or content of the wiki page for the provided path. Content negotiation is done based on the `Accept` header sent in the request.\n        :param str project: Project ID or project name\n        :param str wiki_identifier: Wiki Id or name.\n        :param str path: Wiki page path.\n        :param str recursion_level: Recursion level for subpages retrieval. Defaults to `None` (Optional).\n        :param :class:`<GitVersionDescriptor> <azure.devops.v5_0.wiki.models.GitVersionDescriptor>` version_descriptor: GitVersionDescriptor for the page. Defaults to the default branch (Optional).\n        :param bool include_content: True to include the content of the page in the response for Json content type. Defaults to false (Optional)\n        :rtype: :class:`<WikiPageResponse> <azure.devops.v5_0.wiki.models.WikiPageResponse>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if wiki_identifier is not None:\n            route_values['wikiIdentifier'] = self._serialize.url('wiki_identifier', wiki_identifier, 'str')\n        query_parameters = {}\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        if recursion_level is not None:\n            query_parameters['recursionLevel'] = self._serialize.query('recursion_level', recursion_level, 'str')\n        if version_descriptor is not None:\n            if version_descriptor.version_type is not None:\n                query_parameters['versionDescriptor.versionType'] = version_descriptor.version_type\n            if version_descriptor.version is not None:\n                query_parameters['versionDescriptor.version'] = version_descriptor.version\n            if version_descriptor.version_options is not None:\n                query_parameters['versionDescriptor.versionOptions'] = version_descriptor.version_options\n        if include_content is not None:\n            query_parameters['includeContent'] = self._serialize.query('include_content', include_content, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='25d3fbc7-fe3d-46cb-b5a5-0b6f79caf27b',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        response_object = models.WikiPageResponse()\n        response_object.page = self._deserialize('WikiPage', response)\n        response_object.eTag = response.headers.get('ETag')\n        return response_object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new wiki resource.", "response": "def create_wiki(self, wiki_create_params, project=None):\n        \"\"\"CreateWiki.\n        Creates the wiki resource.\n        :param :class:`<WikiCreateParametersV2> <azure.devops.v5_0.wiki.models.WikiCreateParametersV2>` wiki_create_params: Parameters for the wiki creation.\n        :param str project: Project ID or project name\n        :rtype: :class:`<WikiV2> <azure.devops.v5_0.wiki.models.WikiV2>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(wiki_create_params, 'WikiCreateParametersV2')\n        response = self._send(http_method='POST',\n                              location_id='288d122c-dbd4-451d-aa5f-7dbbba070728',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WikiV2', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the wiki corresponding to the wiki name or Id provided.", "response": "def delete_wiki(self, wiki_identifier, project=None):\n        \"\"\"DeleteWiki.\n        Deletes the wiki corresponding to the wiki name or Id provided.\n        :param str wiki_identifier: Wiki name or Id.\n        :param str project: Project ID or project name\n        :rtype: :class:`<WikiV2> <azure.devops.v5_0.wiki.models.WikiV2>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if wiki_identifier is not None:\n            route_values['wikiIdentifier'] = self._serialize.url('wiki_identifier', wiki_identifier, 'str')\n        response = self._send(http_method='DELETE',\n                              location_id='288d122c-dbd4-451d-aa5f-7dbbba070728',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('WikiV2', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_logs(self, source, entry_id=None, start_time=None, end_time=None):\n        route_values = {}\n        if source is not None:\n            route_values['source'] = self._serialize.url('source', source, 'str')\n        if entry_id is not None:\n            route_values['entryId'] = self._serialize.url('entry_id', entry_id, 'str')\n        query_parameters = {}\n        if start_time is not None:\n            query_parameters['startTime'] = self._serialize.query('start_time', start_time, 'iso-8601')\n        if end_time is not None:\n            query_parameters['endTime'] = self._serialize.query('end_time', end_time, 'iso-8601')\n        response = self._send(http_method='GET',\n                              location_id='991842f3-eb16-4aea-ac81-81353ef2b75c',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[INotificationDiagnosticLog]', self._unwrap_collection(response))", "response": "List diagnostic logs for a specific source entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a specific event type.", "response": "def get_event_type(self, event_type):\n        \"\"\"GetEventType.\n        [Preview API] Get a specific event type.\n        :param str event_type:\n        :rtype: :class:`<NotificationEventType> <azure.devops.v5_0.notification.models.NotificationEventType>`\n        \"\"\"\n        route_values = {}\n        if event_type is not None:\n            route_values['eventType'] = self._serialize.url('event_type', event_type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='cc84fb5f-6247-4c7a-aeae-e5a3c3fddb21',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('NotificationEventType', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_settings(self):\n        response = self._send(http_method='GET',\n                              location_id='cbe076d8-2803-45ff-8d8d-44653686ea2a',\n                              version='5.0-preview.1')\n        return self._deserialize('NotificationAdminSettings', response)", "response": "Get the current settings of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the settings of the current user.", "response": "def update_settings(self, update_parameters):\n        \"\"\"UpdateSettings.\n        [Preview API]\n        :param :class:`<NotificationAdminSettingsUpdateParameters> <azure.devops.v5_0.notification.models.NotificationAdminSettingsUpdateParameters>` update_parameters:\n        :rtype: :class:`<NotificationAdminSettings> <azure.devops.v5_0.notification.models.NotificationAdminSettings>`\n        \"\"\"\n        content = self._serialize.body(update_parameters, 'NotificationAdminSettingsUpdateParameters')\n        response = self._send(http_method='PATCH',\n                              location_id='cbe076d8-2803-45ff-8d8d-44653686ea2a',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('NotificationAdminSettings', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_subscriber(self, subscriber_id):\n        route_values = {}\n        if subscriber_id is not None:\n            route_values['subscriberId'] = self._serialize.url('subscriber_id', subscriber_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='4d5caff1-25ba-430b-b808-7a1f352cc197',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('NotificationSubscriber', response)", "response": "Get a specific subscriber."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_subscriber(self, update_parameters, subscriber_id):\n        route_values = {}\n        if subscriber_id is not None:\n            route_values['subscriberId'] = self._serialize.url('subscriber_id', subscriber_id, 'str')\n        content = self._serialize.body(update_parameters, 'NotificationSubscriberUpdateParameters')\n        response = self._send(http_method='PATCH',\n                              location_id='4d5caff1-25ba-430b-b808-7a1f352cc197',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('NotificationSubscriber', response)", "response": "Update the subscriber with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_subscription(self, subscription_id, query_flags=None):\n        route_values = {}\n        if subscription_id is not None:\n            route_values['subscriptionId'] = self._serialize.url('subscription_id', subscription_id, 'str')\n        query_parameters = {}\n        if query_flags is not None:\n            query_parameters['queryFlags'] = self._serialize.query('query_flags', query_flags, 'str')\n        response = self._send(http_method='GET',\n                              location_id='70f911d6-abac-488c-85b3-a206bf57e165',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('NotificationSubscription', response)", "response": "Get a specific subscription by its ID."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_subscriptions(self, target_id=None, ids=None, query_flags=None):\n        query_parameters = {}\n        if target_id is not None:\n            query_parameters['targetId'] = self._serialize.query('target_id', target_id, 'str')\n        if ids is not None:\n            ids = \",\".join(ids)\n            query_parameters['ids'] = self._serialize.query('ids', ids, 'str')\n        if query_flags is not None:\n            query_parameters['queryFlags'] = self._serialize.query('query_flags', query_flags, 'str')\n        response = self._send(http_method='GET',\n                              location_id='70f911d6-abac-488c-85b3-a206bf57e165',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('[NotificationSubscription]', self._unwrap_collection(response))", "response": "List the subscriptions of a specific target."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates an existing subscription.", "response": "def update_subscription(self, update_parameters, subscription_id):\n        \"\"\"UpdateSubscription.\n        [Preview API] Update an existing subscription. Depending on the type of subscription and permissions, the caller can update the description, filter settings, channel (delivery) settings and more.\n        :param :class:`<NotificationSubscriptionUpdateParameters> <azure.devops.v5_0.notification.models.NotificationSubscriptionUpdateParameters>` update_parameters:\n        :param str subscription_id:\n        :rtype: :class:`<NotificationSubscription> <azure.devops.v5_0.notification.models.NotificationSubscription>`\n        \"\"\"\n        route_values = {}\n        if subscription_id is not None:\n            route_values['subscriptionId'] = self._serialize.url('subscription_id', subscription_id, 'str')\n        content = self._serialize.body(update_parameters, 'NotificationSubscriptionUpdateParameters')\n        response = self._send(http_method='PATCH',\n                              location_id='70f911d6-abac-488c-85b3-a206bf57e165',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('NotificationSubscription', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_subscription_user_settings(self, user_settings, subscription_id, user_id):\n        route_values = {}\n        if subscription_id is not None:\n            route_values['subscriptionId'] = self._serialize.url('subscription_id', subscription_id, 'str')\n        if user_id is not None:\n            route_values['userId'] = self._serialize.url('user_id', user_id, 'str')\n        content = self._serialize.body(user_settings, 'SubscriptionUserSettings')\n        response = self._send(http_method='PUT',\n                              location_id='ed5a3dff-aeb5-41b1-b4f7-89e66e58b62e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('SubscriptionUserSettings', response)", "response": "Update the user s settings for the specified subscription."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_session(self, session_request, protocol):\n        route_values = {}\n        if protocol is not None:\n            route_values['protocol'] = self._serialize.url('protocol', protocol, 'str')\n        content = self._serialize.body(session_request, 'SessionRequest')\n        response = self._send(http_method='POST',\n                              location_id='503b4e54-ebf4-4d04-8eee-21c00823c2ac',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('SessionResponse', response)", "response": "CreateSession.\n        [Preview API] Creates a session, a wrapper around a feed that can store additional metadata on the packages published to it.\n        :param :class:`<SessionRequest> <azure.devops.v5_0.provenance.models.SessionRequest>` session_request: The feed and metadata for the session\n        :param str protocol: The protocol that the session will target\n        :rtype: :class:`<SessionResponse> <azure.devops.v5_0.provenance.models.SessionResponse>`"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_certificate(self, **kwargs):\n        response = self._send(http_method='GET',\n                              location_id='2e0dbce7-a327-4bc0-a291-056139393f6d',\n                              version='5.0-preview.1',\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get certificate from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget Client Rights of a specific class.", "response": "def get_client_rights(self, right_name=None, product_version=None, edition=None, rel_type=None, include_certificate=None, canary=None, machine_id=None):\n        \"\"\"GetClientRights.\n        [Preview API]\n        :param str right_name:\n        :param str product_version:\n        :param str edition:\n        :param str rel_type:\n        :param bool include_certificate:\n        :param str canary:\n        :param str machine_id:\n        :rtype: :class:`<ClientRightsContainer> <azure.devops.v5_0.licensing.models.ClientRightsContainer>`\n        \"\"\"\n        route_values = {}\n        if right_name is not None:\n            route_values['rightName'] = self._serialize.url('right_name', right_name, 'str')\n        query_parameters = {}\n        if product_version is not None:\n            query_parameters['productVersion'] = self._serialize.query('product_version', product_version, 'str')\n        if edition is not None:\n            query_parameters['edition'] = self._serialize.query('edition', edition, 'str')\n        if rel_type is not None:\n            query_parameters['relType'] = self._serialize.query('rel_type', rel_type, 'str')\n        if include_certificate is not None:\n            query_parameters['includeCertificate'] = self._serialize.query('include_certificate', include_certificate, 'bool')\n        if canary is not None:\n            query_parameters['canary'] = self._serialize.query('canary', canary, 'str')\n        if machine_id is not None:\n            query_parameters['machineId'] = self._serialize.query('machine_id', machine_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='643c72da-eaee-4163-9f07-d748ef5c2a0c',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ClientRightsContainer', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nassign an available account entitilement to a user.", "response": "def assign_available_account_entitlement(self, user_id, dont_notify_user=None, origin=None):\n        \"\"\"AssignAvailableAccountEntitlement.\n        [Preview API] Assign an available entitilement to a user\n        :param str user_id: The user to which to assign the entitilement\n        :param bool dont_notify_user:\n        :param str origin:\n        :rtype: :class:`<AccountEntitlement> <azure.devops.v5_0.licensing.models.AccountEntitlement>`\n        \"\"\"\n        query_parameters = {}\n        if user_id is not None:\n            query_parameters['userId'] = self._serialize.query('user_id', user_id, 'str')\n        if dont_notify_user is not None:\n            query_parameters['dontNotifyUser'] = self._serialize.query('dont_notify_user', dont_notify_user, 'bool')\n        if origin is not None:\n            query_parameters['origin'] = self._serialize.query('origin', origin, 'str')\n        response = self._send(http_method='POST',\n                              location_id='c01e9fd5-0d8c-4d5e-9a68-734bd8da6a38',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('AccountEntitlement', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the account entitlement of the current user.", "response": "def get_account_entitlement(self):\n        \"\"\"GetAccountEntitlement.\n        [Preview API] Gets the account entitlement of the current user it is mapped to _apis/licensing/entitlements/me so specifically is looking for the user of the request\n        :rtype: :class:`<AccountEntitlement> <azure.devops.v5_0.licensing.models.AccountEntitlement>`\n        \"\"\"\n        response = self._send(http_method='GET',\n                              location_id='c01e9fd5-0d8c-4d5e-9a68-734bd8da6a38',\n                              version='5.0-preview.1')\n        return self._deserialize('AccountEntitlement', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assign_account_entitlement_for_user(self, body, user_id, dont_notify_user=None, origin=None):\n        route_values = {}\n        if user_id is not None:\n            route_values['userId'] = self._serialize.url('user_id', user_id, 'str')\n        query_parameters = {}\n        if dont_notify_user is not None:\n            query_parameters['dontNotifyUser'] = self._serialize.query('dont_notify_user', dont_notify_user, 'bool')\n        if origin is not None:\n            query_parameters['origin'] = self._serialize.query('origin', origin, 'str')\n        content = self._serialize.body(body, 'AccountEntitlementUpdateModel')\n        response = self._send(http_method='PUT',\n                              location_id='6490e566-b299-49a7-a4e4-28749752581f',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('AccountEntitlement', response)", "response": "Assign an explicit account entitlement for a user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_eligible_users_for_extension(self, extension_id, options):\n        route_values = {}\n        if extension_id is not None:\n            route_values['extensionId'] = self._serialize.url('extension_id', extension_id, 'str')\n        query_parameters = {}\n        if options is not None:\n            query_parameters['options'] = self._serialize.query('options', options, 'str')\n        response = self._send(http_method='GET',\n                              location_id='5434f182-7f32-4135-8326-9340d887c08a',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[str]', self._unwrap_collection(response))", "response": "Get the list of users that are eligible to assign the extension to."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the status of all account users for the given extension.", "response": "def get_extension_status_for_users(self, extension_id):\n        \"\"\"GetExtensionStatusForUsers.\n        [Preview API] Returns extension assignment status of all account users for the given extension\n        :param str extension_id: The extension to check the status of the users for.\n        :rtype: {ExtensionAssignmentDetails}\n        \"\"\"\n        route_values = {}\n        if extension_id is not None:\n            route_values['extensionId'] = self._serialize.url('extension_id', extension_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='5434f182-7f32-4135-8326-9340d887c08a',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('{ExtensionAssignmentDetails}', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assign_extension_to_users(self, body):\n        content = self._serialize.body(body, 'ExtensionAssignment')\n        response = self._send(http_method='PUT',\n                              location_id='8cec75ea-044f-4245-ab0d-a82dafcc85ea',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('[ExtensionOperationResult]', self._unwrap_collection(response))", "response": "Assign the access to the given extension for a given list of users."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_extensions_assigned_to_user(self, user_id):\n        route_values = {}\n        if user_id is not None:\n            route_values['userId'] = self._serialize.url('user_id', user_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='8cec75ea-044f-4245-ab0d-a82dafcc85ea',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('{LicensingSource}', self._unwrap_collection(response))", "response": "Get the list of extensions that are currently assigned to a user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_extension_license_data(self, extension_id):\n        route_values = {}\n        if extension_id is not None:\n            route_values['extensionId'] = self._serialize.url('extension_id', extension_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='004a420a-7bef-4b7f-8a50-22975d2067cc',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('ExtensionLicenseData', response)", "response": "Get the license data for the specified extension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_extension_license(self, extension_license_data):\n        content = self._serialize.body(extension_license_data, 'ExtensionLicenseData')\n        response = self._send(http_method='POST',\n                              location_id='004a420a-7bef-4b7f-8a50-22975d2067cc',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('bool', response)", "response": "RegisterExtensionLicense.\n        [Preview API]\n        :param :class:`<ExtensionLicenseData> <azure.devops.v5_0.licensing.models.ExtensionLicenseData>` extension_license_data:\n        :rtype: bool"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_extension_rights(self, ids):\n        content = self._serialize.body(ids, '[str]')\n        response = self._send(http_method='POST',\n                              location_id='5f1dbe21-f748-47c7-b5fd-3770c8bc2c08',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('{bool}', self._unwrap_collection(response))", "response": "Compute extension rights for the given set of identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the list of rights for the current locale.", "response": "def get_extension_rights(self):\n        \"\"\"GetExtensionRights.\n        [Preview API]\n        :rtype: :class:`<ExtensionRightsResult> <azure.devops.v5_0.licensing.models.ExtensionRightsResult>`\n        \"\"\"\n        response = self._send(http_method='GET',\n                              location_id='5f1dbe21-f748-47c7-b5fd-3770c8bc2c08',\n                              version='5.0-preview.1')\n        return self._deserialize('ExtensionRightsResult', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating Items in the referenced container.", "response": "def create_items(self, items, container_id, scope=None):\n        \"\"\"CreateItems.\n        [Preview API] Creates the specified items in in the referenced container.\n        :param :class:`<VssJsonCollectionWrapper> <azure.devops.v5_0.file_container.models.VssJsonCollectionWrapper>` items:\n        :param int container_id:\n        :param str scope: A guid representing the scope of the container. This is often the project id.\n        :rtype: [FileContainerItem]\n        \"\"\"\n        route_values = {}\n        if container_id is not None:\n            route_values['containerId'] = self._serialize.url('container_id', container_id, 'int')\n        query_parameters = {}\n        if scope is not None:\n            query_parameters['scope'] = self._serialize.query('scope', scope, 'str')\n        content = self._serialize.body(items, 'VssJsonCollectionWrapper')\n        response = self._send(http_method='POST',\n                              location_id='e4f5c81e-e250-447b-9fef-bd48471bea5e',\n                              version='5.0-preview.4',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('[FileContainerItem]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes an item in a container.", "response": "def delete_item(self, container_id, item_path, scope=None):\n        \"\"\"DeleteItem.\n        [Preview API] Deletes the specified items in a container.\n        :param long container_id: Container Id.\n        :param str item_path: Path to delete.\n        :param str scope: A guid representing the scope of the container. This is often the project id.\n        \"\"\"\n        route_values = {}\n        if container_id is not None:\n            route_values['containerId'] = self._serialize.url('container_id', container_id, 'long')\n        query_parameters = {}\n        if item_path is not None:\n            query_parameters['itemPath'] = self._serialize.query('item_path', item_path, 'str')\n        if scope is not None:\n            query_parameters['scope'] = self._serialize.query('scope', scope, 'str')\n        self._send(http_method='DELETE',\n                   location_id='e4f5c81e-e250-447b-9fef-bd48471bea5e',\n                   version='5.0-preview.4',\n                   route_values=route_values,\n                   query_parameters=query_parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting containers filtered by a comma separated list of artifact uris within the same scope.", "response": "def get_containers(self, scope=None, artifact_uris=None):\n        \"\"\"GetContainers.\n        [Preview API] Gets containers filtered by a comma separated list of artifact uris within the same scope, if not specified returns all containers\n        :param str scope: A guid representing the scope of the container. This is often the project id.\n        :param str artifact_uris:\n        :rtype: [FileContainer]\n        \"\"\"\n        query_parameters = {}\n        if scope is not None:\n            query_parameters['scope'] = self._serialize.query('scope', scope, 'str')\n        if artifact_uris is not None:\n            query_parameters['artifactUris'] = self._serialize.query('artifact_uris', artifact_uris, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e4f5c81e-e250-447b-9fef-bd48471bea5e',\n                              version='5.0-preview.4',\n                              query_parameters=query_parameters)\n        return self._deserialize('[FileContainer]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget items in a file container.", "response": "def get_items(self, container_id, scope=None, item_path=None, metadata=None, format=None, download_file_name=None, include_download_tickets=None, is_shallow=None):\n        \"\"\"GetItems.\n        [Preview API]\n        :param long container_id:\n        :param str scope:\n        :param str item_path:\n        :param bool metadata:\n        :param str format:\n        :param str download_file_name:\n        :param bool include_download_tickets:\n        :param bool is_shallow:\n        :rtype: [FileContainerItem]\n        \"\"\"\n        route_values = {}\n        if container_id is not None:\n            route_values['containerId'] = self._serialize.url('container_id', container_id, 'long')\n        query_parameters = {}\n        if scope is not None:\n            query_parameters['scope'] = self._serialize.query('scope', scope, 'str')\n        if item_path is not None:\n            query_parameters['itemPath'] = self._serialize.query('item_path', item_path, 'str')\n        if metadata is not None:\n            query_parameters['metadata'] = self._serialize.query('metadata', metadata, 'bool')\n        if format is not None:\n            query_parameters['$format'] = self._serialize.query('format', format, 'str')\n        if download_file_name is not None:\n            query_parameters['downloadFileName'] = self._serialize.query('download_file_name', download_file_name, 'str')\n        if include_download_tickets is not None:\n            query_parameters['includeDownloadTickets'] = self._serialize.query('include_download_tickets', include_download_tickets, 'bool')\n        if is_shallow is not None:\n            query_parameters['isShallow'] = self._serialize.query('is_shallow', is_shallow, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='e4f5c81e-e250-447b-9fef-bd48471bea5e',\n                              version='5.0-preview.4',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[FileContainerItem]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_page_text(self, project, wiki_identifier, path=None, recursion_level=None, version_descriptor=None, include_content=None, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if wiki_identifier is not None:\n            route_values['wikiIdentifier'] = self._serialize.url('wiki_identifier', wiki_identifier, 'str')\n        query_parameters = {}\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        if recursion_level is not None:\n            query_parameters['recursionLevel'] = self._serialize.query('recursion_level', recursion_level, 'str')\n        if version_descriptor is not None:\n            if version_descriptor.version_type is not None:\n                query_parameters['versionDescriptor.versionType'] = version_descriptor.version_type\n            if version_descriptor.version is not None:\n                query_parameters['versionDescriptor.version'] = version_descriptor.version\n            if version_descriptor.version_options is not None:\n                query_parameters['versionDescriptor.versionOptions'] = version_descriptor.version_options\n        if include_content is not None:\n            query_parameters['includeContent'] = self._serialize.query('include_content', include_content, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='25d3fbc7-fe3d-46cb-b5a5-0b6f79caf27b',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='text/plain')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get page text for the provided path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting page by id text.", "response": "def get_page_by_id_text(self, project, wiki_identifier, id, recursion_level=None, include_content=None, **kwargs):\n        \"\"\"GetPageByIdText.\n        [Preview API] Gets metadata or content of the wiki page for the provided page id. Content negotiation is done based on the `Accept` header sent in the request.\n        :param str project: Project ID or project name\n        :param str wiki_identifier: Wiki Id or name.\n        :param int id: Wiki page id.\n        :param str recursion_level: Recursion level for subpages retrieval. Defaults to `None` (Optional).\n        :param bool include_content: True to include the content of the page in the response for Json content type. Defaults to false (Optional)\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if wiki_identifier is not None:\n            route_values['wikiIdentifier'] = self._serialize.url('wiki_identifier', wiki_identifier, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        query_parameters = {}\n        if recursion_level is not None:\n            query_parameters['recursionLevel'] = self._serialize.query('recursion_level', recursion_level, 'str')\n        if include_content is not None:\n            query_parameters['includeContent'] = self._serialize.query('include_content', include_content, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='ceddcf75-1068-452d-8b13-2d4d76e1f970',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='text/plain')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_wiki(self, update_parameters, wiki_identifier, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if wiki_identifier is not None:\n            route_values['wikiIdentifier'] = self._serialize.url('wiki_identifier', wiki_identifier, 'str')\n        content = self._serialize.body(update_parameters, 'WikiUpdateParameters')\n        response = self._send(http_method='PATCH',\n                              location_id='288d122c-dbd4-451d-aa5f-7dbbba070728',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WikiV2', response)", "response": "Update the wiki with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the client package.", "response": "def get_client(self, client_type):\n        \"\"\"GetClient.\n        [Preview API] Get the client package.\n        :param str client_type: Either \"EXE\" for a zip file containing a Windows symbol client (a.k.a. symbol.exe) along with dependencies, or \"TASK\" for a VSTS task that can be run on a VSTS build agent. All the other values are invalid. The parameter is case-insensitive.\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if client_type is not None:\n            route_values['clientType'] = self._serialize.url('client_type', client_type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='79c83865-4de3-460c-8a16-01be238e0818',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('object', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_requests(self, request_to_create):\n        content = self._serialize.body(request_to_create, 'Request')\n        response = self._send(http_method='POST',\n                              location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('Request', response)", "response": "Create a new symbol request."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a batch of debug entries for a symbol request.", "response": "def create_requests_request_id_debug_entries(self, batch, request_id, collection):\n        \"\"\"CreateRequestsRequestIdDebugEntries.\n        [Preview API] Create debug entries for a symbol request as specified by its identifier.\n        :param :class:`<DebugEntryCreateBatch> <azure.devops.v5_0.symbol.models.DebugEntryCreateBatch>` batch: A batch that contains debug entries to create.\n        :param str request_id: The symbol request identifier.\n        :param str collection: A valid debug entry collection name. Must be \"debugentries\".\n        :rtype: [DebugEntry]\n        \"\"\"\n        route_values = {}\n        if request_id is not None:\n            route_values['requestId'] = self._serialize.url('request_id', request_id, 'str')\n        query_parameters = {}\n        if collection is not None:\n            query_parameters['collection'] = self._serialize.query('collection', collection, 'str')\n        content = self._serialize.body(batch, 'DebugEntryCreateBatch')\n        response = self._send(http_method='POST',\n                              location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('[DebugEntry]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_requests_request_name_debug_entries(self, batch, request_name, collection):\n        query_parameters = {}\n        if request_name is not None:\n            query_parameters['requestName'] = self._serialize.query('request_name', request_name, 'str')\n        if collection is not None:\n            query_parameters['collection'] = self._serialize.query('collection', collection, 'str')\n        content = self._serialize.body(batch, 'DebugEntryCreateBatch')\n        response = self._send(http_method='POST',\n                              location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('[DebugEntry]', self._unwrap_collection(response))", "response": "Create a batch of debug entries for a symbol request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_requests_request_id(self, request_id, synchronous=None):\n        route_values = {}\n        if request_id is not None:\n            route_values['requestId'] = self._serialize.url('request_id', request_id, 'str')\n        query_parameters = {}\n        if synchronous is not None:\n            query_parameters['synchronous'] = self._serialize.query('synchronous', synchronous, 'bool')\n        self._send(http_method='DELETE',\n                   location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   query_parameters=query_parameters)", "response": "Delete a symbol request by request identifier."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a symbol request by request name.", "response": "def delete_requests_request_name(self, request_name, synchronous=None):\n        \"\"\"DeleteRequestsRequestName.\n        [Preview API] Delete a symbol request by request name.\n        :param str request_name:\n        :param bool synchronous: If true, delete all the debug entries under this request synchronously in the current session. If false, the deletion will be postponed to a later point and be executed automatically by the system.\n        \"\"\"\n        query_parameters = {}\n        if request_name is not None:\n            query_parameters['requestName'] = self._serialize.query('request_name', request_name, 'str')\n        if synchronous is not None:\n            query_parameters['synchronous'] = self._serialize.query('synchronous', synchronous, 'bool')\n        self._send(http_method='DELETE',\n                   location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                   version='5.0-preview.1',\n                   query_parameters=query_parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a symbol request by request identifier.", "response": "def get_requests_request_id(self, request_id):\n        \"\"\"GetRequestsRequestId.\n        [Preview API] Get a symbol request by request identifier.\n        :param str request_id: The symbol request identifier.\n        :rtype: :class:`<Request> <azure.devops.v5_0.symbol.models.Request>`\n        \"\"\"\n        route_values = {}\n        if request_id is not None:\n            route_values['requestId'] = self._serialize.url('request_id', request_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('Request', response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_requests_request_name(self, request_name):\n        query_parameters = {}\n        if request_name is not None:\n            query_parameters['requestName'] = self._serialize.query('request_name', request_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('Request', response)", "response": "Get a symbol request by request name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating a symbol request by request identifier.", "response": "def update_requests_request_id(self, update_request, request_id):\n        \"\"\"UpdateRequestsRequestId.\n        [Preview API] Update a symbol request by request identifier.\n        :param :class:`<Request> <azure.devops.v5_0.symbol.models.Request>` update_request: The symbol request.\n        :param str request_id: The symbol request identifier.\n        :rtype: :class:`<Request> <azure.devops.v5_0.symbol.models.Request>`\n        \"\"\"\n        route_values = {}\n        if request_id is not None:\n            route_values['requestId'] = self._serialize.url('request_id', request_id, 'str')\n        content = self._serialize.body(update_request, 'Request')\n        response = self._send(http_method='PATCH',\n                              location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Request', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_requests_request_name(self, update_request, request_name):\n        query_parameters = {}\n        if request_name is not None:\n            query_parameters['requestName'] = self._serialize.query('request_name', request_name, 'str')\n        content = self._serialize.body(update_request, 'Request')\n        response = self._send(http_method='PATCH',\n                              location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('Request', response)", "response": "Update a symbol request by request name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the best matched debug entry for a given client key.", "response": "def get_sym_srv_debug_entry_client_key(self, debug_entry_client_key):\n        \"\"\"GetSymSrvDebugEntryClientKey.\n        [Preview API] Given a client key, returns the best matched debug entry.\n        :param str debug_entry_client_key: A \"client key\" used by both ends of Microsoft's symbol protocol to identify a debug entry. The semantics of client key is governed by symsrv and is beyond the scope of this documentation.\n        \"\"\"\n        route_values = {}\n        if debug_entry_client_key is not None:\n            route_values['debugEntryClientKey'] = self._serialize.url('debug_entry_client_key', debug_entry_client_key, 'str')\n        self._send(http_method='GET',\n                   location_id='9648e256-c9f9-4f16-8a27-630b06396942',\n                   version='5.0-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget an operation from the operationId using the given pluginId.", "response": "def get_operation(self, operation_id, plugin_id=None):\n        \"\"\"GetOperation.\n        Gets an operation from the the operationId using the given pluginId.\n        :param str operation_id: The ID for the operation.\n        :param str plugin_id: The ID for the plugin.\n        :rtype: :class:`<Operation> <azure.devops.v5_0.operations.models.Operation>`\n        \"\"\"\n        route_values = {}\n        if operation_id is not None:\n            route_values['operationId'] = self._serialize.url('operation_id', operation_id, 'str')\n        query_parameters = {}\n        if plugin_id is not None:\n            query_parameters['pluginId'] = self._serialize.query('plugin_id', plugin_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='9a1b74b4-2ca8-4a9f-8470-c2f2e6fdc949',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Operation', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a package version from the recycle bin.", "response": "def delete_package_version_from_recycle_bin(self, feed_id, package_name, package_version):\n        \"\"\"DeletePackageVersionFromRecycleBin.\n        [Preview API] Delete a package version from the feed, moving it to the recycle bin.\n        :param str feed_id: Name or ID of the feed.\n        :param str package_name: Name of the package.\n        :param str package_version: Version of the package.\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_name is not None:\n            route_values['packageName'] = self._serialize.url('package_name', package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        self._send(http_method='DELETE',\n                   location_id='07143752-3d94-45fd-86c2-0c77ed87847b',\n                   version='5.0-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef restore_package_version_from_recycle_bin(self, package_version_details, feed_id, package_name, package_version):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_name is not None:\n            route_values['packageName'] = self._serialize.url('package_name', package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        content = self._serialize.body(package_version_details, 'PyPiRecycleBinPackageVersionDetails')\n        self._send(http_method='PATCH',\n                   location_id='07143752-3d94-45fd-86c2-0c77ed87847b',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   content=content)", "response": "Restores a package version from the recycle bin to its associated feed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_package_version(self, feed_id, package_name, package_version, show_deleted=None):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_name is not None:\n            route_values['packageName'] = self._serialize.url('package_name', package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        query_parameters = {}\n        if show_deleted is not None:\n            query_parameters['showDeleted'] = self._serialize.query('show_deleted', show_deleted, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='d146ac7e-9e3f-4448-b956-f9bb3bdf9b2e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Package', response)", "response": "Get information about a package version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_annotated_tag(self, tag_object, project, repository_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        content = self._serialize.body(tag_object, 'GitAnnotatedTag')\n        response = self._send(http_method='POST',\n                              location_id='5e8a8081-3851-4626-b677-9891cc04102e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitAnnotatedTag', response)", "response": "Create an annotated tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_blob(self, repository_id, sha1, project=None, download=None, file_name=None, resolve_lfs=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if sha1 is not None:\n            route_values['sha1'] = self._serialize.url('sha1', sha1, 'str')\n        query_parameters = {}\n        if download is not None:\n            query_parameters['download'] = self._serialize.query('download', download, 'bool')\n        if file_name is not None:\n            query_parameters['fileName'] = self._serialize.query('file_name', file_name, 'str')\n        if resolve_lfs is not None:\n            query_parameters['resolveLfs'] = self._serialize.query('resolve_lfs', resolve_lfs, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='7b28e929-2c99-405d-9c5c-6167a06e6816',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitBlobRef', response)", "response": "Get a single blob."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_branch(self, repository_id, name, project=None, base_version_descriptor=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if name is not None:\n            query_parameters['name'] = self._serialize.query('name', name, 'str')\n        if base_version_descriptor is not None:\n            if base_version_descriptor.version_type is not None:\n                query_parameters['baseVersionDescriptor.versionType'] = base_version_descriptor.version_type\n            if base_version_descriptor.version is not None:\n                query_parameters['baseVersionDescriptor.version'] = base_version_descriptor.version\n            if base_version_descriptor.version_options is not None:\n                query_parameters['baseVersionDescriptor.versionOptions'] = base_version_descriptor.version_options\n        response = self._send(http_method='GET',\n                              location_id='d5b216de-d8d5-4d32-ae76-51df755b16d3',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitBranchStats', response)", "response": "Get branch statistics about a single branch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_branches(self, repository_id, project=None, base_version_descriptor=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if base_version_descriptor is not None:\n            if base_version_descriptor.version_type is not None:\n                query_parameters['baseVersionDescriptor.versionType'] = base_version_descriptor.version_type\n            if base_version_descriptor.version is not None:\n                query_parameters['baseVersionDescriptor.version'] = base_version_descriptor.version\n            if base_version_descriptor.version_options is not None:\n                query_parameters['baseVersionDescriptor.versionOptions'] = base_version_descriptor.version_options\n        response = self._send(http_method='GET',\n                              location_id='d5b216de-d8d5-4d32-ae76-51df755b16d3',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitBranchStats]', self._unwrap_collection(response))", "response": "Get branches statistics about all branches in a repository."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the changes for a particular commit.", "response": "def get_changes(self, commit_id, repository_id, project=None, top=None, skip=None):\n        \"\"\"GetChanges.\n        Retrieve changes for a particular commit.\n        :param str commit_id: The id of the commit.\n        :param str repository_id: The id or friendly name of the repository. To use the friendly name, projectId must also be specified.\n        :param str project: Project ID or project name\n        :param int top: The maximum number of changes to return.\n        :param int skip: The number of changes to skip.\n        :rtype: :class:`<GitCommitChanges> <azure.devops.v5_0.git.models.GitCommitChanges>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if commit_id is not None:\n            route_values['commitId'] = self._serialize.url('commit_id', commit_id, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if top is not None:\n            query_parameters['top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['skip'] = self._serialize.query('skip', skip, 'int')\n        response = self._send(http_method='GET',\n                              location_id='5bf884f5-3e07-42e9-afb8-1b872267bf16',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitCommitChanges', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting commits for a specific project.", "response": "def get_commits(self, repository_id, search_criteria, project=None, skip=None, top=None):\n        \"\"\"GetCommits.\n        Retrieve git commits for a project\n        :param str repository_id: The id or friendly name of the repository. To use the friendly name, projectId must also be specified.\n        :param :class:`<GitQueryCommitsCriteria> <azure.devops.v5_0.git.models.GitQueryCommitsCriteria>` search_criteria:\n        :param str project: Project ID or project name\n        :param int skip:\n        :param int top:\n        :rtype: [GitCommitRef]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if search_criteria is not None:\n            if search_criteria.ids is not None:\n                query_parameters['searchCriteria.ids'] = search_criteria.ids\n            if search_criteria.from_date is not None:\n                query_parameters['searchCriteria.fromDate'] = search_criteria.from_date\n            if search_criteria.to_date is not None:\n                query_parameters['searchCriteria.toDate'] = search_criteria.to_date\n            if search_criteria.item_version is not None:\n                if search_criteria.item_version.version_type is not None:\n                    query_parameters['searchCriteria.itemVersion.versionType'] = search_criteria.item_version.version_type\n                if search_criteria.item_version.version is not None:\n                    query_parameters['searchCriteria.itemVersion.version'] = search_criteria.item_version.version\n                if search_criteria.item_version.version_options is not None:\n                    query_parameters['searchCriteria.itemVersion.versionOptions'] = search_criteria.item_version.version_options\n            if search_criteria.compare_version is not None:\n                if search_criteria.compare_version.version_type is not None:\n                    query_parameters['searchCriteria.compareVersion.versionType'] = search_criteria.compare_version.version_type\n                if search_criteria.compare_version.version is not None:\n                    query_parameters['searchCriteria.compareVersion.version'] = search_criteria.compare_version.version\n                if search_criteria.compare_version.version_options is not None:\n                    query_parameters['searchCriteria.compareVersion.versionOptions'] = search_criteria.compare_version.version_options\n            if search_criteria.from_commit_id is not None:\n                query_parameters['searchCriteria.fromCommitId'] = search_criteria.from_commit_id\n            if search_criteria.to_commit_id is not None:\n                query_parameters['searchCriteria.toCommitId'] = search_criteria.to_commit_id\n            if search_criteria.user is not None:\n                query_parameters['searchCriteria.user'] = search_criteria.user\n            if search_criteria.author is not None:\n                query_parameters['searchCriteria.author'] = search_criteria.author\n            if search_criteria.item_path is not None:\n                query_parameters['searchCriteria.itemPath'] = search_criteria.item_path\n            if search_criteria.exclude_deletes is not None:\n                query_parameters['searchCriteria.excludeDeletes'] = search_criteria.exclude_deletes\n            if search_criteria.skip is not None:\n                query_parameters['searchCriteria.$skip'] = search_criteria.skip\n            if search_criteria.top is not None:\n                query_parameters['searchCriteria.$top'] = search_criteria.top\n            if search_criteria.include_links is not None:\n                query_parameters['searchCriteria.includeLinks'] = search_criteria.include_links\n            if search_criteria.include_work_items is not None:\n                query_parameters['searchCriteria.includeWorkItems'] = search_criteria.include_work_items\n            if search_criteria.include_user_image_url is not None:\n                query_parameters['searchCriteria.includeUserImageUrl'] = search_criteria.include_user_image_url\n            if search_criteria.include_push_data is not None:\n                query_parameters['searchCriteria.includePushData'] = search_criteria.include_push_data\n            if search_criteria.history_mode is not None:\n                query_parameters['searchCriteria.historyMode'] = search_criteria.history_mode\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        response = self._send(http_method='GET',\n                              location_id='c2570c3b-5b3f-41b8-98bf-5407bfde8d58',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitCommitRef]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_push_commits(self, repository_id, push_id, project=None, top=None, skip=None, include_links=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if push_id is not None:\n            query_parameters['pushId'] = self._serialize.query('push_id', push_id, 'int')\n        if top is not None:\n            query_parameters['top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['skip'] = self._serialize.query('skip', skip, 'int')\n        if include_links is not None:\n            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='c2570c3b-5b3f-41b8-98bf-5407bfde8d58',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitCommitRef]', self._unwrap_collection(response))", "response": "Get a list of commits associated with a particular push."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets commits for a project matching the search criteria.", "response": "def get_commits_batch(self, search_criteria, repository_id, project=None, skip=None, top=None, include_statuses=None):\n        \"\"\"GetCommitsBatch.\n        Retrieve git commits for a project matching the search criteria\n        :param :class:`<GitQueryCommitsCriteria> <azure.devops.v5_0.git.models.GitQueryCommitsCriteria>` search_criteria: Search options\n        :param str repository_id: The name or ID of the repository.\n        :param str project: Project ID or project name\n        :param int skip: Number of commits to skip.\n        :param int top: Maximum number of commits to return.\n        :param bool include_statuses: True to include additional commit status information.\n        :rtype: [GitCommitRef]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if include_statuses is not None:\n            query_parameters['includeStatuses'] = self._serialize.query('include_statuses', include_statuses, 'bool')\n        content = self._serialize.body(search_criteria, 'GitQueryCommitsCriteria')\n        response = self._send(http_method='POST',\n                              location_id='6400dfb2-0bcb-462b-b992-5a57f8f1416c',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('[GitCommitRef]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_import_request(self, import_request, project, repository_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        content = self._serialize.body(import_request, 'GitImportRequest')\n        response = self._send(http_method='POST',\n                              location_id='01828ddc-3600-4a41-8633-99b3a73a0eb3',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitImportRequest', response)", "response": "Create an import request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying the import requests for a specific repository.", "response": "def query_import_requests(self, project, repository_id, include_abandoned=None):\n        \"\"\"QueryImportRequests.\n        [Preview API] Retrieve import requests for a repository.\n        :param str project: Project ID or project name\n        :param str repository_id: The name or ID of the repository.\n        :param bool include_abandoned: True to include abandoned import requests in the results.\n        :rtype: [GitImportRequest]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if include_abandoned is not None:\n            query_parameters['includeAbandoned'] = self._serialize.query('include_abandoned', include_abandoned, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='01828ddc-3600-4a41-8633-99b3a73a0eb3',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitImportRequest]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_import_request(self, import_request_to_update, project, repository_id, import_request_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if import_request_id is not None:\n            route_values['importRequestId'] = self._serialize.url('import_request_id', import_request_id, 'int')\n        content = self._serialize.body(import_request_to_update, 'GitImportRequest')\n        response = self._send(http_method='PATCH',\n                              location_id='01828ddc-3600-4a41-8633-99b3a73a0eb3',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitImportRequest', response)", "response": "Updates the contents of the specified import request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_item(self, repository_id, path, project=None, scope_path=None, recursion_level=None, include_content_metadata=None, latest_processed_change=None, download=None, version_descriptor=None, include_content=None, resolve_lfs=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        if scope_path is not None:\n            query_parameters['scopePath'] = self._serialize.query('scope_path', scope_path, 'str')\n        if recursion_level is not None:\n            query_parameters['recursionLevel'] = self._serialize.query('recursion_level', recursion_level, 'str')\n        if include_content_metadata is not None:\n            query_parameters['includeContentMetadata'] = self._serialize.query('include_content_metadata', include_content_metadata, 'bool')\n        if latest_processed_change is not None:\n            query_parameters['latestProcessedChange'] = self._serialize.query('latest_processed_change', latest_processed_change, 'bool')\n        if download is not None:\n            query_parameters['download'] = self._serialize.query('download', download, 'bool')\n        if version_descriptor is not None:\n            if version_descriptor.version_type is not None:\n                query_parameters['versionDescriptor.versionType'] = version_descriptor.version_type\n            if version_descriptor.version is not None:\n                query_parameters['versionDescriptor.version'] = version_descriptor.version\n            if version_descriptor.version_options is not None:\n                query_parameters['versionDescriptor.versionOptions'] = version_descriptor.version_options\n        if include_content is not None:\n            query_parameters['includeContent'] = self._serialize.query('include_content', include_content, 'bool')\n        if resolve_lfs is not None:\n            query_parameters['resolveLfs'] = self._serialize.query('resolve_lfs', resolve_lfs, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='fb93c0db-47ed-4a31-8c20-47552878fb44',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitItem', response)", "response": "Get Item Metadata and Content for a single item."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets Item Metadata and Content for a single item.", "response": "def get_item_content(self, repository_id, path, project=None, scope_path=None, recursion_level=None, include_content_metadata=None, latest_processed_change=None, download=None, version_descriptor=None, include_content=None, resolve_lfs=None, **kwargs):\n        \"\"\"GetItemContent.\n        Get Item Metadata and/or Content for a single item. The download parameter is to indicate whether the content should be available as a download or just sent as a stream in the response. Doesn't apply to zipped content, which is always returned as a download.\n        :param str repository_id: The name or ID of the repository.\n        :param str path: The item path.\n        :param str project: Project ID or project name\n        :param str scope_path: The path scope.  The default is null.\n        :param str recursion_level: The recursion level of this request. The default is 'none', no recursion.\n        :param bool include_content_metadata: Set to true to include content metadata.  Default is false.\n        :param bool latest_processed_change: Set to true to include the lastest changes.  Default is false.\n        :param bool download: Set to true to download the response as a file.  Default is false.\n        :param :class:`<GitVersionDescriptor> <azure.devops.v5_0.git.models.GitVersionDescriptor>` version_descriptor: Version descriptor.  Default is null.\n        :param bool include_content: Set to true to include item content when requesting json.  Default is false.\n        :param bool resolve_lfs: Set to true to resolve Git LFS pointer files to return actual content from Git LFS.  Default is false.\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        if scope_path is not None:\n            query_parameters['scopePath'] = self._serialize.query('scope_path', scope_path, 'str')\n        if recursion_level is not None:\n            query_parameters['recursionLevel'] = self._serialize.query('recursion_level', recursion_level, 'str')\n        if include_content_metadata is not None:\n            query_parameters['includeContentMetadata'] = self._serialize.query('include_content_metadata', include_content_metadata, 'bool')\n        if latest_processed_change is not None:\n            query_parameters['latestProcessedChange'] = self._serialize.query('latest_processed_change', latest_processed_change, 'bool')\n        if download is not None:\n            query_parameters['download'] = self._serialize.query('download', download, 'bool')\n        if version_descriptor is not None:\n            if version_descriptor.version_type is not None:\n                query_parameters['versionDescriptor.versionType'] = version_descriptor.version_type\n            if version_descriptor.version is not None:\n                query_parameters['versionDescriptor.version'] = version_descriptor.version\n            if version_descriptor.version_options is not None:\n                query_parameters['versionDescriptor.versionOptions'] = version_descriptor.version_options\n        if include_content is not None:\n            query_parameters['includeContent'] = self._serialize.query('include_content', include_content, 'bool')\n        if resolve_lfs is not None:\n            query_parameters['resolveLfs'] = self._serialize.query('resolve_lfs', resolve_lfs, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='fb93c0db-47ed-4a31-8c20-47552878fb44',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_merge_bases(self, repository_name_or_id, commit_id, other_commit_id, project=None, other_collection_id=None, other_repository_id=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_name_or_id is not None:\n            route_values['repositoryNameOrId'] = self._serialize.url('repository_name_or_id', repository_name_or_id, 'str')\n        if commit_id is not None:\n            route_values['commitId'] = self._serialize.url('commit_id', commit_id, 'str')\n        query_parameters = {}\n        if other_commit_id is not None:\n            query_parameters['otherCommitId'] = self._serialize.query('other_commit_id', other_commit_id, 'str')\n        if other_collection_id is not None:\n            query_parameters['otherCollectionId'] = self._serialize.query('other_collection_id', other_collection_id, 'str')\n        if other_repository_id is not None:\n            query_parameters['otherRepositoryId'] = self._serialize.query('other_repository_id', other_repository_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='7cf2abb6-c964-4f7e-9872-f78c66e72e9c',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitCommitRef]', self._unwrap_collection(response))", "response": "Get the merge bases of two commits optionally across forks."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a pull request iteration status on the pull request.", "response": "def create_pull_request_iteration_status(self, status, repository_id, pull_request_id, iteration_id, project=None):\n        \"\"\"CreatePullRequestIterationStatus.\n        [Preview API] Create a pull request status on the iteration. This operation will have the same result as Create status on pull request with specified iteration ID in the request body.\n        :param :class:`<GitPullRequestStatus> <azure.devops.v5_0.git.models.GitPullRequestStatus>` status: Pull request status to create.\n        :param str repository_id: The repository ID of the pull request\u2019s target branch.\n        :param int pull_request_id: ID of the pull request.\n        :param int iteration_id: ID of the pull request iteration.\n        :param str project: Project ID or project name\n        :rtype: :class:`<GitPullRequestStatus> <azure.devops.v5_0.git.models.GitPullRequestStatus>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        if iteration_id is not None:\n            route_values['iterationId'] = self._serialize.url('iteration_id', iteration_id, 'int')\n        content = self._serialize.body(status, 'GitPullRequestStatus')\n        response = self._send(http_method='POST',\n                              location_id='75cf11c5-979f-4038-a76e-058a06adf2bf',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitPullRequestStatus', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all the labels assigned to a pull request.", "response": "def get_pull_request_labels(self, repository_id, pull_request_id, project=None, project_id=None):\n        \"\"\"GetPullRequestLabels.\n        [Preview API] Get all the labels assigned to a pull request.\n        :param str repository_id: The repository ID of the pull request\u2019s target branch.\n        :param int pull_request_id: ID of the pull request.\n        :param str project: Project ID or project name\n        :param str project_id: Project ID or project name.\n        :rtype: [WebApiTagDefinition]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        query_parameters = {}\n        if project_id is not None:\n            query_parameters['projectId'] = self._serialize.query('project_id', project_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='f22387e3-984e-4c52-9c6d-fbb8f14c812d',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[WebApiTagDefinition]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the votes of multiple reviewers on a pull request.", "response": "def update_pull_request_reviewers(self, patch_votes, repository_id, pull_request_id, project=None):\n        \"\"\"UpdatePullRequestReviewers.\n        Reset the votes of multiple reviewers on a pull request.  NOTE: This endpoint only supports updating votes, but does not support updating required reviewers (use policy) or display names.\n        :param [IdentityRefWithVote] patch_votes: IDs of the reviewers whose votes will be reset to zero\n        :param str repository_id: The repository ID of the pull request\u2019s target branch.\n        :param int pull_request_id: ID of the pull request\n        :param str project: Project ID or project name\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        content = self._serialize.body(patch_votes, '[IdentityRefWithVote]')\n        self._send(http_method='PATCH',\n                   location_id='4b6702c7-aa35-4b89-9c96-b9abf6d3e540',\n                   version='5.0',\n                   route_values=route_values,\n                   content=content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends an e - mail notification about a specific pull request.", "response": "def share_pull_request(self, user_message, repository_id, pull_request_id, project=None):\n        \"\"\"SharePullRequest.\n        [Preview API] Sends an e-mail notification about a specific pull request to a set of recipients\n        :param :class:`<ShareNotificationContext> <azure.devops.v5_0.git.models.ShareNotificationContext>` user_message:\n        :param str repository_id: ID of the git repository.\n        :param int pull_request_id: ID of the pull request.\n        :param str project: Project ID or project name\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        content = self._serialize.body(user_message, 'ShareNotificationContext')\n        self._send(http_method='POST',\n                   location_id='696f3a82-47c9-487f-9117-b9d00972ca84',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   content=content)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new push entry in the repository.", "response": "def create_push(self, push, repository_id, project=None):\n        \"\"\"CreatePush.\n        Push changes to the repository.\n        :param :class:`<GitPush> <azure.devops.v5_0.git.models.GitPush>` push:\n        :param str repository_id: The name or ID of the repository.\n        :param str project: Project ID or project name\n        :rtype: :class:`<GitPush> <azure.devops.v5_0.git.models.GitPush>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        content = self._serialize.body(push, 'GitPush')\n        response = self._send(http_method='POST',\n                              location_id='ea98d07b-3c87-4971-8ede-a613694ffb55',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitPush', response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_push(self, repository_id, push_id, project=None, include_commits=None, include_ref_updates=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if push_id is not None:\n            route_values['pushId'] = self._serialize.url('push_id', push_id, 'int')\n        query_parameters = {}\n        if include_commits is not None:\n            query_parameters['includeCommits'] = self._serialize.query('include_commits', include_commits, 'int')\n        if include_ref_updates is not None:\n            query_parameters['includeRefUpdates'] = self._serialize.query('include_ref_updates', include_ref_updates, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='ea98d07b-3c87-4971-8ede-a613694ffb55',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitPush', response)", "response": "Get a specific push."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the refs of a specific branch.", "response": "def update_refs(self, ref_updates, repository_id, project=None, project_id=None):\n        \"\"\"UpdateRefs.\n        Creating, updating, or deleting refs(branches).\n        :param [GitRefUpdate] ref_updates: List of ref updates to attempt to perform\n        :param str repository_id: The name or ID of the repository.\n        :param str project: Project ID or project name\n        :param str project_id: ID or name of the team project. Optional if specifying an ID for repository.\n        :rtype: [GitRefUpdateResult]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if project_id is not None:\n            query_parameters['projectId'] = self._serialize.query('project_id', project_id, 'str')\n        content = self._serialize.body(ref_updates, '[GitRefUpdate]')\n        response = self._send(http_method='POST',\n                              location_id='2d874a60-a811-4f62-9c9f-963a6ea0a55b',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('[GitRefUpdateResult]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a ref favorite.", "response": "def create_favorite(self, favorite, project):\n        \"\"\"CreateFavorite.\n        [Preview API] Creates a ref favorite\n        :param :class:`<GitRefFavorite> <azure.devops.v5_0.git.models.GitRefFavorite>` favorite: The ref favorite to create.\n        :param str project: Project ID or project name\n        :rtype: :class:`<GitRefFavorite> <azure.devops.v5_0.git.models.GitRefFavorite>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(favorite, 'GitRefFavorite')\n        response = self._send(http_method='POST',\n                              location_id='876f70af-5792-485a-a1c7-d0a7b2f42bbb',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitRefFavorite', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a git repository with parent.", "response": "def get_repository_with_parent(self, repository_id, include_parent, project=None):\n        \"\"\"GetRepositoryWithParent.\n        Retrieve a git repository.\n        :param str repository_id: The name or ID of the repository.\n        :param bool include_parent: True to include parent repository. Only available in authenticated calls.\n        :param str project: Project ID or project name\n        :rtype: :class:`<GitRepository> <azure.devops.v5_0.git.models.GitRepository>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if include_parent is not None:\n            query_parameters['includeParent'] = self._serialize.query('include_parent', include_parent, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='225f7195-f9c7-4d14-ab28-a83f7ff77e1f',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitRepository', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_process_template(self, id, **kwargs):\n        route_values = {}\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'str')\n        route_values['action'] = 'Export'\n        response = self._send(http_method='GET',\n                              location_id='29e1f38d-9e9c-4358-86a5-cdf9896a5759',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              accept_media_type='application/zip')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Returns the process template with the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports a process from a zip file.", "response": "def import_process_template(self, upload_stream, ignore_warnings=None, **kwargs):\n        \"\"\"ImportProcessTemplate.\n        [Preview API] Imports a process from zip file.\n        :param object upload_stream: Stream to upload\n        :param bool ignore_warnings: Default value is false\n        :rtype: :class:`<ProcessImportResult> <azure.devops.v5_0.work_item_tracking_process_template.models.ProcessImportResult>`\n        \"\"\"\n        route_values = {}\n        route_values['action'] = 'Import'\n        query_parameters = {}\n        if ignore_warnings is not None:\n            query_parameters['ignoreWarnings'] = self._serialize.query('ignore_warnings', ignore_warnings, 'bool')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        content = self._client.stream_upload(upload_stream, callback=callback)\n        response = self._send(http_method='POST',\n                              location_id='29e1f38d-9e9c-4358-86a5-cdf9896a5759',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content,\n                              media_type='application/octet-stream')\n        return self._deserialize('ProcessImportResult', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef import_process_template_status(self, id):\n        route_values = {}\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'str')\n        route_values['action'] = 'Status'\n        response = self._send(http_method='GET',\n                              location_id='29e1f38d-9e9c-4358-86a5-cdf9896a5759',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('ProcessPromoteStatus', response)", "response": "This method returns the status of the process template with the specified ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npublishes the events to the API.", "response": "def publish_events(self, events):\n        \"\"\"PublishEvents.\n        [Preview API]\n        :param [CustomerIntelligenceEvent] events:\n        \"\"\"\n        content = self._serialize.body(events, '[CustomerIntelligenceEvent]')\n        self._send(http_method='POST',\n                   location_id='b5cc35c2-ff2b-491d-a085-24b6e9f396fd',\n                   version='5.0-preview.1',\n                   content=content)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_connection_data(self, connect_options=None, last_change_id=None, last_change_id64=None):\n        query_parameters = {}\n        if connect_options is not None:\n            query_parameters['connectOptions'] = self._serialize.query('connect_options', connect_options, 'str')\n        if last_change_id is not None:\n            query_parameters['lastChangeId'] = self._serialize.query('last_change_id', last_change_id, 'int')\n        if last_change_id64 is not None:\n            query_parameters['lastChangeId64'] = self._serialize.query('last_change_id64', last_change_id64, 'long')\n        response = self._send(http_method='GET',\n                              location_id='00d9565f-ed9c-4a06-9a50-00e7896ccab4',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('ConnectionData', response)", "response": "GetConnectionData.\n        [Preview API] This was copied and adapted from TeamFoundationConnectionService.Connect()\n        :param str connect_options:\n        :param int last_change_id: Obsolete 32-bit LastChangeId\n        :param long last_change_id64: Non-truncated 64-bit LastChangeId\n        :rtype: :class:`<ConnectionData> <azure.devops.v5_0.location.models.ConnectionData>`"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_resource_area(self, area_id, enterprise_name=None, organization_name=None):\n        route_values = {}\n        if area_id is not None:\n            route_values['areaId'] = self._serialize.url('area_id', area_id, 'str')\n        query_parameters = {}\n        if enterprise_name is not None:\n            query_parameters['enterpriseName'] = self._serialize.query('enterprise_name', enterprise_name, 'str')\n        if organization_name is not None:\n            query_parameters['organizationName'] = self._serialize.query('organization_name', organization_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e81700f7-3be2-46de-8624-2eb35882fcaa',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ResourceAreaInfo', response)", "response": "Get Resource Area Info."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets Resource Area by Host.", "response": "def get_resource_area_by_host(self, area_id, host_id):\n        \"\"\"GetResourceAreaByHost.\n        [Preview API]\n        :param str area_id:\n        :param str host_id:\n        :rtype: :class:`<ResourceAreaInfo> <azure.devops.v5_0.location.models.ResourceAreaInfo>`\n        \"\"\"\n        route_values = {}\n        if area_id is not None:\n            route_values['areaId'] = self._serialize.url('area_id', area_id, 'str')\n        query_parameters = {}\n        if host_id is not None:\n            query_parameters['hostId'] = self._serialize.query('host_id', host_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e81700f7-3be2-46de-8624-2eb35882fcaa',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ResourceAreaInfo', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting Resource Area Info.", "response": "def get_resource_areas(self, enterprise_name=None, organization_name=None):\n        \"\"\"GetResourceAreas.\n        [Preview API]\n        :param str enterprise_name:\n        :param str organization_name:\n        :rtype: [ResourceAreaInfo]\n        \"\"\"\n        query_parameters = {}\n        if enterprise_name is not None:\n            query_parameters['enterpriseName'] = self._serialize.query('enterprise_name', enterprise_name, 'str')\n        if organization_name is not None:\n            query_parameters['organizationName'] = self._serialize.query('organization_name', organization_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e81700f7-3be2-46de-8624-2eb35882fcaa',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('[ResourceAreaInfo]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_resource_areas_by_host(self, host_id):\n        query_parameters = {}\n        if host_id is not None:\n            query_parameters['hostId'] = self._serialize.query('host_id', host_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e81700f7-3be2-46de-8624-2eb35882fcaa',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('[ResourceAreaInfo]', self._unwrap_collection(response))", "response": "Get resource areas by host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a specific service definition.", "response": "def delete_service_definition(self, service_type, identifier):\n        \"\"\"DeleteServiceDefinition.\n        [Preview API]\n        :param str service_type:\n        :param str identifier:\n        \"\"\"\n        route_values = {}\n        if service_type is not None:\n            route_values['serviceType'] = self._serialize.url('service_type', service_type, 'str')\n        if identifier is not None:\n            route_values['identifier'] = self._serialize.url('identifier', identifier, 'str')\n        self._send(http_method='DELETE',\n                   location_id='d810a47d-f4f4-4a62-a03f-fa1860585c4c',\n                   version='5.0-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the service definitions.", "response": "def update_service_definitions(self, service_definitions):\n        \"\"\"UpdateServiceDefinitions.\n        [Preview API]\n        :param :class:`<VssJsonCollectionWrapper> <azure.devops.v5_0.location.models.VssJsonCollectionWrapper>` service_definitions:\n        \"\"\"\n        content = self._serialize.body(service_definitions, 'VssJsonCollectionWrapper')\n        self._send(http_method='PATCH',\n                   location_id='d810a47d-f4f4-4a62-a03f-fa1860585c4c',\n                   version='5.0-preview.1',\n                   content=content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_accounts(self, owner_id=None, member_id=None, properties=None):\n        query_parameters = {}\n        if owner_id is not None:\n            query_parameters['ownerId'] = self._serialize.query('owner_id', owner_id, 'str')\n        if member_id is not None:\n            query_parameters['memberId'] = self._serialize.query('member_id', member_id, 'str')\n        if properties is not None:\n            query_parameters['properties'] = self._serialize.query('properties', properties, 'str')\n        response = self._send(http_method='GET',\n                              location_id='229a6a53-b428-4ffb-a835-e8f36b5b4b1e',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[Account]', self._unwrap_collection(response))", "response": "Get a list of accounts for a specific owner or a specific member."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the backlog configuration for a team.", "response": "def get_backlog_configurations(self, team_context):\n        \"\"\"GetBacklogConfigurations.\n        Gets backlog configuration for a team\n        :param :class:`<TeamContext> <azure.devops.v5_0.work.models.TeamContext>` team_context: The team context for the operation\n        :rtype: :class:`<BacklogConfiguration> <azure.devops.v5_0.work.models.BacklogConfiguration>`\n        \"\"\"\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        response = self._send(http_method='GET',\n                              location_id='7799f497-3cb5-4f16-ad4f-5cd06012db64',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('BacklogConfiguration', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the list of parent field filter models for the given list of workitem ids.", "response": "def get_board_mapping_parent_items(self, team_context, child_backlog_context_category_ref_name, workitem_ids):\n        \"\"\"GetBoardMappingParentItems.\n        [Preview API] Returns the list of parent field filter model for the given list of workitem ids\n        :param :class:`<TeamContext> <azure.devops.v5_0.work.models.TeamContext>` team_context: The team context for the operation\n        :param str child_backlog_context_category_ref_name:\n        :param [int] workitem_ids:\n        :rtype: [ParentChildWIMap]\n        \"\"\"\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        query_parameters = {}\n        if child_backlog_context_category_ref_name is not None:\n            query_parameters['childBacklogContextCategoryRefName'] = self._serialize.query('child_backlog_context_category_ref_name', child_backlog_context_category_ref_name, 'str')\n        if workitem_ids is not None:\n            workitem_ids = \",\".join(map(str, workitem_ids))\n            query_parameters['workitemIds'] = self._serialize.query('workitem_ids', workitem_ids, 'str')\n        response = self._send(http_method='GET',\n                              location_id='186abea3-5c35-432f-9e28-7a15b4312a0e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[ParentChildWIMap]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates board user settings for the board id.", "response": "def update_board_user_settings(self, board_user_settings, team_context, board):\n        \"\"\"UpdateBoardUserSettings.\n        [Preview API] Update board user settings for the board id\n        :param {str} board_user_settings:\n        :param :class:`<TeamContext> <azure.devops.v5_0.work.models.TeamContext>` team_context: The team context for the operation\n        :param str board:\n        :rtype: :class:`<BoardUserSettings> <azure.devops.v5_0.work.models.BoardUserSettings>`\n        \"\"\"\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if board is not None:\n            route_values['board'] = self._serialize.url('board', board, 'str')\n        content = self._serialize.body(board_user_settings, '{str}')\n        response = self._send(http_method='PATCH',\n                              location_id='b30d9f58-1891-4b0a-b168-c46408f919b0',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('BoardUserSettings', response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_capacities(self, capacities, team_context, iteration_id):\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if iteration_id is not None:\n            route_values['iterationId'] = self._serialize.url('iteration_id', iteration_id, 'str')\n        content = self._serialize.body(capacities, '[TeamMemberCapacity]')\n        response = self._send(http_method='PUT',\n                              location_id='74412d15-8c1a-4352-a48d-ef1ed5587d57',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[TeamMemberCapacity]', self._unwrap_collection(response))", "response": "Replace a team s capacity with a new one."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_board_chart(self, chart, team_context, board, name):\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if board is not None:\n            route_values['board'] = self._serialize.url('board', board, 'str')\n        if name is not None:\n            route_values['name'] = self._serialize.url('name', name, 'str')\n        content = self._serialize.body(chart, 'BoardChart')\n        response = self._send(http_method='PATCH',\n                              location_id='45fe888c-239e-49fd-958c-df1a1ab21d97',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('BoardChart', response)", "response": "Update a board chart."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_delivery_timeline_data(self, project, id, revision=None, start_date=None, end_date=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'str')\n        query_parameters = {}\n        if revision is not None:\n            query_parameters['revision'] = self._serialize.query('revision', revision, 'int')\n        if start_date is not None:\n            query_parameters['startDate'] = self._serialize.query('start_date', start_date, 'iso-8601')\n        if end_date is not None:\n            query_parameters['endDate'] = self._serialize.query('end_date', end_date, 'iso-8601')\n        response = self._send(http_method='GET',\n                              location_id='bdd0834e-101f-49f0-a6ae-509f384a12b4',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('DeliveryViewData', response)", "response": "Get delivery timeline data for a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_team_iteration(self, team_context, id):\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='c9175577-28a1-4b06-9197-8636af9f64ad',\n                   version='5.0',\n                   route_values=route_values)", "response": "Delete a team s iteration by iterationId."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npost an iteration to the team.", "response": "def post_team_iteration(self, iteration, team_context):\n        \"\"\"PostTeamIteration.\n        Add an iteration to the team\n        :param :class:`<TeamSettingsIteration> <azure.devops.v5_0.work.models.TeamSettingsIteration>` iteration: Iteration to add\n        :param :class:`<TeamContext> <azure.devops.v5_0.work.models.TeamContext>` team_context: The team context for the operation\n        :rtype: :class:`<TeamSettingsIteration> <azure.devops.v5_0.work.models.TeamSettingsIteration>`\n        \"\"\"\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        content = self._serialize.body(iteration, 'TeamSettingsIteration')\n        response = self._send(http_method='POST',\n                              location_id='c9175577-28a1-4b06-9197-8636af9f64ad',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('TeamSettingsIteration', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_plan(self, posted_plan, project):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(posted_plan, 'CreatePlan')\n        response = self._send(http_method='POST',\n                              location_id='0b42cb47-cd73-4810-ac90-19c9ba147453',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Plan', response)", "response": "Create a new plan for the team."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_plan(self, updated_plan, project, id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'str')\n        content = self._serialize.body(updated_plan, 'UpdatePlan')\n        response = self._send(http_method='PUT',\n                              location_id='0b42cb47-cd73-4810-ac90-19c9ba147453',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Plan', response)", "response": "Update the information for the specified plan."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_board_rows(self, board_rows, team_context, board):\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if board is not None:\n            route_values['board'] = self._serialize.url('board', board, 'str')\n        content = self._serialize.body(board_rows, '[BoardRow]')\n        response = self._send(http_method='PUT',\n                              location_id='0863355d-aefd-4d63-8669-984c9b7b0e78',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[BoardRow]', self._unwrap_collection(response))", "response": "Update the board rows on a specific board."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_team_days_off(self, days_off_patch, team_context, iteration_id):\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if iteration_id is not None:\n            route_values['iterationId'] = self._serialize.url('iteration_id', iteration_id, 'str')\n        content = self._serialize.body(days_off_patch, 'TeamSettingsDaysOffPatch')\n        response = self._send(http_method='PATCH',\n                              location_id='2d4faa2e-9150-4cbf-a47a-932b1b4a0773',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('TeamSettingsDaysOff', response)", "response": "Updates the team s days off for a specific iteration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an artifact in a build.", "response": "def create_artifact(self, artifact, project, build_id):\n        \"\"\"CreateArtifact.\n        Associates an artifact with a build.\n        :param :class:`<BuildArtifact> <azure.devops.v5_0.build.models.BuildArtifact>` artifact: The artifact.\n        :param str project: Project ID or project name\n        :param int build_id: The ID of the build.\n        :rtype: :class:`<BuildArtifact> <azure.devops.v5_0.build.models.BuildArtifact>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        content = self._serialize.body(artifact, 'BuildArtifact')\n        response = self._send(http_method='POST',\n                              location_id='1db06c96-014e-44e1-ac91-90b2d4b3e984',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('BuildArtifact', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_artifact(self, project, build_id, artifact_name):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        query_parameters = {}\n        if artifact_name is not None:\n            query_parameters['artifactName'] = self._serialize.query('artifact_name', artifact_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='1db06c96-014e-44e1-ac91-90b2d4b3e984',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('BuildArtifact', response)", "response": "Get a specific artifact for a build."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_file(self, project, build_id, artifact_name, file_id, file_name, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        query_parameters = {}\n        if artifact_name is not None:\n            query_parameters['artifactName'] = self._serialize.query('artifact_name', artifact_name, 'str')\n        if file_id is not None:\n            query_parameters['fileId'] = self._serialize.query('file_id', file_id, 'str')\n        if file_name is not None:\n            query_parameters['fileName'] = self._serialize.query('file_name', file_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='1db06c96-014e-44e1-ac91-90b2d4b3e984',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get a file from the build."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_attachment(self, project, build_id, timeline_id, record_id, type, name, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        if timeline_id is not None:\n            route_values['timelineId'] = self._serialize.url('timeline_id', timeline_id, 'str')\n        if record_id is not None:\n            route_values['recordId'] = self._serialize.url('record_id', record_id, 'str')\n        if type is not None:\n            route_values['type'] = self._serialize.url('type', type, 'str')\n        if name is not None:\n            route_values['name'] = self._serialize.url('name', name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='af5122d3-3438-485e-a25a-2dbbfde84ee6',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get a specific attachment."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets build badge data.", "response": "def get_build_badge_data(self, project, repo_type, repo_id=None, branch_name=None):\n        \"\"\"GetBuildBadgeData.\n        [Preview API] Gets a badge that indicates the status of the most recent build for the specified branch.\n        :param str project: Project ID or project name\n        :param str repo_type: The repository type.\n        :param str repo_id: The repository ID.\n        :param str branch_name: The branch name.\n        :rtype: str\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repo_type is not None:\n            route_values['repoType'] = self._serialize.url('repo_type', repo_type, 'str')\n        query_parameters = {}\n        if repo_id is not None:\n            query_parameters['repoId'] = self._serialize.query('repo_id', repo_id, 'str')\n        if branch_name is not None:\n            query_parameters['branchName'] = self._serialize.query('branch_name', branch_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='21b3b9ce-fad5-4567-9ad0-80679794e003',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('str', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a build from the server.", "response": "def get_build(self, project, build_id, property_filters=None):\n        \"\"\"GetBuild.\n        Gets a build\n        :param str project: Project ID or project name\n        :param int build_id:\n        :param str property_filters:\n        :rtype: :class:`<Build> <azure.devops.v5_0.build.models.Build>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        query_parameters = {}\n        if property_filters is not None:\n            query_parameters['propertyFilters'] = self._serialize.query('property_filters', property_filters, 'str')\n        response = self._send(http_method='GET',\n                              location_id='0cd358e1-9217-4d94-8269-1c1ee6f93dcf',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Build', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of builds for a given project.", "response": "def get_builds(self, project, definitions=None, queues=None, build_number=None, min_time=None, max_time=None, requested_for=None, reason_filter=None, status_filter=None, result_filter=None, tag_filters=None, properties=None, top=None, continuation_token=None, max_builds_per_definition=None, deleted_filter=None, query_order=None, branch_name=None, build_ids=None, repository_id=None, repository_type=None):\n        \"\"\"GetBuilds.\n        Gets a list of builds.\n        :param str project: Project ID or project name\n        :param [int] definitions: A comma-delimited list of definition IDs. If specified, filters to builds for these definitions.\n        :param [int] queues: A comma-delimited list of queue IDs. If specified, filters to builds that ran against these queues.\n        :param str build_number: If specified, filters to builds that match this build number. Append * to do a prefix search.\n        :param datetime min_time: If specified, filters to builds that finished/started/queued after this date based on the queryOrder specified.\n        :param datetime max_time: If specified, filters to builds that finished/started/queued before this date based on the queryOrder specified.\n        :param str requested_for: If specified, filters to builds requested for the specified user.\n        :param str reason_filter: If specified, filters to builds that match this reason.\n        :param str status_filter: If specified, filters to builds that match this status.\n        :param str result_filter: If specified, filters to builds that match this result.\n        :param [str] tag_filters: A comma-delimited list of tags. If specified, filters to builds that have the specified tags.\n        :param [str] properties: A comma-delimited list of properties to retrieve.\n        :param int top: The maximum number of builds to return.\n        :param str continuation_token: A continuation token, returned by a previous call to this method, that can be used to return the next set of builds.\n        :param int max_builds_per_definition: The maximum number of builds to return per definition.\n        :param str deleted_filter: Indicates whether to exclude, include, or only return deleted builds.\n        :param str query_order: The order in which builds should be returned.\n        :param str branch_name: If specified, filters to builds that built branches that built this branch.\n        :param [int] build_ids: A comma-delimited list that specifies the IDs of builds to retrieve.\n        :param str repository_id: If specified, filters to builds that built from this repository.\n        :param str repository_type: If specified, filters to builds that built from repositories of this type.\n        :rtype: [Build]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if definitions is not None:\n            definitions = \",\".join(map(str, definitions))\n            query_parameters['definitions'] = self._serialize.query('definitions', definitions, 'str')\n        if queues is not None:\n            queues = \",\".join(map(str, queues))\n            query_parameters['queues'] = self._serialize.query('queues', queues, 'str')\n        if build_number is not None:\n            query_parameters['buildNumber'] = self._serialize.query('build_number', build_number, 'str')\n        if min_time is not None:\n            query_parameters['minTime'] = self._serialize.query('min_time', min_time, 'iso-8601')\n        if max_time is not None:\n            query_parameters['maxTime'] = self._serialize.query('max_time', max_time, 'iso-8601')\n        if requested_for is not None:\n            query_parameters['requestedFor'] = self._serialize.query('requested_for', requested_for, 'str')\n        if reason_filter is not None:\n            query_parameters['reasonFilter'] = self._serialize.query('reason_filter', reason_filter, 'str')\n        if status_filter is not None:\n            query_parameters['statusFilter'] = self._serialize.query('status_filter', status_filter, 'str')\n        if result_filter is not None:\n            query_parameters['resultFilter'] = self._serialize.query('result_filter', result_filter, 'str')\n        if tag_filters is not None:\n            tag_filters = \",\".join(tag_filters)\n            query_parameters['tagFilters'] = self._serialize.query('tag_filters', tag_filters, 'str')\n        if properties is not None:\n            properties = \",\".join(properties)\n            query_parameters['properties'] = self._serialize.query('properties', properties, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        if max_builds_per_definition is not None:\n            query_parameters['maxBuildsPerDefinition'] = self._serialize.query('max_builds_per_definition', max_builds_per_definition, 'int')\n        if deleted_filter is not None:\n            query_parameters['deletedFilter'] = self._serialize.query('deleted_filter', deleted_filter, 'str')\n        if query_order is not None:\n            query_parameters['queryOrder'] = self._serialize.query('query_order', query_order, 'str')\n        if branch_name is not None:\n            query_parameters['branchName'] = self._serialize.query('branch_name', branch_name, 'str')\n        if build_ids is not None:\n            build_ids = \",\".join(map(str, build_ids))\n            query_parameters['buildIds'] = self._serialize.query('build_ids', build_ids, 'str')\n        if repository_id is not None:\n            query_parameters['repositoryId'] = self._serialize.query('repository_id', repository_id, 'str')\n        if repository_type is not None:\n            query_parameters['repositoryType'] = self._serialize.query('repository_type', repository_type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='0cd358e1-9217-4d94-8269-1c1ee6f93dcf',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[Build]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef queue_build(self, build, project, ignore_warnings=None, check_in_ticket=None, source_build_id=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if ignore_warnings is not None:\n            query_parameters['ignoreWarnings'] = self._serialize.query('ignore_warnings', ignore_warnings, 'bool')\n        if check_in_ticket is not None:\n            query_parameters['checkInTicket'] = self._serialize.query('check_in_ticket', check_in_ticket, 'str')\n        if source_build_id is not None:\n            query_parameters['sourceBuildId'] = self._serialize.query('source_build_id', source_build_id, 'int')\n        content = self._serialize.body(build, 'Build')\n        response = self._send(http_method='POST',\n                              location_id='0cd358e1-9217-4d94-8269-1c1ee6f93dcf',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('Build', response)", "response": "Queues a build to be used for building."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_builds(self, builds, project):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(builds, '[Build]')\n        response = self._send(http_method='PATCH',\n                              location_id='0cd358e1-9217-4d94-8269-1c1ee6f93dcf',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[Build]', self._unwrap_collection(response))", "response": "UpdateBuilds.\n        Updates multiple builds.\n        :param [Build] builds: The builds to update.\n        :param str project: Project ID or project name\n        :rtype: [Build]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_changes_between_builds(self, project, from_build_id=None, to_build_id=None, top=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if from_build_id is not None:\n            query_parameters['fromBuildId'] = self._serialize.query('from_build_id', from_build_id, 'int')\n        if to_build_id is not None:\n            query_parameters['toBuildId'] = self._serialize.query('to_build_id', to_build_id, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        response = self._send(http_method='GET',\n                              location_id='f10f0ea5-18a1-43ec-a8fb-2042c7be9b43',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[Change]', self._unwrap_collection(response))", "response": "Get the changes made to the repository between two given builds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a build controller by its ID.", "response": "def get_build_controller(self, controller_id):\n        \"\"\"GetBuildController.\n        Gets a controller\n        :param int controller_id:\n        :rtype: :class:`<BuildController> <azure.devops.v5_0.build.models.BuildController>`\n        \"\"\"\n        route_values = {}\n        if controller_id is not None:\n            route_values['controllerId'] = self._serialize.url('controller_id', controller_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='fcac1932-2ee1-437f-9b6f-7f696be858f6',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('BuildController', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_build_controllers(self, name=None):\n        query_parameters = {}\n        if name is not None:\n            query_parameters['name'] = self._serialize.query('name', name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='fcac1932-2ee1-437f-9b6f-7f696be858f6',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[BuildController]', self._unwrap_collection(response))", "response": "Get build controllers by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_definition(self, definition, project, definition_to_clone_id=None, definition_to_clone_revision=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if definition_to_clone_id is not None:\n            query_parameters['definitionToCloneId'] = self._serialize.query('definition_to_clone_id', definition_to_clone_id, 'int')\n        if definition_to_clone_revision is not None:\n            query_parameters['definitionToCloneRevision'] = self._serialize.query('definition_to_clone_revision', definition_to_clone_revision, 'int')\n        content = self._serialize.body(definition, 'BuildDefinition')\n        response = self._send(http_method='POST',\n                              location_id='dbeaf647-6167-421a-bda9-c9327b25e2e6',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('BuildDefinition', response)", "response": "Creates a new definition."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_definition(self, project, definition_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='dbeaf647-6167-421a-bda9-c9327b25e2e6',\n                   version='5.0',\n                   route_values=route_values)", "response": "Delete a definition and all associated builds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a specific build definition.", "response": "def get_definition(self, project, definition_id, revision=None, min_metrics_time=None, property_filters=None, include_latest_builds=None):\n        \"\"\"GetDefinition.\n        Gets a definition, optionally at a specific revision.\n        :param str project: Project ID or project name\n        :param int definition_id: The ID of the definition.\n        :param int revision: The revision number to retrieve. If this is not specified, the latest version will be returned.\n        :param datetime min_metrics_time: If specified, indicates the date from which metrics should be included.\n        :param [str] property_filters: A comma-delimited list of properties to include in the results.\n        :param bool include_latest_builds:\n        :rtype: :class:`<BuildDefinition> <azure.devops.v5_0.build.models.BuildDefinition>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        query_parameters = {}\n        if revision is not None:\n            query_parameters['revision'] = self._serialize.query('revision', revision, 'int')\n        if min_metrics_time is not None:\n            query_parameters['minMetricsTime'] = self._serialize.query('min_metrics_time', min_metrics_time, 'iso-8601')\n        if property_filters is not None:\n            property_filters = \",\".join(property_filters)\n            query_parameters['propertyFilters'] = self._serialize.query('property_filters', property_filters, 'str')\n        if include_latest_builds is not None:\n            query_parameters['includeLatestBuilds'] = self._serialize.query('include_latest_builds', include_latest_builds, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='dbeaf647-6167-421a-bda9-c9327b25e2e6',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('BuildDefinition', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_definitions(self, project, name=None, repository_id=None, repository_type=None, query_order=None, top=None, continuation_token=None, min_metrics_time=None, definition_ids=None, path=None, built_after=None, not_built_after=None, include_all_properties=None, include_latest_builds=None, task_id_filter=None, process_type=None, yaml_filename=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if name is not None:\n            query_parameters['name'] = self._serialize.query('name', name, 'str')\n        if repository_id is not None:\n            query_parameters['repositoryId'] = self._serialize.query('repository_id', repository_id, 'str')\n        if repository_type is not None:\n            query_parameters['repositoryType'] = self._serialize.query('repository_type', repository_type, 'str')\n        if query_order is not None:\n            query_parameters['queryOrder'] = self._serialize.query('query_order', query_order, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        if min_metrics_time is not None:\n            query_parameters['minMetricsTime'] = self._serialize.query('min_metrics_time', min_metrics_time, 'iso-8601')\n        if definition_ids is not None:\n            definition_ids = \",\".join(map(str, definition_ids))\n            query_parameters['definitionIds'] = self._serialize.query('definition_ids', definition_ids, 'str')\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        if built_after is not None:\n            query_parameters['builtAfter'] = self._serialize.query('built_after', built_after, 'iso-8601')\n        if not_built_after is not None:\n            query_parameters['notBuiltAfter'] = self._serialize.query('not_built_after', not_built_after, 'iso-8601')\n        if include_all_properties is not None:\n            query_parameters['includeAllProperties'] = self._serialize.query('include_all_properties', include_all_properties, 'bool')\n        if include_latest_builds is not None:\n            query_parameters['includeLatestBuilds'] = self._serialize.query('include_latest_builds', include_latest_builds, 'bool')\n        if task_id_filter is not None:\n            query_parameters['taskIdFilter'] = self._serialize.query('task_id_filter', task_id_filter, 'str')\n        if process_type is not None:\n            query_parameters['processType'] = self._serialize.query('process_type', process_type, 'int')\n        if yaml_filename is not None:\n            query_parameters['yamlFilename'] = self._serialize.query('yaml_filename', yaml_filename, 'str')\n        response = self._send(http_method='GET',\n                              location_id='dbeaf647-6167-421a-bda9-c9327b25e2e6',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[BuildDefinitionReference]', self._unwrap_collection(response))", "response": "Get a list of definitions for a given project."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef restore_definition(self, project, definition_id, deleted):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        query_parameters = {}\n        if deleted is not None:\n            query_parameters['deleted'] = self._serialize.query('deleted', deleted, 'bool')\n        response = self._send(http_method='PATCH',\n                              location_id='dbeaf647-6167-421a-bda9-c9327b25e2e6',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('BuildDefinition', response)", "response": "Restores a deleted definition."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating an existing definition.", "response": "def update_definition(self, definition, project, definition_id, secrets_source_definition_id=None, secrets_source_definition_revision=None):\n        \"\"\"UpdateDefinition.\n        Updates an existing definition.\n        :param :class:`<BuildDefinition> <azure.devops.v5_0.build.models.BuildDefinition>` definition: The new version of the defintion.\n        :param str project: Project ID or project name\n        :param int definition_id: The ID of the definition.\n        :param int secrets_source_definition_id:\n        :param int secrets_source_definition_revision:\n        :rtype: :class:`<BuildDefinition> <azure.devops.v5_0.build.models.BuildDefinition>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        query_parameters = {}\n        if secrets_source_definition_id is not None:\n            query_parameters['secretsSourceDefinitionId'] = self._serialize.query('secrets_source_definition_id', secrets_source_definition_id, 'int')\n        if secrets_source_definition_revision is not None:\n            query_parameters['secretsSourceDefinitionRevision'] = self._serialize.query('secrets_source_definition_revision', secrets_source_definition_revision, 'int')\n        content = self._serialize.body(definition, 'BuildDefinition')\n        response = self._send(http_method='PUT',\n                              location_id='dbeaf647-6167-421a-bda9-c9327b25e2e6',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('BuildDefinition', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the contents of a file in a source code repository.", "response": "def get_file_contents(self, project, provider_name, service_endpoint_id=None, repository=None, commit_or_branch=None, path=None, **kwargs):\n        \"\"\"GetFileContents.\n        [Preview API] Gets the contents of a file in the given source code repository.\n        :param str project: Project ID or project name\n        :param str provider_name: The name of the source provider.\n        :param str service_endpoint_id: If specified, the ID of the service endpoint to query. Can only be omitted for providers that do not use service endpoints, e.g. TFVC or TFGit.\n        :param str repository: If specified, the vendor-specific identifier or the name of the repository to get branches. Can only be omitted for providers that do not support multiple repositories.\n        :param str commit_or_branch: The identifier of the commit or branch from which a file's contents are retrieved.\n        :param str path: The path to the file to retrieve, relative to the root of the repository.\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if provider_name is not None:\n            route_values['providerName'] = self._serialize.url('provider_name', provider_name, 'str')\n        query_parameters = {}\n        if service_endpoint_id is not None:\n            query_parameters['serviceEndpointId'] = self._serialize.query('service_endpoint_id', service_endpoint_id, 'str')\n        if repository is not None:\n            query_parameters['repository'] = self._serialize.query('repository', repository, 'str')\n        if commit_or_branch is not None:\n            query_parameters['commitOrBranch'] = self._serialize.query('commit_or_branch', commit_or_branch, 'str')\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        response = self._send(http_method='GET',\n                              location_id='29d12225-b1d9-425f-b668-6c594a981313',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='text/plain')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_folder(self, folder, project, path):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if path is not None:\n            route_values['path'] = self._serialize.url('path', path, 'str')\n        content = self._serialize.body(folder, 'Folder')\n        response = self._send(http_method='PUT',\n                              location_id='a906531b-d2da-4f55-bda7-f3e676cc50d9',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Folder', response)", "response": "CreateFolder. This method creates a new folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_folder(self, project, path):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if path is not None:\n            route_values['path'] = self._serialize.url('path', path, 'str')\n        self._send(http_method='DELETE',\n                   location_id='a906531b-d2da-4f55-bda7-f3e676cc50d9',\n                   version='5.0-preview.2',\n                   route_values=route_values)", "response": "Delete a definition folder."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_folders(self, project, path=None, query_order=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if path is not None:\n            route_values['path'] = self._serialize.url('path', path, 'str')\n        query_parameters = {}\n        if query_order is not None:\n            query_parameters['queryOrder'] = self._serialize.query('query_order', query_order, 'str')\n        response = self._send(http_method='GET',\n                              location_id='a906531b-d2da-4f55-bda7-f3e676cc50d9',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[Folder]', self._unwrap_collection(response))", "response": "Get a list of build definition folders."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the individual log file for a build.", "response": "def get_build_log_lines(self, project, build_id, log_id, start_line=None, end_line=None):\n        \"\"\"GetBuildLogLines.\n        Gets an individual log file for a build.\n        :param str project: Project ID or project name\n        :param int build_id: The ID of the build.\n        :param int log_id: The ID of the log file.\n        :param long start_line: The start line.\n        :param long end_line: The end line.\n        :rtype: [str]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        if log_id is not None:\n            route_values['logId'] = self._serialize.url('log_id', log_id, 'int')\n        query_parameters = {}\n        if start_line is not None:\n            query_parameters['startLine'] = self._serialize.query('start_line', start_line, 'long')\n        if end_line is not None:\n            query_parameters['endLine'] = self._serialize.query('end_line', end_line, 'long')\n        response = self._send(http_method='GET',\n                              location_id='35a80daf-7f30-45fc-86e8-6b813d9c90df',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[str]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_build_logs_zip(self, project, build_id, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='35a80daf-7f30-45fc-86e8-6b813d9c90df',\n                              version='5.0',\n                              route_values=route_values,\n                              accept_media_type='application/zip')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get the logs for a build."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a build log file for a given build.", "response": "def get_build_log_zip(self, project, build_id, log_id, start_line=None, end_line=None, **kwargs):\n        \"\"\"GetBuildLogZip.\n        Gets an individual log file for a build.\n        :param str project: Project ID or project name\n        :param int build_id: The ID of the build.\n        :param int log_id: The ID of the log file.\n        :param long start_line: The start line.\n        :param long end_line: The end line.\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        if log_id is not None:\n            route_values['logId'] = self._serialize.url('log_id', log_id, 'int')\n        query_parameters = {}\n        if start_line is not None:\n            query_parameters['startLine'] = self._serialize.query('start_line', start_line, 'long')\n        if end_line is not None:\n            query_parameters['endLine'] = self._serialize.query('end_line', end_line, 'long')\n        response = self._send(http_method='GET',\n                              location_id='35a80daf-7f30-45fc-86e8-6b813d9c90df',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='application/zip')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_project_metrics(self, project, metric_aggregation_type=None, min_metrics_time=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if metric_aggregation_type is not None:\n            route_values['metricAggregationType'] = self._serialize.url('metric_aggregation_type', metric_aggregation_type, 'str')\n        query_parameters = {}\n        if min_metrics_time is not None:\n            query_parameters['minMetricsTime'] = self._serialize.query('min_metrics_time', min_metrics_time, 'iso-8601')\n        response = self._send(http_method='GET',\n                              location_id='7433fae7-a6bc-41dc-a6e2-eef9005ce41a',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[BuildMetric]', self._unwrap_collection(response))", "response": "GetProjectMetrics.\n        [Preview API] Gets build metrics for a project.\n        :param str project: Project ID or project name\n        :param str metric_aggregation_type: The aggregation type to use (hourly, daily).\n        :param datetime min_metrics_time: The date from which to calculate metrics.\n        :rtype: [BuildMetric]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget build metrics for a given definition.", "response": "def get_definition_metrics(self, project, definition_id, min_metrics_time=None):\n        \"\"\"GetDefinitionMetrics.\n        [Preview API] Gets build metrics for a definition.\n        :param str project: Project ID or project name\n        :param int definition_id: The ID of the definition.\n        :param datetime min_metrics_time: The date from which to calculate metrics.\n        :rtype: [BuildMetric]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        query_parameters = {}\n        if min_metrics_time is not None:\n            query_parameters['minMetricsTime'] = self._serialize.query('min_metrics_time', min_metrics_time, 'iso-8601')\n        response = self._send(http_method='GET',\n                              location_id='d973b939-0ce0-4fec-91d8-da3940fa1827',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[BuildMetric]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the contents of a file in a source code repository.", "response": "def get_path_contents(self, project, provider_name, service_endpoint_id=None, repository=None, commit_or_branch=None, path=None):\n        \"\"\"GetPathContents.\n        [Preview API] Gets the contents of a directory in the given source code repository.\n        :param str project: Project ID or project name\n        :param str provider_name: The name of the source provider.\n        :param str service_endpoint_id: If specified, the ID of the service endpoint to query. Can only be omitted for providers that do not use service endpoints, e.g. TFVC or TFGit.\n        :param str repository: If specified, the vendor-specific identifier or the name of the repository to get branches. Can only be omitted for providers that do not support multiple repositories.\n        :param str commit_or_branch: The identifier of the commit or branch from which a file's contents are retrieved.\n        :param str path: The path contents to list, relative to the root of the repository.\n        :rtype: [SourceRepositoryItem]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if provider_name is not None:\n            route_values['providerName'] = self._serialize.url('provider_name', provider_name, 'str')\n        query_parameters = {}\n        if service_endpoint_id is not None:\n            query_parameters['serviceEndpointId'] = self._serialize.query('service_endpoint_id', service_endpoint_id, 'str')\n        if repository is not None:\n            query_parameters['repository'] = self._serialize.query('repository', repository, 'str')\n        if commit_or_branch is not None:\n            query_parameters['commitOrBranch'] = self._serialize.query('commit_or_branch', commit_or_branch, 'str')\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        response = self._send(http_method='GET',\n                              location_id='7944d6fb-df01-4709-920a-7a189aa34037',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[SourceRepositoryItem]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_build_properties(self, project, build_id, filter=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        query_parameters = {}\n        if filter is not None:\n            filter = \",\".join(filter)\n            query_parameters['filter'] = self._serialize.query('filter', filter, 'str')\n        response = self._send(http_method='GET',\n                              location_id='0a6312e9-0627-49b7-8083-7d74a64849c9',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('object', response)", "response": "Get properties for a build."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate build properties for a build.", "response": "def update_build_properties(self, document, project, build_id):\n        \"\"\"UpdateBuildProperties.\n        [Preview API] Updates properties for a build.\n        :param :class:`<[JsonPatchOperation]> <azure.devops.v5_0.build.models.[JsonPatchOperation]>` document: A json-patch document describing the properties to update.\n        :param str project: Project ID or project name\n        :param int build_id: The ID of the build.\n        :rtype: :class:`<object> <azure.devops.v5_0.build.models.object>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        content = self._serialize.body(document, '[JsonPatchOperation]')\n        response = self._send(http_method='PATCH',\n                              location_id='0a6312e9-0627-49b7-8083-7d74a64849c9',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content,\n                              media_type='application/json-patch+json')\n        return self._deserialize('object', response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_definition_properties(self, project, definition_id, filter=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        query_parameters = {}\n        if filter is not None:\n            filter = \",\".join(filter)\n            query_parameters['filter'] = self._serialize.query('filter', filter, 'str')\n        response = self._send(http_method='GET',\n                              location_id='d9826ad7-2a68-46a9-a6e9-677698777895',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('object', response)", "response": "Get properties for a specific definition."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the properties of a definition.", "response": "def update_definition_properties(self, document, project, definition_id):\n        \"\"\"UpdateDefinitionProperties.\n        [Preview API] Updates properties for a definition.\n        :param :class:`<[JsonPatchOperation]> <azure.devops.v5_0.build.models.[JsonPatchOperation]>` document: A json-patch document describing the properties to update.\n        :param str project: Project ID or project name\n        :param int definition_id: The ID of the definition.\n        :rtype: :class:`<object> <azure.devops.v5_0.build.models.object>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        content = self._serialize.body(document, '[JsonPatchOperation]')\n        response = self._send(http_method='PATCH',\n                              location_id='d9826ad7-2a68-46a9-a6e9-677698777895',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content,\n                              media_type='application/json-patch+json')\n        return self._deserialize('object', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a pull request object from source provider.", "response": "def get_pull_request(self, project, provider_name, pull_request_id, repository_id=None, service_endpoint_id=None):\n        \"\"\"GetPullRequest.\n        [Preview API] Gets a pull request object from source provider.\n        :param str project: Project ID or project name\n        :param str provider_name: The name of the source provider.\n        :param str pull_request_id: Vendor-specific id of the pull request.\n        :param str repository_id: Vendor-specific identifier or the name of the repository that contains the pull request.\n        :param str service_endpoint_id: If specified, the ID of the service endpoint to query. Can only be omitted for providers that do not use service endpoints, e.g. TFVC or TFGit.\n        :rtype: :class:`<PullRequest> <azure.devops.v5_0.build.models.PullRequest>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if provider_name is not None:\n            route_values['providerName'] = self._serialize.url('provider_name', provider_name, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'str')\n        query_parameters = {}\n        if repository_id is not None:\n            query_parameters['repositoryId'] = self._serialize.query('repository_id', repository_id, 'str')\n        if service_endpoint_id is not None:\n            query_parameters['serviceEndpointId'] = self._serialize.query('service_endpoint_id', service_endpoint_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='d8763ec7-9ff0-4fb4-b2b2-9d757906ff14',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('PullRequest', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_build_report(self, project, build_id, type=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        query_parameters = {}\n        if type is not None:\n            query_parameters['type'] = self._serialize.query('type', type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='45bcaa88-67e1-4042-a035-56d3b4a7d44c',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('BuildReportMetadata', response)", "response": "Get a build report."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists repositories for a specific source code provider.", "response": "def list_repositories(self, project, provider_name, service_endpoint_id=None, repository=None, result_set=None, page_results=None, continuation_token=None):\n        \"\"\"ListRepositories.\n        [Preview API] Gets a list of source code repositories.\n        :param str project: Project ID or project name\n        :param str provider_name: The name of the source provider.\n        :param str service_endpoint_id: If specified, the ID of the service endpoint to query. Can only be omitted for providers that do not use service endpoints, e.g. TFVC or TFGit.\n        :param str repository: If specified, the vendor-specific identifier or the name of a single repository to get.\n        :param str result_set: 'top' for the repositories most relevant for the endpoint. If not set, all repositories are returned. Ignored if 'repository' is set.\n        :param bool page_results: If set to true, this will limit the set of results and will return a continuation token to continue the query.\n        :param str continuation_token: When paging results, this is a continuation token, returned by a previous call to this method, that can be used to return the next set of repositories.\n        :rtype: :class:`<SourceRepositories> <azure.devops.v5_0.build.models.SourceRepositories>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if provider_name is not None:\n            route_values['providerName'] = self._serialize.url('provider_name', provider_name, 'str')\n        query_parameters = {}\n        if service_endpoint_id is not None:\n            query_parameters['serviceEndpointId'] = self._serialize.query('service_endpoint_id', service_endpoint_id, 'str')\n        if repository is not None:\n            query_parameters['repository'] = self._serialize.query('repository', repository, 'str')\n        if result_set is not None:\n            query_parameters['resultSet'] = self._serialize.query('result_set', result_set, 'str')\n        if page_results is not None:\n            query_parameters['pageResults'] = self._serialize.query('page_results', page_results, 'bool')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        response = self._send(http_method='GET',\n                              location_id='d44d1680-f978-4834-9b93-8c6e132329c9',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('SourceRepositories', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the build settings.", "response": "def update_build_settings(self, settings, project=None):\n        \"\"\"UpdateBuildSettings.\n        Updates the build settings.\n        :param :class:`<BuildSettings> <azure.devops.v5_0.build.models.BuildSettings>` settings: The new settings.\n        :param str project: Project ID or project name\n        :rtype: :class:`<BuildSettings> <azure.devops.v5_0.build.models.BuildSettings>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(settings, 'BuildSettings')\n        response = self._send(http_method='PATCH',\n                              location_id='aa8c1c9c-ef8b-474a-b8c4-785c7b191d0d',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('BuildSettings', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the build status for a specific branch stage job and configuration.", "response": "def get_status_badge(self, project, definition, branch_name=None, stage_name=None, job_name=None, configuration=None, label=None):\n        \"\"\"GetStatusBadge.\n        [Preview API] <p>Gets the build status for a definition, optionally scoped to a specific branch, stage, job, and configuration.</p> <p>If there are more than one, then it is required to pass in a stageName value when specifying a jobName, and the same rule then applies for both if passing a configuration parameter.</p>\n        :param str project: Project ID or project name\n        :param str definition: Either the definition name with optional leading folder path, or the definition id.\n        :param str branch_name: Only consider the most recent build for this branch.\n        :param str stage_name: Use this stage within the pipeline to render the status.\n        :param str job_name: Use this job within a stage of the pipeline to render the status.\n        :param str configuration: Use this job configuration to render the status\n        :param str label: Replaces the default text on the left side of the badge.\n        :rtype: str\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition is not None:\n            route_values['definition'] = self._serialize.url('definition', definition, 'str')\n        query_parameters = {}\n        if branch_name is not None:\n            query_parameters['branchName'] = self._serialize.query('branch_name', branch_name, 'str')\n        if stage_name is not None:\n            query_parameters['stageName'] = self._serialize.query('stage_name', stage_name, 'str')\n        if job_name is not None:\n            query_parameters['jobName'] = self._serialize.query('job_name', job_name, 'str')\n        if configuration is not None:\n            query_parameters['configuration'] = self._serialize.query('configuration', configuration, 'str')\n        if label is not None:\n            query_parameters['label'] = self._serialize.query('label', label, 'str')\n        response = self._send(http_method='GET',\n                              location_id='07acfdce-4757-4439-b422-ddd13a2fcc10',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('str', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd tags to a build.", "response": "def add_build_tags(self, tags, project, build_id):\n        \"\"\"AddBuildTags.\n        Adds tags to a build.\n        :param [str] tags: The tags to add.\n        :param str project: Project ID or project name\n        :param int build_id: The ID of the build.\n        :rtype: [str]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        content = self._serialize.body(tags, '[str]')\n        response = self._send(http_method='POST',\n                              location_id='6e6114b2-8161-44c8-8f6c-c5505782427f',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[str]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds multiple tags to a definition.", "response": "def add_definition_tags(self, tags, project, definition_id):\n        \"\"\"AddDefinitionTags.\n        [Preview API] Adds multiple tags to a definition.\n        :param [str] tags: The tags to add.\n        :param str project: Project ID or project name\n        :param int definition_id: The ID of the definition.\n        :rtype: [str]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        content = self._serialize.body(tags, '[str]')\n        response = self._send(http_method='POST',\n                              location_id='cb894432-134a-4d31-a839-83beceaace4b',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[str]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the tags for a specific definition.", "response": "def get_definition_tags(self, project, definition_id, revision=None):\n        \"\"\"GetDefinitionTags.\n        [Preview API] Gets the tags for a definition.\n        :param str project: Project ID or project name\n        :param int definition_id: The ID of the definition.\n        :param int revision: The definition revision number. If not specified, uses the latest revision of the definition.\n        :rtype: [str]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        query_parameters = {}\n        if revision is not None:\n            query_parameters['revision'] = self._serialize.query('revision', revision, 'int')\n        response = self._send(http_method='GET',\n                              location_id='cb894432-134a-4d31-a839-83beceaace4b',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[str]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_template(self, project, template_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if template_id is not None:\n            route_values['templateId'] = self._serialize.url('template_id', template_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='e884571e-7f92-4d6a-9274-3f5649900835',\n                   version='5.0',\n                   route_values=route_values)", "response": "Delete a build definition template."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a specific build definition template.", "response": "def get_template(self, project, template_id):\n        \"\"\"GetTemplate.\n        Gets a specific build definition template.\n        :param str project: Project ID or project name\n        :param str template_id: The ID of the requested template.\n        :rtype: :class:`<BuildDefinitionTemplate> <azure.devops.v5_0.build.models.BuildDefinitionTemplate>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if template_id is not None:\n            route_values['templateId'] = self._serialize.url('template_id', template_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e884571e-7f92-4d6a-9274-3f5649900835',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('BuildDefinitionTemplate', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves a build definition template.", "response": "def save_template(self, template, project, template_id):\n        \"\"\"SaveTemplate.\n        Updates an existing build definition template.\n        :param :class:`<BuildDefinitionTemplate> <azure.devops.v5_0.build.models.BuildDefinitionTemplate>` template: The new version of the template.\n        :param str project: Project ID or project name\n        :param str template_id: The ID of the template.\n        :rtype: :class:`<BuildDefinitionTemplate> <azure.devops.v5_0.build.models.BuildDefinitionTemplate>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if template_id is not None:\n            route_values['templateId'] = self._serialize.url('template_id', template_id, 'str')\n        content = self._serialize.body(template, 'BuildDefinitionTemplate')\n        response = self._send(http_method='PUT',\n                              location_id='e884571e-7f92-4d6a-9274-3f5649900835',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('BuildDefinitionTemplate', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_build_timeline(self, project, build_id, timeline_id=None, change_id=None, plan_id=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        if timeline_id is not None:\n            route_values['timelineId'] = self._serialize.url('timeline_id', timeline_id, 'str')\n        query_parameters = {}\n        if change_id is not None:\n            query_parameters['changeId'] = self._serialize.query('change_id', change_id, 'int')\n        if plan_id is not None:\n            query_parameters['planId'] = self._serialize.query('plan_id', plan_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='8baac422-4c6e-4de5-8532-db96d92acffa',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Timeline', response)", "response": "Get a build s build timeline."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef restore_webhooks(self, trigger_types, project, provider_name, service_endpoint_id=None, repository=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if provider_name is not None:\n            route_values['providerName'] = self._serialize.url('provider_name', provider_name, 'str')\n        query_parameters = {}\n        if service_endpoint_id is not None:\n            query_parameters['serviceEndpointId'] = self._serialize.query('service_endpoint_id', service_endpoint_id, 'str')\n        if repository is not None:\n            query_parameters['repository'] = self._serialize.query('repository', repository, 'str')\n        content = self._serialize.body(trigger_types, '[DefinitionTriggerType]')\n        self._send(http_method='POST',\n                   location_id='793bceb8-9736-4030-bd2f-fb3ce6d6b478',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   query_parameters=query_parameters,\n                   content=content)", "response": "This method recreates the webhooks for the specified source code repository."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the work items associated with a build.", "response": "def get_build_work_items_refs(self, project, build_id, top=None):\n        \"\"\"GetBuildWorkItemsRefs.\n        Gets the work items associated with a build.\n        :param str project: Project ID or project name\n        :param int build_id: The ID of the build.\n        :param int top: The maximum number of work items to return.\n        :rtype: [ResourceRef]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        query_parameters = {}\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        response = self._send(http_method='GET',\n                              location_id='5a21f5d2-5642-47e4-a0bd-1356e6731bee',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[ResourceRef]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_build_work_items_refs_from_commits(self, commit_ids, project, build_id, top=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        query_parameters = {}\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        content = self._serialize.body(commit_ids, '[str]')\n        response = self._send(http_method='POST',\n                              location_id='5a21f5d2-5642-47e4-a0bd-1356e6731bee',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('[ResourceRef]', self._unwrap_collection(response))", "response": "Get the work items associated with a build from a list of commits."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshares an extension by its id", "response": "def share_extension_by_id(self, extension_id, account_name):\n        \"\"\"ShareExtensionById.\n        [Preview API]\n        :param str extension_id:\n        :param str account_name:\n        \"\"\"\n        route_values = {}\n        if extension_id is not None:\n            route_values['extensionId'] = self._serialize.url('extension_id', extension_id, 'str')\n        if account_name is not None:\n            route_values['accountName'] = self._serialize.url('account_name', account_name, 'str')\n        self._send(http_method='POST',\n                   location_id='1f19631b-a0b4-4a03-89c2-d79785d24360',\n                   version='5.0-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef associate_azure_publisher(self, publisher_name, azure_publisher_id):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        query_parameters = {}\n        if azure_publisher_id is not None:\n            query_parameters['azurePublisherId'] = self._serialize.query('azure_publisher_id', azure_publisher_id, 'str')\n        response = self._send(http_method='PUT',\n                              location_id='efd202a6-9d87-4ebc-9229-d2b8ae2fdb6d',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('AzurePublisher', response)", "response": "Associate AzurePublisher. azure_publisher_id is the publisher id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget certificate for a specific publisher and extension.", "response": "def get_certificate(self, publisher_name, extension_name, version=None, **kwargs):\n        \"\"\"GetCertificate.\n        [Preview API]\n        :param str publisher_name:\n        :param str extension_name:\n        :param str version:\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        if version is not None:\n            route_values['version'] = self._serialize.url('version', version, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e905ad6a-3f1f-4d08-9f6d-7d357ff8b7d0',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_draft_for_edit_extension(self, publisher_name, extension_name):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        response = self._send(http_method='POST',\n                              location_id='02b33873-4e61-496e-83a2-59d1df46b7d8',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('ExtensionDraft', response)", "response": "Create a draft for an extension."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_payload_in_draft_for_edit_extension(self, upload_stream, publisher_name, extension_name, draft_id, file_name=None, **kwargs):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        if draft_id is not None:\n            route_values['draftId'] = self._serialize.url('draft_id', draft_id, 'str')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        content = self._client.stream_upload(upload_stream, callback=callback)\n        response = self._send(http_method='PUT',\n                              location_id='02b33873-4e61-496e-83a2-59d1df46b7d8',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content,\n                              media_type='application/octet-stream')\n        return self._deserialize('ExtensionDraft', response)", "response": "Update the payload of an extension draft for an edit extension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_asset_from_edit_extension_draft(self, publisher_name, draft_id, asset_type, extension_name, **kwargs):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if draft_id is not None:\n            route_values['draftId'] = self._serialize.url('draft_id', draft_id, 'str')\n        if asset_type is not None:\n            route_values['assetType'] = self._serialize.url('asset_type', asset_type, 'str')\n        query_parameters = {}\n        if extension_name is not None:\n            query_parameters['extensionName'] = self._serialize.query('extension_name', extension_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='88c0b1c8-b4f1-498a-9b2a-8446ef9f32e7',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get Asset from Edit Extension Draft."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_extension_by_id(self, extension_id, version=None):\n        route_values = {}\n        if extension_id is not None:\n            route_values['extensionId'] = self._serialize.url('extension_id', extension_id, 'str')\n        query_parameters = {}\n        if version is not None:\n            query_parameters['version'] = self._serialize.query('version', version, 'str')\n        self._send(http_method='DELETE',\n                   location_id='a41192c8-9525-4b58-bc86-179fa549d80d',\n                   version='5.0-preview.2',\n                   route_values=route_values,\n                   query_parameters=query_parameters)", "response": "Delete an extension by its ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_extension_by_id(self, extension_id, version=None, flags=None):\n        route_values = {}\n        if extension_id is not None:\n            route_values['extensionId'] = self._serialize.url('extension_id', extension_id, 'str')\n        query_parameters = {}\n        if version is not None:\n            query_parameters['version'] = self._serialize.query('version', version, 'str')\n        if flags is not None:\n            query_parameters['flags'] = self._serialize.query('flags', flags, 'str')\n        response = self._send(http_method='GET',\n                              location_id='a41192c8-9525-4b58-bc86-179fa549d80d',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('PublishedExtension', response)", "response": "Get a published extension by its ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef share_extension_with_host(self, publisher_name, extension_name, host_type, host_name):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        if host_type is not None:\n            route_values['hostType'] = self._serialize.url('host_type', host_type, 'str')\n        if host_name is not None:\n            route_values['hostName'] = self._serialize.url('host_name', host_name, 'str')\n        self._send(http_method='POST',\n                   location_id='328a3af8-d124-46e9-9483-01690cd415b9',\n                   version='5.0-preview.1',\n                   route_values=route_values)", "response": "Share an extension with a host."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extension_validator(self, azure_rest_api_request_model):\n        content = self._serialize.body(azure_rest_api_request_model, 'AzureRestApiRequestModel')\n        self._send(http_method='POST',\n                   location_id='05e8a5e1-8c59-4c2c-8856-0ff087d1a844',\n                   version='5.0-preview.1',\n                   content=content)", "response": "ExtensionValidator.\n        [ Preview API ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_publisher_asset(self, upload_stream, publisher_name, asset_type=None, file_name=None, **kwargs):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        query_parameters = {}\n        if asset_type is not None:\n            query_parameters['assetType'] = self._serialize.query('asset_type', asset_type, 'str')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        content = self._client.stream_upload(upload_stream, callback=callback)\n        response = self._send(http_method='PUT',\n                              location_id='21143299-34f9-4c62-8ca8-53da691192f9',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content,\n                              media_type='application/octet-stream')\n        return self._deserialize('{str}', self._unwrap_collection(response))", "response": "Update publisher asset like logo. It accepts file as an octet stream and file name is passed in header values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a publisher from the API.", "response": "def get_publisher(self, publisher_name, flags=None):\n        \"\"\"GetPublisher.\n        [Preview API]\n        :param str publisher_name:\n        :param int flags:\n        :rtype: :class:`<Publisher> <azure.devops.v5_0.gallery.models.Publisher>`\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        query_parameters = {}\n        if flags is not None:\n            query_parameters['flags'] = self._serialize.query('flags', flags, 'int')\n        response = self._send(http_method='GET',\n                              location_id='4ddec66a-e4f6-4f5d-999e-9e77710d7ff4',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Publisher', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes an existing question and all its associated responses for an extension.", "response": "def delete_question(self, publisher_name, extension_name, question_id):\n        \"\"\"DeleteQuestion.\n        [Preview API] Deletes an existing question and all its associated responses for an extension. (soft delete)\n        :param str publisher_name: Name of the publisher who published the extension.\n        :param str extension_name: Name of the extension.\n        :param long question_id: Identifier of the question to be deleted for the extension.\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        if question_id is not None:\n            route_values['questionId'] = self._serialize.url('question_id', question_id, 'long')\n        self._send(http_method='DELETE',\n                   location_id='6d1d9741-eca8-4701-a3a5-235afc82dfa4',\n                   version='5.0-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the summary of the reviews of the extension.", "response": "def get_reviews_summary(self, pub_name, ext_name, before_date=None, after_date=None):\n        \"\"\"GetReviewsSummary.\n        [Preview API] Returns a summary of the reviews\n        :param str pub_name: Name of the publisher who published the extension\n        :param str ext_name: Name of the extension\n        :param datetime before_date: Use if you want to fetch summary of reviews older than the specified date, defaults to null\n        :param datetime after_date: Use if you want to fetch summary of reviews newer than the specified date, defaults to null\n        :rtype: :class:`<ReviewSummary> <azure.devops.v5_0.gallery.models.ReviewSummary>`\n        \"\"\"\n        route_values = {}\n        if pub_name is not None:\n            route_values['pubName'] = self._serialize.url('pub_name', pub_name, 'str')\n        if ext_name is not None:\n            route_values['extName'] = self._serialize.url('ext_name', ext_name, 'str')\n        query_parameters = {}\n        if before_date is not None:\n            query_parameters['beforeDate'] = self._serialize.query('before_date', before_date, 'iso-8601')\n        if after_date is not None:\n            query_parameters['afterDate'] = self._serialize.query('after_date', after_date, 'iso-8601')\n        response = self._send(http_method='GET',\n                              location_id='b7b44e21-209e-48f0-ae78-04727fc37d77',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ReviewSummary', response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_review(self, review, pub_name, ext_name):\n        route_values = {}\n        if pub_name is not None:\n            route_values['pubName'] = self._serialize.url('pub_name', pub_name, 'str')\n        if ext_name is not None:\n            route_values['extName'] = self._serialize.url('ext_name', ext_name, 'str')\n        content = self._serialize.body(review, 'Review')\n        response = self._send(http_method='POST',\n                              location_id='e6e85b9d-aa70-40e6-aa28-d0fbf40b91a3',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Review', response)", "response": "Create a new Review for an extension."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_signing_key(self, key_type):\n        route_values = {}\n        if key_type is not None:\n            route_values['keyType'] = self._serialize.url('key_type', key_type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='92ed5cf4-c38b-465a-9059-2f2fb7c624b5',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('str', response)", "response": "Get the signing key for a specific key type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_extension_statistics(self, extension_statistics_update, publisher_name, extension_name):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        content = self._serialize.body(extension_statistics_update, 'ExtensionStatisticUpdate')\n        self._send(http_method='PATCH',\n                   location_id='a0ea3204-11e9-422d-a9ca-45851cc41400',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   content=content)", "response": "Update the extension statistics for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef increment_extension_daily_stat(self, publisher_name, extension_name, version, stat_type):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        if version is not None:\n            route_values['version'] = self._serialize.url('version', version, 'str')\n        query_parameters = {}\n        if stat_type is not None:\n            query_parameters['statType'] = self._serialize.query('stat_type', stat_type, 'str')\n        self._send(http_method='POST',\n                   location_id='4fa7adb6-ca65-4075-a232-5f28323288ea',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   query_parameters=query_parameters)", "response": "Increment the daily statistic associated with the extension_name and version."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_avatar(self, subject_descriptor):\n        route_values = {}\n        if subject_descriptor is not None:\n            route_values['subjectDescriptor'] = self._serialize.url('subject_descriptor', subject_descriptor, 'str')\n        self._send(http_method='DELETE',\n                   location_id='801eaf9c-0585-4be8-9cdb-b0efa074de91',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "DeleteAvatar.\n        [Preview API]\n        :param str subject_descriptor:"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets avatar for subject_descriptor size and format.", "response": "def get_avatar(self, subject_descriptor, size=None, format=None):\n        \"\"\"GetAvatar.\n        [Preview API]\n        :param str subject_descriptor:\n        :param str size:\n        :param str format:\n        :rtype: :class:`<Avatar> <azure.devops.v5_1.graph.models.Avatar>`\n        \"\"\"\n        route_values = {}\n        if subject_descriptor is not None:\n            route_values['subjectDescriptor'] = self._serialize.url('subject_descriptor', subject_descriptor, 'str')\n        query_parameters = {}\n        if size is not None:\n            query_parameters['size'] = self._serialize.query('size', size, 'str')\n        if format is not None:\n            query_parameters['format'] = self._serialize.query('format', format, 'str')\n        response = self._send(http_method='GET',\n                              location_id='801eaf9c-0585-4be8-9cdb-b0efa074de91',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Avatar', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the avatar for the current resource.", "response": "def set_avatar(self, avatar, subject_descriptor):\n        \"\"\"SetAvatar.\n        [Preview API]\n        :param :class:`<Avatar> <azure.devops.v5_1.graph.models.Avatar>` avatar:\n        :param str subject_descriptor:\n        \"\"\"\n        route_values = {}\n        if subject_descriptor is not None:\n            route_values['subjectDescriptor'] = self._serialize.url('subject_descriptor', subject_descriptor, 'str')\n        content = self._serialize.body(avatar, 'Avatar')\n        self._send(http_method='PUT',\n                   location_id='801eaf9c-0585-4be8-9cdb-b0efa074de91',\n                   version='5.1-preview.1',\n                   route_values=route_values,\n                   content=content)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a descriptor for a given storage key.", "response": "def get_descriptor(self, storage_key):\n        \"\"\"GetDescriptor.\n        [Preview API] Resolve a storage key to a descriptor\n        :param str storage_key: Storage key of the subject (user, group, scope, etc.) to resolve\n        :rtype: :class:`<GraphDescriptorResult> <azure.devops.v5_1.graph.models.GraphDescriptorResult>`\n        \"\"\"\n        route_values = {}\n        if storage_key is not None:\n            route_values['storageKey'] = self._serialize.url('storage_key', storage_key, 'str')\n        response = self._send(http_method='GET',\n                              location_id='048aee0a-7072-4cde-ab73-7af77b1e0b4e',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('GraphDescriptorResult', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_membership(self, subject_descriptor, container_descriptor):\n        route_values = {}\n        if subject_descriptor is not None:\n            route_values['subjectDescriptor'] = self._serialize.url('subject_descriptor', subject_descriptor, 'str')\n        if container_descriptor is not None:\n            route_values['containerDescriptor'] = self._serialize.url('container_descriptor', container_descriptor, 'str')\n        response = self._send(http_method='PUT',\n                              location_id='3fd2e6ca-fb30-443a-b579-95b19ed0934c',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('GraphMembership', response)", "response": "Add a new membership between a group or user and a container."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_membership_existence(self, subject_descriptor, container_descriptor):\n        route_values = {}\n        if subject_descriptor is not None:\n            route_values['subjectDescriptor'] = self._serialize.url('subject_descriptor', subject_descriptor, 'str')\n        if container_descriptor is not None:\n            route_values['containerDescriptor'] = self._serialize.url('container_descriptor', container_descriptor, 'str')\n        self._send(http_method='HEAD',\n                   location_id='3fd2e6ca-fb30-443a-b579-95b19ed0934c',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "Check to see if a membership relationship between a group or user and a container exists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting the direct memberships of this descriptor.", "response": "def list_memberships(self, subject_descriptor, direction=None, depth=None):\n        \"\"\"ListMemberships.\n        [Preview API] Get all the memberships where this descriptor is a member in the relationship.\n        :param str subject_descriptor: Fetch all direct memberships of this descriptor.\n        :param str direction: Defaults to Up.\n        :param int depth: The maximum number of edges to traverse up or down the membership tree. Currently the only supported value is '1'.\n        :rtype: [GraphMembership]\n        \"\"\"\n        route_values = {}\n        if subject_descriptor is not None:\n            route_values['subjectDescriptor'] = self._serialize.url('subject_descriptor', subject_descriptor, 'str')\n        query_parameters = {}\n        if direction is not None:\n            query_parameters['direction'] = self._serialize.query('direction', direction, 'str')\n        if depth is not None:\n            query_parameters['depth'] = self._serialize.query('depth', depth, 'int')\n        response = self._send(http_method='GET',\n                              location_id='e34b6394-6b30-4435-94a9-409a5eef3e31',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GraphMembership]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaterializes an existing AAD or MSA user into the VSTS account.", "response": "def create_user(self, creation_context, group_descriptors=None):\n        \"\"\"CreateUser.\n        [Preview API] Materialize an existing AAD or MSA user into the VSTS account.\n        :param :class:`<GraphUserCreationContext> <azure.devops.v5_1.graph.models.GraphUserCreationContext>` creation_context: The subset of the full graph user used to uniquely find the graph subject in an external provider.\n        :param [str] group_descriptors: A comma separated list of descriptors of groups you want the graph user to join\n        :rtype: :class:`<GraphUser> <azure.devops.v5_1.graph.models.GraphUser>`\n        \"\"\"\n        query_parameters = {}\n        if group_descriptors is not None:\n            group_descriptors = \",\".join(group_descriptors)\n            query_parameters['groupDescriptors'] = self._serialize.query('group_descriptors', group_descriptors, 'str')\n        content = self._serialize.body(creation_context, 'GraphUserCreationContext')\n        response = self._send(http_method='POST',\n                              location_id='005e26ec-6b77-4e4f-a986-b3827bf241f5',\n                              version='5.1-preview.1',\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('GraphUser', response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_users(self, subject_types=None, continuation_token=None):\n        query_parameters = {}\n        if subject_types is not None:\n            subject_types = \",\".join(subject_types)\n            query_parameters['subjectTypes'] = self._serialize.query('subject_types', subject_types, 'str')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        response = self._send(http_method='GET',\n                              location_id='005e26ec-6b77-4e4f-a986-b3827bf241f5',\n                              version='5.1-preview.1',\n                              query_parameters=query_parameters)\n        response_object = models.PagedGraphUsers()\n        response_object.graph_users = self._deserialize('[GraphUser]', self._unwrap_collection(response))\n        response_object.continuation_token = response.headers.get('X-MS-ContinuationToken')\n        return response_object", "response": "List all users in a given scope."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the user s identity with a new user.", "response": "def update_user(self, update_context, user_descriptor):\n        \"\"\"UpdateUser.\n        [Preview API] Map an existing user to a different identity\n        :param :class:`<GraphUserUpdateContext> <azure.devops.v5_1.graph.models.GraphUserUpdateContext>` update_context: The subset of the full graph user used to uniquely find the graph subject in an external provider.\n        :param str user_descriptor: the descriptor of the user to update\n        :rtype: :class:`<GraphUser> <azure.devops.v5_1.graph.models.GraphUser>`\n        \"\"\"\n        route_values = {}\n        if user_descriptor is not None:\n            route_values['userDescriptor'] = self._serialize.url('user_descriptor', user_descriptor, 'str')\n        content = self._serialize.body(update_context, 'GraphUserUpdateContext')\n        response = self._send(http_method='PATCH',\n                              location_id='005e26ec-6b77-4e4f-a986-b3827bf241f5',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GraphUser', response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_or_bind_with_claims(self, source_identity):\n        content = self._serialize.body(source_identity, 'Identity')\n        response = self._send(http_method='PUT',\n                              location_id='90ddfe71-171c-446c-bf3b-b597cd562afd',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('Identity', response)", "response": "CreateOrBindWithClaims.\n        creates a new identity and binds it with the claims."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a specific descriptor by its id.", "response": "def get_descriptor_by_id(self, id, is_master_id=None):\n        \"\"\"GetDescriptorById.\n        [Preview API]\n        :param str id:\n        :param bool is_master_id:\n        :rtype: :class:`<str> <azure.devops.v5_0.identity.models.str>`\n        \"\"\"\n        route_values = {}\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'str')\n        query_parameters = {}\n        if is_master_id is not None:\n            query_parameters['isMasterId'] = self._serialize.query('is_master_id', is_master_id, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='a230389a-94f2-496c-839f-c929787496dd',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('str', response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_groups(self, container):\n        content = self._serialize.body(container, 'object')\n        response = self._send(http_method='POST',\n                              location_id='5966283b-4196-4d57-9211-1b68f41ec1c2',\n                              version='5.0',\n                              content=content)\n        return self._deserialize('[Identity]', self._unwrap_collection(response))", "response": "Create a new group in the specified region."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_group(self, group_id):\n        route_values = {}\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='5966283b-4196-4d57-9211-1b68f41ec1c2',\n                   version='5.0',\n                   route_values=route_values)", "response": "DeleteGroup.\n        :param str group_id:"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists the groups in the current user.", "response": "def list_groups(self, scope_ids=None, recurse=None, deleted=None, properties=None):\n        \"\"\"ListGroups.\n        :param str scope_ids:\n        :param bool recurse:\n        :param bool deleted:\n        :param str properties:\n        :rtype: [Identity]\n        \"\"\"\n        query_parameters = {}\n        if scope_ids is not None:\n            query_parameters['scopeIds'] = self._serialize.query('scope_ids', scope_ids, 'str')\n        if recurse is not None:\n            query_parameters['recurse'] = self._serialize.query('recurse', recurse, 'bool')\n        if deleted is not None:\n            query_parameters['deleted'] = self._serialize.query('deleted', deleted, 'bool')\n        if properties is not None:\n            query_parameters['properties'] = self._serialize.query('properties', properties, 'str')\n        response = self._send(http_method='GET',\n                              location_id='5966283b-4196-4d57-9211-1b68f41ec1c2',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[Identity]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting Identity Changes API.", "response": "def get_identity_changes(self, identity_sequence_id, group_sequence_id, organization_identity_sequence_id=None, page_size=None, scope_id=None):\n        \"\"\"GetIdentityChanges.\n        :param int identity_sequence_id:\n        :param int group_sequence_id:\n        :param int organization_identity_sequence_id:\n        :param int page_size:\n        :param str scope_id:\n        :rtype: :class:`<ChangedIdentities> <azure.devops.v5_0.identity.models.ChangedIdentities>`\n        \"\"\"\n        query_parameters = {}\n        if identity_sequence_id is not None:\n            query_parameters['identitySequenceId'] = self._serialize.query('identity_sequence_id', identity_sequence_id, 'int')\n        if group_sequence_id is not None:\n            query_parameters['groupSequenceId'] = self._serialize.query('group_sequence_id', group_sequence_id, 'int')\n        if organization_identity_sequence_id is not None:\n            query_parameters['organizationIdentitySequenceId'] = self._serialize.query('organization_identity_sequence_id', organization_identity_sequence_id, 'int')\n        if page_size is not None:\n            query_parameters['pageSize'] = self._serialize.query('page_size', page_size, 'int')\n        if scope_id is not None:\n            query_parameters['scopeId'] = self._serialize.query('scope_id', scope_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='28010c54-d0c0-4c89-a5b0-1c9e188b9fb7',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('ChangedIdentities', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_user_identity_ids_by_domain_id(self, domain_id):\n        query_parameters = {}\n        if domain_id is not None:\n            query_parameters['domainId'] = self._serialize.query('domain_id', domain_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='28010c54-d0c0-4c89-a5b0-1c9e188b9fb7',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[str]', self._unwrap_collection(response))", "response": "Get user identity ids by domain id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_identities(self, descriptors=None, identity_ids=None, subject_descriptors=None, search_filter=None, filter_value=None, query_membership=None, properties=None, include_restricted_visibility=None, options=None):\n        query_parameters = {}\n        if descriptors is not None:\n            query_parameters['descriptors'] = self._serialize.query('descriptors', descriptors, 'str')\n        if identity_ids is not None:\n            query_parameters['identityIds'] = self._serialize.query('identity_ids', identity_ids, 'str')\n        if subject_descriptors is not None:\n            query_parameters['subjectDescriptors'] = self._serialize.query('subject_descriptors', subject_descriptors, 'str')\n        if search_filter is not None:\n            query_parameters['searchFilter'] = self._serialize.query('search_filter', search_filter, 'str')\n        if filter_value is not None:\n            query_parameters['filterValue'] = self._serialize.query('filter_value', filter_value, 'str')\n        if query_membership is not None:\n            query_parameters['queryMembership'] = self._serialize.query('query_membership', query_membership, 'str')\n        if properties is not None:\n            query_parameters['properties'] = self._serialize.query('properties', properties, 'str')\n        if include_restricted_visibility is not None:\n            query_parameters['includeRestrictedVisibility'] = self._serialize.query('include_restricted_visibility', include_restricted_visibility, 'bool')\n        if options is not None:\n            query_parameters['options'] = self._serialize.query('options', options, 'str')\n        response = self._send(http_method='GET',\n                              location_id='28010c54-d0c0-4c89-a5b0-1c9e188b9fb7',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[Identity]', self._unwrap_collection(response))", "response": "Reads the identities of a specific class from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_identities_by_scope(self, scope_id, query_membership=None, properties=None):\n        query_parameters = {}\n        if scope_id is not None:\n            query_parameters['scopeId'] = self._serialize.query('scope_id', scope_id, 'str')\n        if query_membership is not None:\n            query_parameters['queryMembership'] = self._serialize.query('query_membership', query_membership, 'str')\n        if properties is not None:\n            query_parameters['properties'] = self._serialize.query('properties', properties, 'str')\n        response = self._send(http_method='GET',\n                              location_id='28010c54-d0c0-4c89-a5b0-1c9e188b9fb7',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[Identity]', self._unwrap_collection(response))", "response": "Reads identities by scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the identity for the specified identity.", "response": "def read_identity(self, identity_id, query_membership=None, properties=None):\n        \"\"\"ReadIdentity.\n        :param str identity_id:\n        :param str query_membership:\n        :param str properties:\n        :rtype: :class:`<Identity> <azure.devops.v5_0.identity.models.Identity>`\n        \"\"\"\n        route_values = {}\n        if identity_id is not None:\n            route_values['identityId'] = self._serialize.url('identity_id', identity_id, 'str')\n        query_parameters = {}\n        if query_membership is not None:\n            query_parameters['queryMembership'] = self._serialize.query('query_membership', query_membership, 'str')\n        if properties is not None:\n            query_parameters['properties'] = self._serialize.query('properties', properties, 'str')\n        response = self._send(http_method='GET',\n                              location_id='28010c54-d0c0-4c89-a5b0-1c9e188b9fb7',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Identity', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the identities of a resource.", "response": "def update_identities(self, identities):\n        \"\"\"UpdateIdentities.\n        :param :class:`<VssJsonCollectionWrapper> <azure.devops.v5_0.identity.models.VssJsonCollectionWrapper>` identities:\n        :rtype: [IdentityUpdateData]\n        \"\"\"\n        content = self._serialize.body(identities, 'VssJsonCollectionWrapper')\n        response = self._send(http_method='PUT',\n                              location_id='28010c54-d0c0-4c89-a5b0-1c9e188b9fb7',\n                              version='5.0',\n                              content=content)\n        return self._deserialize('[IdentityUpdateData]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_identity(self, identity, identity_id):\n        route_values = {}\n        if identity_id is not None:\n            route_values['identityId'] = self._serialize.url('identity_id', identity_id, 'str')\n        content = self._serialize.body(identity, 'Identity')\n        self._send(http_method='PUT',\n                   location_id='28010c54-d0c0-4c89-a5b0-1c9e188b9fb7',\n                   version='5.0',\n                   route_values=route_values,\n                   content=content)", "response": "UpdateIdentity.\n        :param :class:`<Identity> <azure.devops.v5_0.identity.models.Identity>` identity:\n        :param str identity_id:"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_identity(self, framework_identity_info):\n        content = self._serialize.body(framework_identity_info, 'FrameworkIdentityInfo')\n        response = self._send(http_method='PUT',\n                              location_id='dd55f0eb-6ea2-4fe4-9ebe-919e7dd1dfb4',\n                              version='5.0',\n                              content=content)\n        return self._deserialize('Identity', response)", "response": "Create a new identity for the given framework identity info."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the identity batch.", "response": "def read_identity_batch(self, batch_info):\n        \"\"\"ReadIdentityBatch.\n        [Preview API]\n        :param :class:`<IdentityBatchInfo> <azure.devops.v5_0.identity.models.IdentityBatchInfo>` batch_info:\n        :rtype: [Identity]\n        \"\"\"\n        content = self._serialize.body(batch_info, 'IdentityBatchInfo')\n        response = self._send(http_method='POST',\n                              location_id='299e50df-fe45-4d3a-8b5b-a5836fac74dc',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('[Identity]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_max_sequence_id(self):\n        response = self._send(http_method='GET',\n                              location_id='e4a70778-cb2c-4e85-b7cc-3f3c7ae2d408',\n                              version='5.0')\n        return self._deserialize('long', response)", "response": "Get the max sequence id of all the identities."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_self(self):\n        response = self._send(http_method='GET',\n                              location_id='4bb02b5b-c120-4be2-b68e-21f7c50a4b82',\n                              version='5.0')\n        return self._deserialize('IdentitySelf', response)", "response": "Get the current identity of the home tenant request user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_member(self, container_id, member_id):\n        route_values = {}\n        if container_id is not None:\n            route_values['containerId'] = self._serialize.url('container_id', container_id, 'str')\n        if member_id is not None:\n            route_values['memberId'] = self._serialize.url('member_id', member_id, 'str')\n        response = self._send(http_method='PUT',\n                              location_id='8ba35978-138e-41f8-8963-7b1ea2c5f775',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('bool', response)", "response": "Adds a member to a set of resources."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a member from a container.", "response": "def read_member(self, container_id, member_id, query_membership=None):\n        \"\"\"ReadMember.\n        [Preview API]\n        :param str container_id:\n        :param str member_id:\n        :param str query_membership:\n        :rtype: :class:`<str> <azure.devops.v5_0.identity.models.str>`\n        \"\"\"\n        route_values = {}\n        if container_id is not None:\n            route_values['containerId'] = self._serialize.url('container_id', container_id, 'str')\n        if member_id is not None:\n            route_values['memberId'] = self._serialize.url('member_id', member_id, 'str')\n        query_parameters = {}\n        if query_membership is not None:\n            query_parameters['queryMembership'] = self._serialize.query('query_membership', query_membership, 'str')\n        response = self._send(http_method='GET',\n                              location_id='8ba35978-138e-41f8-8963-7b1ea2c5f775',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('str', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the members of a specific class.", "response": "def read_members(self, container_id, query_membership=None):\n        \"\"\"ReadMembers.\n        [Preview API]\n        :param str container_id:\n        :param str query_membership:\n        :rtype: [str]\n        \"\"\"\n        route_values = {}\n        if container_id is not None:\n            route_values['containerId'] = self._serialize.url('container_id', container_id, 'str')\n        query_parameters = {}\n        if query_membership is not None:\n            query_parameters['queryMembership'] = self._serialize.query('query_membership', query_membership, 'str')\n        response = self._send(http_method='GET',\n                              location_id='8ba35978-138e-41f8-8963-7b1ea2c5f775',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[str]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_members_of(self, member_id, query_membership=None):\n        route_values = {}\n        if member_id is not None:\n            route_values['memberId'] = self._serialize.url('member_id', member_id, 'str')\n        query_parameters = {}\n        if query_membership is not None:\n            query_parameters['queryMembership'] = self._serialize.query('query_membership', query_membership, 'str')\n        response = self._send(http_method='GET',\n                              location_id='22865b02-9e4a-479e-9e18-e35b8803b8a0',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[str]', self._unwrap_collection(response))", "response": "Reads the members of a specific entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new identity scope.", "response": "def create_scope(self, info, scope_id):\n        \"\"\"CreateScope.\n        [Preview API]\n        :param :class:`<CreateScopeInfo> <azure.devops.v5_0.identity.models.CreateScopeInfo>` info:\n        :param str scope_id:\n        :rtype: :class:`<IdentityScope> <azure.devops.v5_0.identity.models.IdentityScope>`\n        \"\"\"\n        route_values = {}\n        if scope_id is not None:\n            route_values['scopeId'] = self._serialize.url('scope_id', scope_id, 'str')\n        content = self._serialize.body(info, 'CreateScopeInfo')\n        response = self._send(http_method='PUT',\n                              location_id='4e11e2bf-1e79-4eb5-8f34-a6337bd0de38',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('IdentityScope', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_scope(self, scope_id):\n        route_values = {}\n        if scope_id is not None:\n            route_values['scopeId'] = self._serialize.url('scope_id', scope_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='4e11e2bf-1e79-4eb5-8f34-a6337bd0de38',\n                   version='5.0-preview.2',\n                   route_values=route_values)", "response": "Delete a specific scope."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the scope by id.", "response": "def get_scope_by_id(self, scope_id):\n        \"\"\"GetScopeById.\n        [Preview API]\n        :param str scope_id:\n        :rtype: :class:`<IdentityScope> <azure.devops.v5_0.identity.models.IdentityScope>`\n        \"\"\"\n        route_values = {}\n        if scope_id is not None:\n            route_values['scopeId'] = self._serialize.url('scope_id', scope_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='4e11e2bf-1e79-4eb5-8f34-a6337bd0de38',\n                              version='5.0-preview.2',\n                              route_values=route_values)\n        return self._deserialize('IdentityScope', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the scope by name.", "response": "def get_scope_by_name(self, scope_name):\n        \"\"\"GetScopeByName.\n        [Preview API]\n        :param str scope_name:\n        :rtype: :class:`<IdentityScope> <azure.devops.v5_0.identity.models.IdentityScope>`\n        \"\"\"\n        query_parameters = {}\n        if scope_name is not None:\n            query_parameters['scopeName'] = self._serialize.query('scope_name', scope_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='4e11e2bf-1e79-4eb5-8f34-a6337bd0de38',\n                              version='5.0-preview.2',\n                              query_parameters=query_parameters)\n        return self._deserialize('IdentityScope', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the scope with the given patch document.", "response": "def update_scope(self, patch_document, scope_id):\n        \"\"\"UpdateScope.\n        [Preview API]\n        :param :class:`<[JsonPatchOperation]> <azure.devops.v5_0.identity.models.[JsonPatchOperation]>` patch_document:\n        :param str scope_id:\n        \"\"\"\n        route_values = {}\n        if scope_id is not None:\n            route_values['scopeId'] = self._serialize.url('scope_id', scope_id, 'str')\n        content = self._serialize.body(patch_document, '[JsonPatchOperation]')\n        self._send(http_method='PATCH',\n                   location_id='4e11e2bf-1e79-4eb5-8f34-a6337bd0de38',\n                   version='5.0-preview.2',\n                   route_values=route_values,\n                   content=content,\n                   media_type='application/json-patch+json')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget signed in token.", "response": "def get_signed_in_token(self):\n        \"\"\"GetSignedInToken.\n        [Preview API]\n        :rtype: :class:`<AccessTokenResult> <azure.devops.v5_0.identity.models.AccessTokenResult>`\n        \"\"\"\n        response = self._send(http_method='GET',\n                              location_id='6074ff18-aaad-4abb-a41e-5c75f6178057',\n                              version='5.0-preview.1')\n        return self._deserialize('AccessTokenResult', response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_signout_token(self):\n        response = self._send(http_method='GET',\n                              location_id='be39e83c-7529-45e9-9c67-0410885880da',\n                              version='5.0-preview.1')\n        return self._deserialize('AccessTokenResult', response)", "response": "Get the signout token for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_feed_changes(self, include_deleted=None, continuation_token=None, batch_size=None):\n        query_parameters = {}\n        if include_deleted is not None:\n            query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'long')\n        if batch_size is not None:\n            query_parameters['batchSize'] = self._serialize.query('batch_size', batch_size, 'int')\n        response = self._send(http_method='GET',\n                              location_id='29ba2dad-389a-4661-b5d3-de76397ca05b',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('FeedChangesResponse', response)", "response": "Get the changes for the specified feed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a feed in the preview API.", "response": "def create_feed(self, feed):\n        \"\"\"CreateFeed.\n        [Preview API] Create a feed, a container for various package types.\n        :param :class:`<Feed> <azure.devops.v5_0.feed.models.Feed>` feed: A JSON object containing both required and optional attributes for the feed. Name is the only required value.\n        :rtype: :class:`<Feed> <azure.devops.v5_0.feed.models.Feed>`\n        \"\"\"\n        content = self._serialize.body(feed, 'Feed')\n        response = self._send(http_method='POST',\n                              location_id='c65009a7-474a-4ad1-8b42-7d852107ef8c',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('Feed', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the settings for a specific feed.", "response": "def get_feed(self, feed_id, include_deleted_upstreams=None):\n        \"\"\"GetFeed.\n        [Preview API] Get the settings for a specific feed.\n        :param str feed_id: Name or Id of the feed.\n        :param bool include_deleted_upstreams: Include upstreams that have been deleted in the response.\n        :rtype: :class:`<Feed> <azure.devops.v5_0.feed.models.Feed>`\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        query_parameters = {}\n        if include_deleted_upstreams is not None:\n            query_parameters['includeDeletedUpstreams'] = self._serialize.query('include_deleted_upstreams', include_deleted_upstreams, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='c65009a7-474a-4ad1-8b42-7d852107ef8c',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Feed', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_feed(self, feed, feed_id):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        content = self._serialize.body(feed, 'FeedUpdate')\n        response = self._send(http_method='PATCH',\n                              location_id='c65009a7-474a-4ad1-8b42-7d852107ef8c',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Feed', response)", "response": "Update the attributes of a feed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_global_permissions(self, global_permissions):\n        content = self._serialize.body(global_permissions, '[GlobalPermission]')\n        response = self._send(http_method='PATCH',\n                              location_id='a74419ef-b477-43df-8758-3cd1cd5f56c6',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('[GlobalPermission]', self._unwrap_collection(response))", "response": "Set global permissions for the organization."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_package_changes(self, feed_id, continuation_token=None, batch_size=None):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        query_parameters = {}\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'long')\n        if batch_size is not None:\n            query_parameters['batchSize'] = self._serialize.query('batch_size', batch_size, 'int')\n        response = self._send(http_method='GET',\n                              location_id='323a0631-d083-4005-85ae-035114dfb681',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('PackageChangesResponse', response)", "response": "Get a batch of package changes made to a feed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget details about a specific package in a specific Feed.", "response": "def get_package(self, feed_id, package_id, include_all_versions=None, include_urls=None, is_listed=None, is_release=None, include_deleted=None, include_description=None):\n        \"\"\"GetPackage.\n        [Preview API] Get details about a specific package.\n        :param str feed_id: Name or Id of the feed.\n        :param str package_id: The package Id (GUID Id, not the package name).\n        :param bool include_all_versions: True to return all versions of the package in the response.  Default is false (latest version only).\n        :param bool include_urls: True to return REST Urls with the response.  Default is True.\n        :param bool is_listed: Only applicable for NuGet packages, setting it for other package types will result in a 404.  If false, delisted package versions will be returned. Use this to filter the response when includeAllVersions is set to true.  Default is unset (do not return delisted packages).\n        :param bool is_release: Only applicable for Nuget packages.  Use this to filter the response when includeAllVersions is set to true.  Default is True (only return packages without prerelease versioning).\n        :param bool include_deleted: Return deleted or unpublished versions of packages in the response. Default is False.\n        :param bool include_description: Return the description for every version of each package in the response.  Default is False.\n        :rtype: :class:`<Package> <azure.devops.v5_0.feed.models.Package>`\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_id is not None:\n            route_values['packageId'] = self._serialize.url('package_id', package_id, 'str')\n        query_parameters = {}\n        if include_all_versions is not None:\n            query_parameters['includeAllVersions'] = self._serialize.query('include_all_versions', include_all_versions, 'bool')\n        if include_urls is not None:\n            query_parameters['includeUrls'] = self._serialize.query('include_urls', include_urls, 'bool')\n        if is_listed is not None:\n            query_parameters['isListed'] = self._serialize.query('is_listed', is_listed, 'bool')\n        if is_release is not None:\n            query_parameters['isRelease'] = self._serialize.query('is_release', is_release, 'bool')\n        if include_deleted is not None:\n            query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')\n        if include_description is not None:\n            query_parameters['includeDescription'] = self._serialize.query('include_description', include_description, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='7a20d846-c929-4acc-9ea2-0d5a7df1b197',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Package', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_packages(self, feed_id, protocol_type=None, package_name_query=None, normalized_package_name=None, include_urls=None, include_all_versions=None, is_listed=None, get_top_package_versions=None, is_release=None, include_description=None, top=None, skip=None, include_deleted=None, is_cached=None, direct_upstream_id=None):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        query_parameters = {}\n        if protocol_type is not None:\n            query_parameters['protocolType'] = self._serialize.query('protocol_type', protocol_type, 'str')\n        if package_name_query is not None:\n            query_parameters['packageNameQuery'] = self._serialize.query('package_name_query', package_name_query, 'str')\n        if normalized_package_name is not None:\n            query_parameters['normalizedPackageName'] = self._serialize.query('normalized_package_name', normalized_package_name, 'str')\n        if include_urls is not None:\n            query_parameters['includeUrls'] = self._serialize.query('include_urls', include_urls, 'bool')\n        if include_all_versions is not None:\n            query_parameters['includeAllVersions'] = self._serialize.query('include_all_versions', include_all_versions, 'bool')\n        if is_listed is not None:\n            query_parameters['isListed'] = self._serialize.query('is_listed', is_listed, 'bool')\n        if get_top_package_versions is not None:\n            query_parameters['getTopPackageVersions'] = self._serialize.query('get_top_package_versions', get_top_package_versions, 'bool')\n        if is_release is not None:\n            query_parameters['isRelease'] = self._serialize.query('is_release', is_release, 'bool')\n        if include_description is not None:\n            query_parameters['includeDescription'] = self._serialize.query('include_description', include_description, 'bool')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if include_deleted is not None:\n            query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')\n        if is_cached is not None:\n            query_parameters['isCached'] = self._serialize.query('is_cached', is_cached, 'bool')\n        if direct_upstream_id is not None:\n            query_parameters['directUpstreamId'] = self._serialize.query('direct_upstream_id', direct_upstream_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='7a20d846-c929-4acc-9ea2-0d5a7df1b197',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[Package]', self._unwrap_collection(response))", "response": "Get the details of all of the packages in a feed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the packages within the recycle bin.", "response": "def get_recycle_bin_packages(self, feed_id, protocol_type=None, package_name_query=None, include_urls=None, top=None, skip=None, include_all_versions=None):\n        \"\"\"GetRecycleBinPackages.\n        [Preview API] Query for packages within the recycle bin.\n        :param str feed_id: Name or Id of the feed.\n        :param str protocol_type: Type of package (e.g. NuGet, npm, ...).\n        :param str package_name_query: Filter to packages matching this name.\n        :param bool include_urls: True to return REST Urls with the response.  Default is True.\n        :param int top: Get the top N packages.\n        :param int skip: Skip the first N packages.\n        :param bool include_all_versions: True to return all versions of the package in the response.  Default is false (latest version only).\n        :rtype: [Package]\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        query_parameters = {}\n        if protocol_type is not None:\n            query_parameters['protocolType'] = self._serialize.query('protocol_type', protocol_type, 'str')\n        if package_name_query is not None:\n            query_parameters['packageNameQuery'] = self._serialize.query('package_name_query', package_name_query, 'str')\n        if include_urls is not None:\n            query_parameters['includeUrls'] = self._serialize.query('include_urls', include_urls, 'bool')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if include_all_versions is not None:\n            query_parameters['includeAllVersions'] = self._serialize.query('include_all_versions', include_all_versions, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='2704e72c-f541-4141-99be-2004b50b05fa',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[Package]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a feed view.", "response": "def delete_feed_view(self, feed_id, view_id):\n        \"\"\"DeleteFeedView.\n        [Preview API] Delete a feed view.\n        :param str feed_id: Name or Id of the feed.\n        :param str view_id: Name or Id of the view.\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if view_id is not None:\n            route_values['viewId'] = self._serialize.url('view_id', view_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='42a8502a-6785-41bc-8c16-89477d930877',\n                   version='5.0-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_user_entitlements(self, user_id):\n        route_values = {}\n        if user_id is not None:\n            route_values['userId'] = self._serialize.url('user_id', user_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='6490e566-b299-49a7-a4e4-28749752581f',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "DeleteUserEntitlements.\n        [Preview API]\n        :param str user_id:"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_account_entitlement_for_user(self, user_id, determine_rights=None, create_if_not_exists=None):\n        route_values = {}\n        if user_id is not None:\n            route_values['userId'] = self._serialize.url('user_id', user_id, 'str')\n        query_parameters = {}\n        if determine_rights is not None:\n            query_parameters['determineRights'] = self._serialize.query('determine_rights', determine_rights, 'bool')\n        if create_if_not_exists is not None:\n            query_parameters['createIfNotExists'] = self._serialize.query('create_if_not_exists', create_if_not_exists, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='6490e566-b299-49a7-a4e4-28749752581f',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('AccountEntitlement', response)", "response": "Get the entitlements for a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transfer_extensions_for_identities(self, identity_mapping):\n        content = self._serialize.body(identity_mapping, '[IdentityMapping]')\n        self._send(http_method='POST',\n                   location_id='da46fe26-dbb6-41d9-9d6b-86bf47e4e444',\n                   version='5.1-preview.1',\n                   content=content)", "response": "This method transfers extensions for the given identities."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying Data Provider Resources.", "response": "def query_data_providers(self, query, scope_name=None, scope_value=None):\n        \"\"\"QueryDataProviders.\n        [Preview API]\n        :param :class:`<DataProviderQuery> <azure.devops.v5_0.contributions.models.DataProviderQuery>` query:\n        :param str scope_name:\n        :param str scope_value:\n        :rtype: :class:`<DataProviderResult> <azure.devops.v5_0.contributions.models.DataProviderResult>`\n        \"\"\"\n        route_values = {}\n        if scope_name is not None:\n            route_values['scopeName'] = self._serialize.url('scope_name', scope_name, 'str')\n        if scope_value is not None:\n            route_values['scopeValue'] = self._serialize.url('scope_value', scope_value, 'str')\n        content = self._serialize.body(query, 'DataProviderQuery')\n        response = self._send(http_method='POST',\n                              location_id='738368db-35ee-4b85-9f94-77ed34af2b0d',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('DataProviderResult', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_installed_extensions(self, contribution_ids=None, include_disabled_apps=None, asset_types=None):\n        query_parameters = {}\n        if contribution_ids is not None:\n            contribution_ids = \";\".join(contribution_ids)\n            query_parameters['contributionIds'] = self._serialize.query('contribution_ids', contribution_ids, 'str')\n        if include_disabled_apps is not None:\n            query_parameters['includeDisabledApps'] = self._serialize.query('include_disabled_apps', include_disabled_apps, 'bool')\n        if asset_types is not None:\n            asset_types = \":\".join(asset_types)\n            query_parameters['assetTypes'] = self._serialize.query('asset_types', asset_types, 'str')\n        response = self._send(http_method='GET',\n                              location_id='2648442b-fd63-4b9a-902f-0c913510f139',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('[InstalledExtension]', self._unwrap_collection(response))", "response": "GetInstalledExtensions.\n        [Preview API]\n        :param [str] contribution_ids:\n        :param bool include_disabled_apps:\n        :param [str] asset_types:\n        :rtype: [InstalledExtension]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute_service_endpoint_request(self, service_endpoint_request, project, endpoint_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if endpoint_id is not None:\n            query_parameters['endpointId'] = self._serialize.query('endpoint_id', endpoint_id, 'str')\n        content = self._serialize.body(service_endpoint_request, 'ServiceEndpointRequest')\n        response = self._send(http_method='POST',\n                              location_id='cc63bb57-2a5f-4a7a-b79c-c142d308657e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('ServiceEndpointRequestResult', response)", "response": "Execute a Service Endpoint Request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a service endpoint.", "response": "def create_service_endpoint(self, endpoint, project):\n        \"\"\"CreateServiceEndpoint.\n        [Preview API] Create a service endpoint.\n        :param :class:`<ServiceEndpoint> <azure.devops.v5_0.service_endpoint.models.ServiceEndpoint>` endpoint: Service endpoint to create.\n        :param str project: Project ID or project name\n        :rtype: :class:`<ServiceEndpoint> <azure.devops.v5_0.service_endpoint.models.ServiceEndpoint>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(endpoint, 'ServiceEndpoint')\n        response = self._send(http_method='POST',\n                              location_id='e85f1c62-adfc-4b74-b618-11a150fb195e',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ServiceEndpoint', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a service endpoint.", "response": "def delete_service_endpoint(self, project, endpoint_id, deep=None):\n        \"\"\"DeleteServiceEndpoint.\n        [Preview API] Delete a service endpoint.\n        :param str project: Project ID or project name\n        :param str endpoint_id: Id of the service endpoint to delete.\n        :param bool deep: Specific to AzureRM endpoint created in Automatic flow. When set to true, this will also delete corresponding AAD application in Azure. Default value is true.\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if endpoint_id is not None:\n            route_values['endpointId'] = self._serialize.url('endpoint_id', endpoint_id, 'str')\n        query_parameters = {}\n        if deep is not None:\n            query_parameters['deep'] = self._serialize.query('deep', deep, 'bool')\n        self._send(http_method='DELETE',\n                   location_id='e85f1c62-adfc-4b74-b618-11a150fb195e',\n                   version='5.0-preview.2',\n                   route_values=route_values,\n                   query_parameters=query_parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_service_endpoint_details(self, project, endpoint_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if endpoint_id is not None:\n            route_values['endpointId'] = self._serialize.url('endpoint_id', endpoint_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e85f1c62-adfc-4b74-b618-11a150fb195e',\n                              version='5.0-preview.2',\n                              route_values=route_values)\n        return self._deserialize('ServiceEndpoint', response)", "response": "Get the details of a specific service endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_service_endpoints_by_names(self, project, endpoint_names, type=None, auth_schemes=None, include_failed=None, include_details=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if endpoint_names is not None:\n            endpoint_names = \",\".join(endpoint_names)\n            query_parameters['endpointNames'] = self._serialize.query('endpoint_names', endpoint_names, 'str')\n        if type is not None:\n            query_parameters['type'] = self._serialize.query('type', type, 'str')\n        if auth_schemes is not None:\n            auth_schemes = \",\".join(auth_schemes)\n            query_parameters['authSchemes'] = self._serialize.query('auth_schemes', auth_schemes, 'str')\n        if include_failed is not None:\n            query_parameters['includeFailed'] = self._serialize.query('include_failed', include_failed, 'bool')\n        if include_details is not None:\n            query_parameters['includeDetails'] = self._serialize.query('include_details', include_details, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='e85f1c62-adfc-4b74-b618-11a150fb195e',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[ServiceEndpoint]', self._unwrap_collection(response))", "response": "Get the service endpoints by name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_service_endpoint(self, endpoint, project, endpoint_id, operation=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if endpoint_id is not None:\n            route_values['endpointId'] = self._serialize.url('endpoint_id', endpoint_id, 'str')\n        query_parameters = {}\n        if operation is not None:\n            query_parameters['operation'] = self._serialize.query('operation', operation, 'str')\n        content = self._serialize.body(endpoint, 'ServiceEndpoint')\n        response = self._send(http_method='PUT',\n                              location_id='e85f1c62-adfc-4b74-b618-11a150fb195e',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('ServiceEndpoint', response)", "response": "Update a service endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the service endpoints.", "response": "def update_service_endpoints(self, endpoints, project):\n        \"\"\"UpdateServiceEndpoints.\n        [Preview API] Update the service endpoints.\n        :param [ServiceEndpoint] endpoints: Names of the service endpoints to update.\n        :param str project: Project ID or project name\n        :rtype: [ServiceEndpoint]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(endpoints, '[ServiceEndpoint]')\n        response = self._send(http_method='PUT',\n                              location_id='e85f1c62-adfc-4b74-b618-11a150fb195e',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[ServiceEndpoint]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_service_endpoint_execution_records(self, project, endpoint_id, top=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if endpoint_id is not None:\n            route_values['endpointId'] = self._serialize.url('endpoint_id', endpoint_id, 'str')\n        query_parameters = {}\n        if top is not None:\n            query_parameters['top'] = self._serialize.query('top', top, 'int')\n        response = self._send(http_method='GET',\n                              location_id='10a16738-9299-4cd1-9a81-fd23ad6200d0',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[ServiceEndpointExecutionRecord]', self._unwrap_collection(response))", "response": "Get Service Endpoint Execution Records."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_service_endpoint_types(self, type=None, scheme=None):\n        query_parameters = {}\n        if type is not None:\n            query_parameters['type'] = self._serialize.query('type', type, 'str')\n        if scheme is not None:\n            query_parameters['scheme'] = self._serialize.query('scheme', scheme, 'str')\n        response = self._send(http_method='GET',\n                              location_id='5a7938a4-655e-486c-b562-b78c54a7e87b',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('[ServiceEndpointType]', self._unwrap_collection(response))", "response": "Get service endpoint types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shell_safe_json_parse(json_or_dict_string, preserve_order=False):\n    try:\n        if not preserve_order:\n            return json.loads(json_or_dict_string)\n        from collections import OrderedDict\n        return json.loads(json_or_dict_string, object_pairs_hook=OrderedDict)\n    except ValueError:\n        import ast\n        return ast.literal_eval(json_or_dict_string)", "response": "Allows the passing of JSON or Python dictionary strings in CMD shell."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_package_versions(self, batch_request, feed_id):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        content = self._serialize.body(batch_request, 'NuGetPackagesBatchRequest')\n        self._send(http_method='POST',\n                   location_id='00c58ea7-d55f-49de-b59f-983533ae11dc',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   content=content)", "response": "Update several packages from a single feed in a single request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an attachment for a specific resource.", "response": "def create_attachment(self, upload_stream, scope_identifier, hub_name, plan_id, timeline_id, record_id, type, name, **kwargs):\n        \"\"\"CreateAttachment.\n        [Preview API]\n        :param object upload_stream: Stream to upload\n        :param str scope_identifier: The project GUID to scope the request\n        :param str hub_name: The name of the server hub: \"build\" for the Build server or \"rm\" for the Release Management server\n        :param str plan_id:\n        :param str timeline_id:\n        :param str record_id:\n        :param str type:\n        :param str name:\n        :rtype: :class:`<TaskAttachment> <azure.devops.v5_0.task.models.TaskAttachment>`\n        \"\"\"\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        if timeline_id is not None:\n            route_values['timelineId'] = self._serialize.url('timeline_id', timeline_id, 'str')\n        if record_id is not None:\n            route_values['recordId'] = self._serialize.url('record_id', record_id, 'str')\n        if type is not None:\n            route_values['type'] = self._serialize.url('type', type, 'str')\n        if name is not None:\n            route_values['name'] = self._serialize.url('name', name, 'str')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        content = self._client.stream_upload(upload_stream, callback=callback)\n        response = self._send(http_method='PUT',\n                              location_id='7898f959-9cdf-4096-b29e-7f293031629e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content,\n                              media_type='application/octet-stream')\n        return self._deserialize('TaskAttachment', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the metadata for a specific resource.", "response": "def get_attachment(self, scope_identifier, hub_name, plan_id, timeline_id, record_id, type, name):\n        \"\"\"GetAttachment.\n        [Preview API]\n        :param str scope_identifier: The project GUID to scope the request\n        :param str hub_name: The name of the server hub: \"build\" for the Build server or \"rm\" for the Release Management server\n        :param str plan_id:\n        :param str timeline_id:\n        :param str record_id:\n        :param str type:\n        :param str name:\n        :rtype: :class:`<TaskAttachment> <azure.devops.v5_0.task.models.TaskAttachment>`\n        \"\"\"\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        if timeline_id is not None:\n            route_values['timelineId'] = self._serialize.url('timeline_id', timeline_id, 'str')\n        if record_id is not None:\n            route_values['recordId'] = self._serialize.url('record_id', record_id, 'str')\n        if type is not None:\n            route_values['type'] = self._serialize.url('type', type, 'str')\n        if name is not None:\n            route_values['name'] = self._serialize.url('name', name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='7898f959-9cdf-4096-b29e-7f293031629e',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('TaskAttachment', response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_attachment_content(self, scope_identifier, hub_name, plan_id, timeline_id, record_id, type, name, **kwargs):\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        if timeline_id is not None:\n            route_values['timelineId'] = self._serialize.url('timeline_id', timeline_id, 'str')\n        if record_id is not None:\n            route_values['recordId'] = self._serialize.url('record_id', record_id, 'str')\n        if type is not None:\n            route_values['type'] = self._serialize.url('type', type, 'str')\n        if name is not None:\n            route_values['name'] = self._serialize.url('name', name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='7898f959-9cdf-4096-b29e-7f293031629e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get the content of an attachment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nappending log content to the specified log.", "response": "def append_log_content(self, upload_stream, scope_identifier, hub_name, plan_id, log_id, **kwargs):\n        \"\"\"AppendLogContent.\n        :param object upload_stream: Stream to upload\n        :param str scope_identifier: The project GUID to scope the request\n        :param str hub_name: The name of the server hub: \"build\" for the Build server or \"rm\" for the Release Management server\n        :param str plan_id:\n        :param int log_id:\n        :rtype: :class:`<TaskLog> <azure.devops.v5_0.task.models.TaskLog>`\n        \"\"\"\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        if log_id is not None:\n            route_values['logId'] = self._serialize.url('log_id', log_id, 'int')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        content = self._client.stream_upload(upload_stream, callback=callback)\n        response = self._send(http_method='POST',\n                              location_id='46f5667d-263a-4684-91b1-dff7fdcf64e2',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content,\n                              media_type='application/octet-stream')\n        return self._deserialize('TaskLog', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new log in the specified project.", "response": "def create_log(self, log, scope_identifier, hub_name, plan_id):\n        \"\"\"CreateLog.\n        :param :class:`<TaskLog> <azure.devops.v5_0.task.models.TaskLog>` log:\n        :param str scope_identifier: The project GUID to scope the request\n        :param str hub_name: The name of the server hub: \"build\" for the Build server or \"rm\" for the Release Management server\n        :param str plan_id:\n        :rtype: :class:`<TaskLog> <azure.devops.v5_0.task.models.TaskLog>`\n        \"\"\"\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        content = self._serialize.body(log, 'TaskLog')\n        response = self._send(http_method='POST',\n                              location_id='46f5667d-263a-4684-91b1-dff7fdcf64e2',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('TaskLog', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_log(self, scope_identifier, hub_name, plan_id, log_id, start_line=None, end_line=None):\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        if log_id is not None:\n            route_values['logId'] = self._serialize.url('log_id', log_id, 'int')\n        query_parameters = {}\n        if start_line is not None:\n            query_parameters['startLine'] = self._serialize.query('start_line', start_line, 'long')\n        if end_line is not None:\n            query_parameters['endLine'] = self._serialize.query('end_line', end_line, 'long')\n        response = self._send(http_method='GET',\n                              location_id='46f5667d-263a-4684-91b1-dff7fdcf64e2',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[str]', self._unwrap_collection(response))", "response": "Get the log for a given project."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the records for a given scope.", "response": "def get_records(self, scope_identifier, hub_name, plan_id, timeline_id, change_id=None):\n        \"\"\"GetRecords.\n        :param str scope_identifier: The project GUID to scope the request\n        :param str hub_name: The name of the server hub: \"build\" for the Build server or \"rm\" for the Release Management server\n        :param str plan_id:\n        :param str timeline_id:\n        :param int change_id:\n        :rtype: [TimelineRecord]\n        \"\"\"\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        if timeline_id is not None:\n            route_values['timelineId'] = self._serialize.url('timeline_id', timeline_id, 'str')\n        query_parameters = {}\n        if change_id is not None:\n            query_parameters['changeId'] = self._serialize.query('change_id', change_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='8893bc5b-35b2-4be7-83cb-99e683551db4',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TimelineRecord]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the records in the specified project.", "response": "def update_records(self, records, scope_identifier, hub_name, plan_id, timeline_id):\n        \"\"\"UpdateRecords.\n        :param :class:`<VssJsonCollectionWrapper> <azure.devops.v5_0.task.models.VssJsonCollectionWrapper>` records:\n        :param str scope_identifier: The project GUID to scope the request\n        :param str hub_name: The name of the server hub: \"build\" for the Build server or \"rm\" for the Release Management server\n        :param str plan_id:\n        :param str timeline_id:\n        :rtype: [TimelineRecord]\n        \"\"\"\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        if timeline_id is not None:\n            route_values['timelineId'] = self._serialize.url('timeline_id', timeline_id, 'str')\n        content = self._serialize.body(records, 'VssJsonCollectionWrapper')\n        response = self._send(http_method='PATCH',\n                              location_id='8893bc5b-35b2-4be7-83cb-99e683551db4',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[TimelineRecord]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_timeline(self, timeline, scope_identifier, hub_name, plan_id):\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        content = self._serialize.body(timeline, 'Timeline')\n        response = self._send(http_method='POST',\n                              location_id='83597576-cc2c-453c-bea6-2882ae6a1653',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Timeline', response)", "response": "Create a new resource in the specified project."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a specific resource from the cache.", "response": "def delete_timeline(self, scope_identifier, hub_name, plan_id, timeline_id):\n        \"\"\"DeleteTimeline.\n        :param str scope_identifier: The project GUID to scope the request\n        :param str hub_name: The name of the server hub: \"build\" for the Build server or \"rm\" for the Release Management server\n        :param str plan_id:\n        :param str timeline_id:\n        \"\"\"\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        if timeline_id is not None:\n            route_values['timelineId'] = self._serialize.url('timeline_id', timeline_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='83597576-cc2c-453c-bea6-2882ae6a1653',\n                   version='5.0',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the list of Timelines for a given project.", "response": "def get_timelines(self, scope_identifier, hub_name, plan_id):\n        \"\"\"GetTimelines.\n        :param str scope_identifier: The project GUID to scope the request\n        :param str hub_name: The name of the server hub: \"build\" for the Build server or \"rm\" for the Release Management server\n        :param str plan_id:\n        :rtype: [Timeline]\n        \"\"\"\n        route_values = {}\n        if scope_identifier is not None:\n            route_values['scopeIdentifier'] = self._serialize.url('scope_identifier', scope_identifier, 'str')\n        if hub_name is not None:\n            route_values['hubName'] = self._serialize.url('hub_name', hub_name, 'str')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='83597576-cc2c-453c-bea6-2882ae6a1653',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('[Timeline]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new connected service.", "response": "def create_connected_service(self, connected_service_creation_data, project_id):\n        \"\"\"CreateConnectedService.\n        [Preview API]\n        :param :class:`<WebApiConnectedServiceDetails> <azure.devops.v5_0.core.models.WebApiConnectedServiceDetails>` connected_service_creation_data:\n        :param str project_id:\n        :rtype: :class:`<WebApiConnectedService> <azure.devops.v5_0.core.models.WebApiConnectedService>`\n        \"\"\"\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        content = self._serialize.body(connected_service_creation_data, 'WebApiConnectedServiceDetails')\n        response = self._send(http_method='POST',\n                              location_id='b4f70219-e18b-42c5-abe3-98b07d35525e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WebApiConnectedService', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_connected_service_details(self, project_id, name):\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        if name is not None:\n            route_values['name'] = self._serialize.url('name', name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='b4f70219-e18b-42c5-abe3-98b07d35525e',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('WebApiConnectedServiceDetails', response)", "response": "Get the connected service details."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a specific project by id or name.", "response": "def get_project(self, project_id, include_capabilities=None, include_history=None):\n        \"\"\"GetProject.\n        Get project with the specified id or name, optionally including capabilities.\n        :param str project_id:\n        :param bool include_capabilities: Include capabilities (such as source control) in the team project result (default: false).\n        :param bool include_history: Search within renamed projects (that had such name in the past).\n        :rtype: :class:`<TeamProject> <azure.devops.v5_0.core.models.TeamProject>`\n        \"\"\"\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        query_parameters = {}\n        if include_capabilities is not None:\n            query_parameters['includeCapabilities'] = self._serialize.query('include_capabilities', include_capabilities, 'bool')\n        if include_history is not None:\n            query_parameters['includeHistory'] = self._serialize.query('include_history', include_history, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='603fe2ac-9723-48b9-88ad-09305aa6c6e1',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('TeamProject', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_projects(self, state_filter=None, top=None, skip=None, continuation_token=None, get_default_team_image_url=None):\n        query_parameters = {}\n        if state_filter is not None:\n            query_parameters['stateFilter'] = self._serialize.query('state_filter', state_filter, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        if get_default_team_image_url is not None:\n            query_parameters['getDefaultTeamImageUrl'] = self._serialize.query('get_default_team_image_url', get_default_team_image_url, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='603fe2ac-9723-48b9-88ad-09305aa6c6e1',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[TeamProjectReference]', self._unwrap_collection(response))", "response": "Get all projects in the organization that the authenticated user has access to."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef queue_create_project(self, project_to_create):\n        content = self._serialize.body(project_to_create, 'TeamProject')\n        response = self._send(http_method='POST',\n                              location_id='603fe2ac-9723-48b9-88ad-09305aa6c6e1',\n                              version='5.0',\n                              content=content)\n        return self._deserialize('OperationReference', response)", "response": "Queues a project to be created."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_or_update_proxy(self, proxy):\n        content = self._serialize.body(proxy, 'Proxy')\n        response = self._send(http_method='PUT',\n                              location_id='ec1f4311-f2b4-4c15-b2b8-8990b80d2908',\n                              version='5.0-preview.2',\n                              content=content)\n        return self._deserialize('Proxy', response)", "response": "Create or update a proxy."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_proxies(self, proxy_url=None):\n        query_parameters = {}\n        if proxy_url is not None:\n            query_parameters['proxyUrl'] = self._serialize.query('proxy_url', proxy_url, 'str')\n        response = self._send(http_method='GET',\n                              location_id='ec1f4311-f2b4-4c15-b2b8-8990b80d2908',\n                              version='5.0-preview.2',\n                              query_parameters=query_parameters)\n        return self._deserialize('[Proxy]', self._unwrap_collection(response))", "response": "GetProxies.\n        [Preview API]\n        :param str proxy_url:\n        :rtype: [Proxy]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_content_scoped_package(self, feed_id, package_scope, unscoped_package_name, package_version, **kwargs):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_scope is not None:\n            route_values['packageScope'] = self._serialize.url('package_scope', package_scope, 'str')\n        if unscoped_package_name is not None:\n            route_values['unscopedPackageName'] = self._serialize.url('unscoped_package_name', unscoped_package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        response = self._send(http_method='GET',\n                              location_id='09a4eafd-123a-495c-979c-0eda7bdb9a14',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get a content - scoped version of a package."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_content_unscoped_package(self, feed_id, package_name, package_version, **kwargs):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_name is not None:\n            route_values['packageName'] = self._serialize.url('package_name', package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        response = self._send(http_method='GET',\n                              location_id='75caa482-cb1e-47cd-9f2c-c048a4b7a43e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get an unscoped npm package."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_scoped_package_version_from_recycle_bin(self, feed_id, package_scope, unscoped_package_name, package_version):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_scope is not None:\n            route_values['packageScope'] = self._serialize.url('package_scope', package_scope, 'str')\n        if unscoped_package_name is not None:\n            route_values['unscopedPackageName'] = self._serialize.url('unscoped_package_name', unscoped_package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        self._send(http_method='DELETE',\n                   location_id='220f45eb-94a5-432c-902a-5b8c6372e415',\n                   version='5.0-preview.1',\n                   route_values=route_values)", "response": "Delete a package version with an npm scope from the recycle bin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_scoped_package_version_metadata_from_recycle_bin(self, feed_id, package_scope, unscoped_package_name, package_version):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_scope is not None:\n            route_values['packageScope'] = self._serialize.url('package_scope', package_scope, 'str')\n        if unscoped_package_name is not None:\n            route_values['unscopedPackageName'] = self._serialize.url('unscoped_package_name', unscoped_package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        response = self._send(http_method='GET',\n                              location_id='220f45eb-94a5-432c-902a-5b8c6372e415',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('NpmPackageVersionDeletionState', response)", "response": "Get the metadata about a scoped package version in the recycle bin."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restore_scoped_package_version_from_recycle_bin(self, package_version_details, feed_id, package_scope, unscoped_package_name, package_version):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_scope is not None:\n            route_values['packageScope'] = self._serialize.url('package_scope', package_scope, 'str')\n        if unscoped_package_name is not None:\n            route_values['unscopedPackageName'] = self._serialize.url('unscoped_package_name', unscoped_package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        content = self._serialize.body(package_version_details, 'NpmRecycleBinPackageVersionDetails')\n        self._send(http_method='PATCH',\n                   location_id='220f45eb-94a5-432c-902a-5b8c6372e415',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   content=content)", "response": "Restores a package version with an npm scope from the recycle bin to its feed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_scoped_package(self, package_version_details, feed_id, package_scope, unscoped_package_name, package_version):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_scope is not None:\n            route_values['packageScope'] = self._serialize.url('package_scope', package_scope, 'str')\n        if unscoped_package_name is not None:\n            route_values['unscopedPackageName'] = self._serialize.url('unscoped_package_name', unscoped_package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        content = self._serialize.body(package_version_details, 'PackageVersionDetails')\n        response = self._send(http_method='PATCH',\n                              location_id='e93d9ec3-4022-401e-96b0-83ea5d911e09',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Package', response)", "response": "Update the current scoped package for the given feed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_package_info(self, feed_id, package_name, package_version):\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_name is not None:\n            route_values['packageName'] = self._serialize.url('package_name', package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        response = self._send(http_method='GET',\n                              location_id='ed579d62-67c9-4271-be66-9b029af5bcf9',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('Package', response)", "response": "Get information about an unscoped package version."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the feed s package.", "response": "def update_package(self, package_version_details, feed_id, package_name, package_version):\n        \"\"\"UpdatePackage.\n        [Preview API]\n        :param :class:`<PackageVersionDetails> <azure.devops.v5_0.npm.models.PackageVersionDetails>` package_version_details:\n        :param str feed_id:\n        :param str package_name:\n        :param str package_version:\n        :rtype: :class:`<Package> <azure.devops.v5_0.npm.models.Package>`\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_name is not None:\n            route_values['packageName'] = self._serialize.url('package_name', package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        content = self._serialize.body(package_version_details, 'PackageVersionDetails')\n        response = self._send(http_method='PATCH',\n                              location_id='ed579d62-67c9-4271-be66-9b029af5bcf9',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Package', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a dashboard given its ID.", "response": "def delete_dashboard(self, team_context, dashboard_id):\n        \"\"\"DeleteDashboard.\n        [Preview API] Delete a dashboard given its ID. This also deletes the widgets associated with this dashboard.\n        :param :class:`<TeamContext> <azure.devops.v5_0.dashboard.models.TeamContext>` team_context: The team context for the operation\n        :param str dashboard_id: ID of the dashboard to delete.\n        \"\"\"\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if dashboard_id is not None:\n            route_values['dashboardId'] = self._serialize.url('dashboard_id', dashboard_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='454b3e51-2e6e-48d4-ad81-978154089351',\n                   version='5.0-preview.2',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replace_widgets(self, widgets, team_context, dashboard_id, eTag=None):\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if dashboard_id is not None:\n            route_values['dashboardId'] = self._serialize.url('dashboard_id', dashboard_id, 'str')\n        content = self._serialize.body(widgets, '[Widget]')\n        response = self._send(http_method='PUT',\n                              location_id='bdcff53a-8355-4172-a00a-40497ea23afc',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        response_object = models.WidgetsVersionedList()\n        response_object.widgets = self._deserialize('[Widget]', self._unwrap_collection(response))\n        response_object.eTag = response.headers.get('ETag')\n        return response_object", "response": "Replace the widgets on the specified dashboard with the supplied widgets."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_widget_metadata(self, contribution_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if contribution_id is not None:\n            route_values['contributionId'] = self._serialize.url('contribution_id', contribution_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='6b3628d3-e96f-4fc7-b176-50240b03b515',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('WidgetMetadataResponse', response)", "response": "Get the widget metadata for the specified contribution ID."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_widget_types(self, scope, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if scope is not None:\n            query_parameters['$scope'] = self._serialize.query('scope', scope, 'str')\n        response = self._send(http_method='GET',\n                              location_id='6b3628d3-e96f-4fc7-b176-50240b03b515',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('WidgetTypesResponse', response)", "response": "Get all available widget types in alphabetical order."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_feature(self, feature_id):\n        route_values = {}\n        if feature_id is not None:\n            route_values['featureId'] = self._serialize.url('feature_id', feature_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='c4209f25-7a27-41dd-9f04-06080c7b6afd',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('ContributedFeature', response)", "response": "Get a specific feature by its id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_features(self, target_contribution_id=None):\n        query_parameters = {}\n        if target_contribution_id is not None:\n            query_parameters['targetContributionId'] = self._serialize.query('target_contribution_id', target_contribution_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='c4209f25-7a27-41dd-9f04-06080c7b6afd',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('[ContributedFeature]', self._unwrap_collection(response))", "response": "Get a list of all defined features."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the state of a specific feature for a given user and all - users scope.", "response": "def get_feature_state(self, feature_id, user_scope):\n        \"\"\"GetFeatureState.\n        [Preview API] Get the state of the specified feature for the given user/all-users scope\n        :param str feature_id: Contribution id of the feature\n        :param str user_scope: User-Scope at which to get the value. Should be \"me\" for the current user or \"host\" for all users.\n        :rtype: :class:`<ContributedFeatureState> <azure.devops.v5_0.feature_management.models.ContributedFeatureState>`\n        \"\"\"\n        route_values = {}\n        if feature_id is not None:\n            route_values['featureId'] = self._serialize.url('feature_id', feature_id, 'str')\n        if user_scope is not None:\n            route_values['userScope'] = self._serialize.url('user_scope', user_scope, 'str')\n        response = self._send(http_method='GET',\n                              location_id='98911314-3f9b-4eaf-80e8-83900d8e85d9',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('ContributedFeatureState', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_feature_state(self, feature, feature_id, user_scope, reason=None, reason_code=None):\n        route_values = {}\n        if feature_id is not None:\n            route_values['featureId'] = self._serialize.url('feature_id', feature_id, 'str')\n        if user_scope is not None:\n            route_values['userScope'] = self._serialize.url('user_scope', user_scope, 'str')\n        query_parameters = {}\n        if reason is not None:\n            query_parameters['reason'] = self._serialize.query('reason', reason, 'str')\n        if reason_code is not None:\n            query_parameters['reasonCode'] = self._serialize.query('reason_code', reason_code, 'str')\n        content = self._serialize.body(feature, 'ContributedFeatureState')\n        response = self._send(http_method='PATCH',\n                              location_id='98911314-3f9b-4eaf-80e8-83900d8e85d9',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('ContributedFeatureState', response)", "response": "Set the state of a feature."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_feature_state_for_scope(self, feature_id, user_scope, scope_name, scope_value):\n        route_values = {}\n        if feature_id is not None:\n            route_values['featureId'] = self._serialize.url('feature_id', feature_id, 'str')\n        if user_scope is not None:\n            route_values['userScope'] = self._serialize.url('user_scope', user_scope, 'str')\n        if scope_name is not None:\n            route_values['scopeName'] = self._serialize.url('scope_name', scope_name, 'str')\n        if scope_value is not None:\n            route_values['scopeValue'] = self._serialize.url('scope_value', scope_value, 'str')\n        response = self._send(http_method='GET',\n                              location_id='dd291e43-aa9f-4cee-8465-a93c78e414a4',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('ContributedFeatureState', response)", "response": "Get the state of a specific feature for a specific user - scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef query_feature_states_for_default_scope(self, query, user_scope):\n        route_values = {}\n        if user_scope is not None:\n            route_values['userScope'] = self._serialize.url('user_scope', user_scope, 'str')\n        content = self._serialize.body(query, 'ContributedFeatureStateQuery')\n        response = self._send(http_method='POST',\n                              location_id='3f810f28-03e2-4239-b0bc-788add3005e5',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ContributedFeatureStateQuery', response)", "response": "Query the states of the specified features for the default scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_pipeline(self, input_parameters, project):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(input_parameters, 'CreatePipelineParameters')\n        response = self._send(http_method='POST',\n                              location_id='28e1305e-2afe-47bf-abaf-cbb0e6a91988',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Pipeline', response)", "response": "Create a new pipeline."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_pipeline(self, project, pipeline_id, revision=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if pipeline_id is not None:\n            route_values['pipelineId'] = self._serialize.url('pipeline_id', pipeline_id, 'int')\n        query_parameters = {}\n        if revision is not None:\n            query_parameters['revision'] = self._serialize.query('revision', revision, 'int')\n        response = self._send(http_method='GET',\n                              location_id='28e1305e-2afe-47bf-abaf-cbb0e6a91988',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Pipeline', response)", "response": "Get a specific pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_agent(self, pool_id, agent_id):\n        route_values = {}\n        if pool_id is not None:\n            route_values['poolId'] = self._serialize.url('pool_id', pool_id, 'int')\n        if agent_id is not None:\n            route_values['agentId'] = self._serialize.url('agent_id', agent_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='e298ef32-5878-4cab-993c-043836571f42',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "Delete an agent from a pool."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_agent(self, pool_id, agent_id, include_capabilities=None, include_assigned_request=None, include_last_completed_request=None, property_filters=None):\n        route_values = {}\n        if pool_id is not None:\n            route_values['poolId'] = self._serialize.url('pool_id', pool_id, 'int')\n        if agent_id is not None:\n            route_values['agentId'] = self._serialize.url('agent_id', agent_id, 'int')\n        query_parameters = {}\n        if include_capabilities is not None:\n            query_parameters['includeCapabilities'] = self._serialize.query('include_capabilities', include_capabilities, 'bool')\n        if include_assigned_request is not None:\n            query_parameters['includeAssignedRequest'] = self._serialize.query('include_assigned_request', include_assigned_request, 'bool')\n        if include_last_completed_request is not None:\n            query_parameters['includeLastCompletedRequest'] = self._serialize.query('include_last_completed_request', include_last_completed_request, 'bool')\n        if property_filters is not None:\n            property_filters = \",\".join(property_filters)\n            query_parameters['propertyFilters'] = self._serialize.query('property_filters', property_filters, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e298ef32-5878-4cab-993c-043836571f42',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('TaskAgent', response)", "response": "Get information about an agent."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_agents(self, pool_id, agent_name=None, include_capabilities=None, include_assigned_request=None, include_last_completed_request=None, property_filters=None, demands=None):\n        route_values = {}\n        if pool_id is not None:\n            route_values['poolId'] = self._serialize.url('pool_id', pool_id, 'int')\n        query_parameters = {}\n        if agent_name is not None:\n            query_parameters['agentName'] = self._serialize.query('agent_name', agent_name, 'str')\n        if include_capabilities is not None:\n            query_parameters['includeCapabilities'] = self._serialize.query('include_capabilities', include_capabilities, 'bool')\n        if include_assigned_request is not None:\n            query_parameters['includeAssignedRequest'] = self._serialize.query('include_assigned_request', include_assigned_request, 'bool')\n        if include_last_completed_request is not None:\n            query_parameters['includeLastCompletedRequest'] = self._serialize.query('include_last_completed_request', include_last_completed_request, 'bool')\n        if property_filters is not None:\n            property_filters = \",\".join(property_filters)\n            query_parameters['propertyFilters'] = self._serialize.query('property_filters', property_filters, 'str')\n        if demands is not None:\n            demands = \",\".join(demands)\n            query_parameters['demands'] = self._serialize.query('demands', demands, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e298ef32-5878-4cab-993c-043836571f42',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TaskAgent]', self._unwrap_collection(response))", "response": "Get a list of agents in a specific agent pool."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_agent(self, agent, pool_id, agent_id):\n        route_values = {}\n        if pool_id is not None:\n            route_values['poolId'] = self._serialize.url('pool_id', pool_id, 'int')\n        if agent_id is not None:\n            route_values['agentId'] = self._serialize.url('agent_id', agent_id, 'int')\n        content = self._serialize.body(agent, 'TaskAgent')\n        response = self._send(http_method='PUT',\n                              location_id='e298ef32-5878-4cab-993c-043836571f42',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('TaskAgent', response)", "response": "Replace an agent in a pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_deployment_group(self, deployment_group, project):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(deployment_group, 'DeploymentGroupCreateParameter')\n        response = self._send(http_method='POST',\n                              location_id='083c4d89-ab35-45af-aa11-7cf66895c53e',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('DeploymentGroup', response)", "response": "Create a new deployment group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_agent_pool(self, pool):\n        content = self._serialize.body(pool, 'TaskAgentPool')\n        response = self._send(http_method='POST',\n                              location_id='a8c47e17-4d56-4a56-92bb-de7ea7dc65be',\n                              version='5.1-preview.1',\n                              content=content)\n        return self._deserialize('TaskAgentPool', response)", "response": "Add an agent pool."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_agent_pool(self, pool_id):\n        route_values = {}\n        if pool_id is not None:\n            route_values['poolId'] = self._serialize.url('pool_id', pool_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='a8c47e17-4d56-4a56-92bb-de7ea7dc65be',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "Delete an agent pool."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget information about an agent pool.", "response": "def get_agent_pool(self, pool_id, properties=None, action_filter=None):\n        \"\"\"GetAgentPool.\n        [Preview API] Get information about an agent pool.\n        :param int pool_id: An agent pool ID\n        :param [str] properties: Agent pool properties (comma-separated)\n        :param str action_filter: Filter by whether the calling user has use or manage permissions\n        :rtype: :class:`<TaskAgentPool> <azure.devops.v5_1.task-agent.models.TaskAgentPool>`\n        \"\"\"\n        route_values = {}\n        if pool_id is not None:\n            route_values['poolId'] = self._serialize.url('pool_id', pool_id, 'int')\n        query_parameters = {}\n        if properties is not None:\n            properties = \",\".join(properties)\n            query_parameters['properties'] = self._serialize.query('properties', properties, 'str')\n        if action_filter is not None:\n            query_parameters['actionFilter'] = self._serialize.query('action_filter', action_filter, 'str')\n        response = self._send(http_method='GET',\n                              location_id='a8c47e17-4d56-4a56-92bb-de7ea7dc65be',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('TaskAgentPool', response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_agent_pools(self, pool_name=None, properties=None, pool_type=None, action_filter=None):\n        query_parameters = {}\n        if pool_name is not None:\n            query_parameters['poolName'] = self._serialize.query('pool_name', pool_name, 'str')\n        if properties is not None:\n            properties = \",\".join(properties)\n            query_parameters['properties'] = self._serialize.query('properties', properties, 'str')\n        if pool_type is not None:\n            query_parameters['poolType'] = self._serialize.query('pool_type', pool_type, 'str')\n        if action_filter is not None:\n            query_parameters['actionFilter'] = self._serialize.query('action_filter', action_filter, 'str')\n        response = self._send(http_method='GET',\n                              location_id='a8c47e17-4d56-4a56-92bb-de7ea7dc65be',\n                              version='5.1-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('[TaskAgentPool]', self._unwrap_collection(response))", "response": "Get a list of Agent Pool objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_agent_pool(self, pool, pool_id):\n        route_values = {}\n        if pool_id is not None:\n            route_values['poolId'] = self._serialize.url('pool_id', pool_id, 'int')\n        content = self._serialize.body(pool, 'TaskAgentPool')\n        response = self._send(http_method='PATCH',\n                              location_id='a8c47e17-4d56-4a56-92bb-de7ea7dc65be',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('TaskAgentPool', response)", "response": "Update agent pool details."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_agent_queue(self, queue, project=None, authorize_pipelines=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if authorize_pipelines is not None:\n            query_parameters['authorizePipelines'] = self._serialize.query('authorize_pipelines', authorize_pipelines, 'bool')\n        content = self._serialize.body(queue, 'TaskAgentQueue')\n        response = self._send(http_method='POST',\n                              location_id='900fa995-c559-4923-aae7-f8424fe4fbea',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('TaskAgentQueue', response)", "response": "Create a new agent queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete an agent queue from a project.", "response": "def delete_agent_queue(self, queue_id, project=None):\n        \"\"\"DeleteAgentQueue.\n        [Preview API] Removes an agent queue from a project.\n        :param int queue_id: The agent queue to remove\n        :param str project: Project ID or project name\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if queue_id is not None:\n            route_values['queueId'] = self._serialize.url('queue_id', queue_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='900fa995-c559-4923-aae7-f8424fe4fbea',\n                   version='5.1-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_agent_queue(self, queue_id, project=None, action_filter=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if queue_id is not None:\n            route_values['queueId'] = self._serialize.url('queue_id', queue_id, 'int')\n        query_parameters = {}\n        if action_filter is not None:\n            query_parameters['actionFilter'] = self._serialize.query('action_filter', action_filter, 'str')\n        response = self._send(http_method='GET',\n                              location_id='900fa995-c559-4923-aae7-f8424fe4fbea',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('TaskAgentQueue', response)", "response": "Get information about an agent queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a list of all the Agent Queues in a project.", "response": "def get_agent_queues(self, project=None, queue_name=None, action_filter=None):\n        \"\"\"GetAgentQueues.\n        [Preview API] Get a list of agent queues.\n        :param str project: Project ID or project name\n        :param str queue_name: Filter on the agent queue name\n        :param str action_filter: Filter by whether the calling user has use or manage permissions\n        :rtype: [TaskAgentQueue]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if queue_name is not None:\n            query_parameters['queueName'] = self._serialize.query('queue_name', queue_name, 'str')\n        if action_filter is not None:\n            query_parameters['actionFilter'] = self._serialize.query('action_filter', action_filter, 'str')\n        response = self._send(http_method='GET',\n                              location_id='900fa995-c559-4923-aae7-f8424fe4fbea',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TaskAgentQueue]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_agent_queues_by_ids(self, queue_ids, project=None, action_filter=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if queue_ids is not None:\n            queue_ids = \",\".join(map(str, queue_ids))\n            query_parameters['queueIds'] = self._serialize.query('queue_ids', queue_ids, 'str')\n        if action_filter is not None:\n            query_parameters['actionFilter'] = self._serialize.query('action_filter', action_filter, 'str')\n        response = self._send(http_method='GET',\n                              location_id='900fa995-c559-4923-aae7-f8424fe4fbea',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TaskAgentQueue]', self._unwrap_collection(response))", "response": "Get a list of Agent Queues by their IDs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of TaskAgentQueues by their names.", "response": "def get_agent_queues_by_names(self, queue_names, project=None, action_filter=None):\n        \"\"\"GetAgentQueuesByNames.\n        [Preview API] Get a list of agent queues by their names\n        :param [str] queue_names: A comma-separated list of agent names to retrieve\n        :param str project: Project ID or project name\n        :param str action_filter: Filter by whether the calling user has use or manage permissions\n        :rtype: [TaskAgentQueue]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if queue_names is not None:\n            queue_names = \",\".join(queue_names)\n            query_parameters['queueNames'] = self._serialize.query('queue_names', queue_names, 'str')\n        if action_filter is not None:\n            query_parameters['actionFilter'] = self._serialize.query('action_filter', action_filter, 'str')\n        response = self._send(http_method='GET',\n                              location_id='900fa995-c559-4923-aae7-f8424fe4fbea',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TaskAgentQueue]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_agent_cloud_requests(self, agent_cloud_id):\n        route_values = {}\n        if agent_cloud_id is not None:\n            route_values['agentCloudId'] = self._serialize.url('agent_cloud_id', agent_cloud_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='20189bd7-5134-49c2-b8e9-f9e856eea2b2',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('[TaskAgentCloudRequest]', self._unwrap_collection(response))", "response": "Get Agent Cloud Requests."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a deployment target in a deployment group.", "response": "def delete_deployment_target(self, project, deployment_group_id, target_id):\n        \"\"\"DeleteDeploymentTarget.\n        [Preview API] Delete a deployment target in a deployment group. This deletes the agent from associated deployment pool too.\n        :param str project: Project ID or project name\n        :param int deployment_group_id: ID of the deployment group in which deployment target is deleted.\n        :param int target_id: ID of the deployment target to delete.\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if deployment_group_id is not None:\n            route_values['deploymentGroupId'] = self._serialize.url('deployment_group_id', deployment_group_id, 'int')\n        if target_id is not None:\n            route_values['targetId'] = self._serialize.url('target_id', target_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='2f0aa599-c121-4256-a5fd-ba370e0ae7b6',\n                   version='5.1-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_deployment_target(self, project, deployment_group_id, target_id, expand=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if deployment_group_id is not None:\n            route_values['deploymentGroupId'] = self._serialize.url('deployment_group_id', deployment_group_id, 'int')\n        if target_id is not None:\n            route_values['targetId'] = self._serialize.url('target_id', target_id, 'int')\n        query_parameters = {}\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='2f0aa599-c121-4256-a5fd-ba370e0ae7b6',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('DeploymentMachine', response)", "response": "Get a deployment target by its ID in a deployment group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a list of deployment targets in a deployment group.", "response": "def get_deployment_targets(self, project, deployment_group_id, tags=None, name=None, partial_name_match=None, expand=None, agent_status=None, agent_job_result=None, continuation_token=None, top=None, enabled=None, property_filters=None):\n        \"\"\"GetDeploymentTargets.\n        [Preview API] Get a list of deployment targets in a deployment group.\n        :param str project: Project ID or project name\n        :param int deployment_group_id: ID of the deployment group.\n        :param [str] tags: Get only the deployment targets that contain all these comma separted list of tags.\n        :param str name: Name pattern of the deployment targets to return.\n        :param bool partial_name_match: When set to true, treats **name** as pattern. Else treats it as absolute match. Default is **false**.\n        :param str expand: Include these additional details in the returned objects.\n        :param str agent_status: Get only deployment targets that have this status.\n        :param str agent_job_result: Get only deployment targets that have this last job result.\n        :param str continuation_token: Get deployment targets with names greater than this continuationToken lexicographically.\n        :param int top: Maximum number of deployment targets to return. Default is **1000**.\n        :param bool enabled: Get only deployment targets that are enabled or disabled. Default is 'null' which returns all the targets.\n        :param [str] property_filters:\n        :rtype: [DeploymentMachine]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if deployment_group_id is not None:\n            route_values['deploymentGroupId'] = self._serialize.url('deployment_group_id', deployment_group_id, 'int')\n        query_parameters = {}\n        if tags is not None:\n            tags = \",\".join(tags)\n            query_parameters['tags'] = self._serialize.query('tags', tags, 'str')\n        if name is not None:\n            query_parameters['name'] = self._serialize.query('name', name, 'str')\n        if partial_name_match is not None:\n            query_parameters['partialNameMatch'] = self._serialize.query('partial_name_match', partial_name_match, 'bool')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        if agent_status is not None:\n            query_parameters['agentStatus'] = self._serialize.query('agent_status', agent_status, 'str')\n        if agent_job_result is not None:\n            query_parameters['agentJobResult'] = self._serialize.query('agent_job_result', agent_job_result, 'str')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if enabled is not None:\n            query_parameters['enabled'] = self._serialize.query('enabled', enabled, 'bool')\n        if property_filters is not None:\n            property_filters = \",\".join(property_filters)\n            query_parameters['propertyFilters'] = self._serialize.query('property_filters', property_filters, 'str')\n        response = self._send(http_method='GET',\n                              location_id='2f0aa599-c121-4256-a5fd-ba370e0ae7b6',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[DeploymentMachine]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a task group.", "response": "def delete_task_group(self, project, task_group_id, comment=None):\n        \"\"\"DeleteTaskGroup.\n        [Preview API] Delete a task group.\n        :param str project: Project ID or project name\n        :param str task_group_id: Id of the task group to be deleted.\n        :param str comment: Comments to delete.\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if task_group_id is not None:\n            route_values['taskGroupId'] = self._serialize.url('task_group_id', task_group_id, 'str')\n        query_parameters = {}\n        if comment is not None:\n            query_parameters['comment'] = self._serialize.query('comment', comment, 'str')\n        self._send(http_method='DELETE',\n                   location_id='6c08ffbf-dbf1-4f9a-94e5-a1cbd47005e7',\n                   version='5.1-preview.1',\n                   route_values=route_values,\n                   query_parameters=query_parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all task groups in a specific project.", "response": "def get_task_groups(self, project, task_group_id=None, expanded=None, task_id_filter=None, deleted=None, top=None, continuation_token=None, query_order=None):\n        \"\"\"GetTaskGroups.\n        [Preview API] List task groups.\n        :param str project: Project ID or project name\n        :param str task_group_id: Id of the task group.\n        :param bool expanded: 'true' to recursively expand task groups. Default is 'false'.\n        :param str task_id_filter: Guid of the taskId to filter.\n        :param bool deleted: 'true'to include deleted task groups. Default is 'false'.\n        :param int top: Number of task groups to get.\n        :param datetime continuation_token: Gets the task groups after the continuation token provided.\n        :param str query_order: Gets the results in the defined order. Default is 'CreatedOnDescending'.\n        :rtype: [TaskGroup]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if task_group_id is not None:\n            route_values['taskGroupId'] = self._serialize.url('task_group_id', task_group_id, 'str')\n        query_parameters = {}\n        if expanded is not None:\n            query_parameters['expanded'] = self._serialize.query('expanded', expanded, 'bool')\n        if task_id_filter is not None:\n            query_parameters['taskIdFilter'] = self._serialize.query('task_id_filter', task_id_filter, 'str')\n        if deleted is not None:\n            query_parameters['deleted'] = self._serialize.query('deleted', deleted, 'bool')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'iso-8601')\n        if query_order is not None:\n            query_parameters['queryOrder'] = self._serialize.query('query_order', query_order, 'str')\n        response = self._send(http_method='GET',\n                              location_id='6c08ffbf-dbf1-4f9a-94e5-a1cbd47005e7',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TaskGroup]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a variable group.", "response": "def delete_variable_group(self, project, group_id):\n        \"\"\"DeleteVariableGroup.\n        [Preview API] Delete a variable group\n        :param str project: Project ID or project name\n        :param int group_id: Id of the variable group.\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='f5b09dd5-9d54-45a1-8b5a-1c8287d634cc',\n                   version='5.1-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_variable_group(self, project, group_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='f5b09dd5-9d54-45a1-8b5a-1c8287d634cc',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('VariableGroup', response)", "response": "Get a variable group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_variable_group(self, group, project, group_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'int')\n        content = self._serialize.body(group, 'VariableGroupParameters')\n        response = self._send(http_method='PUT',\n                              location_id='f5b09dd5-9d54-45a1-8b5a-1c8287d634cc',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('VariableGroup', response)", "response": "Update a variable group."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the YAML schema of the current resource.", "response": "def get_yaml_schema(self):\n        \"\"\"GetYamlSchema.\n        [Preview API]\n        :rtype: object\n        \"\"\"\n        response = self._send(http_method='GET',\n                              location_id='1f9990b9-1dba-441f-9c2e-6485888c42b6',\n                              version='5.1-preview.1')\n        return self._deserialize('object', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_branch(self, path, project=None, include_parent=None, include_children=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        if include_parent is not None:\n            query_parameters['includeParent'] = self._serialize.query('include_parent', include_parent, 'bool')\n        if include_children is not None:\n            query_parameters['includeChildren'] = self._serialize.query('include_children', include_children, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='bc1f417e-239d-42e7-85e1-76e80cb2d6eb',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('TfvcBranch', response)", "response": "Get a single branch at the given path with parents or children as specified."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a collection of branch roots.", "response": "def get_branches(self, project=None, include_parent=None, include_children=None, include_deleted=None, include_links=None):\n        \"\"\"GetBranches.\n        Get a collection of branch roots -- first-level children, branches with no parents.\n        :param str project: Project ID or project name\n        :param bool include_parent: Return the parent branch, if there is one. Default: False\n        :param bool include_children: Return the child branches for each root branch. Default: False\n        :param bool include_deleted: Return deleted branches. Default: False\n        :param bool include_links: Return links. Default: False\n        :rtype: [TfvcBranch]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if include_parent is not None:\n            query_parameters['includeParent'] = self._serialize.query('include_parent', include_parent, 'bool')\n        if include_children is not None:\n            query_parameters['includeChildren'] = self._serialize.query('include_children', include_children, 'bool')\n        if include_deleted is not None:\n            query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')\n        if include_links is not None:\n            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='bc1f417e-239d-42e7-85e1-76e80cb2d6eb',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TfvcBranch]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_branch_refs(self, scope_path, project=None, include_deleted=None, include_links=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if scope_path is not None:\n            query_parameters['scopePath'] = self._serialize.query('scope_path', scope_path, 'str')\n        if include_deleted is not None:\n            query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')\n        if include_links is not None:\n            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='bc1f417e-239d-42e7-85e1-76e80cb2d6eb',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TfvcBranchRef]', self._unwrap_collection(response))", "response": "Get branch hierarchies below the specified scope_path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new changeset.", "response": "def create_changeset(self, changeset, project=None):\n        \"\"\"CreateChangeset.\n        Create a new changeset.\n        :param :class:`<TfvcChangeset> <azure.devops.v5_0.tfvc.models.TfvcChangeset>` changeset:\n        :param str project: Project ID or project name\n        :rtype: :class:`<TfvcChangesetRef> <azure.devops.v5_0.tfvc.models.TfvcChangesetRef>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(changeset, 'TfvcChangeset')\n        response = self._send(http_method='POST',\n                              location_id='0bc8f0a4-6bfb-42a9-ba84-139da7b99c49',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('TfvcChangesetRef', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_changeset(self, id, project=None, max_change_count=None, include_details=None, include_work_items=None, max_comment_length=None, include_source_rename=None, skip=None, top=None, orderby=None, search_criteria=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        query_parameters = {}\n        if max_change_count is not None:\n            query_parameters['maxChangeCount'] = self._serialize.query('max_change_count', max_change_count, 'int')\n        if include_details is not None:\n            query_parameters['includeDetails'] = self._serialize.query('include_details', include_details, 'bool')\n        if include_work_items is not None:\n            query_parameters['includeWorkItems'] = self._serialize.query('include_work_items', include_work_items, 'bool')\n        if max_comment_length is not None:\n            query_parameters['maxCommentLength'] = self._serialize.query('max_comment_length', max_comment_length, 'int')\n        if include_source_rename is not None:\n            query_parameters['includeSourceRename'] = self._serialize.query('include_source_rename', include_source_rename, 'bool')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if orderby is not None:\n            query_parameters['$orderby'] = self._serialize.query('orderby', orderby, 'str')\n        if search_criteria is not None:\n            if search_criteria.item_path is not None:\n                query_parameters['searchCriteria.itemPath'] = search_criteria.item_path\n            if search_criteria.author is not None:\n                query_parameters['searchCriteria.author'] = search_criteria.author\n            if search_criteria.from_date is not None:\n                query_parameters['searchCriteria.fromDate'] = search_criteria.from_date\n            if search_criteria.to_date is not None:\n                query_parameters['searchCriteria.toDate'] = search_criteria.to_date\n            if search_criteria.from_id is not None:\n                query_parameters['searchCriteria.fromId'] = search_criteria.from_id\n            if search_criteria.to_id is not None:\n                query_parameters['searchCriteria.toId'] = search_criteria.to_id\n            if search_criteria.follow_renames is not None:\n                query_parameters['searchCriteria.followRenames'] = search_criteria.follow_renames\n            if search_criteria.include_links is not None:\n                query_parameters['searchCriteria.includeLinks'] = search_criteria.include_links\n            if search_criteria.mappings is not None:\n                query_parameters['searchCriteria.mappings'] = search_criteria.mappings\n        response = self._send(http_method='GET',\n                              location_id='0bc8f0a4-6bfb-42a9-ba84-139da7b99c49',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('TfvcChangeset', response)", "response": "Get a Tfvc Changeset by ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_changesets(self, project=None, max_comment_length=None, skip=None, top=None, orderby=None, search_criteria=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if max_comment_length is not None:\n            query_parameters['maxCommentLength'] = self._serialize.query('max_comment_length', max_comment_length, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if orderby is not None:\n            query_parameters['$orderby'] = self._serialize.query('orderby', orderby, 'str')\n        if search_criteria is not None:\n            if search_criteria.item_path is not None:\n                query_parameters['searchCriteria.itemPath'] = search_criteria.item_path\n            if search_criteria.author is not None:\n                query_parameters['searchCriteria.author'] = search_criteria.author\n            if search_criteria.from_date is not None:\n                query_parameters['searchCriteria.fromDate'] = search_criteria.from_date\n            if search_criteria.to_date is not None:\n                query_parameters['searchCriteria.toDate'] = search_criteria.to_date\n            if search_criteria.from_id is not None:\n                query_parameters['searchCriteria.fromId'] = search_criteria.from_id\n            if search_criteria.to_id is not None:\n                query_parameters['searchCriteria.toId'] = search_criteria.to_id\n            if search_criteria.follow_renames is not None:\n                query_parameters['searchCriteria.followRenames'] = search_criteria.follow_renames\n            if search_criteria.include_links is not None:\n                query_parameters['searchCriteria.includeLinks'] = search_criteria.include_links\n            if search_criteria.mappings is not None:\n                query_parameters['searchCriteria.mappings'] = search_criteria.mappings\n        response = self._send(http_method='GET',\n                              location_id='0bc8f0a4-6bfb-42a9-ba84-139da7b99c49',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TfvcChangesetRef]', self._unwrap_collection(response))", "response": "GetChangesets.\n        Retrieve Tfvc Changesets\n        :param str project: Project ID or project name\n        :param int max_comment_length: Include details about associated work items in the response. Default: null\n        :param int skip: Number of results to skip. Default: null\n        :param int top: The maximum number of results to return. Default: null\n        :param str orderby: Results are sorted by ID in descending order by default. Use id asc to sort by ID in ascending order.\n        :param :class:`<TfvcChangesetSearchCriteria> <azure.devops.v5_0.tfvc.models.TfvcChangesetSearchCriteria>` search_criteria: Following criteria available (.itemPath, .version, .versionType, .versionOption, .author, .fromId, .toId, .fromDate, .toDate) Default: null\n        :rtype: [TfvcChangesetRef]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_batched_changesets(self, changesets_request_data):\n        content = self._serialize.body(changesets_request_data, 'TfvcChangesetsRequestData')\n        response = self._send(http_method='POST',\n                              location_id='b7e7c173-803c-4fea-9ec8-31ee35c5502a',\n                              version='5.0',\n                              content=content)\n        return self._deserialize('[TfvcChangesetRef]', self._unwrap_collection(response))", "response": "Get a list of batched changesets for a given list of changeset Ids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the work items associated with a particular changeset.", "response": "def get_changeset_work_items(self, id=None):\n        \"\"\"GetChangesetWorkItems.\n        Retrieves the work items associated with a particular changeset.\n        :param int id: ID of the changeset. Default: null\n        :rtype: [AssociatedWorkItem]\n        \"\"\"\n        route_values = {}\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='64ae0bea-1d71-47c9-a9e5-fe73f5ea0ff4',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('[AssociatedWorkItem]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget items batch request.", "response": "def get_items_batch(self, item_request_data, project=None):\n        \"\"\"GetItemsBatch.\n        Post for retrieving a set of items given a list of paths or a long path. Allows for specifying the recursionLevel and version descriptors for each path.\n        :param :class:`<TfvcItemRequestData> <azure.devops.v5_0.tfvc.models.TfvcItemRequestData>` item_request_data:\n        :param str project: Project ID or project name\n        :rtype: [[TfvcItem]]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(item_request_data, 'TfvcItemRequestData')\n        response = self._send(http_method='POST',\n                              location_id='fe6f827b-5f64-480f-b8af-1eca3b80e833',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[[TfvcItem]]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_items_batch_zip(self, item_request_data, project=None, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(item_request_data, 'TfvcItemRequestData')\n        response = self._send(http_method='POST',\n                              location_id='fe6f827b-5f64-480f-b8af-1eca3b80e833',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content,\n                              accept_media_type='application/zip')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get a set of items given a list of paths or a long path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_item(self, path, project=None, file_name=None, download=None, scope_path=None, recursion_level=None, version_descriptor=None, include_content=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        if file_name is not None:\n            query_parameters['fileName'] = self._serialize.query('file_name', file_name, 'str')\n        if download is not None:\n            query_parameters['download'] = self._serialize.query('download', download, 'bool')\n        if scope_path is not None:\n            query_parameters['scopePath'] = self._serialize.query('scope_path', scope_path, 'str')\n        if recursion_level is not None:\n            query_parameters['recursionLevel'] = self._serialize.query('recursion_level', recursion_level, 'str')\n        if version_descriptor is not None:\n            if version_descriptor.version_option is not None:\n                query_parameters['versionDescriptor.versionOption'] = version_descriptor.version_option\n            if version_descriptor.version_type is not None:\n                query_parameters['versionDescriptor.versionType'] = version_descriptor.version_type\n            if version_descriptor.version is not None:\n                query_parameters['versionDescriptor.version'] = version_descriptor.version\n        if include_content is not None:\n            query_parameters['includeContent'] = self._serialize.query('include_content', include_content, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='ba9fc436-9a38-4578-89d6-e4f3241f5040',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('TfvcItem', response)", "response": "Get Item Metadata and Content for a single item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_item_content(self, path, project=None, file_name=None, download=None, scope_path=None, recursion_level=None, version_descriptor=None, include_content=None, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        if file_name is not None:\n            query_parameters['fileName'] = self._serialize.query('file_name', file_name, 'str')\n        if download is not None:\n            query_parameters['download'] = self._serialize.query('download', download, 'bool')\n        if scope_path is not None:\n            query_parameters['scopePath'] = self._serialize.query('scope_path', scope_path, 'str')\n        if recursion_level is not None:\n            query_parameters['recursionLevel'] = self._serialize.query('recursion_level', recursion_level, 'str')\n        if version_descriptor is not None:\n            if version_descriptor.version_option is not None:\n                query_parameters['versionDescriptor.versionOption'] = version_descriptor.version_option\n            if version_descriptor.version_type is not None:\n                query_parameters['versionDescriptor.versionType'] = version_descriptor.version_type\n            if version_descriptor.version is not None:\n                query_parameters['versionDescriptor.version'] = version_descriptor.version\n        if include_content is not None:\n            query_parameters['includeContent'] = self._serialize.query('include_content', include_content, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='ba9fc436-9a38-4578-89d6-e4f3241f5040',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get Item Metadata and Content for a single item."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of items in a project.", "response": "def get_items(self, project=None, scope_path=None, recursion_level=None, include_links=None, version_descriptor=None):\n        \"\"\"GetItems.\n        Get a list of Tfvc items\n        :param str project: Project ID or project name\n        :param str scope_path: Version control path of a folder to return multiple items.\n        :param str recursion_level: None (just the item), or OneLevel (contents of a folder).\n        :param bool include_links: True to include links.\n        :param :class:`<TfvcVersionDescriptor> <azure.devops.v5_0.tfvc.models.TfvcVersionDescriptor>` version_descriptor:\n        :rtype: [TfvcItem]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if scope_path is not None:\n            query_parameters['scopePath'] = self._serialize.query('scope_path', scope_path, 'str')\n        if recursion_level is not None:\n            query_parameters['recursionLevel'] = self._serialize.query('recursion_level', recursion_level, 'str')\n        if include_links is not None:\n            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')\n        if version_descriptor is not None:\n            if version_descriptor.version_option is not None:\n                query_parameters['versionDescriptor.versionOption'] = version_descriptor.version_option\n            if version_descriptor.version_type is not None:\n                query_parameters['versionDescriptor.versionType'] = version_descriptor.version_type\n            if version_descriptor.version is not None:\n                query_parameters['versionDescriptor.version'] = version_descriptor.version\n        response = self._send(http_method='GET',\n                              location_id='ba9fc436-9a38-4578-89d6-e4f3241f5040',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TfvcItem]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting items under a label.", "response": "def get_label_items(self, label_id, top=None, skip=None):\n        \"\"\"GetLabelItems.\n        Get items under a label.\n        :param str label_id: Unique identifier of label\n        :param int top: Max number of items to return\n        :param int skip: Number of items to skip\n        :rtype: [TfvcItem]\n        \"\"\"\n        route_values = {}\n        if label_id is not None:\n            route_values['labelId'] = self._serialize.url('label_id', label_id, 'str')\n        query_parameters = {}\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        response = self._send(http_method='GET',\n                              location_id='06166e34-de17-4b60-8cd1-23182a346fda',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TfvcItem]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_label(self, label_id, request_data, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if label_id is not None:\n            route_values['labelId'] = self._serialize.url('label_id', label_id, 'str')\n        query_parameters = {}\n        if request_data is not None:\n            if request_data.label_scope is not None:\n                query_parameters['requestData.labelScope'] = request_data.label_scope\n            if request_data.name is not None:\n                query_parameters['requestData.name'] = request_data.name\n            if request_data.owner is not None:\n                query_parameters['requestData.owner'] = request_data.owner\n            if request_data.item_label_filter is not None:\n                query_parameters['requestData.itemLabelFilter'] = request_data.item_label_filter\n            if request_data.max_item_count is not None:\n                query_parameters['requestData.maxItemCount'] = request_data.max_item_count\n            if request_data.include_links is not None:\n                query_parameters['requestData.includeLinks'] = request_data.include_links\n        response = self._send(http_method='GET',\n                              location_id='a5d9bd7f-b661-4d0e-b9be-d9c16affae54',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('TfvcLabel', response)", "response": "Get a single deep label."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_labels(self, request_data, project=None, top=None, skip=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if request_data is not None:\n            if request_data.label_scope is not None:\n                query_parameters['requestData.labelScope'] = request_data.label_scope\n            if request_data.name is not None:\n                query_parameters['requestData.name'] = request_data.name\n            if request_data.owner is not None:\n                query_parameters['requestData.owner'] = request_data.owner\n            if request_data.item_label_filter is not None:\n                query_parameters['requestData.itemLabelFilter'] = request_data.item_label_filter\n            if request_data.max_item_count is not None:\n                query_parameters['requestData.maxItemCount'] = request_data.max_item_count\n            if request_data.include_links is not None:\n                query_parameters['requestData.includeLinks'] = request_data.include_links\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        response = self._send(http_method='GET',\n                              location_id='a5d9bd7f-b661-4d0e-b9be-d9c16affae54',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TfvcLabelRef]', self._unwrap_collection(response))", "response": "Get a collection of shallow label references."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_shelveset(self, shelveset_id, request_data=None):\n        query_parameters = {}\n        if shelveset_id is not None:\n            query_parameters['shelvesetId'] = self._serialize.query('shelveset_id', shelveset_id, 'str')\n        if request_data is not None:\n            if request_data.name is not None:\n                query_parameters['requestData.name'] = request_data.name\n            if request_data.owner is not None:\n                query_parameters['requestData.owner'] = request_data.owner\n            if request_data.max_comment_length is not None:\n                query_parameters['requestData.maxCommentLength'] = request_data.max_comment_length\n            if request_data.max_change_count is not None:\n                query_parameters['requestData.maxChangeCount'] = request_data.max_change_count\n            if request_data.include_details is not None:\n                query_parameters['requestData.includeDetails'] = request_data.include_details\n            if request_data.include_work_items is not None:\n                query_parameters['requestData.includeWorkItems'] = request_data.include_work_items\n            if request_data.include_links is not None:\n                query_parameters['requestData.includeLinks'] = request_data.include_links\n        response = self._send(http_method='GET',\n                              location_id='e36d44fb-e907-4b0a-b194-f83f1ed32ad3',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('TfvcShelveset', response)", "response": "Get a single deep shelveset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a collection of shallow shelvesets.", "response": "def get_shelvesets(self, request_data=None, top=None, skip=None):\n        \"\"\"GetShelvesets.\n        Return a collection of shallow shelveset references.\n        :param :class:`<TfvcShelvesetRequestData> <azure.devops.v5_0.tfvc.models.TfvcShelvesetRequestData>` request_data: name, owner, and maxCommentLength\n        :param int top: Max number of shelvesets to return\n        :param int skip: Number of shelvesets to skip\n        :rtype: [TfvcShelvesetRef]\n        \"\"\"\n        query_parameters = {}\n        if request_data is not None:\n            if request_data.name is not None:\n                query_parameters['requestData.name'] = request_data.name\n            if request_data.owner is not None:\n                query_parameters['requestData.owner'] = request_data.owner\n            if request_data.max_comment_length is not None:\n                query_parameters['requestData.maxCommentLength'] = request_data.max_comment_length\n            if request_data.max_change_count is not None:\n                query_parameters['requestData.maxChangeCount'] = request_data.max_change_count\n            if request_data.include_details is not None:\n                query_parameters['requestData.includeDetails'] = request_data.include_details\n            if request_data.include_work_items is not None:\n                query_parameters['requestData.includeWorkItems'] = request_data.include_work_items\n            if request_data.include_links is not None:\n                query_parameters['requestData.includeLinks'] = request_data.include_links\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        response = self._send(http_method='GET',\n                              location_id='e36d44fb-e907-4b0a-b194-f83f1ed32ad3',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[TfvcShelvesetRef]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_shelveset_work_items(self, shelveset_id):\n        query_parameters = {}\n        if shelveset_id is not None:\n            query_parameters['shelvesetId'] = self._serialize.query('shelveset_id', shelveset_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='a7a0c1c1-373e-425a-b031-a519474d743d',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[AssociatedWorkItem]', self._unwrap_collection(response))", "response": "Get all work items associated with a shelveset."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads a package from the server.", "response": "def download_package(self, feed_id, group_id, artifact_id, version, file_name):\n        \"\"\"DownloadPackage.\n        [Preview API] Fulfills maven package file download requests by returning the url of the package file requested.\n        :param str feed_id: Name or ID of the feed.\n        :param str group_id: GroupId of the maven package\n        :param str artifact_id: ArtifactId of the maven package\n        :param str version: Version of the package\n        :param str file_name: File name to download\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        if artifact_id is not None:\n            route_values['artifactId'] = self._serialize.url('artifact_id', artifact_id, 'str')\n        if version is not None:\n            route_values['version'] = self._serialize.url('version', version, 'str')\n        if file_name is not None:\n            route_values['fileName'] = self._serialize.url('file_name', file_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='c338d4b5-d30a-47e2-95b7-f157ef558833',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('object', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_package_version_from_recycle_bin(self, feed, group_id, artifact_id, version):\n        route_values = {}\n        if feed is not None:\n            route_values['feed'] = self._serialize.url('feed', feed, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        if artifact_id is not None:\n            route_values['artifactId'] = self._serialize.url('artifact_id', artifact_id, 'str')\n        if version is not None:\n            route_values['version'] = self._serialize.url('version', version, 'str')\n        self._send(http_method='DELETE',\n                   location_id='f67e10eb-1254-4953-add7-d49b83a16c9f',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "Delete a package from a recycle bin."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets information about a package version in the recycle bin.", "response": "def get_package_version_metadata_from_recycle_bin(self, feed, group_id, artifact_id, version):\n        \"\"\"GetPackageVersionMetadataFromRecycleBin.\n        [Preview API] Get information about a package version in the recycle bin.\n        :param str feed: Name or ID of the feed.\n        :param str group_id: Group ID of the package.\n        :param str artifact_id: Artifact ID of the package.\n        :param str version: Version of the package.\n        :rtype: :class:`<MavenPackageVersionDeletionState> <azure.devops.v5_1.maven.models.MavenPackageVersionDeletionState>`\n        \"\"\"\n        route_values = {}\n        if feed is not None:\n            route_values['feed'] = self._serialize.url('feed', feed, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        if artifact_id is not None:\n            route_values['artifactId'] = self._serialize.url('artifact_id', artifact_id, 'str')\n        if version is not None:\n            route_values['version'] = self._serialize.url('version', version, 'str')\n        response = self._send(http_method='GET',\n                              location_id='f67e10eb-1254-4953-add7-d49b83a16c9f',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('MavenPackageVersionDeletionState', response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restore_package_version_from_recycle_bin(self, package_version_details, feed, group_id, artifact_id, version):\n        route_values = {}\n        if feed is not None:\n            route_values['feed'] = self._serialize.url('feed', feed, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        if artifact_id is not None:\n            route_values['artifactId'] = self._serialize.url('artifact_id', artifact_id, 'str')\n        if version is not None:\n            route_values['version'] = self._serialize.url('version', version, 'str')\n        content = self._serialize.body(package_version_details, 'MavenRecycleBinPackageVersionDetails')\n        self._send(http_method='PATCH',\n                   location_id='f67e10eb-1254-4953-add7-d49b83a16c9f',\n                   version='5.1-preview.1',\n                   route_values=route_values,\n                   content=content)", "response": "Restores a package version from the recycle bin to its associated feed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_package_version(self, feed, group_id, artifact_id, version, show_deleted=None):\n        route_values = {}\n        if feed is not None:\n            route_values['feed'] = self._serialize.url('feed', feed, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        if artifact_id is not None:\n            route_values['artifactId'] = self._serialize.url('artifact_id', artifact_id, 'str')\n        if version is not None:\n            route_values['version'] = self._serialize.url('version', version, 'str')\n        query_parameters = {}\n        if show_deleted is not None:\n            query_parameters['showDeleted'] = self._serialize.query('show_deleted', show_deleted, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='180ed967-377a-4112-986b-607adb14ded4',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Package', response)", "response": "Get information about a package version."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_consumer_action(self, consumer_id, consumer_action_id, publisher_id=None):\n        route_values = {}\n        if consumer_id is not None:\n            route_values['consumerId'] = self._serialize.url('consumer_id', consumer_id, 'str')\n        if consumer_action_id is not None:\n            route_values['consumerActionId'] = self._serialize.url('consumer_action_id', consumer_action_id, 'str')\n        query_parameters = {}\n        if publisher_id is not None:\n            query_parameters['publisherId'] = self._serialize.query('publisher_id', publisher_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='c3428e90-7a69-4194-8ed8-0f153185ee0d',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ConsumerAction', response)", "response": "Get details about a specific consumer action."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting available service hook consumers. Optionally filter by available event types from the specific publisher.", "response": "def list_consumers(self, publisher_id=None):\n        \"\"\"ListConsumers.\n        Get a list of available service hook consumer services. Optionally filter by consumers that support at least one event type from the specific publisher.\n        :param str publisher_id:\n        :rtype: [Consumer]\n        \"\"\"\n        query_parameters = {}\n        if publisher_id is not None:\n            query_parameters['publisherId'] = self._serialize.query('publisher_id', publisher_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='4301c514-5f34-4f5d-a145-f0ea7b5b7d19',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[Consumer]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_event_type(self, publisher_id, event_type_id):\n        route_values = {}\n        if publisher_id is not None:\n            route_values['publisherId'] = self._serialize.url('publisher_id', publisher_id, 'str')\n        if event_type_id is not None:\n            route_values['eventTypeId'] = self._serialize.url('event_type_id', event_type_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='db4777cd-8e08-4a84-8ba3-c974ea033718',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('EventTypeDescriptor', response)", "response": "Get a specific event type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_event_types(self, publisher_id):\n        route_values = {}\n        if publisher_id is not None:\n            route_values['publisherId'] = self._serialize.url('publisher_id', publisher_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='db4777cd-8e08-4a84-8ba3-c974ea033718',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('[EventTypeDescriptor]', self._unwrap_collection(response))", "response": "List the event types for a specific publisher."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a specific notification for a specific subscription.", "response": "def get_notification(self, subscription_id, notification_id):\n        \"\"\"GetNotification.\n        Get a specific notification for a subscription.\n        :param str subscription_id: ID for a subscription.\n        :param int notification_id:\n        :rtype: :class:`<Notification> <azure.devops.v5_0.service_hooks.models.Notification>`\n        \"\"\"\n        route_values = {}\n        if subscription_id is not None:\n            route_values['subscriptionId'] = self._serialize.url('subscription_id', subscription_id, 'str')\n        if notification_id is not None:\n            route_values['notificationId'] = self._serialize.url('notification_id', notification_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='0c62d343-21b0-4732-997b-017fde84dc28',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('Notification', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a list of all notifications for a specific subscription.", "response": "def get_notifications(self, subscription_id, max_results=None, status=None, result=None):\n        \"\"\"GetNotifications.\n        Get a list of notifications for a specific subscription. A notification includes details about the event, the request to and the response from the consumer service.\n        :param str subscription_id: ID for a subscription.\n        :param int max_results: Maximum number of notifications to return. Default is **100**.\n        :param str status: Get only notifications with this status.\n        :param str result: Get only notifications with this result type.\n        :rtype: [Notification]\n        \"\"\"\n        route_values = {}\n        if subscription_id is not None:\n            route_values['subscriptionId'] = self._serialize.url('subscription_id', subscription_id, 'str')\n        query_parameters = {}\n        if max_results is not None:\n            query_parameters['maxResults'] = self._serialize.query('max_results', max_results, 'int')\n        if status is not None:\n            query_parameters['status'] = self._serialize.query('status', status, 'str')\n        if result is not None:\n            query_parameters['result'] = self._serialize.query('result', result, 'str')\n        response = self._send(http_method='GET',\n                              location_id='0c62d343-21b0-4732-997b-017fde84dc28',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[Notification]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_input_values(self, input_values_query, publisher_id):\n        route_values = {}\n        if publisher_id is not None:\n            route_values['publisherId'] = self._serialize.url('publisher_id', publisher_id, 'str')\n        content = self._serialize.body(input_values_query, 'InputValuesQuery')\n        response = self._send(http_method='POST',\n                              location_id='d815d352-a566-4dc1-a3e3-fd245acf688c',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('InputValuesQuery', response)", "response": "Query the input values for the specified publisher."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_publisher(self, publisher_id):\n        route_values = {}\n        if publisher_id is not None:\n            route_values['publisherId'] = self._serialize.url('publisher_id', publisher_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='1e83a210-5b53-43bc-90f0-d476a4e5d731',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('Publisher', response)", "response": "Get a specific service hooks publisher."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a specific service hooks subscription.", "response": "def delete_subscription(self, subscription_id):\n        \"\"\"DeleteSubscription.\n        Delete a specific service hooks subscription.\n        :param str subscription_id: ID for a subscription.\n        \"\"\"\n        route_values = {}\n        if subscription_id is not None:\n            route_values['subscriptionId'] = self._serialize.url('subscription_id', subscription_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='fc50d02a-849f-41fb-8af1-0a5216103269',\n                   version='5.0',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a specific service hooks subscription.", "response": "def get_subscription(self, subscription_id):\n        \"\"\"GetSubscription.\n        Get a specific service hooks subscription.\n        :param str subscription_id: ID for a subscription.\n        :rtype: :class:`<Subscription> <azure.devops.v5_0.service_hooks.models.Subscription>`\n        \"\"\"\n        route_values = {}\n        if subscription_id is not None:\n            route_values['subscriptionId'] = self._serialize.url('subscription_id', subscription_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='fc50d02a-849f-41fb-8af1-0a5216103269',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('Subscription', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist the subscriptions for a given topic.", "response": "def list_subscriptions(self, publisher_id=None, event_type=None, consumer_id=None, consumer_action_id=None):\n        \"\"\"ListSubscriptions.\n        Get a list of subscriptions.\n        :param str publisher_id: ID for a subscription.\n        :param str event_type: Maximum number of notifications to return. Default is 100.\n        :param str consumer_id: ID for a consumer.\n        :param str consumer_action_id: ID for a consumerActionId.\n        :rtype: [Subscription]\n        \"\"\"\n        query_parameters = {}\n        if publisher_id is not None:\n            query_parameters['publisherId'] = self._serialize.query('publisher_id', publisher_id, 'str')\n        if event_type is not None:\n            query_parameters['eventType'] = self._serialize.query('event_type', event_type, 'str')\n        if consumer_id is not None:\n            query_parameters['consumerId'] = self._serialize.query('consumer_id', consumer_id, 'str')\n        if consumer_action_id is not None:\n            query_parameters['consumerActionId'] = self._serialize.query('consumer_action_id', consumer_action_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='fc50d02a-849f-41fb-8af1-0a5216103269',\n                              version='5.0',\n                              query_parameters=query_parameters)\n        return self._deserialize('[Subscription]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_project_language_analytics(self, project):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        response = self._send(http_method='GET',\n                              location_id='5b02a779-1867-433f-90b7-d23ed5e33e57',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('ProjectLanguageAnalytics', response)", "response": "Get Project Language Analytics."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets Project Activity Metrics.", "response": "def get_project_activity_metrics(self, project, from_date, aggregation_type):\n        \"\"\"GetProjectActivityMetrics.\n        [Preview API]\n        :param str project: Project ID or project name\n        :param datetime from_date:\n        :param str aggregation_type:\n        :rtype: :class:`<ProjectActivityMetrics> <azure.devops.v5_0.project_analysis.models.ProjectActivityMetrics>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if from_date is not None:\n            query_parameters['fromDate'] = self._serialize.query('from_date', from_date, 'iso-8601')\n        if aggregation_type is not None:\n            query_parameters['aggregationType'] = self._serialize.query('aggregation_type', aggregation_type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e40ae584-9ea6-4f06-a7c7-6284651b466b',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ProjectActivityMetrics', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets git repositories activity metrics for a given project from a given date and aggregation type.", "response": "def get_git_repositories_activity_metrics(self, project, from_date, aggregation_type, skip, top):\n        \"\"\"GetGitRepositoriesActivityMetrics.\n        [Preview API] Retrieves git activity metrics for repositories matching a specified criteria.\n        :param str project: Project ID or project name\n        :param datetime from_date: Date from which, the trends are to be fetched.\n        :param str aggregation_type: Bucket size on which, trends are to be aggregated.\n        :param int skip: The number of repositories to ignore.\n        :param int top: The number of repositories for which activity metrics are to be retrieved.\n        :rtype: [RepositoryActivityMetrics]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if from_date is not None:\n            query_parameters['fromDate'] = self._serialize.query('from_date', from_date, 'iso-8601')\n        if aggregation_type is not None:\n            query_parameters['aggregationType'] = self._serialize.query('aggregation_type', aggregation_type, 'str')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        response = self._send(http_method='GET',\n                              location_id='df7fbbca-630a-40e3-8aa3-7a3faf66947e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[RepositoryActivityMetrics]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the specified ACEs from the specified token.", "response": "def remove_access_control_entries(self, security_namespace_id, token=None, descriptors=None):\n        \"\"\"RemoveAccessControlEntries.\n        Remove the specified ACEs from the ACL belonging to the specified token.\n        :param str security_namespace_id: Security namespace identifier.\n        :param str token: The token whose ACL should be modified.\n        :param str descriptors: String containing a list of identity descriptors separated by ',' whose entries should be removed.\n        :rtype: bool\n        \"\"\"\n        route_values = {}\n        if security_namespace_id is not None:\n            route_values['securityNamespaceId'] = self._serialize.url('security_namespace_id', security_namespace_id, 'str')\n        query_parameters = {}\n        if token is not None:\n            query_parameters['token'] = self._serialize.query('token', token, 'str')\n        if descriptors is not None:\n            query_parameters['descriptors'] = self._serialize.query('descriptors', descriptors, 'str')\n        response = self._send(http_method='DELETE',\n                              location_id='ac08c8ff-4323-4b08-af90-bcd018d380ce',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('bool', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_access_control_entries(self, container, security_namespace_id):\n        route_values = {}\n        if security_namespace_id is not None:\n            route_values['securityNamespaceId'] = self._serialize.url('security_namespace_id', security_namespace_id, 'str')\n        content = self._serialize.body(container, 'object')\n        response = self._send(http_method='POST',\n                              location_id='ac08c8ff-4323-4b08-af90-bcd018d380ce',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[AccessControlEntry]', self._unwrap_collection(response))", "response": "Set the access control entries for the provided token."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query_access_control_lists(self, security_namespace_id, token=None, descriptors=None, include_extended_info=None, recurse=None):\n        route_values = {}\n        if security_namespace_id is not None:\n            route_values['securityNamespaceId'] = self._serialize.url('security_namespace_id', security_namespace_id, 'str')\n        query_parameters = {}\n        if token is not None:\n            query_parameters['token'] = self._serialize.query('token', token, 'str')\n        if descriptors is not None:\n            query_parameters['descriptors'] = self._serialize.query('descriptors', descriptors, 'str')\n        if include_extended_info is not None:\n            query_parameters['includeExtendedInfo'] = self._serialize.query('include_extended_info', include_extended_info, 'bool')\n        if recurse is not None:\n            query_parameters['recurse'] = self._serialize.query('recurse', recurse, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='18a2ad18-7571-46ae-bec7-0c7da1495885',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[AccessControlList]', self._unwrap_collection(response))", "response": "Query the access control lists for the specified security namespace and token."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves access control lists under the specified security namespace.", "response": "def remove_access_control_lists(self, security_namespace_id, tokens=None, recurse=None):\n        \"\"\"RemoveAccessControlLists.\n        Remove access control lists under the specfied security namespace.\n        :param str security_namespace_id: Security namespace identifier.\n        :param str tokens: One or more comma-separated security tokens\n        :param bool recurse: If true and this is a hierarchical namespace, also remove child ACLs of the specified tokens.\n        :rtype: bool\n        \"\"\"\n        route_values = {}\n        if security_namespace_id is not None:\n            route_values['securityNamespaceId'] = self._serialize.url('security_namespace_id', security_namespace_id, 'str')\n        query_parameters = {}\n        if tokens is not None:\n            query_parameters['tokens'] = self._serialize.query('tokens', tokens, 'str')\n        if recurse is not None:\n            query_parameters['recurse'] = self._serialize.query('recurse', recurse, 'bool')\n        response = self._send(http_method='DELETE',\n                              location_id='18a2ad18-7571-46ae-bec7-0c7da1495885',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('bool', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_access_control_lists(self, access_control_lists, security_namespace_id):\n        route_values = {}\n        if security_namespace_id is not None:\n            route_values['securityNamespaceId'] = self._serialize.url('security_namespace_id', security_namespace_id, 'str')\n        content = self._serialize.body(access_control_lists, 'VssJsonCollectionWrapper')\n        self._send(http_method='POST',\n                   location_id='18a2ad18-7571-46ae-bec7-0c7da1495885',\n                   version='5.0',\n                   route_values=route_values,\n                   content=content)", "response": "SetAccessControlLists. A new list of access control lists can be created or updated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nevaluating multiple permissions for the calling user.", "response": "def has_permissions_batch(self, eval_batch):\n        \"\"\"HasPermissionsBatch.\n        Evaluates multiple permissions for the calling user.  Note: This method does not aggregate the results, nor does it short-circuit if one of the permissions evaluates to false.\n        :param :class:`<PermissionEvaluationBatch> <azure.devops.v5_0.security.models.PermissionEvaluationBatch>` eval_batch: The set of evaluation requests.\n        :rtype: :class:`<PermissionEvaluationBatch> <azure.devops.v5_0.security.models.PermissionEvaluationBatch>`\n        \"\"\"\n        content = self._serialize.body(eval_batch, 'PermissionEvaluationBatch')\n        response = self._send(http_method='POST',\n                              location_id='cf1faa59-1b63-4448-bf04-13d981a46f5d',\n                              version='5.0',\n                              content=content)\n        return self._deserialize('PermissionEvaluationBatch', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nevaluate whether the caller has the specified permissions on the specified set of security tokens.", "response": "def has_permissions(self, security_namespace_id, permissions=None, tokens=None, always_allow_administrators=None, delimiter=None):\n        \"\"\"HasPermissions.\n        Evaluates whether the caller has the specified permissions on the specified set of security tokens.\n        :param str security_namespace_id: Security namespace identifier.\n        :param int permissions: Permissions to evaluate.\n        :param str tokens: One or more security tokens to evaluate.\n        :param bool always_allow_administrators: If true and if the caller is an administrator, always return true.\n        :param str delimiter: Optional security token separator. Defaults to \",\".\n        :rtype: [bool]\n        \"\"\"\n        route_values = {}\n        if security_namespace_id is not None:\n            route_values['securityNamespaceId'] = self._serialize.url('security_namespace_id', security_namespace_id, 'str')\n        if permissions is not None:\n            route_values['permissions'] = self._serialize.url('permissions', permissions, 'int')\n        query_parameters = {}\n        if tokens is not None:\n            query_parameters['tokens'] = self._serialize.query('tokens', tokens, 'str')\n        if always_allow_administrators is not None:\n            query_parameters['alwaysAllowAdministrators'] = self._serialize.query('always_allow_administrators', always_allow_administrators, 'bool')\n        if delimiter is not None:\n            query_parameters['delimiter'] = self._serialize.query('delimiter', delimiter, 'str')\n        response = self._send(http_method='GET',\n                              location_id='dd3b8bd6-c7fc-4cbd-929a-933d9c011c9d',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[bool]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the specified permissions on a security token for a user or group.", "response": "def remove_permission(self, security_namespace_id, descriptor, permissions=None, token=None):\n        \"\"\"RemovePermission.\n        Removes the specified permissions on a security token for a user or group.\n        :param str security_namespace_id: Security namespace identifier.\n        :param str descriptor: Identity descriptor of the user to remove permissions for.\n        :param int permissions: Permissions to remove.\n        :param str token: Security token to remove permissions for.\n        :rtype: :class:`<AccessControlEntry> <azure.devops.v5_0.security.models.AccessControlEntry>`\n        \"\"\"\n        route_values = {}\n        if security_namespace_id is not None:\n            route_values['securityNamespaceId'] = self._serialize.url('security_namespace_id', security_namespace_id, 'str')\n        if permissions is not None:\n            route_values['permissions'] = self._serialize.url('permissions', permissions, 'int')\n        query_parameters = {}\n        if descriptor is not None:\n            query_parameters['descriptor'] = self._serialize.query('descriptor', descriptor, 'str')\n        if token is not None:\n            query_parameters['token'] = self._serialize.query('token', token, 'str')\n        response = self._send(http_method='DELETE',\n                              location_id='dd3b8bd6-c7fc-4cbd-929a-933d9c011c9d',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('AccessControlEntry', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query_security_namespaces(self, security_namespace_id=None, local_only=None):\n        route_values = {}\n        if security_namespace_id is not None:\n            route_values['securityNamespaceId'] = self._serialize.url('security_namespace_id', security_namespace_id, 'str')\n        query_parameters = {}\n        if local_only is not None:\n            query_parameters['localOnly'] = self._serialize.query('local_only', local_only, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='ce7b9f95-fde9-4be8-a86d-83b366f0b87a',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[SecurityNamespaceDescription]', self._unwrap_collection(response))", "response": "QuerySecurityNamespaces.\n        List all security namespaces or just the specified namespace.\n        :param str security_namespace_id: Security namespace identifier.\n        :param bool local_only: If true, retrieve only local security namespaces.\n        :rtype: [SecurityNamespaceDescription]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_process_behavior(self, behavior, process_id):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        content = self._serialize.body(behavior, 'ProcessBehaviorCreateRequest')\n        response = self._send(http_method='POST',\n                              location_id='d1800200-f184-4e75-a5f2-ad0b04b4373e',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ProcessBehavior', response)", "response": "Create a single behavior in a given process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a behavior in a process.", "response": "def delete_process_behavior(self, process_id, behavior_ref_name):\n        \"\"\"DeleteProcessBehavior.\n        [Preview API] Removes a behavior in the process.\n        :param str process_id: The ID of the process\n        :param str behavior_ref_name: The reference name of the behavior\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if behavior_ref_name is not None:\n            route_values['behaviorRefName'] = self._serialize.url('behavior_ref_name', behavior_ref_name, 'str')\n        self._send(http_method='DELETE',\n                   location_id='d1800200-f184-4e75-a5f2-ad0b04b4373e',\n                   version='5.0-preview.2',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_process_behavior(self, process_id, behavior_ref_name, expand=None):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if behavior_ref_name is not None:\n            route_values['behaviorRefName'] = self._serialize.url('behavior_ref_name', behavior_ref_name, 'str')\n        query_parameters = {}\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='d1800200-f184-4e75-a5f2-ad0b04b4373e',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ProcessBehavior', response)", "response": "Get a behavior of the process."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates a process behavior in the specified process.", "response": "def update_process_behavior(self, behavior_data, process_id, behavior_ref_name):\n        \"\"\"UpdateProcessBehavior.\n        [Preview API] Replaces a behavior in the process.\n        :param :class:`<ProcessBehaviorUpdateRequest> <azure.devops.v5_0.work_item_tracking_process.models.ProcessBehaviorUpdateRequest>` behavior_data:\n        :param str process_id: The ID of the process\n        :param str behavior_ref_name: The reference name of the behavior\n        :rtype: :class:`<ProcessBehavior> <azure.devops.v5_0.work_item_tracking_process.models.ProcessBehavior>`\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if behavior_ref_name is not None:\n            route_values['behaviorRefName'] = self._serialize.url('behavior_ref_name', behavior_ref_name, 'str')\n        content = self._serialize.body(behavior_data, 'ProcessBehaviorUpdateRequest')\n        response = self._send(http_method='PUT',\n                              location_id='d1800200-f184-4e75-a5f2-ad0b04b4373e',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ProcessBehavior', response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef move_control_to_group(self, control, process_id, wit_ref_name, group_id, control_id, remove_from_group_id=None):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        if control_id is not None:\n            route_values['controlId'] = self._serialize.url('control_id', control_id, 'str')\n        query_parameters = {}\n        if remove_from_group_id is not None:\n            query_parameters['removeFromGroupId'] = self._serialize.query('remove_from_group_id', remove_from_group_id, 'str')\n        content = self._serialize.body(control, 'Control')\n        response = self._send(http_method='PUT',\n                              location_id='1f59b363-a2d0-4b7e-9bc6-eb9f5f3f0e58',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('Control', response)", "response": "Moves a control to a group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_control_from_group(self, process_id, wit_ref_name, group_id, control_id):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        if control_id is not None:\n            route_values['controlId'] = self._serialize.url('control_id', control_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='1f59b363-a2d0-4b7e-9bc6-eb9f5f3f0e58',\n                   version='5.0-preview.1',\n                   route_values=route_values)", "response": "Remove a control from a group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_control(self, control, process_id, wit_ref_name, group_id, control_id):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        if control_id is not None:\n            route_values['controlId'] = self._serialize.url('control_id', control_id, 'str')\n        content = self._serialize.body(control, 'Control')\n        response = self._send(http_method='PATCH',\n                              location_id='1f59b363-a2d0-4b7e-9bc6-eb9f5f3f0e58',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Control', response)", "response": "Updates a control on the work item form."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_field_to_work_item_type(self, field, process_id, wit_ref_name):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        content = self._serialize.body(field, 'AddProcessWorkItemTypeFieldRequest')\n        response = self._send(http_method='POST',\n                              location_id='bc0ad8dc-e3f3-46b0-b06c-5bf861793196',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ProcessWorkItemTypeField', response)", "response": "Adds a field to a work item type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_work_item_type_field(self, process_id, wit_ref_name, field_ref_name):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if field_ref_name is not None:\n            route_values['fieldRefName'] = self._serialize.url('field_ref_name', field_ref_name, 'str')\n        self._send(http_method='DELETE',\n                   location_id='bc0ad8dc-e3f3-46b0-b06c-5bf861793196',\n                   version='5.0-preview.2',\n                   route_values=route_values)", "response": "Remove a field from a work item type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef move_group_to_page(self, group, process_id, wit_ref_name, page_id, section_id, group_id, remove_from_page_id, remove_from_section_id):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if page_id is not None:\n            route_values['pageId'] = self._serialize.url('page_id', page_id, 'str')\n        if section_id is not None:\n            route_values['sectionId'] = self._serialize.url('section_id', section_id, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        query_parameters = {}\n        if remove_from_page_id is not None:\n            query_parameters['removeFromPageId'] = self._serialize.query('remove_from_page_id', remove_from_page_id, 'str')\n        if remove_from_section_id is not None:\n            query_parameters['removeFromSectionId'] = self._serialize.query('remove_from_section_id', remove_from_section_id, 'str')\n        content = self._serialize.body(group, 'Group')\n        response = self._send(http_method='PUT',\n                              location_id='766e44e1-36a8-41d7-9050-c343ff02f7a5',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('Group', response)", "response": "Moves a group to a different page and section."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_group(self, group, process_id, wit_ref_name, page_id, section_id, group_id):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if page_id is not None:\n            route_values['pageId'] = self._serialize.url('page_id', page_id, 'str')\n        if section_id is not None:\n            route_values['sectionId'] = self._serialize.url('section_id', section_id, 'str')\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        content = self._serialize.body(group, 'Group')\n        response = self._send(http_method='PATCH',\n                              location_id='766e44e1-36a8-41d7-9050-c343ff02f7a5',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Group', response)", "response": "Updates a group in the preview API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_form_layout(self, process_id, wit_ref_name):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='fa8646eb-43cd-4b71-9564-40106fd63e40',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('FormLayout', response)", "response": "Get the form layout for a given work item type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_list(self, list_id):\n        route_values = {}\n        if list_id is not None:\n            route_values['listId'] = self._serialize.url('list_id', list_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='01e15468-e27c-4e20-a974-bd957dcccebc',\n                   version='5.0-preview.1',\n                   route_values=route_values)", "response": "DeleteList.\n        [Preview API] Removes a picklist.\n        :param str list_id: The ID of the list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_list(self, list_id):\n        route_values = {}\n        if list_id is not None:\n            route_values['listId'] = self._serialize.url('list_id', list_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='01e15468-e27c-4e20-a974-bd957dcccebc',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('PickList', response)", "response": "GetList.\n        [Preview API] Returns a picklist.\n        :param str list_id: The ID of the list\n        :rtype: :class:`<PickList> <azure.devops.v5_0.work_item_tracking_process.models.PickList>`"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_list(self, picklist, list_id):\n        route_values = {}\n        if list_id is not None:\n            route_values['listId'] = self._serialize.url('list_id', list_id, 'str')\n        content = self._serialize.body(picklist, 'PickList')\n        response = self._send(http_method='PUT',\n                              location_id='01e15468-e27c-4e20-a974-bd957dcccebc',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('PickList', response)", "response": "UpdateList.\n        [Preview API] Updates a list.\n        :param :class:`<PickList> <azure.devops.v5_0.work_item_tracking_process.models.PickList>` picklist:\n        :param str list_id: The ID of the list\n        :rtype: :class:`<PickList> <azure.devops.v5_0.work_item_tracking_process.models.PickList>`"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a page to the work item form.", "response": "def add_page(self, page, process_id, wit_ref_name):\n        \"\"\"AddPage.\n        [Preview API] Adds a page to the work item form.\n        :param :class:`<Page> <azure.devops.v5_0.work_item_tracking_process.models.Page>` page: The page.\n        :param str process_id: The ID of the process.\n        :param str wit_ref_name: The reference name of the work item type.\n        :rtype: :class:`<Page> <azure.devops.v5_0.work_item_tracking_process.models.Page>`\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        content = self._serialize.body(page, 'Page')\n        response = self._send(http_method='POST',\n                              location_id='1cc7b29f-6697-4d9d-b0a1-2650d3e1d584',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Page', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_page(self, process_id, wit_ref_name, page_id):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if page_id is not None:\n            route_values['pageId'] = self._serialize.url('page_id', page_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='1cc7b29f-6697-4d9d-b0a1-2650d3e1d584',\n                   version='5.0-preview.1',\n                   route_values=route_values)", "response": "Remove a page from the work item form\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_new_process(self, create_request):\n        content = self._serialize.body(create_request, 'CreateProcessModel')\n        response = self._send(http_method='POST',\n                              location_id='02cc6a73-5cfb-427d-8c8e-b49fb086e8af',\n                              version='5.0-preview.2',\n                              content=content)\n        return self._deserialize('ProcessInfo', response)", "response": "Create a new process."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_process_by_id(self, process_type_id):\n        route_values = {}\n        if process_type_id is not None:\n            route_values['processTypeId'] = self._serialize.url('process_type_id', process_type_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='02cc6a73-5cfb-427d-8c8e-b49fb086e8af',\n                   version='5.0-preview.2',\n                   route_values=route_values)", "response": "Delete a process of a specific ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef edit_process(self, update_request, process_type_id):\n        route_values = {}\n        if process_type_id is not None:\n            route_values['processTypeId'] = self._serialize.url('process_type_id', process_type_id, 'str')\n        content = self._serialize.body(update_request, 'UpdateProcessModel')\n        response = self._send(http_method='PATCH',\n                              location_id='02cc6a73-5cfb-427d-8c8e-b49fb086e8af',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ProcessInfo', response)", "response": "Edit a specific process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_list_of_processes(self, expand=None):\n        query_parameters = {}\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='02cc6a73-5cfb-427d-8c8e-b49fb086e8af',\n                              version='5.0-preview.2',\n                              query_parameters=query_parameters)\n        return self._deserialize('[ProcessInfo]', self._unwrap_collection(response))", "response": "Get list of all processes including system and inherited."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a single process of a specified type ID.", "response": "def get_process_by_its_id(self, process_type_id, expand=None):\n        \"\"\"GetProcessByItsId.\n        [Preview API] Get a single process of a specified ID.\n        :param str process_type_id:\n        :param str expand:\n        :rtype: :class:`<ProcessInfo> <azure.devops.v5_0.work_item_tracking_process.models.ProcessInfo>`\n        \"\"\"\n        route_values = {}\n        if process_type_id is not None:\n            route_values['processTypeId'] = self._serialize.url('process_type_id', process_type_id, 'str')\n        query_parameters = {}\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='02cc6a73-5cfb-427d-8c8e-b49fb086e8af',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ProcessInfo', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a rule to a process item type.", "response": "def add_process_work_item_type_rule(self, process_rule_create, process_id, wit_ref_name):\n        \"\"\"AddProcessWorkItemTypeRule.\n        [Preview API] Adds a rule to work item type in the process.\n        :param :class:`<CreateProcessRuleRequest> <azure.devops.v5_0.work_item_tracking_process.models.CreateProcessRuleRequest>` process_rule_create:\n        :param str process_id: The ID of the process\n        :param str wit_ref_name: The reference name of the work item type\n        :rtype: :class:`<ProcessRule> <azure.devops.v5_0.work_item_tracking_process.models.ProcessRule>`\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        content = self._serialize.body(process_rule_create, 'CreateProcessRuleRequest')\n        response = self._send(http_method='POST',\n                              location_id='76fe3432-d825-479d-a5f6-983bbb78b4f3',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ProcessRule', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_process_work_item_type_rule(self, process_id, wit_ref_name, rule_id):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if rule_id is not None:\n            route_values['ruleId'] = self._serialize.url('rule_id', rule_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='76fe3432-d825-479d-a5f6-983bbb78b4f3',\n                   version='5.0-preview.2',\n                   route_values=route_values)", "response": "Delete a rule from a process item type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate a rule in the process item type of the process.", "response": "def update_process_work_item_type_rule(self, process_rule, process_id, wit_ref_name, rule_id):\n        \"\"\"UpdateProcessWorkItemTypeRule.\n        [Preview API] Updates a rule in the work item type of the process.\n        :param :class:`<UpdateProcessRuleRequest> <azure.devops.v5_0.work_item_tracking_process.models.UpdateProcessRuleRequest>` process_rule:\n        :param str process_id: The ID of the process\n        :param str wit_ref_name: The reference name of the work item type\n        :param str rule_id: The ID of the rule\n        :rtype: :class:`<ProcessRule> <azure.devops.v5_0.work_item_tracking_process.models.ProcessRule>`\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if rule_id is not None:\n            route_values['ruleId'] = self._serialize.url('rule_id', rule_id, 'str')\n        content = self._serialize.body(process_rule, 'UpdateProcessRuleRequest')\n        response = self._send(http_method='PUT',\n                              location_id='76fe3432-d825-479d-a5f6-983bbb78b4f3',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ProcessRule', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a state definition in the work item type of the process.", "response": "def delete_state_definition(self, process_id, wit_ref_name, state_id):\n        \"\"\"DeleteStateDefinition.\n        [Preview API] Removes a state definition in the work item type of the process.\n        :param str process_id: ID of the process\n        :param str wit_ref_name: The reference name of the work item type\n        :param str state_id: ID of the state\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if state_id is not None:\n            route_values['stateId'] = self._serialize.url('state_id', state_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='31015d57-2dff-4a46-adb3-2fb4ee3dcec9',\n                   version='5.0-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hide_state_definition(self, hide_state_model, process_id, wit_ref_name, state_id):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if state_id is not None:\n            route_values['stateId'] = self._serialize.url('state_id', state_id, 'str')\n        content = self._serialize.body(hide_state_model, 'HideStateModel')\n        response = self._send(http_method='PUT',\n                              location_id='31015d57-2dff-4a46-adb3-2fb4ee3dcec9',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WorkItemStateResultModel', response)", "response": "Hides a state definition in the process. Only states with customizationType System can be hidden."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates a given state definition in the given process.", "response": "def update_state_definition(self, state_model, process_id, wit_ref_name, state_id):\n        \"\"\"UpdateStateDefinition.\n        [Preview API] Updates a given state definition in the work item type of the process.\n        :param :class:`<WorkItemStateInputModel> <azure.devops.v5_0.work_item_tracking_process.models.WorkItemStateInputModel>` state_model:\n        :param str process_id: ID of the process\n        :param str wit_ref_name: The reference name of the work item type\n        :param str state_id: ID of the state\n        :rtype: :class:`<WorkItemStateResultModel> <azure.devops.v5_0.work_item_tracking_process.models.WorkItemStateResultModel>`\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        if state_id is not None:\n            route_values['stateId'] = self._serialize.url('state_id', state_id, 'str')\n        content = self._serialize.body(state_model, 'WorkItemStateInputModel')\n        response = self._send(http_method='PATCH',\n                              location_id='31015d57-2dff-4a46-adb3-2fb4ee3dcec9',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WorkItemStateResultModel', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new process item type in the process.", "response": "def create_process_work_item_type(self, work_item_type, process_id):\n        \"\"\"CreateProcessWorkItemType.\n        [Preview API] Creates a work item type in the process.\n        :param :class:`<CreateProcessWorkItemTypeRequest> <azure.devops.v5_0.work_item_tracking_process.models.CreateProcessWorkItemTypeRequest>` work_item_type:\n        :param str process_id: The ID of the process on which to create work item type.\n        :rtype: :class:`<ProcessWorkItemType> <azure.devops.v5_0.work_item_tracking_process.models.ProcessWorkItemType>`\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        content = self._serialize.body(work_item_type, 'CreateProcessWorkItemTypeRequest')\n        response = self._send(http_method='POST',\n                              location_id='e2e9d1a6-432d-4062-8870-bfcb8c324ad7',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ProcessWorkItemType', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_process_work_item_type(self, process_id, wit_ref_name):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        self._send(http_method='DELETE',\n                   location_id='e2e9d1a6-432d-4062-8870-bfcb8c324ad7',\n                   version='5.0-preview.2',\n                   route_values=route_values)", "response": "Delete a work item type in a process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_process_work_item_type(self, process_id, wit_ref_name, expand=None):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        query_parameters = {}\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e2e9d1a6-432d-4062-8870-bfcb8c324ad7',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ProcessWorkItemType', response)", "response": "Get a single work item type in a process."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_process_work_item_types(self, process_id, expand=None):\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        query_parameters = {}\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e2e9d1a6-432d-4062-8870-bfcb8c324ad7',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[ProcessWorkItemType]', self._unwrap_collection(response))", "response": "Get all work item types in a process."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates a process s work item type.", "response": "def update_process_work_item_type(self, work_item_type_update, process_id, wit_ref_name):\n        \"\"\"UpdateProcessWorkItemType.\n        [Preview API] Updates a work item type of the process.\n        :param :class:`<UpdateProcessWorkItemTypeRequest> <azure.devops.v5_0.work_item_tracking_process.models.UpdateProcessWorkItemTypeRequest>` work_item_type_update:\n        :param str process_id: The ID of the process\n        :param str wit_ref_name: The reference name of the work item type\n        :rtype: :class:`<ProcessWorkItemType> <azure.devops.v5_0.work_item_tracking_process.models.ProcessWorkItemType>`\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name is not None:\n            route_values['witRefName'] = self._serialize.url('wit_ref_name', wit_ref_name, 'str')\n        content = self._serialize.body(work_item_type_update, 'UpdateProcessWorkItemTypeRequest')\n        response = self._send(http_method='PATCH',\n                              location_id='e2e9d1a6-432d-4062-8870-bfcb8c324ad7',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ProcessWorkItemType', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the behavior for a particular work item type.", "response": "def get_behavior_for_work_item_type(self, process_id, wit_ref_name_for_behaviors, behavior_ref_name):\n        \"\"\"GetBehaviorForWorkItemType.\n        [Preview API] Returns a behavior for the work item type of the process.\n        :param str process_id: The ID of the process\n        :param str wit_ref_name_for_behaviors: Work item type reference name for the behavior\n        :param str behavior_ref_name: The reference name of the behavior\n        :rtype: :class:`<WorkItemTypeBehavior> <azure.devops.v5_0.work_item_tracking_process.models.WorkItemTypeBehavior>`\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        if wit_ref_name_for_behaviors is not None:\n            route_values['witRefNameForBehaviors'] = self._serialize.url('wit_ref_name_for_behaviors', wit_ref_name_for_behaviors, 'str')\n        if behavior_ref_name is not None:\n            route_values['behaviorRefName'] = self._serialize.url('behavior_ref_name', behavior_ref_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='6d765a2e-4e1b-4b11-be93-f953be676024',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('WorkItemTypeBehavior', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the country and region for a given IPv4 address.", "response": "def get_geo_region(self, ip):\n        \"\"\"GetGeoRegion.\n        [Preview API] Lookup up country/region based on provided IPv4, null if using the remote IPv4 address.\n        :param str ip:\n        :rtype: :class:`<GeoRegion> <azure.devops.v5_1.profile-regions.models.GeoRegion>`\n        \"\"\"\n        query_parameters = {}\n        if ip is not None:\n            query_parameters['ip'] = self._serialize.query('ip', ip, 'str')\n        response = self._send(http_method='GET',\n                              location_id='35b3ff1d-ab4c-4d1c-98bb-f6ea21d86bd9',\n                              version='5.1-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('GeoRegion', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_regions(self):\n        response = self._send(http_method='GET',\n                              location_id='b129ca90-999d-47bb-ab37-0dcf784ee633',\n                              version='5.1-preview.1')\n        return self._deserialize('ProfileRegions', response)", "response": "GetRegions.\n        [Preview API]\n        :rtype: :class:`<ProfileRegions> <azure.devops.v5_1.profile-regions.models.ProfileRegions>`"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget an attachment zip.", "response": "def get_attachment_zip(self, id, project=None, file_name=None, download=None, **kwargs):\n        \"\"\"GetAttachmentZip.\n        [Preview API] Downloads an attachment.\n        :param str id: Attachment ID\n        :param str project: Project ID or project name\n        :param str file_name: Name of the file\n        :param bool download: If set to <c>true</c> always download attachment\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'str')\n        query_parameters = {}\n        if file_name is not None:\n            query_parameters['fileName'] = self._serialize.query('file_name', file_name, 'str')\n        if download is not None:\n            query_parameters['download'] = self._serialize.query('download', download, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='e07b5fa4-1499-494d-a496-64b860fd64ff',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='application/zip')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_classification_nodes(self, project, ids, depth=None, error_policy=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if ids is not None:\n            ids = \",\".join(map(str, ids))\n            query_parameters['ids'] = self._serialize.query('ids', ids, 'str')\n        if depth is not None:\n            query_parameters['$depth'] = self._serialize.query('depth', depth, 'int')\n        if error_policy is not None:\n            query_parameters['errorPolicy'] = self._serialize.query('error_policy', error_policy, 'str')\n        response = self._send(http_method='GET',\n                              location_id='a70579d1-f53a-48ee-a5be-7be8659023b9',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[WorkItemClassificationNode]', self._unwrap_collection(response))", "response": "Get the root classification nodes for a given list of nodes ids."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the root classification nodes under the project.", "response": "def get_root_nodes(self, project, depth=None):\n        \"\"\"GetRootNodes.\n        [Preview API] Gets root classification nodes under the project.\n        :param str project: Project ID or project name\n        :param int depth: Depth of children to fetch.\n        :rtype: [WorkItemClassificationNode]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if depth is not None:\n            query_parameters['$depth'] = self._serialize.query('depth', depth, 'int')\n        response = self._send(http_method='GET',\n                              location_id='a70579d1-f53a-48ee-a5be-7be8659023b9',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[WorkItemClassificationNode]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_comment(self, project, work_item_id, comment_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if work_item_id is not None:\n            route_values['workItemId'] = self._serialize.url('work_item_id', work_item_id, 'int')\n        if comment_id is not None:\n            route_values['commentId'] = self._serialize.url('comment_id', comment_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='608aac0a-32e1-4493-a863-b9cf4566d257',\n                   version='5.1-preview.3',\n                   route_values=route_values)", "response": "Delete a comment on a specific work item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a comment for a given work item.", "response": "def get_comment(self, project, work_item_id, comment_id, include_deleted=None, expand=None):\n        \"\"\"GetComment.\n        [Preview API] Returns a work item comment.\n        :param str project: Project ID or project name\n        :param int work_item_id: Id of a work item to get the comment.\n        :param int comment_id: Id of the comment to return.\n        :param bool include_deleted: Specify if the deleted comment should be retrieved.\n        :param str expand: Specifies the additional data retrieval options for work item comments.\n        :rtype: :class:`<Comment> <azure.devops.v5_1.work-item-tracking.models.Comment>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if work_item_id is not None:\n            route_values['workItemId'] = self._serialize.url('work_item_id', work_item_id, 'int')\n        if comment_id is not None:\n            route_values['commentId'] = self._serialize.url('comment_id', comment_id, 'int')\n        query_parameters = {}\n        if include_deleted is not None:\n            query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='608aac0a-32e1-4493-a863-b9cf4566d257',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Comment', response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_comments(self, project, work_item_id, top=None, continuation_token=None, include_deleted=None, expand=None, order=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if work_item_id is not None:\n            route_values['workItemId'] = self._serialize.url('work_item_id', work_item_id, 'int')\n        query_parameters = {}\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        if include_deleted is not None:\n            query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        if order is not None:\n            query_parameters['order'] = self._serialize.query('order', order, 'str')\n        response = self._send(http_method='GET',\n                              location_id='608aac0a-32e1-4493-a863-b9cf4566d257',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('CommentList', response)", "response": "Get comments for a given work item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_comments_batch(self, project, work_item_id, ids, include_deleted=None, expand=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if work_item_id is not None:\n            route_values['workItemId'] = self._serialize.url('work_item_id', work_item_id, 'int')\n        query_parameters = {}\n        if ids is not None:\n            ids = \",\".join(map(str, ids))\n            query_parameters['ids'] = self._serialize.query('ids', ids, 'str')\n        if include_deleted is not None:\n            query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='608aac0a-32e1-4493-a863-b9cf4566d257',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('CommentList', response)", "response": "Get comments for a list of ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_field(self, work_item_field, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(work_item_field, 'WorkItemField')\n        response = self._send(http_method='POST',\n                              location_id='b51fd764-e5c2-4b9b-aaf7-3395cf4bdd94',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WorkItemField', response)", "response": "Create a new field."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_field(self, field_name_or_ref_name, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if field_name_or_ref_name is not None:\n            route_values['fieldNameOrRefName'] = self._serialize.url('field_name_or_ref_name', field_name_or_ref_name, 'str')\n        self._send(http_method='DELETE',\n                   location_id='b51fd764-e5c2-4b9b-aaf7-3395cf4bdd94',\n                   version='5.1-preview.2',\n                   route_values=route_values)", "response": "DeleteField.\n        [Preview API] Deletes the field.\n        :param str field_name_or_ref_name: Field simple name or reference name\n        :param str project: Project ID or project name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a specific field.", "response": "def get_field(self, field_name_or_ref_name, project=None):\n        \"\"\"GetField.\n        [Preview API] Gets information on a specific field.\n        :param str field_name_or_ref_name: Field simple name or reference name\n        :param str project: Project ID or project name\n        :rtype: :class:`<WorkItemField> <azure.devops.v5_1.work-item-tracking.models.WorkItemField>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if field_name_or_ref_name is not None:\n            route_values['fieldNameOrRefName'] = self._serialize.url('field_name_or_ref_name', field_name_or_ref_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='b51fd764-e5c2-4b9b-aaf7-3395cf4bdd94',\n                              version='5.1-preview.2',\n                              route_values=route_values)\n        return self._deserialize('WorkItemField', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_query(self, project, query):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if query is not None:\n            route_values['query'] = self._serialize.url('query', query, 'str')\n        self._send(http_method='DELETE',\n                   location_id='a67d190c-c41f-424b-814d-0e906f659301',\n                   version='5.1-preview.2',\n                   route_values=route_values)", "response": "Delete a query or folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the root queries and their children.", "response": "def get_queries(self, project, expand=None, depth=None, include_deleted=None):\n        \"\"\"GetQueries.\n        [Preview API] Gets the root queries and their children\n        :param str project: Project ID or project name\n        :param str expand: Include the query string (wiql), clauses, query result columns, and sort options in the results.\n        :param int depth: In the folder of queries, return child queries and folders to this depth.\n        :param bool include_deleted: Include deleted queries and folders\n        :rtype: [QueryHierarchyItem]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        if depth is not None:\n            query_parameters['$depth'] = self._serialize.query('depth', depth, 'int')\n        if include_deleted is not None:\n            query_parameters['$includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='a67d190c-c41f-424b-814d-0e906f659301',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[QueryHierarchyItem]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_query(self, query_update, project, query, undelete_descendants=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if query is not None:\n            route_values['query'] = self._serialize.url('query', query, 'str')\n        query_parameters = {}\n        if undelete_descendants is not None:\n            query_parameters['$undeleteDescendants'] = self._serialize.query('undelete_descendants', undelete_descendants, 'bool')\n        content = self._serialize.body(query_update, 'QueryHierarchyItem')\n        response = self._send(http_method='PATCH',\n                              location_id='a67d190c-c41f-424b-814d-0e906f659301',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('QueryHierarchyItem', response)", "response": "Update a query or a folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of queries by ids", "response": "def get_queries_batch(self, query_get_request, project):\n        \"\"\"GetQueriesBatch.\n        [Preview API] Gets a list of queries by ids (Maximum 1000)\n        :param :class:`<QueryBatchGetRequest> <azure.devops.v5_1.work_item_tracking.models.QueryBatchGetRequest>` query_get_request:\n        :param str project: Project ID or project name\n        :rtype: [QueryHierarchyItem]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(query_get_request, 'QueryBatchGetRequest')\n        response = self._send(http_method='POST',\n                              location_id='549816f9-09b0-4e75-9e81-01fbfcd07426',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[QueryHierarchyItem]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_deleted_work_items(self, ids, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if ids is not None:\n            ids = \",\".join(map(str, ids))\n            query_parameters['ids'] = self._serialize.query('ids', ids, 'str')\n        response = self._send(http_method='GET',\n                              location_id='b70d8d39-926c-465e-b927-b1bf0e5ca0e0',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[WorkItemDeleteReference]', self._unwrap_collection(response))", "response": "Get the deleted work items from the recycle bin."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restore_work_item(self, payload, id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        content = self._serialize.body(payload, 'WorkItemDeleteUpdate')\n        response = self._send(http_method='PATCH',\n                              location_id='b70d8d39-926c-465e-b927-b1bf0e5ca0e0',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WorkItemDelete', response)", "response": "Restores the deleted work item from Recycle Bin."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a revision of a resource.", "response": "def get_revision(self, id, revision_number, project=None, expand=None):\n        \"\"\"GetRevision.\n        [Preview API] Returns a fully hydrated work item for the requested revision\n        :param int id:\n        :param int revision_number:\n        :param str project: Project ID or project name\n        :param str expand:\n        :rtype: :class:`<WorkItem> <azure.devops.v5_1.work-item-tracking.models.WorkItem>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        if revision_number is not None:\n            route_values['revisionNumber'] = self._serialize.url('revision_number', revision_number, 'int')\n        query_parameters = {}\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='a00c85a5-80fa-4565-99c3-bcd2181434bb',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('WorkItem', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreplacing a template with a new one.", "response": "def replace_template(self, template_content, team_context, template_id):\n        \"\"\"ReplaceTemplate.\n        [Preview API] Replace template contents\n        :param :class:`<WorkItemTemplate> <azure.devops.v5_1.work_item_tracking.models.WorkItemTemplate>` template_content: Template contents to replace with\n        :param :class:`<TeamContext> <azure.devops.v5_1.work_item_tracking.models.TeamContext>` team_context: The team context for the operation\n        :param str template_id: Template id\n        :rtype: :class:`<WorkItemTemplate> <azure.devops.v5_1.work-item-tracking.models.WorkItemTemplate>`\n        \"\"\"\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if template_id is not None:\n            route_values['templateId'] = self._serialize.url('template_id', template_id, 'str')\n        content = self._serialize.body(template_content, 'WorkItemTemplate')\n        response = self._send(http_method='PUT',\n                              location_id='fb10264a-8836-48a0-8033-1b0ccd2748d5',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WorkItemTemplate', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a single update for a work item.", "response": "def get_update(self, id, update_number, project=None):\n        \"\"\"GetUpdate.\n        [Preview API] Returns a single update for a work item\n        :param int id:\n        :param int update_number:\n        :param str project: Project ID or project name\n        :rtype: :class:`<WorkItemUpdate> <azure.devops.v5_1.work-item-tracking.models.WorkItemUpdate>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        if update_number is not None:\n            route_values['updateNumber'] = self._serialize.url('update_number', update_number, 'int')\n        response = self._send(http_method='GET',\n                              location_id='6570bf97-d02c-4a91-8d93-3abe9895b1a9',\n                              version='5.1-preview.3',\n                              route_values=route_values)\n        return self._deserialize('WorkItemUpdate', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting updates between a specific work item and a specific project.", "response": "def get_updates(self, id, project=None, top=None, skip=None):\n        \"\"\"GetUpdates.\n        [Preview API] Returns a the deltas between work item revisions\n        :param int id:\n        :param str project: Project ID or project name\n        :param int top:\n        :param int skip:\n        :rtype: [WorkItemUpdate]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        query_parameters = {}\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        response = self._send(http_method='GET',\n                              location_id='6570bf97-d02c-4a91-8d93-3abe9895b1a9',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[WorkItemUpdate]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a work item icon given the friendly name and icon color.", "response": "def get_work_item_icon_svg(self, icon, color=None, v=None, **kwargs):\n        \"\"\"GetWorkItemIconSvg.\n        [Preview API] Get a work item icon given the friendly name and icon color.\n        :param str icon: The name of the icon\n        :param str color: The 6-digit hex color for the icon\n        :param int v: The version of the icon (used only for cache invalidation)\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if icon is not None:\n            route_values['icon'] = self._serialize.url('icon', icon, 'str')\n        query_parameters = {}\n        if color is not None:\n            query_parameters['color'] = self._serialize.query('color', color, 'str')\n        if v is not None:\n            query_parameters['v'] = self._serialize.query('v', v, 'int')\n        response = self._send(http_method='GET',\n                              location_id='4e1eb4a5-1970-4228-a682-ec48eb2dca30',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='image/svg+xml')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_reporting_links_by_link_type(self, project=None, link_types=None, types=None, continuation_token=None, start_date_time=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if link_types is not None:\n            link_types = \",\".join(link_types)\n            query_parameters['linkTypes'] = self._serialize.query('link_types', link_types, 'str')\n        if types is not None:\n            types = \",\".join(types)\n            query_parameters['types'] = self._serialize.query('types', types, 'str')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        if start_date_time is not None:\n            query_parameters['startDateTime'] = self._serialize.query('start_date_time', start_date_time, 'iso-8601')\n        response = self._send(http_method='GET',\n                              location_id='b5b5b6d0-0308-40a1-b3f4-b9bb3c66878f',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ReportingWorkItemLinksBatch', response)", "response": "Get a batch of work item links by link type."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the work item relation type definition.", "response": "def get_relation_type(self, relation):\n        \"\"\"GetRelationType.\n        [Preview API] Gets the work item relation type definition.\n        :param str relation: The relation name\n        :rtype: :class:`<WorkItemRelationType> <azure.devops.v5_1.work-item-tracking.models.WorkItemRelationType>`\n        \"\"\"\n        route_values = {}\n        if relation is not None:\n            route_values['relation'] = self._serialize.url('relation', relation, 'str')\n        response = self._send(http_method='GET',\n                              location_id='f5d33bc9-5b49-4a3c-a9bd-f3cd46dd2165',\n                              version='5.1-preview.2',\n                              route_values=route_values)\n        return self._deserialize('WorkItemRelationType', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the reporting discussion list for a specific project.", "response": "def read_reporting_discussions(self, project=None, continuation_token=None, max_page_size=None):\n        \"\"\"ReadReportingDiscussions.\n        [Preview API]\n        :param str project: Project ID or project name\n        :param str continuation_token:\n        :param int max_page_size:\n        :rtype: :class:`<ReportingWorkItemRevisionsBatch> <azure.devops.v5_1.work-item-tracking.models.ReportingWorkItemRevisionsBatch>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        if max_page_size is not None:\n            query_parameters['$maxPageSize'] = self._serialize.query('max_page_size', max_page_size, 'int')\n        response = self._send(http_method='GET',\n                              location_id='4a644469-90c5-4fcc-9a9f-be0827d369ec',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ReportingWorkItemRevisionsBatch', response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_work_item(self, document, project, type, validate_only=None, bypass_rules=None, suppress_notifications=None, expand=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if type is not None:\n            route_values['type'] = self._serialize.url('type', type, 'str')\n        query_parameters = {}\n        if validate_only is not None:\n            query_parameters['validateOnly'] = self._serialize.query('validate_only', validate_only, 'bool')\n        if bypass_rules is not None:\n            query_parameters['bypassRules'] = self._serialize.query('bypass_rules', bypass_rules, 'bool')\n        if suppress_notifications is not None:\n            query_parameters['suppressNotifications'] = self._serialize.query('suppress_notifications', suppress_notifications, 'bool')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        content = self._serialize.body(document, '[JsonPatchOperation]')\n        response = self._send(http_method='POST',\n                              location_id='62d3d110-0047-428c-ad3c-4fe872c91c74',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content,\n                              media_type='application/json-patch+json')\n        return self._deserialize('WorkItem', response)", "response": "Creates a single work item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_work_item_template(self, project, type, fields=None, as_of=None, expand=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if type is not None:\n            route_values['type'] = self._serialize.url('type', type, 'str')\n        query_parameters = {}\n        if fields is not None:\n            query_parameters['fields'] = self._serialize.query('fields', fields, 'str')\n        if as_of is not None:\n            query_parameters['asOf'] = self._serialize.query('as_of', as_of, 'iso-8601')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='62d3d110-0047-428c-ad3c-4fe872c91c74',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('WorkItem', response)", "response": "Get a single work item from a template."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a work item from the Recycle Bin.", "response": "def delete_work_item(self, id, project=None, destroy=None):\n        \"\"\"DeleteWorkItem.\n        [Preview API] Deletes the specified work item and sends it to the Recycle Bin, so that it can be restored back, if required. Optionally, if the destroy parameter has been set to true, it destroys the work item permanently. WARNING: If the destroy parameter is set to true, work items deleted by this command will NOT go to recycle-bin and there is no way to restore/recover them after deletion. It is recommended NOT to use this parameter. If you do, please use this parameter with extreme caution.\n        :param int id: ID of the work item to be deleted\n        :param str project: Project ID or project name\n        :param bool destroy: Optional parameter, if set to true, the work item is deleted permanently. Please note: the destroy action is PERMANENT and cannot be undone.\n        :rtype: :class:`<WorkItemDelete> <azure.devops.v5_1.work-item-tracking.models.WorkItemDelete>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        query_parameters = {}\n        if destroy is not None:\n            query_parameters['destroy'] = self._serialize.query('destroy', destroy, 'bool')\n        response = self._send(http_method='DELETE',\n                              location_id='72c7ddf8-2cdc-4f60-90cd-ab71c14a399b',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('WorkItemDelete', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_work_item(self, id, project=None, fields=None, as_of=None, expand=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        query_parameters = {}\n        if fields is not None:\n            fields = \",\".join(fields)\n            query_parameters['fields'] = self._serialize.query('fields', fields, 'str')\n        if as_of is not None:\n            query_parameters['asOf'] = self._serialize.query('as_of', as_of, 'iso-8601')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='72c7ddf8-2cdc-4f60-90cd-ab71c14a399b',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('WorkItem', response)", "response": "Get a single work item."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_work_item_next_states_on_checkin_action(self, ids, action=None):\n        query_parameters = {}\n        if ids is not None:\n            ids = \",\".join(map(str, ids))\n            query_parameters['ids'] = self._serialize.query('ids', ids, 'str')\n        if action is not None:\n            query_parameters['action'] = self._serialize.query('action', action, 'str')\n        response = self._send(http_method='GET',\n                              location_id='afae844b-e2f6-44c2-8053-17b3bb936a40',\n                              version='5.1-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('[WorkItemNextStateOnTransition]', self._unwrap_collection(response))", "response": "Get the next states on a given list of work items."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_work_item_type_category(self, project, category):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if category is not None:\n            route_values['category'] = self._serialize.url('category', category, 'str')\n        response = self._send(http_method='GET',\n                              location_id='9b9f5734-36c8-415e-ba67-f83b45c31408',\n                              version='5.1-preview.2',\n                              route_values=route_values)\n        return self._deserialize('WorkItemTypeCategory', response)", "response": "Get specific work item type category by name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a field for a work item type with detailed references.", "response": "def get_work_item_type_field_with_references(self, project, type, field, expand=None):\n        \"\"\"GetWorkItemTypeFieldWithReferences.\n        [Preview API] Get a field for a work item type with detailed references.\n        :param str project: Project ID or project name\n        :param str type: Work item type.\n        :param str field:\n        :param str expand: Expand level for the API response. Properties: to include allowedvalues, default value, isRequired etc. as a part of response; None: to skip these properties.\n        :rtype: :class:`<WorkItemTypeFieldWithReferences> <azure.devops.v5_1.work-item-tracking.models.WorkItemTypeFieldWithReferences>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if type is not None:\n            route_values['type'] = self._serialize.url('type', type, 'str')\n        if field is not None:\n            route_values['field'] = self._serialize.url('field', field, 'str')\n        query_parameters = {}\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='bd293ce5-3d25-4192-8e67-e8092e879efb',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('WorkItemTypeFieldWithReferences', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_installed_extensions(self, include_disabled_extensions=None, include_errors=None, asset_types=None, include_installation_issues=None):\n        query_parameters = {}\n        if include_disabled_extensions is not None:\n            query_parameters['includeDisabledExtensions'] = self._serialize.query('include_disabled_extensions', include_disabled_extensions, 'bool')\n        if include_errors is not None:\n            query_parameters['includeErrors'] = self._serialize.query('include_errors', include_errors, 'bool')\n        if asset_types is not None:\n            asset_types = \":\".join(asset_types)\n            query_parameters['assetTypes'] = self._serialize.query('asset_types', asset_types, 'str')\n        if include_installation_issues is not None:\n            query_parameters['includeInstallationIssues'] = self._serialize.query('include_installation_issues', include_installation_issues, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='275424d0-c844-4fe2-bda6-04933a1357d8',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('[InstalledExtension]', self._unwrap_collection(response))", "response": "Get the installed extensions in the account / project collection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting an installed extension by its publisher and extension name.", "response": "def get_installed_extension_by_name(self, publisher_name, extension_name, asset_types=None):\n        \"\"\"GetInstalledExtensionByName.\n        [Preview API] Get an installed extension by its publisher and extension name.\n        :param str publisher_name: Name of the publisher. Example: \"fabrikam\".\n        :param str extension_name: Name of the extension. Example: \"ops-tools\".\n        :param [str] asset_types:\n        :rtype: :class:`<InstalledExtension> <azure.devops.v5_0.extension_management.models.InstalledExtension>`\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        query_parameters = {}\n        if asset_types is not None:\n            asset_types = \":\".join(asset_types)\n            query_parameters['assetTypes'] = self._serialize.query('asset_types', asset_types, 'str')\n        response = self._send(http_method='GET',\n                              location_id='fb0da285-f23e-4b56-8b53-3ef5f9f6de66',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('InstalledExtension', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshare an extension with a publisher.", "response": "def share_extension(self, publisher_name, extension_name, account_name):\n        \"\"\"ShareExtension.\n        [Preview API]\n        :param str publisher_name:\n        :param str extension_name:\n        :param str account_name:\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        if account_name is not None:\n            route_values['accountName'] = self._serialize.url('account_name', account_name, 'str')\n        self._send(http_method='POST',\n                   location_id='a1e66d8f-f5de-4d16-8309-91a4e015ee46',\n                   version='5.1-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_acquisition_options(self, item_id, installation_target, test_commerce=None, is_free_or_trial_install=None):\n        route_values = {}\n        if item_id is not None:\n            route_values['itemId'] = self._serialize.url('item_id', item_id, 'str')\n        query_parameters = {}\n        if installation_target is not None:\n            query_parameters['installationTarget'] = self._serialize.query('installation_target', installation_target, 'str')\n        if test_commerce is not None:\n            query_parameters['testCommerce'] = self._serialize.query('test_commerce', test_commerce, 'bool')\n        if is_free_or_trial_install is not None:\n            query_parameters['isFreeOrTrialInstall'] = self._serialize.query('is_free_or_trial_install', is_free_or_trial_install, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='9d0a0105-075e-4760-aa15-8bcf54d1bd7d',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('AcquisitionOptions', response)", "response": "Get acquisition options for a specific item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_acquisition(self, acquisition_request):\n        content = self._serialize.body(acquisition_request, 'ExtensionAcquisitionRequest')\n        response = self._send(http_method='POST',\n                              location_id='3adb1f2d-e328-446e-be73-9f6d98071c45',\n                              version='5.1-preview.1',\n                              content=content)\n        return self._deserialize('ExtensionAcquisitionRequest', response)", "response": "Request the acquisition for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting Asset by name.", "response": "def get_asset_by_name(self, publisher_name, extension_name, version, asset_type, account_token=None, accept_default=None, account_token_header=None, **kwargs):\n        \"\"\"GetAssetByName.\n        [Preview API]\n        :param str publisher_name:\n        :param str extension_name:\n        :param str version:\n        :param str asset_type:\n        :param str account_token:\n        :param bool accept_default:\n        :param String account_token_header: Header to pass the account token\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        if version is not None:\n            route_values['version'] = self._serialize.url('version', version, 'str')\n        if asset_type is not None:\n            route_values['assetType'] = self._serialize.url('asset_type', asset_type, 'str')\n        query_parameters = {}\n        if account_token is not None:\n            query_parameters['accountToken'] = self._serialize.query('account_token', account_token, 'str')\n        if accept_default is not None:\n            query_parameters['acceptDefault'] = self._serialize.query('accept_default', accept_default, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='7529171f-a002-4180-93ba-685f358a0482',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query_associated_azure_publisher(self, publisher_name):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='efd202a6-9d87-4ebc-9229-d2b8ae2fdb6d',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('AzurePublisher', response)", "response": "Query associated Azure publisher."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_category_tree(self, product, category_id, lcid=None, source=None, product_version=None, skus=None, sub_skus=None):\n        route_values = {}\n        if product is not None:\n            route_values['product'] = self._serialize.url('product', product, 'str')\n        if category_id is not None:\n            route_values['categoryId'] = self._serialize.url('category_id', category_id, 'str')\n        query_parameters = {}\n        if lcid is not None:\n            query_parameters['lcid'] = self._serialize.query('lcid', lcid, 'int')\n        if source is not None:\n            query_parameters['source'] = self._serialize.query('source', source, 'str')\n        if product_version is not None:\n            query_parameters['productVersion'] = self._serialize.query('product_version', product_version, 'str')\n        if skus is not None:\n            query_parameters['skus'] = self._serialize.query('skus', skus, 'str')\n        if sub_skus is not None:\n            query_parameters['subSkus'] = self._serialize.query('sub_skus', sub_skus, 'str')\n        response = self._send(http_method='GET',\n                              location_id='1102bb42-82b0-4955-8d8a-435d6b4cedd3',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ProductCategory', response)", "response": "GetCategoryTree.\n        [Preview API]\n        :param str product:\n        :param str category_id:\n        :param int lcid:\n        :param str source:\n        :param str product_version:\n        :param str skus:\n        :param str sub_skus:\n        :rtype: :class:`<ProductCategory> <azure.devops.v5_1.gallery.models.ProductCategory>`"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef perform_edit_extension_draft_operation(self, draft_patch, publisher_name, extension_name, draft_id):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        if draft_id is not None:\n            route_values['draftId'] = self._serialize.url('draft_id', draft_id, 'str')\n        content = self._serialize.body(draft_patch, 'ExtensionDraftPatch')\n        response = self._send(http_method='PATCH',\n                              location_id='02b33873-4e61-496e-83a2-59d1df46b7d8',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ExtensionDraft', response)", "response": "Perform an edit extension draft operation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_asset_from_new_extension_draft(self, publisher_name, draft_id, asset_type, **kwargs):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if draft_id is not None:\n            route_values['draftId'] = self._serialize.url('draft_id', draft_id, 'str')\n        if asset_type is not None:\n            route_values['assetType'] = self._serialize.url('asset_type', asset_type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='88c0b1c8-b4f1-498a-9b2a-8446ef9f32e7',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get Asset from New Extension Draft."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget install or uninstall events of an extension.", "response": "def get_extension_events(self, publisher_name, extension_name, count=None, after_date=None, include=None, include_property=None):\n        \"\"\"GetExtensionEvents.\n        [Preview API] Get install/uninstall events of an extension. If both count and afterDate parameters are specified, count takes precedence.\n        :param str publisher_name: Name of the publisher\n        :param str extension_name: Name of the extension\n        :param int count: Count of events to fetch, applies to each event type.\n        :param datetime after_date: Fetch events that occurred on or after this date\n        :param str include: Filter options. Supported values: install, uninstall, review, acquisition, sales. Default is to fetch all types of events\n        :param str include_property: Event properties to include. Currently only 'lastContactDetails' is supported for uninstall events\n        :rtype: :class:`<ExtensionEvents> <azure.devops.v5_1.gallery.models.ExtensionEvents>`\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        query_parameters = {}\n        if count is not None:\n            query_parameters['count'] = self._serialize.query('count', count, 'int')\n        if after_date is not None:\n            query_parameters['afterDate'] = self._serialize.query('after_date', after_date, 'iso-8601')\n        if include is not None:\n            query_parameters['include'] = self._serialize.query('include', include, 'str')\n        if include_property is not None:\n            query_parameters['includeProperty'] = self._serialize.query('include_property', include_property, 'str')\n        response = self._send(http_method='GET',\n                              location_id='3d13c499-2168-4d06-bef4-14aba185dcd5',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ExtensionEvents', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef publish_extension_events(self, extension_events):\n        content = self._serialize.body(extension_events, '[ExtensionEvents]')\n        self._send(http_method='POST',\n                   location_id='0bf2bd3a-70e0-4d5d-8bf7-bd4a9c2ab6e7',\n                   version='5.1-preview.1',\n                   content=content)", "response": "Publishes the given extension events."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query_extensions(self, extension_query, account_token=None, account_token_header=None):\n        query_parameters = {}\n        if account_token is not None:\n            query_parameters['accountToken'] = self._serialize.query('account_token', account_token, 'str')\n        content = self._serialize.body(extension_query, 'ExtensionQuery')\n        response = self._send(http_method='POST',\n                              location_id='eb9d5ee1-6d43-456b-b80e-8a96fbc014b6',\n                              version='5.1-preview.1',\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('ExtensionQueryResult', response)", "response": "Query the extensions for the specified account token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_extension(self, upload_stream, **kwargs):\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        content = self._client.stream_upload(upload_stream, callback=callback)\n        response = self._send(http_method='POST',\n                              location_id='a41192c8-9525-4b58-bc86-179fa549d80d',\n                              version='5.1-preview.2',\n                              content=content,\n                              media_type='application/octet-stream')\n        return self._deserialize('PublishedExtension', response)", "response": "Create a published extension from a stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_extension(self, publisher_name, extension_name, version=None, flags=None, account_token=None, account_token_header=None):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        query_parameters = {}\n        if version is not None:\n            query_parameters['version'] = self._serialize.query('version', version, 'str')\n        if flags is not None:\n            query_parameters['flags'] = self._serialize.query('flags', flags, 'str')\n        if account_token is not None:\n            query_parameters['accountToken'] = self._serialize.query('account_token', account_token, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e11ea35a-16fe-4b80-ab11-c4cab88a0966',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('PublishedExtension', response)", "response": "Get the published extension for a given publisher and extension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_notifications(self, notification_data):\n        content = self._serialize.body(notification_data, 'NotificationsData')\n        self._send(http_method='POST',\n                   location_id='eab39817-413c-4602-a49f-07ad00844980',\n                   version='5.1-preview.1',\n                   content=content)", "response": "Send a notification to the current page."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete publisher asset like logo.", "response": "def delete_publisher_asset(self, publisher_name, asset_type=None):\n        \"\"\"DeletePublisherAsset.\n        [Preview API] Delete publisher asset like logo\n        :param str publisher_name: Internal name of the publisher\n        :param str asset_type: Type of asset. Default value is 'logo'.\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        query_parameters = {}\n        if asset_type is not None:\n            query_parameters['assetType'] = self._serialize.query('asset_type', asset_type, 'str')\n        self._send(http_method='DELETE',\n                   location_id='21143299-34f9-4c62-8ca8-53da691192f9',\n                   version='5.1-preview.1',\n                   route_values=route_values,\n                   query_parameters=query_parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_publisher(self, publisher_name):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        self._send(http_method='DELETE',\n                   location_id='4ddec66a-e4f6-4f5d-999e-9e77710d7ff4',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "DeletePublisher.\n        [Preview API]\n        :param str publisher_name:"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_publisher(self, publisher, publisher_name):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        content = self._serialize.body(publisher, 'Publisher')\n        response = self._send(http_method='PUT',\n                              location_id='4ddec66a-e4f6-4f5d-999e-9e77710d7ff4',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Publisher', response)", "response": "Update the publishers related to a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the publisher members of a user.", "response": "def update_publisher_members(self, role_assignments, publisher_name, limit_to_caller_identity_domain=None):\n        \"\"\"UpdatePublisherMembers.\n        [Preview API] Endpoint to add/modify publisher membership. Currently Supports only addition/modification of 1 user at a time Works only for adding members of same tenant.\n        :param [PublisherUserRoleAssignmentRef] role_assignments: List of user identifiers(email address) and role to be added. Currently only one entry is supported.\n        :param str publisher_name: The name/id of publisher to which users have to be added\n        :param bool limit_to_caller_identity_domain: Should cross tenant addtions be allowed or not.\n        :rtype: [PublisherRoleAssignment]\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        query_parameters = {}\n        if limit_to_caller_identity_domain is not None:\n            query_parameters['limitToCallerIdentityDomain'] = self._serialize.query('limit_to_caller_identity_domain', limit_to_caller_identity_domain, 'bool')\n        content = self._serialize.body(role_assignments, '[PublisherUserRoleAssignmentRef]')\n        response = self._send(http_method='POST',\n                              location_id='4ddec66a-e4f6-4f5d-999e-9e77710d7ff4',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('[PublisherRoleAssignment]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreporting a new question for an extension.", "response": "def report_question(self, concern, pub_name, ext_name, question_id):\n        \"\"\"ReportQuestion.\n        [Preview API] Flags a concern with an existing question for an extension.\n        :param :class:`<Concern> <azure.devops.v5_1.gallery.models.Concern>` concern: User reported concern with a question for the extension.\n        :param str pub_name: Name of the publisher who published the extension.\n        :param str ext_name: Name of the extension.\n        :param long question_id: Identifier of the question to be updated for the extension.\n        :rtype: :class:`<Concern> <azure.devops.v5_1.gallery.models.Concern>`\n        \"\"\"\n        route_values = {}\n        if pub_name is not None:\n            route_values['pubName'] = self._serialize.url('pub_name', pub_name, 'str')\n        if ext_name is not None:\n            route_values['extName'] = self._serialize.url('ext_name', ext_name, 'str')\n        if question_id is not None:\n            route_values['questionId'] = self._serialize.url('question_id', question_id, 'long')\n        content = self._serialize.body(concern, 'Concern')\n        response = self._send(http_method='POST',\n                              location_id='784910cd-254a-494d-898b-0728549b2f10',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Concern', response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_question(self, question, publisher_name, extension_name, question_id):\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        if question_id is not None:\n            route_values['questionId'] = self._serialize.url('question_id', question_id, 'long')\n        content = self._serialize.body(question, 'Question')\n        response = self._send(http_method='PATCH',\n                              location_id='6d1d9741-eca8-4701-a3a5-235afc82dfa4',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Question', response)", "response": "Update an existing question for an extension."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a response for an extension.", "response": "def delete_response(self, publisher_name, extension_name, question_id, response_id):\n        \"\"\"DeleteResponse.\n        [Preview API] Deletes a response for an extension. (soft delete)\n        :param str publisher_name: Name of the publisher who published the extension.\n        :param str extension_name: Name of the extension.\n        :param long question_id: Identifies the question whose response is to be deleted.\n        :param long response_id: Identifies the response to be deleted.\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        if question_id is not None:\n            route_values['questionId'] = self._serialize.url('question_id', question_id, 'long')\n        if response_id is not None:\n            route_values['responseId'] = self._serialize.url('response_id', response_id, 'long')\n        self._send(http_method='DELETE',\n                   location_id='7f8ae5e0-46b0-438f-b2e8-13e8513517bd',\n                   version='5.1-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the extension reports for a given date.", "response": "def get_extension_reports(self, publisher_name, extension_name, days=None, count=None, after_date=None):\n        \"\"\"GetExtensionReports.\n        [Preview API] Returns extension reports\n        :param str publisher_name: Name of the publisher who published the extension\n        :param str extension_name: Name of the extension\n        :param int days: Last n days report. If afterDate and days are specified, days will take priority\n        :param int count: Number of events to be returned\n        :param datetime after_date: Use if you want to fetch events newer than the specified date\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        query_parameters = {}\n        if days is not None:\n            query_parameters['days'] = self._serialize.query('days', days, 'int')\n        if count is not None:\n            query_parameters['count'] = self._serialize.query('count', count, 'int')\n        if after_date is not None:\n            query_parameters['afterDate'] = self._serialize.query('after_date', after_date, 'iso-8601')\n        response = self._send(http_method='GET',\n                              location_id='79e0c74f-157f-437e-845f-74fbb4121d4c',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('object', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the reviews associated with an extension.", "response": "def get_reviews(self, publisher_name, extension_name, count=None, filter_options=None, before_date=None, after_date=None):\n        \"\"\"GetReviews.\n        [Preview API] Returns a list of reviews associated with an extension\n        :param str publisher_name: Name of the publisher who published the extension\n        :param str extension_name: Name of the extension\n        :param int count: Number of reviews to retrieve (defaults to 5)\n        :param str filter_options: FilterOptions to filter out empty reviews etcetera, defaults to none\n        :param datetime before_date: Use if you want to fetch reviews older than the specified date, defaults to null\n        :param datetime after_date: Use if you want to fetch reviews newer than the specified date, defaults to null\n        :rtype: :class:`<ReviewsResult> <azure.devops.v5_1.gallery.models.ReviewsResult>`\n        \"\"\"\n        route_values = {}\n        if publisher_name is not None:\n            route_values['publisherName'] = self._serialize.url('publisher_name', publisher_name, 'str')\n        if extension_name is not None:\n            route_values['extensionName'] = self._serialize.url('extension_name', extension_name, 'str')\n        query_parameters = {}\n        if count is not None:\n            query_parameters['count'] = self._serialize.query('count', count, 'int')\n        if filter_options is not None:\n            query_parameters['filterOptions'] = self._serialize.query('filter_options', filter_options, 'str')\n        if before_date is not None:\n            query_parameters['beforeDate'] = self._serialize.query('before_date', before_date, 'iso-8601')\n        if after_date is not None:\n            query_parameters['afterDate'] = self._serialize.query('after_date', after_date, 'iso-8601')\n        response = self._send(http_method='GET',\n                              location_id='5b3f819f-f247-42ad-8c00-dd9ab9ab246d',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ReviewsResult', response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_review(self, pub_name, ext_name, review_id):\n        route_values = {}\n        if pub_name is not None:\n            route_values['pubName'] = self._serialize.url('pub_name', pub_name, 'str')\n        if ext_name is not None:\n            route_values['extName'] = self._serialize.url('ext_name', ext_name, 'str')\n        if review_id is not None:\n            route_values['reviewId'] = self._serialize.url('review_id', review_id, 'long')\n        self._send(http_method='DELETE',\n                   location_id='e6e85b9d-aa70-40e6-aa28-d0fbf40b91a3',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "Delete a review from the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_review(self, review_patch, pub_name, ext_name, review_id):\n        route_values = {}\n        if pub_name is not None:\n            route_values['pubName'] = self._serialize.url('pub_name', pub_name, 'str')\n        if ext_name is not None:\n            route_values['extName'] = self._serialize.url('ext_name', ext_name, 'str')\n        if review_id is not None:\n            route_values['reviewId'] = self._serialize.url('review_id', review_id, 'long')\n        content = self._serialize.body(review_patch, 'ReviewPatch')\n        response = self._send(http_method='PATCH',\n                              location_id='e6e85b9d-aa70-40e6-aa28-d0fbf40b91a3',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ReviewPatch', response)", "response": "Update the contents of the specified ReviewPatch object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new category.", "response": "def create_category(self, category):\n        \"\"\"CreateCategory.\n        [Preview API]\n        :param :class:`<ExtensionCategory> <azure.devops.v5_1.gallery.models.ExtensionCategory>` category:\n        :rtype: :class:`<ExtensionCategory> <azure.devops.v5_1.gallery.models.ExtensionCategory>`\n        \"\"\"\n        content = self._serialize.body(category, 'ExtensionCategory')\n        response = self._send(http_method='POST',\n                              location_id='476531a3-7024-4516-a76a-ed64d3008ad6',\n                              version='5.1-preview.1',\n                              content=content)\n        return self._deserialize('ExtensionCategory', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a new key in the cache.", "response": "def generate_key(self, key_type, expire_current_seconds=None):\n        \"\"\"GenerateKey.\n        [Preview API]\n        :param str key_type:\n        :param int expire_current_seconds:\n        \"\"\"\n        route_values = {}\n        if key_type is not None:\n            route_values['keyType'] = self._serialize.url('key_type', key_type, 'str')\n        query_parameters = {}\n        if expire_current_seconds is not None:\n            query_parameters['expireCurrentSeconds'] = self._serialize.query('expire_current_seconds', expire_current_seconds, 'int')\n        self._send(http_method='POST',\n                   location_id='92ed5cf4-c38b-465a-9059-2f2fb7c624b5',\n                   version='5.1-preview.1',\n                   route_values=route_values,\n                   query_parameters=query_parameters)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a group by its descriptor.", "response": "def get_group(self, group_descriptor):\n        \"\"\"GetGroup.\n        [Preview API] Get a group by its descriptor.\n        :param str group_descriptor: The descriptor of the desired graph group.\n        :rtype: :class:`<GraphGroup> <azure.devops.v5_0.graph.models.GraphGroup>`\n        \"\"\"\n        route_values = {}\n        if group_descriptor is not None:\n            route_values['groupDescriptor'] = self._serialize.url('group_descriptor', group_descriptor, 'str')\n        response = self._send(http_method='GET',\n                              location_id='ebbe6af8-0b91-4c13-8cf1-777c14858188',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('GraphGroup', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist Groups in the current scope.", "response": "def list_groups(self, scope_descriptor=None, subject_types=None, continuation_token=None):\n        \"\"\"ListGroups.\n        [Preview API] Gets a list of all groups in the current scope (usually organization or account).\n        :param str scope_descriptor: Specify a non-default scope (collection, project) to search for groups.\n        :param [str] subject_types: A comma separated list of user subject subtypes to reduce the retrieved results, e.g. Microsoft.IdentityModel.Claims.ClaimsIdentity\n        :param str continuation_token: An opaque data blob that allows the next page of data to resume immediately after where the previous page ended. The only reliable way to know if there is more data left is the presence of a continuation token.\n        :rtype: :class:`<PagedGraphGroups> <azure.devops.v5_0.graph.models.PagedGraphGroups>`\n        \"\"\"\n        query_parameters = {}\n        if scope_descriptor is not None:\n            query_parameters['scopeDescriptor'] = self._serialize.query('scope_descriptor', scope_descriptor, 'str')\n        if subject_types is not None:\n            subject_types = \",\".join(subject_types)\n            query_parameters['subjectTypes'] = self._serialize.query('subject_types', subject_types, 'str')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        response = self._send(http_method='GET',\n                              location_id='ebbe6af8-0b91-4c13-8cf1-777c14858188',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        response_object = models.PagedGraphGroups()\n        response_object.graph_groups = self._deserialize('[GraphGroup]', self._unwrap_collection(response))\n        response_object.continuation_token = response.headers.get('X-MS-ContinuationToken')\n        return response_object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the properties of a VSTS group.", "response": "def update_group(self, group_descriptor, patch_document):\n        \"\"\"UpdateGroup.\n        [Preview API] Update the properties of a VSTS group.\n        :param str group_descriptor: The descriptor of the group to modify.\n        :param :class:`<[JsonPatchOperation]> <azure.devops.v5_0.graph.models.[JsonPatchOperation]>` patch_document: The JSON+Patch document containing the fields to alter.\n        :rtype: :class:`<GraphGroup> <azure.devops.v5_0.graph.models.GraphGroup>`\n        \"\"\"\n        route_values = {}\n        if group_descriptor is not None:\n            route_values['groupDescriptor'] = self._serialize.url('group_descriptor', group_descriptor, 'str')\n        content = self._serialize.body(patch_document, '[JsonPatchOperation]')\n        response = self._send(http_method='PATCH',\n                              location_id='ebbe6af8-0b91-4c13-8cf1-777c14858188',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content,\n                              media_type='application/json-patch+json')\n        return self._deserialize('GraphGroup', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the state of a subject.", "response": "def get_membership_state(self, subject_descriptor):\n        \"\"\"GetMembershipState.\n        [Preview API] Check whether a subject is active or inactive.\n        :param str subject_descriptor: Descriptor of the subject (user, group, scope, etc.) to check state of\n        :rtype: :class:`<GraphMembershipState> <azure.devops.v5_0.graph.models.GraphMembershipState>`\n        \"\"\"\n        route_values = {}\n        if subject_descriptor is not None:\n            route_values['subjectDescriptor'] = self._serialize.url('subject_descriptor', subject_descriptor, 'str')\n        response = self._send(http_method='GET',\n                              location_id='1ffe5c94-1144-4191-907b-d0211cad36a8',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('GraphMembershipState', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the list of work item tracking outbound artifact link types.", "response": "def get_work_artifact_link_types(self):\n        \"\"\"GetWorkArtifactLinkTypes.\n        [Preview API] Get the list of work item tracking outbound artifact link types.\n        :rtype: [WorkArtifactLink]\n        \"\"\"\n        response = self._send(http_method='GET',\n                              location_id='1a31de40-e318-41cd-a6c6-881077df52e3',\n                              version='5.0-preview.1')\n        return self._deserialize('[WorkArtifactLink]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query_work_items_for_artifact_uris(self, artifact_uri_query, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(artifact_uri_query, 'ArtifactUriQuery')\n        response = self._send(http_method='POST',\n                              location_id='a9a9aa7a-8c09-44d3-ad1b-46e855c1e3d3',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ArtifactUriQueryResult', response)", "response": "Query work items linked to a given list of artifact URIs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_attachment(self, upload_stream, project=None, file_name=None, upload_type=None, area_path=None, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if file_name is not None:\n            query_parameters['fileName'] = self._serialize.query('file_name', file_name, 'str')\n        if upload_type is not None:\n            query_parameters['uploadType'] = self._serialize.query('upload_type', upload_type, 'str')\n        if area_path is not None:\n            query_parameters['areaPath'] = self._serialize.query('area_path', area_path, 'str')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        content = self._client.stream_upload(upload_stream, callback=callback)\n        response = self._send(http_method='POST',\n                              location_id='e07b5fa4-1499-494d-a496-64b860fd64ff',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content,\n                              media_type='application/octet-stream')\n        return self._deserialize('AttachmentReference', response)", "response": "Uploads an attachment to the Cloud Storage account."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete an existing classification node.", "response": "def delete_classification_node(self, project, structure_group, path=None, reclassify_id=None):\n        \"\"\"DeleteClassificationNode.\n        Delete an existing classification node.\n        :param str project: Project ID or project name\n        :param TreeStructureGroup structure_group: Structure group of the classification node, area or iteration.\n        :param str path: Path of the classification node.\n        :param int reclassify_id: Id of the target classification node for reclassification.\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if structure_group is not None:\n            route_values['structureGroup'] = self._serialize.url('structure_group', structure_group, 'TreeStructureGroup')\n        if path is not None:\n            route_values['path'] = self._serialize.url('path', path, 'str')\n        query_parameters = {}\n        if reclassify_id is not None:\n            query_parameters['$reclassifyId'] = self._serialize.query('reclassify_id', reclassify_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='5a172953-1b41-49d3-840a-33f79c3ce89f',\n                   version='5.0',\n                   route_values=route_values,\n                   query_parameters=query_parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the classification node for a given path.", "response": "def get_classification_node(self, project, structure_group, path=None, depth=None):\n        \"\"\"GetClassificationNode.\n        Gets the classification node for a given node path.\n        :param str project: Project ID or project name\n        :param TreeStructureGroup structure_group: Structure group of the classification node, area or iteration.\n        :param str path: Path of the classification node.\n        :param int depth: Depth of children to fetch.\n        :rtype: :class:`<WorkItemClassificationNode> <azure.devops.v5_0.work_item_tracking.models.WorkItemClassificationNode>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if structure_group is not None:\n            route_values['structureGroup'] = self._serialize.url('structure_group', structure_group, 'TreeStructureGroup')\n        if path is not None:\n            route_values['path'] = self._serialize.url('path', path, 'str')\n        query_parameters = {}\n        if depth is not None:\n            query_parameters['$depth'] = self._serialize.query('depth', depth, 'int')\n        response = self._send(http_method='GET',\n                              location_id='5a172953-1b41-49d3-840a-33f79c3ce89f',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('WorkItemClassificationNode', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_classification_node(self, posted_node, project, structure_group, path=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if structure_group is not None:\n            route_values['structureGroup'] = self._serialize.url('structure_group', structure_group, 'TreeStructureGroup')\n        if path is not None:\n            route_values['path'] = self._serialize.url('path', path, 'str')\n        content = self._serialize.body(posted_node, 'WorkItemClassificationNode')\n        response = self._send(http_method='PATCH',\n                              location_id='5a172953-1b41-49d3-840a-33f79c3ce89f',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WorkItemClassificationNode', response)", "response": "Update an existing classification node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_comments(self, id, project=None, from_revision=None, top=None, order=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        query_parameters = {}\n        if from_revision is not None:\n            query_parameters['fromRevision'] = self._serialize.query('from_revision', from_revision, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if order is not None:\n            query_parameters['order'] = self._serialize.query('order', order, 'str')\n        response = self._send(http_method='GET',\n                              location_id='19335ae7-22f7-4308-93d8-261f9384b7cf',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('WorkItemComments', response)", "response": "Get comments for a specific work item from a specified revision."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_fields(self, project=None, expand=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='b51fd764-e5c2-4b9b-aaf7-3395cf4bdd94',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[WorkItemField]', self._unwrap_collection(response))", "response": "GetFields.\n        Returns information for all fields.\n        :param str project: Project ID or project name\n        :param str expand: Use ExtensionFields to include extension fields, otherwise exclude them. Unless the feature flag for this parameter is enabled, extension fields are always included.\n        :rtype: [WorkItemField]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_query(self, posted_query, project, query):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if query is not None:\n            route_values['query'] = self._serialize.url('query', query, 'str')\n        content = self._serialize.body(posted_query, 'QueryHierarchyItem')\n        response = self._send(http_method='POST',\n                              location_id='a67d190c-c41f-424b-814d-0e906f659301',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('QueryHierarchyItem', response)", "response": "Creates a query or moves it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndestroy the specified work item permanently from the Recycle Bin. This action can not be undone.", "response": "def destroy_work_item(self, id, project=None):\n        \"\"\"DestroyWorkItem.\n        Destroys the specified work item permanently from the Recycle Bin. This action can not be undone.\n        :param int id: ID of the work item to be destroyed permanently\n        :param str project: Project ID or project name\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='b70d8d39-926c-465e-b927-b1bf0e5ca0e0',\n                   version='5.0',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the list of the IDs and URLs of the deleted work items in the Recycle Bin.", "response": "def get_deleted_work_item_shallow_references(self, project=None):\n        \"\"\"GetDeletedWorkItemShallowReferences.\n        Gets a list of the IDs and the URLs of the deleted the work items in the Recycle Bin.\n        :param str project: Project ID or project name\n        :rtype: [WorkItemDeleteShallowReference]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        response = self._send(http_method='GET',\n                              location_id='b70d8d39-926c-465e-b927-b1bf0e5ca0e0',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('[WorkItemDeleteShallowReference]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_templates(self, team_context, workitemtypename=None):\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        query_parameters = {}\n        if workitemtypename is not None:\n            query_parameters['workitemtypename'] = self._serialize.query('workitemtypename', workitemtypename, 'str')\n        response = self._send(http_method='GET',\n                              location_id='6a90345f-a676-4969-afce-8e163e1d5642',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[WorkItemTemplateReference]', self._unwrap_collection(response))", "response": "Get the templates for a given workitem type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the template with given id.", "response": "def delete_template(self, team_context, template_id):\n        \"\"\"DeleteTemplate.\n        [Preview API] Deletes the template with given id\n        :param :class:`<TeamContext> <azure.devops.v5_0.work_item_tracking.models.TeamContext>` team_context: The team context for the operation\n        :param str template_id: Template id\n        \"\"\"\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if template_id is not None:\n            route_values['templateId'] = self._serialize.url('template_id', template_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='fb10264a-8836-48a0-8033-1b0ccd2748d5',\n                   version='5.0-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery by the ID.", "response": "def query_by_id(self, id, team_context=None, time_precision=None, top=None):\n        \"\"\"QueryById.\n        Gets the results of the query given the query ID.\n        :param str id: The query ID.\n        :param :class:`<TeamContext> <azure.devops.v5_0.work_item_tracking.models.TeamContext>` team_context: The team context for the operation\n        :param bool time_precision: Whether or not to use time precision.\n        :param int top: The max number of results to return.\n        :rtype: :class:`<WorkItemQueryResult> <azure.devops.v5_0.work_item_tracking.models.WorkItemQueryResult>`\n        \"\"\"\n        project = None\n        team = None\n        if team_context is not None:\n            if team_context.project_id:\n                project = team_context.project_id\n            else:\n                project = team_context.project\n            if team_context.team_id:\n                team = team_context.team_id\n            else:\n                team = team_context.team\n\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'string')\n        if team is not None:\n            route_values['team'] = self._serialize.url('team', team, 'string')\n        if id is not None:\n            route_values['id'] = self._serialize.url('id', id, 'str')\n        query_parameters = {}\n        if time_precision is not None:\n            query_parameters['timePrecision'] = self._serialize.query('time_precision', time_precision, 'bool')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        response = self._send(http_method='GET',\n                              location_id='a02355f5-5f8a-4671-8e32-369d23aac83d',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('WorkItemQueryResult', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a work item icon given the friendly name and icon color.", "response": "def get_work_item_icon_json(self, icon, color=None, v=None):\n        \"\"\"GetWorkItemIconJson.\n        [Preview API] Get a work item icon given the friendly name and icon color.\n        :param str icon: The name of the icon\n        :param str color: The 6-digit hex color for the icon\n        :param int v: The version of the icon (used only for cache invalidation)\n        :rtype: :class:`<WorkItemIcon> <azure.devops.v5_0.work_item_tracking.models.WorkItemIcon>`\n        \"\"\"\n        route_values = {}\n        if icon is not None:\n            route_values['icon'] = self._serialize.url('icon', icon, 'str')\n        query_parameters = {}\n        if color is not None:\n            query_parameters['color'] = self._serialize.query('color', color, 'str')\n        if v is not None:\n            query_parameters['v'] = self._serialize.query('v', v, 'int')\n        response = self._send(http_method='GET',\n                              location_id='4e1eb4a5-1970-4228-a682-ec48eb2dca30',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('WorkItemIcon', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the ReportingRevisions Batch for a specific set of fields and types.", "response": "def read_reporting_revisions_get(self, project=None, fields=None, types=None, continuation_token=None, start_date_time=None, include_identity_ref=None, include_deleted=None, include_tag_ref=None, include_latest_only=None, expand=None, include_discussion_changes_only=None, max_page_size=None):\n        \"\"\"ReadReportingRevisionsGet.\n        Get a batch of work item revisions with the option of including deleted items\n        :param str project: Project ID or project name\n        :param [str] fields: A list of fields to return in work item revisions. Omit this parameter to get all reportable fields.\n        :param [str] types: A list of types to filter the results to specific work item types. Omit this parameter to get work item revisions of all work item types.\n        :param str continuation_token: Specifies the watermark to start the batch from. Omit this parameter to get the first batch of revisions.\n        :param datetime start_date_time: Date/time to use as a starting point for revisions, all revisions will occur after this date/time. Cannot be used in conjunction with 'watermark' parameter.\n        :param bool include_identity_ref: Return an identity reference instead of a string value for identity fields.\n        :param bool include_deleted: Specify if the deleted item should be returned.\n        :param bool include_tag_ref: Specify if the tag objects should be returned for System.Tags field.\n        :param bool include_latest_only: Return only the latest revisions of work items, skipping all historical revisions\n        :param str expand: Return all the fields in work item revisions, including long text fields which are not returned by default\n        :param bool include_discussion_changes_only: Return only the those revisions of work items, where only history field was changed\n        :param int max_page_size: The maximum number of results to return in this batch\n        :rtype: :class:`<ReportingWorkItemRevisionsBatch> <azure.devops.v5_0.work_item_tracking.models.ReportingWorkItemRevisionsBatch>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if fields is not None:\n            fields = \",\".join(fields)\n            query_parameters['fields'] = self._serialize.query('fields', fields, 'str')\n        if types is not None:\n            types = \",\".join(types)\n            query_parameters['types'] = self._serialize.query('types', types, 'str')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        if start_date_time is not None:\n            query_parameters['startDateTime'] = self._serialize.query('start_date_time', start_date_time, 'iso-8601')\n        if include_identity_ref is not None:\n            query_parameters['includeIdentityRef'] = self._serialize.query('include_identity_ref', include_identity_ref, 'bool')\n        if include_deleted is not None:\n            query_parameters['includeDeleted'] = self._serialize.query('include_deleted', include_deleted, 'bool')\n        if include_tag_ref is not None:\n            query_parameters['includeTagRef'] = self._serialize.query('include_tag_ref', include_tag_ref, 'bool')\n        if include_latest_only is not None:\n            query_parameters['includeLatestOnly'] = self._serialize.query('include_latest_only', include_latest_only, 'bool')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        if include_discussion_changes_only is not None:\n            query_parameters['includeDiscussionChangesOnly'] = self._serialize.query('include_discussion_changes_only', include_discussion_changes_only, 'bool')\n        if max_page_size is not None:\n            query_parameters['$maxPageSize'] = self._serialize.query('max_page_size', max_page_size, 'int')\n        response = self._send(http_method='GET',\n                              location_id='f828fe59-dd87-495d-a17c-7a8d6211ca6c',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ReportingWorkItemRevisionsBatch', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a list of work items for a given project.", "response": "def get_work_items_batch(self, work_item_get_request, project=None):\n        \"\"\"GetWorkItemsBatch.\n        Gets work items for a list of work item ids (Maximum 200)\n        :param :class:`<WorkItemBatchGetRequest> <azure.devops.v5_0.work_item_tracking.models.WorkItemBatchGetRequest>` work_item_get_request:\n        :param str project: Project ID or project name\n        :rtype: [WorkItem]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(work_item_get_request, 'WorkItemBatchGetRequest')\n        response = self._send(http_method='POST',\n                              location_id='908509b6-4248-4475-a1cd-829139ba419f',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[WorkItem]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_project_avatar(self, project_id):\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='54b2a2a0-859b-4d05-827c-ec4c862f641a',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "Remove the avatar for the project."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_project_avatar(self, avatar_blob, project_id):\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        content = self._serialize.body(avatar_blob, 'ProjectAvatar')\n        self._send(http_method='PUT',\n                   location_id='54b2a2a0-859b-4d05-827c-ec4c862f641a',\n                   version='5.1-preview.1',\n                   route_values=route_values,\n                   content=content)", "response": "Set the avatar for the project."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_team_members_with_extended_properties(self, project_id, team_id, top=None, skip=None):\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        if team_id is not None:\n            route_values['teamId'] = self._serialize.url('team_id', team_id, 'str')\n        query_parameters = {}\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        response = self._send(http_method='GET',\n                              location_id='294c494c-2600-4d7e-b76c-3dd50c3c95be',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TeamMember]', self._unwrap_collection(response))", "response": "Get a list of members for a specific team."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a process by ID.", "response": "def get_process_by_id(self, process_id):\n        \"\"\"GetProcessById.\n        [Preview API] Get a process by ID.\n        :param str process_id: ID for a process.\n        :rtype: :class:`<Process> <azure.devops.v5_1.core.models.Process>`\n        \"\"\"\n        route_values = {}\n        if process_id is not None:\n            route_values['processId'] = self._serialize.url('process_id', process_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='93878975-88c5-4e6a-8abb-7ddd77a8a7d8',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('Process', response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_project_collection(self, collection_id):\n        route_values = {}\n        if collection_id is not None:\n            route_values['collectionId'] = self._serialize.url('collection_id', collection_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='8031090f-ef1d-4af6-85fc-698cd75d42bf',\n                              version='5.1-preview.2',\n                              route_values=route_values)\n        return self._deserialize('TeamProjectCollection', response)", "response": "Get project collection with the specified id or name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nqueue a project to be deleted.", "response": "def queue_delete_project(self, project_id):\n        \"\"\"QueueDeleteProject.\n        [Preview API] Queues a project to be deleted. Use the [GetOperation](../../operations/operations/get) to periodically check for delete project status.\n        :param str project_id: The project id of the project to delete.\n        :rtype: :class:`<OperationReference> <azure.devops.v5_1.core.models.OperationReference>`\n        \"\"\"\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        response = self._send(http_method='DELETE',\n                              location_id='603fe2ac-9723-48b9-88ad-09305aa6c6e1',\n                              version='5.1-preview.4',\n                              route_values=route_values)\n        return self._deserialize('OperationReference', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate an existing project s name abbreviation description or restore a project.", "response": "def update_project(self, project_update, project_id):\n        \"\"\"UpdateProject.\n        [Preview API] Update an existing project's name, abbreviation, description, or restore a project.\n        :param :class:`<TeamProject> <azure.devops.v5_1.core.models.TeamProject>` project_update: The updates for the project. The state must be set to wellFormed to restore the project.\n        :param str project_id: The project id of the project to update.\n        :rtype: :class:`<OperationReference> <azure.devops.v5_1.core.models.OperationReference>`\n        \"\"\"\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        content = self._serialize.body(project_update, 'TeamProject')\n        response = self._send(http_method='PATCH',\n                              location_id='603fe2ac-9723-48b9-88ad-09305aa6c6e1',\n                              version='5.1-preview.4',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('OperationReference', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a collection of team project properties.", "response": "def get_project_properties(self, project_id, keys=None):\n        \"\"\"GetProjectProperties.\n        [Preview API] Get a collection of team project properties.\n        :param str project_id: The team project ID.\n        :param [str] keys: A comma-delimited string of team project property names. Wildcard characters (\"?\" and \"*\") are supported. If no key is specified, all properties will be returned.\n        :rtype: [ProjectProperty]\n        \"\"\"\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        query_parameters = {}\n        if keys is not None:\n            keys = \",\".join(keys)\n            query_parameters['keys'] = self._serialize.query('keys', keys, 'str')\n        response = self._send(http_method='GET',\n                              location_id='4976a71a-4487-49aa-8aab-a1eda469037a',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[ProjectProperty]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the properties of the team project.", "response": "def set_project_properties(self, project_id, patch_document):\n        \"\"\"SetProjectProperties.\n        [Preview API] Create, update, and delete team project properties.\n        :param str project_id: The team project ID.\n        :param :class:`<[JsonPatchOperation]> <azure.devops.v5_1.core.models.[JsonPatchOperation]>` patch_document: A JSON Patch document that represents an array of property operations. See RFC 6902 for more details on JSON Patch. The accepted operation verbs are Add and Remove, where Add is used for both creating and updating properties. The path consists of a forward slash and a property name.\n        \"\"\"\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        content = self._serialize.body(patch_document, '[JsonPatchOperation]')\n        self._send(http_method='PATCH',\n                   location_id='4976a71a-4487-49aa-8aab-a1eda469037a',\n                   version='5.1-preview.1',\n                   route_values=route_values,\n                   content=content,\n                   media_type='application/json-patch+json')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a team in a team project.", "response": "def create_team(self, team, project_id):\n        \"\"\"CreateTeam.\n        [Preview API] Create a team in a team project.\n        :param :class:`<WebApiTeam> <azure.devops.v5_1.core.models.WebApiTeam>` team: The team data used to create the team.\n        :param str project_id: The name or ID (GUID) of the team project in which to create the team.\n        :rtype: :class:`<WebApiTeam> <azure.devops.v5_1.core.models.WebApiTeam>`\n        \"\"\"\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        content = self._serialize.body(team, 'WebApiTeam')\n        response = self._send(http_method='POST',\n                              location_id='d30a3dd1-f8ba-442a-b86a-bd0c0c383e59',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WebApiTeam', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_team(self, project_id, team_id):\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        if team_id is not None:\n            route_values['teamId'] = self._serialize.url('team_id', team_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='d30a3dd1-f8ba-442a-b86a-bd0c0c383e59',\n                   version='5.1-preview.2',\n                   route_values=route_values)", "response": "DeleteTeam.\n        [Preview API] Delete a team.\n        :param str project_id: The name or ID (GUID) of the team project containing the team to delete.\n        :param str team_id: The name of ID of the team to delete."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a specific team.", "response": "def get_team(self, project_id, team_id):\n        \"\"\"GetTeam.\n        [Preview API] Get a specific team.\n        :param str project_id: The name or ID (GUID) of the team project containing the team.\n        :param str team_id: The name or ID (GUID) of the team.\n        :rtype: :class:`<WebApiTeam> <azure.devops.v5_1.core.models.WebApiTeam>`\n        \"\"\"\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        if team_id is not None:\n            route_values['teamId'] = self._serialize.url('team_id', team_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='d30a3dd1-f8ba-442a-b86a-bd0c0c383e59',\n                              version='5.1-preview.2',\n                              route_values=route_values)\n        return self._deserialize('WebApiTeam', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating a team s name and description.", "response": "def update_team(self, team_data, project_id, team_id):\n        \"\"\"UpdateTeam.\n        [Preview API] Update a team's name and/or description.\n        :param :class:`<WebApiTeam> <azure.devops.v5_1.core.models.WebApiTeam>` team_data:\n        :param str project_id: The name or ID (GUID) of the team project containing the team to update.\n        :param str team_id: The name of ID of the team to update.\n        :rtype: :class:`<WebApiTeam> <azure.devops.v5_1.core.models.WebApiTeam>`\n        \"\"\"\n        route_values = {}\n        if project_id is not None:\n            route_values['projectId'] = self._serialize.url('project_id', project_id, 'str')\n        if team_id is not None:\n            route_values['teamId'] = self._serialize.url('team_id', team_id, 'str')\n        content = self._serialize.body(team_data, 'WebApiTeam')\n        response = self._send(http_method='PATCH',\n                              location_id='d30a3dd1-f8ba-442a-b86a-bd0c0c383e59',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WebApiTeam', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new package to the feed.", "response": "def add_package(self, metadata, feed_id, package_name, package_version):\n        \"\"\"AddPackage.\n        [Preview API]\n        :param :class:`<UPackPackagePushMetadata> <azure.devops.v5_0.upack_packaging.models.UPackPackagePushMetadata>` metadata:\n        :param str feed_id:\n        :param str package_name:\n        :param str package_version:\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_name is not None:\n            route_values['packageName'] = self._serialize.url('package_name', package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        content = self._serialize.body(metadata, 'UPackPackagePushMetadata')\n        self._send(http_method='PUT',\n                   location_id='4cdb2ced-0758-4651-8032-010f070dd7e5',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   content=content)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the package metadata for a given feed.", "response": "def get_package_metadata(self, feed_id, package_name, package_version, intent=None):\n        \"\"\"GetPackageMetadata.\n        [Preview API]\n        :param str feed_id:\n        :param str package_name:\n        :param str package_version:\n        :param str intent:\n        :rtype: :class:`<UPackPackageMetadata> <azure.devops.v5_0.upack_packaging.models.UPackPackageMetadata>`\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_name is not None:\n            route_values['packageName'] = self._serialize.url('package_name', package_name, 'str')\n        if package_version is not None:\n            route_values['packageVersion'] = self._serialize.url('package_version', package_version, 'str')\n        query_parameters = {}\n        if intent is not None:\n            query_parameters['intent'] = self._serialize.query('intent', intent, 'str')\n        response = self._send(http_method='GET',\n                              location_id='4cdb2ced-0758-4651-8032-010f070dd7e5',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('UPackPackageMetadata', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the status of an approval.", "response": "def update_release_approval(self, approval, project, approval_id):\n        \"\"\"UpdateReleaseApproval.\n        Update status of an approval\n        :param :class:`<ReleaseApproval> <azure.devops.v5_0.release.models.ReleaseApproval>` approval: ReleaseApproval object having status, approver and comments.\n        :param str project: Project ID or project name\n        :param int approval_id: Id of the approval.\n        :rtype: :class:`<ReleaseApproval> <azure.devops.v5_0.release.models.ReleaseApproval>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if approval_id is not None:\n            route_values['approvalId'] = self._serialize.url('approval_id', approval_id, 'int')\n        content = self._serialize.body(approval, 'ReleaseApproval')\n        response = self._send(http_method='PATCH',\n                              location_id='9328e074-59fb-465a-89d9-b09c82ee5109',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ReleaseApproval', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_release_task_attachment_content(self, project, release_id, environment_id, attempt_id, plan_id, timeline_id, record_id, type, name, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        if environment_id is not None:\n            route_values['environmentId'] = self._serialize.url('environment_id', environment_id, 'int')\n        if attempt_id is not None:\n            route_values['attemptId'] = self._serialize.url('attempt_id', attempt_id, 'int')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        if timeline_id is not None:\n            route_values['timelineId'] = self._serialize.url('timeline_id', timeline_id, 'str')\n        if record_id is not None:\n            route_values['recordId'] = self._serialize.url('record_id', record_id, 'str')\n        if type is not None:\n            route_values['type'] = self._serialize.url('type', type, 'str')\n        if name is not None:\n            route_values['name'] = self._serialize.url('name', name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='60b86efb-7b8c-4853-8f9f-aa142b77b479',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get release task attachment content."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_release_task_attachments(self, project, release_id, environment_id, attempt_id, plan_id, type):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        if environment_id is not None:\n            route_values['environmentId'] = self._serialize.url('environment_id', environment_id, 'int')\n        if attempt_id is not None:\n            route_values['attemptId'] = self._serialize.url('attempt_id', attempt_id, 'int')\n        if plan_id is not None:\n            route_values['planId'] = self._serialize.url('plan_id', plan_id, 'str')\n        if type is not None:\n            route_values['type'] = self._serialize.url('type', type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='a4d06688-0dfa-4895-82a5-f43ec9452306',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('[ReleaseTaskAttachment]', self._unwrap_collection(response))", "response": "Get release task attachments."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a release definition.", "response": "def delete_release_definition(self, project, definition_id, comment=None, force_delete=None):\n        \"\"\"DeleteReleaseDefinition.\n        Delete a release definition.\n        :param str project: Project ID or project name\n        :param int definition_id: Id of the release definition.\n        :param str comment: Comment for deleting a release definition.\n        :param bool force_delete: 'true' to automatically cancel any in-progress release deployments and proceed with release definition deletion . Default is 'false'.\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        query_parameters = {}\n        if comment is not None:\n            query_parameters['comment'] = self._serialize.query('comment', comment, 'str')\n        if force_delete is not None:\n            query_parameters['forceDelete'] = self._serialize.query('force_delete', force_delete, 'bool')\n        self._send(http_method='DELETE',\n                   location_id='d8f96f24-8ea7-4cb6-baab-2df8fc515665',\n                   version='5.0',\n                   route_values=route_values,\n                   query_parameters=query_parameters)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of deployments for a given definition.", "response": "def get_deployments(self, project, definition_id=None, definition_environment_id=None, created_by=None, min_modified_time=None, max_modified_time=None, deployment_status=None, operation_status=None, latest_attempts_only=None, query_order=None, top=None, continuation_token=None, created_for=None, min_started_time=None, max_started_time=None, source_branch=None):\n        \"\"\"GetDeployments.\n        :param str project: Project ID or project name\n        :param int definition_id:\n        :param int definition_environment_id:\n        :param str created_by:\n        :param datetime min_modified_time:\n        :param datetime max_modified_time:\n        :param str deployment_status:\n        :param str operation_status:\n        :param bool latest_attempts_only:\n        :param str query_order:\n        :param int top:\n        :param int continuation_token:\n        :param str created_for:\n        :param datetime min_started_time:\n        :param datetime max_started_time:\n        :param str source_branch:\n        :rtype: [Deployment]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if definition_id is not None:\n            query_parameters['definitionId'] = self._serialize.query('definition_id', definition_id, 'int')\n        if definition_environment_id is not None:\n            query_parameters['definitionEnvironmentId'] = self._serialize.query('definition_environment_id', definition_environment_id, 'int')\n        if created_by is not None:\n            query_parameters['createdBy'] = self._serialize.query('created_by', created_by, 'str')\n        if min_modified_time is not None:\n            query_parameters['minModifiedTime'] = self._serialize.query('min_modified_time', min_modified_time, 'iso-8601')\n        if max_modified_time is not None:\n            query_parameters['maxModifiedTime'] = self._serialize.query('max_modified_time', max_modified_time, 'iso-8601')\n        if deployment_status is not None:\n            query_parameters['deploymentStatus'] = self._serialize.query('deployment_status', deployment_status, 'str')\n        if operation_status is not None:\n            query_parameters['operationStatus'] = self._serialize.query('operation_status', operation_status, 'str')\n        if latest_attempts_only is not None:\n            query_parameters['latestAttemptsOnly'] = self._serialize.query('latest_attempts_only', latest_attempts_only, 'bool')\n        if query_order is not None:\n            query_parameters['queryOrder'] = self._serialize.query('query_order', query_order, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'int')\n        if created_for is not None:\n            query_parameters['createdFor'] = self._serialize.query('created_for', created_for, 'str')\n        if min_started_time is not None:\n            query_parameters['minStartedTime'] = self._serialize.query('min_started_time', min_started_time, 'iso-8601')\n        if max_started_time is not None:\n            query_parameters['maxStartedTime'] = self._serialize.query('max_started_time', max_started_time, 'iso-8601')\n        if source_branch is not None:\n            query_parameters['sourceBranch'] = self._serialize.query('source_branch', source_branch, 'str')\n        response = self._send(http_method='GET',\n                              location_id='b005ef73-cddc-448e-9ba2-5193bf36b19f',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[Deployment]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_release_environment(self, environment_update_data, project, release_id, environment_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        if environment_id is not None:\n            route_values['environmentId'] = self._serialize.url('environment_id', environment_id, 'int')\n        content = self._serialize.body(environment_update_data, 'ReleaseEnvironmentUpdateMetadata')\n        response = self._send(http_method='PATCH',\n                              location_id='a7e426b1-03dc-48af-9dfe-c98bac612dcb',\n                              version='5.0-preview.6',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ReleaseEnvironment', response)", "response": "Update the status of a release environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_gates(self, gate_update_metadata, project, gate_step_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if gate_step_id is not None:\n            route_values['gateStepId'] = self._serialize.url('gate_step_id', gate_step_id, 'int')\n        content = self._serialize.body(gate_update_metadata, 'GateUpdateMetadata')\n        response = self._send(http_method='PATCH',\n                              location_id='2666a539-2001-4f80-bcc7-0379956749d4',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ReleaseGates', response)", "response": "Update the metadata of the Release Gates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget logs for a given release Id.", "response": "def get_logs(self, project, release_id, **kwargs):\n        \"\"\"GetLogs.\n        [Preview API] Get logs for a release Id.\n        :param str project: Project ID or project name\n        :param int release_id: Id of the release.\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='c37fbab5-214b-48e4-a55b-cb6b4f6e4038',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              accept_media_type='application/zip')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a list of releases from a release definition.", "response": "def get_releases(self, project=None, definition_id=None, definition_environment_id=None, search_text=None, created_by=None, status_filter=None, environment_status_filter=None, min_created_time=None, max_created_time=None, query_order=None, top=None, continuation_token=None, expand=None, artifact_type_id=None, source_id=None, artifact_version_id=None, source_branch_filter=None, is_deleted=None, tag_filter=None, property_filters=None, release_id_filter=None):\n        \"\"\"GetReleases.\n        Get a list of releases\n        :param str project: Project ID or project name\n        :param int definition_id: Releases from this release definition Id.\n        :param int definition_environment_id:\n        :param str search_text: Releases with names containing searchText.\n        :param str created_by: Releases created by this user.\n        :param str status_filter: Releases that have this status.\n        :param int environment_status_filter:\n        :param datetime min_created_time: Releases that were created after this time.\n        :param datetime max_created_time: Releases that were created before this time.\n        :param str query_order: Gets the results in the defined order of created date for releases. Default is descending.\n        :param int top: Number of releases to get. Default is 50.\n        :param int continuation_token: Gets the releases after the continuation token provided.\n        :param str expand: The property that should be expanded in the list of releases.\n        :param str artifact_type_id: Releases with given artifactTypeId will be returned. Values can be Build, Jenkins, GitHub, Nuget, Team Build (external), ExternalTFSBuild, Git, TFVC, ExternalTfsXamlBuild.\n        :param str source_id: Unique identifier of the artifact used. e.g. For build it would be {projectGuid}:{BuildDefinitionId}, for Jenkins it would be {JenkinsConnectionId}:{JenkinsDefinitionId}, for TfsOnPrem it would be {TfsOnPremConnectionId}:{ProjectName}:{TfsOnPremDefinitionId}. For third-party artifacts e.g. TeamCity, BitBucket you may refer 'uniqueSourceIdentifier' inside vss-extension.json https://github.com/Microsoft/vsts-rm-extensions/blob/master/Extensions.\n        :param str artifact_version_id: Releases with given artifactVersionId will be returned. E.g. in case of Build artifactType, it is buildId.\n        :param str source_branch_filter: Releases with given sourceBranchFilter will be returned.\n        :param bool is_deleted: Gets the soft deleted releases, if true.\n        :param [str] tag_filter: A comma-delimited list of tags. Only releases with these tags will be returned.\n        :param [str] property_filters: A comma-delimited list of extended properties to be retrieved. If set, the returned Releases will contain values for the specified property Ids (if they exist). If not set, properties will not be included. Note that this will not filter out any Release from results irrespective of whether it has property set or not.\n        :param [int] release_id_filter: A comma-delimited list of releases Ids. Only releases with these Ids will be returned.\n        :rtype: [Release]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if definition_id is not None:\n            query_parameters['definitionId'] = self._serialize.query('definition_id', definition_id, 'int')\n        if definition_environment_id is not None:\n            query_parameters['definitionEnvironmentId'] = self._serialize.query('definition_environment_id', definition_environment_id, 'int')\n        if search_text is not None:\n            query_parameters['searchText'] = self._serialize.query('search_text', search_text, 'str')\n        if created_by is not None:\n            query_parameters['createdBy'] = self._serialize.query('created_by', created_by, 'str')\n        if status_filter is not None:\n            query_parameters['statusFilter'] = self._serialize.query('status_filter', status_filter, 'str')\n        if environment_status_filter is not None:\n            query_parameters['environmentStatusFilter'] = self._serialize.query('environment_status_filter', environment_status_filter, 'int')\n        if min_created_time is not None:\n            query_parameters['minCreatedTime'] = self._serialize.query('min_created_time', min_created_time, 'iso-8601')\n        if max_created_time is not None:\n            query_parameters['maxCreatedTime'] = self._serialize.query('max_created_time', max_created_time, 'iso-8601')\n        if query_order is not None:\n            query_parameters['queryOrder'] = self._serialize.query('query_order', query_order, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'int')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        if artifact_type_id is not None:\n            query_parameters['artifactTypeId'] = self._serialize.query('artifact_type_id', artifact_type_id, 'str')\n        if source_id is not None:\n            query_parameters['sourceId'] = self._serialize.query('source_id', source_id, 'str')\n        if artifact_version_id is not None:\n            query_parameters['artifactVersionId'] = self._serialize.query('artifact_version_id', artifact_version_id, 'str')\n        if source_branch_filter is not None:\n            query_parameters['sourceBranchFilter'] = self._serialize.query('source_branch_filter', source_branch_filter, 'str')\n        if is_deleted is not None:\n            query_parameters['isDeleted'] = self._serialize.query('is_deleted', is_deleted, 'bool')\n        if tag_filter is not None:\n            tag_filter = \",\".join(tag_filter)\n            query_parameters['tagFilter'] = self._serialize.query('tag_filter', tag_filter, 'str')\n        if property_filters is not None:\n            property_filters = \",\".join(property_filters)\n            query_parameters['propertyFilters'] = self._serialize.query('property_filters', property_filters, 'str')\n        if release_id_filter is not None:\n            release_id_filter = \",\".join(map(str, release_id_filter))\n            query_parameters['releaseIdFilter'] = self._serialize.query('release_id_filter', release_id_filter, 'str')\n        response = self._send(http_method='GET',\n                              location_id='a166fde7-27ad-408e-ba75-703c2cc9d500',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[Release]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_release(self, project, release_id, approval_filters=None, property_filters=None, expand=None, top_gate_records=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        query_parameters = {}\n        if approval_filters is not None:\n            query_parameters['approvalFilters'] = self._serialize.query('approval_filters', approval_filters, 'str')\n        if property_filters is not None:\n            property_filters = \",\".join(property_filters)\n            query_parameters['propertyFilters'] = self._serialize.query('property_filters', property_filters, 'str')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        if top_gate_records is not None:\n            query_parameters['$topGateRecords'] = self._serialize.query('top_gate_records', top_gate_records, 'int')\n        response = self._send(http_method='GET',\n                              location_id='a166fde7-27ad-408e-ba75-703c2cc9d500',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('Release', response)", "response": "Get a specific release."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_release_revision(self, project, release_id, definition_snapshot_revision, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        query_parameters = {}\n        if definition_snapshot_revision is not None:\n            query_parameters['definitionSnapshotRevision'] = self._serialize.query('definition_snapshot_revision', definition_snapshot_revision, 'int')\n        response = self._send(http_method='GET',\n                              location_id='a166fde7-27ad-408e-ba75-703c2cc9d500',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='text/plain')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get release for a given revision number."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a single blob.", "response": "def get_blob_content(self, repository_id, sha1, project=None, download=None, file_name=None, resolve_lfs=None, **kwargs):\n        \"\"\"GetBlobContent.\n        [Preview API] Get a single blob.\n        :param str repository_id: The name or ID of the repository.\n        :param str sha1: SHA1 hash of the file. You can get the SHA1 of a file using the \"Git/Items/Get Item\" endpoint.\n        :param str project: Project ID or project name\n        :param bool download: If true, prompt for a download rather than rendering in a browser. Note: this value defaults to true if $format is zip\n        :param str file_name: Provide a fileName to use for a download.\n        :param bool resolve_lfs: If true, try to resolve a blob to its LFS contents, if it's an LFS pointer file. Only compatible with octet-stream Accept headers or $format types\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if sha1 is not None:\n            route_values['sha1'] = self._serialize.url('sha1', sha1, 'str')\n        query_parameters = {}\n        if download is not None:\n            query_parameters['download'] = self._serialize.query('download', download, 'bool')\n        if file_name is not None:\n            query_parameters['fileName'] = self._serialize.query('file_name', file_name, 'str')\n        if resolve_lfs is not None:\n            query_parameters['resolveLfs'] = self._serialize.query('resolve_lfs', resolve_lfs, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='7b28e929-2c99-405d-9c5c-6167a06e6816',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget one or more blobs in a zip file download.", "response": "def get_blobs_zip(self, blob_ids, repository_id, project=None, filename=None, **kwargs):\n        \"\"\"GetBlobsZip.\n        [Preview API] Gets one or more blobs in a zip file download.\n        :param [str] blob_ids: Blob IDs (SHA1 hashes) to be returned in the zip file.\n        :param str repository_id: The name or ID of the repository.\n        :param str project: Project ID or project name\n        :param str filename:\n        :rtype: object\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if filename is not None:\n            query_parameters['filename'] = self._serialize.query('filename', filename, 'str')\n        content = self._serialize.body(blob_ids, '[str]')\n        response = self._send(http_method='POST',\n                              location_id='7b28e929-2c99-405d-9c5c-6167a06e6816',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content,\n                              accept_media_type='application/zip')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cherry_pick_for_ref_name(self, project, repository_id, ref_name):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if ref_name is not None:\n            query_parameters['refName'] = self._serialize.query('ref_name', ref_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='033bad68-9a14-43d1-90e0-59cb8856fef6',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitCherryPick', response)", "response": "Get Cherry Pick for a specific ref name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_commit_diffs(self, repository_id, project=None, diff_common_commit=None, top=None, skip=None, base_version_descriptor=None, target_version_descriptor=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if diff_common_commit is not None:\n            query_parameters['diffCommonCommit'] = self._serialize.query('diff_common_commit', diff_common_commit, 'bool')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if base_version_descriptor is not None:\n            if base_version_descriptor.base_version_type is not None:\n                query_parameters['baseVersionType'] = base_version_descriptor.base_version_type\n            if base_version_descriptor.base_version is not None:\n                query_parameters['baseVersion'] = base_version_descriptor.base_version\n            if base_version_descriptor.base_version_options is not None:\n                query_parameters['baseVersionOptions'] = base_version_descriptor.base_version_options\n        if target_version_descriptor is not None:\n            if target_version_descriptor.target_version_type is not None:\n                query_parameters['targetVersionType'] = target_version_descriptor.target_version_type\n            if target_version_descriptor.target_version is not None:\n                query_parameters['targetVersion'] = target_version_descriptor.target_version\n            if target_version_descriptor.target_version_options is not None:\n                query_parameters['targetVersionOptions'] = target_version_descriptor.target_version_options\n        response = self._send(http_method='GET',\n                              location_id='615588d5-c0c7-4b88-88f8-e625306446e8',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitCommitDiffs', response)", "response": "Get the diff between two merge base and target commits."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_commit(self, commit_id, repository_id, project=None, change_count=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if commit_id is not None:\n            route_values['commitId'] = self._serialize.url('commit_id', commit_id, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if change_count is not None:\n            query_parameters['changeCount'] = self._serialize.query('change_count', change_count, 'int')\n        response = self._send(http_method='GET',\n                              location_id='c2570c3b-5b3f-41b8-98bf-5407bfde8d58',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitCommit', response)", "response": "Get a specific commit."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_forks(self, repository_name_or_id, collection_id, project=None, include_links=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_name_or_id is not None:\n            route_values['repositoryNameOrId'] = self._serialize.url('repository_name_or_id', repository_name_or_id, 'str')\n        if collection_id is not None:\n            route_values['collectionId'] = self._serialize.url('collection_id', collection_id, 'str')\n        query_parameters = {}\n        if include_links is not None:\n            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='158c0340-bf6f-489c-9625-d572a1480d57',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitRepositoryRef]', self._unwrap_collection(response))", "response": "Get all forks of a repository in a collection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_fork_sync_request(self, sync_params, repository_name_or_id, project=None, include_links=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_name_or_id is not None:\n            route_values['repositoryNameOrId'] = self._serialize.url('repository_name_or_id', repository_name_or_id, 'str')\n        query_parameters = {}\n        if include_links is not None:\n            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')\n        content = self._serialize.body(sync_params, 'GitForkSyncRequestParameters')\n        response = self._send(http_method='POST',\n                              location_id='1703f858-b9d1-46af-ab62-483e9e1055b5',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('GitForkSyncRequest', response)", "response": "Create a fork sync request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fork_sync_request(self, repository_name_or_id, fork_sync_operation_id, project=None, include_links=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_name_or_id is not None:\n            route_values['repositoryNameOrId'] = self._serialize.url('repository_name_or_id', repository_name_or_id, 'str')\n        if fork_sync_operation_id is not None:\n            route_values['forkSyncOperationId'] = self._serialize.url('fork_sync_operation_id', fork_sync_operation_id, 'int')\n        query_parameters = {}\n        if include_links is not None:\n            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='1703f858-b9d1-46af-ab62-483e9e1055b5',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitForkSyncRequest', response)", "response": "Get a specific fork sync operation s details."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting Items for a given repository.", "response": "def get_items(self, repository_id, project=None, scope_path=None, recursion_level=None, include_content_metadata=None, latest_processed_change=None, download=None, include_links=None, version_descriptor=None):\n        \"\"\"GetItems.\n        [Preview API] Get Item Metadata and/or Content for a collection of items. The download parameter is to indicate whether the content should be available as a download or just sent as a stream in the response. Doesn't apply to zipped content which is always returned as a download.\n        :param str repository_id: The name or ID of the repository.\n        :param str project: Project ID or project name\n        :param str scope_path: The path scope.  The default is null.\n        :param str recursion_level: The recursion level of this request. The default is 'none', no recursion.\n        :param bool include_content_metadata: Set to true to include content metadata.  Default is false.\n        :param bool latest_processed_change: Set to true to include the lastest changes.  Default is false.\n        :param bool download: Set to true to download the response as a file.  Default is false.\n        :param bool include_links: Set to true to include links to items.  Default is false.\n        :param :class:`<GitVersionDescriptor> <azure.devops.v5_1.git.models.GitVersionDescriptor>` version_descriptor: Version descriptor.  Default is the default branch for the repository.\n        :rtype: [GitItem]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if scope_path is not None:\n            query_parameters['scopePath'] = self._serialize.query('scope_path', scope_path, 'str')\n        if recursion_level is not None:\n            query_parameters['recursionLevel'] = self._serialize.query('recursion_level', recursion_level, 'str')\n        if include_content_metadata is not None:\n            query_parameters['includeContentMetadata'] = self._serialize.query('include_content_metadata', include_content_metadata, 'bool')\n        if latest_processed_change is not None:\n            query_parameters['latestProcessedChange'] = self._serialize.query('latest_processed_change', latest_processed_change, 'bool')\n        if download is not None:\n            query_parameters['download'] = self._serialize.query('download', download, 'bool')\n        if include_links is not None:\n            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')\n        if version_descriptor is not None:\n            if version_descriptor.version_type is not None:\n                query_parameters['versionDescriptor.versionType'] = version_descriptor.version_type\n            if version_descriptor.version is not None:\n                query_parameters['versionDescriptor.version'] = version_descriptor.version\n            if version_descriptor.version_options is not None:\n                query_parameters['versionDescriptor.versionOptions'] = version_descriptor.version_options\n        response = self._send(http_method='GET',\n                              location_id='fb93c0db-47ed-4a31-8c20-47552878fb44',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitItem]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a specific merge request.", "response": "def get_merge_request(self, project, repository_name_or_id, merge_operation_id, include_links=None):\n        \"\"\"GetMergeRequest.\n        [Preview API] Get a specific merge operation's details.\n        :param str project: Project ID or project name\n        :param str repository_name_or_id: The name or ID of the repository.\n        :param int merge_operation_id: OperationId of the merge request.\n        :param bool include_links: True to include links\n        :rtype: :class:`<GitMerge> <azure.devops.v5_1.git.models.GitMerge>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_name_or_id is not None:\n            route_values['repositoryNameOrId'] = self._serialize.url('repository_name_or_id', repository_name_or_id, 'str')\n        if merge_operation_id is not None:\n            route_values['mergeOperationId'] = self._serialize.url('merge_operation_id', merge_operation_id, 'int')\n        query_parameters = {}\n        if include_links is not None:\n            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='985f7ae9-844f-4906-9897-7ef41516c0e2',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitMerge', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of policy configurations by a given set of filtering criteria.", "response": "def get_policy_configurations(self, project, repository_id=None, ref_name=None, policy_type=None, top=None, continuation_token=None):\n        \"\"\"GetPolicyConfigurations.\n        [Preview API] Retrieve a list of policy configurations by a given set of scope/filtering criteria.\n        :param str project: Project ID or project name\n        :param str repository_id: The repository id.\n        :param str ref_name: The fully-qualified Git ref name (e.g. refs/heads/master).\n        :param str policy_type: The policy type filter.\n        :param int top: Maximum number of policies to return.\n        :param str continuation_token: Pass a policy configuration ID to fetch the next page of results, up to top number of results, for this endpoint.\n        :rtype: :class:`<GitPolicyConfigurationResponse> <azure.devops.v5_1.git.models.GitPolicyConfigurationResponse>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if repository_id is not None:\n            query_parameters['repositoryId'] = self._serialize.query('repository_id', repository_id, 'str')\n        if ref_name is not None:\n            query_parameters['refName'] = self._serialize.query('ref_name', ref_name, 'str')\n        if policy_type is not None:\n            query_parameters['policyType'] = self._serialize.query('policy_type', policy_type, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        response = self._send(http_method='GET',\n                              location_id='2c420070-a0a2-49cc-9639-c9f271c5ff07',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        response_object = models.GitPolicyConfigurationResponse()\n        response_object.policy_configurations = self._deserialize('[PolicyConfiguration]', self._unwrap_collection(response))\n        response_object.continuation_token = response.headers.get('x-ms-continuationtoken')\n        return response_object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new attachment to a pull request.", "response": "def create_attachment(self, upload_stream, file_name, repository_id, pull_request_id, project=None, **kwargs):\n        \"\"\"CreateAttachment.\n        [Preview API] Attach a new file to a pull request.\n        :param object upload_stream: Stream to upload\n        :param str file_name: The name of the file.\n        :param str repository_id: The repository ID of the pull request\u2019s target branch.\n        :param int pull_request_id: ID of the pull request.\n        :param str project: Project ID or project name\n        :rtype: :class:`<Attachment> <azure.devops.v5_1.git.models.Attachment>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if file_name is not None:\n            route_values['fileName'] = self._serialize.url('file_name', file_name, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        content = self._client.stream_upload(upload_stream, callback=callback)\n        response = self._send(http_method='POST',\n                              location_id='965d9361-878b-413b-a494-45d5b5fd8ab7',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content,\n                              media_type='application/octet-stream')\n        return self._deserialize('Attachment', response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_attachment(self, file_name, repository_id, pull_request_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if file_name is not None:\n            route_values['fileName'] = self._serialize.url('file_name', file_name, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='965d9361-878b-413b-a494-45d5b5fd8ab7',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "Delete an attachment from a pull request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_attachment_content(self, file_name, repository_id, pull_request_id, project=None, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if file_name is not None:\n            route_values['fileName'] = self._serialize.url('file_name', file_name, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='965d9361-878b-413b-a494-45d5b5fd8ab7',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get the content of an attachment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_like(self, repository_id, pull_request_id, thread_id, comment_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        if thread_id is not None:\n            route_values['threadId'] = self._serialize.url('thread_id', thread_id, 'int')\n        if comment_id is not None:\n            route_values['commentId'] = self._serialize.url('comment_id', comment_id, 'int')\n        self._send(http_method='POST',\n                   location_id='5f2e2851-1389-425b-a00b-fb2adb3ef31b',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "Create a like on a comment."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the iteration statuses for a pull request.", "response": "def update_pull_request_iteration_statuses(self, patch_document, repository_id, pull_request_id, iteration_id, project=None):\n        \"\"\"UpdatePullRequestIterationStatuses.\n        [Preview API] Update pull request iteration statuses collection. The only supported operation type is `remove`.\n        :param :class:`<[JsonPatchOperation]> <azure.devops.v5_1.git.models.[JsonPatchOperation]>` patch_document: Operations to apply to the pull request statuses in JSON Patch format.\n        :param str repository_id: The repository ID of the pull request\u2019s target branch.\n        :param int pull_request_id: ID of the pull request.\n        :param int iteration_id: ID of the pull request iteration.\n        :param str project: Project ID or project name\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        if iteration_id is not None:\n            route_values['iterationId'] = self._serialize.url('iteration_id', iteration_id, 'int')\n        content = self._serialize.body(patch_document, '[JsonPatchOperation]')\n        self._send(http_method='PATCH',\n                   location_id='75cf11c5-979f-4038-a76e-058a06adf2bf',\n                   version='5.1-preview.1',\n                   route_values=route_values,\n                   content=content,\n                   media_type='application/json-patch+json')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_pull_request_label(self, label, repository_id, pull_request_id, project=None, project_id=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        query_parameters = {}\n        if project_id is not None:\n            query_parameters['projectId'] = self._serialize.query('project_id', project_id, 'str')\n        content = self._serialize.body(label, 'WebApiCreateTagRequestData')\n        response = self._send(http_method='POST',\n                              location_id='f22387e3-984e-4c52-9c6d-fbb8f14c812d',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('WebApiTagDefinition', response)", "response": "Create a pull request label."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a label from a pull request.", "response": "def delete_pull_request_labels(self, repository_id, pull_request_id, label_id_or_name, project=None, project_id=None):\n        \"\"\"DeletePullRequestLabels.\n        [Preview API] Removes a label from the set of those assigned to the pull request.\n        :param str repository_id: The repository ID of the pull request\u2019s target branch.\n        :param int pull_request_id: ID of the pull request.\n        :param str label_id_or_name: The name or ID of the label requested.\n        :param str project: Project ID or project name\n        :param str project_id: Project ID or project name.\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        if label_id_or_name is not None:\n            route_values['labelIdOrName'] = self._serialize.url('label_id_or_name', label_id_or_name, 'str')\n        query_parameters = {}\n        if project_id is not None:\n            query_parameters['projectId'] = self._serialize.query('project_id', project_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='f22387e3-984e-4c52-9c6d-fbb8f14c812d',\n                   version='5.1-preview.1',\n                   route_values=route_values,\n                   query_parameters=query_parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_pull_request_properties(self, patch_document, repository_id, pull_request_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        content = self._serialize.body(patch_document, '[JsonPatchOperation]')\n        response = self._send(http_method='PATCH',\n                              location_id='48a52185-5b9e-4736-9dc1-bb1e2feac80b',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content,\n                              media_type='application/json-patch+json')\n        return self._deserialize('object', response)", "response": "Update external properties of a pull request."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_pull_request_reviewer(self, reviewer, repository_id, pull_request_id, reviewer_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        if reviewer_id is not None:\n            route_values['reviewerId'] = self._serialize.url('reviewer_id', reviewer_id, 'str')\n        content = self._serialize.body(reviewer, 'IdentityRefWithVote')\n        response = self._send(http_method='PUT',\n                              location_id='4b6702c7-aa35-4b89-9c96-b9abf6d3e540',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('IdentityRefWithVote', response)", "response": "Creates a new reviewer for a pull request."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a reviewer from a pull request.", "response": "def delete_pull_request_reviewer(self, repository_id, pull_request_id, reviewer_id, project=None):\n        \"\"\"DeletePullRequestReviewer.\n        [Preview API] Remove a reviewer from a pull request.\n        :param str repository_id: The repository ID of the pull request\u2019s target branch.\n        :param int pull_request_id: ID of the pull request.\n        :param str reviewer_id: ID of the reviewer to remove.\n        :param str project: Project ID or project name\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        if reviewer_id is not None:\n            route_values['reviewerId'] = self._serialize.url('reviewer_id', reviewer_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='4b6702c7-aa35-4b89-9c96-b9abf6d3e540',\n                   version='5.1-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_pull_requests_by_project(self, project, search_criteria, max_comment_length=None, skip=None, top=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if search_criteria is not None:\n            if search_criteria.repository_id is not None:\n                query_parameters['searchCriteria.repositoryId'] = search_criteria.repository_id\n            if search_criteria.creator_id is not None:\n                query_parameters['searchCriteria.creatorId'] = search_criteria.creator_id\n            if search_criteria.reviewer_id is not None:\n                query_parameters['searchCriteria.reviewerId'] = search_criteria.reviewer_id\n            if search_criteria.status is not None:\n                query_parameters['searchCriteria.status'] = search_criteria.status\n            if search_criteria.target_ref_name is not None:\n                query_parameters['searchCriteria.targetRefName'] = search_criteria.target_ref_name\n            if search_criteria.source_repository_id is not None:\n                query_parameters['searchCriteria.sourceRepositoryId'] = search_criteria.source_repository_id\n            if search_criteria.source_ref_name is not None:\n                query_parameters['searchCriteria.sourceRefName'] = search_criteria.source_ref_name\n            if search_criteria.include_links is not None:\n                query_parameters['searchCriteria.includeLinks'] = search_criteria.include_links\n        if max_comment_length is not None:\n            query_parameters['maxCommentLength'] = self._serialize.query('max_comment_length', max_comment_length, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        response = self._send(http_method='GET',\n                              location_id='a5d28130-9cd2-40fa-9f08-902e7daa9efb',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitPullRequest]', self._unwrap_collection(response))", "response": "Get all pull requests that match a specified criteria."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a pull request.", "response": "def create_pull_request(self, git_pull_request_to_create, repository_id, project=None, supports_iterations=None):\n        \"\"\"CreatePullRequest.\n        [Preview API] Create a pull request.\n        :param :class:`<GitPullRequest> <azure.devops.v5_1.git.models.GitPullRequest>` git_pull_request_to_create: The pull request to create.\n        :param str repository_id: The repository ID of the pull request's target branch.\n        :param str project: Project ID or project name\n        :param bool supports_iterations: If true, subsequent pushes to the pull request will be individually reviewable. Set this to false for large pull requests for performance reasons if this functionality is not needed.\n        :rtype: :class:`<GitPullRequest> <azure.devops.v5_1.git.models.GitPullRequest>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if supports_iterations is not None:\n            query_parameters['supportsIterations'] = self._serialize.query('supports_iterations', supports_iterations, 'bool')\n        content = self._serialize.body(git_pull_request_to_create, 'GitPullRequest')\n        response = self._send(http_method='POST',\n                              location_id='9946fd70-0d40-406e-b686-b4744cbbcc37',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('GitPullRequest', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_pull_request(self, repository_id, pull_request_id, project=None, max_comment_length=None, skip=None, top=None, include_commits=None, include_work_item_refs=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        query_parameters = {}\n        if max_comment_length is not None:\n            query_parameters['maxCommentLength'] = self._serialize.query('max_comment_length', max_comment_length, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if include_commits is not None:\n            query_parameters['includeCommits'] = self._serialize.query('include_commits', include_commits, 'bool')\n        if include_work_item_refs is not None:\n            query_parameters['includeWorkItemRefs'] = self._serialize.query('include_work_item_refs', include_work_item_refs, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='9946fd70-0d40-406e-b686-b4744cbbcc37',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitPullRequest', response)", "response": "Get a pull request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_pull_request(self, git_pull_request_to_update, repository_id, pull_request_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        content = self._serialize.body(git_pull_request_to_update, 'GitPullRequest')\n        response = self._send(http_method='PATCH',\n                              location_id='9946fd70-0d40-406e-b686-b4744cbbcc37',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitPullRequest', response)", "response": "Update a pull request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_pull_request_status(self, repository_id, pull_request_id, status_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        if status_id is not None:\n            route_values['statusId'] = self._serialize.url('status_id', status_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='b5f6bb4f-8d1e-4d79-8d11-4c9172c99c35',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "Delete a pull request status."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_comment(self, comment, repository_id, pull_request_id, thread_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        if thread_id is not None:\n            route_values['threadId'] = self._serialize.url('thread_id', thread_id, 'int')\n        content = self._serialize.body(comment, 'Comment')\n        response = self._send(http_method='POST',\n                              location_id='965a3ec7-5ed8-455a-bdcb-835a5ea7fe7b',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Comment', response)", "response": "Create a comment on a specific pull request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a single thread in a pull request.", "response": "def get_pull_request_thread(self, repository_id, pull_request_id, thread_id, project=None, iteration=None, base_iteration=None):\n        \"\"\"GetPullRequestThread.\n        [Preview API] Retrieve a thread in a pull request.\n        :param str repository_id: The repository ID of the pull request's target branch.\n        :param int pull_request_id: ID of the pull request.\n        :param int thread_id: ID of the thread.\n        :param str project: Project ID or project name\n        :param int iteration: If specified, thread position will be tracked using this iteration as the right side of the diff.\n        :param int base_iteration: If specified, thread position will be tracked using this iteration as the left side of the diff.\n        :rtype: :class:`<GitPullRequestCommentThread> <azure.devops.v5_1.git.models.GitPullRequestCommentThread>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if pull_request_id is not None:\n            route_values['pullRequestId'] = self._serialize.url('pull_request_id', pull_request_id, 'int')\n        if thread_id is not None:\n            route_values['threadId'] = self._serialize.url('thread_id', thread_id, 'int')\n        query_parameters = {}\n        if iteration is not None:\n            query_parameters['$iteration'] = self._serialize.query('iteration', iteration, 'int')\n        if base_iteration is not None:\n            query_parameters['$baseIteration'] = self._serialize.query('base_iteration', base_iteration, 'int')\n        response = self._send(http_method='GET',\n                              location_id='ab6e2e5d-a0b7-4153-b64a-a4efe0d49449',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitPullRequestCommentThread', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_pushes(self, repository_id, project=None, skip=None, top=None, search_criteria=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if search_criteria is not None:\n            if search_criteria.from_date is not None:\n                query_parameters['searchCriteria.fromDate'] = search_criteria.from_date\n            if search_criteria.to_date is not None:\n                query_parameters['searchCriteria.toDate'] = search_criteria.to_date\n            if search_criteria.pusher_id is not None:\n                query_parameters['searchCriteria.pusherId'] = search_criteria.pusher_id\n            if search_criteria.ref_name is not None:\n                query_parameters['searchCriteria.refName'] = search_criteria.ref_name\n            if search_criteria.include_ref_updates is not None:\n                query_parameters['searchCriteria.includeRefUpdates'] = search_criteria.include_ref_updates\n            if search_criteria.include_links is not None:\n                query_parameters['searchCriteria.includeLinks'] = search_criteria.include_links\n        response = self._send(http_method='GET',\n                              location_id='ea98d07b-3c87-4971-8ede-a613694ffb55',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitPush]', self._unwrap_collection(response))", "response": "Get the pushes associated with the specified repository."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a soft - deleted Git repository from the recycle bin.", "response": "def delete_repository_from_recycle_bin(self, project, repository_id):\n        \"\"\"DeleteRepositoryFromRecycleBin.\n        [Preview API] Destroy (hard delete) a soft-deleted Git repository.\n        :param str project: Project ID or project name\n        :param str repository_id: The ID of the repository.\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='a663da97-81db-4eb3-8b83-287670f63073',\n                   version='5.1-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restore_repository_from_recycle_bin(self, repository_details, project, repository_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        content = self._serialize.body(repository_details, 'GitRecycleBinRepositoryDetails')\n        response = self._send(http_method='PATCH',\n                              location_id='a663da97-81db-4eb3-8b83-287670f63073',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitRepository', response)", "response": "Restores a soft - deleted Git repository from a recycle bin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_refs(self, repository_id, project=None, filter=None, include_links=None, include_statuses=None, include_my_branches=None, latest_statuses_only=None, peel_tags=None, filter_contains=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if filter is not None:\n            query_parameters['filter'] = self._serialize.query('filter', filter, 'str')\n        if include_links is not None:\n            query_parameters['includeLinks'] = self._serialize.query('include_links', include_links, 'bool')\n        if include_statuses is not None:\n            query_parameters['includeStatuses'] = self._serialize.query('include_statuses', include_statuses, 'bool')\n        if include_my_branches is not None:\n            query_parameters['includeMyBranches'] = self._serialize.query('include_my_branches', include_my_branches, 'bool')\n        if latest_statuses_only is not None:\n            query_parameters['latestStatusesOnly'] = self._serialize.query('latest_statuses_only', latest_statuses_only, 'bool')\n        if peel_tags is not None:\n            query_parameters['peelTags'] = self._serialize.query('peel_tags', peel_tags, 'bool')\n        if filter_contains is not None:\n            query_parameters['filterContains'] = self._serialize.query('filter_contains', filter_contains, 'str')\n        response = self._send(http_method='GET',\n                              location_id='2d874a60-a811-4f62-9c9f-963a6ea0a55b',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitRef]', self._unwrap_collection(response))", "response": "Queries the provided repository for its refs and returns them."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating a branch s lock or unlock state.", "response": "def update_ref(self, new_ref_info, repository_id, filter, project=None, project_id=None):\n        \"\"\"UpdateRef.\n        [Preview API] Lock or Unlock a branch.\n        :param :class:`<GitRefUpdate> <azure.devops.v5_1.git.models.GitRefUpdate>` new_ref_info: The ref update action (lock/unlock) to perform\n        :param str repository_id: The name or ID of the repository.\n        :param str filter: The name of the branch to lock/unlock\n        :param str project: Project ID or project name\n        :param str project_id: ID or name of the team project. Optional if specifying an ID for repository.\n        :rtype: :class:`<GitRef> <azure.devops.v5_1.git.models.GitRef>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        query_parameters = {}\n        if filter is not None:\n            query_parameters['filter'] = self._serialize.query('filter', filter, 'str')\n        if project_id is not None:\n            query_parameters['projectId'] = self._serialize.query('project_id', project_id, 'str')\n        content = self._serialize.body(new_ref_info, 'GitRefUpdate')\n        response = self._send(http_method='PATCH',\n                              location_id='2d874a60-a811-4f62-9c9f-963a6ea0a55b',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('GitRef', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the refs favorite specified by the user.", "response": "def delete_ref_favorite(self, project, favorite_id):\n        \"\"\"DeleteRefFavorite.\n        [Preview API] Deletes the refs favorite specified\n        :param str project: Project ID or project name\n        :param int favorite_id: The Id of the ref favorite to delete.\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if favorite_id is not None:\n            route_values['favoriteId'] = self._serialize.url('favorite_id', favorite_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='876f70af-5792-485a-a1c7-d0a7b2f42bbb',\n                   version='5.1-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ref_favorite(self, project, favorite_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if favorite_id is not None:\n            route_values['favoriteId'] = self._serialize.url('favorite_id', favorite_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='876f70af-5792-485a-a1c7-d0a7b2f42bbb',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('GitRefFavorite', response)", "response": "Get the refs favorite for a given id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the refs favorites for a repository and identity.", "response": "def get_ref_favorites(self, project, repository_id=None, identity_id=None):\n        \"\"\"GetRefFavorites.\n        [Preview API] Gets the refs favorites for a repo and an identity.\n        :param str project: Project ID or project name\n        :param str repository_id: The id of the repository.\n        :param str identity_id: The id of the identity whose favorites are to be retrieved. If null, the requesting identity is used.\n        :rtype: [GitRefFavorite]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if repository_id is not None:\n            query_parameters['repositoryId'] = self._serialize.query('repository_id', repository_id, 'str')\n        if identity_id is not None:\n            query_parameters['identityId'] = self._serialize.query('identity_id', identity_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='876f70af-5792-485a-a1c7-d0a7b2f42bbb',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[GitRefFavorite]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a git repository in a team project.", "response": "def create_repository(self, git_repository_to_create, project=None, source_ref=None):\n        \"\"\"CreateRepository.\n        [Preview API] Create a git repository in a team project.\n        :param :class:`<GitRepositoryCreateOptions> <azure.devops.v5_1.git.models.GitRepositoryCreateOptions>` git_repository_to_create: Specify the repo name, team project and/or parent repository. Team project information can be ommitted from gitRepositoryToCreate if the request is project-scoped (i.e., includes project Id).\n        :param str project: Project ID or project name\n        :param str source_ref: [optional] Specify the source refs to use while creating a fork repo\n        :rtype: :class:`<GitRepository> <azure.devops.v5_1.git.models.GitRepository>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if source_ref is not None:\n            query_parameters['sourceRef'] = self._serialize.query('source_ref', source_ref, 'str')\n        content = self._serialize.body(git_repository_to_create, 'GitRepositoryCreateOptions')\n        response = self._send(http_method='POST',\n                              location_id='225f7195-f9c7-4d14-ab28-a83f7ff77e1f',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('GitRepository', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_repository(self, new_repository_info, repository_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        content = self._serialize.body(new_repository_info, 'GitRepository')\n        response = self._send(http_method='PATCH',\n                              location_id='225f7195-f9c7-4d14-ab28-a83f7ff77e1f',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitRepository', response)", "response": "Update the Git repository with either a new repo name or a new default branch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_revert(self, revert_to_create, project, repository_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        content = self._serialize.body(revert_to_create, 'GitAsyncRefOperationParameters')\n        response = self._send(http_method='POST',\n                              location_id='bc866058-5449-4715-9cf1-a510b6ff193c',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitRevert', response)", "response": "Creates a new branch which reverts changes introduced by a specific commit or commits that are associated to a pull request."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_commit_status(self, git_commit_status_to_create, commit_id, repository_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if commit_id is not None:\n            route_values['commitId'] = self._serialize.url('commit_id', commit_id, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        content = self._serialize.body(git_commit_status_to_create, 'GitStatus')\n        response = self._send(http_method='POST',\n                              location_id='428dd4fb-fda5-4722-af02-9313b80305da',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('GitStatus', response)", "response": "Create Git commit status."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a tree object.", "response": "def get_tree(self, repository_id, sha1, project=None, project_id=None, recursive=None, file_name=None):\n        \"\"\"GetTree.\n        [Preview API] The Tree endpoint returns the collection of objects underneath the specified tree. Trees are folders in a Git repository.\n        :param str repository_id: Repository Id.\n        :param str sha1: SHA1 hash of the tree object.\n        :param str project: Project ID or project name\n        :param str project_id: Project Id.\n        :param bool recursive: Search recursively. Include trees underneath this tree. Default is false.\n        :param str file_name: Name to use if a .zip file is returned. Default is the object ID.\n        :rtype: :class:`<GitTreeRef> <azure.devops.v5_1.git.models.GitTreeRef>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if sha1 is not None:\n            route_values['sha1'] = self._serialize.url('sha1', sha1, 'str')\n        query_parameters = {}\n        if project_id is not None:\n            query_parameters['projectId'] = self._serialize.query('project_id', project_id, 'str')\n        if recursive is not None:\n            query_parameters['recursive'] = self._serialize.query('recursive', recursive, 'bool')\n        if file_name is not None:\n            query_parameters['fileName'] = self._serialize.query('file_name', file_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='729f6437-6f92-44ec-8bee-273a7111063c',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GitTreeRef', response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tree_zip(self, repository_id, sha1, project=None, project_id=None, recursive=None, file_name=None, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if repository_id is not None:\n            route_values['repositoryId'] = self._serialize.url('repository_id', repository_id, 'str')\n        if sha1 is not None:\n            route_values['sha1'] = self._serialize.url('sha1', sha1, 'str')\n        query_parameters = {}\n        if project_id is not None:\n            query_parameters['projectId'] = self._serialize.query('project_id', project_id, 'str')\n        if recursive is not None:\n            query_parameters['recursive'] = self._serialize.query('recursive', recursive, 'bool')\n        if file_name is not None:\n            query_parameters['fileName'] = self._serialize.query('file_name', file_name, 'str')\n        response = self._send(http_method='GET',\n                              location_id='729f6437-6f92-44ec-8bee-273a7111063c',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='application/zip')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get a tree zip file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_configurations(self, project, repository_type=None, repository_id=None, branch=None, service_connection_id=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if repository_type is not None:\n            query_parameters['repositoryType'] = self._serialize.query('repository_type', repository_type, 'str')\n        if repository_id is not None:\n            query_parameters['repositoryId'] = self._serialize.query('repository_id', repository_id, 'str')\n        if branch is not None:\n            query_parameters['branch'] = self._serialize.query('branch', branch, 'str')\n        if service_connection_id is not None:\n            query_parameters['serviceConnectionId'] = self._serialize.query('service_connection_id', service_connection_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='8fc87684-9ebc-4c37-ab92-f4ac4a58cb3a',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[ConfigurationFile]', self._unwrap_collection(response))", "response": "Get a list of existing configuration files for a given project."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_project_connection(self, create_connection_inputs, project):\n        query_parameters = {}\n        if project is not None:\n            query_parameters['project'] = self._serialize.query('project', project, 'str')\n        content = self._serialize.body(create_connection_inputs, 'CreatePipelineConnectionInputs')\n        response = self._send(http_method='POST',\n                              location_id='00df4879-9216-45d5-b38d-4a487b626b2c',\n                              version='5.1-preview.1',\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('PipelineConnection', response)", "response": "Create a new Pipeline Connection between the provider installation and the specified project."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new resource in the specified project.", "response": "def create_resources(self, creation_parameters, project):\n        \"\"\"CreateResources.\n        [Preview API]\n        :param {ResourceCreationParameter} creation_parameters:\n        :param str project: Project ID or project name\n        :rtype: :class:`<CreatedResources> <azure.devops.v5_1.cix.models.CreatedResources>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(creation_parameters, '{ResourceCreationParameter}')\n        response = self._send(http_method='POST',\n                              location_id='43201899-7690-4870-9c79-ab69605f21ed',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('CreatedResources', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_template(self, template_parameters, template_id):\n        route_values = {}\n        if template_id is not None:\n            route_values['templateId'] = self._serialize.url('template_id', template_id, 'str')\n        content = self._serialize.body(template_parameters, 'TemplateParameters')\n        response = self._send(http_method='POST',\n                              location_id='eb5d6d1d-98a2-4bbd-9028-f9a6b2d66515',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Template', response)", "response": "RenderTemplate.\n        [Preview API]\n        :param :class:`<TemplateParameters> <azure.devops.v5_1.cix.models.TemplateParameters>` template_parameters:\n        :param str template_id:\n        :rtype: :class:`<Template> <azure.devops.v5_1.cix.models.Template>`"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a comment to a specific work item.", "response": "def add_comment(self, request, project, work_item_id):\n        \"\"\"AddComment.\n        [Preview API] Add a comment on a work item.\n        :param :class:`<WorkItemCommentCreateRequest> <azure.devops.v5_1.work_item_tracking_comments.models.WorkItemCommentCreateRequest>` request: Comment create request.\n        :param str project: Project ID or project name\n        :param int work_item_id: Id of a work item.\n        :rtype: :class:`<WorkItemCommentResponse> <azure.devops.v5_1.work-item-tracking-comments.models.WorkItemCommentResponse>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if work_item_id is not None:\n            route_values['workItemId'] = self._serialize.url('work_item_id', work_item_id, 'int')\n        content = self._serialize.body(request, 'WorkItemCommentCreateRequest')\n        response = self._send(http_method='POST',\n                              location_id='608aac0a-32e1-4493-a863-b9cf4566d257',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('WorkItemCommentResponse', response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_personal_access_token(self, feed_name=None, token_type=None):\n        route_values = {}\n        if feed_name is not None:\n            route_values['feedName'] = self._serialize.url('feed_name', feed_name, 'str')\n        query_parameters = {}\n        if token_type is not None:\n            query_parameters['tokenType'] = self._serialize.query('token_type', token_type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='dfdb7ad7-3d8e-4907-911e-19b4a8330550',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('object', response)", "response": "Get a time - limited session token representing the current user with permissions scoped to the read and write of Artifacts."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a policy configuration by its ID.", "response": "def delete_policy_configuration(self, project, configuration_id):\n        \"\"\"DeletePolicyConfiguration.\n        Delete a policy configuration by its ID.\n        :param str project: Project ID or project name\n        :param int configuration_id: ID of the policy configuration to delete.\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if configuration_id is not None:\n            route_values['configurationId'] = self._serialize.url('configuration_id', configuration_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='dad91cbe-d183-45f8-9c6e-9c1164472121',\n                   version='5.0',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_policy_configurations(self, project, scope=None, policy_type=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if scope is not None:\n            query_parameters['scope'] = self._serialize.query('scope', scope, 'str')\n        if policy_type is not None:\n            query_parameters['policyType'] = self._serialize.query('policy_type', policy_type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='dad91cbe-d183-45f8-9c6e-9c1164472121',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[PolicyConfiguration]', self._unwrap_collection(response))", "response": "Get a list of policy configurations in a project."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates a policy configuration by its ID.", "response": "def update_policy_configuration(self, configuration, project, configuration_id):\n        \"\"\"UpdatePolicyConfiguration.\n        Update a policy configuration by its ID.\n        :param :class:`<PolicyConfiguration> <azure.devops.v5_0.policy.models.PolicyConfiguration>` configuration: The policy configuration to update.\n        :param str project: Project ID or project name\n        :param int configuration_id: ID of the existing policy configuration to be updated.\n        :rtype: :class:`<PolicyConfiguration> <azure.devops.v5_0.policy.models.PolicyConfiguration>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if configuration_id is not None:\n            route_values['configurationId'] = self._serialize.url('configuration_id', configuration_id, 'int')\n        content = self._serialize.body(configuration, 'PolicyConfiguration')\n        response = self._send(http_method='PUT',\n                              location_id='dad91cbe-d183-45f8-9c6e-9c1164472121',\n                              version='5.0',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('PolicyConfiguration', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_policy_evaluation(self, project, evaluation_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if evaluation_id is not None:\n            route_values['evaluationId'] = self._serialize.url('evaluation_id', evaluation_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='46aecb7a-5d2c-4647-897b-0209505a9fe4',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('PolicyEvaluationRecord', response)", "response": "Get the present evaluation state of a policy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the list of all the policy evaluations for a specific pull request.", "response": "def get_policy_evaluations(self, project, artifact_id, include_not_applicable=None, top=None, skip=None):\n        \"\"\"GetPolicyEvaluations.\n        [Preview API] Retrieves a list of all the policy evaluation statuses for a specific pull request.\n        :param str project: Project ID or project name\n        :param str artifact_id: A string which uniquely identifies the target of a policy evaluation.\n        :param bool include_not_applicable: Some policies might determine that they do not apply to a specific pull request. Setting this parameter to true will return evaluation records even for policies which don't apply to this pull request.\n        :param int top: The number of policy evaluation records to retrieve.\n        :param int skip: The number of policy evaluation records to ignore. For example, to retrieve results 101-150, set top to 50 and skip to 100.\n        :rtype: [PolicyEvaluationRecord]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if artifact_id is not None:\n            query_parameters['artifactId'] = self._serialize.query('artifact_id', artifact_id, 'str')\n        if include_not_applicable is not None:\n            query_parameters['includeNotApplicable'] = self._serialize.query('include_not_applicable', include_not_applicable, 'bool')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        response = self._send(http_method='GET',\n                              location_id='c23ddff5-229c-4d04-a80b-0fdce9f360c8',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[PolicyEvaluationRecord]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_policy_configuration_revision(self, project, configuration_id, revision_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if configuration_id is not None:\n            route_values['configurationId'] = self._serialize.url('configuration_id', configuration_id, 'int')\n        if revision_id is not None:\n            route_values['revisionId'] = self._serialize.url('revision_id', revision_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='fe1e68a2-60d3-43cb-855b-85e41ae97c95',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('PolicyConfiguration', response)", "response": "Get a specific revision of a given policy configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a specific policy type by ID.", "response": "def get_policy_type(self, project, type_id):\n        \"\"\"GetPolicyType.\n        Retrieve a specific policy type by ID.\n        :param str project: Project ID or project name\n        :param str type_id: The policy ID.\n        :rtype: :class:`<PolicyType> <azure.devops.v5_0.policy.models.PolicyType>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if type_id is not None:\n            route_values['typeId'] = self._serialize.url('type_id', type_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='44096322-2d3d-466a-bb30-d1b7de69f61f',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('PolicyType', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_feature_flag_by_name(self, name, check_feature_exists=None):\n        route_values = {}\n        if name is not None:\n            route_values['name'] = self._serialize.url('name', name, 'str')\n        query_parameters = {}\n        if check_feature_exists is not None:\n            query_parameters['checkFeatureExists'] = self._serialize.query('check_feature_exists', check_feature_exists, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='3e2b80f8-9e6f-441e-8393-005610692d9c',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('FeatureFlag', response)", "response": "Get a single feature flag by its name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the state of an individual feature flag for a specific feature.", "response": "def update_feature_flag(self, state, name, user_email=None, check_feature_exists=None, set_at_application_level_also=None):\n        \"\"\"UpdateFeatureFlag.\n        [Preview API] Change the state of an individual feature flag for a name\n        :param :class:`<FeatureFlagPatch> <azure.devops.v5_0.feature_availability.models.FeatureFlagPatch>` state: State that should be set\n        :param str name: The name of the feature to change\n        :param str user_email:\n        :param bool check_feature_exists: Checks if the feature exists before setting the state\n        :param bool set_at_application_level_also:\n        :rtype: :class:`<FeatureFlag> <azure.devops.v5_0.feature_availability.models.FeatureFlag>`\n        \"\"\"\n        route_values = {}\n        if name is not None:\n            route_values['name'] = self._serialize.url('name', name, 'str')\n        query_parameters = {}\n        if user_email is not None:\n            query_parameters['userEmail'] = self._serialize.query('user_email', user_email, 'str')\n        if check_feature_exists is not None:\n            query_parameters['checkFeatureExists'] = self._serialize.query('check_feature_exists', check_feature_exists, 'bool')\n        if set_at_application_level_also is not None:\n            query_parameters['setAtApplicationLevelAlso'] = self._serialize.query('set_at_application_level_also', set_at_application_level_also, 'bool')\n        content = self._serialize.body(state, 'FeatureFlagPatch')\n        response = self._send(http_method='PATCH',\n                              location_id='3e2b80f8-9e6f-441e-8393-005610692d9c',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('FeatureFlag', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprepare and send the request according to configuration.", "response": "def _send_request(self, request, headers=None, content=None, **operation_config):\n        \"\"\"Prepare and send request object according to configuration.\n        :param ClientRequest request: The request object to be sent.\n        :param dict headers: Any headers to add to the request.\n        :param content: Any body data to add to the request.\n        :param config: Any specific config overrides\n        \"\"\"\n        if (TRACE_ENV_VAR in os.environ and os.environ[TRACE_ENV_VAR] == 'true')\\\n                or (TRACE_ENV_VAR_COMPAT in os.environ and os.environ[TRACE_ENV_VAR_COMPAT] == 'true'):\n            print(request.method + ' ' + request.url)\n        logger.debug('%s %s', request.method, request.url)\n        logger.debug('Request content: %s', content)\n        response = self._client.send(request=request, headers=headers,\n                                     content=content, **operation_config)\n        logger.debug('Response content: %s', response.content)\n        if response.status_code < 200 or response.status_code >= 300:\n            self._handle_error(request, response)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a group entitlement to a project level group.", "response": "def add_group_entitlement(self, group_entitlement, rule_option=None):\n        \"\"\"AddGroupEntitlement.\n        [Preview API] Create a group entitlement with license rule, extension rule.\n        :param :class:`<GroupEntitlement> <azure.devops.v5_0.member_entitlement_management.models.GroupEntitlement>` group_entitlement: GroupEntitlement object specifying License Rule, Extensions Rule for the group. Based on the rules the members of the group will be given licenses and extensions. The Group Entitlement can be used to add the group to another project level groups\n        :param str rule_option: RuleOption [ApplyGroupRule/TestApplyGroupRule] - specifies if the rules defined in group entitlement should be created and applied to it\u2019s members (default option) or just be tested\n        :rtype: :class:`<GroupEntitlementOperationReference> <azure.devops.v5_0.member_entitlement_management.models.GroupEntitlementOperationReference>`\n        \"\"\"\n        query_parameters = {}\n        if rule_option is not None:\n            query_parameters['ruleOption'] = self._serialize.query('rule_option', rule_option, 'str')\n        content = self._serialize.body(group_entitlement, 'GroupEntitlement')\n        response = self._send(http_method='POST',\n                              location_id='2280bffa-58a2-49da-822e-0764a1bb44f7',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters,\n                              content=content)\n        return self._deserialize('GroupEntitlementOperationReference', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a group entitlement.", "response": "def delete_group_entitlement(self, group_id, rule_option=None, remove_group_membership=None):\n        \"\"\"DeleteGroupEntitlement.\n        [Preview API] Delete a group entitlement.\n        :param str group_id: ID of the group to delete.\n        :param str rule_option: RuleOption [ApplyGroupRule/TestApplyGroupRule] - specifies if the rules defined in group entitlement should be deleted and the changes are applied to it\u2019s members (default option) or just be tested\n        :param bool remove_group_membership: Optional parameter that specifies whether the group with the given ID should be removed from all other groups\n        :rtype: :class:`<GroupEntitlementOperationReference> <azure.devops.v5_0.member_entitlement_management.models.GroupEntitlementOperationReference>`\n        \"\"\"\n        route_values = {}\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        query_parameters = {}\n        if rule_option is not None:\n            query_parameters['ruleOption'] = self._serialize.query('rule_option', rule_option, 'str')\n        if remove_group_membership is not None:\n            query_parameters['removeGroupMembership'] = self._serialize.query('remove_group_membership', remove_group_membership, 'bool')\n        response = self._send(http_method='DELETE',\n                              location_id='2280bffa-58a2-49da-822e-0764a1bb44f7',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('GroupEntitlementOperationReference', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a group entitlement.", "response": "def get_group_entitlement(self, group_id):\n        \"\"\"GetGroupEntitlement.\n        [Preview API] Get a group entitlement.\n        :param str group_id: ID of the group.\n        :rtype: :class:`<GroupEntitlement> <azure.devops.v5_0.member_entitlement_management.models.GroupEntitlement>`\n        \"\"\"\n        route_values = {}\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='2280bffa-58a2-49da-822e-0764a1bb44f7',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('GroupEntitlement', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the entitlements for a group.", "response": "def update_group_entitlement(self, document, group_id, rule_option=None):\n        \"\"\"UpdateGroupEntitlement.\n        [Preview API] Update entitlements (License Rule, Extensions Rule, Project memberships etc.) for a group.\n        :param :class:`<[JsonPatchOperation]> <azure.devops.v5_0.member_entitlement_management.models.[JsonPatchOperation]>` document: JsonPatchDocument containing the operations to perform on the group.\n        :param str group_id: ID of the group.\n        :param str rule_option: RuleOption [ApplyGroupRule/TestApplyGroupRule] - specifies if the rules defined in group entitlement should be updated and the changes are applied to it\u2019s members (default option) or just be tested\n        :rtype: :class:`<GroupEntitlementOperationReference> <azure.devops.v5_0.member_entitlement_management.models.GroupEntitlementOperationReference>`\n        \"\"\"\n        route_values = {}\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        query_parameters = {}\n        if rule_option is not None:\n            query_parameters['ruleOption'] = self._serialize.query('rule_option', rule_option, 'str')\n        content = self._serialize.body(document, '[JsonPatchOperation]')\n        response = self._send(http_method='PATCH',\n                              location_id='2280bffa-58a2-49da-822e-0764a1bb44f7',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              content=content,\n                              media_type='application/json-patch+json')\n        return self._deserialize('GroupEntitlementOperationReference', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_member_to_group(self, group_id, member_id):\n        route_values = {}\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        if member_id is not None:\n            route_values['memberId'] = self._serialize.url('member_id', member_id, 'str')\n        self._send(http_method='PUT',\n                   location_id='45a36e53-5286-4518-aa72-2d29f7acc5d8',\n                   version='5.0-preview.1',\n                   route_values=route_values)", "response": "Add a member to a group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_group_members(self, group_id, max_results=None, paging_token=None):\n        route_values = {}\n        if group_id is not None:\n            route_values['groupId'] = self._serialize.url('group_id', group_id, 'str')\n        query_parameters = {}\n        if max_results is not None:\n            query_parameters['maxResults'] = self._serialize.query('max_results', max_results, 'int')\n        if paging_token is not None:\n            query_parameters['pagingToken'] = self._serialize.query('paging_token', paging_token, 'str')\n        response = self._send(http_method='GET',\n                              location_id='45a36e53-5286-4518-aa72-2d29f7acc5d8',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('PagedGraphMemberList', response)", "response": "Get the direct members of a group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_user_entitlement(self, user_entitlement):\n        content = self._serialize.body(user_entitlement, 'UserEntitlement')\n        response = self._send(http_method='POST',\n                              location_id='387f832c-dbf2-4643-88e9-c1aa94dbb737',\n                              version='5.0-preview.2',\n                              content=content)\n        return self._deserialize('UserEntitlementsPostResponse', response)", "response": "Adds a user entitlement to a project group."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_user_entitlements(self, top=None, skip=None, filter=None, sort_option=None):\n        query_parameters = {}\n        if top is not None:\n            query_parameters['top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['skip'] = self._serialize.query('skip', skip, 'int')\n        if filter is not None:\n            query_parameters['filter'] = self._serialize.query('filter', filter, 'str')\n        if sort_option is not None:\n            query_parameters['sortOption'] = self._serialize.query('sort_option', sort_option, 'str')\n        response = self._send(http_method='GET',\n                              location_id='387f832c-dbf2-4643-88e9-c1aa94dbb737',\n                              version='5.0-preview.2',\n                              query_parameters=query_parameters)\n        return self._deserialize('PagedGraphMemberList', response)", "response": "Get a paged set of user entitlements matching the filter criteria."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_user_entitlements(self, document, do_not_send_invite_for_new_users=None):\n        query_parameters = {}\n        if do_not_send_invite_for_new_users is not None:\n            query_parameters['doNotSendInviteForNewUsers'] = self._serialize.query('do_not_send_invite_for_new_users', do_not_send_invite_for_new_users, 'bool')\n        content = self._serialize.body(document, '[JsonPatchOperation]')\n        response = self._send(http_method='PATCH',\n                              location_id='387f832c-dbf2-4643-88e9-c1aa94dbb737',\n                              version='5.0-preview.2',\n                              query_parameters=query_parameters,\n                              content=content,\n                              media_type='application/json-patch+json')\n        return self._deserialize('UserEntitlementOperationReference', response)", "response": "Updates the entitlements for one or more users."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_user_entitlement(self, user_id):\n        route_values = {}\n        if user_id is not None:\n            route_values['userId'] = self._serialize.url('user_id', user_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='8480c6eb-ce60-47e9-88df-eca3c801638b',\n                              version='5.0-preview.2',\n                              route_values=route_values)\n        return self._deserialize('UserEntitlement', response)", "response": "Get User Entitlement for a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_user_entitlement(self, document, user_id):\n        route_values = {}\n        if user_id is not None:\n            route_values['userId'] = self._serialize.url('user_id', user_id, 'str')\n        content = self._serialize.body(document, '[JsonPatchOperation]')\n        response = self._send(http_method='PATCH',\n                              location_id='8480c6eb-ce60-47e9-88df-eca3c801638b',\n                              version='5.0-preview.2',\n                              route_values=route_values,\n                              content=content,\n                              media_type='application/json-patch+json')\n        return self._deserialize('UserEntitlementsPatchResponse', response)", "response": "Update the entitlements for a user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget users summary of the current resource.", "response": "def get_users_summary(self, select=None):\n        \"\"\"GetUsersSummary.\n        [Preview API] Get summary of Licenses, Extension, Projects, Groups and their assignments in the collection.\n        :param str select: Comma (\",\") separated list of properties to select. Supported property names are {AccessLevels, Licenses, Extensions, Projects, Groups}.\n        :rtype: :class:`<UsersSummary> <azure.devops.v5_0.member_entitlement_management.models.UsersSummary>`\n        \"\"\"\n        query_parameters = {}\n        if select is not None:\n            query_parameters['select'] = self._serialize.query('select', select, 'str')\n        response = self._send(http_method='GET',\n                              location_id='5ae55b13-c9dd-49d1-957e-6e76c152e3d9',\n                              version='5.0-preview.1',\n                              query_parameters=query_parameters)\n        return self._deserialize('UsersSummary', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding Agent Cloud to Task Agent.", "response": "def add_agent_cloud(self, agent_cloud):\n        \"\"\"AddAgentCloud.\n        [Preview API]\n        :param :class:`<TaskAgentCloud> <azure.devops.v5_0.task_agent.models.TaskAgentCloud>` agent_cloud:\n        :rtype: :class:`<TaskAgentCloud> <azure.devops.v5_0.task_agent.models.TaskAgentCloud>`\n        \"\"\"\n        content = self._serialize.body(agent_cloud, 'TaskAgentCloud')\n        response = self._send(http_method='POST',\n                              location_id='bfa72b3d-0fc6-43fb-932b-a7f6559f93b9',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('TaskAgentCloud', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_deployment_group(self, project, deployment_group_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if deployment_group_id is not None:\n            route_values['deploymentGroupId'] = self._serialize.url('deployment_group_id', deployment_group_id, 'int')\n        self._send(http_method='DELETE',\n                   location_id='083c4d89-ab35-45af-aa11-7cf66895c53e',\n                   version='5.0-preview.1',\n                   route_values=route_values)", "response": "Delete a deployment group."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a deployment group by its ID.", "response": "def get_deployment_group(self, project, deployment_group_id, action_filter=None, expand=None):\n        \"\"\"GetDeploymentGroup.\n        [Preview API] Get a deployment group by its ID.\n        :param str project: Project ID or project name\n        :param int deployment_group_id: ID of the deployment group.\n        :param str action_filter: Get the deployment group only if this action can be performed on it.\n        :param str expand: Include these additional details in the returned object.\n        :rtype: :class:`<DeploymentGroup> <azure.devops.v5_0.task_agent.models.DeploymentGroup>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if deployment_group_id is not None:\n            route_values['deploymentGroupId'] = self._serialize.url('deployment_group_id', deployment_group_id, 'int')\n        query_parameters = {}\n        if action_filter is not None:\n            query_parameters['actionFilter'] = self._serialize.query('action_filter', action_filter, 'str')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        response = self._send(http_method='GET',\n                              location_id='083c4d89-ab35-45af-aa11-7cf66895c53e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('DeploymentGroup', response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a list of deployment groups by name or IDs.", "response": "def get_deployment_groups(self, project, name=None, action_filter=None, expand=None, continuation_token=None, top=None, ids=None):\n        \"\"\"GetDeploymentGroups.\n        [Preview API] Get a list of deployment groups by name or IDs.\n        :param str project: Project ID or project name\n        :param str name: Name of the deployment group.\n        :param str action_filter: Get only deployment groups on which this action can be performed.\n        :param str expand: Include these additional details in the returned objects.\n        :param str continuation_token: Get deployment groups with names greater than this continuationToken lexicographically.\n        :param int top: Maximum number of deployment groups to return. Default is **1000**.\n        :param [int] ids: Comma separated list of IDs of the deployment groups.\n        :rtype: [DeploymentGroup]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if name is not None:\n            query_parameters['name'] = self._serialize.query('name', name, 'str')\n        if action_filter is not None:\n            query_parameters['actionFilter'] = self._serialize.query('action_filter', action_filter, 'str')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if ids is not None:\n            ids = \",\".join(map(str, ids))\n            query_parameters['ids'] = self._serialize.query('ids', ids, 'str')\n        response = self._send(http_method='GET',\n                              location_id='083c4d89-ab35-45af-aa11-7cf66895c53e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[DeploymentGroup]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating a list of deployment targets in a deployment group.", "response": "def update_deployment_targets(self, machines, project, deployment_group_id):\n        \"\"\"UpdateDeploymentTargets.\n        [Preview API] Update tags of a list of deployment targets in a deployment group.\n        :param [DeploymentTargetUpdateParameter] machines: Deployment targets with tags to udpdate.\n        :param str project: Project ID or project name\n        :param int deployment_group_id: ID of the deployment group in which deployment targets are updated.\n        :rtype: [DeploymentMachine]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if deployment_group_id is not None:\n            route_values['deploymentGroupId'] = self._serialize.url('deployment_group_id', deployment_group_id, 'int')\n        content = self._serialize.body(machines, '[DeploymentTargetUpdateParameter]')\n        response = self._send(http_method='PATCH',\n                              location_id='2f0aa599-c121-4256-a5fd-ba370e0ae7b6',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[DeploymentMachine]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a task group to the project.", "response": "def add_task_group(self, task_group, project):\n        \"\"\"AddTaskGroup.\n        [Preview API] Create a task group.\n        :param :class:`<TaskGroupCreateParameter> <azure.devops.v5_0.task_agent.models.TaskGroupCreateParameter>` task_group: Task group object to create.\n        :param str project: Project ID or project name\n        :rtype: :class:`<TaskGroup> <azure.devops.v5_0.task_agent.models.TaskGroup>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(task_group, 'TaskGroupCreateParameter')\n        response = self._send(http_method='POST',\n                              location_id='6c08ffbf-dbf1-4f9a-94e5-a1cbd47005e7',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('TaskGroup', response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_variable_groups(self, project, group_name=None, action_filter=None, top=None, continuation_token=None, query_order=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if group_name is not None:\n            query_parameters['groupName'] = self._serialize.query('group_name', group_name, 'str')\n        if action_filter is not None:\n            query_parameters['actionFilter'] = self._serialize.query('action_filter', action_filter, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'int')\n        if query_order is not None:\n            query_parameters['queryOrder'] = self._serialize.query('query_order', query_order, 'str')\n        response = self._send(http_method='GET',\n                              location_id='f5b09dd5-9d54-45a1-8b5a-1c8287d634cc',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[VariableGroup]', self._unwrap_collection(response))", "response": "GetVariableGroups.\n        [Preview API] Get variable groups.\n        :param str project: Project ID or project name\n        :param str group_name: Name of variable group.\n        :param str action_filter: Action filter for the variable group. It specifies the action which can be performed on the variable groups.\n        :param int top: Number of variable groups to get.\n        :param int continuation_token: Gets the variable groups after the continuation token provided.\n        :param str query_order: Gets the results in the defined order. Default is 'IdDescending'.\n        :rtype: [VariableGroup]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all setting entries for the given user and all - users scope.", "response": "def get_entries(self, user_scope, key=None):\n        \"\"\"GetEntries.\n        [Preview API] Get all setting entries for the given user/all-users scope\n        :param str user_scope: User-Scope at which to get the value. Should be \"me\" for the current user or \"host\" for all users.\n        :param str key: Optional key under which to filter all the entries\n        :rtype: {object}\n        \"\"\"\n        route_values = {}\n        if user_scope is not None:\n            route_values['userScope'] = self._serialize.url('user_scope', user_scope, 'str')\n        if key is not None:\n            route_values['key'] = self._serialize.url('key', key, 'str')\n        response = self._send(http_method='GET',\n                              location_id='cd006711-163d-4cd4-a597-b05bad2556ff',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('{object}', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove the entry or entries under the specified path.", "response": "def remove_entries(self, user_scope, key):\n        \"\"\"RemoveEntries.\n        [Preview API] Remove the entry or entries under the specified path\n        :param str user_scope: User-Scope at which to remove the value. Should be \"me\" for the current user or \"host\" for all users.\n        :param str key: Root key of the entry or entries to remove\n        \"\"\"\n        route_values = {}\n        if user_scope is not None:\n            route_values['userScope'] = self._serialize.url('user_scope', user_scope, 'str')\n        if key is not None:\n            route_values['key'] = self._serialize.url('key', key, 'str')\n        self._send(http_method='DELETE',\n                   location_id='cd006711-163d-4cd4-a597-b05bad2556ff',\n                   version='5.0-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the specified entries for the current user and all - users scope.", "response": "def set_entries(self, entries, user_scope):\n        \"\"\"SetEntries.\n        [Preview API] Set the specified setting entry values for the given user/all-users scope\n        :param {object} entries: The entries to set\n        :param str user_scope: User-Scope at which to set the values. Should be \"me\" for the current user or \"host\" for all users.\n        \"\"\"\n        route_values = {}\n        if user_scope is not None:\n            route_values['userScope'] = self._serialize.url('user_scope', user_scope, 'str')\n        content = self._serialize.body(entries, '{object}')\n        self._send(http_method='PATCH',\n                   location_id='cd006711-163d-4cd4-a597-b05bad2556ff',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   content=content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_entries_for_scope(self, user_scope, scope_name, scope_value, key=None):\n        route_values = {}\n        if user_scope is not None:\n            route_values['userScope'] = self._serialize.url('user_scope', user_scope, 'str')\n        if scope_name is not None:\n            route_values['scopeName'] = self._serialize.url('scope_name', scope_name, 'str')\n        if scope_value is not None:\n            route_values['scopeValue'] = self._serialize.url('scope_value', scope_value, 'str')\n        if key is not None:\n            route_values['key'] = self._serialize.url('key', key, 'str')\n        response = self._send(http_method='GET',\n                              location_id='4cbaafaf-e8af-4570-98d1-79ee99c56327',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('{object}', self._unwrap_collection(response))", "response": "Get all entries for a given scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the entries for a specific scope.", "response": "def remove_entries_for_scope(self, user_scope, scope_name, scope_value, key):\n        \"\"\"RemoveEntriesForScope.\n        [Preview API] Remove the entry or entries under the specified path\n        :param str user_scope: User-Scope at which to remove the value. Should be \"me\" for the current user or \"host\" for all users.\n        :param str scope_name: Scope at which to get the setting for (e.g. \"project\" or \"team\")\n        :param str scope_value: Value of the scope (e.g. the project or team id)\n        :param str key: Root key of the entry or entries to remove\n        \"\"\"\n        route_values = {}\n        if user_scope is not None:\n            route_values['userScope'] = self._serialize.url('user_scope', user_scope, 'str')\n        if scope_name is not None:\n            route_values['scopeName'] = self._serialize.url('scope_name', scope_name, 'str')\n        if scope_value is not None:\n            route_values['scopeValue'] = self._serialize.url('scope_value', scope_value, 'str')\n        if key is not None:\n            route_values['key'] = self._serialize.url('key', key, 'str')\n        self._send(http_method='DELETE',\n                   location_id='4cbaafaf-e8af-4570-98d1-79ee99c56327',\n                   version='5.0-preview.1',\n                   route_values=route_values)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the specified entries for the given scope.", "response": "def set_entries_for_scope(self, entries, user_scope, scope_name, scope_value):\n        \"\"\"SetEntriesForScope.\n        [Preview API] Set the specified entries for the given named scope\n        :param {object} entries: The entries to set\n        :param str user_scope: User-Scope at which to set the values. Should be \"me\" for the current user or \"host\" for all users.\n        :param str scope_name: Scope at which to set the settings on (e.g. \"project\" or \"team\")\n        :param str scope_value: Value of the scope (e.g. the project or team id)\n        \"\"\"\n        route_values = {}\n        if user_scope is not None:\n            route_values['userScope'] = self._serialize.url('user_scope', user_scope, 'str')\n        if scope_name is not None:\n            route_values['scopeName'] = self._serialize.url('scope_name', scope_name, 'str')\n        if scope_value is not None:\n            route_values['scopeValue'] = self._serialize.url('scope_value', scope_value, 'str')\n        content = self._serialize.body(entries, '{object}')\n        self._send(http_method='PATCH',\n                   location_id='4cbaafaf-e8af-4570-98d1-79ee99c56327',\n                   version='5.0-preview.1',\n                   route_values=route_values,\n                   content=content)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_feed(self, feed_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='c65009a7-474a-4ad1-8b42-7d852107ef8c',\n                   version='5.1-preview.1',\n                   route_values=route_values)", "response": "Delete a feed and all its packages."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all feeds in an account where you have the provided role access.", "response": "def get_feeds(self, project=None, feed_role=None, include_deleted_upstreams=None):\n        \"\"\"GetFeeds.\n        [Preview API] Get all feeds in an account where you have the provided role access.\n        :param str project: Project ID or project name\n        :param str feed_role: Filter by this role, either Administrator(4), Contributor(3), or Reader(2) level permissions.\n        :param bool include_deleted_upstreams: Include upstreams that have been deleted in the response.\n        :rtype: [Feed]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if feed_role is not None:\n            query_parameters['feedRole'] = self._serialize.query('feed_role', feed_role, 'str')\n        if include_deleted_upstreams is not None:\n            query_parameters['includeDeletedUpstreams'] = self._serialize.query('include_deleted_upstreams', include_deleted_upstreams, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='c65009a7-474a-4ad1-8b42-7d852107ef8c',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[Feed]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef query_package_metrics(self, package_id_query, feed_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        content = self._serialize.body(package_id_query, 'PackageMetricsQuery')\n        response = self._send(http_method='POST',\n                              location_id='bddc9b3c-8a59-4a9f-9b40-ee1dcaa2cc0d',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[PackageMetrics]', self._unwrap_collection(response))", "response": "Query the package metrics for a given feed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the permissions for a feed.", "response": "def get_feed_permissions(self, feed_id, include_ids=None, exclude_inherited_permissions=None, identity_descriptor=None):\n        \"\"\"GetFeedPermissions.\n        [Preview API] Get the permissions for a feed.\n        :param str feed_id: Name or Id of the feed.\n        :param bool include_ids: True to include user Ids in the response.  Default is false.\n        :param bool exclude_inherited_permissions: True to only return explicitly set permissions on the feed.  Default is false.\n        :param str identity_descriptor: Filter permissions to the provided identity.\n        :rtype: [FeedPermission]\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        query_parameters = {}\n        if include_ids is not None:\n            query_parameters['includeIds'] = self._serialize.query('include_ids', include_ids, 'bool')\n        if exclude_inherited_permissions is not None:\n            query_parameters['excludeInheritedPermissions'] = self._serialize.query('exclude_inherited_permissions', exclude_inherited_permissions, 'bool')\n        if identity_descriptor is not None:\n            query_parameters['identityDescriptor'] = self._serialize.query('identity_descriptor', identity_descriptor, 'str')\n        response = self._send(http_method='GET',\n                              location_id='be8c1476-86a7-44ed-b19d-aec0e9275cd8',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[FeedPermission]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the permissions on a feed.", "response": "def set_feed_permissions(self, feed_permission, feed_id):\n        \"\"\"SetFeedPermissions.\n        [Preview API] Update the permissions on a feed.\n        :param [FeedPermission] feed_permission: Permissions to set.\n        :param str feed_id: Name or Id of the feed.\n        :rtype: [FeedPermission]\n        \"\"\"\n        route_values = {}\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        content = self._serialize.body(feed_permission, '[FeedPermission]')\n        response = self._send(http_method='PATCH',\n                              location_id='be8c1476-86a7-44ed-b19d-aec0e9275cd8',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[FeedPermission]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_package_version_provenance(self, feed_id, package_id, package_version_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_id is not None:\n            route_values['packageId'] = self._serialize.url('package_id', package_id, 'str')\n        if package_version_id is not None:\n            route_values['packageVersionId'] = self._serialize.url('package_version_id', package_version_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='0aaeabd4-85cd-4686-8a77-8d31c15690b8',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('PackageVersionProvenance', response)", "response": "Get the provenance for a package version."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_feed_retention_policies(self, policy, feed_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        content = self._serialize.body(policy, 'FeedRetentionPolicy')\n        response = self._send(http_method='PUT',\n                              location_id='ed52a011-0112-45b5-9f9e-e14efffb3193',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('FeedRetentionPolicy', response)", "response": "Set the retention policy for a specific feed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery the package version metrics for a specific feed.", "response": "def query_package_version_metrics(self, package_version_id_query, feed_id, package_id, project=None):\n        \"\"\"QueryPackageVersionMetrics.\n        [Preview API]\n        :param :class:`<PackageVersionMetricsQuery> <azure.devops.v5_1.feed.models.PackageVersionMetricsQuery>` package_version_id_query:\n        :param str feed_id:\n        :param str package_id:\n        :param str project: Project ID or project name\n        :rtype: [PackageVersionMetrics]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_id is not None:\n            route_values['packageId'] = self._serialize.url('package_id', package_id, 'str')\n        content = self._serialize.body(package_version_id_query, 'PackageVersionMetricsQuery')\n        response = self._send(http_method='POST',\n                              location_id='e6ae8caa-b6a8-4809-b840-91b2a42c19ad',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('[PackageVersionMetrics]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_package_version(self, feed_id, package_id, package_version_id, project=None, include_urls=None, is_listed=None, is_deleted=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if package_id is not None:\n            route_values['packageId'] = self._serialize.url('package_id', package_id, 'str')\n        if package_version_id is not None:\n            route_values['packageVersionId'] = self._serialize.url('package_version_id', package_version_id, 'str')\n        query_parameters = {}\n        if include_urls is not None:\n            query_parameters['includeUrls'] = self._serialize.query('include_urls', include_urls, 'bool')\n        if is_listed is not None:\n            query_parameters['isListed'] = self._serialize.query('is_listed', is_listed, 'bool')\n        if is_deleted is not None:\n            query_parameters['isDeleted'] = self._serialize.query('is_deleted', is_deleted, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='3b331909-6a86-44cc-b9ec-c1834c35498f',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('PackageVersion', response)", "response": "Get details about a specific package version."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_feed_view(self, view, feed_id, view_id, project=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        if view_id is not None:\n            route_values['viewId'] = self._serialize.url('view_id', view_id, 'str')\n        content = self._serialize.body(view, 'FeedView')\n        response = self._send(http_method='PATCH',\n                              location_id='42a8502a-6785-41bc-8c16-89477d930877',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('FeedView', response)", "response": "Update a feed view."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_approvals(self, project, assigned_to_filter=None, status_filter=None, release_ids_filter=None, type_filter=None, top=None, continuation_token=None, query_order=None, include_my_group_approvals=None):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if assigned_to_filter is not None:\n            query_parameters['assignedToFilter'] = self._serialize.query('assigned_to_filter', assigned_to_filter, 'str')\n        if status_filter is not None:\n            query_parameters['statusFilter'] = self._serialize.query('status_filter', status_filter, 'str')\n        if release_ids_filter is not None:\n            release_ids_filter = \",\".join(map(str, release_ids_filter))\n            query_parameters['releaseIdsFilter'] = self._serialize.query('release_ids_filter', release_ids_filter, 'str')\n        if type_filter is not None:\n            query_parameters['typeFilter'] = self._serialize.query('type_filter', type_filter, 'str')\n        if top is not None:\n            query_parameters['top'] = self._serialize.query('top', top, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'int')\n        if query_order is not None:\n            query_parameters['queryOrder'] = self._serialize.query('query_order', query_order, 'str')\n        if include_my_group_approvals is not None:\n            query_parameters['includeMyGroupApprovals'] = self._serialize.query('include_my_group_approvals', include_my_group_approvals, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='b47c6458-e73b-47cb-a770-4df1e8813a91',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[ReleaseApproval]', self._unwrap_collection(response))", "response": "Get a list of approvals for a specific release."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a release definition.", "response": "def get_release_definition(self, project, definition_id, property_filters=None):\n        \"\"\"GetReleaseDefinition.\n        [Preview API] Get a release definition.\n        :param str project: Project ID or project name\n        :param int definition_id: Id of the release definition.\n        :param [str] property_filters: A comma-delimited list of extended properties to be retrieved. If set, the returned Release Definition will contain values for the specified property Ids (if they exist). If not set, properties will not be included.\n        :rtype: :class:`<ReleaseDefinition> <azure.devops.v5_1.release.models.ReleaseDefinition>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        query_parameters = {}\n        if property_filters is not None:\n            property_filters = \",\".join(property_filters)\n            query_parameters['propertyFilters'] = self._serialize.query('property_filters', property_filters, 'str')\n        response = self._send(http_method='GET',\n                              location_id='d8f96f24-8ea7-4cb6-baab-2df8fc515665',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ReleaseDefinition', response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of release definitions for a given project.", "response": "def get_release_definitions(self, project, search_text=None, expand=None, artifact_type=None, artifact_source_id=None, top=None, continuation_token=None, query_order=None, path=None, is_exact_name_match=None, tag_filter=None, property_filters=None, definition_id_filter=None, is_deleted=None, search_text_contains_folder_name=None):\n        \"\"\"GetReleaseDefinitions.\n        [Preview API] Get a list of release definitions.\n        :param str project: Project ID or project name\n        :param str search_text: Get release definitions with names containing searchText.\n        :param str expand: The properties that should be expanded in the list of Release definitions.\n        :param str artifact_type: Release definitions with given artifactType will be returned. Values can be Build, Jenkins, GitHub, Nuget, Team Build (external), ExternalTFSBuild, Git, TFVC, ExternalTfsXamlBuild.\n        :param str artifact_source_id: Release definitions with given artifactSourceId will be returned. e.g. For build it would be {projectGuid}:{BuildDefinitionId}, for Jenkins it would be {JenkinsConnectionId}:{JenkinsDefinitionId}, for TfsOnPrem it would be {TfsOnPremConnectionId}:{ProjectName}:{TfsOnPremDefinitionId}. For third-party artifacts e.g. TeamCity, BitBucket you may refer 'uniqueSourceIdentifier' inside vss-extension.json at https://github.com/Microsoft/vsts-rm-extensions/blob/master/Extensions.\n        :param int top: Number of release definitions to get.\n        :param str continuation_token: Gets the release definitions after the continuation token provided.\n        :param str query_order: Gets the results in the defined order. Default is 'IdAscending'.\n        :param str path: Gets the release definitions under the specified path.\n        :param bool is_exact_name_match: 'true'to gets the release definitions with exact match as specified in searchText. Default is 'false'.\n        :param [str] tag_filter: A comma-delimited list of tags. Only release definitions with these tags will be returned.\n        :param [str] property_filters: A comma-delimited list of extended properties to be retrieved. If set, the returned Release Definitions will contain values for the specified property Ids (if they exist). If not set, properties will not be included. Note that this will not filter out any Release Definition from results irrespective of whether it has property set or not.\n        :param [str] definition_id_filter: A comma-delimited list of release definitions to retrieve.\n        :param bool is_deleted: 'true' to get release definitions that has been deleted. Default is 'false'\n        :param bool search_text_contains_folder_name: 'true' to get the release definitions under the folder with name as specified in searchText. Default is 'false'.\n        :rtype: [ReleaseDefinition]\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if search_text is not None:\n            query_parameters['searchText'] = self._serialize.query('search_text', search_text, 'str')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query('expand', expand, 'str')\n        if artifact_type is not None:\n            query_parameters['artifactType'] = self._serialize.query('artifact_type', artifact_type, 'str')\n        if artifact_source_id is not None:\n            query_parameters['artifactSourceId'] = self._serialize.query('artifact_source_id', artifact_source_id, 'str')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if continuation_token is not None:\n            query_parameters['continuationToken'] = self._serialize.query('continuation_token', continuation_token, 'str')\n        if query_order is not None:\n            query_parameters['queryOrder'] = self._serialize.query('query_order', query_order, 'str')\n        if path is not None:\n            query_parameters['path'] = self._serialize.query('path', path, 'str')\n        if is_exact_name_match is not None:\n            query_parameters['isExactNameMatch'] = self._serialize.query('is_exact_name_match', is_exact_name_match, 'bool')\n        if tag_filter is not None:\n            tag_filter = \",\".join(tag_filter)\n            query_parameters['tagFilter'] = self._serialize.query('tag_filter', tag_filter, 'str')\n        if property_filters is not None:\n            property_filters = \",\".join(property_filters)\n            query_parameters['propertyFilters'] = self._serialize.query('property_filters', property_filters, 'str')\n        if definition_id_filter is not None:\n            definition_id_filter = \",\".join(definition_id_filter)\n            query_parameters['definitionIdFilter'] = self._serialize.query('definition_id_filter', definition_id_filter, 'str')\n        if is_deleted is not None:\n            query_parameters['isDeleted'] = self._serialize.query('is_deleted', is_deleted, 'bool')\n        if search_text_contains_folder_name is not None:\n            query_parameters['searchTextContainsFolderName'] = self._serialize.query('search_text_contains_folder_name', search_text_contains_folder_name, 'bool')\n        response = self._send(http_method='GET',\n                              location_id='d8f96f24-8ea7-4cb6-baab-2df8fc515665',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[ReleaseDefinition]', self._unwrap_collection(response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate a release definition.", "response": "def update_release_definition(self, release_definition, project):\n        \"\"\"UpdateReleaseDefinition.\n        [Preview API] Update a release definition.\n        :param :class:`<ReleaseDefinition> <azure.devops.v5_1.release.models.ReleaseDefinition>` release_definition: Release definition object to update.\n        :param str project: Project ID or project name\n        :rtype: :class:`<ReleaseDefinition> <azure.devops.v5_1.release.models.ReleaseDefinition>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        content = self._serialize.body(release_definition, 'ReleaseDefinition')\n        response = self._send(http_method='PUT',\n                              location_id='d8f96f24-8ea7-4cb6-baab-2df8fc515665',\n                              version='5.1-preview.3',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ReleaseDefinition', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_release_environment(self, project, release_id, environment_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        if environment_id is not None:\n            route_values['environmentId'] = self._serialize.url('environment_id', environment_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='a7e426b1-03dc-48af-9dfe-c98bac612dcb',\n                              version='5.1-preview.6',\n                              route_values=route_values)\n        return self._deserialize('ReleaseEnvironment', response)", "response": "Get a release environment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_task_log(self, project, release_id, environment_id, release_deploy_phase_id, task_id, start_line=None, end_line=None, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        if environment_id is not None:\n            route_values['environmentId'] = self._serialize.url('environment_id', environment_id, 'int')\n        if release_deploy_phase_id is not None:\n            route_values['releaseDeployPhaseId'] = self._serialize.url('release_deploy_phase_id', release_deploy_phase_id, 'int')\n        if task_id is not None:\n            route_values['taskId'] = self._serialize.url('task_id', task_id, 'int')\n        query_parameters = {}\n        if start_line is not None:\n            query_parameters['startLine'] = self._serialize.query('start_line', start_line, 'long')\n        if end_line is not None:\n            query_parameters['endLine'] = self._serialize.query('end_line', end_line, 'long')\n        response = self._send(http_method='GET',\n                              location_id='17c91af7-09fd-4256-bff1-c24ee4f73bc0',\n                              version='5.1-preview.2',\n                              route_values=route_values,\n                              query_parameters=query_parameters,\n                              accept_media_type='text/plain')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get the task log of a release."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_manual_intervention(self, project, release_id, manual_intervention_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        if manual_intervention_id is not None:\n            route_values['manualInterventionId'] = self._serialize.url('manual_intervention_id', manual_intervention_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='616c46e4-f370-4456-adaa-fbaf79c7b79e',\n                              version='5.1-preview.1',\n                              route_values=route_values)\n        return self._deserialize('ManualIntervention', response)", "response": "Get manual intervention for a given release and manual intervention id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the metadata for a manual intervention.", "response": "def update_manual_intervention(self, manual_intervention_update_metadata, project, release_id, manual_intervention_id):\n        \"\"\"UpdateManualIntervention.\n        [Preview API] Update manual intervention.\n        :param :class:`<ManualInterventionUpdateMetadata> <azure.devops.v5_1.release.models.ManualInterventionUpdateMetadata>` manual_intervention_update_metadata: Meta data to update manual intervention.\n        :param str project: Project ID or project name\n        :param int release_id: Id of the release.\n        :param int manual_intervention_id: Id of the manual intervention.\n        :rtype: :class:`<ManualIntervention> <azure.devops.v5_1.release.models.ManualIntervention>`\n        \"\"\"\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        if manual_intervention_id is not None:\n            route_values['manualInterventionId'] = self._serialize.url('manual_intervention_id', manual_intervention_id, 'int')\n        content = self._serialize.body(manual_intervention_update_metadata, 'ManualInterventionUpdateMetadata')\n        response = self._send(http_method='PATCH',\n                              location_id='616c46e4-f370-4456-adaa-fbaf79c7b79e',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('ManualIntervention', response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_release(self, release, project, release_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        content = self._serialize.body(release, 'Release')\n        response = self._send(http_method='PUT',\n                              location_id='a166fde7-27ad-408e-ba75-703c2cc9d500',\n                              version='5.1-preview.8',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Release', response)", "response": "Update a complete release object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_release_resource(self, release_update_metadata, project, release_id):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if release_id is not None:\n            route_values['releaseId'] = self._serialize.url('release_id', release_id, 'int')\n        content = self._serialize.body(release_update_metadata, 'ReleaseUpdateMetadata')\n        response = self._send(http_method='PATCH',\n                              location_id='a166fde7-27ad-408e-ba75-703c2cc9d500',\n                              version='5.1-preview.8',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Release', response)", "response": "Update the few properties of a release."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_definition_revision(self, project, definition_id, revision, **kwargs):\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if definition_id is not None:\n            route_values['definitionId'] = self._serialize.url('definition_id', definition_id, 'int')\n        if revision is not None:\n            route_values['revision'] = self._serialize.url('revision', revision, 'int')\n        response = self._send(http_method='GET',\n                              location_id='258b82e0-9d41-43f3-86d6-fef14ddd44bc',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              accept_media_type='text/plain')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "response": "Get release definition for a given definitionId and revision."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a Reset command to reset the operation mode to 0.", "response": "def reset_mode(self):\n        \"\"\"Send a Reset command to set the operation mode to 0.\"\"\"\n        self.command(0x18, b\"\\x01\", timeout=0.1)\n        self.transport.write(Chipset.ACK)\n        time.sleep(0.010)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sense_tta(self, target):\n        target = super(Device, self).sense_tta(target)\n        if target and target.rid_res:\n            # This is a TT1 tag. Unfortunately we can only read it if\n            # it is a static memory tag. The RCS956 has implemented\n            # the same wrong command codes as PN531/2/3 and directly\n            # programming the CIU does not work.\n            if target.rid_res[0] >> 4 == 1 and target.rid_res[0] & 15 != 1:\n                msg = \"The {device} can not read this Type 1 Tag.\"\n                self.log.warning(msg.format(device=self))\n                return None\n        return target", "response": "Activate the RF field and probe for a Type A Target."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sense_ttb(self, target):\n        return super(Device, self).sense_ttb(target, did=b'\\x01')", "response": "Activate the RF field and probe for a Type B Target."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches for a DEP Target in active or passive communication mode.", "response": "def sense_dep(self, target):\n        \"\"\"Search for a DEP Target in active or passive communication mode.\n\n        \"\"\"\n        # Set timeout for PSL_RES and ATR_RES\n        self.chipset.rf_configuration(0x02, b\"\\x0B\\x0B\\x0A\")\n        return super(Device, self).sense_dep(target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef listen_tta(self, target, timeout):\n        if target.sel_res and target.sel_res[0] & 0x20:\n            info = \"{device} does not support listen as Type 4A Target\"\n            raise nfc.clf.UnsupportedTargetError(info.format(device=self))\n        return super(Device, self).listen_tta(target, timeout)", "response": "Listen at 106 kbps."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlisten for a DEP Target.", "response": "def listen_dep(self, target, timeout):\n        \"\"\"Listen *timeout* seconds to become initialized as a DEP Target.\n\n        The RC-S956 can be set to listen as a DEP Target for passive\n        communication mode. Target active communication mode is\n        disabled by the driver due to performance issues. It is also\n        not possible to fully control the ATR_RES response, only the\n        response waiting time (TO byte of ATR_RES) and the general\n        bytes can be set by the driver. Because the TO value must be\n        set before calling the hardware listen function, it can not be\n        different for the Type A of Type F passive initalization (the\n        driver uses the higher value if they are different).\n\n        \"\"\"\n        # The RCS956 internal state machine must be in Mode 0 before\n        # we enter the listen phase. Also the RFConfiguration command\n        # for setting the TO parameter won't work in any other mode.\n        self.chipset.reset_mode()\n\n        # Set the WaitForSelected bit in CIU_FelNFC2 register to\n        # prevent active mode activation. Target active mode is not\n        # really working with this device.\n        self.chipset.write_register(\"CIU_FelNFC2\", 0x80)\n\n        # We can not send ATR_RES as as a regular response but must\n        # use TgSetGeneralBytes to advance the chipset state machine\n        # to mode 3. Thus the ATR_RES is mostly determined by the\n        # firmware, we can only control the TO parameter for RWT, but\n        # must do it before the actual listen.\n        to = target.atr_res[15] & 0x0F\n        self.chipset.rf_configuration(0x82, bytearray([to, 2, to]))\n\n        # Disable automatic ATR_RES transmission. This must be done\n        # all again because the chipset reactivates the setting after\n        # ATR_RES was once send in TgSetGeneralBytes.\n        self.chipset.set_parameters(0b00001000)\n\n        # Now we can use the generic pn53x implementation\n        return super(Device, self).listen_dep(target, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self, recv_miu=248, recv_buf=2):\n        socket = nfc.llcp.Socket(self.llc, nfc.llcp.DATA_LINK_CONNECTION)\n        socket.setsockopt(nfc.llcp.SO_RCVBUF, recv_buf)\n        socket.setsockopt(nfc.llcp.SO_RCVMIU, recv_miu)\n        socket.connect(\"urn:nfc:sn:handover\")\n        server = socket.getpeername()\n        log.debug(\"handover client connected to remote sap {0}\".format(server))\n        self.socket = socket", "response": "Connect to the remote handover server if available. Raises nfc. llcp. ConnectRefused if the remote device does not have a handover service or the service does not accept any\n        more connections."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send(self, message):\n        log.debug(\"sending '{0}' message\".format(message.type))\n        send_miu = self.socket.getsockopt(nfc.llcp.SO_SNDMIU)\n        try:\n            data = str(message)\n        except nfc.llcp.EncodeError as e:\n            log.error(\"message encoding failed: {0}\".format(e))\n        else:\n            return self._send(data, send_miu)", "response": "Send a handover request message to the remote server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recv(self, timeout=None):\n        message = self._recv(timeout)\n        if message and message.type == \"urn:nfc:wkt:Hs\":\n            log.debug(\"received '{0}' message\".format(message.type))\n            return nfc.ndef.HandoverSelectMessage(message)\n        else:\n            log.error(\"received invalid message type {0}\".format(message.type))\n            return None", "response": "Receive a handover select message from the remote server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends the SAMConfiguration command to configure the Security Access Module.", "response": "def sam_configuration(self, mode, timeout=0):\n        \"\"\"Send the SAMConfiguration command to configure the Security Access\n        Module. The *mode* argument must be one of the string values\n        in :data:`sam_configuration_modes`. The *timeout* argument is\n        only relevant for the virtual card configuration mode.\n\n        \"\"\"\n        mode = self.sam_configuration_modes.index(mode) + 1\n        self.command(0x14, chr(mode) + chr(timeout), timeout=0.1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend the PowerDown command to put the PN531 into power down mode.", "response": "def power_down(self, wakeup_enable):\n        \"\"\"Send the PowerDown command to put the PN531 (including the\n        contactless analog front end) into power down mode in order to\n        save power consumption. The *wakeup_enable* argument must be a\n        list of wake up sources with values from the\n        :data:`power_down_wakeup_sources`.\n\n        \"\"\"\n        wakeup_set = 0\n        for i, src in enumerate(self.power_down_wakeup_sources):\n            if src in wakeup_enable:\n                wakeup_set |= 1 << i\n        data = self.command(0x16, chr(wakeup_set), timeout=0.1)\n        if data[0] != 0:\n            self.chipset_error(data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nactivate the RF field and probe for a Type A Target.", "response": "def sense_tta(self, target):\n        \"\"\"Activate the RF field and probe for a Type A Target.\n\n        The PN531 can discover some Type A Targets (Type 2 Tag and\n        Type 4A Tag) at 106 kbps. Type 1 Tags (Jewel/Topaz) are\n        completely unsupported. Because the firmware does not evaluate\n        the SENS_RES before sending SDD_REQ, it may be that a warning\n        message about missing Type 1 Tag support is logged even if a\n        Type 2 or 4A Tag was present. This typically happens when the\n        SDD_RES or SEL_RES are lost due to communication errors\n        (normally when the tag is moved away).\n\n        \"\"\"\n        target = super(Device, self).sense_tta(target)\n        if target and target.sdd_res and len(target.sdd_res) > 4:\n            # Remove the cascade tag(s) from SDD_RES, only the PN531\n            # has them included and we've set the policy that cascade\n            # tags are not part of the sel_req/sdd_res parameters.\n            if len(target.sdd_res) == 8:\n                target.sdd_res = target.sdd_res[1:]\n            elif len(target.sdd_res) == 12:\n                target.sdd_res = target.sdd_res[1:4] + target.sdd_res[5:]\n            # Also the SENS_RES bytes are reversed compared to PN532/533\n            target.sens_res = bytearray(reversed(target.sens_res))\n        return target"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sense_ttb(self, target):\n        info = \"{device} does not support sense for Type B Target\"\n        raise nfc.clf.UnsupportedTargetError(info.format(device=self))", "response": "Sense for a Type B Target is not supported."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches for a DEP Target in active communication mode.", "response": "def sense_dep(self, target):\n        \"\"\"Search for a DEP Target in active communication mode.\n\n        Because the PN531 does not implement the extended frame syntax\n        for host controller communication, it can not support the\n        maximum payload size of 254 byte. The driver handles this by\n        modifying the length-reduction values in atr_req and atr_res.\n\n        \"\"\"\n        if target.atr_req[15] & 0x30 == 0x30:\n            self.log.warning(\"must reduce the max payload size in atr_req\")\n            target.atr_req[15] = (target.atr_req[15] & 0xCF) | 0x20\n\n        target = super(Device, self).sense_dep(target)\n        if target is None:\n            return\n\n        if target.atr_res[16] & 0x30 == 0x30:\n            self.log.warning(\"must reduce the max payload size in atr_res\")\n            target.atr_res[16] = (target.atr_res[16] & 0xCF) | 0x20\n\n        return target"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlisten for a Type A activation at 106 kbps.", "response": "def listen_tta(self, target, timeout):\n        \"\"\"Listen *timeout* seconds for a Type A activation at 106 kbps. The\n        ``sens_res``, ``sdd_res``, and ``sel_res`` response data must\n        be provided and ``sdd_res`` must be a 4 byte UID that starts\n        with ``08h``. Depending on ``sel_res`` an activation may\n        return a target with a ``tt2_cmd``, ``tt4_cmd`` or ``atr_req``\n        attribute. The default RATS response sent for a Type 4 Tag\n        activation can be replaced with a ``rats_res`` attribute.\n\n        \"\"\"\n        return super(Device, self).listen_tta(target, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlistens for a Type F card activation.", "response": "def listen_ttf(self, target, timeout):\n        \"\"\"Listen *timeout* seconds for a Type F card activation. The target\n        ``brty`` must be set to either 212F or 424F and ``sensf_res``\n        provide 19 byte response data (response code + 8 byte IDm + 8\n        byte PMm + 2 byte system code). Note that the maximum command\n        an response frame length is 64 bytes only (including the frame\n        length byte), because the driver must directly program the\n        contactless interface unit within the PN533.\n\n        \"\"\"\n        return super(Device, self).listen_ttf(target, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef listen_dep(self, target, timeout):\n        return super(Device, self).listen_dep(target, timeout)", "response": "Listen to a DEP Target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef protect(self, password=None, read_protect=False, protect_from=0):\n        return super(NTAG203, self).protect(\n            password, read_protect, protect_from)", "response": "Set lock bits to disable future memory modifications."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the password protection bits for all memory pages of this NTAG21x family.", "response": "def protect(self, password=None, read_protect=False, protect_from=0):\n        \"\"\"Set password protection or permanent lock bits.\n\n        If the *password* argument is None, all memory pages will be\n        protected by setting the relevant lock bits (note that lock\n        bits can not be reset). If valid NDEF management data is\n        found, protect() also sets the NDEF write flag to read-only.\n\n        All Tags of the NTAG21x family can alternatively be protected\n        by password. If a *password* argument is provided, the\n        protect() method writes the first 4 byte of the *password*\n        string into the Tag's password (PWD) memory bytes and the\n        following 2 byte of the *password* string into the password\n        acknowledge (PACK) memory bytes. Factory default values are\n        used if the *password* argument is an empty string. Lock bits\n        are not set for password protection.\n\n        The *read_protect* and *protect_from* arguments are only\n        evaluated if *password* is not None. If *read_protect* is\n        True, the memory protection bit (PROT) is set to require\n        password verification also for reading of protected memory\n        pages. The value of *protect_from* determines the first\n        password protected memory page (one page is 4 byte) with the\n        exception that the smallest set value is page 3 even if\n        *protect_from* is smaller.\n\n        \"\"\"\n        args = (password, read_protect, protect_from)\n        return super(NTAG21x, self).protect(*args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to a local USB device identified by path and return a device instance.", "response": "def connect(path):\n    \"\"\"Connect to a local device identified by *path* and load the\n    appropriate device driver. The *path* argument is documented at\n    :meth:`nfc.clf.ContactlessFrontend.open`. The return value is\n    either a :class:`Device` instance or :const:`None`. Note that not\n    all drivers can be autodetected, specifically for serial devices\n    *path* must usually also specify the driver.\n\n    \"\"\"\n    assert isinstance(path, str) and len(path) > 0\n\n    found = transport.USB.find(path)\n    if found is not None:\n        for vid, pid, bus, dev in found:\n            module = usb_device_map.get((vid, pid))\n            if module is None:\n                continue\n\n            log.debug(\"loading {mod} driver for usb:{vid:04x}:{pid:04x}\"\n                      .format(mod=module, vid=vid, pid=pid))\n\n            if sys.platform.startswith(\"linux\"):\n                devnode = \"/dev/bus/usb/%03d/%03d\" % (int(bus), int(dev))\n                if not os.access(devnode, os.R_OK | os.W_OK):\n                    log.debug(\"access denied to \" + devnode)\n                    if len(path.split(':')) < 3:\n                        continue\n                    else:\n                        raise IOError(errno.EACCES, os.strerror(errno.EACCES))\n\n            driver = importlib.import_module(\"nfc.clf.\" + module)\n            try:\n                device = driver.init(transport.USB(bus, dev))\n            except IOError as error:\n                log.debug(error)\n                if len(path.split(':')) < 3:\n                    continue\n                else:\n                    raise error\n\n            device._path = \"usb:{0:03}:{1:03}\".format(int(bus), int(dev))\n            return device\n\n    found = transport.TTY.find(path)\n    if found is not None:\n        devices = found[0]\n        drivers = [found[1]] if found[1] else tty_driver_list\n        globbed = found[2] or drivers is tty_driver_list\n        for drv in drivers:\n            for dev in devices:\n                log.debug(\"trying {0} on {1}\".format(drv, dev))\n                driver = importlib.import_module(\"nfc.clf.\" + drv)\n                try:\n                    tty = transport.TTY(dev)\n                    device = driver.init(tty)\n                    device._path = dev\n                    return device\n                except IOError as error:\n                    log.debug(error)\n                    tty.close()\n                    if not globbed:\n                        raise\n\n    if path.startswith(\"udp\"):\n        path = path.split(':')\n        host = str(path[1]) if len(path) > 1 and path[1] else 'localhost'\n        port = int(path[2]) if len(path) > 2 and path[2] else 54321\n        driver = importlib.import_module(\"nfc.clf.udp\")\n        device = driver.init(host, port)\n        device._path = \"udp:{0}:{1}\".format(host, port)\n        return device"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmutes all existing communication", "response": "def mute(self):\n        \"\"\"Mutes all existing communication, most notably the device will no\n        longer generate a 13.56 MHz carrier signal when operating as\n        Initiator.\n\n        \"\"\"\n        fname = \"mute\"\n        cname = self.__class__.__module__ + '.' + self.__class__.__name__\n        raise NotImplementedError(\"%s.%s() is required\" % (cname, fname))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef listen_tta(self, target, timeout):\n        fname = \"listen_tta\"\n        cname = self.__class__.__module__ + '.' + self.__class__.__name__\n        raise NotImplementedError(\"%s.%s() is required\" % (cname, fname))", "response": "Listen as Type A Target."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a command to a remote target and receive a response.", "response": "def send_cmd_recv_rsp(self, target, data, timeout):\n        \"\"\"Exchange data with a remote Target\n\n        Sends command *data* to the remote *target* discovered in the\n        most recent call to one of the sense_xxx() methods. Note that\n        *target* becomes invalid with any call to mute(), sense_xxx()\n        or listen_xxx()\n\n        Arguments:\n\n          target (nfc.clf.RemoteTarget): The target returned by the\n            last successful call of a sense_xxx() method.\n\n          data (bytearray): The binary data to send to the remote\n            device.\n\n          timeout (float): The maximum number of seconds to wait for\n            response data from the remote device.\n\n        Returns:\n\n          bytearray: Response data received from the remote device.\n\n        Raises:\n\n          nfc.clf.CommunicationError: When no data was received.\n\n        \"\"\"\n        fname = \"send_cmd_recv_rsp\"\n        cname = self.__class__.__module__ + '.' + self.__class__.__name__\n        raise NotImplementedError(\"%s.%s() is required\" % (cname, fname))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the maximum number of data bytes acceptable for receiving with the given target.", "response": "def get_max_recv_data_size(self, target):\n        \"\"\"Returns the maximum number of data bytes for receiving.\n\n        The maximum number of data bytes acceptable for receiving with\n        either :meth:`send_cmd_recv_rsp` or :meth:`send_rsp_recv_cmd`.\n        The value reflects the local device capabilities for receiving\n        in the mode determined by *target*. It does not relate to any\n        protocol capabilities and negotiations.\n\n        Arguments:\n\n          target (nfc.clf.Target): The current local or remote\n            communication target.\n\n        Returns:\n\n          int: Maximum number of data bytes supported for receiving.\n\n        \"\"\"\n        fname = \"get_max_recv_data_size\"\n        cname = self.__class__.__module__ + '.' + self.__class__.__name__\n        raise NotImplementedError(\"%s.%s() is required\" % (cname, fname))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format(self, version=None, wipe=None):\n        return super(Type2Tag, self).format(version, wipe)", "response": "Erase the NDEF message on a Type 2 Tag."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprotect the tag against write access.", "response": "def protect(self, password=None, read_protect=False, protect_from=0):\n        \"\"\"Protect the tag against write access, i.e. make it read-only.\n\n        :meth:`Type2Tag.protect` switches an NFC Forum Type 2 Tag to\n        read-only state by setting all lock bits to 1. This operation\n        can not be reversed. If the tag is not an NFC Forum Tag,\n        i.e. it is not formatted with an NDEF Capability Container,\n        the :meth:`protect` method simply returns :const:`False`.\n\n        A generic Type 2 Tag can not be protected with a password. If\n        the *password* argument is provided, the :meth:`protect`\n        method does nothing else than return :const:`False`. The\n        *read_protect* and *protect_from* arguments are safely\n        ignored.\n\n        \"\"\"\n        return super(Type2Tag, self).protect(\n            password, read_protect, protect_from)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(self, page):\n        log.debug(\"read pages {0} to {1}\".format(page, page+3))\n\n        data = self.transceive(\"\\x30\"+chr(page % 256), timeout=0.005)\n\n        if len(data) == 1 and data[0] & 0xFA == 0x00:\n            log.debug(\"received nak response\")\n            self.target.sel_req = self.target.sdd_res[:]\n            self._target = self.clf.sense(self.target)\n            raise Type2TagCommandError(\n                INVALID_PAGE_ERROR if self.target else nfc.tag.RECEIVE_ERROR)\n\n        if len(data) != 16:\n            log.debug(\"invalid response \" + hexlify(data))\n            raise Type2TagCommandError(INVALID_RESPONSE_ERROR)\n\n        return data", "response": "Send a READ command to retrieve data from the tag."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, page, data):\n        if len(data) != 4:\n            raise ValueError(\"data must be a four byte string or array\")\n\n        log.debug(\"write {0} to page {1}\".format(hexlify(data), page))\n        rsp = self.transceive(\"\\xA2\" + chr(page % 256) + data)\n\n        if len(rsp) != 1:\n            log.debug(\"invalid response \" + hexlify(data))\n            raise Type2TagCommandError(INVALID_RESPONSE_ERROR)\n\n        if rsp[0] != 0x0A:  # NAK\n            log.debug(\"invalid page, received nak\")\n            raise Type2TagCommandError(INVALID_PAGE_ERROR)\n\n        return True", "response": "Send a WRITE command to store data on the tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sector_select(self, sector):\n        if sector != self._current_sector:\n            log.debug(\"select sector {0} (pages {1} to {2})\".format(\n                sector, sector << 10, ((sector+1) << 8) - 1))\n\n            sector_select_1 = b'\\xC2\\xFF'\n            sector_select_2 = pack('Bxxx', sector)\n\n            rsp = self.transceive(sector_select_1)\n            if len(rsp) == 1 and rsp[0] == 0x0A:\n                try:\n                    # command is passively ack'd, i.e. there's no response\n                    # and we must make sure there's no retries attempted\n                    self.transceive(sector_select_2, timeout=0.001, retries=0)\n                except Type2TagCommandError as error:\n                    assert int(error) == TIMEOUT_ERROR  # passive ack\n                else:\n                    log.debug(\"sector {0} does not exist\".format(sector))\n                    raise Type2TagCommandError(INVALID_SECTOR_ERROR)\n            else:\n                log.debug(\"sector select is not supported for this tag\")\n                raise Type2TagCommandError(INVALID_SECTOR_ERROR)\n\n            log.debug(\"sector {0} is now selected\".format(sector))\n            self._current_sector = sector\n        return self._current_sector", "response": "Send a SECTOR_SELECT command to switch the 1K address sector."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a Type 2 Tag command and receive the response. :meth:`transceive` is a type 2 tag specific wrapper around the :meth:`nfc.ContactlessFrontend.exchange` method. It can be used to send custom commands as a sequence of *data* bytes to the tag and receive the response data bytes. If *timeout* seconds pass without a response, the operation is aborted and :exc:`~nfc.tag.TagCommandError` raised with the TIMEOUT_ERROR error code. Command execution errors raise :exc:`Type2TagCommandError`.", "response": "def transceive(self, data, timeout=0.1, retries=2):\n        \"\"\"Send a Type 2 Tag command and receive the response.\n\n        :meth:`transceive` is a type 2 tag specific wrapper around the\n        :meth:`nfc.ContactlessFrontend.exchange` method. It can be\n        used to send custom commands as a sequence of *data* bytes to\n        the tag and receive the response data bytes. If *timeout*\n        seconds pass without a response, the operation is aborted and\n        :exc:`~nfc.tag.TagCommandError` raised with the TIMEOUT_ERROR\n        error code.\n\n        Command execution errors raise :exc:`Type2TagCommandError`.\n\n        \"\"\"\n        log.debug(\">> {0} ({1:f}s)\".format(hexlify(data), timeout))\n\n        if not self.target:\n            # Sometimes we have to (re)sense the target during\n            # communication. If that failed (tag gone) then any\n            # further attempt to transceive() is the same as\n            # \"unrecoverable timeout error\".\n            raise Type2TagCommandError(nfc.tag.TIMEOUT_ERROR)\n\n        started = time.time()\n        for retry in range(1 + retries):\n            try:\n                data = self.clf.exchange(data, timeout)\n                break\n            except nfc.clf.CommunicationError as error:\n                reason = error.__class__.__name__\n                log.debug(\"%s after %d retries\" % (reason, retry))\n        else:\n            if type(error) is nfc.clf.TimeoutError:\n                raise Type2TagCommandError(nfc.tag.TIMEOUT_ERROR)\n            if type(error) is nfc.clf.TransmissionError:\n                raise Type2TagCommandError(nfc.tag.RECEIVE_ERROR)\n            if type(error) is nfc.clf.ProtocolError:\n                raise Type2TagCommandError(nfc.tag.PROTOCOL_ERROR)\n            raise RuntimeError(\"unexpected \" + repr(error))\n\n        elapsed = time.time() - started\n        log.debug(\"<< {0} ({1:f}s)\".format(hexlify(data), elapsed))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setsockopt(self, option, value):\n        return self.llc.setsockopt(self._tco, option, value)", "response": "Set the value of the given socket option and return the current value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef accept(self):\n        socket = Socket(self._llc, None)\n        socket._tco = self.llc.accept(self._tco)\n        return socket", "response": "Accept a connection. The return value is a new\n        socket object usable to send and receive data on the new\n        connection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending data to the remote socket.", "response": "def send(self, data, flags=0):\n        \"\"\"Send data to the socket. The socket must be connected to a remote\n        socket. Returns a boolean value that indicates success or\n        failure. A false value is typically an indication that the\n        socket or connection was closed.\n\n        \"\"\"\n        return self.llc.send(self._tco, data, flags)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sendto(self, data, addr, flags=0):\n        return self.llc.sendto(self._tco, data, addr, flags)", "response": "Send data to the socket."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef poll(self, event, timeout=None):\n        return self.llc.poll(self._tco, event, timeout)", "response": "Wait for a socket event. Posssible event values are the strings recv send and acks. Posssible event values are the strings recv and send and acks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simple_pairing_hash(self):\n        try:\n            if len(self.eir[0x0E]) != 16:\n                raise DecodeError(\"wrong length of simple pairing hash\")\n            return bytearray(self.eir[0x0E])\n        except KeyError:\n            return None", "response": "Simple Pairing Hash C. Received and transmitted as EIR type\n        0x0E. Set to None."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists of complete 128 - bit UUIDs. Set and retrieved as a list of complete 128 - bit UUIDs. Decoded from and encoded as a list of complete 128 - bit UUIDs.", "response": "def service_class_uuid_list(self):\n        \"\"\"Listq of Service Class UUIDs. Set and retrieved as a list\n        of complete 128-bit UUIDs. Decoded from and encoded as EIR\n        types 0x02/0x03 (16-bit partial/complete UUIDs), 0x04/0x05\n        (32-bit partial/complete UUIDs), 0x06/0x07 (128-bit\n        partial/complete UUIDs).\"\"\"\n        L = list()\n        try: uuid_list = self.eir[0x03]\n        except KeyError: uuid_list = self.eir.get(0x02, '')\n        for x in struct.unpack(\"<\"+\"H\"*(len(uuid_list)/2), uuid_list):\n            L.append(\"{0:08x}-0000-1000-8000-00805f9b34fb\".format(x))\n        try: uuid_list = self.eir[0x05]\n        except KeyError: uuid_list = self.eir.get(0x04, '')\n        for x in struct.unpack(\"<\"+\"L\"*(len(uuid_list)/4), uuid_list):\n            L.append(\"{0:08x}-0000-1000-8000-00805f9b34fb\".format(x))\n        try: uuid_list = self.eir[0x07]\n        except KeyError: uuid_list = self.eir.get(0x06, '')\n        for i in range(0, len(uuid_list), 16):\n            L.append(str(UUID(bytes_le=uuid_list[i:i+16])))\n        return L"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_carrier(self, carrier_record, power_state, aux_data_records=None):\n        carrier = Carrier(carrier_record, power_state)\n        if aux_data_records is not None:\n            for aux in RecordList(aux_data_records):\n                carrier.auxiliary_data_records.append(aux)\n        self.carriers.append(carrier)", "response": "Add a new carrier to the handover request message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string with a formatted representation that might be considered pretty - printable.", "response": "def pretty(self, indent=0):\n        \"\"\"Returns a string with a formatted representation that might\n        be considered pretty-printable.\"\"\"\n        indent = indent * ' '\n        lines = list()\n        version_string = \"{v.major}.{v.minor}\".format(v=self.version)\n        lines.append((\"handover version\", version_string))\n        if self.error.reason:\n            lines.append((\"error reason\", self.error.reason))\n            lines.append((\"error value\", self.error.value))\n        for index, carrier in enumerate(self.carriers):\n            lines.append((\"carrier {0}:\".format(index+1),))\n            lines.append((indent + \"power state\", carrier.power_state))\n            if carrier.record.type == \"urn:nfc:wkt:Hc\":\n                carrier_type = carrier.record.carrier_type\n                carrier_data = carrier.record.carrier_data\n                lines.append((indent + \"carrier type\", carrier_type))\n                lines.append((indent + \"carrier data\", repr(carrier_data)))\n            else:\n                if carrier.type == \"application/vnd.bluetooth.ep.oob\":\n                    carrier_record = BluetoothConfigRecord(carrier.record)\n                elif carrier.type == \"application/vnd.wfa.wsc\":\n                    carrier_record = WifiConfigRecord(carrier.record)\n                else:\n                    carrier_record = carrier.record\n                lines.append((indent + \"carrier type\", carrier.type))\n                pretty_lines = carrier_record.pretty(2).split('\\n')\n                lines.extend([tuple(l.split(' = ')) for l in pretty_lines\n                              if not l.strip().startswith(\"identifier\")])\n            for record in carrier.auxiliary_data_records:\n                lines.append((indent + \"auxiliary data\",))\n                lines.append((2*indent + \"record type\", record.type))\n                lines.append((2*indent + \"record data\", repr(record.data)))\n\n        lwidth = max([len(line[0]) for line in lines])\n        lines = [(line[0].ljust(lwidth),) + line[1:] for line in lines]\n        lines = [\" = \".join(line) for line in lines]\n        return (\"\\n\").join([indent + line for line in lines])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef type(self):\n        return self.record.type if self.record.type != \"urn:nfc:wkt:Hc\" \\\n            else self.record.carrier_type", "response": "The alternative carrier type name equivalent to\n       ."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nactivate DEP communication with a target.", "response": "def activate(self, target=None, **options):\n        \"\"\"Activate DEP communication with a target.\"\"\"\n        log.debug(\"initiator options: {0}\".format(options))\n\n        self.did = options.get('did', None)\n        self.nad = options.get('nad', None)\n        self.gbi = options.get('gbi', '')[0:48]\n        self.brs = min(max(0, options.get('brs', 2)), 2)\n        self.lri = min(max(0, options.get('lri', 3)), 3)\n        if self._acm is None or 'acm' in options:\n            self._acm = bool(options.get('acm', True))\n\n        assert self.did is None or 0 <= self.did <= 255\n        assert self.nad is None or 0 <= self.nad <= 255\n\n        ppi = (self.lri << 4) | (bool(self.gbi) << 1) | int(bool(self.nad))\n        did = 0 if self.did is None else self.did\n        atr_req = ATR_REQ(os.urandom(10), did, 0, 0, ppi, self.gbi)\n        psl_req = PSL_REQ(did, (0, 9, 18)[self.brs], self.lri)\n        atr_res = psl_res = None\n        self.target = target\n\n        if self.target is None and self.acm is True:\n            log.debug(\"searching active communication mode target at 106A\")\n            tg = nfc.clf.RemoteTarget(\"106A\", atr_req=atr_req.encode())\n            try:\n                self.target = self.clf.sense(tg, iterations=2, interval=0.1)\n            except nfc.clf.UnsupportedTargetError:\n                self._acm = False\n            except nfc.clf.CommunicationError:\n                pass\n            else:\n                if self.target:\n                    atr_res = ATR_RES.decode(self.target.atr_res)\n                else:\n                    self._acm = None\n\n        if self.target is None:\n            log.debug(\"searching passive communication mode target at 106A\")\n            target = nfc.clf.RemoteTarget(\"106A\")\n            target = self.clf.sense(target, iterations=2, interval=0.1)\n            if target and target.sel_res and bool(target.sel_res[0] & 0x40):\n                self.target = target\n\n        if self.target is None and self.brs > 0:\n            log.debug(\"searching passive communication mode target at 212F\")\n            target = nfc.clf.RemoteTarget(\"212F\", sensf_req=b'\\0\\xFF\\xFF\\0\\0')\n            target = self.clf.sense(target, iterations=2, interval=0.1)\n            if target and target.sensf_res.startswith(b'\\1\\1\\xFE'):\n                atr_req.nfcid3 = target.sensf_res[1:9] + b'ST'\n                self.target = target\n\n        if self.target and self.target.atr_res is None:\n            try:\n                atr_res = self.send_req_recv_res(atr_req, 1.0)\n            except nfc.clf.CommunicationError:\n                pass\n            if atr_res is None:\n                log.debug(\"NFC-DEP Attribute Request failed\")\n                return None\n\n        if self.target and atr_res:\n            if self.brs > ('106A', '212F', '424F').index(self.target.brty):\n                try:\n                    psl_res = self.send_req_recv_res(psl_req, 0.1)\n                except nfc.clf.CommunicationError:\n                    pass\n                if psl_res is None:\n                    log.debug(\"NFC-DEP Parameter Selection failed\")\n                    return None\n                self.target.brty = ('212F', '424F')[self.brs-1]\n\n            self.rwt = (4096/13.56E6\n                        * 2**(atr_res.wt if atr_res.wt < 15 else 14))\n            self.miu = (atr_res.lr-3 - int(self.did is not None)\n                        - int(self.nad is not None))\n            self.gbt = atr_res.gb\n            self.pni = 0\n\n            log.info(\"running as \" + str(self))\n            return self.gbt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef activate(self, timeout=None, **options):\n\n        if timeout is None:\n            timeout = 1.0\n        gbt = options.get('gbt', '')[0:47]\n        lrt = min(max(0, options.get('lrt', 3)), 3)\n        rwt = min(max(0, options.get('rwt', 8)), 14)\n\n        pp = (lrt << 4) | (bool(gbt) << 1) | int(bool(self.nad))\n        nfcid3t = bytearray.fromhex(\"01FE\") + os.urandom(6) + \"ST\"\n        atr_res = ATR_RES(nfcid3t, 0, 0, 0, rwt, pp, gbt)\n        atr_res = atr_res.encode()\n\n        target = nfc.clf.LocalTarget(atr_res=atr_res)\n        target.sens_res = bytearray.fromhex(\"0101\")\n        target.sdd_res = bytearray.fromhex(\"08\") + os.urandom(3)\n        target.sel_res = bytearray.fromhex(\"40\")\n        target.sensf_res = bytearray.fromhex(\"01\") + nfcid3t[0:8]\n        target.sensf_res += bytearray.fromhex(\"00000000 00000000 FFFF\")\n\n        target = self.clf.listen(target, timeout)\n\n        if target and target.atr_req and target.dep_req:\n            log.debug(\"activated as \" + str(target))\n\n            atr_req = ATR_REQ.decode(target.atr_req)\n            self.lrt = lrt\n            self.gbt = gbt\n            self.gbi = atr_req.gb\n            self.miu = atr_req.lr - 3\n            self.rwt = 4096/13.56E6 * pow(2, rwt)\n            self.did = atr_req.did if atr_req.did > 0 else None\n            self.acm = not (target.sens_res or target.sensf_res)\n            self.cmd = chr(len(target.dep_req)+1) + target.dep_req\n            if target.brty == \"106A\":\n                self.cmd = b\"\\xF0\" + self.cmd\n            self.target = target\n\n            self.pcnt.rcvd[\"ATR\"] += 1\n            self.pcnt.sent[\"ATR\"] += 1\n            log.info(\"running as \" + str(self))\n\n            return self.gbi", "response": "Activate DEP communication as a target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef insert(self, i, record):\n        \n        if not isinstance(record, nfc.ndef.Record):\n            raise TypeError(\"an nfc.ndef.Record object is required\")\n        self._records.append(copy.copy(record))", "response": "Insert a record at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a message representation that might be considered pretty - printed.", "response": "def pretty(self):\n        \"\"\"Returns a message representation that might be considered\n        pretty-printable.\"\"\"\n        lines = list()\n        for index, record in enumerate(self._records):\n            lines.append((\"record {0}\".format(index+1),))\n            lines.append((\"  type\", repr(record.type)))\n            lines.append((\"  name\", repr(record.name)))\n            lines.append((\"  data\", repr(record.data)))\n        lwidth = max([len(line[0]) for line in lines])\n        lines = [(line[0].ljust(lwidth),) + line[1:] for line in lines]\n        lines = [\" = \".join(line) for line in lines]\n        return (\"\\n\").join([line for line in lines])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_cmd_recv_rsp(self, target, data, timeout):\n        return super(Device, self).send_cmd_recv_rsp(target, data, timeout)", "response": "Send a command to the remote device and return the response if received within the timeout."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwhile operating as target send response data to the remote device and return new command data if received within timeout seconds.", "response": "def send_rsp_recv_cmd(self, target, data, timeout):\n        \"\"\"While operating as *target* send response *data* to the remote\n        device and return new command data if received within\n        *timeout* seconds.\n\n        \"\"\"\n        return super(Device, self).send_rsp_recv_cmd(target, data, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat a Topaz tag for NDEF use.", "response": "def format(self, version=None, wipe=None):\n        \"\"\"Format a Topaz tag for NDEF use.\n\n        The implementation of :meth:`nfc.tag.Tag.format` for a Topaz\n        tag creates a capability container and an NDEF TLV with length\n        zero. Data bytes of the NDEF data area are left untouched\n        unless the wipe argument is set.\n\n        \"\"\"\n        return super(Topaz, self).format(version, wipe)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef protect(self, password=None, read_protect=False, protect_from=0):\n        return super(Topaz, self).protect(\n            password, read_protect, protect_from)", "response": "This method protects the memory of the tag - related data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting a Topaz - 512 tag for NDEF use.", "response": "def format(self, version=None, wipe=None):\n        \"\"\"Format a Topaz-512 tag for NDEF use.\n\n        The implementation of :meth:`nfc.tag.Tag.format` for a\n        Topaz-512 tag creates a capability container, a Lock Control\n        and a Memory Control TLV, and an NDEF TLV with length\n        zero. Data bytes of the NDEF data area are left untouched\n        unless the wipe argument is set.\n\n        \"\"\"\n        return super(Topaz512, self).format(version, wipe)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef protect(self, password=None, read_protect=False, protect_from=0):\n        return super(Type1Tag, self).protect(\n            password, read_protect, protect_from)", "response": "This method is used to protect the type 1 tag."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_all(self):\n        log.debug(\"read all static memory\")\n        cmd = \"\\x00\\x00\\x00\" + self.uid\n        return self.transceive(cmd)", "response": "Reads all static memory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a single byte from the memory area.", "response": "def read_byte(self, addr):\n        \"\"\"Read a single byte from static memory area (blocks 0-14).\n        \"\"\"\n        if addr < 0 or addr > 127:\n            raise ValueError(\"invalid byte address\")\n        log.debug(\"read byte at address {0} ({0:02X}h)\".format(addr))\n        cmd = \"\\x01\" + chr(addr) + \"\\x00\" + self.uid\n        return self.transceive(cmd)[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_block(self, block):\n        if block < 0 or block > 255:\n            raise ValueError(\"invalid block number\")\n        log.debug(\"read block {0}\".format(block))\n        cmd = \"\\x02\" + chr(block) + 8 * chr(0) + self.uid\n        return self.transceive(cmd)[1:9]", "response": "Read an 8 - byte data block at the specified address."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread one memory segment.", "response": "def read_segment(self, segment):\n        \"\"\"Read one memory segment (128 byte).\n        \"\"\"\n        log.debug(\"read segment {0}\".format(segment))\n        if segment < 0 or segment > 15:\n            raise ValueError(\"invalid segment number\")\n        cmd = \"\\x10\" + chr(segment << 4) + 8 * chr(0) + self.uid\n        rsp = self.transceive(cmd)\n        if len(rsp) < 129:\n            raise Type1TagCommandError(RESPONSE_ERROR)\n        return rsp[1:129]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a single byte to the static memory area.", "response": "def write_byte(self, addr, data, erase=True):\n        \"\"\"Write a single byte to static memory area (blocks 0-14). The\n        target byte is zero'd first if *erase* is True.\n\n        \"\"\"\n        if addr < 0 or addr >= 128:\n            raise ValueError(\"invalid byte address\")\n        log.debug(\"write byte at address {0} ({0:02X}h)\".format(addr))\n        cmd = \"\\x53\" if erase is True else \"\\x1A\"\n        cmd = cmd + chr(addr) + chr(data) + self.uid\n        return self.transceive(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites an 8 - byte data block at the current address.", "response": "def write_block(self, block, data, erase=True):\n        \"\"\"Write an 8-byte data block at address (block * 8). The target\n        bytes are zero'd first if *erase* is True.\n\n        \"\"\"\n        if block < 0 or block > 255:\n            raise ValueError(\"invalid block number\")\n        log.debug(\"write block {0}\".format(block))\n        cmd = \"\\x54\" if erase is True else \"\\x1B\"\n        cmd = cmd + chr(block) + data + self.uid\n        rsp = self.transceive(cmd)\n        if len(rsp) < 9:\n            raise Type1TagCommandError(RESPONSE_ERROR)\n        if erase is True and rsp[1:9] != data:\n            raise Type1TagCommandError(WRITE_ERROR)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef listen_tta(self, target, timeout):\n        info = \"{device} does not support listen as Type A Target\"\n        raise nfc.clf.UnsupportedTargetError(info.format(device=self))", "response": "Listen as Type A Target is not supported."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_buzzer_and_led_to_active(self, duration_in_ms=300):\n        duration_in_tenths_of_second = min(duration_in_ms / 100, 255)\n        timeout_in_seconds = (duration_in_tenths_of_second + 1) / 10.0\n        data = \"FF00400D04{:02X}000101\".format(duration_in_tenths_of_second)\n        self.ccid_xfr_block(bytearray.fromhex(data),\n                            timeout=timeout_in_seconds)", "response": "Turn on buzzer and LED to active."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencapsulating host command to be sent to the device and extract the chip response if received is within timeout seconds.", "response": "def ccid_xfr_block(self, data, timeout=0.1):\n        \"\"\"Encapsulate host command *data* into an PC/SC Escape command to\n        send to the device and extract the chip response if received\n        within *timeout* seconds.\n\n        \"\"\"\n        frame = struct.pack(\"<BI5B\", 0x6F, len(data), 0, 0, 0, 0, 0) + data\n        self.transport.write(bytearray(frame))\n        frame = self.transport.read(int(timeout * 1000))\n        if not frame or len(frame) < 10:\n            log.error(\"insufficient data for decoding ccid response\")\n            raise IOError(errno.EIO, os.strerror(errno.EIO))\n        if frame[0] != 0x80:\n            log.error(\"expected a RDR_to_PC_DataBlock\")\n            raise IOError(errno.EIO, os.strerror(errno.EIO))\n        # if len(frame) != 10 + struct.unpack(\"<I\", buffer(frame, 1, 4))[0]:\n        if len(frame) != 10 + struct.unpack(\"<I\", frame[1:5])[0]:\n            log.error(\"RDR_to_PC_DataBlock length mismatch\")\n            raise IOError(errno.EIO, os.strerror(errno.EIO))\n        return frame[10:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef command(self, cmd_code, cmd_data, timeout):\n        log.log(logging.DEBUG-1, self.CMD[cmd_code]+\" \"+hexlify(cmd_data))\n\n        frame = bytearray([0xD4, cmd_code]) + bytearray(cmd_data)\n        frame = bytearray([0xFF, 0x00, 0x00, 0x00, len(frame)]) + frame\n\n        frame = self.ccid_xfr_block(frame, timeout)\n        if not frame or len(frame) < 4:\n            log.error(\"insufficient data for decoding chip response\")\n            raise IOError(errno.EIO, os.strerror(errno.EIO))\n        if not (frame[0] == 0xD5 and frame[1] == cmd_code + 1):\n            log.error(\"received invalid chip response\")\n            raise IOError(errno.EIO, os.strerror(errno.EIO))\n        if not (frame[-2] == 0x90 and frame[-1] == 0x00):\n            log.error(\"received pseudo apdu with error status\")\n            raise IOError(errno.EIO, os.strerror(errno.EIO))\n        return frame[2:-2]", "response": "Send a command to the chip and return the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nerases the NDEF message on a Type 4 Tag.", "response": "def format(self, version=None, wipe=None):\n        \"\"\"Erase the NDEF message on a Type 4 Tag.\n\n        The :meth:`format` method writes the length of the NDEF\n        message on a Type 4 Tag to zero, thus the tag will appear to\n        be empty. If the *wipe* argument is set to some integer then\n        :meth:`format` will also overwrite all user data with that\n        integer (mod 256).\n\n        Despite it's name, the :meth:`format` method can not format a\n        blank tag to make it NDEF compatible; this requires\n        proprietary information from the manufacturer.\n\n        \"\"\"\n        return super(Type4Tag, self).format(version, wipe)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransmit arbitrary data and receive the response.", "response": "def transceive(self, data, timeout=None):\n        \"\"\"Transmit arbitrary data and receive the response.\n\n        This is a low level method to send arbitrary data to the\n        tag. While it should almost always be better to use\n        :meth:`send_apdu` this is the only way to force a specific\n        timeout value (which is otherwise derived from the Tag's\n        answer to select). The *timeout* value is expected as a float\n        specifying the seconds to wait.\n\n        \"\"\"\n        log.debug(\">> {0}\".format(hexlify(data)))\n        data = self._dep.exchange(data, timeout)\n        log.debug(\"<< {0}\".format(hexlify(data) if data else \"None\"))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_apdu(self, cla, ins, p1, p2, data=None, mrl=0, check_status=True):\n        apdu = bytearray([cla, ins, p1, p2])\n\n        if not self._extended_length_support:\n            if data and len(data) > 255:\n                raise ValueError(\"unsupported command data length\")\n            if mrl and mrl > 256:\n                raise ValueError(\"unsupported max response length\")\n            if data:\n                apdu += pack('>B', len(data)) + data\n            if mrl > 0:\n                apdu += pack('>B', 0 if mrl == 256 else mrl)\n        else:\n            if data and len(data) > 65535:\n                raise ValueError(\"invalid command data length\")\n            if mrl and mrl > 65536:\n                raise ValueError(\"invalid max response length\")\n            if data:\n                apdu += pack(\">xH\", len(data)) + data\n            if mrl > 0:\n                le = 0 if mrl == 65536 else mrl\n                apdu += pack(\">H\", le) if data else pack(\">xH\", le)\n\n        apdu = self.transceive(apdu)\n\n        if not apdu or len(apdu) < 2:\n            raise Type4TagCommandError(nfc.tag.PROTOCOL_ERROR)\n\n        if check_status and apdu[-2:] != b\"\\x90\\x00\":\n            raise Type4TagCommandError.from_status(apdu[-2:])\n\n        return apdu[:-2] if check_status else apdu", "response": "This method sends an ISO - IEC 7816 - 4 APDU to the Type 4 Tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ndef(self):\n        if self._ndef is None:\n            ndef = self.NDEF(self)\n            if ndef.has_changed:\n                self._ndef = ndef\n        return self._ndef", "response": "An object containing the NDEF object if found otherwise None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format(self, version=None, wipe=None):\n        if hasattr(self, \"_format\"):\n            args = \"version={0!r}, wipe={1!r}\"\n            args = args.format(version, wipe)\n            log.debug(\"format({0})\".format(args))\n            status = self._format(version, wipe)\n            if status is True:\n                self._ndef = None\n            return status\n        else:\n            log.debug(\"this tag can not be formatted with nfcpy\")\n            return None", "response": "Format the tag to make it NDEF compatible or erase content."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef protect(self, password=None, read_protect=False, protect_from=0):\n        if hasattr(self, \"_protect\"):\n            args = \"password={0!r}, read_protect={1!r}, protect_from={2!r}\"\n            args = args.format(password, read_protect, protect_from)\n            log.debug(\"protect({0})\".format(args))\n            status = self._protect(password, read_protect, protect_from)\n            if status is True:\n                self._ndef = None\n            return status\n        else:\n            log.error(\"this tag can not be protected with nfcpy\")\n            return None", "response": "Protect a tag against future write or read access."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authenticate(self, password):\n        if hasattr(self, \"_authenticate\"):\n            args = \"password={0!r}\".format(password)\n            log.debug(\"authenticate({0})\".format(args))\n            self._authenticated = self._authenticate(password)\n            if self._authenticated is True:\n                self._ndef = None\n            return self._authenticated\n        else:\n            log.error(\"this tag can not be authenticated with nfcpy\")\n            return None", "response": "Authenticate a tag with a password."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect(self, service_name):\n        self.close()\n        self.socket = nfc.llcp.Socket(self.llc, nfc.llcp.DATA_LINK_CONNECTION)\n        self.socket.connect(service_name)\n        self.send_miu = self.socket.getsockopt(nfc.llcp.SO_SNDMIU)", "response": "Connect to a specific service."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets an NDEF message from the server.", "response": "def get(self, request=None, timeout=1.0):\n        \"\"\"Get an NDEF message from the server. Temporarily connects\n        to the default SNEP server if the client is not yet connected.\n\n        .. deprecated:: 0.13\n           Use :meth:`get_records` or :meth:`get_octets`.\n\n        \"\"\"\n        if request is None:\n            request = nfc.ndef.Message(nfc.ndef.Record())\n\n        if not isinstance(request, nfc.ndef.Message):\n            raise TypeError(\"request type must be nfc.ndef.Message\")\n\n        response_data = self._get(request, timeout)\n\n        if response_data is not None:\n            try:\n                response = nfc.ndef.Message(response_data)\n            except Exception as error:\n                log.error(repr(error))\n            else:\n                return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets an NDEF message from the server.", "response": "def _get(self, ndef_message, timeout=1.0):\n        \"\"\"Get an NDEF message from the server. Temporarily connects\n        to the default SNEP server if the client is not yet connected.\n        \"\"\"\n        if not self.socket:\n            try:\n                self.connect('urn:nfc:sn:snep')\n            except nfc.llcp.ConnectRefused:\n                return None\n            else:\n                self.release_connection = True\n        else:\n            self.release_connection = False\n        try:\n            snep_request = b'\\x10\\x01'\n            snep_request += struct.pack('>L', 4 + len(str(ndef_message)))\n            snep_request += struct.pack('>L', self.acceptable_length)\n            snep_request += str(ndef_message)\n            if send_request(self.socket, snep_request, self.send_miu):\n                response = recv_response(\n                    self.socket, self.acceptable_length, timeout)\n                if response is not None:\n                    if response[1] != 0x81:\n                        raise SnepError(response[1])\n                    return response[6:]\n        finally:\n            if self.release_connection:\n                self.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_records(self, records=None, timeout=1.0):\n        octets = b''.join(ndef.message_encoder(records)) if records else None\n        octets = self.get_octets(octets, timeout)\n        if octets and len(octets) >= 3:\n            return list(ndef.message_decoder(octets))", "response": "Get NDEF message records from a SNEP Server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget NDEF message octets from a SNEP Server.", "response": "def get_octets(self, octets=None, timeout=1.0):\n        \"\"\"Get NDEF message octets from a SNEP Server.\n\n        .. versionadded:: 0.13\n\n        If the client has not yet a data link connection with a SNEP\n        Server, it temporarily connects to the default SNEP Server,\n        sends the message octets, disconnects after the server\n        response, and returns the received message octets.\n\n        \"\"\"\n        if octets is None:\n            # Send NDEF Message with one empty Record.\n            octets = b'\\xd0\\x00\\x00'\n\n        if not self.socket:\n            try:\n                self.connect('urn:nfc:sn:snep')\n            except nfc.llcp.ConnectRefused:\n                return None\n            else:\n                self.release_connection = True\n        else:\n            self.release_connection = False\n\n        try:\n            request = struct.pack('>BBLL', 0x10, 0x01, 4 + len(octets),\n                                  self.acceptable_length) + octets\n\n            if not send_request(self.socket, request, self.send_miu):\n                return None\n\n            response = recv_response(\n                self.socket, self.acceptable_length, timeout)\n\n            if response is not None:\n                if response[1] != 0x81:\n                    raise SnepError(response[1])\n\n                return response[6:]\n\n        finally:\n            if self.release_connection:\n                self.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend an NDEF message to the server. Temporarily connects to the default SNEP server.", "response": "def put(self, ndef_message, timeout=1.0):\n        \"\"\"Send an NDEF message to the server. Temporarily connects to\n        the default SNEP server if the client is not yet connected.\n\n        .. deprecated:: 0.13\n           Use :meth:`put_records` or :meth:`put_octets`.\n\n        \"\"\"\n        if not self.socket:\n            try:\n                self.connect('urn:nfc:sn:snep')\n            except nfc.llcp.ConnectRefused:\n                return False\n            else:\n                self.release_connection = True\n        else:\n            self.release_connection = False\n        try:\n            ndef_msgsize = struct.pack('>L', len(str(ndef_message)))\n            snep_request = b'\\x10\\x02' + ndef_msgsize + str(ndef_message)\n            if send_request(self.socket, snep_request, self.send_miu):\n                response = recv_response(self.socket, 0, timeout)\n                if response is not None:\n                    if response[1] != 0x81:\n                        raise SnepError(response[1])\n                    return True\n            return False\n        finally:\n            if self.release_connection:\n                self.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_records(self, records, timeout=1.0):\n        octets = b''.join(ndef.message_encoder(records))\n        return self.put_octets(octets, timeout)", "response": "Send NDEF message records to a SNEP Server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsenses for a Type A Target.", "response": "def sense_tta(self, target):\n        \"\"\"Sense for a Type A Target is supported for 106, 212 and 424\n        kbps. However, there may not be any target that understands the\n        activation commands in other than 106 kbps.\n\n        \"\"\"\n        log.debug(\"polling for NFC-A technology\")\n\n        if target.brty not in (\"106A\", \"212A\", \"424A\"):\n            message = \"unsupported bitrate {0}\".format(target.brty)\n            raise nfc.clf.UnsupportedTargetError(message)\n\n        self.chipset.in_set_rf(target.brty)\n        self.chipset.in_set_protocol(self.chipset.in_set_protocol_defaults)\n        self.chipset.in_set_protocol(initial_guard_time=6, add_crc=0,\n                                     check_crc=0, check_parity=1,\n                                     last_byte_bit_count=7)\n\n        sens_req = (target.sens_req if target.sens_req else\n                    bytearray.fromhex(\"26\"))\n\n        try:\n            sens_res = self.chipset.in_comm_rf(sens_req, 30)\n            if len(sens_res) != 2:\n                return None\n        except CommunicationError as error:\n            if error != \"RECEIVE_TIMEOUT_ERROR\":\n                log.debug(error)\n            return None\n\n        log.debug(\"rcvd SENS_RES \" + hexlify(sens_res))\n\n        if sens_res[0] & 0x1F == 0:\n            log.debug(\"type 1 tag target found\")\n            self.chipset.in_set_protocol(last_byte_bit_count=8, add_crc=2,\n                                         check_crc=2, type_1_tag_rrdd=2)\n            target = nfc.clf.RemoteTarget(target.brty, sens_res=sens_res)\n            if sens_res[1] & 0x0F == 0b1100:\n                rid_cmd = bytearray.fromhex(\"78 0000 00000000\")\n                log.debug(\"send RID_CMD \" + hexlify(rid_cmd))\n                try:\n                    target.rid_res = self.chipset.in_comm_rf(rid_cmd, 30)\n                except CommunicationError as error:\n                    log.debug(error)\n                    return None\n            return target\n\n        # other than type 1 tag\n        try:\n            self.chipset.in_set_protocol(last_byte_bit_count=8, add_parity=1)\n            if target.sel_req:\n                uid = target.sel_req\n                if len(uid) > 4:\n                    uid = b\"\\x88\" + uid\n                if len(uid) > 8:\n                    uid = uid[0:4] + b\"\\x88\" + uid[4:]\n                self.chipset.in_set_protocol(add_crc=1, check_crc=1)\n                for i, sel_cmd in zip(range(0, len(uid), 4), b\"\\x93\\x95\\x97\"):\n                    sel_req = sel_cmd + b\"\\x70\" + uid[i:i+4]\n                    sel_req.append(reduce(operator.xor, sel_req[2:6]))  # BCC\n                    log.debug(\"send SEL_REQ \" + hexlify(sel_req))\n                    sel_res = self.chipset.in_comm_rf(sel_req, 30)\n                    log.debug(\"rcvd SEL_RES \" + hexlify(sel_res))\n                uid = target.sel_req\n            else:\n                uid = bytearray()\n                for sel_cmd in b\"\\x93\\x95\\x97\":\n                    self.chipset.in_set_protocol(add_crc=0, check_crc=0)\n                    sdd_req = sel_cmd + b\"\\x20\"\n                    log.debug(\"send SDD_REQ \" + hexlify(sdd_req))\n                    sdd_res = self.chipset.in_comm_rf(sdd_req, 30)\n                    log.debug(\"rcvd SDD_RES \" + hexlify(sdd_res))\n                    self.chipset.in_set_protocol(add_crc=1, check_crc=1)\n                    sel_req = sel_cmd + b\"\\x70\" + sdd_res\n                    log.debug(\"send SEL_REQ \" + hexlify(sel_req))\n                    sel_res = self.chipset.in_comm_rf(sel_req, 30)\n                    log.debug(\"rcvd SEL_RES \" + hexlify(sel_res))\n                    if sel_res[0] & 0b00000100:\n                        uid = uid + sdd_res[1:4]\n                    else:\n                        uid = uid + sdd_res[0:4]\n                        break\n            if sel_res[0] & 0b00000100 == 0:\n                return nfc.clf.RemoteTarget(target.brty, sens_res=sens_res,\n                                            sel_res=sel_res, sdd_res=uid)\n        except CommunicationError as error:\n            log.debug(error)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sense_ttb(self, target):\n        log.debug(\"polling for NFC-B technology\")\n\n        if target.brty not in (\"106B\", \"212B\", \"424B\"):\n            message = \"unsupported bitrate {0}\".format(target.brty)\n            raise nfc.clf.UnsupportedTargetError(message)\n\n        self.chipset.in_set_rf(target.brty)\n        self.chipset.in_set_protocol(self.chipset.in_set_protocol_defaults)\n        self.chipset.in_set_protocol(initial_guard_time=20, add_sof=1,\n                                     check_sof=1, add_eof=1, check_eof=1)\n\n        sensb_req = (target.sensb_req if target.sensb_req else\n                     bytearray.fromhex(\"050010\"))\n\n        log.debug(\"send SENSB_REQ \" + hexlify(sensb_req))\n        try:\n            sensb_res = self.chipset.in_comm_rf(sensb_req, 30)\n        except CommunicationError as error:\n            if error != \"RECEIVE_TIMEOUT_ERROR\":\n                log.debug(error)\n            return None\n\n        if len(sensb_res) >= 12 and sensb_res[0] == 0x50:\n            log.debug(\"rcvd SENSB_RES \" + hexlify(sensb_res))\n            return nfc.clf.RemoteTarget(target.brty, sensb_res=sensb_res)", "response": "Sense for a Type B Target."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsense for a Type F Target.", "response": "def sense_ttf(self, target):\n        \"\"\"Sense for a Type F Target is supported for 212 and 424 kbps.\n\n        \"\"\"\n        log.debug(\"polling for NFC-F technology\")\n\n        if target.brty not in (\"212F\", \"424F\"):\n            message = \"unsupported bitrate {0}\".format(target.brty)\n            raise nfc.clf.UnsupportedTargetError(message)\n\n        self.chipset.in_set_rf(target.brty)\n        self.chipset.in_set_protocol(self.chipset.in_set_protocol_defaults)\n        self.chipset.in_set_protocol(initial_guard_time=24)\n\n        sensf_req = (target.sensf_req if target.sensf_req else\n                     bytearray.fromhex(\"00FFFF0100\"))\n\n        log.debug(\"send SENSF_REQ \" + hexlify(sensf_req))\n        try:\n            frame = chr(len(sensf_req)+1) + sensf_req\n            frame = self.chipset.in_comm_rf(frame, 10)\n        except CommunicationError as error:\n            if error != \"RECEIVE_TIMEOUT_ERROR\":\n                log.debug(error)\n            return None\n\n        if len(frame) >= 18 and frame[0] == len(frame) and frame[1] == 1:\n            log.debug(\"rcvd SENSF_RES \" + hexlify(frame[1:]))\n            return nfc.clf.RemoteTarget(target.brty, sensf_res=frame[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsense for an active DEP Target is not supported.", "response": "def sense_dep(self, target):\n        \"\"\"Sense for an active DEP Target is not supported. The device only\n        supports passive activation via sense_tta/sense_ttf.\n\n        \"\"\"\n        message = \"{device} does not support sense for active DEP Target\"\n        raise nfc.clf.UnsupportedTargetError(message.format(device=self))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlisten for Type A Target in 106 kbps.", "response": "def listen_tta(self, target, timeout):\n        \"\"\"Listen as Type A Target in 106 kbps.\n\n        Restrictions:\n\n        * It is not possible to send short frames that are required\n          for ACK and NAK responses. This means that a Type 2 Tag\n          emulation can only implement a single sector memory model.\n\n        * It can not be avoided that the chipset responds to SENSF_REQ\n          commands. The driver configures the SENSF_RES response to\n          all zero and ignores all Type F communication but eventually\n          it depends on the remote device whether Type A Target\n          activation will still be attempted.\n\n        \"\"\"\n        if not target.brty == '106A':\n            info = \"unsupported target bitrate: %r\" % target.brty\n            raise nfc.clf.UnsupportedTargetError(info)\n\n        if target.rid_res:\n            info = \"listening for type 1 tag activation is not supported\"\n            raise nfc.clf.UnsupportedTargetError(info)\n\n        if target.sens_res is None:\n            raise ValueError(\"sens_res is required\")\n        if target.sdd_res is None:\n            raise ValueError(\"sdd_res is required\")\n        if target.sel_res is None:\n            raise ValueError(\"sel_res is required\")\n        if len(target.sens_res) != 2:\n            raise ValueError(\"sens_res must be 2 byte\")\n        if len(target.sdd_res) != 4:\n            raise ValueError(\"sdd_res must be 4 byte\")\n        if len(target.sel_res) != 1:\n            raise ValueError(\"sel_res must be 1 byte\")\n        if target.sdd_res[0] != 0x08:\n            raise ValueError(\"sdd_res[0] must be 08h\")\n\n        nfca_params = target.sens_res + target.sdd_res[1:4] + target.sel_res\n        log.debug(\"nfca_params %s\", hexlify(nfca_params))\n\n        self.chipset.tg_set_rf(\"106A\")\n        self.chipset.tg_set_protocol(self.chipset.tg_set_protocol_defaults)\n        self.chipset.tg_set_protocol(rf_off_error=False)\n\n        time_to_return = time.time() + timeout\n        tg_comm_rf_args = {'mdaa': True, 'nfca_params': nfca_params}\n        tg_comm_rf_args['recv_timeout'] = min(int(1000 * timeout), 0xFFFF)\n\n        def listen_tta_tt2():\n            recv_timeout = tg_comm_rf_args['recv_timeout']\n            while recv_timeout > 0:\n                log.debug(\"wait %d ms for Type 2 Tag activation\", recv_timeout)\n                try:\n                    data = self.chipset.tg_comm_rf(**tg_comm_rf_args)\n                except CommunicationError as error:\n                    log.debug(error)\n                else:\n                    brty = ('106A', '212F', '424F')[data[0]-11]\n                    log.debug(\"%s rcvd %s\", brty, hexlify(buffer(data, 7)))\n                    if brty == \"106A\" and data[2] & 0x03 == 3:\n                        self.chipset.tg_set_protocol(rf_off_error=True)\n                        return nfc.clf.LocalTarget(\n                            \"106A\", sens_res=nfca_params[0:2],\n                            sdd_res=b'\\x08'+nfca_params[2:5],\n                            sel_res=nfca_params[5:6], tt2_cmd=data[7:])\n                    else:\n                        log.debug(\"not a 106A Type 2 Tag command\")\n                finally:\n                    recv_timeout = int(1000 * (time_to_return - time.time()))\n                    tg_comm_rf_args['recv_timeout'] = recv_timeout\n\n        def listen_tta_tt4():\n            rats_cmd = rats_res = None\n            recv_timeout = tg_comm_rf_args['recv_timeout']\n            while recv_timeout > 0:\n                log.debug(\"wait %d ms for 106A TT4 command\", recv_timeout)\n                try:\n                    data = self.chipset.tg_comm_rf(**tg_comm_rf_args)\n                    tg_comm_rf_args['transmit_data'] = None\n                except CommunicationError as error:\n                    tg_comm_rf_args['transmit_data'] = None\n                    rats_cmd = rats_res = None\n                    log.debug(error)\n                else:\n                    brty = ('106A', '212F', '424F')[data[0]-11]\n                    log.debug(\"%s rcvd %s\", brty, hexlify(buffer(data, 7)))\n                    if brty == \"106A\" and data[2] == 3 and data[7] == 0xE0:\n                        (rats_cmd, rats_res) = (data[7:], target.rats_res)\n                        log.debug(\"rcvd RATS_CMD %s\", hexlify(rats_cmd))\n                        if rats_res is None:\n                            rats_res = bytearray.fromhex(\"05 78 80 70 02\")\n                        log.debug(\"send RATS_RES %s\", hexlify(rats_res))\n                        tg_comm_rf_args['transmit_data'] = rats_res\n                    elif brty == \"106A\" and data[7] != 0xF0 and rats_cmd:\n                        did = rats_cmd[1] & 0x0F\n                        cmd = data[7:]\n                        ta_tb_tc = rats_res[2:]\n                        ta = ta_tb_tc.pop(0) if rats_res[1] & 0x10 else None\n                        tb = ta_tb_tc.pop(0) if rats_res[1] & 0x20 else None\n                        tc = ta_tb_tc.pop(0) if rats_res[1] & 0x40 else None\n                        if ta is not None:\n                            log.debug(\"TA(1) = {:08b}\".format(ta))\n                        if tb is not None:\n                            log.debug(\"TB(1) = {:08b}\".format(tb))\n                        if tc is not None:\n                            log.debug(\"TC(1) = {:08b}\".format(tc))\n                        if ta_tb_tc:\n                            log.debug(\"T({}) = {}\".format(\n                                len(ta_tb_tc), hexlify(ta_tb_tc)))\n                        did_supported = tc is None or bool(tc & 0x02)\n                        cmd_with_did = bool(cmd[0] & 0x08)\n                        if (((cmd_with_did and did_supported and cmd[1] == did)\n                             or (did == 0 and not cmd_with_did))):\n                            if cmd[0] in (0xC2, 0xCA):\n                                log.debug(\"rcvd S(DESELECT) %s\", hexlify(cmd))\n                                tg_comm_rf_args['transmit_data'] = cmd\n                                log.debug(\"send S(DESELECT) %s\", hexlify(cmd))\n                                rats_cmd = rats_res = None\n                            else:\n                                log.debug(\"rcvd TT4_CMD %s\", hexlify(cmd))\n                                self.chipset.tg_set_protocol(rf_off_error=True)\n                                return nfc.clf.LocalTarget(\n                                    \"106A\", sens_res=nfca_params[0:2],\n                                    sdd_res=b'\\x08'+nfca_params[2:5],\n                                    sel_res=nfca_params[5:6], tt4_cmd=cmd,\n                                    rats_cmd=rats_cmd, rats_res=rats_res)\n                        else:\n                            log.debug(\"skip TT4_CMD %s (DID)\", hexlify(cmd))\n                    else:\n                        log.debug(\"not a 106A TT4 command\")\n                finally:\n                    recv_timeout = int(1000 * (time_to_return - time.time()))\n                    tg_comm_rf_args['recv_timeout'] = recv_timeout\n\n        if target.sel_res[0] & 0x60 == 0x00:\n            return listen_tta_tt2()\n        if target.sel_res[0] & 0x20 == 0x20:\n            return listen_tta_tt4()\n\n        reason = \"sel_res does not indicate any tag target support\"\n        raise nfc.clf.UnsupportedTargetError(reason)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef listen_ttf(self, target, timeout):\n        if target.brty not in ('212F', '424F'):\n            info = \"unsupported target bitrate: %r\" % target.brty\n            raise nfc.clf.UnsupportedTargetError(info)\n\n        if target.sensf_res is None:\n            raise ValueError(\"sensf_res is required\")\n        if len(target.sensf_res) != 19:\n            raise ValueError(\"sensf_res must be 19 byte\")\n\n        self.chipset.tg_set_rf(target.brty)\n        self.chipset.tg_set_protocol(self.chipset.tg_set_protocol_defaults)\n        self.chipset.tg_set_protocol(rf_off_error=False)\n\n        recv_timeout = min(int(1000 * timeout), 0xFFFF)\n        time_to_return = time.time() + timeout\n        transmit_data = sensf_req = sensf_res = None\n\n        while recv_timeout > 0:\n            if transmit_data:\n                log.debug(\"%s send %s\", target.brty, hexlify(transmit_data))\n            log.debug(\"%s wait recv %d ms\", target.brty, recv_timeout)\n            try:\n                data = self.chipset.tg_comm_rf(recv_timeout=recv_timeout,\n                                               transmit_data=transmit_data)\n            except CommunicationError as error:\n                log.debug(error)\n                continue\n            finally:\n                recv_timeout = int((time_to_return - time.time()) * 1E3)\n                transmit_data = None\n\n            assert target.brty == ('106A', '212F', '424F')[data[0]-11]\n            log.debug(\"%s rcvd %s\", target.brty, hexlify(buffer(data, 7)))\n\n            if len(data) > 7 and len(data)-7 == data[7]:\n                if sensf_req and data[9:17] == target.sensf_res[1:9]:\n                    self.chipset.tg_set_protocol(rf_off_error=True)\n                    target = nfc.clf.LocalTarget(target.brty)\n                    target.sensf_req = sensf_req\n                    target.sensf_res = sensf_res\n                    target.tt3_cmd = data[8:]\n                    return target\n\n            if len(data) == 13 and data[7] == 6 and data[8] == 0:\n                (sensf_req, sensf_res) = (data[8:], target.sensf_res[:])\n                if (((sensf_req[1] == 255 or sensf_req[1] == sensf_res[17]) and\n                     (sensf_req[2] == 255 or sensf_req[2] == sensf_res[18]))):\n                    transmit_data = sensf_res[0:17]\n                    if sensf_req[3] == 1:\n                        transmit_data += sensf_res[17:19]\n                    if sensf_req[3] == 2:\n                        transmit_data += b\"\\x00\"\n                        transmit_data += chr(1 << (target.brty == \"424F\"))\n                    transmit_data = chr(len(transmit_data)+1) + transmit_data", "response": "Listen as Type F Target is supported for either 212 or 424 kbps."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrequests a service from the tag and return the key version.", "response": "def request_service(self, service_list):\n        \"\"\"Verify existence of a service (or area) and get the key version.\n\n        Each service (or area) to verify must be given as a\n        :class:`~nfc.tag.tt3.ServiceCode` in the iterable\n        *service_list*. The key versions are returned as a list of\n        16-bit integers, in the order requested. If a specified\n        service (or area) does not exist, the key version will be\n        0xFFFF.\n\n        Command execution errors raise :exc:`~nfc.tag.TagCommandError`.\n\n        \"\"\"\n        a, b, e = self.pmm[2] & 7, self.pmm[2] >> 3 & 7, self.pmm[2] >> 6\n        timeout = 302E-6 * ((b + 1) * len(service_list) + a + 1) * 4**e\n        pack = lambda x: x.pack()  # noqa: E731\n        data = chr(len(service_list)) + ''.join(map(pack, service_list))\n        data = self.send_cmd_recv_rsp(0x02, data, timeout, check_status=False)\n        if len(data) != 1 + len(service_list) * 2:\n            log.debug(\"insufficient data received from tag\")\n            raise tt3.Type3TagCommandError(tt3.DATA_SIZE_ERROR)\n        return [unpack(\"<H\", data[i:i+2])[0] for i in range(1, len(data), 2)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nverifies that a card is still present and get its operating mode.", "response": "def request_response(self):\n        \"\"\"Verify that a card is still present and get its operating mode.\n\n        The Request Response command returns the current operating\n        state of the card. The operating state changes with the\n        authentication process, a card is in Mode 0 after power-up or\n        a Polling command, transitions to Mode 1 with Authentication1,\n        to Mode 2 with Authentication2, and Mode 3 with any of the\n        card issuance commands. The :meth:`request_response` method\n        returns the mode as an integer.\n\n        Command execution errors raise\n        :exc:`~nfc.tag.TagCommandError`.\n\n        \"\"\"\n        a, b, e = self.pmm[3] & 7, self.pmm[3] >> 3 & 7, self.pmm[3] >> 6\n        timeout = 302E-6 * (b + 1 + a + 1) * 4**e\n        data = self.send_cmd_recv_rsp(0x04, '', timeout, check_status=False)\n        if len(data) != 1:\n            log.debug(\"insufficient data received from tag\")\n            raise tt3.Type3TagCommandError(tt3.DATA_SIZE_ERROR)\n        return data[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch for a service code that corresponds to an index.", "response": "def search_service_code(self, service_index):\n        \"\"\"Search for a service code that corresponds to an index.\n\n        The Search Service Code command provides access to the\n        iterable list of services and areas within the activated\n        system. The *service_index* argument may be any value from 0\n        to 0xffff. As long as there is a service or area found for a\n        given *service_index*, the information returned is a tuple\n        with either one or two 16-bit integer elements. Two integers\n        are returned for an area definition, the first is the area\n        code and the second is the largest possible service index for\n        the area. One integer, the service code, is returned for a\n        service definition. The return value is :const:`None` if the\n        *service_index* was not found.\n\n        For example, to print all services and areas of the active\n        system: ::\n\n            for i in xrange(0x10000):\n                area_or_service = tag.search_service_code(i)\n                if area_or_service is None:\n                    break\n                elif len(area_or_service) == 1:\n                    sc = area_or_service[0]\n                    print(nfc.tag.tt3.ServiceCode(sc >> 6, sc & 0x3f))\n                elif len(area_or_service) == 2:\n                    area_code, area_last = area_or_service\n                    print(\"Area {0:04x}--{0:04x}\".format(area_code, area_last))\n\n        Command execution errors raise :exc:`~nfc.tag.TagCommandError`.\n\n        \"\"\"\n        log.debug(\"search service code index {0}\".format(service_index))\n        # The maximum response time is given by the value of PMM[3].\n        # Some cards (like RC-S860 with IC RC-S915) encode a value\n        # that is too short, thus we use at lest 2 ms.\n        a, e = self.pmm[3] & 7, self.pmm[3] >> 6\n        timeout = max(302E-6 * (a + 1) * 4**e, 0.002)\n        data = pack(\"<H\", service_index)\n        data = self.send_cmd_recv_rsp(0x0A, data, timeout, check_status=False)\n        if data != \"\\xFF\\xFF\":\n            unpack_format = \"<H\" if len(data) == 2 else \"<HH\"\n            return unpack(unpack_format, data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request_system_code(self):\n        log.debug(\"request system code list\")\n        a, e = self.pmm[3] & 7, self.pmm[3] >> 6\n        timeout = max(302E-6 * (a + 1) * 4**e, 0.002)\n        data = self.send_cmd_recv_rsp(0x0C, '', timeout, check_status=False)\n        if len(data) != 1 + data[0] * 2:\n            log.debug(\"insufficient data received from tag\")\n            raise tt3.Type3TagCommandError(tt3.DATA_SIZE_ERROR)\n        return [unpack(\">H\", data[i:i+2])[0] for i in range(1, len(data), 2)]", "response": "Request the system codes that are registered in the card."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprotect a FeliCa Lite Tag.", "response": "def protect(self, password=None, read_protect=False, protect_from=0):\n        \"\"\"Protect a FeliCa Lite Tag.\n\n        A FeliCa Lite Tag can be provisioned with a custom password\n        (or the default manufacturer key if the password is an empty\n        string or bytearray) to ensure that data retrieved by future\n        read operations, after authentication, is genuine. Read\n        protection is not supported.\n\n        A non-empty *password* must provide at least 128 bit key\n        material, in other words it must be a string or bytearray of\n        length 16 or more.\n\n        The memory unit for the value of *protect_from* is 16 byte,\n        thus with ``protect_from=2`` bytes 0 to 31 are not protected.\n        If *protect_from* is zero (the default value) and the Tag has\n        valid NDEF management data, the NDEF RW Flag is set to read\n        only.\n\n        \"\"\"\n        return super(FelicaLite, self).protect(\n            password, read_protect, protect_from)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting a FeliCa Lite Tag for NDEF.", "response": "def format(self, version=0x10, wipe=None):\n        \"\"\"Format a FeliCa Lite Tag for NDEF.\n\n        \"\"\"\n        return super(FelicaLite, self).format(version, wipe)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_without_mac(self, *blocks):\n        log.debug(\"read {0} block(s) without mac\".format(len(blocks)))\n        service_list = [tt3.ServiceCode(0, 0b001011)]\n        block_list = [tt3.BlockCode(n) for n in blocks]\n        return self.read_without_encryption(service_list, block_list)", "response": "Read a number of data blocks without integrity check."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_with_mac(self, *blocks):\n        log.debug(\"read {0} block(s) with mac\".format(len(blocks)))\n\n        if self._sk is None or self._iv is None:\n            raise RuntimeError(\"authentication required\")\n\n        service_list = [tt3.ServiceCode(0, 0b001011)]\n        block_list = [tt3.BlockCode(n) for n in blocks]\n        block_list.append(tt3.BlockCode(0x81))\n\n        data = self.read_without_encryption(service_list, block_list)\n        data, mac = data[0:-16], data[-16:-8]\n        if mac != self.generate_mac(data, self._sk, self._iv):\n            log.warning(\"mac verification failed\")\n        else:\n            return data", "response": "This method reads a number of data blocks with integrity check."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a 16 - byte string or bytearray to the numberedCOOKIE.", "response": "def write_without_mac(self, data, block):\n        \"\"\"Write a data block without integrity check.\n\n        This is the standard write method for a FeliCa Lite. The\n        16-byte string or bytearray *data* is written to the numbered\n        *block* in service 0x0009 (NDEF write service). ::\n\n            data = bytearray(range(16)) # 0x00, 0x01, ... 0x0F\n            try: tag.write_without_mac(data, 5) # write block 5\n            except nfc.tag.TagCommandError:\n                print(\"something went wrong\")\n\n        Tag command errors raise :exc:`~nfc.tag.TagCommandError`.\n\n        \"\"\"\n        # Write a single data block without a mac. Write with mac is\n        # only supported by FeliCa Lite-S.\n        assert len(data) == 16 and type(block) is int\n        log.debug(\"write 1 block without mac\".format())\n        sc_list = [tt3.ServiceCode(0, 0b001001)]\n        bc_list = [tt3.BlockCode(block)]\n        self.write_without_encryption(sc_list, bc_list, data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef authenticate(self, password):\n        if super(FelicaLiteS, self).authenticate(password):\n            # At this point we have achieved internal authentication,\n            # i.e we know that the tag has the same card key as in\n            # password. We now reset the authentication status and do\n            # external authentication to assure the tag that we have\n            # the right card key.\n            self._authenticated = False\n            self.read_from_ndef_service = self.read_without_mac\n            self.write_to_ndef_service = self.write_without_mac\n\n            # To authenticate to the tag we write a 01h into the\n            # ext_auth byte of the state block (block 0x92). The other\n            # bytes of the state block can be all set to zero.\n            self.write_with_mac(\"\\x01\" + 15*\"\\0\", 0x92)\n\n            # Now read the state block and check the value of the\n            # ext_auth to see if we are authenticated. If it's 01h\n            # then we are, otherwise not.\n            if self.read_with_mac(0x92)[0] == 0x01:\n                log.debug(\"mutual authentication completed\")\n                self._authenticated = True\n                self.read_from_ndef_service = self.read_with_mac\n                self.write_to_ndef_service = self.write_with_mac\n            else:\n                log.debug(\"mutual authentication failed\")\n\n        return self._authenticated", "response": "Mutually authenticate with a FeliCa Lite - S Tag."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_with_mac(self, data, block):\n        # Write a single data block protected with a mac. The card\n        # will only accept the write if it computed the same mac.\n        log.debug(\"write 1 block with mac\")\n        if len(data) != 16:\n            raise ValueError(\"data must be 16 octets\")\n        if type(block) is not int:\n            raise ValueError(\"block number must be int\")\n        if self._sk is None or self._iv is None:\n            raise RuntimeError(\"tag must be authenticated first\")\n\n        # The write count is the first three byte of the wcnt block.\n        wcnt = str(self.read_without_mac(0x90)[0:3])\n        log.debug(\"write count is 0x{0}\".format(wcnt[::-1].encode(\"hex\")))\n\n        # We must generate the mac_a block to write the data. The data\n        # to encrypt to the mac is composed of write count and block\n        # numbers (8 byte) and the data we want to write. The mac for\n        # write must be generated with the key flipped (sk2 || sk1).\n        def flip(sk):\n            return sk[8:16] + sk[0:8]\n\n        data = wcnt + \"\\x00\" + chr(block) + \"\\x00\\x91\\x00\" + data\n        maca = self.generate_mac(data, flip(self._sk), self._iv) + wcnt+5*\"\\0\"\n\n        # Now we can write the data block with our computed mac to the\n        # desired block and the maca block. Write without encryption\n        # means that the data is not encrypted with a service key.\n        sc_list = [tt3.ServiceCode(0, 0b001001)]\n        bc_list = [tt3.BlockCode(block), tt3.BlockCode(0x91)]\n        self.write_without_encryption(sc_list, bc_list, data[8:24] + maca)", "response": "Write a single data block with additional integrity check."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse an NDEF record from a file - like object.", "response": "def _read(self, f):\n        \"\"\"Parse an NDEF record from a file-like object.\"\"\"\n        \n        try:\n            self.header = ord(f.read(1))\n        except TypeError:\n            log.debug(\"buffer underflow at offset {0}\".format(f.tell()))\n            raise LengthError(\"insufficient data to parse\")\n        \n        mbf = bool(self.header & 0x80)\n        mef = bool(self.header & 0x40)\n        cff = bool(self.header & 0x20)\n        srf = bool(self.header & 0x10)\n        ilf = bool(self.header & 0x08)\n        tnf = self.header & 0x07\n\n        try:\n            type_length = ord(f.read(1))\n            if srf: # short record\n                data_length = ord(f.read(1))\n            else: # 32-bit length\n                data_length = struct.unpack('>L', f.read(4))[0]\n            if ilf: # id length present\n                name_length = ord(f.read(1))\n            else:\n                name_length = 0\n        except (TypeError, struct.error):\n            log.debug(\"buffer underflow at offset {0}\".format(f.tell()))\n            raise LengthError(\"insufficient data to parse\")\n\n        try:\n            record_type = f.read(type_length)\n            assert len(record_type) == type_length\n            record_name = f.read(name_length)\n            assert len(record_name) == name_length\n            record_data = f.read(data_length)\n            assert len(record_data) == data_length\n        except AssertionError:\n            log.debug(\"buffer underflow at offset {0}\".format(f.tell()))\n            raise LengthError(\"insufficient data to parse\")\n\n        if tnf in (0, 5, 6) and len(record_type) > 0:\n            s = \"ndef type name format {0} doesn't allow a type string\"\n            raise FormatError( s.format(tnf) )\n        if tnf in (1, 2, 3, 4) and len(record_type) == 0:\n            s = \"ndef type name format {0} requires a type string\"\n            raise FormatError( s.format(tnf) )\n        if tnf == 0 and len(record_data) > 0:\n            s = \"ndef type name format {0} doesn't allow a payload\"\n            raise FormatError( s.format(tnf) )\n            \n        self._message_begin, self._message_end = mbf, mef\n        self._type = bytearray(type_name_prefix[tnf] + record_type)\n        self._name = bytearray(record_name)\n        self._data = bytearray(record_data)\n        log.debug(\"parsed {0}\".format(repr(self)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserializing an NDEF record to a file - like object.", "response": "def _write(self, f):\n        \"\"\"Serialize an NDEF record to a file-like object.\"\"\"\n        log.debug(\"writing ndef record at offset {0}\".format(f.tell()))\n\n        record_type = self.type\n        record_name = self.name\n        record_data = self.data\n        \n        if record_type == '':\n            header_flags = 0; record_name = ''; record_data = ''\n        elif record_type.startswith(\"urn:nfc:wkt:\"):\n            header_flags = 1; record_type = record_type[12:]\n        elif re.match(r'[a-zA-Z0-9-]+/[a-zA-Z0-9-+.]+', record_type):\n            header_flags = 2; record_type = record_type\n        elif re.match(r'[a-zA-Z][a-zA-Z0-9+-.]*://', record_type):\n            header_flags = 3; record_type = record_type\n        elif record_type.startswith(\"urn:nfc:ext:\"):\n            header_flags = 4; record_type = record_type[12:]\n        elif record_type == 'unknown':\n            header_flags = 5; record_type = ''\n        elif record_type == 'unchanged':\n            header_flags = 6; record_type = ''\n\n        type_length = len(record_type)\n        data_length = len(record_data)\n        name_length = len(record_name)\n\n        if self._message_begin:\n            header_flags |= 0x80\n        if self._message_end:\n            header_flags |= 0x40\n        if data_length < 256:\n            header_flags |= 0x10\n        if name_length > 0:\n            header_flags |= 0x08\n\n        if data_length < 256:\n            f.write(struct.pack(\">BBB\", header_flags, type_length, data_length))\n        else:\n            f.write(struct.pack(\">BBL\", header_flags, type_length, data_length))\n        if name_length > 0:\n            f.write(struct.pack(\">B\", name_length))\n\n        f.write(record_type)\n        f.write(record_name)\n        f.write(record_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pack(self):\n        sn, sa = self.number, self.attribute\n        return pack(\"<H\", (sn & 0x3ff) << 6 | (sa & 0x3f))", "response": "Pack the service code for transmission. Returns a 2 byte string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pack(self):\n        bn, am, sx = self.number, self.access, self.service\n        return chr(bool(bn < 256) << 7 | (am & 0x7) << 4 | (sx & 0xf)) + \\\n            (chr(bn) if bn < 256 else pack(\"<H\", bn))", "response": "Pack the block code for transmission. Returns a 2 - 3 byte string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps all data blocks of a given service.", "response": "def dump_service(self, sc):\n        \"\"\"Read all data blocks of a given service.\n\n        :meth:`dump_service` reads all data blocks from the service\n        with service code *sc* and returns a list of strings suitable\n        for printing. The number of strings returned does not\n        necessarily reflect the number of data blocks because a range\n        of data blocks with equal content is reduced to fewer lines of\n        output.\n\n        \"\"\"\n        def lprint(fmt, data, index):\n            ispchr = lambda x: x >= 32 and x <= 126  # noqa: E731\n\n            def print_bytes(octets):\n                return ' '.join(['%02x' % x for x in octets])\n\n            def print_chars(octets):\n                return ''.join([chr(x) if ispchr(x) else '.' for x in octets])\n\n            return fmt.format(index, print_bytes(data), print_chars(data))\n\n        data_line_fmt = \"{0:04X}: {1} |{2}|\"\n        same_line_fmt = \"{0:<4s}  {1} |{2}|\"\n\n        lines = list()\n        last_data = None\n        same_data = 0\n\n        for i in itertools.count():  # pragma: no branch\n            assert i < 0x10000\n            try:\n                this_data = self.read_without_encryption([sc], [BlockCode(i)])\n            except Type3TagCommandError:\n                i = i - 1\n                break\n\n            if this_data == last_data:\n                same_data += 1\n            else:\n                if same_data > 1:\n                    lines.append(lprint(same_line_fmt, last_data, \"*\"))\n                lines.append(lprint(data_line_fmt, this_data, i))\n                last_data = this_data\n                same_data = 0\n\n        if same_data > 1:\n            lines.append(lprint(same_line_fmt, last_data, \"*\"))\n        if same_data > 0:\n            lines.append(lprint(data_line_fmt, this_data, i))\n\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format(self, version=None, wipe=None):\n        return super(Type3Tag, self).format(version, wipe)", "response": "Format and blank an NDEF Forum Type 3 Tag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef polling(self, system_code=0xffff, request_code=0, time_slots=0):\n\n        log.debug(\"polling for system 0x{0:04x}\".format(system_code))\n        if time_slots not in (0, 1, 3, 7, 15):\n            log.debug(\"invalid number of time slots: {0}\".format(time_slots))\n            raise ValueError(\"invalid number of time slots\")\n        if request_code not in (0, 1, 2):\n            log.debug(\"invalid request code value: {0}\".format(request_code))\n            raise ValueError(\"invalid request code for polling\")\n\n        timeout = 0.003625 + time_slots * 0.001208\n        data = pack(\">HBB\", system_code, request_code, time_slots)\n        data = self.send_cmd_recv_rsp(0x00, data, timeout, send_idm=False)\n        if len(data) != (16 if request_code == 0 else 18):\n            log.debug(\"unexpected polling response length\")\n            raise Type3TagCommandError(DATA_SIZE_ERROR)\n\n        return (data[0:8], data[8:16]) if len(data) == 16 else \\\n            (data[0:8], data[8:16], data[16:18])", "response": "Aquire and identify a card and return the appropriate data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_from_ndef_service(self, *blocks):\n        if self.sys == 0x12FC:\n            sc_list = [ServiceCode(0, 0b001011)]\n            bc_list = [BlockCode(n) for n in blocks]\n            return self.read_without_encryption(sc_list, bc_list)", "response": "Read block data from an NDEF compatible tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_without_encryption(self, service_list, block_list, data):\n        a, b, e = self.pmm[6] & 7, self.pmm[6] >> 3 & 7, self.pmm[6] >> 6\n        timeout = 302.1E-6 * ((b + 1) * len(block_list) + a + 1) * 4**e\n\n        data = (chr(len(service_list))\n                + ''.join([sc.pack() for sc in service_list])\n                + chr(len(block_list))\n                + ''.join([bc.pack() for bc in block_list])\n                + data)\n\n        log.debug(\"write w/o encryption service/block list: {0} / {1}\".format(\n            ' '.join([hexlify(sc.pack()) for sc in service_list]),\n            ' '.join([hexlify(bc.pack()) for bc in block_list])))\n\n        self.send_cmd_recv_rsp(0x08, data, timeout)", "response": "This method writes data blocks to unencrypted services."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites block data to an NDEF compatible tag.", "response": "def write_to_ndef_service(self, data, *blocks):\n        \"\"\"Write block data to an NDEF compatible tag.\n\n        This is a convinience method to write block data to a tag that\n        has system code 0x12FC (NDEF). For other tags this method\n        simply does nothing. The *data* to write must be a string or\n        bytearray with length equal ``16 * len(blocks)``. All\n        parameters following *data* are interpreted as block numbers\n        to write. To actually pass a list of block numbers requires\n        unpacking. The following example calls would have the same\n        effect of writing 32 byte zeros into blocks 1 and 8.::\n\n            tag.write_to_ndef_service(32 * \"\\\\0\", 1, 8)\n            tag.write_to_ndef_service(32 * \"\\\\0\", *list(1, 8))\n\n        Command execution errors raise :exc:`~nfc.tag.TagCommandError`.\n\n        \"\"\"\n        if self.sys == 0x12FC:\n            sc_list = [ServiceCode(0, 0b001001)]\n            bc_list = [BlockCode(n) for n in blocks]\n            self.write_without_encryption(sc_list, bc_list, data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_cmd_recv_rsp(self, cmd_code, cmd_data, timeout,\n                          send_idm=True, check_status=True):\n        \"\"\"Send a command and receive a response.\n\n        This low level method sends an arbitrary command with the\n        8-bit integer *cmd_code*, followed by the captured tag\n        identifier (IDm) if *send_idm* is :const:`True` and the byte\n        string or bytearray *cmd_data*. It then waits *timeout*\n        seconds for a response, verifies that the response is\n        correctly formatted and, if *check_status* is :const:`True`,\n        that the status flags do not indicate an error.\n\n        All errors raise a :exc:`~nfc.tag.TagCommandError`\n        exception. Errors from response status flags produce an\n        :attr:`~nfc.tag.TagCommandError.errno` that is greater than\n        255, all other errors are below 256.\n\n        \"\"\"\n        idm = self.idm if send_idm else bytearray()\n        cmd = chr(2+len(idm)+len(cmd_data)) + chr(cmd_code) + idm + cmd_data\n        log.debug(\">> {0:02x} {1:02x} {2} {3} ({4}s)\".format(\n            cmd[0], cmd[1], hexlify(cmd[2:10]), hexlify(cmd[10:]), timeout))\n\n        started = time.time()\n        for retry in range(3):\n            try:\n                rsp = self.clf.exchange(cmd, timeout)\n                break\n            except nfc.clf.CommunicationError as error:\n                reason = error.__class__.__name__\n                log.debug(\"%s after %d retries\" % (reason, retry))\n        else:\n            if type(error) is nfc.clf.TimeoutError:\n                raise Type3TagCommandError(nfc.tag.TIMEOUT_ERROR)\n            if type(error) is nfc.clf.TransmissionError:\n                raise Type3TagCommandError(nfc.tag.RECEIVE_ERROR)\n            if type(error) is nfc.clf.ProtocolError:  # pragma: no branch\n                raise Type3TagCommandError(nfc.tag.PROTOCOL_ERROR)\n\n        if rsp[0] != len(rsp):\n            log.debug(\"incorrect response length {0:02x}\".format(rsp[0]))\n            raise Type3TagCommandError(RSP_LENGTH_ERROR)\n        if rsp[1] != cmd_code + 1:\n            log.debug(\"incorrect response code {0:02x}\".format(rsp[1]))\n            raise Type3TagCommandError(RSP_CODE_ERROR)\n        if send_idm and rsp[2:10] != self.idm:\n            log.debug(\"wrong tag or transaction id \" + hexlify(rsp[2:10]))\n            raise Type3TagCommandError(TAG_IDM_ERROR)\n        if not send_idm:\n            log.debug(\"<< {0:02x} {1:02x} {2}\".format(\n                rsp[0], rsp[1], hexlify(rsp[2:])))\n            return rsp[2:]\n        if check_status and rsp[10] != 0:\n            log.debug(\"tag returned error status \" + hexlify(rsp[10:12]))\n            raise Type3TagCommandError(unpack(\">H\", rsp[10:12])[0])\n        if not check_status:\n            log.debug(\"<< {0:02x} {1:02x} {2} {3}\".format(\n                rsp[0], rsp[1], hexlify(rsp[2:10]), hexlify(rsp[10:])))\n            return rsp[10:]\n        log.debug(\"<< {0:02x} {1:02x} {2} {3} {4} ({elapsed:f}s)\".format(\n            rsp[0], rsp[1], hexlify(rsp[2:10]), hexlify(rsp[10:12]),\n            hexlify(rsp[12:]), elapsed=time.time()-started))\n        return rsp[12:]", "response": "Send a command and receive a response."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open(self, path):\n        if not isinstance(path, str):\n            raise TypeError(\"expecting a string type argument *path*\")\n        if not len(path) > 0:\n            raise ValueError(\"argument *path* must not be empty\")\n\n        # Close current device driver if this is not the first\n        # open. This allows to use several devices sequentially or\n        # re-initialize a device.\n        self.close()\n\n        # Acquire the lock and search for a device on *path*\n        with self.lock:\n            log.info(\"searching for reader on path \" + path)\n            self.device = device.connect(path)\n            if self.device:\n                log.info(\"using {0}\".format(self.device))\n            else:\n                log.error(\"no reader available on path \" + path)\n            return bool(self.device)", "response": "Open a contactless reader identified by the search path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclose the contacless reader device.", "response": "def close(self):\n        \"\"\"Close the contacless reader device.\"\"\"\n        with self.lock:\n            if self.device is not None:\n                try:\n                    self.device.close()\n                except IOError:\n                    pass\n                self.device = None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self, **options):\n        if self.device is None:\n            raise IOError(errno.ENODEV, os.strerror(errno.ENODEV))\n\n        log.debug(\"connect{0}\".format(\n            tuple([k for k in options if options[k]])))\n\n        terminate = options.get('terminate', lambda: False)\n        rdwr_options = options.get('rdwr')\n        llcp_options = options.get('llcp')\n        card_options = options.get('card')\n\n        try:\n            assert isinstance(rdwr_options, (dict, type(None))), \"rdwr\"\n            assert isinstance(llcp_options, (dict, type(None))), \"llcp\"\n            assert isinstance(card_options, (dict, type(None))), \"card\"\n        except AssertionError as error:\n            raise TypeError(\"argument '%s' must be a dictionary\" % error)\n\n        if llcp_options is not None:\n            llcp_options = dict(llcp_options)\n            llcp_options.setdefault('on-startup', lambda llc: llc)\n            llcp_options.setdefault('on-connect', lambda llc: True)\n            llcp_options.setdefault('on-release', lambda llc: True)\n\n            llc = nfc.llcp.llc.LogicalLinkController(**llcp_options)\n            llc = llcp_options['on-startup'](llc)\n            if isinstance(llc, nfc.llcp.llc.LogicalLinkController):\n                llcp_options['llc'] = llc\n            else:\n                log.debug(\"removing llcp_options after on-startup\")\n                llcp_options = None\n\n        if rdwr_options is not None:\n            def on_discover(target):\n                if target.sel_res and target.sel_res[0] & 0x40:\n                    return False\n                elif target.sensf_res and target.sensf_res[1:3] == b\"\\x01\\xFE\":\n                    return False\n                else:\n                    return True\n\n            rdwr_options = dict(rdwr_options)\n            rdwr_options.setdefault('targets', ['106A', '106B', '212F'])\n            rdwr_options.setdefault('on-startup', lambda targets: targets)\n            rdwr_options.setdefault('on-discover', on_discover)\n            rdwr_options.setdefault('on-connect', lambda tag: True)\n            rdwr_options.setdefault('on-release', lambda tag: True)\n            rdwr_options.setdefault('iterations', 5)\n            rdwr_options.setdefault('interval', 0.5)\n            rdwr_options.setdefault('beep-on-connect', True)\n\n            targets = [RemoteTarget(brty) for brty in rdwr_options['targets']]\n            targets = rdwr_options['on-startup'](targets)\n            if targets and all([isinstance(o, RemoteTarget) for o in targets]):\n                rdwr_options['targets'] = targets\n            else:\n                log.debug(\"removing rdwr_options after on-startup\")\n                rdwr_options = None\n\n        if card_options is not None:\n            card_options = dict(card_options)\n            card_options.setdefault('on-startup', lambda target: None)\n            card_options.setdefault('on-discover', lambda target: True)\n            card_options.setdefault('on-connect', lambda tag: True)\n            card_options.setdefault('on-release', lambda tag: True)\n\n            target = nfc.clf.LocalTarget()\n            target = card_options['on-startup'](target)\n            if isinstance(target, LocalTarget):\n                card_options['target'] = target\n            else:\n                log.debug(\"removing card_options after on-startup\")\n                card_options = None\n\n        if not (rdwr_options or llcp_options or card_options):\n            log.warning(\"no options to connect\")\n            return None\n\n        log.debug(\"connect options after startup: %s\",\n                  ', '.join(filter(bool, [\"rdwr\" if rdwr_options else None,\n                                          \"llcp\" if llcp_options else None,\n                                          \"card\" if card_options else None])))\n\n        try:\n            while not terminate():\n                if rdwr_options:\n                    result = self._rdwr_connect(rdwr_options, terminate)\n                    if bool(result) is True:\n                        return result\n                if llcp_options:\n                    result = self._llcp_connect(llcp_options, terminate)\n                    if bool(result) is True:\n                        return result\n                if card_options:\n                    result = self._card_connect(card_options, terminate)\n                    if bool(result) is True:\n                        return result\n        except IOError as error:\n            log.error(error)\n            return False\n        except UnsupportedTargetError as error:\n            log.info(error)\n            return False\n        except KeyboardInterrupt:\n            log.debug(\"terminated by keyboard interrupt\")\n            return False", "response": "Connect with a Target or Initiator."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndiscovers a contactless card or listening device.", "response": "def sense(self, *targets, **options):\n        \"\"\"Discover a contactless card or listening device.\n\n        .. note:: The :meth:`sense` method is intended for experts\n                  with a good understanding of the commands and\n                  responses exchanged during target activation (the\n                  notion used for commands and responses follows the\n                  NFC Forum Digital Specification). If the greater\n                  level of control is not needed it is recommended to\n                  use the :meth:`connect` method.\n\n        All positional arguments build the list of potential *targets*\n        to discover and must be of type :class:`RemoteTarget`. Keyword\n        argument *options* may be the number of ``iterations`` of the\n        sense loop set by *targets* and the ``interval`` between\n        iterations. The return value is either a :class:`RemoteTarget`\n        instance or :const:`None`.\n\n        >>> import nfc, nfc.clf\n        >>> clf = nfc.ContactlessFrontend(\"usb\")\n        >>> target1 = nfc.clf.RemoteTarget(\"106A\")\n        >>> target2 = nfc.clf.RemoteTarget(\"212F\")\n        >>> print(clf.sense(target1, target2, iterations=5, interval=0.2))\n        106A(sdd_res=04497622D93881, sel_res=00, sens_res=4400)\n\n        A **Type A Target** is specified with the technology letter\n        ``A`` following the bitrate to be used for the SENS_REQ\n        command (almost always must the bitrate be 106 kbps). To\n        discover only a specific Type A target, the NFCID1 (UID) can\n        be set with a 4, 7, or 10 byte ``sel_req`` attribute (cascade\n        tags are handled internally).\n\n        >>> target = nfc.clf.RemoteTarget(\"106A\")\n        >>> print(clf.sense(target))\n        106A sdd_res=04497622D93881 sel_res=00 sens_res=4400\n        >>> target.sel_req = bytearray.fromhex(\"04497622D93881\")\n        >>> print(clf.sense(target))\n        106A sdd_res=04497622D93881 sel_res=00 sens_res=4400\n        >>> target.sel_req = bytearray.fromhex(\"04497622\")\n        >>> print(clf.sense(target))\n        None\n\n        A **Type B Target** is specified with the technology letter\n        ``B`` following the bitrate to be used for the SENSB_REQ\n        command (almost always must the bitrate be 106 kbps). A\n        specific application family identifier can be set with the\n        first byte of a ``sensb_req`` attribute (the second byte PARAM\n        is ignored when it can not be set to local device, 00h is a\n        safe value in all cases).\n\n        >>> target = nfc.clf.RemoteTarget(\"106B\")\n        >>> print(clf.sense(target))\n        106B sens_res=50E5DD3DC900000011008185\n        >>> target.sensb_req = bytearray.fromhex(\"0000\")\n        >>> print(clf.sense(target))\n        106B sens_res=50E5DD3DC900000011008185\n        >>> target.sensb_req = bytearray.fromhex(\"FF00\")\n        >>> print(clf.sense(target))\n        None\n\n        A **Type F Target** is specified with the technology letter\n        ``F`` following the bitrate to be used for the SENSF_REQ\n        command (the typically supported bitrates are 212 and 424\n        kbps). The default SENSF_REQ command allows all targets to\n        answer, requests system code information, and selects a single\n        time slot for the SENSF_RES response. This can be changed with\n        the ``sensf_req`` attribute.\n\n        >>> target = nfc.clf.RemoteTarget(\"212F\")\n        >>> print(clf.sense(target))\n        212F sensf_res=0101010601B00ADE0B03014B024F4993FF12FC\n        >>> target.sensf_req = bytearray.fromhex(\"0012FC0000\")\n        >>> print(clf.sense(target))\n        212F sensf_res=0101010601B00ADE0B03014B024F4993FF\n        >>> target.sensf_req = bytearray.fromhex(\"00ABCD0000\")\n        >>> print(clf.sense(target))\n        None\n\n        An **Active Communication Mode P2P Target** search is selected\n        with an ``atr_req`` attribute. The choice of bitrate and\n        modulation type is 106A, 212F, and 424F.\n\n        >>> atr = bytearray.fromhex(\"D4000102030405060708091000000030\")\n        >>> target = clf.sense(nfc.clf.RemoteTarget(\"106A\", atr_req=atr))\n        >>> if target and target.atr_res: print(target.atr_res.encode(\"hex\"))\n        d501c023cae6b3182afe3dee0000000e3246666d01011103020013040196\n        >>> target = clf.sense(nfc.clf.RemoteTarget(\"424F\", atr_req=atr))\n        >>> if target and target.atr_res: print(target.atr_res.encode(\"hex\"))\n        d501dc0104f04584e15769700000000e3246666d01011103020013040196\n\n        Some drivers must modify the ATR_REQ to cope with hardware\n        limitations, for example change length reduction value to\n        reduce the maximum size of target responses. The ATR_REQ that\n        has been send is given by the ``atr_req`` attribute of the\n        returned RemoteTarget object.\n\n        A **Passive Communication Mode P2P Target** responds to 106A\n        discovery with bit 6 of SEL_RES set to 1, and to 212F/424F\n        discovery (when the request code RC is 0 in the SENSF_REQ\n        command) with an NFCID2 that starts with 01FEh in the\n        SENSF_RES response. Responses below are from a Nexus 5\n        configured for NFC-DEP Protocol (SEL_RES bit 6 is set) and\n        Type 4A Tag (SEL_RES bit 5 is set).\n\n        >>> print(clf.sense(nfc.clf.RemoteTarget(\"106A\")))\n        106A sdd_res=08796BEB sel_res=60 sens_res=0400\n        >>> sensf_req = bytearray.fromhex(\"00FFFF0000\")\n        >>> print(clf.sense(nfc.clf.RemoteTarget(\"424F\", sensf_req=sensf_req)))\n        424F sensf_res=0101FE1444EFB88FD50000000000000000\n\n        Errors found in the *targets* argument list raise exceptions\n        only if exactly one target is given. If multiple targets are\n        provided, any target that is not supported or has invalid\n        attributes is just ignored (but is logged as a debug message).\n\n        **Exceptions**\n\n        * :exc:`~exceptions.IOError` (ENODEV) when a local contacless\n          communication device has not been opened or communication\n          with the local device is no longer possible.\n\n        * :exc:`nfc.clf.UnsupportedTargetError` if the single target\n          supplied as input is not supported by the active driver.\n          This exception is never raised when :meth:`sense` is called\n          with multiple targets, those unsupported are then silently\n          ignored.\n\n        \"\"\"\n        def sense_tta(target):\n            if target.sel_req and len(target.sel_req) not in (4, 7, 10):\n                raise ValueError(\"sel_req must be 4, 7, or 10 byte\")\n            target = self.device.sense_tta(target)\n            log.debug(\"found %s\", target)\n            if target and len(target.sens_res) != 2:\n                error = \"SENS Response Format Error (wrong length)\"\n                log.debug(error)\n                raise ProtocolError(error)\n            if target and target.sens_res[0] & 0b00011111 == 0:\n                if target.sens_res[1] & 0b00001111 != 0b1100:\n                    error = \"SENS Response Data Error (T1T config)\"\n                    log.debug(error)\n                    raise ProtocolError(error)\n                if not target.rid_res:\n                    error = \"RID Response Error (no response received)\"\n                    log.debug(error)\n                    raise ProtocolError(error)\n                if len(target.rid_res) != 6:\n                    error = \"RID Response Format Error (wrong length)\"\n                    log.debug(error)\n                    raise ProtocolError(error)\n                if target.rid_res[0] >> 4 != 0b0001:\n                    error = \"RID Response Data Error (invalid HR0)\"\n                    log.debug(error)\n                    raise ProtocolError(error)\n            return target\n\n        def sense_ttb(target):\n            return self.device.sense_ttb(target)\n\n        def sense_ttf(target):\n            return self.device.sense_ttf(target)\n\n        def sense_dep(target):\n            if len(target.atr_req) < 16:\n                raise ValueError(\"minimum atr_req length is 16 byte\")\n            if len(target.atr_req) > 64:\n                raise ValueError(\"maximum atr_req length is 64 byte\")\n            return self.device.sense_dep(target)\n\n        for target in targets:\n            if not isinstance(target, RemoteTarget):\n                raise ValueError(\"invalid target argument type: %r\" % target)\n\n        with self.lock:\n            if self.device is None:\n                raise IOError(errno.ENODEV, os.strerror(errno.ENODEV))\n\n            self.target = None  # forget captured target\n            self.device.mute()  # deactivate the rf field\n\n            for i in xrange(max(1, options.get('iterations', 1))):\n                started = time.time()\n                for target in targets:\n                    log.debug(\"sense {0}\".format(target))\n                    try:\n                        if target.atr_req is not None:\n                            self.target = sense_dep(target)\n                        elif target.brty.endswith('A'):\n                            self.target = sense_tta(target)\n                        elif target.brty.endswith('B'):\n                            self.target = sense_ttb(target)\n                        elif target.brty.endswith('F'):\n                            self.target = sense_ttf(target)\n                        else:\n                            info = \"unknown technology type in %r\"\n                            raise UnsupportedTargetError(info % target.brty)\n                    except UnsupportedTargetError as error:\n                        if len(targets) == 1:\n                            raise error\n                        else:\n                            log.debug(error)\n                    except CommunicationError as error:\n                        log.debug(error)\n                    else:\n                        if self.target is not None:\n                            log.debug(\"found {0}\".format(self.target))\n                            return self.target\n                if len(targets) > 0:\n                    self.device.mute()  # deactivate the rf field\n                if i < options.get('iterations', 1) - 1:\n                    elapsed = time.time() - started\n                    time.sleep(max(0, options.get('interval', 0.1)-elapsed))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef listen(self, target, timeout):\n        def listen_tta(target, timeout):\n            return self.device.listen_tta(target, timeout)\n\n        def listen_ttb(target, timeout):\n            return self.device.listen_ttb(target, timeout)\n\n        def listen_ttf(target, timeout):\n            return self.device.listen_ttf(target, timeout)\n\n        def listen_dep(target, timeout):\n            target = self.device.listen_dep(target, timeout)\n            if target and target.atr_req:\n                try:\n                    assert len(target.atr_req) >= 16, \"less than 16 byte\"\n                    assert len(target.atr_req) <= 64, \"more than 64 byte\"\n                    return target\n                except AssertionError as error:\n                    log.debug(\"atr_req is %s\", str(error))\n\n        assert isinstance(target, LocalTarget), \\\n            \"invalid target argument type: %r\" % target\n\n        with self.lock:\n            if self.device is None:\n                raise IOError(errno.ENODEV, os.strerror(errno.ENODEV))\n\n            self.target = None  # forget captured target\n            self.device.mute()  # deactivate the rf field\n\n            info = \"listen %.3f seconds for %s\"\n            if target.atr_res is not None:\n                log.debug(info, timeout, \"DEP\")\n                self.target = listen_dep(target, timeout)\n            elif target.brty in ('106A', '212A', '424A'):\n                log.debug(info, timeout, target)\n                self.target = listen_tta(target, timeout)\n            elif target.brty in ('106B', '212B', '424B', '848B'):\n                log.debug(info, timeout, target)\n                self.target = listen_ttb(target, timeout)\n            elif target.brty in ('212F', '424F'):\n                log.debug(info, timeout, target)\n                self.target = listen_ttf(target, timeout)\n            else:\n                errmsg = \"unsupported bitrate technology type {}\"\n                raise ValueError(errmsg.format(target.brty))\n\n            return self.target", "response": "Listen for a given target."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexchanges data with an activated target.", "response": "def exchange(self, send_data, timeout):\n        \"\"\"Exchange data with an activated target (*send_data* is a command\n        frame) or as an activated target (*send_data* is a response\n        frame). Returns a target response frame (if data is send to an\n        activated target) or a next command frame (if data is send\n        from an activated target). Returns None if the communication\n        link broke during exchange (if data is sent as a target). The\n        timeout is the number of seconds to wait for data to return,\n        if the timeout expires an nfc.clf.TimeoutException is\n        raised. Other nfc.clf.CommunicationError exceptions may be raised if\n        an error is detected during communication.\n\n        \"\"\"\n        with self.lock:\n            if self.device is None:\n                raise IOError(errno.ENODEV, os.strerror(errno.ENODEV))\n\n            log.debug(\">>> %s timeout=%s\", print_data(send_data), str(timeout))\n\n            if isinstance(self.target, RemoteTarget):\n                exchange = self.device.send_cmd_recv_rsp\n            elif isinstance(self.target, LocalTarget):\n                exchange = self.device.send_rsp_recv_cmd\n            else:\n                log.error(\"no target for data exchange\")\n                return None\n\n            send_time = time.time()\n            rcvd_data = exchange(self.target, send_data, timeout)\n            recv_time = time.time() - send_time\n\n            log.debug(\"<<< %s %.3fs\", print_data(rcvd_data), recv_time)\n            return rcvd_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build(self, root, schema):\n        if schema.get(\"subcommands\") and schema[\"subcommands\"]:\n            for subcmd, childSchema in schema[\"subcommands\"].items():\n                child = CommandTree(node=subcmd)\n                child = self.build(child, childSchema)\n                root.children.append(child)\n        # {args: {}, options: {}, help: \"\"}\n        root.help = schema.get(\"help\")\n        for name, desc in schema.get(\"options\").items():\n            if root.node == \"kubectl\":  # register global flags\n                self.globalFlags.append(Option(name, desc[\"help\"]))\n            root.localFlags.append(Option(name, desc[\"help\"]))\n        for arg in schema.get(\"args\"):\n            node = CommandTree(node=arg)\n            root.children.append(node)\n        return root", "response": "Builds the syntax tree for kubectl command line"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a sequence of tokens returning a tuple of parsed unparsed suggestions", "response": "def parse_tokens(self, tokens):\n        \"\"\" Parse a sequence of tokens\n\n        returns tuple of (parsed tokens, suggestions)\n        \"\"\"\n        if len(tokens) == 1:\n            return list(), tokens, {\"kubectl\": self.ast.help}\n        else:\n            tokens.reverse()\n        parsed, unparsed, suggestions = self.treewalk(self.ast, parsed=list(), unparsed=tokens)\n        if not suggestions and unparsed:\n            # TODO: @vogxn: This is hack until we include expected value types for each option and argument.\n            # Whenver we recieve no suggestions but are left with unparsed tokens we pop the value and walk the\n            # tree again without values\n            logger.debug(\"unparsed tokens remain, possible value encountered\")\n            unparsed.pop()\n            parsed.reverse()\n            unparsed.extend(parsed)\n            logger.debug(\"resuming treewalk with tokens: %s\", unparsed)\n            return self.treewalk(self.ast, parsed=list(), unparsed=unparsed)\n        else:\n            return parsed, unparsed, suggestions"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef treewalk(self, root, parsed, unparsed):\n        suggestions = dict()\n        if not unparsed:\n            logger.debug(\"no tokens left unparsed. returning %s, %s\", parsed, suggestions)\n            return parsed, unparsed, suggestions\n\n        token = unparsed.pop().strip()\n        logger.debug(\"begin parsing at %s w/ tokens: %s\", root.node, unparsed)\n        if root.node == token:\n            logger.debug(\"root node: %s matches next token:%s\", root.node, token)\n            parsed.append(token)\n            if self.peekForOption(unparsed):  # check for localFlags and globalFlags\n                logger.debug(\"option(s) upcoming %s\", unparsed)\n                parsed_opts, unparsed, suggestions = self.evalOptions(root, list(), unparsed[:])\n                if parsed_opts:\n                    logger.debug(\"parsed option(s): %s\", parsed_opts)\n                    parsed.extend(parsed_opts)\n            if unparsed and not self.peekForOption(unparsed):  # unparsed bits without options\n                logger.debug(\"begin subtree %s parsing\", root.node)\n                for child in root.children:\n                    parsed_subtree, unparsed, suggestions = self.treewalk(child, list(), unparsed[:])\n                    if parsed_subtree:  # subtree returned further parsed tokens\n                        parsed.extend(parsed_subtree)\n                        logger.debug(\"subtree at: %s has matches. %s, %s\", child.node, parsed, unparsed)\n                        break\n                else:\n                    # no matches found in command tree\n                    # return children of root as suggestions\n                    logger.debug(\"no matches in subtree: %s. returning children as suggestions\", root.node)\n                    for child in root.children:\n                        suggestions[child.node] = child.help\n        else:\n            logger.debug(\"no token or option match\")\n            unparsed.append(token)\n        return parsed, unparsed, suggestions", "response": "Recursively walks the syntax tree at root and returns the items parsed unparsed and possible suggestions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef evalOptions(self, root, parsed, unparsed):\n        logger.debug(\"parsing options at tree: %s with p:%s, u:%s\", root.node, parsed, unparsed)\n        suggestions = dict()\n        token = unparsed.pop().strip()\n\n        parts = token.partition('=')\n        if parts[-1] != '':  # parsing for --option=value type input\n            token = parts[0]\n\n        allFlags = root.localFlags + self.globalFlags\n        for flag in allFlags:\n            if flag.name == token:\n                logger.debug(\"matched token: %s with flag: %s\", token, flag.name)\n                parsed.append(token)\n                if self.peekForOption(unparsed):  # recursively look for further options\n                    parsed, unparsed, suggestions = self.evalOptions(root, parsed, unparsed[:])\n                # elif token == \"--namespace\":\n                #     namespaces = [('default', None), ('minikube', None), ('gitlab', None)]  # self.kube_client.get_resource(\"namespace\")\n                #     suggestions = dict(namespaces)\n                break\n        else:\n            logger.debug(\"no flags match, returning allFlags suggestions\")\n            for flag in allFlags:\n                suggestions[flag.name] = flag.helptext\n\n        if suggestions:  # incomplete parse, replace token\n            logger.debug(\"incomplete option: %s provided. returning suggestions\", token)\n            unparsed.append(token)\n        return parsed, unparsed, suggestions", "response": "Evaluate only the options and return flags as suggestions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_logging(level, monchrome=False, log_file=None):\n    '''\n    Utility function for setting up logging.\n    '''\n    # Logging to file\n    if log_file:\n        logging.basicConfig(filename=log_file, filemode='w',\n                            level=logging.DEBUG)\n\n    # Pretty color terminal logging\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = ColoredFormatter(\"%(levelname)s: %(message)s\", monchrome)\n    ch.setFormatter(formatter)\n    # Which packages do we want to log from.\n    packages = ('__main__', 'fusesoc',)\n    for package in packages:\n        logger = logging.getLogger(package)\n        logger.addHandler(ch)\n        logger.setLevel(level)\n    # Warning only packages\n    warning_only_packages = []\n    for package in warning_only_packages:\n        logger = logging.getLogger(package)\n        logger.addHandler(ch)\n        logger.setLevel(logging.WARNING)\n    logger.debug('Setup logging at level {}.'.format(level))", "response": "Utility function for setting up logging."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning a parametrized generator to generate a list of cores", "response": "def generate(self, cache_root):\n        \"\"\"Run a parametrized generator\n\n        Args:\n            cache_root (str): The directory where to store the generated cores\n\n        Returns:\n            list: Cores created by the generator\n        \"\"\"\n        generator_cwd = os.path.join(cache_root, 'generated', self.vlnv.sanitized_name)\n        generator_input_file  = os.path.join(generator_cwd, self.name+'_input.yml')\n\n        logger.info('Generating ' + str(self.vlnv))\n        if not os.path.exists(generator_cwd):\n            os.makedirs(generator_cwd)\n        with open(generator_input_file, 'w') as f:\n            f.write(yaml.dump(self.generator_input))\n\n        args = [os.path.join(os.path.abspath(self.generator.root), self.generator.command),\n                generator_input_file]\n\n        if self.generator.interpreter:\n            args[0:0] = [self.generator.interpreter]\n\n        Launcher(args[0], args[1:],\n                 cwd=generator_cwd).run()\n\n        cores = []\n        logger.debug(\"Looking for generated cores in \" + generator_cwd)\n        for root, dirs, files in os.walk(generator_cwd):\n            for f in files:\n                if f.endswith('.core'):\n                    try:\n                        cores.append(Core(os.path.join(root, f)))\n                    except SyntaxError as e:\n                        w = \"Failed to parse generated core file \" + f + \": \" + e.msg\n                        raise RuntimeError(w)\n        logger.debug(\"Found \" + ', '.join(str(c.name) for c in cores))\n        return cores"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _range_check(self, value, min_value, max_value):\n        '''\n        Utility method to check that the given value is between min_value and max_value.\n        '''\n        if value < min_value or value > max_value:\n            raise ValueError('%s out of range - %s is not between %s and %s' % (self.__class__.__name__, value, min_value, max_value))", "response": "Utility method to check that the given value is between min_value and max_value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an SQL expression describing the field.", "response": "def get_sql(self, with_default_expression=True):\n        '''\n        Returns an SQL expression describing the field (e.g. for CREATE TABLE).\n        :param with_default_expression: If True, adds default value to sql.\n            It doesn't affect fields with alias and materialized values.\n        '''\n        if with_default_expression:\n            if self.alias:\n                return '%s ALIAS %s' % (self.db_type, self.alias)\n            elif self.materialized:\n                return '%s MATERIALIZED %s' % (self.db_type, self.materialized)\n            else:\n                default = self.to_db_string(self.default)\n                return '%s DEFAULT %s' % (self.db_type, default)\n        else:\n            return self.db_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isinstance(self, types):\n        if isinstance(self, types):\n            return True\n        inner_field = getattr(self, 'inner_field', None)\n        while inner_field:\n            if isinstance(inner_field, types):\n                return True\n            inner_field = getattr(inner_field, 'inner_field', None)\n        return False", "response": "Checks if the instance is one of the types provided or if any of the inner_field child is one of the types\n        provided. Returns True if the instance is one of the types\n        provided False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_ad_hoc_field(cls, db_type):\n        '''\n        Give an SQL column description such as \"Enum8('apple' = 1, 'banana' = 2, 'orange' = 3)\"\n        this method returns a matching enum field.\n        '''\n        import re\n        try:\n            Enum # exists in Python 3.4+\n        except NameError:\n            from enum import Enum # use the enum34 library instead\n        members = {}\n        for match in re.finditer(\"'(\\w+)' = (\\d+)\", db_type):\n            members[match.group(1)] = int(match.group(2))\n        enum_cls = Enum('AdHocEnum', members)\n        field_class = Enum8Field if db_type.startswith('Enum8') else Enum16Field\n        return field_class(enum_cls)", "response": "Create an AdHocField from a SQL column description such as Enum8 Enum16 Enum34 }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_method_sig(method):\n\n    # The return value of ArgSpec is a bit weird, as the list of arguments and\n    # list of defaults are returned in separate array.\n    # eg: ArgSpec(args=['first_arg', 'second_arg', 'third_arg'],\n    # varargs=None, keywords=None, defaults=(42, 'something'))\n    argspec = inspect.getargspec(method)\n    arg_index=0\n    args = []\n\n    # Use the args and defaults array returned by argspec and find out\n    # which arguments has default\n    for arg in argspec.args:\n        default_arg = _get_default_arg(argspec.args, argspec.defaults, arg_index)\n        if default_arg.has_default:\n            val = default_arg.default_value\n            if isinstance(val, basestring):\n                val = '\"' + val + '\"'\n            args.append(\"%s=%s\" % (arg, val))\n        else:\n            args.append(arg)\n        arg_index += 1\n    if argspec.varargs:\n        args.append('*' + argspec.varargs)\n    if argspec.keywords:\n        args.append('**' + argspec.keywords)\n    return \"%s(%s)\" % (method.__name__, \", \".join(args[1:]))", "response": "Given a function it returns a string that pretty much looks how the method signature would be written in python."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new queryset with the specified grouping fields.", "response": "def group_by(self, *args):\n        \"\"\"\n        This method lets you specify the grouping fields explicitly. The `args` must\n        be names of grouping fields or calculated fields that this queryset was\n        created with.\n        \"\"\"\n        for name in args:\n            assert name in self._fields or name in self._calculated_fields, \\\n                   'Cannot group by `%s` since it is not included in the query' % name\n        qs = copy(self)\n        qs._grouping_fields = args\n        return qs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the selected fields or expressions as a SQL string.", "response": "def select_fields_as_sql(self):\n        \"\"\"\n        Returns the selected fields or expressions as a SQL string.\n        \"\"\"\n        return comma_join(list(self._fields) + ['%s AS %s' % (v, k) for k, v in self._calculated_fields.items()])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of rows after aggregation.", "response": "def count(self):\n        \"\"\"\n        Returns the number of rows after aggregation.\n        \"\"\"\n        sql = u'SELECT count() FROM (%s)' % self.as_sql()\n        raw = self._database.raw(sql)\n        return int(raw) if raw else 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the database that this model instance belongs to.", "response": "def set_database(self, db):\n        '''\n        Sets the `Database` that this model instance belongs to.\n        This is done automatically when the instance is read from the database or written to it.\n        '''\n        # This can not be imported globally due to circular import\n        from .database import Database\n        assert isinstance(db, Database), \"database must be database.Database instance\"\n        self._database = db"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_table_sql(cls, db):\n        '''\n        Returns the SQL command for creating a table for this model.\n        '''\n        parts = ['CREATE TABLE IF NOT EXISTS `%s`.`%s` (' % (db.db_name, cls.table_name())]\n        cols = []\n        for name, field in iteritems(cls.fields()):\n            cols.append('    %s %s' % (name, field.get_sql()))\n        parts.append(',\\n'.join(cols))\n        parts.append(')')\n        parts.append('ENGINE = ' + cls.engine.create_table_sql(db))\n        return '\\n'.join(parts)", "response": "Returns the SQL command for creating a table for this model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a model instance from a tab - separated line of TSV - formatted data.", "response": "def from_tsv(cls, line, field_names, timezone_in_use=pytz.utc, database=None):\n        '''\n        Create a model instance from a tab-separated line. The line may or may not include a newline.\n        The `field_names` list must match the fields defined in the model, but does not have to include all of them.\n\n        - `line`: the TSV-formatted data.\n        - `field_names`: names of the model fields in the data.\n        - `timezone_in_use`: the timezone to use when parsing dates and datetimes.\n        - `database`: if given, sets the database that this instance belongs to.\n        '''\n        from six import next\n        values = iter(parse_tsv(line))\n        kwargs = {}\n        for name in field_names:\n            field = getattr(cls, name)\n            kwargs[name] = field.to_python(next(values), timezone_in_use)\n\n        obj = cls(**kwargs)\n        if database is not None:\n            obj.set_database(database)\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the instance s column values as a tab - separated line. A newline is not included.", "response": "def to_tsv(self, include_readonly=True):\n        '''\n        Returns the instance's column values as a tab-separated line. A newline is not included.\n\n        - `include_readonly`: if false, returns only fields that can be inserted into database.\n        '''\n        data = self.__dict__\n        fields = self.fields(writable=not include_readonly)\n        return '\\t'.join(field.to_db_string(data[name], quote=False) for name, field in iteritems(fields))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the instance s column values as a dict.", "response": "def to_dict(self, include_readonly=True, field_names=None):\n        '''\n        Returns the instance's column values as a dict.\n\n        - `include_readonly`: if false, returns only fields that can be inserted into database.\n        - `field_names`: an iterable of field names to return (optional)\n        '''\n        fields = self.fields(writable=not include_readonly)\n\n        if field_names is not None:\n            fields = [f for f in fields if f in field_names]\n\n        data = self.__dict__\n        return {name: data[name] for name in fields}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_table_sql(cls, db):\n        '''\n        Returns the SQL command for creating a table for this model.\n        '''\n        parts = ['CREATE TABLE IF NOT EXISTS `%s`.`%s` AS `%s`.`%s`' % (db.db_name, cls.table_name(), db.db_name,\n                                                                        cls.engine.main_model.table_name())]\n        engine_str = cls.engine.create_table_sql(db)\n        parts.append(engine_str)\n        return ' '.join(parts)", "response": "Returns the SQL command for creating a table for this model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremembers Distributed table does not store any data, just provides distributed access to it. So if we define a model with engine that has no defined table for data storage (see FooDistributed below), that table cannot be successfully created. This routine can automatically fix engine's storage table by finding the first non-distributed model among your model's superclasses. >>> class Foo(Model): ... id = UInt8Field(1) ... >>> class FooDistributed(Foo, DistributedModel): ... engine = Distributed('my_cluster') ... >>> FooDistributed.engine.table None >>> FooDistributed.fix_engine() >>> FooDistributed.engine.table <class '__main__.Foo'> However if you prefer more explicit way of doing things, you can always mention the Foo model twice without bothering with any fixes: >>> class FooDistributedVerbose(Foo, DistributedModel): ... engine = Distributed('my_cluster', Foo) >>> FooDistributedVerbose.engine.table <class '__main__.Foo'> See tests.test_engines:DistributedTestCase for more examples", "response": "def fix_engine_table(cls):\n        \"\"\"\n        Remember: Distributed table does not store any data, just provides distributed access to it.\n\n        So if we define a model with engine that has no defined table for data storage\n        (see FooDistributed below), that table cannot be successfully created.\n        This routine can automatically fix engine's storage table by finding the first\n        non-distributed model among your model's superclasses.\n\n        >>> class Foo(Model):\n        ...     id = UInt8Field(1)\n        ...\n        >>> class FooDistributed(Foo, DistributedModel):\n        ...     engine = Distributed('my_cluster')\n        ...\n        >>> FooDistributed.engine.table\n        None\n        >>> FooDistributed.fix_engine()\n        >>> FooDistributed.engine.table\n        <class '__main__.Foo'>\n\n        However if you prefer more explicit way of doing things,\n        you can always mention the Foo model twice without bothering with any fixes:\n\n        >>> class FooDistributedVerbose(Foo, DistributedModel):\n        ...     engine = Distributed('my_cluster', Foo)\n        >>> FooDistributedVerbose.engine.table\n        <class '__main__.Foo'>\n\n        See tests.test_engines:DistributedTestCase for more examples\n        \"\"\"\n\n        # apply only when engine has no table defined\n        if cls.engine.table_name:\n            return\n\n        # find out all the superclasses of the Model that store any data\n        storage_models = [b for b in cls.__bases__ if issubclass(b, Model)\n                          and not issubclass(b, DistributedModel)]\n        if not storage_models:\n            raise TypeError(\"When defining Distributed engine without the table_name \"\n                            \"ensure that your model has a parent model\")\n\n        if len(storage_models) > 1:\n            raise TypeError(\"When defining Distributed engine without the table_name \"\n                            \"ensure that your model has exactly one non-distributed superclass\")\n\n        # enable correct SQL for engine\n        cls.engine.table = storage_models[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef escape(value, quote=True):\n    '''\n    If the value is a string, escapes any special characters and optionally\n    surrounds it with single quotes. If the value is not a string (e.g. a number),\n    converts it to one.\n    '''\n    def escape_one(match):\n        return SPECIAL_CHARS[match.group(0)]\n\n    if isinstance(value, string_types):\n        value = SPECIAL_CHARS_REGEX.sub(escape_one, value)\n        if quote:\n            value = \"'\" + value + \"'\"\n    return text_type(value)", "response": "Escape the value of a resource element."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_array(array_string):\n    # Sanity check\n    if len(array_string) < 2 or array_string[0] != '[' or array_string[-1] != ']':\n        raise ValueError('Invalid array string: \"%s\"' % array_string)\n    # Drop opening brace\n    array_string = array_string[1:]\n    # Go over the string, lopping off each value at the beginning until nothing is left\n    values = []\n    while True:\n        if array_string == ']':\n            # End of array\n            return values\n        elif array_string[0] in ', ':\n            # In between values\n            array_string = array_string[1:]\n        elif array_string[0] == \"'\":\n            # Start of quoted value, find its end\n            match = re.search(r\"[^\\\\]'\", array_string)\n            if match is None:\n                raise ValueError('Missing closing quote: \"%s\"' % array_string)\n            values.append(array_string[1 : match.start() + 1])\n            array_string = array_string[match.end():]\n        else:\n            # Start of non-quoted value, find its end\n            match = re.search(r\",|\\]\", array_string)\n            values.append(array_string[0 : match.start()])\n            array_string = array_string[match.end() - 1:]", "response": "Parse an array string as returned by clickhouse."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimport all submodules of a module.", "response": "def import_submodules(package_name):\n    \"\"\"\n    Import all submodules of a module.\n    \"\"\"\n    import importlib, pkgutil\n    package = importlib.import_module(package_name)\n    return {\n        name: importlib.import_module(package_name + '.' + name)\n        for _, name, _ in pkgutil.iter_modules(package.__path__)\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting some operation over partition", "response": "def _partition_operation_sql(self, operation, settings=None, from_part=None):\r\n        \"\"\"\r\n        Performs some operation over partition\r\n        :param db: Database object to execute operation on\r\n        :param operation: Operation to execute from SystemPart.OPERATIONS set\r\n        :param settings: Settings for executing request to ClickHouse over db.raw() method\r\n        :return: Operation execution result\r\n        \"\"\"\r\n        operation = operation.upper()\r\n        assert operation in self.OPERATIONS, \"operation must be in [%s]\" % comma_join(self.OPERATIONS)\r\n\r\n        sql = \"ALTER TABLE `%s`.`%s` %s PARTITION %s\" % (self._database.db_name, self.table, operation, self.partition)\r\n        if from_part is not None:\r\n            sql += \" FROM %s\" % from_part\r\n        self._database.raw(sql, settings=settings, stream=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading a partition from another server.", "response": "def fetch(self, zookeeper_path, settings=None):\r\n        \"\"\"\r\n        Download a partition from another server.\r\n        :param zookeeper_path: Path in zookeeper to fetch from\r\n        :param settings: Settings for executing request to ClickHouse over db.raw() method\r\n        :return: SQL Query\r\n        \"\"\"\r\n        return self._partition_operation_sql('FETCH', settings=settings, from_part=zookeeper_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all data from system. parts table", "response": "def get(cls, database, conditions=\"\"):\r\n        \"\"\"\r\n        Get all data from system.parts table\r\n        :param database: A database object to fetch data from.\r\n        :param conditions: WHERE clause conditions. Database condition is added automatically\r\n        :return: A list of SystemPart objects\r\n        \"\"\"\r\n        assert isinstance(database, Database), \"database must be database.Database class instance\"\r\n        assert isinstance(conditions, string_types), \"conditions must be a string\"\r\n        if conditions:\r\n            conditions += \" AND\"\r\n        field_names = ','.join(cls.fields())\r\n        return database.select(\"SELECT %s FROM %s WHERE %s database='%s'\" %\r\n                               (field_names,  cls.table_name(), conditions, database.db_name), model_class=cls)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_active(cls, database, conditions=\"\"):\r\n        if conditions:\r\n            conditions += ' AND '\r\n        conditions += 'active'\r\n        return SystemPart.get(database, conditions=conditions)", "response": "Gets the active data from system. parts table\r\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the code and message of the exception that clickhouse - server generated.", "response": "def get_error_code_msg(cls, full_error_message):\n        \"\"\"\n        Extract the code and message of the exception that clickhouse-server generated.\n\n        See the list of error codes here:\n        https://github.com/yandex/ClickHouse/blob/master/dbms/src/Common/ErrorCodes.cpp\n        \"\"\"\n        for pattern in cls.ERROR_PATTERNS:\n            match = pattern.match(full_error_message)\n            if match:\n                # assert match.group('type1') == match.group('type2')\n                return int(match.group('code')), match.group('msg').strip()\n\n        return 0, full_error_message"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a table for the given model class.", "response": "def create_table(self, model_class):\n        '''\n        Creates a table for the given model class, if it does not exist already.\n        '''\n        if model_class.is_system_model():\n            raise DatabaseException(\"You can't create system table\")\n        if getattr(model_class, 'engine') is None:\n            raise DatabaseException(\"%s class must define an engine\" % model_class.__name__)\n        self._send(model_class.create_table_sql(self))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndrop the database table of the given model class.", "response": "def drop_table(self, model_class):\n        '''\n        Drops the database table of the given model class, if it exists.\n        '''\n        if model_class.is_system_model():\n            raise DatabaseException(\"You can't drop system table\")\n        self._send(model_class.drop_table_sql(self))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef does_table_exist(self, model_class):\n        '''\n        Checks whether a table for the given model class already exists.\n        Note that this only checks for existence of a table with the expected name.\n        '''\n        sql = \"SELECT count() FROM system.tables WHERE database = '%s' AND name = '%s'\"\n        r = self._send(sql % (self.db_name, model_class.table_name()))\n        return r.text.strip() == '1'", "response": "Checks whether a table for the given model class already exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_setting(self, name, value):\n        '''\n        Adds a database setting that will be sent with every request.\n        For example, `db.add_setting(\"max_execution_time\", 10)` will\n        limit query execution time to 10 seconds.\n        The name must be string, and the value is converted to string in case\n        it isn't. To remove a setting, pass `None` as the value.\n        '''\n        assert isinstance(name, string_types), 'Setting name must be a string'\n        if value is None:\n            self.settings.pop(name, None)\n        else:\n            self.settings[name] = str(value)", "response": "Adds a database setting to the internal dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert(self, model_instances, batch_size=1000):\n        '''\n        Insert records into the database.\n\n        - `model_instances`: any iterable containing instances of a single model class.\n        - `batch_size`: number of records to send per chunk (use a lower number if your records are very large).\n        '''\n        from six import next\n        from io import BytesIO\n        i = iter(model_instances)\n        try:\n            first_instance = next(i)\n        except StopIteration:\n            return  # model_instances is empty\n        model_class = first_instance.__class__\n\n        if first_instance.is_read_only() or first_instance.is_system_model():\n            raise DatabaseException(\"You can't insert into read only and system tables\")\n\n        fields_list = ','.join(\n            ['`%s`' % name for name in first_instance.fields(writable=True)])\n\n        def gen():\n            buf = BytesIO()\n            query = 'INSERT INTO $table (%s) FORMAT TabSeparated\\n' % fields_list\n            buf.write(self._substitute(query, model_class).encode('utf-8'))\n            first_instance.set_database(self)\n            buf.write(first_instance.to_tsv(include_readonly=False).encode('utf-8'))\n            buf.write('\\n'.encode('utf-8'))\n            # Collect lines in batches of batch_size\n            lines = 2\n            for instance in i:\n                instance.set_database(self)\n                buf.write(instance.to_tsv(include_readonly=False).encode('utf-8'))\n                buf.write('\\n'.encode('utf-8'))\n                lines += 1\n                if lines >= batch_size:\n                    # Return the current batch of lines\n                    yield buf.getvalue()\n                    # Start a new batch\n                    buf = BytesIO()\n                    lines = 0\n            # Return any remaining lines in partial batch\n            if lines:\n                yield buf.getvalue()\n        self._send(gen())", "response": "Insert records into the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef count(self, model_class, conditions=None):\n        '''\n        Counts the number of records in the model's table.\n\n        - `model_class`: the model to count.\n        - `conditions`: optional SQL conditions (contents of the WHERE clause).\n        '''\n        query = 'SELECT count() FROM $table'\n        if conditions:\n            query += ' WHERE ' + conditions\n        query = self._substitute(query, model_class)\n        r = self._send(query)\n        return int(r.text) if r.text else 0", "response": "Counts the number of records in the model s table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select(self, query, model_class=None, settings=None):\n        '''\n        Performs a query and returns a generator of model instances.\n\n        - `query`: the SQL query to execute.\n        - `model_class`: the model class matching the query's table,\n          or `None` for getting back instances of an ad-hoc model.\n        - `settings`: query settings to send as HTTP GET parameters\n        '''\n        query += ' FORMAT TabSeparatedWithNamesAndTypes'\n        query = self._substitute(query, model_class)\n        r = self._send(query, settings, True)\n        lines = r.iter_lines()\n        field_names = parse_tsv(next(lines))\n        field_types = parse_tsv(next(lines))\n        model_class = model_class or ModelBase.create_ad_hoc_model(zip(field_names, field_types))\n        for line in lines:\n            # skip blank line left by WITH TOTALS modifier\n            if line:\n                yield model_class.from_tsv(line, field_names, self.server_timezone, self)", "response": "Performs a query and returns a generator of model instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms a query and returns its output as text.", "response": "def raw(self, query, settings=None, stream=False):\n        '''\n        Performs a query and returns its output as text.\n\n        - `query`: the SQL query to execute.\n        - `settings`: query settings to send as HTTP GET parameters\n        - `stream`: if true, the HTTP response from ClickHouse will be streamed.\n        '''\n        query = self._substitute(query, None)\n        return self._send(query, settings=settings, stream=stream).text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef paginate(self, model_class, order_by, page_num=1, page_size=100, conditions=None, settings=None):\n        '''\n        Selects records and returns a single page of model instances.\n\n        - `model_class`: the model class matching the query's table,\n          or `None` for getting back instances of an ad-hoc model.\n        - `order_by`: columns to use for sorting the query (contents of the ORDER BY clause).\n        - `page_num`: the page number (1-based), or -1 to get the last page.\n        - `page_size`: number of records to return per page.\n        - `conditions`: optional SQL conditions (contents of the WHERE clause).\n        - `settings`: query settings to send as HTTP GET parameters\n\n        The result is a namedtuple containing `objects` (list), `number_of_objects`,\n        `pages_total`, `number` (of the current page), and `page_size`.\n        '''\n        count = self.count(model_class, conditions)\n        pages_total = int(ceil(count / float(page_size)))\n        if page_num == -1:\n            page_num = max(pages_total, 1)\n        elif page_num < 1:\n            raise ValueError('Invalid page number: %d' % page_num)\n        offset = (page_num - 1) * page_size\n        query = 'SELECT * FROM $table'\n        if conditions:\n            query += ' WHERE ' + conditions\n        query += ' ORDER BY %s' % order_by\n        query += ' LIMIT %d, %d' % (offset, page_size)\n        query = self._substitute(query, model_class)\n        return Page(\n            objects=list(self.select(query, model_class, settings)) if count else [],\n            number_of_objects=count,\n            pages_total=pages_total,\n            number=page_num,\n            page_size=page_size\n        )", "response": "Returns a single page of model instances for the given model class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef migrate(self, migrations_package_name, up_to=9999):\n        '''\n        Executes schema migrations.\n\n        - `migrations_package_name` - fully qualified name of the Python package\n          containing the migrations.\n        - `up_to` - number of the last migration to apply.\n        '''\n        from .migrations import MigrationHistory\n        logger = logging.getLogger('migrations')\n        applied_migrations = self._get_applied_migrations(migrations_package_name)\n        modules = import_submodules(migrations_package_name)\n        unapplied_migrations = set(modules.keys()) - applied_migrations\n        for name in sorted(unapplied_migrations):\n            logger.info('Applying migration %s...', name)\n            for operation in modules[name].operations:\n                operation.apply(self)\n            self.insert([MigrationHistory(package_name=migrations_package_name, module_name=name, applied=datetime.date.today())])\n            if int(name[:4]) >= up_to:\n                break", "response": "Executes schema migrations.\n\n        - `migrations_package_name` - fully qualified name of the Python package\n          containing the migrations.\n        - `up_to` - number of the last migration to apply."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _substitute(self, query, model_class=None):\n        '''\n        Replaces $db and $table placeholders in the query.\n        '''\n        if '$' in query:\n            mapping = dict(db=\"`%s`\" % self.db_name)\n            if model_class:\n                mapping['table'] = \"`%s`.`%s`\" % (self.db_name, model_class.table_name())\n            query = Template(query).safe_substitute(mapping)\n        return query", "response": "Replaces any placeholders in the query with the corresponding database and table names."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _try_get_string(dev, index, langid = None, default_str_i0 = \"\",\n        default_access_error = \"Error Accessing String\"):\n    \"\"\" try to get a string, but return a string no matter what\n    \"\"\"\n    if index == 0 :\n        string = default_str_i0\n    else:\n        try:\n            if langid is None:\n                string = util.get_string(dev, index)\n            else:\n                string = util.get_string(dev, index, langid)\n        except :\n            string = default_access_error\n    return string", "response": "try to get a string from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _try_lookup(table, value, default = \"\"):\n    try:\n        string = table[ value ]\n    except KeyError:\n        string = default\n    return string", "response": "try to get a string from the lookup table"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show_devices(verbose=False, **kwargs):\n    kwargs[\"find_all\"] = True\n    devices = find(**kwargs)\n    strings = \"\"\n    for device in devices:\n        if not verbose:\n            strings +=  \"%s, %s\\n\" % (device._str(), _try_lookup(\n                _lu.device_classes, device.bDeviceClass))\n        else:\n            strings += \"%s\\n\\n\" % str(device)\n\n    return _DescriptorInfo(strings)", "response": "Show information about connected devices."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, data, timeout = None):\n        return self.device.write(self, data, timeout)", "response": "rWrite data to the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, size_or_buffer, timeout = None):\n        return self.device.read(self, size_or_buffer, timeout)", "response": "r Reads data from the endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the USB device s supported language ID codes.", "response": "def langids(self):\n        \"\"\" Return the USB device's supported language ID codes.\n\n        These are 16-bit codes familiar to Windows developers, where for\n        example instead of en-US you say 0x0409. USB_LANGIDS.pdf on the usb.org\n        developer site for more info. String requests using a LANGID not\n        in this array should not be sent to the device.\n\n        This property will cause some USB traffic the first time it is accessed\n        and cache the resulting value for future use.\n        \"\"\"\n        if self._langids is None:\n            try:\n                self._langids = util.get_langids(self)\n            except USBError:\n                self._langids = ()\n        return self._langids"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef serial_number(self):\n        if self._serial_number is None:\n            self._serial_number = util.get_string(self, self.iSerialNumber)\n        return self._serial_number", "response": "Return the USB device s serial number string descriptor."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the USB device s product string descriptor.", "response": "def product(self):\n        \"\"\" Return the USB device's product string descriptor.\n\n        This property will cause some USB traffic the first time it is accessed\n        and cache the resulting value for future use.\n        \"\"\"\n        if self._product is None:\n            self._product = util.get_string(self, self.iProduct)\n        return self._product"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the parent device.", "response": "def parent(self):\n        \"\"\" Return the parent device. \"\"\"\n        if self._has_parent is None:\n            _parent = self._ctx.backend.get_parent(self._ctx.dev)\n            self._has_parent = _parent is not None\n            if self._has_parent:\n                self._parent = Device(_parent, self._ctx.backend)\n            else:\n                self._parent = None\n        return self._parent"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef manufacturer(self):\n        if self._manufacturer is None:\n            self._manufacturer = util.get_string(self, self.iManufacturer)\n        return self._manufacturer", "response": "Return the USB device s manufacturer string descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the alternate setting for an interface.", "response": "def set_interface_altsetting(self, interface = None, alternate_setting = None):\n        r\"\"\"Set the alternate setting for an interface.\n\n        When you want to use an interface and it has more than one alternate\n        setting, you should call this method to select the appropriate\n        alternate setting. If you call the method without one or the two\n        parameters, it will be selected the first one found in the Device in\n        the same way of the set_configuration method.\n\n        Commonly, an interface has only one alternate setting and this call is\n        not necessary. For most devices, either it has more than one\n        alternate setting or not, it is not harmful to make a call to this\n        method with no arguments, as devices will silently ignore the request\n        when there is only one alternate setting, though the USB Spec allows\n        devices with no additional alternate setting return an error to the\n        Host in response to a SET_INTERFACE request.\n\n        If you are in doubt, you may want to call it with no arguments wrapped\n        by a try/except clause:\n\n        >>> try:\n        >>>     dev.set_interface_altsetting()\n        >>> except usb.core.USBError:\n        >>>     pass\n        \"\"\"\n        self._ctx.managed_set_interface(self, interface, alternate_setting)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclears the halt condition for the given endpoint.", "response": "def clear_halt(self, ep):\n        r\"\"\" Clear the halt/stall condition for the endpoint ep.\"\"\"\n        if isinstance(ep, Endpoint):\n            ep = ep.bEndpointAddress\n        self._ctx.managed_open()\n        self._ctx.backend.clear_halt(self._ctx.handle, ep)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self):\n        self._ctx.managed_open()\n        self._ctx.dispose(self, False)\n        self._ctx.backend.reset_device(self._ctx.handle)\n        self._ctx.dispose(self, True)", "response": "r Reset the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self, endpoint, data, timeout = None):\n        backend = self._ctx.backend\n\n        fn_map = {\n                    util.ENDPOINT_TYPE_BULK:backend.bulk_write,\n                    util.ENDPOINT_TYPE_INTR:backend.intr_write,\n                    util.ENDPOINT_TYPE_ISO:backend.iso_write\n                }\n\n        intf, ep = self._ctx.setup_request(self, endpoint)\n        fn = fn_map[util.endpoint_type(ep.bmAttributes)]\n\n        return fn(\n                self._ctx.handle,\n                ep.bEndpointAddress,\n                intf.bInterfaceNumber,\n                _interop.as_array(data),\n                self.__get_timeout(timeout)\n            )", "response": "r This method writes data to the endpoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(self, endpoint, size_or_buffer, timeout = None):\n        backend = self._ctx.backend\n\n        fn_map = {\n                    util.ENDPOINT_TYPE_BULK:backend.bulk_read,\n                    util.ENDPOINT_TYPE_INTR:backend.intr_read,\n                    util.ENDPOINT_TYPE_ISO:backend.iso_read\n                }\n\n        intf, ep = self._ctx.setup_request(self, endpoint)\n        fn = fn_map[util.endpoint_type(ep.bmAttributes)]\n\n        if isinstance(size_or_buffer, array.array):\n            buff = size_or_buffer\n        else: # here we consider it is a integer\n            buff = util.create_buffer(size_or_buffer)\n\n        ret = fn(\n                self._ctx.handle,\n                ep.bEndpointAddress,\n                intf.bInterfaceNumber,\n                buff,\n                self.__get_timeout(timeout))\n\n        if isinstance(size_or_buffer, array.array):\n            return ret\n        elif ret != len(buff) * buff.itemsize:\n            return buff[:ret]\n        else:\n            return buff", "response": "r Reads data from the endpoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ctrl_transfer(self, bmRequestType, bRequest, wValue=0, wIndex=0,\n            data_or_wLength = None, timeout = None):\n        r\"\"\"Do a control transfer on the endpoint 0.\n\n        This method is used to issue a control transfer over the endpoint 0\n        (endpoint 0 is required to always be a control endpoint).\n\n        The parameters bmRequestType, bRequest, wValue and wIndex are the same\n        of the USB Standard Control Request format.\n\n        Control requests may or may not have a data payload to write/read.\n        In cases which it has, the direction bit of the bmRequestType\n        field is used to infer the desired request direction. For\n        host to device requests (OUT), data_or_wLength parameter is\n        the data payload to send, and it must be a sequence type convertible\n        to an array object. In this case, the return value is the number\n        of bytes written in the data payload. For device to host requests\n        (IN), data_or_wLength is either the wLength parameter of the control\n        request specifying the number of bytes to read in data payload, and\n        the return value is an array object with data read, or an array\n        object which the data will be read to, and the return value is the\n        number of bytes read.\n        \"\"\"\n        try:\n            buff = util.create_buffer(data_or_wLength)\n        except TypeError:\n            buff = _interop.as_array(data_or_wLength)\n\n        self._ctx.managed_open()\n\n        # Thanks to Johannes Stezenbach to point me out that we need to\n        # claim the recipient interface\n        recipient = bmRequestType & 3\n        rqtype = bmRequestType & (3 << 5)\n        if recipient == util.CTRL_RECIPIENT_INTERFACE \\\n                and rqtype != util.CTRL_TYPE_VENDOR:\n            interface_number = wIndex & 0xff\n            self._ctx.managed_claim_interface(self, interface_number)\n\n        ret = self._ctx.backend.ctrl_transfer(\n                                    self._ctx.handle,\n                                    bmRequestType,\n                                    bRequest,\n                                    wValue,\n                                    wIndex,\n                                    buff,\n                                    self.__get_timeout(timeout))\n\n        if isinstance(data_or_wLength, array.array) \\\n                or util.ctrl_direction(bmRequestType) == util.CTRL_OUT:\n            return ret\n        elif ret != len(buff) * buff.itemsize:\n            return buff[:ret]\n        else:\n            return buff", "response": "r Perform a control transfer over the USB control endpoint 0."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetach a kernel driver from the specified interface.", "response": "def detach_kernel_driver(self, interface):\n        r\"\"\"Detach a kernel driver.\n\n        If successful, you will then be able to perform I/O.\n\n        The interface parameter is the device interface number to detach the\n        driver from.\n        \"\"\"\n        self._ctx.managed_open()\n        self._ctx.backend.detach_kernel_driver(\n            self._ctx.handle,\n            interface)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef attach_kernel_driver(self, interface):\n        self._ctx.managed_open()\n        self._ctx.backend.attach_kernel_driver(\n            self._ctx.handle,\n            interface)", "response": "Re - attach an interface s kernel driver to the kernel driver."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to locate a library in a list of libraries.", "response": "def locate_library (candidates, find_library=ctypes.util.find_library):\n    \"\"\"Tries to locate a library listed in candidates using the given\n    find_library() function (or ctypes.util.find_library).\n    Returns the first library found, which can be the library's name\n    or the path to the library file, depending on find_library().\n    Returns None if no library is found.\n\n    arguments:\n    * candidates   -- iterable with library names\n    * find_library -- function that takes one positional arg (candidate)\n                      and returns a non-empty str if a library has been found.\n                      Any \"false\" value (None,False,empty str) is interpreted\n                      as \"library not found\".\n                      Defaults to ctypes.util.find_library if not given or\n                      None.\n    \"\"\"\n    if find_library is None:\n        find_library = ctypes.util.find_library\n\n    use_dll_workaround = (\n        sys.platform == 'win32' and find_library is ctypes.util.find_library\n    )\n\n    for candidate in candidates:\n        # Workaround for CPython 3.3 issue#16283 / pyusb #14\n        if use_dll_workaround:\n            candidate += '.dll'\n\n        libname = find_library(candidate)\n        if libname:\n            return libname\n    # -- end for\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_library(lib, name=None, lib_cls=None):\n    try:\n        if lib_cls:\n            return lib_cls(lib)\n        else:\n            return ctypes.CDLL(lib)\n    except Exception:\n        if name:\n            lib_msg = '%s (%s)' % (name, lib)\n        else:\n            lib_msg = lib\n\n        lib_msg += ' could not be loaded'\n\n        if sys.platform == 'cygwin':\n            lib_msg += ' in cygwin'\n        _LOGGER.error(lib_msg, exc_info=True)\n        return None", "response": "Loads a library. Catches and logs exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlocates and loads a library.", "response": "def load_locate_library(candidates, cygwin_lib, name,\n                        win_cls=None, cygwin_cls=None, others_cls=None,\n                        find_library=None, check_symbols=None):\n    \"\"\"Locates and loads a library.\n\n    Returns: the loaded library\n\n    arguments:\n    * candidates    -- candidates list for locate_library()\n    * cygwin_lib    -- name of the cygwin library\n    * name          -- lib identifier (for logging). Defaults to None.\n    * win_cls       -- class that is used to instantiate the library on\n                       win32 platforms. Defaults to None (-> ctypes.CDLL).\n    * cygwin_cls    -- library class for cygwin platforms.\n                       Defaults to None (-> ctypes.CDLL).\n    * others_cls    -- library class for all other platforms.\n                       Defaults to None (-> ctypes.CDLL).\n    * find_library  -- see locate_library(). Defaults to None.\n    * check_symbols -- either None or a list of symbols that the loaded lib\n                       must provide (hasattr(<>)) in order to be considered\n                       valid. LibraryMissingSymbolsException is raised if\n                       any symbol is missing.\n\n    raises:\n    * NoLibraryCandidatesException\n    * LibraryNotFoundException\n    * LibraryNotLoadedException\n    * LibraryMissingSymbolsException\n    \"\"\"\n    if sys.platform == 'cygwin':\n        if cygwin_lib:\n            loaded_lib = load_library(cygwin_lib, name, cygwin_cls)\n        else:\n            raise NoLibraryCandidatesException(name)\n    elif candidates:\n        lib = locate_library(candidates, find_library)\n        if lib:\n            if sys.platform == 'win32':\n                loaded_lib = load_library(lib, name, win_cls)\n            else:\n                loaded_lib = load_library(lib, name, others_cls)\n        else:\n            _LOGGER.error('%r could not be found', (name or candidates))\n            raise LibraryNotFoundException(name)\n    else:\n        raise NoLibraryCandidatesException(name)\n\n    if loaded_lib is None:\n        raise LibraryNotLoadedException(name)\n    elif check_symbols:\n        symbols_missing = [\n                    s for s in check_symbols if not hasattr(loaded_lib, s)\n        ]\n        if symbols_missing:\n            msg = ('%r, missing symbols: %r', lib, symbols_missing )\n            _LOGGER.error(msg)\n            raise LibraryMissingSymbolsException(lib)\n        else:\n            return loaded_lib\n    else:\n        return loaded_lib"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_status(dev, recipient = None):\n    bmRequestType, wIndex = _parse_recipient(recipient, util.CTRL_IN)\n    ret = dev.ctrl_transfer(bmRequestType = bmRequestType,\n                            bRequest = 0x00,\n                            wIndex = wIndex,\n                            data_or_wLength = 2)\n    return ret[0] | (ret[1] << 8)", "response": "r Return the status of the specified recipient."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_descriptor(dev, desc_size, desc_type, desc_index, wIndex = 0):\n    wValue = desc_index | (desc_type << 8)\n\n    bmRequestType = util.build_request_type(\n                        util.CTRL_IN,\n                        util.CTRL_TYPE_STANDARD,\n                        util.CTRL_RECIPIENT_DEVICE)\n\n    return dev.ctrl_transfer(\n            bmRequestType = bmRequestType,\n            bRequest = 0x06,\n            wValue = wValue,\n            wIndex = wIndex,\n            data_or_wLength = desc_size)", "response": "r Return the specified descriptor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_descriptor(dev, desc, desc_type, desc_index, wIndex = None):\n    wValue = desc_index | (desc_type << 8)\n\n    bmRequestType = util.build_request_type(\n                        util.CTRL_OUT,\n                        util.CTRL_TYPE_STANDARD,\n                        util.CTRL_RECIPIENT_DEVICE)\n\n    dev.ctrl_transfer(\n        bmRequestType = bmRequestType,\n        bRequest = 0x07,\n        wValue = wValue,\n        wIndex = wIndex,\n        data_or_wLength = desc)", "response": "r Update an existing descriptor or add a new one."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_configuration(dev):\n    bmRequestType = util.build_request_type(\n                            util.CTRL_IN,\n                            util.CTRL_TYPE_STANDARD,\n                            util.CTRL_RECIPIENT_DEVICE)\n\n    return dev.ctrl_transfer(\n                bmRequestType,\n                bRequest = 0x08,\n                data_or_wLength = 1)[0]", "response": "r Returns the current active configuration of the device."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_interface(dev, bInterfaceNumber):\n    bmRequestType = util.build_request_type(\n                            util.CTRL_IN,\n                            util.CTRL_TYPE_STANDARD,\n                            util.CTRL_RECIPIENT_INTERFACE)\n\n    return dev.ctrl_transfer(\n                bmRequestType = bmRequestType,\n                bRequest = 0x0a,\n                wIndex = bInterfaceNumber,\n                data_or_wLength = 1)[0]", "response": "rGet the current alternate setting of the interface."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconfiguring a queue for accepting characters and return the queue and input thread", "response": "def configInputQueue():\n    \"\"\" configure a queue for accepting characters and return the queue\n    \"\"\"\n    def captureInput(iqueue):\n        while True:\n            c = getch()\n            if c == '\\x03' or c == '\\x04':    # end on ctrl+c / ctrl+d\n                log.debug(\"Break received (\\\\x{0:02X})\".format(ord(c)))\n                iqueue.put(c)\n                break\n            log.debug(\n                \"Input Char '{}' received\".format(\n                    c if c != '\\r' else '\\\\r'))\n            iqueue.put(c)\n\n    input_queue = queue.Queue()\n    input_thread = threading.Thread(target=lambda: captureInput(input_queue))\n    input_thread.daemon = True\n    input_thread.start()\n    return input_queue, input_thread"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fmt_text(text):\n    PRINTABLE_CHAR = set(\n        list(range(ord(' '), ord('~') + 1)) + [ord('\\r'), ord('\\n')])\n    newtext = (\"\\\\x{:02X}\".format(\n        c) if c not in PRINTABLE_CHAR else chr(c) for c in text)\n    textlines = \"\\r\\n\".join(l.strip('\\r')\n                            for l in \"\".join(newtext).split('\\n'))\n    return textlines", "response": "convert characters that aren t printable to hex format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ftdi_to_clkbits(baudrate):  # from libftdi\n    clk = 48000000\n    clk_div = 16\n    frac_code = [0, 3, 2, 4, 1, 5, 6, 7]\n    actual_baud = 0\n    if baudrate >= clk / clk_div:\n        encoded_divisor = 0\n        actual_baud = (clk // clk_div)\n    elif baudrate >= clk / (clk_div + clk_div / 2):\n        encoded_divisor = 1\n        actual_baud = clk // (clk_div + clk_div // 2)\n    elif baudrate >= clk / (2 * clk_div):\n        encoded_divisor = 2\n        actual_baud = clk // (2 * clk_div)\n    else:\n        # We divide by 16 to have 3 fractional bits and one bit for rounding\n        divisor = clk * 16 // clk_div // baudrate\n        best_divisor = (divisor + 1) // 2\n        if best_divisor > 0x20000:\n            best_divisor = 0x1ffff\n        actual_baud = clk * 16 // clk_div // best_divisor\n        actual_baud = (actual_baud + 1) // 2\n        encoded_divisor = ((best_divisor >> 3) +\n                           (frac_code[best_divisor & 0x7] << 14))\n\n    value = encoded_divisor & 0xFFFF\n    index = encoded_divisor >> 16\n    return actual_baud, value, index", "response": "Convert a baudrate from FTDI to CLK bits."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a single entry from the USB interface and add it to the queue and sleep for interval", "response": "def _read(self):\n        \"\"\" check ep for data, add it to queue and sleep for interval \"\"\"\n        while self._rxactive:\n            try:\n                rv = self._ep_in.read(self._ep_in.wMaxPacketSize)\n                if self._isFTDI:\n                    status = rv[:2] # FTDI prepends 2 flow control characters,\n                    # modem status and line status of the UART\n                    if status[0] != 1 or status[1] != 0x60:\n                        log.info(\n                            \"USB Status: 0x{0:02X} 0x{1:02X}\".format(\n                                *status))\n                    rv = rv[2:]\n                for rvi in rv:\n                    self._rxqueue.put(rvi)\n            except usb.USBError as e:\n                log.warn(\"USB Error on _read {}\".format(e))\n                return\n            \n            time.sleep(self._rxinterval)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _resetFTDI(self):\n        if not self._isFTDI:\n            return\n        txdir = 0           # 0:OUT, 1:IN\n        req_type = 2        # 0:std, 1:class, 2:vendor\n        recipient = 0       # 0:device, 1:interface, 2:endpoint, 3:other\n        req_type = (txdir << 7) + (req_type << 5) + recipient\n        self.device.ctrl_transfer(\n            bmRequestType=req_type,\n            bRequest=0,     # RESET\n            wValue=0,       # RESET\n            wIndex=1,\n            data_or_wLength=0)", "response": "reset the FTDI device"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _flushFTDI(self, rx=True, tx=True):\n        if not self._isFTDI:\n            return\n        txdir = 0                               # 0:OUT, 1:IN\n        req_type = 2 if self._isFTDI else 1     # 0:std, 1:class, 2:vendor\n        # 0:device, 1:interface, 2:endpoint, 3:other\n        recipient = 0 if self._isFTDI else 1\n        req_type = (txdir << 7) + (req_type << 5) + recipient\n        if rx:\n            self.device.ctrl_transfer(\n                bmRequestType=req_type,\n                bRequest=0,     # RESET\n                wValue=1,       # PURGE RX\n                wIndex=1,       # INTERFACE 1\n                data_or_wLength=0)\n        if tx:\n            self.device.ctrl_transfer(\n                bmRequestType=req_type,\n                bRequest=0,     # RESET\n                wValue=2,       # PURGE TX\n                wIndex=1,       # INTERFACE 1\n                data_or_wLength=0)", "response": "flush rx and tx buffers for ftdi device"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a bulk write request to the endpoint specified.", "response": "def bulkWrite(self, endpoint, buffer, timeout = 100):\n        r\"\"\"Perform a bulk write request to the endpoint specified.\n\n            Arguments:\n                endpoint: endpoint number.\n                buffer: sequence data buffer to write.\n                        This parameter can be any sequence type.\n                timeout: operation timeout in milliseconds. (default: 100)\n            Returns the number of bytes written.\n        \"\"\"\n        return self.dev.write(endpoint, buffer, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bulkRead(self, endpoint, size, timeout = 100):\n        return self.dev.read(endpoint, size, timeout)", "response": "Performs a bulk read request to the endpoint specified."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef interruptWrite(self, endpoint, buffer, timeout = 100):\n        return self.dev.write(endpoint, buffer, timeout)", "response": "Perform a interrupt write request to the endpoint specified."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interruptRead(self, endpoint, size, timeout = 100):\n        return self.dev.read(endpoint, size, timeout)", "response": "Performs a interrupt read request to the endpoint specified."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclaim the interface with the Operating System.", "response": "def claimInterface(self, interface):\n        r\"\"\"Claims the interface with the Operating System.\n\n        Arguments:\n            interface: interface number or an Interface object.\n        \"\"\"\n        if isinstance(interface, Interface):\n            interface = interface.interfaceNumber\n\n        util.claim_interface(self.dev, interface)\n        self.__claimed_interface = interface"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setAltInterface(self, alternate):\n        if isinstance(alternate, Interface):\n           alternate = alternate.alternateSetting\n\n        self.dev.set_interface_altsetting(self.__claimed_interface, alternate)", "response": "r Sets the active alternate setting of the current interface."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getDescriptor(self, desc_type, desc_index, length, endpoint = -1):\n        return control.get_descriptor(self.dev, length, desc_type, desc_index)", "response": "r Returns a descriptor from the device identified by the type\n        and index of the descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_endpoint_descriptor(self, dev, ep, intf, alt, config):\n        _not_implemented(self.get_endpoint_descriptor)", "response": "r Returns an endpoint descriptor of the given device."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a bulk write on the specified endpoint.", "response": "def bulk_write(self, dev_handle, ep, intf, data, timeout):\n        r\"\"\"Perform a bulk write.\n\n        dev_handle is the value returned by the open_device() method.\n        The ep parameter is the bEndpointAddress field whose endpoint\n        the data will be sent to. intf is the bInterfaceNumber field\n        of the interface containing the endpoint. The data parameter\n        is the data to be sent. It must be an instance of the array.array\n        class. The timeout parameter specifies a time limit to the operation\n        in miliseconds.\n\n        The method returns the number of bytes written.\n        \"\"\"\n        _not_implemented(self.bulk_write)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform an interrupt write.", "response": "def intr_write(self, dev_handle, ep, intf, data, timeout):\n        r\"\"\"Perform an interrupt write.\n\n        dev_handle is the value returned by the open_device() method.\n        The ep parameter is the bEndpointAddress field whose endpoint\n        the data will be sent to. intf is the bInterfaceNumber field\n        of the interface containing the endpoint. The data parameter\n        is the data to be sent. It must be an instance of the array.array\n        class. The timeout parameter specifies a time limit to the operation\n        in miliseconds.\n\n        The method returns the number of bytes written.\n        \"\"\"\n        _not_implemented(self.intr_write)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef intr_read(self, dev_handle, ep, intf, size, timeout):\n        _not_implemented(self.intr_read)", "response": "Perform an interrupt read from the specified device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iso_write(self, dev_handle, ep, intf, data, timeout):\n        _not_implemented(self.iso_write)", "response": "Perform an isochronous write to the specified endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iso_read(self, dev_handle, ep, intf, size, timeout):\n        _not_implemented(self.iso_read)", "response": "Perform an isochronous read from the device."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ctrl_transfer(self,\n                      dev_handle,\n                      bmRequestType,\n                      bRequest,\n                      wValue,\n                      wIndex,\n                      data,\n                      timeout):\n        r\"\"\"Perform a control transfer on the endpoint 0.\n\n        The direction of the transfer is inferred from the bmRequestType\n        field of the setup packet.\n\n        dev_handle is the value returned by the open_device() method.\n        bmRequestType, bRequest, wValue and wIndex are the same fields\n        of the setup packet. data is an array object, for OUT requests\n        it contains the bytes to transmit in the data stage and for\n        IN requests it is the buffer to hold the data read. The number\n        of bytes requested to transmit or receive is equal to the length\n        of the array times the data.itemsize field. The timeout parameter\n        specifies a time limit to the operation in miliseconds.\n\n        Return the number of bytes written (for OUT transfers) or the data\n        read (for IN transfers), as an array.array object.\n        \"\"\"\n        _not_implemented(self.ctrl_transfer)", "response": "r Perform a control transfer on the endpoint 0."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_translations(self):\n\n        pcell = PhonopyAtoms(numbers=[1],\n                             scaled_positions=[[0, 0, 0]],\n                             cell=np.diag([1, 1, 1]))\n        smat = self._supercell_matrix\n        self._trans_s = get_supercell(pcell, smat).get_scaled_positions()\n        self._trans_p = np.dot(self._trans_s, self._supercell_matrix.T)\n        self._N = len(self._trans_s)", "response": "Set the primitive translations in supercell\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_index_map(self, atom_mapping):\n\n        lattice = self._phonon.supercell.get_cell()\n        natom = len(self._ideal_positions)\n        index_map_inv = np.zeros((self._N, natom), dtype='intc')\n        for i, shift in enumerate(self._trans_s):\n            for j, p in enumerate(self._ideal_positions - shift):  # minus r_i\n                diff = self._ideal_positions - p\n                diff -= np.rint(diff)\n                dist = np.sqrt((np.dot(diff, lattice) ** 2).sum(axis=1))\n                # k is index in _ideal_positions.\n                k = np.where(dist < self._symprec)[0][0]\n                index_map_inv[i, j] = k\n        self._index_map_inv = index_map_inv\n\n        self._atom_mapping = np.zeros(len(atom_mapping), dtype='int')\n        for i, idx in enumerate(atom_mapping):\n            if idx is None:\n                self._atom_mapping[i] = -1\n            else:\n                self._atom_mapping[i] = idx", "response": "Sets the index map for the atom indices in the system."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_unfolding_weights(self):\n\n        eigvecs = self._eigvecs[self._q_count]\n        dtype = \"c%d\" % (np.dtype('double').itemsize * 2)\n        weights = np.zeros(eigvecs.shape[1], dtype=dtype)\n        q_p = self._qpoints_p[self._q_count]  # k\n        q_s = self._qpoints_s[self._q_count]  # K\n        diff = q_p - np.dot(q_s, np.linalg.inv(self._supercell_matrix))\n\n        # Search G points corresponding to k = G + K\n        for G in self._comm_points:\n            d = diff - G\n            d -= np.rint(d)\n            if (np.abs(d) < 1e-5).all():\n                break\n\n        e = np.zeros(eigvecs.shape[:2], dtype=dtype)\n        phases = np.exp(2j * np.pi * np.dot(self._trans_p, G))\n        for phase, indices in zip(\n                phases, self._atom_mapping[self._index_map_inv]):\n            eig_indices = (\n                np.c_[indices * 3, indices * 3 + 1, indices * 3 + 2]).ravel()\n            e += eigvecs[eig_indices, :] * phase\n        e /= self._N\n        weights[:] = (e.conj() * e).sum(axis=0)\n\n        # e = np.zeros(eigvecs.shape[:2], dtype=dtype)\n        # phases = np.exp(2j * np.pi * np.dot(self._trans_p, G))\n        # indices = self._atom_mapping\n        # eig_indices_r = (\n        #     np.c_[indices * 3, indices * 3 + 1, indices * 3 + 2]).ravel()\n        # for phase, indices in zip(\n        #         phases, self._atom_mapping[self._index_map_inv]):\n        #     eig_indices_l = (\n        #         np.c_[indices * 3, indices * 3 + 1, indices * 3 + 2]).ravel()\n        #     e += eigvecs[eig_indices_l, :] * eigvecs[eig_indices_r, :].conj() * phase\n        # e /= self._N\n        # weights[:] = e.sum(axis=0)\n\n        if (weights.imag > 1e-5).any():\n            print(\"Phonopy warning: Encountered imaginary values.\")\n\n        # assert (np.abs(weights.real.sum(axis=1) - 1) < 1e-5).all()\n\n        return weights.real", "response": "Calculate Eq. ( 7 )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfit parameters to EOS at temperaturs and return a list of the temperatures and parameters.", "response": "def run(self, verbose=False):\n        \"\"\"Fit parameters to EOS at temperatures\n\n        Even if fitting failed, simply omit the volume point. In this case,\n        the failed temperature point doesn't exist in the returned arrays.\n\n        \"\"\"\n\n        if verbose:\n            print((\"#%11s\" + \"%14s\" * 4) % (\"T\", \"E_0\", \"B_0\", \"B'_0\", \"V_0\"))\n\n        # Plus one temperature point is necessary for computing e.g. beta.\n        num_elems = self._get_num_elems(self._all_temperatures) + 1\n        if num_elems > len(self._all_temperatures):\n            num_elems -= 1\n\n        temperatures = []\n        parameters = []\n        free_energies = []\n\n        for i in range(num_elems):  # loop over temperaturs\n            if self._electronic_energies.ndim == 1:\n                el_energy = self._electronic_energies\n            else:\n                el_energy = self._electronic_energies[i]\n            fe = [ph_e + el_e\n                  for ph_e, el_e in zip(self._fe_phonon[i], el_energy)]\n\n            try:\n                ep = fit_to_eos(self._volumes, fe, self._eos)\n            except TypeError:\n                print(\"Fitting failure at T=%.1f\" % self._all_temperatures[i])\n\n            if ep is None:\n                # Simply omit volume point where the fitting failed.\n                continue\n            else:\n                [ee, eb, ebp, ev] = ep\n                t = self._all_temperatures[i]\n                temperatures.append(t)\n                parameters.append(ep)\n                free_energies.append(fe)\n\n                if verbose:\n                    print((\"%14.6f\" * 5) %\n                          (t, ep[0], ep[1] * EVAngstromToGPa, ep[2], ep[3]))\n\n        self._free_energies = np.array(free_energies)\n        self._temperatures = np.array(temperatures)\n        self._equiv_parameters = np.array(parameters)\n        self._equiv_volumes = np.array(self._equiv_parameters[:, 3])\n        self._equiv_energies = np.array(self._equiv_parameters[:, 0])\n        self._equiv_bulk_modulus = np.array(\n            self._equiv_parameters[:, 1] * EVAngstromToGPa)\n\n        self._num_elems = len(self._temperatures)\n\n        # For computing following values at temperatures, finite difference\n        # method is used. Therefore number of temperature points are needed\n        # larger than self._num_elems that nearly equals to the temparature\n        # point we expect.\n        self._set_thermal_expansion()\n        self._set_heat_capacity_P_numerical()\n        self._set_heat_capacity_P_polyfit()\n        self._set_gruneisen_parameter()  # To be run after thermal expansion.\n\n        self._len = len(self._thermal_expansions)\n        assert(self._len + 1 == self._num_elems)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms the sequence q into a vector v.", "response": "def _transform(self, q):\n        \"\"\" exp(i q.r(i)) v(i)\"\"\"\n\n        s2p = self._primitive.get_supercell_to_primitive_map()\n        p2s = self._primitive.get_primitive_to_supercell_map()\n\n        num_s = self._supercell.get_number_of_atoms()\n        num_p = self._primitive.get_number_of_atoms()\n        v = self._velocities\n\n        q_array = np.reshape(q, (-1, 3))\n        dtype = \"c%d\" % (np.dtype('double').itemsize * 2)\n        v_q = np.zeros((v.shape[0], num_p, len(q_array), 3), dtype=dtype)\n\n        for p_i, s_i in enumerate(p2s):\n            for s_j, s2p_j in enumerate(s2p):\n                if s2p_j == s_i:\n                    for q_i, pf in enumerate(\n                            self._get_phase_factor(p_i, s_j, q_array)):\n                        v_q[:, p_i, q_i, :] += pf * v[:, s_j, :]\n        return v_q"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _trim_cell(relative_axes, cell, symprec):\n    positions = cell.get_scaled_positions()\n    numbers = cell.get_atomic_numbers()\n    masses = cell.get_masses()\n    magmoms = cell.get_magnetic_moments()\n    lattice = cell.get_cell()\n    trimmed_lattice = np.dot(relative_axes.T, lattice)\n\n    trimmed_positions = []\n    trimmed_numbers = []\n    if masses is None:\n        trimmed_masses = None\n    else:\n        trimmed_masses = []\n    if magmoms is None:\n        trimmed_magmoms = None\n    else:\n        trimmed_magmoms = []\n    extracted_atoms = []\n\n    positions_in_new_lattice = np.dot(positions,\n                                      np.linalg.inv(relative_axes).T)\n    positions_in_new_lattice -= np.floor(positions_in_new_lattice)\n    trimmed_positions = np.zeros_like(positions_in_new_lattice)\n    num_atom = 0\n\n    mapping_table = np.arange(len(positions), dtype='intc')\n    for i, pos in enumerate(positions_in_new_lattice):\n        is_overlap = False\n        if num_atom > 0:\n            diff = trimmed_positions[:num_atom] - pos\n            diff -= np.rint(diff)\n            # Older numpy doesn't support axis argument.\n            # distances = np.linalg.norm(np.dot(diff, trimmed_lattice), axis=1)\n            # overlap_indices = np.where(distances < symprec)[0]\n            distances = np.sqrt(\n                np.sum(np.dot(diff, trimmed_lattice) ** 2, axis=1))\n            overlap_indices = np.where(distances < symprec)[0]\n            if len(overlap_indices) > 0:\n                assert len(overlap_indices) == 1\n                is_overlap = True\n                mapping_table[i] = extracted_atoms[overlap_indices[0]]\n\n        if not is_overlap:\n            trimmed_positions[num_atom] = pos\n            num_atom += 1\n            trimmed_numbers.append(numbers[i])\n            if masses is not None:\n                trimmed_masses.append(masses[i])\n            if magmoms is not None:\n                trimmed_magmoms.append(magmoms[i])\n            extracted_atoms.append(i)\n\n    # scale is not always to become integer.\n    scale = 1.0 / np.linalg.det(relative_axes)\n    if len(numbers) == np.rint(scale * len(trimmed_numbers)):\n        trimmed_cell = PhonopyAtoms(\n            numbers=trimmed_numbers,\n            masses=trimmed_masses,\n            magmoms=trimmed_magmoms,\n            scaled_positions=trimmed_positions[:num_atom],\n            cell=trimmed_lattice,\n            pbc=True)\n        return trimmed_cell, extracted_atoms, mapping_table\n    else:\n        return False", "response": "Trim overlapping atoms in a supercell."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning reduced bases of a set of lists", "response": "def get_reduced_bases(lattice,\n                      method='delaunay',\n                      tolerance=1e-5):\n    \"\"\"Search kinds of shortest basis vectors\n\n    Parameters\n    ----------\n    lattice : ndarray or list of list\n        Basis vectors by row vectors, [a, b, c]^T\n        shape=(3, 3)\n    method : str\n        delaunay: Delaunay reduction\n        niggli: Niggli reduction\n    tolerance : float\n        Tolerance to find shortest basis vecotrs\n\n    Returns\n    --------\n    Reduced basis as row vectors, [a_red, b_red, c_red]^T\n        dtype='double'\n        shape=(3, 3)\n        order='C'\n\n    \"\"\"\n\n    if method == 'niggli':\n        return spg.niggli_reduce(lattice, eps=tolerance)\n    else:\n        return spg.delaunay_reduce(lattice, eps=tolerance)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding shortest atomic pair vectors Note ---- Shortest vectors from an atom in primitive cell to an atom in supercell in the fractional coordinates of primitive cell. If an atom in supercell is on the border centered at an atom in primitive and there are multiple vectors that have the same distance (up to tolerance) and different directions, several shortest vectors are stored. In fact, this method is not limited to search shortest vectors between sueprcell atoms and primitive cell atoms, but can be used to measure shortest vectors between atoms in periodic supercell lattice frame. Parameters ---------- supercell_bases : ndarray Supercell basis vectors as row vectors, (a, b, c)^T. Must be order='C'. dtype='double' shape=(3, 3) supercell_pos : array_like Atomic positions in fractional coordinates of supercell. dtype='double' shape=(size_super, 3) primitive_pos : array_like Atomic positions in fractional coordinates of supercell. Note that not in fractional coodinates of primitive cell. dtype='double' shape=(size_prim, 3) symprec : float, optional, default=1e-5 Tolerance to find equal distances of vectors Returns ------- shortest_vectors : ndarray Shortest vectors in supercell coordinates. The 27 in shape is the possible maximum number of elements. dtype='double' shape=(size_super, size_prim, 27, 3) multiplicities : ndarray Number of equidistance shortest vectors dtype='intc' shape=(size_super, size_prim)", "response": "def get_smallest_vectors(supercell_bases,\n                         supercell_pos,\n                         primitive_pos,\n                         symprec=1e-5):\n    \"\"\"Find shortest atomic pair vectors\n\n    Note\n    ----\n    Shortest vectors from an atom in primitive cell to an atom in\n    supercell in the fractional coordinates of primitive cell. If an\n    atom in supercell is on the border centered at an atom in\n    primitive and there are multiple vectors that have the same\n    distance (up to tolerance) and different directions, several\n    shortest vectors are stored.\n    In fact, this method is not limited to search shortest vectors between\n    sueprcell atoms and primitive cell atoms, but can be used to measure\n    shortest vectors between atoms in periodic supercell lattice frame.\n\n    Parameters\n    ----------\n    supercell_bases : ndarray\n        Supercell basis vectors as row vectors, (a, b, c)^T. Must be order='C'.\n        dtype='double'\n        shape=(3, 3)\n    supercell_pos : array_like\n        Atomic positions in fractional coordinates of supercell.\n        dtype='double'\n        shape=(size_super, 3)\n    primitive_pos : array_like\n        Atomic positions in fractional coordinates of supercell. Note that not\n        in fractional coodinates of primitive cell.\n        dtype='double'\n        shape=(size_prim, 3)\n    symprec : float, optional, default=1e-5\n        Tolerance to find equal distances of vectors\n\n    Returns\n    -------\n    shortest_vectors : ndarray\n        Shortest vectors in supercell coordinates. The 27 in shape is the\n        possible maximum number of elements.\n        dtype='double'\n        shape=(size_super, size_prim, 27, 3)\n    multiplicities : ndarray\n        Number of equidistance shortest vectors\n        dtype='intc'\n        shape=(size_super, size_prim)\n\n    \"\"\"\n\n    reduced_bases = get_reduced_bases(supercell_bases,\n                                      method='delaunay',\n                                      tolerance=symprec)\n    trans_mat_float = np.dot(supercell_bases, np.linalg.inv(reduced_bases))\n    trans_mat = np.rint(trans_mat_float).astype(int)\n    assert (np.abs(trans_mat_float - trans_mat) < 1e-8).all()\n    trans_mat_inv_float = np.linalg.inv(trans_mat)\n    trans_mat_inv = np.rint(trans_mat_inv_float).astype(int)\n    assert (np.abs(trans_mat_inv_float - trans_mat_inv) < 1e-8).all()\n\n    # Reduce all positions into the cell formed by the reduced bases.\n    supercell_fracs = np.dot(supercell_pos, trans_mat)\n    supercell_fracs -= np.rint(supercell_fracs)\n    supercell_fracs = np.array(supercell_fracs, dtype='double', order='C')\n    primitive_fracs = np.dot(primitive_pos, trans_mat)\n    primitive_fracs -= np.rint(primitive_fracs)\n    primitive_fracs = np.array(primitive_fracs, dtype='double', order='C')\n\n    # For each vector, we will need to consider all nearby images in the\n    # reduced bases.\n    lattice_points = np.array([[i, j, k]\n                               for i in (-1, 0, 1)\n                               for j in (-1, 0, 1)\n                               for k in (-1, 0, 1)],\n                              dtype='intc', order='C')\n\n    # Here's where things get interesting.\n    # We want to avoid manually iterating over all possible pairings of\n    # supercell atoms and primitive atoms, because doing so creates a\n    # tight loop in larger structures that is difficult to optimize.\n    #\n    # Furthermore, it seems wise to call numpy.dot on as large of an array\n    # as possible, since numpy can shell out to BLAS to handle the\n    # real heavy lifting.\n\n    shortest_vectors = np.zeros(\n        (len(supercell_fracs), len(primitive_fracs), 27, 3),\n        dtype='double', order='C')\n    multiplicity = np.zeros((len(supercell_fracs), len(primitive_fracs)),\n                            dtype='intc', order='C')\n    import phonopy._phonopy as phonoc\n    phonoc.gsv_set_smallest_vectors(\n        shortest_vectors,\n        multiplicity,\n        supercell_fracs,\n        primitive_fracs,\n        lattice_points,\n        np.array(reduced_bases.T, dtype='double', order='C'),\n        np.array(trans_mat_inv.T, dtype='intc', order='C'),\n        symprec)\n\n    # # For every atom in the supercell and every atom in the primitive cell,\n    # # we want 27 images of the vector between them.\n    # #\n    # # 'None' is used to insert trivial axes to make these arrays broadcast.\n    # #\n    # # shape: (size_super, size_prim, 27, 3)\n    # candidate_fracs = (\n    #     supercell_fracs[:, None, None, :]    # shape: (size_super, 1, 1, 3)\n    #     - primitive_fracs[None, :, None, :]  # shape: (1, size_prim, 1, 3)\n    #     + lattice_points[None, None, :, :]   # shape: (1, 1, 27, 3)\n    # )\n\n    # # To compute the lengths, we want cartesian positions.\n    # #\n    # # Conveniently, calling 'numpy.dot' between a 4D array and a 2D array\n    # # does vector-matrix multiplication on each row vector in the last axis\n    # # of the 4D array.\n    # #\n    # # shape: (size_super, size_prim, 27)\n    # lengths = np.array(np.sqrt(\n    #     np.sum(np.dot(candidate_fracs, reduced_bases)**2, axis=-1)),\n    #                    dtype='double', order='C')\n\n    # # Create the output, initially consisting of all candidate vectors scaled\n    # # by the primitive cell.\n    # #\n    # # shape: (size_super, size_prim, 27, 3)\n    # candidate_vectors = np.array(np.dot(candidate_fracs, trans_mat_inv),\n    #                              dtype='double', order='C')\n\n    # # The last final bits are done in C.\n    # #\n    # # We will gather the shortest ones from each list of 27 vectors.\n    # shortest_vectors = np.zeros_like(candidate_vectors,\n    #                                  dtype='double', order='C')\n    # multiplicity = np.zeros(shortest_vectors.shape[:2], dtype='intc',\n    #                         order='C')\n\n    # import phonopy._phonopy as phonoc\n    # phonoc.gsv_copy_smallest_vectors(shortest_vectors,\n    #                                  multiplicity,\n    #                                  candidate_vectors,\n    #                                  lengths,\n    #                                  symprec)\n\n    return shortest_vectors, multiplicity"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute a permutation for every space group operation.", "response": "def compute_all_sg_permutations(positions,  # scaled positions\n                                rotations,  # scaled\n                                translations,  # scaled\n                                lattice,  # column vectors\n                                symprec):\n    \"\"\"Compute a permutation for every space group operation.\n\n    See 'compute_permutation_for_rotation' for more info.\n\n    Output has shape (num_rot, num_pos)\n\n    \"\"\"\n\n    out = []  # Finally the shape is fixed as (num_sym, num_pos_of_supercell).\n    for (sym, t) in zip(rotations, translations):\n        rotated_positions = np.dot(positions, sym.T) + t\n        out.append(compute_permutation_for_rotation(positions,\n                                                    rotated_positions,\n                                                    lattice,\n                                                    symprec))\n    return np.array(out, dtype='intc', order='C')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the overall permutation for a rotation of a set of positions.", "response": "def compute_permutation_for_rotation(positions_a,  # scaled positions\n                                     positions_b,\n                                     lattice,  # column vectors\n                                     symprec):\n    \"\"\"Get the overall permutation such that\n\n        positions_a[perm[i]] == positions_b[i]   (modulo the lattice)\n\n    or in numpy speak,\n\n        positions_a[perm] == positions_b   (modulo the lattice)\n\n    This version is optimized for the case where positions_a and positions_b\n    are related by a rotation.\n\n    \"\"\"\n\n    # Sort both sides by some measure which is likely to produce a small\n    # maximum value of (sorted_rotated_index - sorted_original_index).\n    # The C code is optimized for this case, reducing an O(n^2)\n    # search down to ~O(n). (for O(n log n) work overall, including the sort)\n    #\n    # We choose distance from the nearest bravais lattice point as our measure.\n    def sort_by_lattice_distance(fracs):\n        carts = np.dot(fracs - np.rint(fracs), lattice.T)\n        perm = np.argsort(np.sum(carts**2, axis=1))\n        sorted_fracs = np.array(fracs[perm], dtype='double', order='C')\n        return perm, sorted_fracs\n\n    (perm_a, sorted_a) = sort_by_lattice_distance(positions_a)\n    (perm_b, sorted_b) = sort_by_lattice_distance(positions_b)\n\n    # Call the C code on our conditioned inputs.\n    perm_between = _compute_permutation_c(sorted_a,\n                                          sorted_b,\n                                          lattice,\n                                          symprec)\n\n    # Compose all of the permutations for the full permutation.\n    #\n    # Note the following properties of permutation arrays:\n    #\n    # 1. Inverse:         if  x[perm] == y  then  x == y[argsort(perm)]\n    # 2. Associativity:   x[p][q] == x[p[q]]\n    return perm_a[perm_between][np.argsort(perm_b)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_permutation_c(positions_a,  # scaled positions\n                           positions_b,\n                           lattice,  # column vectors\n                           symprec):\n    \"\"\"Version of '_compute_permutation_for_rotation' which just directly\n    calls the C function, without any conditioning of the data.\n    Skipping the conditioning step makes this EXTREMELY slow on large\n    structures.\n\n    \"\"\"\n\n    permutation = np.zeros(shape=(len(positions_a),), dtype='intc')\n\n    def permutation_error():\n        raise ValueError(\"Input forces are not enough to calculate force constants, \"\n                         \"or something wrong (e.g. crystal structure does not match).\")\n\n    try:\n        import phonopy._phonopy as phonoc\n        is_found = phonoc.compute_permutation(permutation,\n                                              lattice,\n                                              positions_a,\n                                              positions_b,\n                                              symprec)\n\n        if not is_found:\n            permutation_error()\n\n    except ImportError:\n        for i, pos_b in enumerate(positions_b):\n            diffs = positions_a - pos_b\n            diffs -= np.rint(diffs)\n            diffs = np.dot(diffs, lattice.T)\n\n            possible_j = np.nonzero(\n                np.sqrt(np.sum(diffs**2, axis=1)) < symprec)[0]\n            if len(possible_j) != 1:\n                permutation_error()\n\n            permutation[i] = possible_j[0]\n\n        if -1 in permutation:\n            permutation_error()\n\n    return permutation", "response": "Version of _compute_permutation_for_rotation which just directly calls the C function with the conditioning of the data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nestimates supercell matrix from conventional cell lengths max_num_atoms and maximum number of atoms to be accepted.", "response": "def estimate_supercell_matrix(spglib_dataset,\n                              max_num_atoms=120):\n    \"\"\"Estimate supercell matrix from conventional cell\n\n    Diagonal supercell matrix is estimated from basis vector lengths\n    and maximum number of atoms to be accepted. Supercell is assumed\n    to be made from the standardized cell and to be closest to sphere\n    under keeping lattice symmetry. For triclinic, monoclinic, and\n    orthorhombic cells, multiplicities for a, b, c are not constrained\n    by symmetry. For tetragonal and hexagonal cells, multiplicities\n    for a and b are chosen to be the same, and for cubic cell, those\n    of a, b, c are the same.\n\n    Parameters\n    ----------\n    spglib_dataset : tuple\n        Spglib symmetry dataset\n    max_num_atoms : int, optional\n        Maximum number of atoms in created supercell to be tolerated.\n\n    Returns\n    -------\n    list of three integer numbers\n        Multiplicities for a, b, c basis vectors, respectively.\n\n    \"\"\"\n\n    spg_num = spglib_dataset['number']\n    num_atoms = len(spglib_dataset['std_types'])\n    lengths = _get_lattice_parameters(spglib_dataset['std_lattice'])\n\n    if spg_num <= 74:  # Triclinic, monoclinic, and orthorhombic\n        multi = _get_multiplicity_abc(num_atoms, lengths, max_num_atoms)\n    elif spg_num <= 194:  # Tetragonal and hexagonal\n        multi = _get_multiplicity_ac(num_atoms, lengths, max_num_atoms)\n    else:  # Cubic\n        multi = _get_multiplicity_a(num_atoms, lengths, max_num_atoms)\n\n    return multi"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_lattice_parameters(lattice):\n\n    return np.array(np.sqrt(np.dot(lattice.T, lattice).diagonal()),\n                    dtype='double')", "response": "Return basis vector lengths of the given lattice"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds Smith normal form for Right - low 2x2 matrix", "response": "def _second(self):\n        \"\"\"Find Smith normal form for Right-low 2x2 matrix\"\"\"\n\n        self._second_one_loop()\n        A = self._A\n        if A[2, 1] == 0:\n            return True\n        elif A[2, 1] % A[1, 1] == 0:\n            self._second_finalize()\n            self._Ps += self._L\n            self._L = []\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _second_column(self):\n\n        if self._A[1, 1] == 0 and self._A[2, 1] != 0:\n            self._swap_rows(1, 2)\n\n        if self._A[2, 1] != 0:\n            self._zero_second_column()", "response": "Right - low 2x2 matrix\n\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinalizes the second set of the logarithm.", "response": "def _second_finalize(self):\n        \"\"\"Set zero at A[2, 1]\n\n        This is possible only when A[2,1] is dividable by A[1,1].\n\n        \"\"\"\n\n        A = self._A\n        L = np.eye(3, dtype='intc')\n        L[2, 1] = -A[2, 1] // A[1, 1]\n        self._L.append(L.copy())\n        self._A = np.dot(L, self._A)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _swap_rows(self, i, j):\n\n        L = np.eye(3, dtype='intc')\n        L[i, i] = 0\n        L[j, j] = 0\n        L[i, j] = 1\n        L[j, i] = 1\n        self._L.append(L.copy())\n        self._A = np.dot(L, self._A)", "response": "Swap i and j rows\n        As the side effect determinant flips."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nflip the sign row i.", "response": "def _flip_sign_row(self, i):\n        \"\"\"Multiply -1 for all elements in row\"\"\"\n\n        L = np.eye(3, dtype='intc')\n        L[i, i] = -1\n        self._L.append(L.copy())\n        self._A = np.dot(L, self._A)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlet A [ i j ] be zero based on Bezout s identity .", "response": "def _set_zero(self, i, j, a, b, r, s, t):\n        \"\"\"Let A[i, j] be zero based on Bezout's identity\n\n           [ii ij]\n           [ji jj] is a (k,k) minor of original 3x3 matrix.\n\n        \"\"\"\n\n        L = np.eye(3, dtype='intc')\n        L[i, i] = s\n        L[i, j] = t\n        L[j, i] = -b // r\n        L[j, j] = a // r\n        self._L.append(L.copy())\n        self._A = np.dot(L, self._A)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the displacements of the set of unique items in the internal list.", "response": "def displacements(self):\n        \"\"\"Return displacements\n\n        Returns\n        -------\n        There are two types of displacement dataset. See the docstring\n        of set_displacement_dataset about types 1 and 2 for displacement\n        dataset format.\n\n        Type-1, List of list\n            The internal list has 4 elements such as [32, 0.01, 0.0, 0.0]].\n            The first element is the supercell atom index starting with 0.\n            The remaining three elements give the displacement in Cartesian\n            coordinates.\n        Type-2, array_like\n            Displacements of all atoms of all supercells in Cartesian\n            coordinates.\n            shape=(supercells, natom, 3)\n            dtype='double'\n\n        \"\"\"\n\n        disps = []\n        if 'first_atoms' in self._displacement_dataset:\n            for disp in self._displacement_dataset['first_atoms']:\n                x = disp['displacement']\n                disps.append([disp['number'], x[0], x[1], x[2]])\n        elif 'displacements' in self._displacement_dataset:\n            disps = self._displacement_dataset['displacements']\n\n        return disps"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the displacements of all atoms in all supercells.", "response": "def displacements(self, displacements):\n        \"\"\"Set displacemens\n\n        Parameters\n        ----------\n        displacemens : array_like\n            Atomic displacements of all atoms of all supercells.\n            Only all displacements in each supercell case is supported.\n            shape=(supercells, natom, 3), dtype='double', order='C'\n\n        \"\"\"\n\n        disp = np.array(displacements, dtype='double', order='C')\n        if (disp.ndim != 3 or\n            disp.shape[1:] != (self._supercell.get_number_of_atoms(), 3)):\n            raise RuntimeError(\"Array shape of displacements is incorrect.\")\n\n        if 'first_atoms' in self._displacement_dataset:\n            raise RuntimeError(\"This displacement format is not supported.\")\n\n        self._displacement_dataset['displacements'] = disp"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dataset(self, dataset):\n\n        if 'displacements' in dataset:\n            natom = self._supercell.get_number_of_atoms()\n            if type(dataset['displacements']) is np.ndarray:\n                if dataset['displacements'].ndim in (1, 2):\n                    d = dataset['displacements'].reshape((-1, natom, 3))\n                    dataset['displacements'] = d\n            if type(dataset['forces']) is np.ndarray:\n                if dataset['forces'].ndim in (1, 2):\n                    f = dataset['forces'].reshape((-1, natom, 3))\n                    dataset['forces'] = f\n        self._displacement_dataset = dataset\n        self._supercells_with_displacements = None", "response": "Set the displacement and forces of the specified set of items in the given dataset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the atomic forces in the displacement dataset.", "response": "def forces(self, sets_of_forces):\n        \"\"\"Set forces in displacement dataset.\n\n        Parameters\n        ----------\n        sets_of_forces : array_like\n            A set of atomic forces in displaced supercells. The order of\n            displaced supercells has to match with that in displacement\n            dataset.\n            shape=(displaced supercells, atoms in supercell, 3), dtype='double'\n\n            [[[f_1x, f_1y, f_1z], [f_2x, f_2y, f_2z], ...], # first supercell\n             [[f_1x, f_1y, f_1z], [f_2x, f_2y, f_2z], ...], # second supercell\n             ...\n            ]\n\n        \"\"\"\n\n        if 'first_atoms' in self._displacement_dataset:\n            for disp, forces in zip(self._displacement_dataset['first_atoms'],\n                                    sets_of_forces):\n                disp['forces'] = forces\n        elif 'forces' in self._displacement_dataset:\n            forces = np.array(sets_of_forces, dtype='double', order='C')\n            self._displacement_dataset['forces'] = forces"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef force_constants(self, force_constants):\n\n        if type(force_constants) is np.ndarray:\n            fc_shape = force_constants.shape\n            if fc_shape[0] != fc_shape[1]:\n                if self._primitive.get_number_of_atoms() != fc_shape[0]:\n                    msg = (\"Force constants shape disagrees with crystal \"\n                           \"structure setting. This may be due to \"\n                           \"PRIMITIVE_AXIS.\")\n                    raise RuntimeError(msg)\n\n        self._force_constants = force_constants\n        if self._primitive.get_masses() is not None:\n            self._set_dynamical_matrix()", "response": "Set force constants for a specific object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_displacements(self,\n                               distance=0.01,\n                               is_plusminus='auto',\n                               is_diagonal=True,\n                               is_trigonal=False):\n        \"\"\"Generate displacement dataset\"\"\"\n        displacement_directions = get_least_displacements(\n            self._symmetry,\n            is_plusminus=is_plusminus,\n            is_diagonal=is_diagonal,\n            is_trigonal=is_trigonal,\n            log_level=self._log_level)\n        displacement_dataset = directions_to_displacement_dataset(\n            displacement_directions,\n            distance,\n            self._supercell)\n        self.set_displacement_dataset(displacement_dataset)", "response": "Generate displacement dataset from the least displacements of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dynamical_matrix_at_q(self, q):\n\n        self._set_dynamical_matrix()\n        if self._dynamical_matrix is None:\n            msg = (\"Dynamical matrix has not yet built.\")\n            raise RuntimeError(msg)\n\n        self._dynamical_matrix.set_dynamical_matrix(q)\n        return self._dynamical_matrix.get_dynamical_matrix()", "response": "Calculates the dynamical matrix at a given q - point."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates phonon frequencies at a given q - point.", "response": "def get_frequencies(self, q):\n        \"\"\"Calculate phonon frequencies at a given q-point\n\n        Parameters\n        ----------\n        q: array_like\n            A q-vector.\n            shape=(3,), dtype='double'\n\n        Returns\n        -------\n        frequencies: ndarray\n            Phonon frequencies.\n            shape=(bands, ), dtype='double'\n\n        \"\"\"\n        self._set_dynamical_matrix()\n        if self._dynamical_matrix is None:\n            msg = (\"Dynamical matrix has not yet built.\")\n            raise RuntimeError(msg)\n\n        self._dynamical_matrix.set_dynamical_matrix(q)\n        dm = self._dynamical_matrix.get_dynamical_matrix()\n        frequencies = []\n        for eig in np.linalg.eigvalsh(dm).real:\n            if eig < 0:\n                frequencies.append(-np.sqrt(-eig))\n            else:\n                frequencies.append(np.sqrt(eig))\n\n        return np.array(frequencies) * self._factor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_frequencies_with_eigenvectors(self, q):\n        self._set_dynamical_matrix()\n        if self._dynamical_matrix is None:\n            msg = (\"Dynamical matrix has not yet built.\")\n            raise RuntimeError(msg)\n\n        self._dynamical_matrix.set_dynamical_matrix(q)\n        dm = self._dynamical_matrix.get_dynamical_matrix()\n        frequencies = []\n        eigvals, eigenvectors = np.linalg.eigh(dm)\n        frequencies = []\n        for eig in eigvals:\n            if eig < 0:\n                frequencies.append(-np.sqrt(-eig))\n            else:\n                frequencies.append(np.sqrt(eig))\n\n        return np.array(frequencies) * self._factor, eigenvectors", "response": "Calculate phonon frequencies and eigenvectors at a given q - point."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_band_structure(self,\n                           paths,\n                           with_eigenvectors=False,\n                           with_group_velocities=False,\n                           is_band_connection=False,\n                           path_connections=None,\n                           labels=None,\n                           is_legacy_plot=False):\n        \"\"\"Run phonon band structure calculation.\n\n        Parameters\n        ----------\n        paths : List of array_like\n            Sets of qpoints that can be passed to phonopy.set_band_structure().\n            Numbers of qpoints can be different.\n            shape of each array_like : (qpoints, 3)\n        with_eigenvectors : bool, optional\n            Flag whether eigenvectors are calculated or not. Default is False.\n        with_group_velocities : bool, optional\n            Flag whether group velocities are calculated or not. Default is\n            False.\n        is_band_connection : bool, optional\n            Flag whether each band is connected or not. This is achieved by\n            comparing similarity of eigenvectors of neghboring poins. Sometimes\n            this fails. Default is False.\n        path_connections : List of bool, optional\n            This is only used in graphical plot of band structure and gives\n            whether each path is connected to the next path or not,\n            i.e., if False, there is a jump of q-points. Number of elements is\n            the same at that of paths. Default is None.\n        labels : List of str, optional\n            This is only used in graphical plot of band structure and gives\n            labels of end points of each path. The number of labels is equal\n            to (2 - np.array(path_connections)).sum().\n        is_legacy_plot: bool, optional\n            This makes the old style band structure plot. Default is False.\n\n        \"\"\"\n\n        if self._dynamical_matrix is None:\n            msg = (\"Dynamical matrix has not yet built.\")\n            raise RuntimeError(msg)\n\n        if with_group_velocities:\n            if self._group_velocity is None:\n                self._set_group_velocity()\n            group_velocity = self._group_velocity\n        else:\n            group_velocity = None\n\n        self._band_structure = BandStructure(\n            paths,\n            self._dynamical_matrix,\n            with_eigenvectors=with_eigenvectors,\n            is_band_connection=is_band_connection,\n            group_velocity=group_velocity,\n            path_connections=path_connections,\n            labels=labels,\n            is_legacy_plot=is_legacy_plot,\n            factor=self._factor)", "response": "This function runs phonon band structure calculation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict containing the current band structure.", "response": "def get_band_structure_dict(self):\n        \"\"\"Returns calclated band structures\n\n        Returns\n        -------\n        dict\n            keys: qpoints, distances, frequencies, eigenvectors, and\n                  group_velocities\n            Each dict value is a list containing properties on number of paths.\n            The number of q-points on one path can be different from that on\n            the other path. Each set of properties on a path is ndarray and is\n            explained as below:\n\n            qpoints[i]: ndarray\n                q-points in reduced coordinates of reciprocal space without\n                2pi.\n                shape=(q-points, 3), dtype='double'\n            distances[i]: ndarray\n                Distances in reciprocal space along paths.\n                shape=(q-points,), dtype='double'\n            frequencies[i]: ndarray\n                Phonon frequencies\n                shape=(q-points, bands), dtype='double'\n            eigenvectors[i]: ndarray\n                Phonon eigenvectors. None if eigenvectors are not stored.\n                shape=(q-points, bands, bands), dtype='complex'\n            group_velocities[i]: ndarray\n                Phonon group velocities. None if group velocities are not\n                calculated.\n                shape=(q-points, bands, 3), dtype='double'\n\n        \"\"\"\n\n        if self._band_structure is None:\n            msg = (\"run_band_structure has to be done.\")\n            raise RuntimeError(msg)\n\n        retdict = {'qpoints': self._band_structure.qpoints,\n                   'distances': self._band_structure.distances,\n                   'frequencies': self._band_structure.frequencies,\n                   'eigenvectors': self._band_structure.eigenvectors,\n                   'group_velocities': self._band_structure.group_velocities}\n\n        return retdict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_band_structure(self):\n\n        warnings.warn(\"Phonopy.get_band_structure is deprecated. \"\n                      \"Use Phonopy.get_band_structure_dict.\",\n                      DeprecationWarning)\n\n        if self._band_structure is None:\n            msg = (\"run_band_structure has to be done.\")\n            raise RuntimeError(msg)\n\n        retvals = (self._band_structure.qpoints,\n                   self._band_structure.distances,\n                   self._band_structure.frequencies,\n                   self._band_structure.eigenvectors)\n        return retvals", "response": "Returns a list of numpy arrays containing the values of the _band_structure attribute."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_mesh(self,\n                  mesh=100.0,\n                  shift=None,\n                  is_time_reversal=True,\n                  is_mesh_symmetry=True,\n                  with_eigenvectors=False,\n                  with_group_velocities=False,\n                  is_gamma_center=False,\n                  use_iter_mesh=False):\n        \"\"\"Initialize mesh sampling phonon calculation without starting to run.\n\n        Phonon calculation starts explicitly with calling Mesh.run() or\n        implicitly with accessing getters of Mesh instance, e.g.,\n        Mesh.frequencies.\n\n        Parameters\n        ----------\n        mesh: array_like or float, optional\n            Mesh numbers along a, b, c axes when array_like object is given.\n            dtype='intc', shape=(3,)\n            When float value is given, uniform mesh is generated following\n            VASP convention by\n                N = max(1, nint(l * |a|^*))\n            where 'nint' is the function to return the nearest integer. In this\n            case, it is forced to set is_gamma_center=True.\n            Default value is 100.0.\n        shift: array_like, optional\n            Mesh shifts along a*, b*, c* axes with respect to neighboring grid\n            points from the original mesh (Monkhorst-Pack or Gamma center).\n            0.5 gives half grid shift. Normally 0 or 0.5 is given.\n            Otherwise q-points symmetry search is not performed.\n            Default is None (no additional shift).\n            dtype='double', shape=(3, )\n        is_time_reversal: bool, optional\n            Time reversal symmetry is considered in symmetry search. By this,\n            inversion symmetry is always included. Default is True.\n        is_mesh_symmetry: bool, optional\n            Wheather symmetry search is done or not. Default is True\n        with_eigenvectors: bool, optional\n            Eigenvectors are stored by setting True. Default False.\n        with_group_velocities : bool, optional\n            Group velocities are calculated by setting True. Default is\n            False.\n        is_gamma_center: bool, default False\n            Uniform mesh grids are generated centring at Gamma point but not\n            the Monkhorst-Pack scheme. When type(mesh) is float, this parameter\n            setting is ignored and it is forced to set is_gamma_center=True.\n        use_iter_mesh: bool\n            Use IterMesh instead of Mesh class not to store phonon properties\n            in its instance to save memory consumption. This is used with\n            ThermalDisplacements and ThermalDisplacementMatrices.\n            Default is False.\n\n        \"\"\"\n\n        if self._dynamical_matrix is None:\n            msg = \"Dynamical matrix has not yet built.\"\n            raise RuntimeError(msg)\n\n        _mesh = np.array(mesh)\n        mesh_nums = None\n        if _mesh.shape:\n            if _mesh.shape == (3,):\n                mesh_nums = mesh\n                _is_gamma_center = is_gamma_center\n        else:\n            if self._primitive_symmetry is not None:\n                rots = self._primitive_symmetry.get_pointgroup_operations()\n                mesh_nums = length2mesh(mesh,\n                                        self._primitive.get_cell(),\n                                        rotations=rots)\n            else:\n                mesh_nums = length2mesh(mesh, self._primitive.get_cell())\n            _is_gamma_center = True\n        if mesh_nums is None:\n            msg = \"mesh has inappropriate type.\"\n            raise TypeError(msg)\n\n        if with_group_velocities:\n            if self._group_velocity is None:\n                self._set_group_velocity()\n            group_velocity = self._group_velocity\n        else:\n            group_velocity = None\n\n        if use_iter_mesh:\n            self._mesh = IterMesh(\n                self._dynamical_matrix,\n                mesh,\n                shift=shift,\n                is_time_reversal=is_time_reversal,\n                is_mesh_symmetry=is_mesh_symmetry,\n                with_eigenvectors=with_eigenvectors,\n                is_gamma_center=is_gamma_center,\n                rotations=self._primitive_symmetry.get_pointgroup_operations(),\n                factor=self._factor)\n        else:\n            self._mesh = Mesh(\n                self._dynamical_matrix,\n                mesh_nums,\n                shift=shift,\n                is_time_reversal=is_time_reversal,\n                is_mesh_symmetry=is_mesh_symmetry,\n                with_eigenvectors=with_eigenvectors,\n                is_gamma_center=_is_gamma_center,\n                group_velocity=group_velocity,\n                rotations=self._primitive_symmetry.get_pointgroup_operations(),\n                factor=self._factor,\n                use_lapack_solver=self._use_lapack_solver)", "response": "Initialize the phonon mesh for the current entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_mesh(self,\n                 mesh=100.0,\n                 shift=None,\n                 is_time_reversal=True,\n                 is_mesh_symmetry=True,\n                 with_eigenvectors=False,\n                 with_group_velocities=False,\n                 is_gamma_center=False):\n        \"\"\"Run mesh sampling phonon calculation.\n\n        See the parameter details in Phonopy.init_mesh().\n\n        \"\"\"\n\n        self.init_mesh(mesh=mesh,\n                       shift=shift,\n                       is_time_reversal=is_time_reversal,\n                       is_mesh_symmetry=is_mesh_symmetry,\n                       with_eigenvectors=with_eigenvectors,\n                       with_group_velocities=with_group_velocities,\n                       is_gamma_center=is_gamma_center)\n        self._mesh.run()", "response": "Run the mesh sampling phonon calculation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the mesh of the current object.", "response": "def set_mesh(self,\n                 mesh,\n                 shift=None,\n                 is_time_reversal=True,\n                 is_mesh_symmetry=True,\n                 is_eigenvectors=False,\n                 is_gamma_center=False,\n                 run_immediately=True):\n        \"\"\"Phonon calculations on sampling mesh grids\n\n        Parameters\n        ----------\n        mesh: array_like\n            Mesh numbers along a, b, c axes.\n            dtype='intc'\n            shape=(3,)\n        shift: array_like, optional, default None (no shift)\n            Mesh shifts along a*, b*, c* axes with respect to neighboring grid\n            points from the original mesh (Monkhorst-Pack or Gamma center).\n            0.5 gives half grid shift. Normally 0 or 0.5 is given.\n            Otherwise q-points symmetry search is not performed.\n            dtype='double'\n            shape=(3, )\n        is_time_reversal: bool, optional, default True\n            Time reversal symmetry is considered in symmetry search. By this,\n            inversion symmetry is always included.\n        is_mesh_symmetry: bool, optional, default True\n            Wheather symmetry search is done or not.\n        is_eigenvectors: bool, optional, default False\n            Eigenvectors are stored by setting True.\n        is_gamma_center: bool, default False\n            Uniform mesh grids are generated centring at Gamma point but not\n            the Monkhorst-Pack scheme.\n        run_immediately: bool, default True\n            With True, phonon calculations are performed immediately, which is\n            usual usage.\n\n        \"\"\"\n\n        warnings.warn(\"Phonopy.set_mesh is deprecated. \"\n                      \"Use Phonopy.run_mesh.\", DeprecationWarning)\n\n        if self._group_velocity is None:\n            with_group_velocities = False\n        else:\n            with_group_velocities = True\n        if run_immediately:\n            self.run_mesh(mesh,\n                          shift=shift,\n                          is_time_reversal=is_time_reversal,\n                          is_mesh_symmetry=is_mesh_symmetry,\n                          with_eigenvectors=is_eigenvectors,\n                          with_group_velocities=with_group_velocities,\n                          is_gamma_center=is_gamma_center)\n        else:\n            self.init_mesh(mesh,\n                           shift=shift,\n                           is_time_reversal=is_time_reversal,\n                           is_mesh_symmetry=is_mesh_symmetry,\n                           with_eigenvectors=is_eigenvectors,\n                           with_group_velocities=with_group_velocities,\n                           is_gamma_center=is_gamma_center)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_mesh_dict(self):\n        if self._mesh is None:\n            msg = (\"run_mesh has to be done.\")\n            raise RuntimeError(msg)\n\n        retdict = {'qpoints': self._mesh.qpoints,\n                   'weights': self._mesh.weights,\n                   'frequencies': self._mesh.frequencies,\n                   'eigenvectors': self._mesh.eigenvectors,\n                   'group_velocities': self._mesh.group_velocities}\n\n        return retdict", "response": "Returns a dictionary of the calculated mesh sampling phonons and phonon - specific keys."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_iter_mesh(self,\n                      mesh,\n                      shift=None,\n                      is_time_reversal=True,\n                      is_mesh_symmetry=True,\n                      is_eigenvectors=False,\n                      is_gamma_center=False):\n        \"\"\"Create an IterMesh instancer\n\n        Attributes\n        ----------\n        See set_mesh method.\n\n        \"\"\"\n\n        warnings.warn(\"Phonopy.set_iter_mesh is deprecated. \"\n                      \"Use Phonopy.run_mesh with use_iter_mesh=True.\",\n                      DeprecationWarning)\n\n        self.run_mesh(mesh=mesh,\n                      shift=shift,\n                      is_time_reversal=is_time_reversal,\n                      is_mesh_symmetry=is_mesh_symmetry,\n                      with_eigenvectors=is_eigenvectors,\n                      is_gamma_center=is_gamma_center,\n                      use_iter_mesh=True)", "response": "Create an IterMesh instancer\n        instance and run it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun Phonon calculations on the specified q - points.", "response": "def run_qpoints(self,\n                    q_points,\n                    with_eigenvectors=False,\n                    with_group_velocities=False,\n                    with_dynamical_matrices=False,\n                    nac_q_direction=None):\n        \"\"\"Phonon calculations on q-points.\n\n        Parameters\n        ----------\n        q_points: array_like or float, optional\n            q-points in reduced coordinates.\n            dtype='double', shape=(q-points, 3)\n        with_eigenvectors: bool, optional\n            Eigenvectors are stored by setting True. Default False.\n        with_group_velocities : bool, optional\n            Group velocities are calculated by setting True. Default is False.\n        with_dynamical_matrices : bool, optional\n            Calculated dynamical matrices are stored by setting True.\n            Default is False.\n        nac_q_direction : array_like\n            q=(0,0,0) is replaced by q=epsilon * nac_q_direction where epsilon\n            is infinitsimal for non-analytical term correction. This is used,\n            e.g., to observe LO-TO splitting,\n\n        \"\"\"\n\n        if self._dynamical_matrix is None:\n            msg = (\"Dynamical matrix has not yet built.\")\n            raise RuntimeError(msg)\n\n        if with_group_velocities:\n            if self._group_velocity is None:\n                self._set_group_velocity()\n            group_velocity = self._group_velocity\n        else:\n            group_velocity = None\n\n        self._qpoints = QpointsPhonon(\n            np.reshape(q_points, (-1, 3)),\n            self._dynamical_matrix,\n            nac_q_direction=nac_q_direction,\n            with_eigenvectors=with_eigenvectors,\n            group_velocity=group_velocity,\n            with_dynamical_matrices=with_dynamical_matrices,\n            factor=self._factor)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating total DOS from phonons on sampling mesh. Parameters ---------- sigma : float, optional Smearing width for smearing method. Default is None freq_min, freq_max, freq_pitch : float, optional Minimum and maximum frequencies in which range DOS is computed with the specified interval (freq_pitch). Defaults are None and they are automatically determined. use_tetrahedron_method : float, optional Use tetrahedron method when this is True. When sigma is set, smearing method is used.", "response": "def run_total_dos(self,\n                      sigma=None,\n                      freq_min=None,\n                      freq_max=None,\n                      freq_pitch=None,\n                      use_tetrahedron_method=True):\n        \"\"\"Calculate total DOS from phonons on sampling mesh.\n\n        Parameters\n        ----------\n        sigma : float, optional\n            Smearing width for smearing method. Default is None\n        freq_min, freq_max, freq_pitch : float, optional\n            Minimum and maximum frequencies in which range DOS is computed\n            with the specified interval (freq_pitch).\n            Defaults are None and they are automatically determined.\n        use_tetrahedron_method : float, optional\n            Use tetrahedron method when this is True. When sigma is set,\n            smearing method is used.\n\n        \"\"\"\n        if self._mesh is None:\n            msg = \"run_mesh has to be done before DOS calculation.\"\n            raise RuntimeError(msg)\n\n        total_dos = TotalDos(self._mesh,\n                             sigma=sigma,\n                             use_tetrahedron_method=use_tetrahedron_method)\n        total_dos.set_draw_area(freq_min, freq_max, freq_pitch)\n        total_dos.run()\n        self._total_dos = total_dos"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn frequency points and total DOS as a tuple.", "response": "def get_total_DOS(self):\n        \"\"\"Return frequency points and total DOS as a tuple.\n\n        Returns\n        -------\n        A tuple with (frequency_points, total_dos).\n\n        frequency_points: ndarray\n            shape=(frequency_sampling_points, ), dtype='double'\n        total_dos:\n            shape=(frequency_sampling_points, ), dtype='double'\n\n        \"\"\"\n\n        warnings.warn(\"Phonopy.get_total_DOS is deprecated. \"\n                      \"Use Phonopy.get_total_dos_dict.\", DeprecationWarning)\n\n        dos = self.get_total_dos_dict()\n\n        return dos['frequency_points'], dos['total_dos']"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_projected_dos(self,\n                          sigma=None,\n                          freq_min=None,\n                          freq_max=None,\n                          freq_pitch=None,\n                          use_tetrahedron_method=True,\n                          direction=None,\n                          xyz_projection=False):\n        \"\"\"Calculate projected DOS from phonons on sampling mesh.\n\n        Parameters\n        ----------\n        sigma : float, optional\n            Smearing width for smearing method. Default is None\n        freq_min, freq_max, freq_pitch : float, optional\n            Minimum and maximum frequencies in which range DOS is computed\n            with the specified interval (freq_pitch).\n            Defaults are None and they are automatically determined.\n        use_tetrahedron_method : float, optional\n            Use tetrahedron method when this is True. When sigma is set,\n            smearing method is used.\n        direction : array_like, optional\n            Specific projection direction. This is specified three values\n            along basis vectors or the primitive cell. Default is None,\n            i.e., no projection.\n        xyz_projection : bool, optional\n            This determines whether projected along Cartesian directions or\n            not. Default is False, i.e., no projection.\n\n        \"\"\"\n\n        self._pdos = None\n\n        if self._mesh is None:\n            msg = \"run_mesh has to be done before PDOS calculation.\"\n            raise RuntimeError(msg)\n\n        if not self._mesh.with_eigenvectors:\n            msg = \"run_mesh has to be called with with_eigenvectors=True.\"\n            raise RuntimeError(msg)\n\n        if np.prod(self._mesh.mesh_numbers) != len(self._mesh.ir_grid_points):\n            msg = \"run_mesh has to be done with is_mesh_symmetry=False.\"\n            raise RuntimeError(msg)\n\n        if direction is not None:\n            direction_cart = np.dot(direction, self._primitive.get_cell())\n        else:\n            direction_cart = None\n        self._pdos = PartialDos(self._mesh,\n                                sigma=sigma,\n                                use_tetrahedron_method=use_tetrahedron_method,\n                                direction=direction_cart,\n                                xyz_projection=xyz_projection)\n        self._pdos.set_draw_area(freq_min, freq_max, freq_pitch)\n        self._pdos.run()", "response": "Calculates projected DOS from phonons on the mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_partial_DOS(self):\n        warnings.warn(\"Phonopy.get_partial_DOS is deprecated. \"\n                      \"Use Phonopy.get_projected_dos_dict.\",\n                      DeprecationWarning)\n\n        pdos = self.get_projected_dos_dict()\n\n        return pdos['frequency_points'], pdos['projected_dos']", "response": "Return the frequency points and partial DOS as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_projected_dos(self, pdos_indices=None, legend=None):\n\n        import matplotlib.pyplot as plt\n\n        fig, ax = plt.subplots()\n        ax.xaxis.set_ticks_position('both')\n        ax.yaxis.set_ticks_position('both')\n        ax.xaxis.set_tick_params(which='both', direction='in')\n        ax.yaxis.set_tick_params(which='both', direction='in')\n\n        self._pdos.plot(ax,\n                        indices=pdos_indices,\n                        legend=legend,\n                        draw_grid=False)\n\n        ax.set_ylim((0, None))\n\n        return plt", "response": "Plot projected DOS\n\n        Parameters\n        ----------\n        pdos_indices : list of list, optional\n            Sets of indices of atoms whose projected DOS are summed over.\n            The indices start with 0. An example is as follwos:\n                pdos_indices=[[0, 1], [2, 3, 4, 5]]\n             Default is None, which means\n                pdos_indices=[[i] for i in range(natom)]\n        legend : list of instances such as str or int, optional\n             The str(instance) are shown in legend.\n             It has to be len(pdos_indices)==len(legend). Default is None.\n             When None, legend is not shown."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates thermal properties at constant volume Parameters ---------- t_min, t_max, t_step : float, optional Minimum and maximum temperatures and the interval in this temperature range. Default values are 0, 1000, and 10. temperatures : array_like, optional Temperature points where thermal properties are calculated. When this is set, t_min, t_max, and t_step are ignored.", "response": "def run_thermal_properties(self,\n                               t_min=0,\n                               t_max=1000,\n                               t_step=10,\n                               temperatures=None,\n                               is_projection=False,\n                               band_indices=None,\n                               cutoff_frequency=None,\n                               pretend_real=False):\n        \"\"\"Calculate thermal properties at constant volume\n\n        Parameters\n        ----------\n        t_min, t_max, t_step : float, optional\n            Minimum and maximum temperatures and the interval in this\n            temperature range. Default values are 0, 1000, and 10.\n        temperatures : array_like, optional\n            Temperature points where thermal properties are calculated.\n            When this is set, t_min, t_max, and t_step are ignored.\n\n        \"\"\"\n        if self._mesh is None:\n            msg = (\"run_mesh has to be done before\"\n                   \"run_thermal_properties.\")\n            raise RuntimeError(msg)\n\n        tp = ThermalProperties(self._mesh,\n                               is_projection=is_projection,\n                               band_indices=band_indices,\n                               cutoff_frequency=cutoff_frequency,\n                               pretend_real=pretend_real)\n        if temperatures is None:\n            tp.set_temperature_range(t_step=t_step,\n                                     t_max=t_max,\n                                     t_min=t_min)\n        else:\n            tp.set_temperatures(temperatures)\n        tp.run()\n        self._thermal_properties = tp"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns thermal properties of the current object.", "response": "def get_thermal_properties(self):\n        \"\"\"Return thermal properties\n\n        Returns\n        -------\n        (temperatures, free energy, entropy, heat capacity)\n\n        \"\"\"\n        warnings.warn(\"Phonopy.get_thermal_properties is deprecated. \"\n                      \"Use Phonopy.get_thermal_properties_dict.\",\n                      DeprecationWarning)\n\n        tp = self.get_thermal_properties_dict()\n        return (tp['temperatures'],\n                tp['free_energy'],\n                tp['entropy'],\n                tp['heat_capacity'])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_thermal_displacements(self,\n                                  t_min=0,\n                                  t_max=1000,\n                                  t_step=10,\n                                  temperatures=None,\n                                  direction=None,\n                                  freq_min=None,\n                                  freq_max=None):\n        \"\"\"Prepare thermal displacements calculation\n\n        Parameters\n        ----------\n        t_min, t_max, t_step : float, optional\n            Minimum and maximum temperatures and the interval in this\n            temperature range. Default valuues are 0, 1000, and 10.\n        temperatures : array_like, optional\n            Temperature points where thermal properties are calculated.\n            When this is set, t_min, t_max, and t_step are ignored.\n        direction : array_like, optional\n            Projection direction in reduced coordinates. Default is None,\n            i.e., no projection.\n            dtype=float, shape=(3,)\n        freq_min, freq_max : float, optional\n            Phonon frequencies larger than freq_min and smaller than\n            freq_max are included. Default is None, i.e., all phonons.\n\n        \"\"\"\n\n        if self._dynamical_matrix is None:\n            msg = (\"Dynamical matrix has not yet built.\")\n            raise RuntimeError(msg)\n        if self._mesh is None:\n            msg = (\"run_mesh has to be done.\")\n            raise RuntimeError(msg)\n        mesh_nums = self._mesh.mesh_numbers\n        ir_grid_points = self._mesh.ir_grid_points\n        if not self._mesh.with_eigenvectors:\n            msg = (\"run_mesh has to be done with with_eigenvectors=True.\")\n            raise RuntimeError(msg)\n        if np.prod(mesh_nums) != len(ir_grid_points):\n            msg = (\"run_mesh has to be done with is_mesh_symmetry=False.\")\n            raise RuntimeError(msg)\n\n        if direction is not None:\n            projection_direction = np.dot(direction,\n                                          self._primitive.get_cell())\n            td = ThermalDisplacements(\n                self._mesh,\n                projection_direction=projection_direction,\n                freq_min=freq_min,\n                freq_max=freq_max)\n        else:\n            td = ThermalDisplacements(self._mesh,\n                                      freq_min=freq_min,\n                                      freq_max=freq_max)\n\n        if temperatures is None:\n            td.set_temperature_range(t_min, t_max, t_step)\n        else:\n            td.set_temperatures(temperatures)\n        td.run()\n\n        self._thermal_displacements = td", "response": "This method runs the thermal displacements calculation for the specified temperature range."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_thermal_displacement_matrices(self,\n                                          t_min=0,\n                                          t_max=1000,\n                                          t_step=10,\n                                          temperatures=None,\n                                          freq_min=None,\n                                          freq_max=None):\n        \"\"\"Prepare thermal displacement matrices\n\n        Parameters\n        ----------\n        t_min, t_max, t_step : float, optional\n            Minimum and maximum temperatures and the interval in this\n            temperature range. Default valuues are 0, 1000, and 10.\n        freq_min, freq_max : float, optional\n            Phonon frequencies larger than freq_min and smaller than\n            freq_max are included. Default is None, i.e., all phonons.\n        temperatures : array_like, optional\n            Temperature points where thermal properties are calculated.\n            When this is set, t_min, t_max, and t_step are ignored.\n            Default is None.\n\n        \"\"\"\n        if self._dynamical_matrix is None:\n            msg = (\"Dynamical matrix has not yet built.\")\n            raise RuntimeError(msg)\n        if self._mesh is None:\n            msg = (\"run_mesh has to be done.\")\n            raise RuntimeError(msg)\n        mesh_nums = self._mesh.mesh_numbers\n        ir_grid_points = self._mesh.ir_grid_points\n        if not self._mesh.with_eigenvectors:\n            msg = (\"run_mesh has to be done with with_eigenvectors=True.\")\n            raise RuntimeError(msg)\n        if np.prod(mesh_nums) != len(ir_grid_points):\n            msg = (\"run_mesh has to be done with is_mesh_symmetry=False.\")\n            raise RuntimeError(msg)\n\n        tdm = ThermalDisplacementMatrices(\n            self._mesh,\n            freq_min=freq_min,\n            freq_max=freq_max,\n            lattice=self._primitive.get_cell().T)\n\n        if temperatures is None:\n            tdm.set_temperature_range(t_min, t_max, t_step)\n        else:\n            tdm.set_temperatures(temperatures)\n        tdm.run()\n\n        self._thermal_displacement_matrices = tdm", "response": "This method runs the thermal displacement matrices of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate atomic displacements of phonon modes.", "response": "def set_modulations(self,\n                        dimension,\n                        phonon_modes,\n                        delta_q=None,\n                        derivative_order=None,\n                        nac_q_direction=None):\n        \"\"\"Generate atomic displacements of phonon modes.\n\n        The design of this feature is not very satisfactory, and thus API.\n        Therefore it should be reconsidered someday in the fugure.\n\n        Parameters\n        ----------\n        dimension : array_like\n           Supercell dimension with respect to the primitive cell.\n           dtype='intc', shape=(3, ), (3, 3), (9, )\n        phonon_modes : list of phonon mode settings\n           Each element of the outer list gives one phonon mode information:\n\n               [q-point, band index (int), amplitude (float), phase (float)]\n\n           In each list of the phonon mode information, the first element is\n           a list that represents q-point in reduced coordinates. The second,\n           third, and fourth elements show the band index starting with 0,\n           amplitude, and phase factor, respectively.\n\n        \"\"\"\n        if self._dynamical_matrix is None:\n            msg = (\"Dynamical matrix has not yet built.\")\n            raise RuntimeError(msg)\n\n        self._modulation = Modulation(self._dynamical_matrix,\n                                      dimension,\n                                      phonon_modes,\n                                      delta_q=delta_q,\n                                      derivative_order=derivative_order,\n                                      nac_q_direction=nac_q_direction,\n                                      factor=self._factor)\n        self._modulation.run()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets ir - reps of phonon modes.", "response": "def set_irreps(self,\n                   q,\n                   is_little_cogroup=False,\n                   nac_q_direction=None,\n                   degeneracy_tolerance=1e-4):\n        \"\"\"Identify ir-reps of phonon modes.\n\n        The design of this API is not very satisfactory and is expceted\n        to be redesined in the next major versions once the use case\n        of the API for ir-reps feature becomes clearer.\n\n        \"\"\"\n\n        if self._dynamical_matrix is None:\n            msg = (\"Dynamical matrix has not yet built.\")\n            raise RuntimeError(msg)\n\n        self._irreps = IrReps(\n            self._dynamical_matrix,\n            q,\n            is_little_cogroup=is_little_cogroup,\n            nac_q_direction=nac_q_direction,\n            factor=self._factor,\n            symprec=self._symprec,\n            degeneracy_tolerance=degeneracy_tolerance,\n            log_level=self._log_level)\n\n        return self._irreps.run()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the dynamic structure factor calculation.", "response": "def init_dynamic_structure_factor(self,\n                                      Qpoints,\n                                      T,\n                                      atomic_form_factor_func=None,\n                                      scattering_lengths=None,\n                                      freq_min=None,\n                                      freq_max=None):\n        \"\"\"Initialize dynamic structure factor calculation.\n\n        *******************************************************************\n         This is still an experimental feature. API can be changed without\n         notification.\n        *******************************************************************\n\n        Need to call DynamicStructureFactor.run() to start calculation.\n\n        Parameters\n        ----------\n        Qpoints: array_like\n            Q-points in any Brillouin zone.\n            dtype='double'\n            shape=(qpoints, 3)\n        T: float\n            Temperature in K.\n        atomic_form_factor_func: Function object\n            Function that returns atomic form factor (``func`` below):\n\n                f_params = {'Na': [3.148690, 2.594987, 4.073989, 6.046925,\n                                   0.767888, 0.070139, 0.995612, 14.1226457,\n                                   0.968249, 0.217037, 0.045300],\n                            'Cl': [1.061802, 0.144727, 7.139886, 1.171795,\n                                   6.524271, 19.467656, 2.355626, 60.320301,\n                                   35.829404, 0.000436, -34.916604],b|\n\n                def get_func_AFF(f_params):\n                    def func(symbol, Q):\n                        return atomic_form_factor_WK1995(Q, f_params[symbol])\n                    return func\n\n        scattering_lengths: dictionary\n            Coherent scattering lengths averaged over isotopes and spins.\n            Supposed for INS. For example, {'Na': 3.63, 'Cl': 9.5770}.\n        freq_min, freq_min: float\n            Minimum and maximum phonon frequencies to determine whether\n            phonons are included in the calculation.\n\n        \"\"\"\n        if self._mesh is None:\n            msg = (\"run_mesh has to be done before initializing dynamic\"\n                   \"structure factor.\")\n            raise RuntimeError(msg)\n\n        if not self._mesh.with_eigenvectors:\n            msg = \"run_mesh has to be called with with_eigenvectors=True.\"\n            raise RuntimeError(msg)\n\n        if np.prod(self._mesh.mesh_numbers) != len(self._mesh.ir_grid_points):\n            msg = \"run_mesh has to be done with is_mesh_symmetry=False.\"\n            raise RuntimeError(msg)\n\n        self._dynamic_structure_factor = DynamicStructureFactor(\n            self._mesh,\n            Qpoints,\n            T,\n            atomic_form_factor_func=atomic_form_factor_func,\n            scattering_lengths=scattering_lengths,\n            freq_min=freq_min,\n            freq_max=freq_max)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_dynamic_structure_factor(self,\n                                     Qpoints,\n                                     T,\n                                     atomic_form_factor_func=None,\n                                     scattering_lengths=None,\n                                     freq_min=None,\n                                     freq_max=None):\n        \"\"\"Run dynamic structure factor calculation\n\n        See the detail of parameters at\n        Phonopy.init_dynamic_structure_factor().\n\n        \"\"\"\n\n        self.init_dynamic_structure_factor(\n            Qpoints,\n            T,\n            atomic_form_factor_func=atomic_form_factor_func,\n            scattering_lengths=scattering_lengths,\n            freq_min=freq_min,\n            freq_max=freq_max)\n        self._dynamic_structure_factor.run()", "response": "Run dynamic structure factor calculation at the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves parameters in Phonopy instants into file.", "response": "def save(self,\n             filename=\"phonopy_params.yaml\",\n             settings=None):\n        \"\"\"Save parameters in Phonopy instants into file.\n\n        Parameters\n        ----------\n        filename: str, optional\n            File name. Default is \"phonopy_params.yaml\"\n        settings: dict, optional\n            It is described which parameters are written out. Only\n            the settings expected to be updated from the following\n            default settings are needed to be set in the dictionary.\n            The possible parameters and their default settings are:\n                {'force_sets': True,\n                 'displacements': True,\n                 'force_constants': False,\n                 'born_effective_charge': True,\n                 'dielectric_constant': True}\n\n        \"\"\"\n        phpy_yaml = PhonopyYaml(calculator=self._calculator,\n                                settings=settings)\n        phpy_yaml.set_phonon_info(self)\n        with open(filename, 'w') as w:\n            w.write(str(phpy_yaml))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the primitive cell.", "response": "def _build_primitive_cell(self):\n        \"\"\"\n        primitive_matrix:\n          Relative axes of primitive cell to the input unit cell.\n          Relative axes to the supercell is calculated by:\n             supercell_matrix^-1 * primitive_matrix\n          Therefore primitive cell lattice is finally calculated by:\n             (supercell_lattice * (supercell_matrix)^-1 * primitive_matrix)^T\n        \"\"\"\n\n        inv_supercell_matrix = np.linalg.inv(self._supercell_matrix)\n        if self._primitive_matrix is None:\n            trans_mat = inv_supercell_matrix\n        else:\n            trans_mat = np.dot(inv_supercell_matrix, self._primitive_matrix)\n\n        try:\n            self._primitive = get_primitive(\n                self._supercell, trans_mat, self._symprec)\n        except ValueError:\n            msg = (\"Creating primitive cell is failed. \"\n                   \"PRIMITIVE_AXIS may be incorrectly specified.\")\n            raise RuntimeError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef length2mesh(length, lattice, rotations=None):\n\n    rec_lattice = np.linalg.inv(lattice)\n    rec_lat_lengths = np.sqrt(np.diagonal(np.dot(rec_lattice.T, rec_lattice)))\n    mesh_numbers = np.rint(rec_lat_lengths * length).astype(int)\n\n    if rotations is not None:\n        reclat_equiv = get_lattice_vector_equivalence(\n            [r.T for r in np.array(rotations)])\n        m = mesh_numbers\n        mesh_equiv = [m[1] == m[2], m[2] == m[0], m[0] == m[1]]\n        for i, pair in enumerate(([1, 2], [2, 0], [0, 1])):\n            if reclat_equiv[i] and not mesh_equiv:\n                m[pair] = max(m[pair])\n\n    return np.maximum(mesh_numbers, [1, 1, 1])", "response": "Convert length to mesh for q - point sampling"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_population(self, freq, t):  # freq in THz\n        condition = t > 1.0\n        if type(condition) == bool or type(condition) == np.bool_:\n            if condition:\n                return 1.0 / (np.exp(freq * THzToEv / (Kb * t)) - 1)\n            else:\n                return 0.0\n        else:\n            vals = np.zeros(len(t), dtype='double')\n            vals[condition] = 1.0 / (\n                np.exp(freq * THzToEv / (Kb * t[condition])) - 1)\n            return vals", "response": "Return phonon population number for a given frequency"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nproject the Eigenvectors of the resource to Cartesian direction", "response": "def _project_eigenvectors(self):\n        \"\"\"Eigenvectors are projected along Cartesian direction\"\"\"\n\n        self._p_eigenvectors = []\n        for vecs_q in self._eigenvectors:\n            p_vecs_q = []\n            for vecs in vecs_q.T:\n                p_vecs_q.append(np.dot(vecs.reshape(-1, 3),\n                                       self._projection_direction))\n            self._p_eigenvectors.append(np.transpose(p_vecs_q))\n        self._p_eigenvectors = np.array(self._p_eigenvectors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_symmetry_dataset(cell,\n                         symprec=1e-5,\n                         angle_tolerance=-1.0,\n                         hall_number=0):\n    \"\"\"Search symmetry dataset from an input cell.\n\n    Args:\n        cell, symprec, angle_tolerance:\n            See the docstring of get_symmetry.\n        hall_number: If a serial number of Hall symbol (>0) is given,\n                     the database corresponding to the Hall symbol is made.\n\n    Return:\n        A dictionary is returned. Dictionary keys:\n            number (int): International space group number\n            international (str): International symbol\n            hall (str): Hall symbol\n            choice (str): Centring, origin, basis vector setting\n            transformation_matrix (3x3 float):\n                Transformation matrix from input lattice to standardized\n                lattice:\n                    L^original = L^standardized * Tmat\n            origin shift (3 float):\n                Origin shift from standardized to input origin\n            rotations (3x3 int), translations (float vector):\n                Rotation matrices and translation vectors. Space group\n                operations are obtained by\n                [(r,t) for r, t in zip(rotations, translations)]\n            wyckoffs (n char): Wyckoff letters\n            equivalent_atoms (n int): Symmetrically equivalent atoms\n            mapping_to_primitive (n int):\n                Original cell atom index mapping to primivie cell atom index\n            Idealized standardized unit cell:\n                std_lattice (3x3 float, row vectors),\n                std_positions (Nx3 float), std_types (N int)\n            std_rotation_matrix:\n                Rigid rotation matrix to rotate from standardized basis\n                vectors to idealized standardized basis vectors\n                    L^idealized = R * L^standardized\n            std_mapping_to_primitive (m int):\n                std_positions index mapping to those of primivie cell atoms\n            pointgroup (str): Pointgroup symbol\n\n        If it fails, None is returned.\n\n    \"\"\"\n    _set_no_error()\n\n    lattice, positions, numbers, _ = _expand_cell(cell)\n    if lattice is None:\n        return None\n\n    spg_ds = spg.dataset(lattice, positions, numbers, hall_number,\n                         symprec, angle_tolerance)\n    if spg_ds is None:\n        _set_error_message()\n        return None\n\n    keys = ('number',\n            'hall_number',\n            'international',\n            'hall',\n            'choice',\n            'transformation_matrix',\n            'origin_shift',\n            'rotations',\n            'translations',\n            'wyckoffs',\n            'site_symmetry_symbols',\n            'equivalent_atoms',\n            'mapping_to_primitive',\n            'std_lattice',\n            'std_types',\n            'std_positions',\n            'std_rotation_matrix',\n            'std_mapping_to_primitive',\n            # 'pointgroup_number',\n            'pointgroup')\n    dataset = {}\n    for key, data in zip(keys, spg_ds):\n        dataset[key] = data\n\n    dataset['international'] = dataset['international'].strip()\n    dataset['hall'] = dataset['hall'].strip()\n    dataset['choice'] = dataset['choice'].strip()\n    dataset['transformation_matrix'] = np.array(\n        dataset['transformation_matrix'], dtype='double', order='C')\n    dataset['origin_shift'] = np.array(dataset['origin_shift'], dtype='double')\n    dataset['rotations'] = np.array(dataset['rotations'],\n                                    dtype='intc', order='C')\n    dataset['translations'] = np.array(dataset['translations'],\n                                       dtype='double', order='C')\n    letters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    dataset['wyckoffs'] = [letters[x] for x in dataset['wyckoffs']]\n    dataset['site_symmetry_symbols'] = [\n        s.strip() for s in dataset['site_symmetry_symbols']]\n    dataset['equivalent_atoms'] = np.array(dataset['equivalent_atoms'],\n                                           dtype='intc')\n    dataset['mapping_to_primitive'] = np.array(dataset['mapping_to_primitive'],\n                                               dtype='intc')\n    dataset['std_lattice'] = np.array(np.transpose(dataset['std_lattice']),\n                                      dtype='double', order='C')\n    dataset['std_types'] = np.array(dataset['std_types'], dtype='intc')\n    dataset['std_positions'] = np.array(dataset['std_positions'],\n                                        dtype='double', order='C')\n    dataset['std_rotation_matrix'] = np.array(dataset['std_rotation_matrix'],\n                                              dtype='double', order='C')\n    dataset['std_mapping_to_primitive'] = np.array(\n        dataset['std_mapping_to_primitive'], dtype='intc')\n    dataset['pointgroup'] = dataset['pointgroup'].strip()\n\n    _set_error_message()\n    return dataset", "response": "Search symmetry dataset from an input cell."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn space group in international table symbol and number as a string.", "response": "def get_spacegroup(cell, symprec=1e-5, angle_tolerance=-1.0, symbol_type=0):\n    \"\"\"Return space group in international table symbol and number as a string.\n\n    If it fails, None is returned.\n    \"\"\"\n    _set_no_error()\n\n    dataset = get_symmetry_dataset(cell,\n                                   symprec=symprec,\n                                   angle_tolerance=angle_tolerance)\n    if dataset is None:\n        return None\n\n    spg_type = get_spacegroup_type(dataset['hall_number'])\n    if symbol_type == 1:\n        return \"%s (%d)\" % (spg_type['schoenflies'], dataset['number'])\n    else:\n        return \"%s (%d)\" % (spg_type['international_short'], dataset['number'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntranslates Hall number to space group type information.", "response": "def get_spacegroup_type(hall_number):\n    \"\"\"Translate Hall number to space group type information.\n\n    If it fails, None is returned.\n    \"\"\"\n    _set_no_error()\n\n    keys = ('number',\n            'international_short',\n            'international_full',\n            'international',\n            'schoenflies',\n            'hall_symbol',\n            'choice',\n            'pointgroup_schoenflies',\n            'pointgroup_international',\n            'arithmetic_crystal_class_number',\n            'arithmetic_crystal_class_symbol')\n    spg_type_list = spg.spacegroup_type(hall_number)\n    _set_error_message()\n\n    if spg_type_list is not None:\n        spg_type = dict(zip(keys, spg_type_list))\n        for key in spg_type:\n            if key != 'number' and key != 'arithmetic_crystal_class_number':\n                spg_type[key] = spg_type[key].strip()\n        return spg_type\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_pointgroup(rotations):\n    _set_no_error()\n\n    # (symbol, pointgroup_number, transformation_matrix)\n    pointgroup = spg.pointgroup(np.array(rotations, dtype='intc', order='C'))\n    _set_error_message()\n    return pointgroup", "response": "Return a pointgroup in international table symbol and number."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a standardized unit cell or primitive cell.", "response": "def standardize_cell(cell,\n                     to_primitive=False,\n                     no_idealize=False,\n                     symprec=1e-5,\n                     angle_tolerance=-1.0):\n    \"\"\"Return standardized cell.\n\n    Args:\n        cell, symprec, angle_tolerance:\n            See the docstring of get_symmetry.\n        to_primitive:\n            bool: If True, the standardized primitive cell is created.\n        no_idealize:\n            bool: If True,  it is disabled to idealize lengths and angles of\n                  basis vectors and positions of atoms according to crystal\n                  symmetry.\n    Return:\n        The standardized unit cell or primitive cell is returned by a tuple of\n        (lattice, positions, numbers).\n        If it fails, None is returned.\n    \"\"\"\n    _set_no_error()\n\n    lattice, _positions, _numbers, _ = _expand_cell(cell)\n    if lattice is None:\n        return None\n\n    # Atomic positions have to be specified by scaled positions for spglib.\n    num_atom = len(_positions)\n    positions = np.zeros((num_atom * 4, 3), dtype='double', order='C')\n    positions[:num_atom] = _positions\n    numbers = np.zeros(num_atom * 4, dtype='intc')\n    numbers[:num_atom] = _numbers\n    num_atom_std = spg.standardize_cell(lattice,\n                                        positions,\n                                        numbers,\n                                        num_atom,\n                                        to_primitive * 1,\n                                        no_idealize * 1,\n                                        symprec,\n                                        angle_tolerance)\n    _set_error_message()\n\n    if num_atom_std > 0:\n        return (np.array(lattice.T, dtype='double', order='C'),\n                np.array(positions[:num_atom_std], dtype='double', order='C'),\n                np.array(numbers[:num_atom_std], dtype='intc'))\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds a primitive cell in the input cell.", "response": "def find_primitive(cell, symprec=1e-5, angle_tolerance=-1.0):\n    \"\"\"Primitive cell is searched in the input cell.\n\n    The primitive cell is returned by a tuple of (lattice, positions, numbers).\n    If it fails, None is returned.\n    \"\"\"\n    _set_no_error()\n\n    lattice, positions, numbers, _ = _expand_cell(cell)\n    if lattice is None:\n        return None\n\n    num_atom_prim = spg.primitive(lattice,\n                                  positions,\n                                  numbers,\n                                  symprec,\n                                  angle_tolerance)\n    _set_error_message()\n\n    if num_atom_prim > 0:\n        return (np.array(lattice.T, dtype='double', order='C'),\n                np.array(positions[:num_atom_prim], dtype='double', order='C'),\n                np.array(numbers[:num_atom_prim], dtype='intc'))\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning symmetry operations corresponding to a Hall symbol in the database.", "response": "def get_symmetry_from_database(hall_number):\n    \"\"\"Return symmetry operations corresponding to a Hall symbol.\n\n    The Hall symbol is given by the serial number in between 1 and 530.\n    The symmetry operations are given by a dictionary whose keys are\n    'rotations' and 'translations'.\n    If it fails, None is returned.\n    \"\"\"\n    _set_no_error()\n\n    rotations = np.zeros((192, 3, 3), dtype='intc')\n    translations = np.zeros((192, 3), dtype='double')\n    num_sym = spg.symmetry_from_database(rotations, translations, hall_number)\n    _set_error_message()\n\n    if num_sym is None:\n        return None\n    else:\n        return {'rotations':\n                np.array(rotations[:num_sym], dtype='intc', order='C'),\n                'translations':\n                np.array(translations[:num_sym], dtype='double', order='C')}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_grid_point_from_address(grid_address, mesh):\n    _set_no_error()\n\n    return spg.grid_point_from_address(np.array(grid_address, dtype='intc'),\n                                       np.array(mesh, dtype='intc'))", "response": "Return index of the grid point in the given mesh by tranlating grid address"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_ir_reciprocal_mesh(mesh,\n                           cell,\n                           is_shift=None,\n                           is_time_reversal=True,\n                           symprec=1e-5,\n                           is_dense=False):\n    \"\"\"Return k-points mesh and k-point map to the irreducible k-points.\n\n    The symmetry is serched from the input cell.\n\n    Parameters\n    ----------\n    mesh : array_like\n        Uniform sampling mesh numbers.\n        dtype='intc', shape=(3,)\n    cell : spglib cell tuple\n        Crystal structure.\n    is_shift : array_like, optional\n        [0, 0, 0] gives Gamma center mesh and value 1 gives half mesh shift.\n        Default is None which equals to [0, 0, 0].\n        dtype='intc', shape=(3,)\n    is_time_reversal : bool, optional\n        Whether time reversal symmetry is included or not. Default is True.\n    symprec : float, optional\n        Symmetry tolerance in distance. Default is 1e-5.\n    is_dense : bool, optional\n        grid_mapping_table is returned with dtype='uintp' if True. Otherwise\n        its dtype='intc'. Default is False.\n\n    Returns\n    -------\n    grid_mapping_table : ndarray\n        Grid point mapping table to ir-gird-points.\n        dtype='intc' or 'uintp', shape=(prod(mesh),)\n    grid_address : ndarray\n        Address of all grid points.\n        dtype='intc', shspe=(prod(mesh), 3)\n\n    \"\"\"\n    _set_no_error()\n\n    lattice, positions, numbers, _ = _expand_cell(cell)\n    if lattice is None:\n        return None\n\n    if is_dense:\n        dtype = 'uintp'\n    else:\n        dtype = 'intc'\n    grid_mapping_table = np.zeros(np.prod(mesh), dtype=dtype)\n    grid_address = np.zeros((np.prod(mesh), 3), dtype='intc')\n    if is_shift is None:\n        is_shift = [0, 0, 0]\n    if spg.ir_reciprocal_mesh(\n            grid_address,\n            grid_mapping_table,\n            np.array(mesh, dtype='intc'),\n            np.array(is_shift, dtype='intc'),\n            is_time_reversal * 1,\n            lattice,\n            positions,\n            numbers,\n            symprec) > 0:\n        return grid_mapping_table, grid_address\n    else:\n        return None", "response": "Returns k - points mesh and k - point map to irreducible k - points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning k - point map to irreducible k - points and k - point grid points.", "response": "def get_stabilized_reciprocal_mesh(mesh,\n                                   rotations,\n                                   is_shift=None,\n                                   is_time_reversal=True,\n                                   qpoints=None,\n                                   is_dense=False):\n    \"\"\"Return k-point map to the irreducible k-points and k-point grid points.\n\n    The symmetry is searched from the input rotation matrices in real space.\n\n    Parameters\n    ----------\n    mesh : array_like\n        Uniform sampling mesh numbers.\n        dtype='intc', shape=(3,)\n    rotations : array_like\n        Rotation matrices with respect to real space basis vectors.\n        dtype='intc', shape=(rotations, 3)\n    is_shift : array_like\n        [0, 0, 0] gives Gamma center mesh and value 1 gives  half mesh shift.\n        dtype='intc', shape=(3,)\n    is_time_reversal : bool\n        Time reversal symmetry is included or not.\n    qpoints : array_like\n        q-points used as stabilizer(s) given in reciprocal space with respect\n        to reciprocal basis vectors.\n        dtype='double', shape=(qpoints ,3) or (3,)\n    is_dense : bool, optional\n        grid_mapping_table is returned with dtype='uintp' if True. Otherwise\n        its dtype='intc'. Default is False.\n\n    Returns\n    -------\n    grid_mapping_table : ndarray\n        Grid point mapping table to ir-gird-points.\n        dtype='intc', shape=(prod(mesh),)\n    grid_address : ndarray\n        Address of all grid points. Each address is given by three unsigned\n        integers.\n        dtype='intc', shape=(prod(mesh), 3)\n\n    \"\"\"\n    _set_no_error()\n\n    if is_dense:\n        dtype = 'uintp'\n    else:\n        dtype = 'intc'\n    mapping_table = np.zeros(np.prod(mesh), dtype=dtype)\n    grid_address = np.zeros((np.prod(mesh), 3), dtype='intc')\n    if is_shift is None:\n        is_shift = [0, 0, 0]\n    if qpoints is None:\n        qpoints = np.array([[0, 0, 0]], dtype='double', order='C')\n    else:\n        qpoints = np.array(qpoints, dtype='double', order='C')\n        if qpoints.shape == (3,):\n            qpoints = np.array([qpoints], dtype='double', order='C')\n\n    if spg.stabilized_reciprocal_mesh(\n            grid_address,\n            mapping_table,\n            np.array(mesh, dtype='intc'),\n            np.array(is_shift, dtype='intc'),\n            is_time_reversal * 1,\n            np.array(rotations, dtype='intc', order='C'),\n            qpoints) > 0:\n        return mapping_table, grid_address\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_grid_points_by_rotations(address_orig,\n                                 reciprocal_rotations,\n                                 mesh,\n                                 is_shift=None,\n                                 is_dense=False):\n    \"\"\"Returns grid points obtained after rotating input grid address\n\n    Parameters\n    ----------\n    address_orig : array_like\n        Grid point address to be rotated.\n        dtype='intc', shape=(3,)\n    reciprocal_rotations : array_like\n        Rotation matrices {R} with respect to reciprocal basis vectors.\n        Defined by q'=Rq.\n        dtype='intc', shape=(rotations, 3, 3)\n    mesh : array_like\n        dtype='intc', shape=(3,)\n    is_shift : array_like, optional\n        With (1) or without (0) half grid shifts with respect to grid intervals\n        sampled along reciprocal basis vectors. Default is None, which\n        gives [0, 0, 0].\n    is_dense : bool, optional\n        rot_grid_points is returned with dtype='uintp' if True. Otherwise\n        its dtype='intc'. Default is False.\n\n    Returns\n    -------\n    rot_grid_points : ndarray\n        Grid points obtained after rotating input grid address\n        dtype='intc' or 'uintp', shape=(rotations,)\n\n    \"\"\"\n\n    _set_no_error()\n\n    if is_shift is None:\n        _is_shift = np.zeros(3, dtype='intc')\n    else:\n        _is_shift = np.array(is_shift, dtype='intc')\n\n    rot_grid_points = np.zeros(len(reciprocal_rotations), dtype='uintp')\n    spg.grid_points_by_rotations(\n        rot_grid_points,\n        np.array(address_orig, dtype='intc'),\n        np.array(reciprocal_rotations, dtype='intc', order='C'),\n        np.array(mesh, dtype='intc'),\n        _is_shift)\n\n    if is_dense:\n        return rot_grid_points\n    else:\n        return np.array(rot_grid_points, dtype='intc')", "response": "Returns the grid points obtained after rotating input grid point address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning grid points obtained after rotating input grid point address.", "response": "def get_BZ_grid_points_by_rotations(address_orig,\n                                    reciprocal_rotations,\n                                    mesh,\n                                    bz_map,\n                                    is_shift=None,\n                                    is_dense=False):\n    \"\"\"Returns grid points obtained after rotating input grid address\n\n    Parameters\n    ----------\n    address_orig : array_like\n        Grid point address to be rotated.\n        dtype='intc', shape=(3,)\n    reciprocal_rotations : array_like\n        Rotation matrices {R} with respect to reciprocal basis vectors.\n        Defined by q'=Rq.\n        dtype='intc', shape=(rotations, 3, 3)\n    mesh : array_like\n        dtype='intc', shape=(3,)\n    is_shift : array_like, optional\n        With (1) or without (0) half grid shifts with respect to grid intervals\n        sampled along reciprocal basis vectors. Default is None, which\n        gives [0, 0, 0].\n    is_dense : bool, optional\n        rot_grid_points is returned with dtype='uintp' if True. Otherwise\n        its dtype='intc'. Default is False.\n\n    Returns\n    -------\n    rot_grid_points : ndarray\n        Grid points obtained after rotating input grid address\n        dtype='intc' or 'uintp', shape=(rotations,)\n\n    \"\"\"\n\n    _set_no_error()\n\n    if is_shift is None:\n        _is_shift = np.zeros(3, dtype='intc')\n    else:\n        _is_shift = np.array(is_shift, dtype='intc')\n\n    if bz_map.dtype == 'uintp' and bz_map.flags.c_contiguous:\n        _bz_map = bz_map\n    else:\n        _bz_map = np.array(bz_map, dtype='uintp')\n\n    rot_grid_points = np.zeros(len(reciprocal_rotations), dtype='uintp')\n    spg.BZ_grid_points_by_rotations(\n        rot_grid_points,\n        np.array(address_orig, dtype='intc'),\n        np.array(reciprocal_rotations, dtype='intc', order='C'),\n        np.array(mesh, dtype='intc'),\n        _is_shift,\n        _bz_map)\n\n    if is_dense:\n        return rot_grid_points\n    else:\n        return np.array(rot_grid_points, dtype='intc')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns Delaunay reduction on a set of parameters.", "response": "def delaunay_reduce(lattice, eps=1e-5):\n    \"\"\"Run Delaunay reduction\n\n    Args:\n        lattice: Lattice parameters in the form of\n            [[a_x, a_y, a_z],\n             [b_x, b_y, b_z],\n             [c_x, c_y, c_z]]\n        symprec:\n            float: Tolerance to check if volume is close to zero or not and\n                   if two basis vectors are orthogonal by the value of dot\n                   product being close to zero or not.\n\n    Returns:\n        if the Delaunay reduction succeeded:\n            Reduced lattice parameters are given as a numpy 'double' array:\n            [[a_x, a_y, a_z],\n             [b_x, b_y, b_z],\n             [c_x, c_y, c_z]]\n        otherwise None is returned.\n    \"\"\"\n    _set_no_error()\n\n    delaunay_lattice = np.array(np.transpose(lattice),\n                                dtype='double', order='C')\n    result = spg.delaunay_reduce(delaunay_lattice, float(eps))\n    _set_error_message()\n\n    if result == 0:\n        return None\n    else:\n        return np.array(np.transpose(delaunay_lattice),\n                        dtype='double', order='C')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef niggli_reduce(lattice, eps=1e-5):\n    _set_no_error()\n\n    niggli_lattice = np.array(np.transpose(lattice), dtype='double', order='C')\n    result = spg.niggli_reduce(niggli_lattice, float(eps))\n    _set_error_message()\n\n    if result == 0:\n        return None\n    else:\n        return np.array(np.transpose(niggli_lattice),\n                        dtype='double', order='C')", "response": "Run Niggli reduction\n\n    Args:\n        lattice: Lattice parameters in the form of\n            [[a_x, a_y, a_z],\n             [b_x, b_y, b_z],\n             [c_x, c_y, c_z]]\n        eps:\n            float: Tolerance to check if difference of norms of two basis\n                   vectors is close to zero or not and if two basis vectors are\n                   orthogonal by the value of dot product being close to zero or\n                   not. The detail is shown at\n                   https://atztogo.github.io/niggli/.\n\n    Returns:\n        if the Niggli reduction succeeded:\n            Reduced lattice parameters are given as a numpy 'double' array:\n            [[a_x, a_y, a_z],\n             [b_x, b_y, b_z],\n             [c_x, c_y, c_z]]\n        otherwise None is returned."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_FORCE_SETS_lines(dataset, forces=None):\n\n    if 'first_atoms' in dataset:\n        return _get_FORCE_SETS_lines_type1(dataset, forces=forces)\n    elif 'forces' in dataset:\n        return _get_FORCE_SETS_lines_type2(dataset)", "response": "Generate FORCE_SETS string from Phonopy. set_displacement_dataset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite force constants in text file format.", "response": "def write_FORCE_CONSTANTS(force_constants,\n                          filename='FORCE_CONSTANTS',\n                          p2s_map=None):\n    \"\"\"Write force constants in text file format.\n\n    Parameters\n    ----------\n    force_constants: ndarray\n        Force constants\n        shape=(n_satom,n_satom,3,3) or (n_patom,n_satom,3,3)\n        dtype=double\n    filename: str\n        Filename to be saved\n    p2s_map: ndarray\n        Primitive atom indices in supercell index system\n        dtype=intc\n\n    \"\"\"\n\n    lines = get_FORCE_CONSTANTS_lines(force_constants, p2s_map=p2s_map)\n    with open(filename, 'w') as w:\n        w.write(\"\\n\".join(lines))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_force_constants_to_hdf5(force_constants,\n                                  filename='force_constants.hdf5',\n                                  p2s_map=None,\n                                  physical_unit=None,\n                                  compression=None):\n    \"\"\"Write force constants in hdf5 format.\n\n    Parameters\n    ----------\n    force_constants: ndarray\n        Force constants\n        shape=(n_satom,n_satom,3,3) or (n_patom,n_satom,3,3)\n        dtype=double\n    filename: str\n        Filename to be saved\n    p2s_map: ndarray\n        Primitive atom indices in supercell index system\n        shape=(n_patom,)\n        dtype=intc\n    physical_unit : str, optional\n        Physical unit used for force contants. Default is None.\n    compression : str or int, optional\n        h5py's lossless compression filters (e.g., \"gzip\", \"lzf\").\n        See the detail at docstring of h5py.Group.create_dataset. Default is\n        None.\n\n    \"\"\"\n\n    try:\n        import h5py\n    except ImportError:\n        raise ModuleNotFoundError(\"You need to install python-h5py.\")\n\n    with h5py.File(filename, 'w') as w:\n        w.create_dataset('force_constants', data=force_constants,\n                         compression=compression)\n        if p2s_map is not None:\n            w.create_dataset('p2s_map', data=p2s_map)\n        if physical_unit is not None:\n            dset = w.create_dataset('physical_unit', (1,),\n                                    dtype='S%d' % len(physical_unit))\n            dset[0] = np.string_(physical_unit)", "response": "Writes force constants in hdf5 format."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the smearing function of the log - likelihood.", "response": "def set_smearing_function(self, function_name):\n        \"\"\"\n        function_name ==\n        'Normal': smearing is done by normal distribution.\n        'Cauchy': smearing is done by Cauchy distribution.\n        \"\"\"\n        if function_name == 'Cauchy':\n            self._smearing_function = CauchyDistribution(self._sigma)\n        else:\n            self._smearing_function = NormalDistribution(self._sigma)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate q - points for each band structure path.", "response": "def get_band_qpoints(band_paths, npoints=51, rec_lattice=None):\n    \"\"\"Generate qpoints for band structure path\n\n    Note\n    ----\n\n    Behavior changes with and without rec_lattice given.\n\n    Parameters\n    ----------\n    band_paths: list of array_likes\n        Sets of end points of paths\n        dtype='double'\n        shape=(sets of paths, paths, 3)\n\n        example:\n            [[[0, 0, 0], [0.5, 0.5, 0], [0.5, 0.5, 0.5]],\n             [[0.5, 0.25, 0.75], [0, 0, 0]]]\n\n    npoints: int, optional\n        Number of q-points in each path including end points. Default is 51.\n\n    rec_lattice: array_like, optional\n        When given, q-points are sampled in a similar interval. The longest\n        path length divided by npoints including end points is used as the\n        reference interval. Reciprocal basis vectors given in column vectors.\n        dtype='double'\n        shape=(3, 3)\n\n    \"\"\"\n\n    npts = _get_npts(band_paths, npoints, rec_lattice)\n    qpoints_of_paths = []\n    c = 0\n    for band_path in band_paths:\n        nd = len(band_path)\n        for i in range(nd - 1):\n            delta = np.subtract(band_path[i + 1], band_path[i]) / (npts[c] - 1)\n            qpoints = [delta * j for j in range(npts[c])]\n            qpoints_of_paths.append(np.array(qpoints) + band_path[i])\n            c += 1\n\n    return qpoints_of_paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_band_qpoints_by_seekpath(primitive, npoints, is_const_interval=False):\n\n    try:\n        import seekpath\n    except ImportError:\n        raise ImportError(\"You need to install seekpath.\")\n\n    band_path = seekpath.get_path(primitive.totuple())\n    point_coords = band_path['point_coords']\n    qpoints_of_paths = []\n    if is_const_interval:\n        reclat = np.linalg.inv(primitive.get_cell())\n    else:\n        reclat = None\n    band_paths = [[point_coords[path[0]], point_coords[path[1]]]\n                  for path in band_path['path']]\n    npts = _get_npts(band_paths, npoints, reclat)\n    for c, path in enumerate(band_path['path']):\n        q_s = np.array(point_coords[path[0]])\n        q_e = np.array(point_coords[path[1]])\n        band = [q_s + (q_e - q_s) / (npts[c] - 1) * i for i in range(npts[c])]\n        qpoints_of_paths.append(band)\n    labels, path_connections = _get_labels(band_path['path'])\n\n    return qpoints_of_paths, labels, path_connections", "response": "Returns a list of q - points along BZ high symmetry paths."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget group velocity at q - point.", "response": "def get_group_velocity(q,  # q-point\n                       dynamical_matrix,\n                       q_length=None,  # finite distance in q\n                       symmetry=None,\n                       frequency_factor_to_THz=VaspToTHz):\n    \"\"\"\n    If frequencies and eigenvectors are supplied they are used\n    instead of calculating them at q-point (but not at q+dq and q-dq).\n\n    reciprocal lattice has to be given as\n    [[a_x, b_x, c_x],\n     [a_y, b_y, c_y],\n     [a_z, b_z, c_z]]\n    \"\"\"\n\n    gv = GroupVelocity(dynamical_matrix,\n                       q_length=q_length,\n                       symmetry=symmetry,\n                       frequency_factor_to_THz=frequency_factor_to_THz)\n    gv.set_q_points([q])\n    return gv.get_group_velocity()[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_commensurate_points(supercell_matrix):  # wrt primitive cell\n\n    smat = np.array(supercell_matrix, dtype=int)\n    rec_primitive = PhonopyAtoms(numbers=[1],\n                                 scaled_positions=[[0, 0, 0]],\n                                 cell=np.diag([1, 1, 1]),\n                                 pbc=True)\n    rec_supercell = get_supercell(rec_primitive, smat.T)\n    q_pos = rec_supercell.get_scaled_positions()\n    return np.array(np.where(q_pos > 1 - 1e-15, q_pos - 1, q_pos),\n                    dtype='double', order='C')", "response": "Returns the commensurate q - points of the current system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_commensurate_points_in_integers(supercell_matrix):\n    smat = np.array(supercell_matrix, dtype=int)\n    snf = SNF3x3(smat.T)\n    snf.run()\n    D = snf.A.diagonal()\n    b, c, a = np.meshgrid(range(D[1]), range(D[2]), range(D[0]))\n    lattice_points = np.dot(np.c_[a.ravel() * D[1] * D[2],\n                                  b.ravel() * D[0] * D[2],\n                                  c.ravel() * D[0] * D[1]], snf.Q.T)\n    lattice_points = np.array(lattice_points % np.prod(D),\n                              dtype='intc', order='C')\n    return lattice_points", "response": "This function returns the set of integer representation of lattice points in supercell vectors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_vasp(filename, cell, direct=True):\n\n    lines = get_vasp_structure_lines(cell, direct=direct)\n    with open(filename, 'w') as w:\n        w.write(\"\\n\".join(lines))", "response": "Writes crystal structure to a VASP POSCAR style file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_forces(self, vasprun_etree):\n        forces = []\n        for event, element in vasprun_etree:\n            if element.attrib['name'] == 'forces':\n                for v in element:\n                    forces.append([float(x) for x in v.text.split()])\n        return np.array(forces)", "response": "Get the forces from the vasprun XML file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn ndarray containing k - point weights in the k - star in BZ.", "response": "def k_weights_int(self):\n        \"\"\"\n        Returns\n        -------\n        ndarray\n            Geometric k-point weights (number of arms of k-star in BZ).\n            dtype='intc'\n            shape=(irreducible_kpoints,)\n\n        \"\"\"\n        nk = np.prod(self.k_mesh)\n        _weights = self.k_weights * nk\n        weights = np.rint(_weights).astype('intc')\n        assert (np.abs(weights - _weights) < 1e-7 * nk).all()\n        return np.array(weights, dtype='intc')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_lattice(self):\n\n        unit = self._values[0]\n        if unit == 'alat':\n            if not self._tags['celldm(1)']:\n                print(\"celldm(1) has to be specified when using alat.\")\n                sys.exit(1)\n            else:\n                factor = self._tags['celldm(1)']  # in Bohr\n        elif unit == 'angstrom':\n            factor = 1.0 / Bohr\n        else:\n            factor = 1.0\n\n        if len(self._values[1:]) < 9:\n            print(\"CELL_PARAMETERS is wrongly set.\")\n            sys.exit(1)\n\n        lattice = np.reshape([float(x) for x in self._values[1:10]], (3, 3))\n        self._tags['cell_parameters'] = lattice * factor", "response": "Calculate and set lattice parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, cell, is_full_fc=False, parse_fc=True):\n\n        with open(self._filename) as f:\n            fc_dct = self._parse_q2r(f)\n            self.dimension = fc_dct['dimension']\n            self.epsilon = fc_dct['dielectric']\n            self.borns = fc_dct['born']\n            if parse_fc:\n                (self.fc,\n                 self.primitive,\n                 self.supercell) = self._arrange_supercell_fc(\n                     cell, fc_dct['fc'], is_full_fc=is_full_fc)", "response": "This function takes phonopy cell and creates the force constants matrix for phonopy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_q2r(self, f):\n\n        natom, dim, epsilon, borns = self._parse_parameters(f)\n        fc_dct = {'fc': self._parse_fc(f, natom, dim),\n                  'dimension': dim,\n                  'dielectric': epsilon,\n                  'born': borns}\n        return fc_dct", "response": "Parse the q2r output file and return a dictionary of the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_fc(self, f, natom, dim):\n\n        ndim = np.prod(dim)\n        fc = np.zeros((natom, natom * ndim, 3, 3), dtype='double', order='C')\n        for k, l, i, j in np.ndindex((3, 3, natom, natom)):\n            line = f.readline()\n            for i_dim in range(ndim):\n                line = f.readline()\n                # fc[i, j * ndim + i_dim, k, l] = float(line.split()[3])\n                fc[j, i * ndim + i_dim, l, k] = float(line.split()[3])\n        return fc", "response": "Parse force constants part\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_cp2k_structure(atoms):\n\n    from cp2k_tools.generator import dict2cp2k\n\n    # CP2K's default unit is angstrom, convert it, but still declare it explictly:\n    cp2k_cell = {sym: ('[angstrom]',) + tuple(coords) for sym, coords in zip(('a', 'b', 'c'), atoms.get_cell()*Bohr)}\n    cp2k_cell['periodic'] = 'XYZ'  # anything else does not make much sense\n    cp2k_coord = {\n        'scaled': True,\n        '*': [[sym] + list(coord) for sym, coord in zip(atoms.get_chemical_symbols(), atoms.get_scaled_positions())],\n        }\n\n    return dict2cp2k(\n        {\n            'global': {\n                'run_type': 'ENERGY_FORCE',\n                },\n            'force_eval': {\n                'subsys': {\n                    'cell': cp2k_cell,\n                    'coord': cp2k_coord,\n                    },\n                'print': {\n                    'forces': {\n                        'filename': 'forces',\n                        },\n                    },\n                },\n            }\n        )", "response": "Convert the atoms structure to a CP2K input file skeleton string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn matplotlib. pyplot of QHA fitting curves at temperatures", "response": "def plot_qha(self, thin_number=10, volume_temp_exp=None):\n        \"\"\"Returns matplotlib.pyplot of QHA fitting curves at temperatures\"\"\"\n        return self._qha.plot(thin_number=thin_number,\n                              volume_temp_exp=volume_temp_exp)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_fc2(supercell,\n            symmetry,\n            dataset,\n            atom_list=None,\n            decimals=None):\n    \"\"\"Force constants are computed.\n\n    Force constants, Phi, are calculated from sets for forces, F, and\n    atomic displacement, d:\n      Phi = -F / d\n    This is solved by matrix pseudo-inversion.\n    Crystal symmetry is included when creating F and d matrices.\n\n    Returns\n    -------\n    ndarray\n        Force constants[ i, j, a, b ]\n        i: Atom index of finitely displaced atom.\n        j: Atom index at which force on the atom is measured.\n        a, b: Cartesian direction indices = (0, 1, 2) for i and j, respectively\n        dtype=double\n        shape=(len(atom_list),n_satom,3,3),\n\n    \"\"\"\n\n    if atom_list is None:\n        fc_dim0 = supercell.get_number_of_atoms()\n    else:\n        fc_dim0 = len(atom_list)\n\n    force_constants = np.zeros((fc_dim0,\n                                supercell.get_number_of_atoms(),\n                                3, 3), dtype='double', order='C')\n\n    # Fill force_constants[ displaced_atoms, all_atoms_in_supercell ]\n    atom_list_done = _get_force_constants_disps(\n        force_constants,\n        supercell,\n        dataset,\n        symmetry,\n        atom_list=atom_list)\n\n    rotations = symmetry.get_symmetry_operations()['rotations']\n    lattice = np.array(supercell.get_cell().T, dtype='double', order='C')\n    permutations = symmetry.get_atomic_permutations()\n    distribute_force_constants(force_constants,\n                               atom_list_done,\n                               lattice,\n                               rotations,\n                               permutations,\n                               atom_list=atom_list)\n\n    if decimals:\n        force_constants = force_constants.round(decimals=decimals)\n\n    return force_constants", "response": "Returns the force constants for a single Crystal Entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfulling force constants are symmetrized using crystal symmetry. This method extracts symmetrically equivalent sets of atomic pairs and take sum of their force constants and average the sum. Since get_force_constants_disps may include crystal symmetry, this method is usually meaningless.", "response": "def set_tensor_symmetry_old(force_constants,\n                            lattice,  # column vectors\n                            positions,\n                            symmetry):\n    \"\"\"Full force constants are symmetrized using crystal symmetry.\n\n    This method extracts symmetrically equivalent sets of atomic pairs and\n    take sum of their force constants and average the sum.\n\n    Since get_force_constants_disps may include crystal symmetry, this method\n    is usually meaningless.\n\n    \"\"\"\n\n    rotations = symmetry.get_symmetry_operations()['rotations']\n    translations = symmetry.get_symmetry_operations()['translations']\n    symprec = symmetry.get_symmetry_tolerance()\n\n    fc_bak = force_constants.copy()\n\n    # Create mapping table between an atom and the symmetry operated atom\n    # map[ i, j ]\n    # i: atom index\n    # j: operation index\n    mapping = []\n    for pos_i in positions:\n        map_local = []\n        for rot, trans in zip(rotations, translations):\n            rot_pos = np.dot(pos_i, rot.T) + trans\n            for j, pos_j in enumerate(positions):\n                diff = pos_j - rot_pos\n                diff -= np.rint(diff)\n                diff = np.dot(diff, lattice.T)\n                if np.linalg.norm(diff) < symprec:\n                    map_local.append(j)\n                    break\n        mapping.append(map_local)\n    mapping = np.array(mapping)\n\n    # Look for the symmetrically equivalent force constant tensors\n    for i, pos_i in enumerate(positions):\n        for j, pos_j in enumerate(positions):\n            tmp_fc = np.zeros((3, 3), dtype='double')\n            for k, rot in enumerate(rotations):\n                cart_rot = similarity_transformation(lattice, rot)\n\n                # Reverse rotation of force constant is summed\n                tmp_fc += similarity_transformation(cart_rot.T,\n                                                    fc_bak[mapping[i, k],\n                                                           mapping[j, k]])\n            # Take average and set to new force cosntants\n            force_constants[i, j] = tmp_fc / len(rotations)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfull force constants are symmetrized using crystal symmetry. This method extracts symmetrically equivalent sets of atomic pairs and take sum of their force constants and average the sum. Since get_force_constants_disps may include crystal symmetry, this method is usually meaningless.", "response": "def set_tensor_symmetry_PJ(force_constants,\n                           lattice,\n                           positions,\n                           symmetry):\n    \"\"\"Full force constants are symmetrized using crystal symmetry.\n\n    This method extracts symmetrically equivalent sets of atomic pairs and\n    take sum of their force constants and average the sum.\n\n    Since get_force_constants_disps may include crystal symmetry, this method\n    is usually meaningless.\n\n    \"\"\"\n\n    rotations = symmetry.get_symmetry_operations()['rotations']\n    translations = symmetry.get_symmetry_operations()['translations']\n    symprec = symmetry.get_symmetry_tolerance()\n\n    N = len(rotations)\n\n    mapa = _get_atom_indices_by_symmetry(lattice,\n                                         positions,\n                                         rotations,\n                                         translations,\n                                         symprec)\n    cart_rot = np.array([similarity_transformation(lattice, rot).T\n                         for rot in rotations])\n    cart_rot_inv = np.array([np.linalg.inv(rot) for rot in cart_rot])\n    fcm = np.array([force_constants[mapa[n], :, :, :][:, mapa[n], :, :]\n                    for n in range(N)])\n    s = np.transpose(np.array([np.dot(cart_rot[n],\n                                      np.dot(fcm[n], cart_rot_inv[n]))\n                               for n in range(N)]), (0, 2, 3, 1, 4))\n    force_constants[:] = np.array(np.average(s, axis=0),\n                                  dtype='double',\n                                  order='C')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenforcing permutation symmetry to force cosntants by", "response": "def set_permutation_symmetry(force_constants):\n    \"\"\"Enforce permutation symmetry to force cosntants by\n\n    Phi_ij_ab = Phi_ji_ba\n\n    i, j: atom index\n    a, b: Cartesian axis index\n\n    This is not necessary for harmonic phonon calculation because this\n    condition is imposed when making dynamical matrix Hermite in\n    dynamical_matrix.py.\n\n    \"\"\"\n\n    fc_copy = force_constants.copy()\n    for i in range(force_constants.shape[0]):\n        for j in range(force_constants.shape[1]):\n            force_constants[i, j] = (force_constants[i, j] +\n                                     fc_copy[j, i].T) / 2"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the force constants for all atoms in a single site.", "response": "def _get_force_constants_disps(force_constants,\n                               supercell,\n                               dataset,\n                               symmetry,\n                               atom_list=None):\n    \"\"\"Calculate force constants Phi = -F / d\n\n    Force constants are obtained by one of the following algorithm.\n\n    Parameters\n    ----------\n    force_constants: ndarray\n        Force constants\n        shape=(len(atom_list),n_satom,3,3)\n        dtype=double\n    supercell: Supercell\n        Supercell\n    dataset: dict\n        Distplacement dataset. Forces are also stored.\n    symmetry: Symmetry\n        Symmetry information of supercell\n    atom_list: list\n        List of atom indices corresponding to the first index of force\n        constants. None assigns all atoms in supercell.\n\n    \"\"\"\n\n    symprec = symmetry.get_symmetry_tolerance()\n    disp_atom_list = np.unique([x['number'] for x in dataset['first_atoms']])\n    for disp_atom_number in disp_atom_list:\n        disps = []\n        sets_of_forces = []\n\n        for x in dataset['first_atoms']:\n            if x['number'] != disp_atom_number:\n                continue\n            disps.append(x['displacement'])\n            sets_of_forces.append(x['forces'])\n\n        site_symmetry = symmetry.get_site_symmetry(disp_atom_number)\n\n        solve_force_constants(force_constants,\n                              disp_atom_number,\n                              disps,\n                              sets_of_forces,\n                              supercell,\n                              site_symmetry,\n                              symprec,\n                              atom_list=atom_list)\n\n    return disp_atom_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a set of displacements for all atoms in the system.", "response": "def get_least_displacements(symmetry,\n                            is_plusminus='auto',\n                            is_diagonal=True,\n                            is_trigonal=False,\n                            log_level=0):\n    \"\"\"Return a set of displacements\n\n    Returns\n    -------\n    array_like\n        List of directions with respect to axes. This gives only the\n        symmetrically non equivalent directions. The format is like:\n           [[0, 1, 0, 0],\n            [7, 1, 0, 1], ...]\n        where each list is defined by:\n           First value:      Atom index in supercell starting with 0\n           Second to fourth: If the direction is displaced or not (1, 0, or -1)\n                             with respect to the axes.\n\n    \"\"\"\n    displacements = []\n    if is_diagonal:\n        directions = directions_diag\n    else:\n        directions = directions_axis\n\n    if log_level > 2:\n        print(\"Site point symmetry:\")\n\n    for atom_num in symmetry.get_independent_atoms():\n        site_symmetry = symmetry.get_site_symmetry(atom_num)\n\n        if log_level > 2:\n            print(\"Atom %d\" % (atom_num + 1))\n            for i, rot in enumerate(site_symmetry):\n                print(\"----%d----\" % (i + 1))\n                for v in rot:\n                    print(\"%2d %2d %2d\" % tuple(v))\n\n        for disp in get_displacement(site_symmetry,\n                                     directions,\n                                     is_trigonal,\n                                     log_level):\n            displacements.append([atom_num,\n                                  disp[0], disp[1], disp[2]])\n            if is_plusminus == 'auto':\n                if is_minus_displacement(disp, site_symmetry):\n                    displacements.append([atom_num,\n                                          -disp[0], -disp[1], -disp[2]])\n            elif is_plusminus is True:\n                displacements.append([atom_num,\n                                      -disp[0], -disp[1], -disp[2]])\n\n    return displacements"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_aims(filename, atoms):\n\n    lines = \"\"\n    lines += \"# geometry.in for FHI-aims \\n\"\n    lines += \"# | generated by phonopy.FHIaims.write_aims() \\n\"\n\n    lattice_vector_line = \"lattice_vector \" + \"%16.16f \"*3 + \"\\n\"\n    for vec in atoms.get_cell():\n        lines += lattice_vector_line % tuple(vec)\n\n    N = atoms.get_number_of_atoms()\n\n    atom_line = \"atom \" + \"%16.16f \"*3 + \"%s \\n\"\n    positions = atoms.get_positions()\n    symbols = atoms.get_chemical_symbols()\n\n    initial_moment_line = \"initial_moment %16.6f\\n\"\n    magmoms = atoms.get_magnetic_moments()\n\n    for n in range(N):\n        lines += atom_line % (tuple(positions[n]) + (symbols[n],))\n        if magmoms is not None:\n            lines += initial_moment_line % magmoms[n]\n\n    with open(filename, 'w') as f:\n        f.write(lines)", "response": "Method to write FHI - aims geometry files in phonopy context."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads FHI - aims output and return geometry energy and forces from last self - consistency iteration", "response": "def read_aims_output(filename):\n    \"\"\" Read FHI-aims output and \n        return geometry, energy and forces from last self-consistency iteration\"\"\"\n\n    lines = open(filename, 'r').readlines()\n\n    l = 0\n    N = 0\n    while l < len(lines):\n        line = lines[l]\n        if \"| Number of atoms\" in line:\n            N = int(line.split()[5])\n        elif \"| Unit cell:\" in line:\n            cell = []\n            for i in range(3):\n                l += 1\n                vec = lmap(float, lines[l].split()[1:4])\n                cell.append(vec)\n        elif (\"Atomic structure:\" in line) or (\"Updated atomic structure:\" in line):\n            if \"Atomic structure:\" in line:\n                i_sym = 3\n                i_pos_min = 4 ; i_pos_max = 7\n            elif \"Updated atomic structure:\" in line:\n                i_sym = 4\n                i_pos_min = 1 ; i_pos_max = 4\n            l += 1\n            symbols = []\n            positions = []\n            for n in range(N):\n               l += 1\n               fields = lines[l].split()\n               sym = fields[i_sym]\n               pos = lmap(float, fields[i_pos_min:i_pos_max])\n               symbols.append(sym)\n               positions.append(pos)\n        elif \"Total atomic forces\" in line:\n            forces = []\n            for i in range(N):\n                l += 1\n                force = lmap(float, lines[l].split()[-3:])\n                forces.append(force)\n        l += 1\n    \n    atoms = Atoms_with_forces(cell=cell, symbols=symbols, positions=positions)\n    atoms.forces = forces\n\n    return atoms"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_primitive(cell, symprec=1e-5):\n    lattice, positions, numbers = spg.find_primitive(cell.totuple(), symprec)\n    if lattice is None:\n        return None\n    else:\n        return Atoms(numbers=numbers,\n                     scaled_positions=positions,\n                     cell=lattice,\n                     pbc=True)", "response": "Find a primitive cell in the input cell."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the equivalence of the lattice vectors.", "response": "def get_lattice_vector_equivalence(point_symmetry):\n    \"\"\"Return (b==c, c==a, a==b)\"\"\"\n    # primitive_vectors: column vectors\n\n    equivalence = [False, False, False]\n    for r in point_symmetry:\n        if (np.abs(r[:, 0]) == [0, 1, 0]).all():\n            equivalence[2] = True\n        if (np.abs(r[:, 0]) == [0, 0, 1]).all():\n            equivalence[1] = True\n        if (np.abs(r[:, 1]) == [1, 0, 0]).all():\n            equivalence[2] = True\n        if (np.abs(r[:, 1]) == [0, 0, 1]).all():\n            equivalence[0] = True\n        if (np.abs(r[:, 2]) == [1, 0, 0]).all():\n            equivalence[1] = True\n        if (np.abs(r[:, 2]) == [0, 1, 0]).all():\n            equivalence[0] = True\n\n    return equivalence"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the NAC term for the current species.", "response": "def _nac(self, q_direction):\n        \"\"\"nac_term = (A1 (x) A2) / B * coef.\n        \"\"\"\n        num_atom = self._pcell.get_number_of_atoms()\n        nac_q = np.zeros((num_atom, num_atom, 3, 3), dtype='double')\n        if (np.abs(q_direction) < 1e-5).all():\n            return nac_q\n\n        rec_lat = np.linalg.inv(self._pcell.get_cell())\n        nac_factor = self._dynmat.get_nac_factor()\n        Z = self._dynmat.get_born_effective_charges()\n        e = self._dynmat.get_dielectric_constant()\n        q = np.dot(rec_lat, q_direction)\n\n        B = self._B(e, q)\n        for i in range(num_atom):\n            A_i = self._A(q, Z, i)\n            for j in range(num_atom):\n                A_j = self._A(q, Z, j)\n                nac_q[i, j] = np.outer(A_i, A_j) / B\n\n        num_satom = self._scell.get_number_of_atoms()\n        N = num_satom // num_atom\n\n        return nac_q * nac_factor / N"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading Phonopy from a phonopy. yaml - like file.", "response": "def load(phonopy_yaml=None,  # phonopy.yaml-like must be the first argument.\n         supercell_matrix=None,\n         primitive_matrix=None,\n         is_nac=True,\n         calculator=None,\n         unitcell=None,\n         supercell=None,\n         nac_params=None,\n         unitcell_filename=None,\n         supercell_filename=None,\n         born_filename=None,\n         force_sets_filename=None,\n         force_constants_filename=None,\n         use_alm=False,\n         factor=None,\n         frequency_scale_factor=None,\n         symprec=1e-5,\n         is_symmetry=True,\n         log_level=0):\n    \"\"\"Create Phonopy instance from parameters and/or input files.\n\n    When unitcell and unitcell_filename are not given, file name that is\n    default for the chosen calculator is looked for in the current directory\n    as the default behaviour.\n\n    When force_sets_filename and force_constants_filename are not given,\n    'FORCE_SETS' is looked for in the current directory as the default\n    behaviour.\n\n    Parameters\n    ----------\n    phonopy_yaml : str, optional\n        Filename of \"phonopy.yaml\"-like file. If this is given, the data\n        in the file are parsed. Default is None.\n    supercell_matrix : array_like, optional\n        Supercell matrix multiplied to input cell basis vectors.\n        shape=(3, ) or (3, 3), where the former is considered a diagonal\n        matrix. Default is the unit matrix.\n        dtype=int\n    primitive_matrix : array_like or str, optional\n        Primitive matrix multiplied to input cell basis vectors. Default is\n        the identity matrix.\n        shape=(3, 3)\n        dtype=float\n        When 'F', 'I', 'A', 'C', or 'R' is given instead of a 3x3 matrix,\n        the primitive matrix defined at\n        https://atztogo.github.io/spglib/definition.html\n        is used.\n    is_nac : bool, optional\n        If True, look for 'BORN' file. If False, NAS is turned off.\n        The priority for NAC is nac_params > born_filename > is_nac ('BORN').\n        Default is True.\n    calculator : str, optional.\n        Calculator used for computing forces. This is used to switch the set\n        of physical units. Default is None, which is equivalent to \"vasp\".\n    unitcell : PhonopyAtoms, optional\n        Input unit cell. Default is None. The priority for cell is\n        unitcell_filename > supercell_filename > unitcell > supercell.\n    supercell : PhonopyAtoms, optional\n        Input supercell cell. Default value of primitive_matrix is set to\n        'auto' (can be overwitten). supercell_matrix is ignored. Default is\n        None. The priority for cell is\n        unitcell_filename > supercell_filename > unitcell > supercell.\n    nac_params : dict, optional\n        Parameters required for non-analytical term correction. Default is\n        None. The priority for NAC is nac_params > born_filename > is_nac.\n        {'born': Born effective charges\n                 (array_like, shape=(primitive cell atoms, 3, 3), dtype=float),\n         'dielectric': Dielectric constant matrix\n                       (array_like, shape=(3, 3), dtype=float),\n         'factor': unit conversion facotr (float)}\n    unitcell_filename : str, optional\n        Input unit cell filename. Default is None. The priority for cell is\n        unitcell_filename > supercell_filename > unitcell > supercell.\n    supercell_filename : str, optional\n        Input supercell filename. Default value of primitive_matrix is set to\n        'auto' (can be overwitten). supercell_matrix is ignored. Default is\n        None. The priority for cell is\n        unitcell_filename > supercell_filename > unitcell > supercell.\n    born_filename : str, optional\n        Filename corresponding to 'BORN', a file contains non-analytical term\n        correction parameters.\n        The priority for NAC is nac_params > born_filename > is_nac ('BORN').\n    force_sets_filename : str, optional\n        Filename of a file corresponding to 'FORCE_SETS', a file contains sets\n        of forces and displacements. Default is None.\n        The priority for force constants is\n        force_constants_filename > force_sets_filename > 'FORCE_SETS'.\n    force_constants_filename : str, optional\n        Filename of a file corresponding to 'FORCE_CONSTANTS' or\n        'force_constants.hdf5', a file contains force constants.\n        Default is None.\n        The priority for force constants is\n        force_constants_filename > force_sets_filename > 'FORCE_SETS'.\n    use_alm : bool, optional\n        Default is False.\n    factor : float, optional\n        Phonon frequency unit conversion factor. Unless specified, default\n        unit conversion factor for each calculator is used.\n    frequency_scale_factor : float, optional\n        Factor multiplied to calculated phonon frequency. Default is None,\n        i.e., effectively 1.\n    symprec : float, optional\n        Tolerance used to find crystal symmetry. Default is 1e-5.\n    is_symmetry : bool, optional\n        Setting False, crystal symmetry except for lattice translation is not\n        considered. Default is True.\n    log_level : int, optional\n        Verbosity control. Default is 0.\n\n    \"\"\"\n\n    if phonopy_yaml is None:\n        cell, smat, pmat = load_helper.get_cell_settings(\n            supercell_matrix=supercell_matrix,\n            primitive_matrix=primitive_matrix,\n            unitcell=unitcell,\n            supercell=supercell,\n            unitcell_filename=unitcell_filename,\n            supercell_filename=supercell_filename,\n            calculator=calculator,\n            symprec=symprec)\n        _nac_params = nac_params\n        _dataset = None\n        _fc = None\n    else:\n        phpy_yaml = PhonopyYaml()\n        phpy_yaml.read(phonopy_yaml)\n        cell = phpy_yaml.unitcell\n        smat = phpy_yaml.supercell_matrix\n        if smat is None:\n            raise RuntimeError(\"%s could not be parsed.\")\n        if primitive_matrix is 'auto':\n            pmat = 'auto'\n        else:\n            pmat = phpy_yaml.primitive_matrix\n        if is_nac:\n            _nac_params = phpy_yaml.nac_params\n        else:\n            _nac_params = None\n        _dataset = phpy_yaml.dataset\n        _fc =  phpy_yaml.force_constants\n\n    # units keywords: factor, nac_factor, distance_to_A\n    units = get_default_physical_units(calculator)\n    if factor is None:\n        _factor = units['factor']\n    else:\n        _factor = factor\n    phonon = Phonopy(cell,\n                     smat,\n                     primitive_matrix=pmat,\n                     factor=_factor,\n                     frequency_scale_factor=frequency_scale_factor,\n                     symprec=symprec,\n                     is_symmetry=is_symmetry,\n                     calculator=calculator,\n                     log_level=log_level)\n    load_helper.set_nac_params(phonon,\n                               _nac_params,\n                               born_filename,\n                               is_nac,\n                               units['nac_factor'])\n    if _fc is None:\n        load_helper.set_force_constants(\n            phonon,\n            dataset=_dataset,\n            force_constants_filename=force_constants_filename,\n            force_sets_filename=force_sets_filename,\n            calculator=calculator,\n            use_alm=use_alm)\n    else:\n        phonon.force_constants = _fc\n    return phonon"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the DFTB + structure files in gen - format.", "response": "def read_dftbp(filename):\n    \"\"\" Reads DFTB+ structure files in gen format.\n\n    Args:\n        filename: name of the gen-file to be read\n\n    Returns:\n        atoms: an object of the phonopy.Atoms class, representing the structure\n        found in filename\n    \"\"\"\n\n    infile = open(filename, 'r')\n\n    lines = infile.readlines()\n\n    # remove any comments\n    for ss in lines:\n        if ss.strip().startswith('#'):\n            lines.remove(ss)\n\n    natoms = int(lines[0].split()[0])\n    symbols = lines[1].split()\n\n    if (lines[0].split()[1].lower() == 'f'):\n        is_scaled = True\n        scale_pos = 1\n        scale_latvecs = dftbpToBohr\n    else:\n        is_scaled = False\n        scale_pos = dftbpToBohr\n        scale_latvecs = dftbpToBohr\n\n    # assign positions and expanded symbols\n    positions = []\n    expaned_symbols = []\n\n    for ii in range(2, natoms+2):\n        lsplit = lines[ii].split()\n\n        expaned_symbols.append(symbols[int(lsplit[1]) - 1])\n        positions.append([float(ss)*scale_pos for ss in lsplit[2:5]])\n\n    # origin is ignored, may be used in future\n    origin = [float(ss) for ss in lines[natoms+2].split()]\n\n    # assign coords of unitcell\n    cell = []\n\n    for ii in range(natoms+3, natoms+6):\n        lsplit = lines[ii].split()\n\n        cell.append([float(ss)*scale_latvecs for ss in lsplit[:3]])\n    cell = np.array(cell)\n\n    if is_scaled:\n        atoms = Atoms(symbols=expaned_symbols,\n                      cell=cell,\n                      scaled_positions=positions)\n    else:\n        atoms = Atoms(symbols=expaned_symbols,\n                      cell=cell,\n                      positions=positions)\n\n    return atoms"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreducing expanded list of symbols. Taxonomy", "response": "def get_reduced_symbols(symbols):\n    \"\"\"Reduces expanded list of symbols.\n\n    Args:\n        symbols: list containing any chemical symbols as often as\n        the atom appears in the structure\n\n    Returns:\n        reduced_symbols: any symbols appears only once\n    \"\"\"\n\n    reduced_symbols = []\n\n    for ss in symbols:\n        if not (ss in reduced_symbols):\n            reduced_symbols.append(ss)\n\n    return reduced_symbols"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite DFTB + readable gen - formatted structure files to a file.", "response": "def write_dftbp(filename, atoms):\n    \"\"\"Writes DFTB+ readable, gen-formatted structure files\n\n    Args:\n        filename: name of the gen-file to be written\n        atoms: object containing information about structure\n    \"\"\"\n    scale_pos = dftbpToBohr\n\n    lines = \"\"\n\n    # 1. line, use absolute positions\n    natoms = atoms.get_number_of_atoms()\n    lines += str(natoms)\n    lines += ' S \\n'\n\n    # 2. line\n    expaned_symbols = atoms.get_chemical_symbols()\n    symbols = get_reduced_symbols(expaned_symbols)\n    lines += ' '.join(symbols) + '\\n'\n\n    atom_numbers = []\n    for ss in expaned_symbols:\n        atom_numbers.append(symbols.index(ss) + 1)\n\n    positions = atoms.get_positions()/scale_pos\n\n    for ii in range(natoms):\n        pos = positions[ii]\n        pos_str = \"{:3d} {:3d} {:20.15f} {:20.15f} {:20.15f}\\n\".format(\n            ii + 1, atom_numbers[ii], pos[0], pos[1], pos[2])\n        lines += pos_str\n\n    # origin arbitrary\n    lines +='0.0 0.0 0.0\\n'\n\n    cell = atoms.get_cell()/scale_pos\n\n    for ii in range(3):\n        cell_str = \"{:20.15f} {:20.15f} {:20.15f}\\n\".format(\n            cell[ii][0], cell[ii][1], cell[ii][2])\n        lines += cell_str\n\n    outfile = open(filename, 'w')\n    outfile.write(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_supercells_with_displacements(supercell, cells_with_disps, filename=\"geo.gen\"):\n\n    # original cell\n    write_dftbp(filename + \"S\", supercell)\n\n    # displaced cells\n    for ii in range(len(cells_with_disps)):\n        write_dftbp(filename + \"S-{:03d}\".format(ii+1), cells_with_disps[ii])", "response": "Writes perfect supercell and supercells with displaced atoms"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _shift2boolean(self,\n                       q_mesh_shift,\n                       is_gamma_center=False,\n                       tolerance=1e-5):\n        \"\"\"\n        Tolerance is used to judge zero/half gird shift.\n        This value is not necessary to be changed usually.\n        \"\"\"\n        if q_mesh_shift is None:\n            shift = np.zeros(3, dtype='double')\n        else:\n            shift = np.array(q_mesh_shift, dtype='double')\n\n        diffby2 = np.abs(shift * 2 - np.rint(shift * 2))\n\n        if (diffby2 < 0.01).all():  # zero or half shift\n            diff = np.abs(shift - np.rint(shift))\n            if is_gamma_center:\n                is_shift = list(diff > 0.1)\n            else:  # Monkhorst-pack\n                is_shift = list(np.logical_xor((diff > 0.1),\n                                               (self._mesh % 2 == 0)) * 1)\n        else:\n            is_shift = None\n\n        return is_shift", "response": "Convert a shift to a boolean list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_interface_mode(args):\n\n    calculator_list = ['wien2k', 'abinit', 'qe', 'elk', 'siesta', 'cp2k',\n                       'crystal', 'vasp', 'dftbp', 'turbomole']\n    for calculator in calculator_list:\n        mode = \"%s_mode\" % calculator\n        if mode in args and args.__dict__[mode]:\n            return calculator\n    return None", "response": "Return calculator name based on command line arguments where\n    argparse is used."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_physical_units(interface_mode=None):\n\n    from phonopy.units import (Wien2kToTHz, AbinitToTHz, PwscfToTHz, ElkToTHz,\n                               SiestaToTHz, VaspToTHz, CP2KToTHz, CrystalToTHz,\n                               DftbpToTHz, TurbomoleToTHz, Hartree, Bohr)\n\n    units = {'factor': None,\n             'nac_factor': None,\n             'distance_to_A': None,\n             'force_constants_unit': None,\n             'length_unit': None}\n\n    if interface_mode is None or interface_mode == 'vasp':\n        units['factor'] = VaspToTHz\n        units['nac_factor'] = Hartree * Bohr\n        units['distance_to_A'] = 1.0\n        units['force_constants_unit'] = 'eV/Angstrom^2'\n        units['length_unit'] = 'Angstrom'\n    elif interface_mode == 'abinit':\n        units['factor'] = AbinitToTHz\n        units['nac_factor'] = Hartree / Bohr\n        units['distance_to_A'] = Bohr\n        units['force_constants_unit'] = 'eV/Angstrom.au'\n        units['length_unit'] = 'au'\n    elif interface_mode == 'qe':\n        units['factor'] = PwscfToTHz\n        units['nac_factor'] = 2.0\n        units['distance_to_A'] = Bohr\n        units['force_constants_unit'] = 'Ry/au^2'\n        units['length_unit'] = 'au'\n    elif interface_mode == 'wien2k':\n        units['factor'] = Wien2kToTHz\n        units['nac_factor'] = 2000.0\n        units['distance_to_A'] = Bohr\n        units['force_constants_unit'] = 'mRy/au^2'\n        units['length_unit'] = 'au'\n    elif interface_mode == 'elk':\n        units['factor'] = ElkToTHz\n        units['nac_factor'] = 1.0\n        units['distance_to_A'] = Bohr\n        units['force_constants_unit'] = 'hartree/au^2'\n        units['length_unit'] = 'au'\n    elif interface_mode == 'siesta':\n        units['factor'] = SiestaToTHz\n        units['nac_factor'] = Hartree / Bohr\n        units['distance_to_A'] = Bohr\n        units['force_constants_unit'] = 'eV/Angstrom.au'\n        units['length_unit'] = 'au'\n    elif interface_mode == 'cp2k':\n        units['factor'] = CP2KToTHz\n        units['nac_factor'] = Hartree / Bohr  # in a.u.\n        units['distance_to_A'] = Bohr\n        units['force_constants_unit'] = 'hartree/au^2'\n        units['length_unit'] = 'Angstrom'\n    elif interface_mode == 'crystal':\n        units['factor'] = CrystalToTHz\n        units['nac_factor'] = Hartree * Bohr\n        units['distance_to_A'] = 1.0\n        units['force_constants_unit'] = 'eV/Angstrom^2'\n        units['length_unit'] = 'Angstrom'\n    elif interface_mode == 'dftbp':\n        units['factor'] = DftbpToTHz\n        units['nac_factor'] = Hartree * Bohr\n        units['distance_to_A'] = Bohr\n        units['force_constants_unit'] = 'hartree/au^2'\n        units['length_unit'] = 'au'\n    elif interface_mode == 'turbomole':\n        units['factor'] = TurbomoleToTHz\n        units['nac_factor'] = 1.0\n        units['distance_to_A'] = Bohr\n        units['force_constants_unit'] = 'hartree/au^2'\n        units['length_unit'] = 'au'\n\n    return units", "response": "Returns the default physical units for a single object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_tetrahedra_relative_grid_address(microzone_lattice):\n\n    relative_grid_address = np.zeros((24, 4, 3), dtype='intc')\n    phonoc.tetrahedra_relative_grid_address(\n        relative_grid_address,\n        np.array(microzone_lattice, dtype='double', order='C'))\n\n    return relative_grid_address", "response": "Returns the relative grid addresses from the central node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_tetrahedra_integration_weight(omegas,\n                                      tetrahedra_omegas,\n                                      function='I'):\n    \"\"\"Returns integration weights\n\n    Parameters\n    ----------\n    omegas : float or list of float values\n        Energy(s) at which the integration weight(s) are computed.\n    tetrahedra_omegas : ndarray of list of list\n        Energies at vertices of 24 tetrahedra\n        shape=(24, 4)\n        dytpe='double'\n    function : str, 'I' or 'J'\n        'J' is for intetration and 'I' is for its derivative.\n\n    \"\"\"\n\n    if isinstance(omegas, float):\n        return phonoc.tetrahedra_integration_weight(\n            omegas,\n            np.array(tetrahedra_omegas, dtype='double', order='C'),\n            function)\n    else:\n        integration_weights = np.zeros(len(omegas), dtype='double')\n        phonoc.tetrahedra_integration_weight_at_omegas(\n            integration_weights,\n            np.array(omegas, dtype='double'),\n            np.array(tetrahedra_omegas, dtype='double', order='C'),\n            function)\n        return integration_weights", "response": "Returns the integration weights of the tetrahedra in the specified order."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning 2nd order of omega", "response": "def _n_2(self):\n        \"\"\"omega2 < omega < omega3\"\"\"\n        return (self._f(3, 1) * self._f(2, 1) +\n                self._f(3, 0) * self._f(1, 3) * self._f(2, 1) +\n                self._f(3, 0) * self._f(2, 0) * self._f(1, 2))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the G1 of the current species.", "response": "def _g_1(self):\n        \"\"\"omega1 < omega < omega2\"\"\"\n        # return 3 * self._n_1() / (self._omega - self._vertices_omegas[0])\n        return (3 * self._f(1, 0) * self._f(2, 0) /\n                (self._vertices_omegas[3] - self._vertices_omegas[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _g_2(self):\n        return 3 / (self._vertices_omegas[3] - self._vertices_omegas[0]) * (\n            self._f(1, 2) * self._f(2, 0) +\n            self._f(2, 1) * self._f(1, 3))", "response": "get_g_2 returns the G2 of the 2 - polarization of the system"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the G3 of the 3 - th resource.", "response": "def _g_3(self):\n        \"\"\"omega3 < omega < omega4\"\"\"\n        # return 3 * (1.0 - self._n_3()) / (self._vertices_omegas[3] - self._omega)\n        return (3 * self._f(1, 3) * self._f(2, 3) /\n                (self._vertices_omegas[3] - self._vertices_omegas[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a role to a user.", "response": "def add_role_to_user(self, user, role):\n        \"\"\"Adds a role to a user.\n\n        :param user: The user to manipulate\n        :param role: The role to add to the user\n        \"\"\"\n        user, role = self._prepare_role_modify_args(user, role)\n        if role not in user.roles:\n            user.roles.append(role)\n            self.put(user)\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving a role from a user.", "response": "def remove_role_from_user(self, user, role):\n        \"\"\"Removes a role from a user.\n\n        :param user: The user to manipulate\n        :param role: The role to remove from the user\n        \"\"\"\n        rv = False\n        user, role = self._prepare_role_modify_args(user, role)\n        if role in user.roles:\n            rv = True\n            user.roles.remove(role)\n            self.put(user)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndeactivate a specified user. Returns True if a change was made.", "response": "def deactivate_user(self, user):\n        \"\"\"Deactivates a specified user. Returns `True` if a change was made.\n\n        :param user: The user to deactivate\n        \"\"\"\n        if user.active:\n            user.active = False\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nactivate a specified user. Returns True if a change was made.", "response": "def activate_user(self, user):\n        \"\"\"Activates a specified user. Returns `True` if a change was made.\n\n        :param user: The user to activate\n        \"\"\"\n        if not user.active:\n            user.active = True\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_role(self, **kwargs):\n\n        role = self.role_model(**kwargs)\n        return self.put(role)", "response": "Creates and returns a new role from the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_or_create_role(self, name, **kwargs):\n        kwargs[\"name\"] = name\n        return self.find_role(name) or self.create_role(**kwargs)", "response": "Returns a role matching the given name or creates it with any\n        additionally provided parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates and returns a new user from the given parameters.", "response": "def create_user(self, **kwargs):\n        \"\"\"Creates and returns a new user from the given parameters.\"\"\"\n        kwargs = self._prepare_create_user_args(**kwargs)\n        user = self.user_model(**kwargs)\n        return self.put(user)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate and returns a new user from the given parameters.", "response": "def create_user(self, **kwargs):\n        \"\"\"Creates and returns a new user from the given parameters.\"\"\"\n        roles = kwargs.pop('roles', [])\n        user = self.user_model(**self._prepare_create_user_args(**kwargs))\n        user = self.put(user)\n        for role in roles:\n            self.add_role_to_user(user, role)\n        self.put(user)\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_role_to_user(self, user, role):\n        user, role = self._prepare_role_modify_args(user, role)\n        result = self.UserRole.select().where(\n            self.UserRole.user == user.id,\n            self.UserRole.role == role.id,\n        )\n        if result.count():\n            return False\n        else:\n            self.put(self.UserRole.create(user=user.id, role=role.id))\n            return True", "response": "Adds a role to a user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a role from a user.", "response": "def remove_role_from_user(self, user, role):\n        \"\"\"Removes a role from a user.\n\n        :param user: The user to manipulate\n        :param role: The role to remove from the user\n        \"\"\"\n        user, role = self._prepare_role_modify_args(user, role)\n        result = self.UserRole.select().where(\n            self.UserRole.user == user,\n            self.UserRole.role == role,\n        )\n        if result.count():\n            query = self.UserRole.delete().where(\n                self.UserRole.user == user, self.UserRole.role == role)\n            query.execute()\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef http_auth_required(realm):\n\n    def decorator(fn):\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            if _check_http_auth():\n                return fn(*args, **kwargs)\n            if _security._unauthorized_callback:\n                return _security._unauthorized_callback()\n            else:\n                r = _security.default_http_auth_realm \\\n                    if callable(realm) else realm\n                h = {'WWW-Authenticate': 'Basic realm=\"%s\"' % r}\n                return _get_unauthorized_response(headers=h)\n        return wrapper\n\n    if callable(realm):\n        return decorator(realm)\n    return decorator", "response": "Decorator that protects endpoints using Basic HTTP authentication."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef auth_token_required(fn):\n\n    @wraps(fn)\n    def decorated(*args, **kwargs):\n        if _check_token():\n            return fn(*args, **kwargs)\n        if _security._unauthorized_callback:\n            return _security._unauthorized_callback()\n        else:\n            return _get_unauthorized_response()\n    return decorated", "response": "Decorator that protects endpoints using token authentication."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef roles_required(*roles):\n    def wrapper(fn):\n        @wraps(fn)\n        def decorated_view(*args, **kwargs):\n            perms = [Permission(RoleNeed(role)) for role in roles]\n            for perm in perms:\n                if not perm.can():\n                    if _security._unauthorized_callback:\n                        return _security._unauthorized_callback()\n                    else:\n                        return _get_unauthorized_view()\n            return fn(*args, **kwargs)\n        return decorated_view\n    return wrapper", "response": "Decorator which specifies that a user must have all the specified roles."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends the confirmation instructions email for the specified user.", "response": "def send_confirmation_instructions(user):\n    \"\"\"Sends the confirmation instructions email for the specified user.\n\n    :param user: The user to send the instructions to\n    \"\"\"\n\n    confirmation_link, token = generate_confirmation_link(user)\n\n    _security.send_mail(config_value('EMAIL_SUBJECT_CONFIRM'), user.email,\n                        'confirmation_instructions', user=user,\n                        confirmation_link=confirmation_link)\n\n    confirm_instructions_sent.send(app._get_current_object(), user=user,\n                                   token=token)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a unique confirmation token for the specified user.", "response": "def generate_confirmation_token(user):\n    \"\"\"Generates a unique confirmation token for the specified user.\n\n    :param user: The user to work with\n    \"\"\"\n    data = [str(user.id), hash_data(user.email)]\n    return _security.confirm_serializer.dumps(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the expired invalid status and user of a confirmation token. For example :: expired invalid user is not logged in", "response": "def confirm_email_token_status(token):\n    \"\"\"Returns the expired status, invalid status, and user of a confirmation\n    token. For example::\n\n        expired, invalid, user = confirm_email_token_status('...')\n\n    :param token: The confirmation token\n    \"\"\"\n    expired, invalid, user, token_data = \\\n        get_token_status(token, 'confirm', 'CONFIRM_EMAIL', return_data=True)\n    if not invalid and user:\n        user_id, token_email_hash = token_data\n        invalid = not verify_hash(token_email_hash, user.email)\n    return expired, invalid, user"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef confirm_user(user):\n    if user.confirmed_at is not None:\n        return False\n    user.confirmed_at = _security.datetime_factory()\n    _datastore.put(user)\n    user_confirmed.send(app._get_current_object(), user=user)\n    return True", "response": "Confirms the specified user"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_reset_password_instructions(user):\n    token = generate_reset_password_token(user)\n    reset_link = url_for_security(\n        'reset_password', token=token, _external=True\n    )\n\n    if config_value('SEND_PASSWORD_RESET_EMAIL'):\n        _security.send_mail(config_value('EMAIL_SUBJECT_PASSWORD_RESET'),\n                            user.email, 'reset_instructions',\n                            user=user, reset_link=reset_link)\n\n    reset_password_instructions_sent.send(\n        app._get_current_object(), user=user, token=token\n    )", "response": "Sends the reset password instructions email for the specified user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends the password reset notice email for the specified user.", "response": "def send_password_reset_notice(user):\n    \"\"\"Sends the password reset notice email for the specified user.\n\n    :param user: The user to send the notice to\n    \"\"\"\n    if config_value('SEND_PASSWORD_RESET_NOTICE_EMAIL'):\n        _security.send_mail(config_value('EMAIL_SUBJECT_PASSWORD_NOTICE'),\n                            user.email, 'reset_notice', user=user)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_reset_password_token(user):\n    password_hash = hash_data(user.password) if user.password else None\n    data = [str(user.id), password_hash]\n    return _security.reset_serializer.dumps(data)", "response": "Generates a unique reset password token for the specified user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the specified user s password with the given unhashed password.", "response": "def update_password(user, password):\n    \"\"\"Update the specified user's password\n\n    :param user: The user to update_password\n    :param password: The unhashed new password\n    \"\"\"\n    user.password = hash_password(password)\n    _datastore.put(user)\n    send_password_reset_notice(user)\n    password_reset.send(app._get_current_object(), user=user)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_auth_token(self):\n        data = [str(self.id), hash_data(self.password)]\n        return _security.remember_token_serializer.dumps(data)", "response": "Returns the user s authentication token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the user identifies with the specified role.", "response": "def has_role(self, role):\n        \"\"\"Returns `True` if the user identifies with the specified role.\n\n        :param role: A role name or `Role` instance\"\"\"\n        if isinstance(role, string_types):\n            return role in (role.name for role in self.roles)\n        else:\n            return role in self.roles"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the Flask - Security extension for the specified application and datastore implementation.", "response": "def init_app(self, app, datastore=None, register_blueprint=None, **kwargs):\n        \"\"\"Initializes the Flask-Security extension for the specified\n        application and datastore implementation.\n\n        :param app: The application.\n        :param datastore: An instance of a user datastore.\n        :param register_blueprint: to register the Security blueprint or not.\n        \"\"\"\n        self.app = app\n\n        if datastore is None:\n            datastore = self._datastore\n\n        if register_blueprint is None:\n            register_blueprint = self._register_blueprint\n\n        for key, value in self._kwargs.items():\n            kwargs.setdefault(key, value)\n\n        for key, value in _default_config.items():\n            app.config.setdefault('SECURITY_' + key, value)\n\n        for key, value in _default_messages.items():\n            app.config.setdefault('SECURITY_MSG_' + key, value)\n\n        identity_loaded.connect_via(app)(_on_identity_loaded)\n\n        self._state = state = _get_state(app, datastore, **kwargs)\n\n        if register_blueprint:\n            app.register_blueprint(create_blueprint(state, __name__))\n            app.context_processor(_context_processor)\n\n        @app.before_first_request\n        def _register_i18n():\n            if '_' not in app.jinja_env.globals:\n                app.jinja_env.globals['_'] = state.i18n_domain.gettext\n\n        state.render_template = self.render_template\n        state.send_mail = self.send_mail\n        app.extensions['security'] = state\n\n        if hasattr(app, 'cli'):\n            from .cli import users, roles\n            if state.cli_users_name:\n                app.cli.add_command(users, state.cli_users_name)\n            if state.cli_roles_name:\n                app.cli.add_command(roles, state.cli_roles_name)\n\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nview function for login view", "response": "def login():\n    \"\"\"View function for login view\"\"\"\n\n    form_class = _security.login_form\n\n    if request.is_json:\n        form = form_class(MultiDict(request.get_json()))\n    else:\n        form = form_class(request.form)\n\n    if form.validate_on_submit():\n        login_user(form.user, remember=form.remember.data)\n        after_this_request(_commit)\n\n        if not request.is_json:\n            return redirect(get_post_login_redirect(form.next.data))\n\n    if request.is_json:\n        return _render_json(form, include_auth_token=True)\n\n    return _security.render_template(config_value('LOGIN_USER_TEMPLATE'),\n                                     login_user_form=form,\n                                     **_ctx('login'))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register():\n\n    if _security.confirmable or request.is_json:\n        form_class = _security.confirm_register_form\n    else:\n        form_class = _security.register_form\n\n    if request.is_json:\n        form_data = MultiDict(request.get_json())\n    else:\n        form_data = request.form\n\n    form = form_class(form_data)\n\n    if form.validate_on_submit():\n        user = register_user(**form.to_dict())\n        form.user = user\n\n        if not _security.confirmable or _security.login_without_confirmation:\n            after_this_request(_commit)\n            login_user(user)\n\n        if not request.is_json:\n            if 'next' in form:\n                redirect_url = get_post_register_redirect(form.next.data)\n            else:\n                redirect_url = get_post_register_redirect()\n\n            return redirect(redirect_url)\n        return _render_json(form, include_auth_token=True)\n\n    if request.is_json:\n        return _render_json(form)\n\n    return _security.render_template(config_value('REGISTER_USER_TEMPLATE'),\n                                     register_user_form=form,\n                                     **_ctx('register'))", "response": "View function which handles a registration request."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nviews function that sends login instructions for passwordless login", "response": "def send_login():\n    \"\"\"View function that sends login instructions for passwordless login\"\"\"\n\n    form_class = _security.passwordless_login_form\n\n    if request.is_json:\n        form = form_class(MultiDict(request.get_json()))\n    else:\n        form = form_class()\n\n    if form.validate_on_submit():\n        send_login_instructions(form.user)\n        if not request.is_json:\n            do_flash(*get_message('LOGIN_EMAIL_SENT', email=form.user.email))\n\n    if request.is_json:\n        return _render_json(form)\n\n    return _security.render_template(config_value('SEND_LOGIN_TEMPLATE'),\n                                     send_login_form=form,\n                                     **_ctx('send_login'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nview function that handles passwordless login via a token", "response": "def token_login(token):\n    \"\"\"View function that handles passwordless login via a token\"\"\"\n\n    expired, invalid, user = login_token_status(token)\n\n    if invalid:\n        do_flash(*get_message('INVALID_LOGIN_TOKEN'))\n    if expired:\n        send_login_instructions(user)\n        do_flash(*get_message('LOGIN_EXPIRED', email=user.email,\n                              within=_security.login_within))\n    if invalid or expired:\n        return redirect(url_for('login'))\n\n    login_user(user)\n    after_this_request(_commit)\n    do_flash(*get_message('PASSWORDLESS_LOGIN_SUCCESSFUL'))\n\n    return redirect(get_post_login_redirect())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_confirmation():\n\n    form_class = _security.send_confirmation_form\n\n    if request.is_json:\n        form = form_class(MultiDict(request.get_json()))\n    else:\n        form = form_class()\n\n    if form.validate_on_submit():\n        send_confirmation_instructions(form.user)\n        if not request.is_json:\n            do_flash(*get_message('CONFIRMATION_REQUEST',\n                     email=form.user.email))\n\n    if request.is_json:\n        return _render_json(form)\n\n    return _security.render_template(\n        config_value('SEND_CONFIRMATION_TEMPLATE'),\n        send_confirmation_form=form,\n        **_ctx('send_confirmation')\n    )", "response": "View function which sends confirmation instructions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nview function which handles a email confirmation request.", "response": "def confirm_email(token):\n    \"\"\"View function which handles a email confirmation request.\"\"\"\n\n    expired, invalid, user = confirm_email_token_status(token)\n\n    if not user or invalid:\n        invalid = True\n        do_flash(*get_message('INVALID_CONFIRMATION_TOKEN'))\n\n    already_confirmed = user is not None and user.confirmed_at is not None\n\n    if expired and not already_confirmed:\n        send_confirmation_instructions(user)\n        do_flash(*get_message('CONFIRMATION_EXPIRED', email=user.email,\n                              within=_security.confirm_email_within))\n    if invalid or (expired and not already_confirmed):\n        return redirect(get_url(_security.confirm_error_view) or\n                        url_for('send_confirmation'))\n\n    if user != current_user:\n        logout_user()\n\n    if confirm_user(user):\n        after_this_request(_commit)\n        msg = 'EMAIL_CONFIRMED'\n    else:\n        msg = 'ALREADY_CONFIRMED'\n\n    do_flash(*get_message(msg))\n\n    return redirect(get_url(_security.post_confirm_view) or\n                    get_url(_security.login_url))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nviews function that handles a forgotten password request.", "response": "def forgot_password():\n    \"\"\"View function that handles a forgotten password request.\"\"\"\n\n    form_class = _security.forgot_password_form\n\n    if request.is_json:\n        form = form_class(MultiDict(request.get_json()))\n    else:\n        form = form_class()\n\n    if form.validate_on_submit():\n        send_reset_password_instructions(form.user)\n        if not request.is_json:\n            do_flash(*get_message('PASSWORD_RESET_REQUEST',\n                     email=form.user.email))\n\n    if request.is_json:\n        return _render_json(form, include_user=False)\n\n    return _security.render_template(config_value('FORGOT_PASSWORD_TEMPLATE'),\n                                     forgot_password_form=form,\n                                     **_ctx('forgot_password'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nviews function that handles a reset password request.", "response": "def reset_password(token):\n    \"\"\"View function that handles a reset password request.\"\"\"\n\n    expired, invalid, user = reset_password_token_status(token)\n\n    if not user or invalid:\n        invalid = True\n        do_flash(*get_message('INVALID_RESET_PASSWORD_TOKEN'))\n\n    if expired:\n        send_reset_password_instructions(user)\n        do_flash(*get_message('PASSWORD_RESET_EXPIRED', email=user.email,\n                              within=_security.reset_password_within))\n    if invalid or expired:\n        return redirect(url_for('forgot_password'))\n\n    form = _security.reset_password_form()\n\n    if form.validate_on_submit():\n        after_this_request(_commit)\n        update_password(user, form.password.data)\n        do_flash(*get_message('PASSWORD_RESET'))\n        return redirect(get_url(_security.post_reset_view) or\n                        get_url(_security.login_url))\n\n    return _security.render_template(\n        config_value('RESET_PASSWORD_TEMPLATE'),\n        reset_password_form=form,\n        reset_password_token=token,\n        **_ctx('reset_password')\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nviewing function which handles a change password request.", "response": "def change_password():\n    \"\"\"View function which handles a change password request.\"\"\"\n\n    form_class = _security.change_password_form\n\n    if request.is_json:\n        form = form_class(MultiDict(request.get_json()))\n    else:\n        form = form_class()\n\n    if form.validate_on_submit():\n        after_this_request(_commit)\n        change_user_password(current_user._get_current_object(),\n                             form.new_password.data)\n        if not request.is_json:\n            do_flash(*get_message('PASSWORD_CHANGE'))\n            return redirect(get_url(_security.post_change_view) or\n                            get_url(_security.post_login_view))\n\n    if request.is_json:\n        form.user = current_user\n        return _render_json(form)\n\n    return _security.render_template(\n        config_value('CHANGE_PASSWORD_TEMPLATE'),\n        change_password_form=form,\n        **_ctx('change_password')\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_blueprint(state, import_name):\n\n    bp = Blueprint(state.blueprint_name, import_name,\n                   url_prefix=state.url_prefix,\n                   subdomain=state.subdomain,\n                   template_folder='templates')\n\n    bp.route(state.logout_url, endpoint='logout')(logout)\n\n    if state.passwordless:\n        bp.route(state.login_url,\n                 methods=['GET', 'POST'],\n                 endpoint='login')(send_login)\n        bp.route(state.login_url + slash_url_suffix(state.login_url,\n                                                    '<token>'),\n                 endpoint='token_login')(token_login)\n    else:\n        bp.route(state.login_url,\n                 methods=['GET', 'POST'],\n                 endpoint='login')(login)\n\n    if state.registerable:\n        bp.route(state.register_url,\n                 methods=['GET', 'POST'],\n                 endpoint='register')(register)\n\n    if state.recoverable:\n        bp.route(state.reset_url,\n                 methods=['GET', 'POST'],\n                 endpoint='forgot_password')(forgot_password)\n        bp.route(state.reset_url + slash_url_suffix(state.reset_url,\n                                                    '<token>'),\n                 methods=['GET', 'POST'],\n                 endpoint='reset_password')(reset_password)\n\n    if state.changeable:\n        bp.route(state.change_url,\n                 methods=['GET', 'POST'],\n                 endpoint='change_password')(change_password)\n\n    if state.confirmable:\n        bp.route(state.confirm_url,\n                 methods=['GET', 'POST'],\n                 endpoint='send_confirmation')(send_confirmation)\n        bp.route(state.confirm_url + slash_url_suffix(state.confirm_url,\n                                                      '<token>'),\n                 methods=['GET', 'POST'],\n                 endpoint='confirm_email')(confirm_email)\n\n    return bp", "response": "Creates the security extension blueprint"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef login_user(user, remember=None):\n\n    if remember is None:\n        remember = config_value('DEFAULT_REMEMBER_ME')\n\n    if not _login_user(user, remember):  # pragma: no cover\n        return False\n\n    if _security.trackable:\n        remote_addr = request.remote_addr or None  # make sure it is None\n\n        old_current_login, new_current_login = (\n            user.current_login_at, _security.datetime_factory()\n        )\n        old_current_ip, new_current_ip = user.current_login_ip, remote_addr\n\n        user.last_login_at = old_current_login or new_current_login\n        user.current_login_at = new_current_login\n        user.last_login_ip = old_current_ip\n        user.current_login_ip = new_current_ip\n        user.login_count = user.login_count + 1 if user.login_count else 1\n\n        _datastore.put(user)\n\n    identity_changed.send(current_app._get_current_object(),\n                          identity=Identity(user.id))\n    return True", "response": "Log a user in and return True if successful False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlog out the current user.", "response": "def logout_user():\n    \"\"\"Logs out the current.\n\n    This will also clean up the remember me cookie if it exists.\n    \"\"\"\n\n    for key in ('identity.name', 'identity.auth_type'):\n        session.pop(key, None)\n    identity_changed.send(current_app._get_current_object(),\n                          identity=AnonymousIdentity())\n    _logout_user()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Base64 encoded HMAC + SHA512 of the password signed with the salt specified by SECURITY_PASSWORD_SALT.", "response": "def get_hmac(password):\n    \"\"\"Returns a Base64 encoded HMAC+SHA512 of the password signed with\n    the salt specified by ``SECURITY_PASSWORD_SALT``.\n\n    :param password: The password to sign\n    \"\"\"\n    salt = _security.password_salt\n\n    if salt is None:\n        raise RuntimeError(\n            'The configuration value `SECURITY_PASSWORD_SALT` must '\n            'not be None when the value of `SECURITY_PASSWORD_HASH` is '\n            'set to \"%s\"' % _security.password_hash)\n\n    h = hmac.new(encode_string(salt), encode_string(password), hashlib.sha512)\n    return base64.b64encode(h.digest())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nverifying the supplied password against the supplied hash value.", "response": "def verify_password(password, password_hash):\n    \"\"\"Returns ``True`` if the password matches the supplied hash.\n\n    :param password: A plaintext password to verify\n    :param password_hash: The expected hash value of the password\n                          (usually from your database)\n    \"\"\"\n    if use_double_hash(password_hash):\n        password = get_hmac(password)\n\n    return _pwd_context.verify(password, password_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nverifies the password against the specified user.", "response": "def verify_and_update_password(password, user):\n    \"\"\"Returns ``True`` if the password is valid for the specified user.\n\n    Additionally, the hashed password in the database is updated if the\n    hashing algorithm happens to have changed.\n\n    :param password: A plaintext password to verify\n    :param user: The user to verify against\n    \"\"\"\n    if use_double_hash(user.password):\n        verified = _pwd_context.verify(get_hmac(password), user.password)\n    else:\n        # Try with original password.\n        verified = _pwd_context.verify(password, user.password)\n\n    if verified and _pwd_context.needs_update(user.password):\n        user.password = hash_password(password)\n        _datastore.put(user)\n    return verified"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hash_password(password):\n    if use_double_hash():\n        password = get_hmac(password).decode('ascii')\n\n    return _pwd_context.hash(\n        password,\n        **config_value('PASSWORD_HASH_OPTIONS', default={}).get(\n            _security.password_hash, {})\n    )", "response": "Hash the specified plaintext password."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_redirect(key):\n    rv = (get_url(session.pop(key.lower(), None)) or\n          get_url(current_app.config[key.upper()] or None) or '/')\n    return rv", "response": "Returns the URL to redirect to after a user logs in successfully."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_within_delta(key, app=None):\n    txt = config_value(key, app=app)\n    values = txt.split()\n    return timedelta(**{values[1]: int(values[0])})", "response": "Get a timedelta object from the application configuration following the internal convention of the\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_mail(subject, recipient, template, **context):\n\n    context.setdefault('security', _security)\n    context.update(_security._run_ctx_processor('mail'))\n\n    sender = _security.email_sender\n    if isinstance(sender, LocalProxy):\n        sender = sender._get_current_object()\n\n    msg = Message(subject,\n                  sender=sender,\n                  recipients=[recipient])\n\n    ctx = ('security/email', template)\n    if config_value('EMAIL_PLAINTEXT'):\n        msg.body = _security.render_template('%s/%s.txt' % ctx, **context)\n    if config_value('EMAIL_HTML'):\n        msg.html = _security.render_template('%s/%s.html' % ctx, **context)\n\n    if _security._send_mail_task:\n        _security._send_mail_task(msg)\n        return\n\n    mail = current_app.extensions.get('mail')\n    mail.send(msg)", "response": "Send an email via the Flask - Mail extension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_token_status(token, serializer, max_age=None, return_data=False):\n    serializer = getattr(_security, serializer + '_serializer')\n    max_age = get_max_age(max_age)\n    user, data = None, None\n    expired, invalid = False, False\n\n    try:\n        data = serializer.loads(token, max_age=max_age)\n    except SignatureExpired:\n        d, data = serializer.loads_unsafe(token)\n        expired = True\n    except (BadSignature, TypeError, ValueError):\n        invalid = True\n\n    if data:\n        user = _datastore.find_user(id=data[0])\n\n    expired = expired and (user is not None)\n\n    if return_data:\n        return expired, invalid, user, data\n    else:\n        return expired, invalid, user", "response": "Get the status of a token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef use_double_hash(password_hash=None):\n    # Default to plaintext for backward compatibility with\n    # SECURITY_PASSWORD_SINGLE_HASH = False\n    single_hash = config_value('PASSWORD_SINGLE_HASH') or {'plaintext'}\n\n    if password_hash is None:\n        scheme = _security.password_hash\n    else:\n        scheme = _pwd_context.identify(password_hash)\n\n    return not (single_hash is True or scheme in single_hash)", "response": "Return a bool indicating whether a password should be hashed twice."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntests utility for capturing registrations.", "response": "def capture_registrations():\n    \"\"\"Testing utility for capturing registrations.\n\n    :param confirmation_sent_at: An optional datetime object to set the\n                                 user's `confirmation_sent_at` to\n    \"\"\"\n    registrations = []\n\n    def _on(app, **data):\n        registrations.append(data)\n\n    user_registered.connect(_on)\n\n    try:\n        yield registrations\n    finally:\n        user_registered.disconnect(_on)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntests utility for capturing password reset requests.", "response": "def capture_reset_password_requests(reset_password_sent_at=None):\n    \"\"\"Testing utility for capturing password reset requests.\n\n    :param reset_password_sent_at: An optional datetime object to set the\n                                   user's `reset_password_sent_at` to\n    \"\"\"\n    reset_requests = []\n\n    def _on(app, **data):\n        reset_requests.append(data)\n\n    reset_password_instructions_sent.connect(_on)\n\n    try:\n        yield reset_requests\n    finally:\n        reset_password_instructions_sent.disconnect(_on)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_login_instructions(user):\n    token = generate_login_token(user)\n    login_link = url_for_security('token_login', token=token, _external=True)\n\n    _security.send_mail(config_value('EMAIL_SUBJECT_PASSWORDLESS'), user.email,\n                        'login_instructions', user=user, login_link=login_link)\n\n    login_instructions_sent.send(app._get_current_object(), user=user,\n                                 login_token=token)", "response": "Sends the login instructions email for the specified user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef roles_add(user, role):\n    user, role = _datastore._prepare_role_modify_args(user, role)\n    if user is None:\n        raise click.UsageError('Cannot find user.')\n    if role is None:\n        raise click.UsageError('Cannot find role.')\n    if _datastore.add_role_to_user(user, role):\n        click.secho('Role \"{0}\" added to user \"{1}\" '\n                    'successfully.'.format(role, user), fg='green')\n    else:\n        raise click.UsageError('Cannot add role to user.')", "response": "Add user to role."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving user from role.", "response": "def roles_remove(user, role):\n    \"\"\"Remove user from role.\"\"\"\n    user, role = _datastore._prepare_role_modify_args(user, role)\n    if user is None:\n        raise click.UsageError('Cannot find user.')\n    if role is None:\n        raise click.UsageError('Cannot find role.')\n    if _datastore.remove_role_from_user(user, role):\n        click.secho('Role \"{0}\" removed from user \"{1}\" '\n                    'successfully.'.format(role, user), fg='green')\n    else:\n        raise click.UsageError('Cannot remove role from user.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_password_changed_notice(user):\n    if config_value('SEND_PASSWORD_CHANGE_EMAIL'):\n        subject = config_value('EMAIL_SUBJECT_PASSWORD_CHANGE_NOTICE')\n        _security.send_mail(subject, user.email, 'change_notice', user=user)", "response": "Sends the password changed notice email for the specified user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange the specified user s password", "response": "def change_user_password(user, password):\n    \"\"\"Change the specified user's password\n\n    :param user: The user to change_password\n    :param password: The unhashed new password\n    \"\"\"\n    user.password = hash_password(password)\n    _datastore.put(user)\n    send_password_changed_notice(user)\n    password_changed.send(current_app._get_current_object(),\n                          user=user)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping a function with a new Retrying object.", "response": "def retry(*dargs, **dkw):\n    \"\"\"Wrap a function with a new `Retrying` object.\n\n    :param dargs: positional arguments passed to Retrying object\n    :param dkw: keyword arguments passed to the Retrying object\n    \"\"\"\n    # support both @retry and @retry() as valid syntax\n    if len(dargs) == 1 and callable(dargs[0]):\n        return retry()(dargs[0])\n    else:\n        def wrap(f):\n            if asyncio and asyncio.iscoroutinefunction(f):\n                r = AsyncRetrying(*dargs, **dkw)\n            elif tornado and hasattr(tornado.gen, 'is_coroutine_function') \\\n                    and tornado.gen.is_coroutine_function(f):\n                r = TornadoRetrying(*dargs, **dkw)\n            else:\n                r = Retrying(*dargs, **dkw)\n\n            return r.wraps(f)\n\n        return wrap"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy(self, sleep=_unset, stop=_unset, wait=_unset,\n             retry=_unset, before=_unset, after=_unset, before_sleep=_unset,\n             reraise=_unset):\n        \"\"\"Copy this object with some parameters changed if needed.\"\"\"\n        if before_sleep is _unset:\n            before_sleep = self.before_sleep\n        return self.__class__(\n            sleep=self.sleep if sleep is _unset else sleep,\n            stop=self.stop if stop is _unset else stop,\n            wait=self.wait if wait is _unset else wait,\n            retry=self.retry if retry is _unset else retry,\n            before=self.before if before is _unset else before,\n            after=self.after if after is _unset else after,\n            before_sleep=before_sleep,\n            reraise=self.reraise if after is _unset else reraise,\n        )", "response": "Copy this object with some parameters changed if needed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary of runtime statistics.", "response": "def statistics(self):\n        \"\"\"Return a dictionary of runtime statistics.\n\n        This dictionary will be empty when the controller has never been\n        ran. When it is running or has ran previously it should have (but\n        may not) have useful and/or informational keys and values when\n        running is underway and/or completed.\n\n        .. warning:: The keys in this dictionary **should** be some what\n                     stable (not changing), but there existence **may**\n                     change between major releases as new statistics are\n                     gathered or removed so before accessing keys ensure that\n                     they actually exist and handle when they do not.\n\n        .. note:: The values in this dictionary are local to the thread\n                  running call (so if multiple threads share the same retrying\n                  object - either directly or indirectly) they will each have\n                  there own view of statistics they have collected (in the\n                  future we may provide a way to aggregate the various\n                  statistics from each thread).\n        \"\"\"\n        try:\n            return self._local.statistics\n        except AttributeError:\n            self._local.statistics = {}\n            return self._local.statistics"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap a function for retrying. :param f: A function to wraps for retrying.", "response": "def wraps(self, f):\n        \"\"\"Wrap a function for retrying.\n\n        :param f: A function to wraps for retrying.\n        \"\"\"\n        @_utils.wraps(f)\n        def wrapped_f(*args, **kw):\n            return self.call(f, *args, **kw)\n\n        def retry_with(*args, **kwargs):\n            return self.copy(*args, **kwargs).wraps(f)\n\n        wrapped_f.retry = self\n        wrapped_f.retry_with = retry_with\n\n        return wrapped_f"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef construct(cls, attempt_number, value, has_exception):\n        fut = cls(attempt_number)\n        if has_exception:\n            fut.set_exception(value)\n        else:\n            fut.set_result(value)\n        return fut", "response": "Construct a new Future object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef before_sleep_log(logger, log_level):\n    def log_it(retry_state):\n        if retry_state.outcome.failed:\n            verb, value = 'raised', retry_state.outcome.exception()\n        else:\n            verb, value = 'returned', retry_state.outcome.result()\n\n        logger.log(log_level,\n                   \"Retrying %s in %s seconds as it %s %s.\",\n                   _utils.get_callback_name(retry_state.fn),\n                   getattr(retry_state.next_action, 'sleep'),\n                   verb, value)\n    return log_it", "response": "Before sleep log function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef before_log(logger, log_level):\n    def log_it(retry_state):\n        logger.log(log_level,\n                   \"Starting call to '%s', this is the %s time calling it.\",\n                   _utils.get_callback_name(retry_state.fn),\n                   _utils.to_ordinal(retry_state.attempt_number))\n\n    return log_it", "response": "Before log strategy that logs to some logger the attempt."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_callback_name(cb):\n    segments = []\n    try:\n        segments.append(cb.__qualname__)\n    except AttributeError:\n        try:\n            segments.append(cb.__name__)\n            if inspect.ismethod(cb):\n                try:\n                    # This attribute doesn't exist on py3.x or newer, so\n                    # we optionally ignore it... (on those versions of\n                    # python `__qualname__` should have been found anyway).\n                    segments.insert(0, cb.im_class.__name__)\n                except AttributeError:\n                    pass\n        except AttributeError:\n            pass\n    if not segments:\n        return repr(cb)\n    else:\n        try:\n            # When running under sphinx it appears this can be none?\n            if cb.__module__:\n                segments.insert(0, cb.__module__)\n        except AttributeError:\n            pass\n        return \".\".join(segments)", "response": "Get a fully - qualified name for a callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_retry_state(previous_attempt_number, delay_since_first_attempt,\n                     last_result=None):\n    \"\"\"Construct RetryCallState for given attempt number & delay.\n\n    Only used in testing and thus is extra careful about timestamp arithmetics.\n    \"\"\"\n    required_parameter_unset = (previous_attempt_number is _unset or\n                                delay_since_first_attempt is _unset)\n    if required_parameter_unset:\n        raise _make_unset_exception(\n            'wait/stop',\n            previous_attempt_number=previous_attempt_number,\n            delay_since_first_attempt=delay_since_first_attempt)\n\n    from tenacity import RetryCallState\n    retry_state = RetryCallState(None, None, (), {})\n    retry_state.attempt_number = previous_attempt_number\n    if last_result is not None:\n        retry_state.outcome = last_result\n    else:\n        retry_state.set_result(None)\n    _set_delay_since_start(retry_state, delay_since_first_attempt)\n    return retry_state", "response": "Construct RetryCallState for given attempt number & delay."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a function takes last_result parameter.", "response": "def func_takes_last_result(waiter):\n    \"\"\"Check if function has a \"last_result\" parameter.\n\n    Needed to provide backward compatibility for wait functions that didn't\n    take \"last_result\" in the beginning.\n    \"\"\"\n    if not six.callable(waiter):\n        return False\n    if not inspect.isfunction(waiter) and not inspect.ismethod(waiter):\n        # waiter is a class, check dunder-call rather than dunder-init.\n        waiter = waiter.__call__\n    waiter_spec = _utils.getargspec(waiter)\n    return 'last_result' in waiter_spec.args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecorate cls. call__ method to accept old stop signature.", "response": "def stop_dunder_call_accept_old_params(fn):\n    \"\"\"Decorate cls.__call__ method to accept old \"stop\" signature.\"\"\"\n    @_utils.wraps(fn)\n    def new_fn(self,\n               previous_attempt_number=_unset,\n               delay_since_first_attempt=_unset,\n               retry_state=None):\n        if retry_state is None:\n            from tenacity import RetryCallState\n            retry_state_passed_as_non_kwarg = (\n                previous_attempt_number is not _unset and\n                isinstance(previous_attempt_number, RetryCallState))\n            if retry_state_passed_as_non_kwarg:\n                retry_state = previous_attempt_number\n            else:\n                warn_about_dunder_non_retry_state_deprecation(fn, stacklevel=2)\n                retry_state = make_retry_state(\n                    previous_attempt_number=previous_attempt_number,\n                    delay_since_first_attempt=delay_since_first_attempt)\n        return fn(self, retry_state=retry_state)\n    return new_fn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping stop function to accept retry_state parameter.", "response": "def stop_func_accept_retry_state(stop_func):\n    \"\"\"Wrap \"stop\" function to accept \"retry_state\" parameter.\"\"\"\n    if not six.callable(stop_func):\n        return stop_func\n\n    if func_takes_retry_state(stop_func):\n        return stop_func\n\n    @_utils.wraps(stop_func)\n    def wrapped_stop_func(retry_state):\n        warn_about_non_retry_state_deprecation(\n            'stop', stop_func, stacklevel=4)\n        return stop_func(\n            retry_state.attempt_number,\n            retry_state.seconds_since_start,\n        )\n    return wrapped_stop_func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping wait function to accept retry_state parameter.", "response": "def wait_func_accept_retry_state(wait_func):\n    \"\"\"Wrap wait function to accept \"retry_state\" parameter.\"\"\"\n    if not six.callable(wait_func):\n        return wait_func\n\n    if func_takes_retry_state(wait_func):\n        return wait_func\n\n    if func_takes_last_result(wait_func):\n        @_utils.wraps(wait_func)\n        def wrapped_wait_func(retry_state):\n            warn_about_non_retry_state_deprecation(\n                'wait', wait_func, stacklevel=4)\n            return wait_func(\n                retry_state.attempt_number,\n                retry_state.seconds_since_start,\n                last_result=retry_state.outcome,\n            )\n    else:\n        @_utils.wraps(wait_func)\n        def wrapped_wait_func(retry_state):\n            warn_about_non_retry_state_deprecation(\n                'wait', wait_func, stacklevel=4)\n            return wait_func(\n                retry_state.attempt_number,\n                retry_state.seconds_since_start,\n            )\n    return wrapped_wait_func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecorating cls. call__ method to accept old retry signature.", "response": "def retry_dunder_call_accept_old_params(fn):\n    \"\"\"Decorate cls.__call__ method to accept old \"retry\" signature.\"\"\"\n    @_utils.wraps(fn)\n    def new_fn(self, attempt=_unset, retry_state=None):\n        if retry_state is None:\n            from tenacity import RetryCallState\n            if attempt is _unset:\n                raise _make_unset_exception('retry', attempt=attempt)\n            retry_state_passed_as_non_kwarg = (\n                attempt is not _unset and\n                isinstance(attempt, RetryCallState))\n            if retry_state_passed_as_non_kwarg:\n                retry_state = attempt\n            else:\n                warn_about_dunder_non_retry_state_deprecation(fn, stacklevel=2)\n                retry_state = RetryCallState(None, None, (), {})\n                retry_state.outcome = attempt\n        return fn(self, retry_state=retry_state)\n    return new_fn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef retry_func_accept_retry_state(retry_func):\n    if not six.callable(retry_func):\n        return retry_func\n\n    if func_takes_retry_state(retry_func):\n        return retry_func\n\n    @_utils.wraps(retry_func)\n    def wrapped_retry_func(retry_state):\n        warn_about_non_retry_state_deprecation(\n            'retry', retry_func, stacklevel=4)\n        return retry_func(retry_state.outcome)\n    return wrapped_retry_func", "response": "Wrap retry function to accept retry_state parameter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef before_func_accept_retry_state(fn):\n    if not six.callable(fn):\n        return fn\n\n    if func_takes_retry_state(fn):\n        return fn\n\n    @_utils.wraps(fn)\n    def wrapped_before_func(retry_state):\n        # func, trial_number, trial_time_taken\n        warn_about_non_retry_state_deprecation('before', fn, stacklevel=4)\n        return fn(\n            retry_state.fn,\n            retry_state.attempt_number,\n        )\n    return wrapped_before_func", "response": "Wrap before function to accept retry_state."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap after function to accept retry_state.", "response": "def after_func_accept_retry_state(fn):\n    \"\"\"Wrap \"after\" function to accept \"retry_state\".\"\"\"\n    if not six.callable(fn):\n        return fn\n\n    if func_takes_retry_state(fn):\n        return fn\n\n    @_utils.wraps(fn)\n    def wrapped_after_sleep_func(retry_state):\n        # func, trial_number, trial_time_taken\n        warn_about_non_retry_state_deprecation('after', fn, stacklevel=4)\n        return fn(\n            retry_state.fn,\n            retry_state.attempt_number,\n            retry_state.seconds_since_start)\n    return wrapped_after_sleep_func"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping before_sleep function to accept retry_state.", "response": "def before_sleep_func_accept_retry_state(fn):\n    \"\"\"Wrap \"before_sleep\" function to accept \"retry_state\".\"\"\"\n    if not six.callable(fn):\n        return fn\n\n    if func_takes_retry_state(fn):\n        return fn\n\n    @_utils.wraps(fn)\n    def wrapped_before_sleep_func(retry_state):\n        # retry_object, sleep, last_result\n        warn_about_non_retry_state_deprecation(\n            'before_sleep', fn, stacklevel=4)\n        return fn(\n            retry_state.retry_object,\n            sleep=getattr(retry_state.next_action, 'sleep'),\n            last_result=retry_state.outcome)\n    return wrapped_before_sleep_func"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_deleted_objects(objs, opts, user, admin_site, using):\n    # --- begin patch ---\n    collector = PolymorphicAwareNestedObjects(using=using)\n    # --- end patch ---\n    collector.collect(objs)\n    perms_needed = set()\n\n    def format_callback(obj):\n        has_admin = obj.__class__ in admin_site._registry\n        opts = obj._meta\n\n        no_edit_link = '%s: %s' % (capfirst(opts.verbose_name),\n                                   force_text(obj))\n\n        if has_admin:\n            try:\n                admin_url = reverse('%s:%s_%s_change'\n                                    % (admin_site.name,\n                                       opts.app_label,\n                                       opts.model_name),\n                                    None, (quote(obj._get_pk_val()),))\n            except NoReverseMatch:\n                # Change url doesn't exist -- don't display link to edit\n                return no_edit_link\n\n            p = '%s.%s' % (opts.app_label,\n                           get_permission_codename('delete', opts))\n            if not user.has_perm(p):\n                perms_needed.add(opts.verbose_name)\n            # Display a link to the admin page.\n            return format_html('{}: <a href=\"{}\">{}</a>',\n                               capfirst(opts.verbose_name),\n                               admin_url,\n                               obj)\n        else:\n            # Don't display link to edit, because it either has no\n            # admin or is edited inline.\n            return no_edit_link\n\n    to_delete = collector.nested(format_callback)\n\n    protected = [format_callback(obj) for obj in collector.protected]\n    model_count = {model._meta.verbose_name_plural: len(objs) for model, objs in collector.model_objs.items()}\n\n    return to_delete, model_count, perms_needed, protected", "response": "Returns a list of strings suitable for display in the admin page that can be deleted."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nested(self, format_callback=None):\n        seen = set()\n        roots = []\n        for root in self.edges.get(None, ()):\n            roots.extend(self._nested(root, seen, format_callback))\n        return roots", "response": "Return the graph as a nested list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_request_files_upload(request):\n    # FILES is a dictionary in Django but Ajax Upload gives the uploaded file\n    # an ID based on a random number, so it cannot be guessed here in the code.\n    # Rather than editing Ajax Upload to pass the ID in the querystring,\n    # note that each upload is a separate request so FILES should only\n    # have one entry.\n    # Thus, we can just grab the first (and only) value in the dict.\n    is_raw = False\n    upload = list(request.FILES.values())[0]\n    filename = upload.name\n    return upload, filename, is_raw", "response": "Handle request. FILES and return the upload and filename."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_valid_filename(s):\n    s = get_valid_filename_django(s)\n    filename, ext = os.path.splitext(s)\n    filename = slugify(filename)\n    ext = slugify(ext)\n    if ext:\n        return \"%s.%s\" % (filename, ext)\n    else:\n        return \"%s\" % (filename,)", "response": "get_valid_filename is a wrapper for get_valid_filename_django but also slugifies away\n    umlauts and stuff."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a File or Image into the given folder", "response": "def import_file(self, file_obj, folder):\n        \"\"\"\n        Create a File or an Image into the given folder\n        \"\"\"\n        try:\n            iext = os.path.splitext(file_obj.name)[1].lower()\n        except:  # noqa\n            iext = ''\n        if iext in ['.jpg', '.jpeg', '.png', '.gif']:\n            obj, created = Image.objects.get_or_create(\n                original_filename=file_obj.name,\n                file=file_obj,\n                folder=folder,\n                is_public=FILER_IS_PUBLIC_DEFAULT)\n            if created:\n                self.image_created += 1\n        else:\n            obj, created = File.objects.get_or_create(\n                original_filename=file_obj.name,\n                file=file_obj,\n                folder=folder,\n                is_public=FILER_IS_PUBLIC_DEFAULT)\n            if created:\n                self.file_created += 1\n        if self.verbosity >= 2:\n            print(\"file_created #%s / image_created #%s -- file : %s -- created : %s\" % (self.file_created,\n                                                        self.image_created,\n                                                        obj, created))\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting or creates a Folder based on the list of folder names.", "response": "def get_or_create_folder(self, folder_names):\n        \"\"\"\n        Gets or creates a Folder based the list of folder names in hierarchical\n        order (like breadcrumbs).\n\n        get_or_create_folder(['root', 'subfolder', 'subsub folder'])\n\n        creates the folders with correct parent relations and returns the\n        'subsub folder' instance.\n        \"\"\"\n        if not len(folder_names):\n            return None\n        current_parent = None\n        for folder_name in folder_names:\n            current_parent, created = Folder.objects.get_or_create(name=folder_name, parent=current_parent)\n            if created:\n                self.folder_created += 1\n                if self.verbosity >= 2:\n                    print(\"folder_created #%s folder : %s -- created : %s\" % (self.folder_created, current_parent, created))\n        return current_parent"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the url to the changelist on the fly", "response": "def _get_post_url(self, obj):\n        \"\"\"\n        Needed to retrieve the changelist url as Folder/File can be extended\n        and admin url may change\n        \"\"\"\n        # Code from django ModelAdmin to determine changelist on the fly\n        opts = obj._meta\n        return reverse('admin:%s_%s_changelist' %\n                       (opts.app_label, opts.model_name),\n            current_app=self.admin_site.name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scale_and_crop_with_subject_location(im, size, subject_location=False,\n                                         zoom=None, crop=False, upscale=False,\n                                         **kwargs):\n    \"\"\"\n    Like ``easy_thumbnails.processors.scale_and_crop``, but will use the\n    coordinates in ``subject_location`` to make sure that that part of the\n    image is in the center or at least somewhere on the cropped image.\n    Please not that this does *not* work correctly if the image has been\n    resized by a previous processor (e.g ``autocrop``).\n\n    ``crop`` needs to be set for this to work, but any special cropping\n    parameters will be ignored.\n    \"\"\"\n    subject_location = normalize_subject_location(subject_location)\n    if not (subject_location and crop):\n        # use the normal scale_and_crop\n        return processors.scale_and_crop(im, size, zoom=zoom, crop=crop,\n                                         upscale=upscale, **kwargs)\n\n    # for here on we have a subject_location and cropping is on\n\n    # --snip-- this is a copy and paste of the first few\n    #          lines of ``scale_and_crop``\n    source_x, source_y = [float(v) for v in im.size]\n    target_x, target_y = [float(v) for v in size]\n\n    if crop or not target_x or not target_y:\n        scale = max(target_x / source_x, target_y / source_y)\n    else:\n        scale = min(target_x / source_x, target_y / source_y)\n\n    # Handle one-dimensional targets.\n    if not target_x:\n        target_x = source_x * scale\n    elif not target_y:\n        target_y = source_y * scale\n\n    if zoom:\n        if not crop:\n            target_x = round(source_x * scale)\n            target_y = round(source_y * scale)\n        scale *= (100 + int(zoom)) / 100.0\n\n    if scale < 1.0 or (scale > 1.0 and upscale):\n        im = im.resize((int(source_x * scale), int(source_y * scale)),\n                       resample=Image.ANTIALIAS)\n    # --endsnip-- begin real code\n\n    # ===============================\n    # subject location aware cropping\n    # ===============================\n    # res_x, res_y: the resolution of the possibly already resized image\n    res_x, res_y = [float(v) for v in im.size]\n\n    # subj_x, subj_y: the position of the subject (maybe already re-scaled)\n    subj_x = res_x * float(subject_location[0]) / source_x\n    subj_y = res_y * float(subject_location[1]) / source_y\n    ex = (res_x - min(res_x, target_x)) / 2\n    ey = (res_y - min(res_y, target_y)) / 2\n    fx, fy = res_x - ex, res_y - ey\n\n    # box_width, box_height: dimensions of the target image\n    box_width, box_height = fx - ex, fy - ey\n\n    # try putting the box in the center around the subject point\n    # (this will be partially outside of the image in most cases)\n    tex, tey = subj_x - (box_width / 2), subj_y - (box_height / 2)\n    tfx, tfy = subj_x + (box_width / 2), subj_y + (box_height / 2)\n    if tex < 0:\n        # its out of the img to the left, move both to the right until tex is 0\n        tfx = tfx - tex  # tex is negative!\n        tex = 0\n    elif tfx > res_x:\n        # its out of the img to the right\n        tex = tex - (tfx - res_x)\n        tfx = res_x\n\n    if tey < 0:\n        # its out of the img to the top, move both to the bottom until tey is 0\n        tfy = tfy - tey  # tey is negative!)\n        tey = 0\n    elif tfy > res_y:\n        # its out of the img to the bottom\n        tey = tey - (tfy - res_y)\n        tfy = res_y\n    if ex or ey:\n        crop_box = ((int(tex), int(tey), int(tfx), int(tfy)))\n        if FILER_SUBJECT_LOCATION_IMAGE_DEBUG:\n            # draw elipse on focal point for Debugging\n            draw = ImageDraw.Draw(im)\n            esize = 10\n            draw.ellipse(((subj_x - esize, subj_y - esize),\n                          (subj_x + esize, subj_y + esize)), outline=\"#FF0000\")\n        im = im.crop(crop_box)\n    return im", "response": "This function scales the image and crops it with the subject location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndo the current user have permission to do the given action on the given item?", "response": "def filer_has_permission(context, item, action):\n    \"\"\"Does the current user (taken from the request in the context) have\n    permission to do the given action on the given item.\n\n    \"\"\"\n    permission_method_name = 'has_{action}_permission'.format(action=action)\n    permission_method = getattr(item, permission_method_name, None)\n    request = context.get('request')\n\n    if not permission_method or not request:\n        return False\n    # Call the permission method.\n    # This amounts to calling `item.has_X_permission(request)`\n    return permission_method(request)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unzip(file_obj):\n    files = []\n    # TODO: implement try-except here\n    zip = ZipFile(file_obj)\n    bad_file = zip.testzip()\n    if bad_file:\n        raise Exception('\"%s\" in the .zip archive is corrupt.' % bad_file)\n    infolist = zip.infolist()\n    for zipinfo in infolist:\n        if zipinfo.filename.startswith('__'):  # do not process meta files\n            continue\n        file_obj = SimpleUploadedFile(name=zipinfo.filename, content=zip.read(zipinfo))\n        files.append((file_obj, zipinfo.filename))\n    zip.close()\n    return files", "response": "Take a path to a zipfile and checks if it is a valid zip file\n    and returns..."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of bytes in a specific format.", "response": "def filesize(bytes, format='auto1024'):\n    \"\"\"\n    Returns the number of bytes in either the nearest unit or a specific unit\n    (depending on the chosen format method).\n\n    Acceptable formats are:\n\n    auto1024, auto1000\n      convert to the nearest unit, appending the abbreviated unit name to the\n      string (e.g. '2 KiB' or '2 kB').\n      auto1024 is the default format.\n    auto1024long, auto1000long\n      convert to the nearest multiple of 1024 or 1000, appending the correctly\n      pluralized unit name to the string (e.g. '2 kibibytes' or '2 kilobytes').\n    kB, MB, GB, TB, PB, EB, ZB or YB\n      convert to the exact unit (using multiples of 1000).\n    KiB, MiB, GiB, TiB, PiB, EiB, ZiB or YiB\n      convert to the exact unit (using multiples of 1024).\n\n    The auto1024 and auto1000 formats return a string, appending the correct\n    unit to the value. All other formats return the floating point value.\n\n    If an invalid format is specified, the bytes are returned unchanged.\n    \"\"\"\n    format_len = len(format)\n    # Check for valid format\n    if format_len in (2, 3):\n        if format_len == 3 and format[0] == 'K':\n            format = 'k%s' % format[1:]\n        if not format[-1] == 'B' or format[0] not in filesize_formats:\n            return bytes\n        if format_len == 3 and format[1] != 'i':\n            return bytes\n    elif format not in ('auto1024', 'auto1000',\n                        'auto1024long', 'auto1000long'):\n        return bytes\n\n    # Check for valid bytes\n    try:\n        bytes = int(bytes) if six.PY3 else long(bytes)  # NOQA - flake8/py3 reports error: long does not exists in py3\n    except (ValueError, TypeError):\n        return bytes\n\n    # Auto multiple of 1000 or 1024\n    if format.startswith('auto'):\n        if format[4:8] == '1000':\n            base = 1000\n        else:\n            base = 1024\n        logarithm = bytes and math.log(bytes, base) or 0\n        index = min(int(logarithm) - 1, len(filesize_formats) - 1)\n        if index >= 0:\n            if base == 1000:\n                bytes = bytes and bytes / math.pow(1000, index + 1)\n            else:\n                bytes = bytes >> (10 * (index))\n                bytes = bytes and bytes / 1024.0\n            unit = filesize_formats[index]\n        else:\n            # Change the base to 1000 so the unit will just output 'B' not 'iB'\n            base = 1000\n            unit = ''\n        if bytes >= 10 or ('%.1f' % bytes).endswith('.0'):\n            bytes = '%.0f' % bytes\n        else:\n            bytes = '%.1f' % bytes\n        if format.endswith('long'):\n            unit = filesize_long_formats.get(unit, '')\n            if base == 1024 and unit:\n                unit = '%sbi' % unit[:2]\n            unit = '%sbyte%s' % (unit, bytes != '1' and 's' or '')\n        else:\n            unit = '%s%s' % (base == 1024 and unit.upper() or unit,\n                             base == 1024 and 'iB' or 'B')\n\n        return '%s %s' % (bytes, unit)\n\n    if bytes == 0:\n        return bytes\n    base = filesize_formats.index(format[0]) + 1\n    # Exact multiple of 1000\n    if format_len == 2:\n        return bytes / (1000.0 ** base)\n    # Exact multiple of 1024\n    elif format_len == 3:\n        bytes = bytes >> (10 * (base - 1))\n        return bytes / 1024.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides the default to allow to forward to the directory listing view instead of the default change_list_view_", "response": "def response_change(self, request, obj):\n        \"\"\"\n        Overrides the default to be able to forward to the directory listing\n        instead of the default change_list_view\n        \"\"\"\n        if (\n            request.POST\n            and '_continue' not in request.POST\n            and '_saveasnew' not in request.POST\n            and '_addanother' not in request.POST\n        ):\n            # Popup in pick mode or normal mode. In both cases we want to go\n            # back to the folder list view after save. And not the useless file\n            # list view.\n            if obj.folder:\n                url = reverse('admin:filer-directory_listing',\n                              kwargs={'folder_id': obj.folder.id})\n            else:\n                url = reverse(\n                    'admin:filer-directory_listing-unfiled_images')\n            url = \"{0}{1}\".format(\n                url,\n                admin_url_params_encoded(request),\n            )\n            return HttpResponseRedirect(url)\n        return super(FileAdmin, self).response_change(request, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_view(self, request, object_id, extra_context=None):\n        try:\n            obj = self.get_queryset(request).get(pk=unquote(object_id))\n            parent_folder = obj.folder\n        except self.model.DoesNotExist:\n            parent_folder = None\n\n        if request.POST:\n            # Return to folder listing, since there is no usable file listing.\n            super(FileAdmin, self).delete_view(\n                request=request, object_id=object_id,\n                extra_context=extra_context)\n            if parent_folder:\n                url = reverse('admin:filer-directory_listing',\n                              kwargs={'folder_id': parent_folder.id})\n            else:\n                url = reverse('admin:filer-directory_listing-unfiled_images')\n            url = \"{0}{1}\".format(\n                url,\n                admin_url_params_encoded(request)\n            )\n            return HttpResponseRedirect(url)\n\n        return super(FileAdmin, self).delete_view(\n            request=request, object_id=object_id,\n            extra_context=extra_context)", "response": "Override the default delete_view to redirect to the directory listing after deletion of an image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_form(self, request, obj=None, **kwargs):\n        parent_id = request.GET.get('parent_id', None)\n        if not parent_id:\n            parent_id = request.POST.get('parent_id', None)\n        if parent_id:\n            return AddFolderPopupForm\n        else:\n            folder_form = super(FolderAdmin, self).get_form(\n                request, obj=None, **kwargs)\n\n            def folder_form_clean(form_obj):\n                cleaned_data = form_obj.cleaned_data\n                folders_with_same_name = self.get_queryset(request).filter(\n                    parent=form_obj.instance.parent,\n                    name=cleaned_data['name'])\n                if form_obj.instance.pk:\n                    folders_with_same_name = folders_with_same_name.exclude(\n                        pk=form_obj.instance.pk)\n                if folders_with_same_name.exists():\n                    raise ValidationError(\n                        'Folder with this name already exists.')\n                return cleaned_data\n\n            # attach clean to the default form rather than defining a new form class\n            folder_form.clean = folder_form_clean\n            return folder_form", "response": "Returns a new instance of the folder admin form class for use in the admin add view and change view. This is used by the admin add view and change_view."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_form(self, request, form, change):\n        r = form.save(commit=False)\n        parent_id = request.GET.get('parent_id', None)\n        if not parent_id:\n            parent_id = request.POST.get('parent_id', None)\n        if parent_id:\n            parent = self.get_queryset(request).get(id=parent_id)\n            r.parent = parent\n        return r", "response": "Given a ModelForm return an unsaved instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride the default to allow to forward to the directory listing if the change_list_view_view_view_view_view_view_view_view_view_view_view_view_view_params_encoded is set to True.", "response": "def response_change(self, request, obj):\n        \"\"\"\n        Overrides the default to be able to forward to the directory listing\n        instead of the default change_list_view\n        \"\"\"\n        if (\n            request.POST\n            and '_continue' not in request.POST\n            and '_saveasnew' not in request.POST\n            and '_addanother' not in request.POST\n        ):\n\n            if obj.parent:\n                url = reverse('admin:filer-directory_listing',\n                              kwargs={'folder_id': obj.parent.id})\n            else:\n                url = reverse('admin:filer-directory_listing-root')\n            url = \"{0}{1}\".format(\n                url,\n                admin_url_params_encoded(request),\n            )\n            return HttpResponseRedirect(url)\n        return super(FolderAdmin, self).response_change(request, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_view(self, request, object_id, extra_context=None):\n        try:\n            obj = self.get_queryset(request).get(pk=unquote(object_id))\n            parent_folder = obj.parent\n        except self.model.DoesNotExist:\n            parent_folder = None\n\n        if request.POST:\n            self.delete_files_or_folders(\n                request,\n                files_queryset=File.objects.none(),\n                folders_queryset=self.get_queryset(request).filter(id=object_id)\n            )\n            if parent_folder:\n                url = reverse('admin:filer-directory_listing',\n                              kwargs={'folder_id': parent_folder.id})\n            else:\n                url = reverse('admin:filer-directory_listing-root')\n            url = \"{0}{1}\".format(\n                url,\n                admin_url_params_encoded(request),\n            )\n            return HttpResponseRedirect(url)\n\n        return self.delete_files_or_folders(\n            request,\n            files_queryset=File.objects.none(),\n            folders_queryset=self.get_queryset(request).filter(id=object_id)\n        )", "response": "Override the default delete_view to allow redirecting to the directory view after deletion of a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef owner_search_fields(self):\n        try:\n            from django.contrib.auth import get_user_model\n        except ImportError:  # Django < 1.5\n            from django.contrib.auth.models import User\n        else:\n            User = get_user_model()\n        return [\n            field.name for field in User._meta.fields\n            if isinstance(field, models.CharField) and field.name != 'password'\n        ]", "response": "Returns all the fields that are CharFields except for password."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef response_action(self, request, files_queryset, folders_queryset):\n\n        # There can be multiple action forms on the page (at the top\n        # and bottom of the change list, for example). Get the action\n        # whose button was pushed.\n        try:\n            action_index = int(request.POST.get('index', 0))\n        except ValueError:\n            action_index = 0\n\n        # Construct the action form.\n        data = request.POST.copy()\n        data.pop(helpers.ACTION_CHECKBOX_NAME, None)\n        data.pop(\"index\", None)\n\n        # Use the action whose button was pushed\n        try:\n            data.update({'action': data.getlist('action')[action_index]})\n        except IndexError:\n            # If we didn't get an action from the chosen form that's invalid\n            # POST data, so by deleting action it'll fail the validation check\n            # below. So no need to do anything here\n            pass\n\n        action_form = self.action_form(data, auto_id=None)\n        action_form.fields['action'].choices = self.get_action_choices(request)\n\n        # If the form's valid we can handle the action.\n        if action_form.is_valid():\n            action = action_form.cleaned_data['action']\n            select_across = action_form.cleaned_data['select_across']\n            func, name, description = self.get_actions(request)[action]\n\n            # Get the list of selected PKs. If nothing's selected, we can't\n            # perform an action on it, so bail. Except we want to perform\n            # the action explicitly on all objects.\n            selected = request.POST.getlist(helpers.ACTION_CHECKBOX_NAME)\n            if not selected and not select_across:\n                # Reminder that something needs to be selected or nothing\n                # will happen\n                msg = _(\"Items must be selected in order to perform \"\n                        \"actions on them. No items have been changed.\")\n                self.message_user(request, msg)\n                return None\n\n            if not select_across:\n                selected_files = []\n                selected_folders = []\n\n                for pk in selected:\n                    if pk[:5] == \"file-\":\n                        selected_files.append(pk[5:])\n                    else:\n                        selected_folders.append(pk[7:])\n\n                # Perform the action only on the selected objects\n                files_queryset = files_queryset.filter(pk__in=selected_files)\n                folders_queryset = folders_queryset.filter(\n                    pk__in=selected_folders)\n\n            response = func(self, request, files_queryset, folders_queryset)\n\n            # Actions may return an HttpResponse, which will be used as the\n            # response from the POST. If not, we'll be a good little HTTP\n            # citizen and redirect back to the changelist page.\n            if isinstance(response, HttpResponse):\n                return response\n            else:\n                return HttpResponseRedirect(request.get_full_path())\n        else:\n            msg = _(\"No action selected.\")\n            self.message_user(request, msg)\n            return None", "response": "Handles an admin action on the changelist. This is called by the admin page to handle the action. This is called by the admin page to handle the action."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\naction which moves selected files and folders in selected folders to clipboard.", "response": "def move_to_clipboard(self, request, files_queryset, folders_queryset):\n        \"\"\"\n        Action which moves the selected files and files in selected folders\n        to clipboard.\n        \"\"\"\n\n        if not self.has_change_permission(request):\n            raise PermissionDenied\n\n        if request.method != 'POST':\n            return None\n\n        clipboard = tools.get_user_clipboard(request.user)\n\n        check_files_edit_permissions(request, files_queryset)\n        check_folder_edit_permissions(request, folders_queryset)\n\n        # TODO: Display a confirmation page if moving more than X files to\n        # clipboard?\n\n        # We define it like that so that we can modify it inside the\n        # move_files function\n        files_count = [0]\n\n        def move_files(files):\n            files_count[0] += tools.move_file_to_clipboard(files, clipboard)\n\n        def move_folders(folders):\n            for f in folders:\n                move_files(f.files)\n                move_folders(f.children.all())\n\n        move_files(files_queryset)\n        move_folders(folders_queryset)\n\n        self.message_user(request, _(\"Successfully moved %(count)d files to \"\n                                     \"clipboard.\") % {\"count\": files_count[0]})\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\naction which deletes the selected files and/or folders. This action first displays a confirmation page whichs shows all the deleteable files and/or folders, or, if the user has no permission on one of the related childs (foreignkeys), a \"permission denied\" message. Next, it deletes all selected files and/or folders and redirects back to the folder.", "response": "def delete_files_or_folders(self, request, files_queryset, folders_queryset):\n        \"\"\"\n        Action which deletes the selected files and/or folders.\n\n        This action first displays a confirmation page whichs shows all the\n        deleteable files and/or folders, or, if the user has no permission on\n        one of the related childs (foreignkeys), a \"permission denied\" message.\n\n        Next, it deletes all selected files and/or folders and redirects back to\n        the folder.\n        \"\"\"\n        opts = self.model._meta\n        app_label = opts.app_label\n\n        # Check that the user has delete permission for the actual model\n        if not self.has_delete_permission(request):\n            raise PermissionDenied\n\n        current_folder = self._get_current_action_folder(\n            request, files_queryset, folders_queryset)\n\n        all_protected = []\n\n        # Populate deletable_objects, a data structure of all related objects\n        # that will also be deleted. Hopefully this also checks for necessary\n        # permissions.\n        # TODO: Check if permissions are really verified\n        using = router.db_for_write(self.model)\n        deletable_files, model_count_files, perms_needed_files, protected_files = get_deleted_objects(files_queryset, files_queryset.model._meta, request.user, self.admin_site, using)\n        deletable_folders, model_count_folder, perms_needed_folders, protected_folders = get_deleted_objects(folders_queryset, folders_queryset.model._meta, request.user, self.admin_site, using)\n        all_protected.extend(protected_files)\n        all_protected.extend(protected_folders)\n\n        all_deletable_objects = [deletable_files, deletable_folders]\n        all_perms_needed = perms_needed_files.union(perms_needed_folders)\n\n        # The user has already confirmed the deletion. Do the deletion and\n        # return a None to display the change list view again.\n        if request.POST.get('post'):\n            if all_perms_needed:\n                raise PermissionDenied\n            n = files_queryset.count() + folders_queryset.count()\n            if n:\n                # delete all explicitly selected files\n                for f in files_queryset:\n                    self.log_deletion(request, f, force_text(f))\n                    f.delete()\n                # delete all files in all selected folders and their children\n                # This would happen automatically by ways of the delete\n                # cascade, but then the individual .delete() methods won't be\n                # called and the files won't be deleted from the filesystem.\n                folder_ids = set()\n                for folder in folders_queryset:\n                    folder_ids.add(folder.id)\n                    folder_ids.update(\n                        folder.get_descendants().values_list('id', flat=True))\n                for f in File.objects.filter(folder__in=folder_ids):\n                    self.log_deletion(request, f, force_text(f))\n                    f.delete()\n                # delete all folders\n                for f in folders_queryset:\n                    self.log_deletion(request, f, force_text(f))\n                    f.delete()\n                self.message_user(request, _(\"Successfully deleted %(count)d files and/or folders.\") % {\"count\": n, })\n            # Return None to display the change list page again.\n            return None\n\n        if all_perms_needed or all_protected:\n            title = _(\"Cannot delete files and/or folders\")\n        else:\n            title = _(\"Are you sure?\")\n\n        context = self.admin_site.each_context(request)\n        context.update({\n            \"title\": title,\n            \"instance\": current_folder,\n            \"breadcrumbs_action\": _(\"Delete files and/or folders\"),\n            \"deletable_objects\": all_deletable_objects,\n            \"files_queryset\": files_queryset,\n            \"folders_queryset\": folders_queryset,\n            \"perms_lacking\": all_perms_needed,\n            \"protected\": all_protected,\n            \"opts\": opts,\n            'is_popup': popup_status(request),\n            'filer_admin_context': AdminContext(request),\n            \"root_path\": reverse('admin:index'),\n            \"app_label\": app_label,\n            \"action_checkbox_name\": helpers.ACTION_CHECKBOX_NAME,\n        })\n\n        # Display the destination folder selection page\n        return render(\n            request,\n            \"admin/filer/delete_selected_files_confirmation.html\",\n            context\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef admin_url_params(request, params=None):\n    params = params or {}\n    if popup_status(request):\n        params[IS_POPUP_VAR] = '1'\n    pick_type = popup_pick_type(request)\n    if pick_type:\n        params['_pick'] = pick_type\n    return params", "response": "Returns a dictionary of parameters to be passed to the admin_url function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean_subject_location(self):\n        cleaned_data = super(ImageAdminForm, self).clean()\n        subject_location = cleaned_data['subject_location']\n        if not subject_location:\n            # if supplied subject location is empty, do not check it\n            return subject_location\n\n        # use thumbnail's helper function to check the format\n        coordinates = normalize_subject_location(subject_location)\n\n        if not coordinates:\n            err_msg = ugettext_lazy('Invalid subject location format. ')\n            err_code = 'invalid_subject_format'\n\n        elif (\n            coordinates[0] > self.instance.width\n            or coordinates[1] > self.instance.height\n        ):\n            err_msg = ugettext_lazy(\n                'Subject location is outside of the image. ')\n            err_code = 'subject_out_of_bounds'\n        else:\n            return subject_location\n\n        self._set_previous_subject_location(cleaned_data)\n        raise forms.ValidationError(\n            string_concat(\n                err_msg,\n                ugettext_lazy('Your input: \"{subject_location}\". '.format(\n                    subject_location=subject_location)),\n                'Previous value is restored.'),\n            code=err_code)", "response": "Validate subject_location preserving last saved value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_object(import_path):\n    if not isinstance(import_path, six.string_types):\n        return import_path\n    if '.' not in import_path:\n        raise TypeError(\n            \"'import_path' argument to 'django_load.core.load_object' must \"\n            \"contain at least one dot.\")\n    module_name, object_name = import_path.rsplit('.', 1)\n    module = import_module(module_name)\n    return getattr(module, object_name)", "response": "Loads an object from an import_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle(self, *args, **options):\n        pks = Image.objects.all().values_list('id', flat=True)\n        total = len(pks)\n        for idx, pk in enumerate(pks):\n            image = None\n            try:\n                image = Image.objects.get(pk=pk)\n                self.stdout.write(u'Processing image {0} / {1} {2}'.format(idx + 1, total, image))\n                self.stdout.flush()\n                image.thumbnails\n                image.icons\n            except IOError as e:\n                self.stderr.write('Failed to generate thumbnails: {0}'.format(str(e)))\n                self.stderr.flush()\n            finally:\n                del image", "response": "Generate thumbnails for all the images in the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upath(path):\n    if six.PY2 and not isinstance(path, six.text_type):\n        return path.decode(fs_encoding)\n    return path", "response": "Returns a unicode path."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a Content - Disposition header to the response.", "response": "def save_as_header(self, response, **kwargs):\n        \"\"\"\n        * if save_as is False the header will not be added\n        * if save_as is a filename, it will be used in the header\n        * if save_as is True or None the filename will be determined from the\n          file path\n        \"\"\"\n        save_as = kwargs.get('save_as', None)\n        if save_as is False:\n            return\n        file_obj = kwargs.get('file_obj', None)\n        if save_as is True or save_as is None:\n            filename = os.path.basename(file_obj.path)\n        else:\n            filename = save_as\n        response['Content-Disposition'] = smart_str(\n            'attachment; filename=%s' % filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_model_label(model):\n    if isinstance(model, six.string_types):\n        return model\n    else:\n        return \"%s.%s\" % (\n            model._meta.app_label,\n            model.__name__\n        )", "response": "Get a model label from a model class or model label."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ajax_upload(request, folder_id=None):\n    folder = None\n    if folder_id:\n        try:\n            # Get folder\n            folder = Folder.objects.get(pk=folder_id)\n        except Folder.DoesNotExist:\n            return JsonResponse({'error': NO_FOLDER_ERROR})\n\n    # check permissions\n    if folder and not folder.has_add_children_permission(request):\n        return JsonResponse({'error': NO_PERMISSIONS_FOR_FOLDER})\n    try:\n        if len(request.FILES) == 1:\n            # dont check if request is ajax or not, just grab the file\n            upload, filename, is_raw = handle_request_files_upload(request)\n        else:\n            # else process the request as usual\n            upload, filename, is_raw = handle_upload(request)\n        # TODO: Deprecated/refactor\n        # Get clipboad\n        # clipboard = Clipboard.objects.get_or_create(user=request.user)[0]\n\n        # find the file type\n        for filer_class in filer_settings.FILER_FILE_MODELS:\n            FileSubClass = load_model(filer_class)\n            # TODO: What if there are more than one that qualify?\n            if FileSubClass.matches_file_type(filename, upload, request):\n                FileForm = modelform_factory(\n                    model=FileSubClass,\n                    fields=('original_filename', 'owner', 'file')\n                )\n                break\n        uploadform = FileForm({'original_filename': filename,\n                               'owner': request.user.pk},\n                              {'file': upload})\n        if uploadform.is_valid():\n            file_obj = uploadform.save(commit=False)\n            # Enforce the FILER_IS_PUBLIC_DEFAULT\n            file_obj.is_public = filer_settings.FILER_IS_PUBLIC_DEFAULT\n            file_obj.folder = folder\n            file_obj.save()\n            # TODO: Deprecated/refactor\n            # clipboard_item = ClipboardItem(\n            #     clipboard=clipboard, file=file_obj)\n            # clipboard_item.save()\n\n            # Try to generate thumbnails.\n            if not file_obj.icons:\n                # There is no point to continue, as we can't generate\n                # thumbnails for this file. Usual reasons: bad format or\n                # filename.\n                file_obj.delete()\n                # This would be logged in BaseImage._generate_thumbnails()\n                # if FILER_ENABLE_LOGGING is on.\n                return JsonResponse(\n                    {'error': 'failed to generate icons for file'},\n                    status=500,\n                )\n            thumbnail = None\n            # Backwards compatibility: try to get specific icon size (32px)\n            # first. Then try medium icon size (they are already sorted),\n            # fallback to the first (smallest) configured icon.\n            for size in (['32']\n                        + filer_settings.FILER_ADMIN_ICON_SIZES[1::-1]):\n                try:\n                    thumbnail = file_obj.icons[size]\n                    break\n                except KeyError:\n                    continue\n\n            data = {\n                'thumbnail': thumbnail,\n                'alt_text': '',\n                'label': str(file_obj),\n                'file_id': file_obj.pk,\n            }\n            # prepare preview thumbnail\n            if type(file_obj) == Image:\n                thumbnail_180_options = {\n                    'size': (180, 180),\n                    'crop': True,\n                    'upscale': True,\n                }\n                thumbnail_180 = file_obj.file.get_thumbnail(\n                    thumbnail_180_options)\n                data['thumbnail_180'] = thumbnail_180.url\n                data['original_image'] = file_obj.url\n            return JsonResponse(data)\n        else:\n            form_errors = '; '.join(['%s: %s' % (\n                field,\n                ', '.join(errors)) for field, errors in list(\n                    uploadform.errors.items())\n            ])\n            raise UploadException(\n                \"AJAX request not valid: form invalid '%s'\" % (\n                    form_errors,))\n    except UploadException as e:\n        return JsonResponse({'error': str(e)}, status=500)", "response": "Handles an upload from the uploader."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __store_config(self, args, kwargs):\n        signature = (\n            'schema',\n            'ignore_none_values',\n            'allow_unknown',\n            'require_all',\n            'purge_unknown',\n            'purge_readonly',\n        )\n        for i, p in enumerate(signature[: len(args)]):\n            if p in kwargs:\n                raise TypeError(\"__init__ got multiple values for argument \" \"'%s'\" % p)\n            else:\n                kwargs[p] = args[i]\n        self._config = kwargs\n        \"\"\" This dictionary holds the configuration arguments that were used to\n            initialize the :class:`Validator` instance except the\n            ``error_handler``. \"\"\"", "response": "Store the configuration arguments that were passed to the constructor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate and adds one or multiple errors.", "response": "def _error(self, *args):\n        \"\"\" Creates and adds one or multiple errors.\n\n        :param args: Accepts different argument's signatures.\n\n                     *1. Bulk addition of errors:*\n\n                     - :term:`iterable` of\n                       :class:`~cerberus.errors.ValidationError`-instances\n\n                     The errors will be added to\n                     :attr:`~cerberus.Validator._errors`.\n\n                     *2. Custom error:*\n\n                     - the invalid field's name\n\n                     - the error message\n\n                     A custom error containing the message will be created and\n                     added to :attr:`~cerberus.Validator._errors`.\n                     There will however be fewer information contained in the\n                     error (no reference to the violated rule and its\n                     constraint).\n\n                     *3. Defined error:*\n\n                     - the invalid field's name\n\n                     - the error-reference, see :mod:`cerberus.errors`\n\n                     - arbitrary, supplemental information about the error\n\n                     A :class:`~cerberus.errors.ValidationError` instance will\n                     be created and added to\n                     :attr:`~cerberus.Validator._errors`.\n        \"\"\"\n        if len(args) == 1:\n            self._errors.extend(args[0])\n            self._errors.sort()\n            for error in args[0]:\n                self.document_error_tree.add(error)\n                self.schema_error_tree.add(error)\n                self.error_handler.emit(error)\n        elif len(args) == 2 and isinstance(args[1], _str_type):\n            self._error(args[0], errors.CUSTOM, args[1])\n        elif len(args) >= 2:\n            field = args[0]\n            code = args[1].code\n            rule = args[1].rule\n            info = args[2:]\n\n            document_path = self.document_path + (field,)\n\n            schema_path = self.schema_path\n            if code != errors.UNKNOWN_FIELD.code and rule is not None:\n                schema_path += (field, rule)\n\n            if not rule:\n                constraint = None\n            else:\n                field_definitions = self._resolve_rules_set(self.schema[field])\n                if rule == 'nullable':\n                    constraint = field_definitions.get(rule, False)\n                elif rule == 'required':\n                    constraint = field_definitions.get(rule, self.require_all)\n                    if rule not in field_definitions:\n                        schema_path = \"__require_all__\"\n                else:\n                    constraint = field_definitions[rule]\n\n            value = self.document.get(field)\n\n            self.recent_error = errors.ValidationError(\n                document_path, schema_path, code, rule, constraint, value, info\n            )\n            self._error([self.recent_error])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_child_validator(self, document_crumb=None, schema_crumb=None, **kwargs):\n        child_config = self._config.copy()\n        child_config.update(kwargs)\n        if not self.is_child:\n            child_config['is_child'] = True\n            child_config['error_handler'] = toy_error_handler\n            child_config['root_allow_unknown'] = self.allow_unknown\n            child_config['root_require_all'] = self.require_all\n            child_config['root_document'] = self.document\n            child_config['root_schema'] = self.schema\n\n        child_validator = self.__class__(**child_config)\n\n        if document_crumb is None:\n            child_validator.document_path = self.document_path\n        else:\n            if not isinstance(document_crumb, tuple):\n                document_crumb = (document_crumb,)\n            child_validator.document_path = self.document_path + document_crumb\n\n        if schema_crumb is None:\n            child_validator.schema_path = self.schema_path\n        else:\n            if not isinstance(schema_crumb, tuple):\n                schema_crumb = (schema_crumb,)\n            child_validator.schema_path = self.schema_path + schema_crumb\n\n        return child_validator", "response": "Creates a new instance of Validator - ( sub - class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving unknown entries from the mapping.", "response": "def _normalize_purge_unknown(mapping, schema):\n        \"\"\" {'type': 'boolean'} \"\"\"\n        for field in [x for x in mapping if x not in schema]:\n            mapping.pop(field)\n        return mapping"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __validate_definitions(self, definitions, field):\n\n        def validate_rule(rule):\n            validator = self.__get_rule_handler('validate', rule)\n            return validator(definitions.get(rule, None), field, value)\n\n        definitions = self._resolve_rules_set(definitions)\n        value = self.document[field]\n\n        rules_queue = [\n            x\n            for x in self.priority_validations\n            if x in definitions or x in self.mandatory_validations\n        ]\n        rules_queue.extend(\n            x for x in self.mandatory_validations if x not in rules_queue\n        )\n        rules_queue.extend(\n            x\n            for x in definitions\n            if x not in rules_queue\n            and x not in self.normalization_rules\n            and x not in ('allow_unknown', 'require_all', 'meta', 'required')\n        )\n        self._remaining_rules = rules_queue\n\n        while self._remaining_rules:\n            rule = self._remaining_rules.pop(0)\n            try:\n                result = validate_rule(rule)\n                # TODO remove on next breaking release\n                if result:\n                    break\n            except _SchemaRuleTypeError:\n                break\n\n        self._drop_remaining_rules()", "response": "Validate a field s value against its defined rules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate a check_with rule.", "response": "def _validate_check_with(self, checks, field, value):\n        \"\"\" {'oneof': [\n                {'type': 'callable'},\n                {'type': 'list',\n                 'schema': {'oneof': [{'type': 'callable'},\n                                      {'type': 'string'}]}},\n                {'type': 'string'}\n                ]} \"\"\"\n        if isinstance(checks, _str_type):\n            try:\n                value_checker = self.__get_rule_handler('check_with', checks)\n            # TODO remove on next major release\n            except RuntimeError:\n                value_checker = self.__get_rule_handler('validator', checks)\n                warn(\n                    \"The 'validator' rule was renamed to 'check_with'. Please update \"\n                    \"your schema and method names accordingly.\",\n                    DeprecationWarning,\n                )\n            value_checker(field, value)\n        elif isinstance(checks, Iterable):\n            for v in checks:\n                self._validate_check_with(v, field, value)\n        else:\n            checks(field, value, self._error)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_contains(self, expected_values, field, value):\n        if not isinstance(value, Iterable):\n            return\n\n        if not isinstance(expected_values, Iterable) or isinstance(\n            expected_values, _str_type\n        ):\n            expected_values = set((expected_values,))\n        else:\n            expected_values = set(expected_values)\n\n        missing_values = expected_values - set(value)\n        if missing_values:\n            self._error(field, errors.MISSING_MEMBERS, missing_values)", "response": "Validate that the value of the key is contained in the expected_values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_dependencies(self, dependencies, field, value):\n        if isinstance(dependencies, _str_type) or not isinstance(\n            dependencies, (Iterable, Mapping)\n        ):\n            dependencies = (dependencies,)\n\n        if isinstance(dependencies, Sequence):\n            self.__validate_dependencies_sequence(dependencies, field)\n        elif isinstance(dependencies, Mapping):\n            self.__validate_dependencies_mapping(dependencies, field)\n\n        if (\n            self.document_error_tree.fetch_node_from(\n                self.schema_path + (field, 'dependencies')\n            )\n            is not None\n        ):\n            return True", "response": "Validate that the dependencies field is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _validate_items(self, items, field, values):\n        if len(items) != len(values):\n            self._error(field, errors.ITEMS_LENGTH, len(items), len(values))\n        else:\n            schema = dict(\n                (i, definition) for i, definition in enumerate(items)\n            )  # noqa: E501\n            validator = self._get_child_validator(\n                document_crumb=field,\n                schema_crumb=(field, 'items'),  # noqa: E501\n                schema=schema,\n            )\n            if not validator(\n                dict((i, value) for i, value in enumerate(values)),\n                update=self.update,\n                normalize=False,\n            ):\n                self._error(field, errors.BAD_ITEMS, validator._errors)", "response": "{'type': 'list', 'check_with': 'items'}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that the value is not None and that the field is nullable.", "response": "def _validate_nullable(self, nullable, field, value):\n        \"\"\" {'type': 'boolean'} \"\"\"\n        if value is None:\n            if not nullable:\n                self._error(field, errors.NOT_NULLABLE)\n            self._drop_remaining_rules(\n                'allowed',\n                'empty',\n                'forbidden',\n                'items',\n                'keysrules',\n                'min',\n                'max',\n                'minlength',\n                'maxlength',\n                'regex',\n                'schema',\n                'type',\n                'valuesrules',\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a mapping to a set of all the items in the sequence that are semantically equal.", "response": "def mapping_to_frozenset(mapping):\n    \"\"\" Be aware that this treats any sequence type with the equal members as\n        equal. As it is used to identify equality of schemas, this can be\n        considered okay as definitions are semantically equal regardless the\n        container type. \"\"\"\n\n    aggregation = {}\n\n    for key, value in mapping.items():\n        if isinstance(value, Mapping):\n            aggregation[key] = mapping_to_frozenset(value)\n        elif isinstance(value, Sequence):\n            value = list(value)\n            for i, item in enumerate(value):\n                if isinstance(item, Mapping):\n                    value[i] = mapping_to_frozenset(item)\n            aggregation[key] = tuple(value)\n        elif isinstance(value, Set):\n            aggregation[key] = frozenset(value)\n        else:\n            aggregation[key] = value\n\n    return frozenset(aggregation.items())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_user_from_cookie(cookies, app_id, app_secret):\n    cookie = cookies.get(\"fbsr_\" + app_id, \"\")\n    if not cookie:\n        return None\n    parsed_request = parse_signed_request(cookie, app_secret)\n    if not parsed_request:\n        return None\n    try:\n        result = GraphAPI().get_access_token_from_code(\n            parsed_request[\"code\"], \"\", app_id, app_secret\n        )\n    except GraphAPIError:\n        return None\n    result[\"uid\"] = parsed_request[\"user_id\"]\n    return result", "response": "Parses the cookie set by Facebook JavaScript SDK and returns a user object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn dictionary with signed request data.", "response": "def parse_signed_request(signed_request, app_secret):\n    \"\"\" Return dictionary with signed request data.\n\n    We return a dictionary containing the information in the\n    signed_request. This includes a user_id if the user has authorised\n    your application, as well as any information requested.\n\n    If the signed_request is malformed or corrupted, False is returned.\n\n    \"\"\"\n    try:\n        encoded_sig, payload = map(str, signed_request.split(\".\", 1))\n\n        sig = base64.urlsafe_b64decode(\n            encoded_sig + \"=\" * ((4 - len(encoded_sig) % 4) % 4)\n        )\n        data = base64.urlsafe_b64decode(\n            payload + \"=\" * ((4 - len(payload) % 4) % 4)\n        )\n    except IndexError:\n        # Signed request was malformed.\n        return False\n    except TypeError:\n        # Signed request had a corrupted payload.\n        return False\n    except binascii.Error:\n        # Signed request had a corrupted payload.\n        return False\n\n    data = json.loads(data.decode(\"ascii\"))\n    if data.get(\"algorithm\", \"\").upper() != \"HMAC-SHA256\":\n        return False\n\n    # HMAC can only handle ascii (byte) strings\n    # https://bugs.python.org/issue5285\n    app_secret = app_secret.encode(\"ascii\")\n    payload = payload.encode(\"ascii\")\n\n    expected_sig = hmac.new(\n        app_secret, msg=payload, digestmod=hashlib.sha256\n    ).digest()\n    if sig != expected_sig:\n        return False\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching the permissions object from the graph.", "response": "def get_permissions(self, user_id):\n        \"\"\"Fetches the permissions object from the graph.\"\"\"\n        response = self.request(\n            \"{0}/{1}/permissions\".format(self.version, user_id), {}\n        )[\"data\"]\n        return {x[\"permission\"] for x in response if x[\"status\"] == \"granted\"}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_object(self, id, **args):\n        return self.request(\"{0}/{1}\".format(self.version, id), args)", "response": "Fetches the given object from the graph."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_objects(self, ids, **args):\n        args[\"ids\"] = \",\".join(ids)\n        return self.request(self.version + \"/\", args)", "response": "Fetches all of the given objects from the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches for a specific item in the database.", "response": "def search(self, type, **args):\n        \"\"\"https://developers.facebook.com/docs/places/search\"\"\"\n        if type not in VALID_SEARCH_TYPES:\n            raise GraphAPIError(\n                \"Valid types are: %s\" % \", \".join(VALID_SEARCH_TYPES)\n            )\n\n        args[\"type\"] = type\n        return self.request(self.version + \"/search/\", args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch the connections for the given object.", "response": "def get_connections(self, id, connection_name, **args):\n        \"\"\"Fetches the connections for given object.\"\"\"\n        return self.request(\n            \"{0}/{1}/{2}\".format(self.version, id, connection_name), args\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all pages from a get_connections call", "response": "def get_all_connections(self, id, connection_name, **args):\n        \"\"\"Get all pages from a get_connections call\n\n        This will iterate over all pages returned by a get_connections call\n        and yield the individual items.\n        \"\"\"\n        while True:\n            page = self.get_connections(id, connection_name, **args)\n            for post in page[\"data\"]:\n                yield post\n            next = page.get(\"paging\", {}).get(\"next\")\n            if not next:\n                return\n            args = parse_qs(urlparse(next).query)\n            del args[\"access_token\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put_object(self, parent_object, connection_name, **data):\n        assert self.access_token, \"Write operations require an access token\"\n        return self.request(\n            \"{0}/{1}/{2}\".format(self.version, parent_object, connection_name),\n            post_args=data,\n            method=\"POST\",\n        )", "response": "Writes the given object to the graph connected to the given parent object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the object with the given ID from the graph.", "response": "def delete_object(self, id):\n        \"\"\"Deletes the object with the given ID from the graph.\"\"\"\n        return self.request(\n            \"{0}/{1}\".format(self.version, id), method=\"DELETE\"\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_request(self, user_id, request_id):\n        return self.request(\n            \"{0}_{1}\".format(request_id, user_id), method=\"DELETE\"\n        )", "response": "Deletes the Request with the given ID for the given user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupload an image using multipart form - data.", "response": "def put_photo(self, image, album_path=\"me/photos\", **kwargs):\n        \"\"\"\n        Upload an image using multipart/form-data.\n\n        image - A file object representing the image to be uploaded.\n        album_path - A path representing where the image should be uploaded.\n\n        \"\"\"\n        return self.request(\n            \"{0}/{1}\".format(self.version, album_path),\n            post_args=kwargs,\n            files={\"source\": image},\n            method=\"POST\",\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_version(self):\n        args = {\"access_token\": self.access_token}\n        try:\n            response = self.session.request(\n                \"GET\",\n                FACEBOOK_GRAPH_URL + self.version + \"/me\",\n                params=args,\n                timeout=self.timeout,\n                proxies=self.proxies,\n            )\n        except requests.HTTPError as e:\n            response = json.loads(e.read())\n            raise GraphAPIError(response)\n\n        try:\n            headers = response.headers\n            version = headers[\"facebook-api-version\"].replace(\"v\", \"\")\n            return str(version)\n        except Exception:\n            raise GraphAPIError(\"API version number not available\")", "response": "Fetches the current version number of the Graph API being used."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request(\n        self, path, args=None, post_args=None, files=None, method=None\n    ):\n        \"\"\"Fetches the given path in the Graph API.\n\n        We translate args to a valid query string. If post_args is\n        given, we send a POST request to the given path with the given\n        arguments.\n\n        \"\"\"\n        if args is None:\n            args = dict()\n        if post_args is not None:\n            method = \"POST\"\n\n        # Add `access_token` to post_args or args if it has not already been\n        # included.\n        if self.access_token:\n            # If post_args exists, we assume that args either does not exists\n            # or it does not need `access_token`.\n            if post_args and \"access_token\" not in post_args:\n                post_args[\"access_token\"] = self.access_token\n            elif \"access_token\" not in args:\n                args[\"access_token\"] = self.access_token\n\n        try:\n            response = self.session.request(\n                method or \"GET\",\n                FACEBOOK_GRAPH_URL + path,\n                timeout=self.timeout,\n                params=args,\n                data=post_args,\n                proxies=self.proxies,\n                files=files,\n            )\n        except requests.HTTPError as e:\n            response = json.loads(e.read())\n            raise GraphAPIError(response)\n\n        headers = response.headers\n        if \"json\" in headers[\"content-type\"]:\n            result = response.json()\n        elif \"image/\" in headers[\"content-type\"]:\n            mimetype = headers[\"content-type\"]\n            result = {\n                \"data\": response.content,\n                \"mime-type\": mimetype,\n                \"url\": response.url,\n            }\n        elif \"access_token\" in parse_qs(response.text):\n            query_str = parse_qs(response.text)\n            if \"access_token\" in query_str:\n                result = {\"access_token\": query_str[\"access_token\"][0]}\n                if \"expires\" in query_str:\n                    result[\"expires\"] = query_str[\"expires\"][0]\n            else:\n                raise GraphAPIError(response.json())\n        else:\n            raise GraphAPIError(\"Maintype was not text, image, or querystring\")\n\n        if result and isinstance(result, dict) and result.get(\"error\"):\n            raise GraphAPIError(result)\n        return result", "response": "Fetches the given path in the Graph API."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the application s access token as a string.", "response": "def get_app_access_token(self, app_id, app_secret, offline=False):\n        \"\"\"\n        Get the application's access token as a string.\n        If offline=True, use the concatenated app ID and secret\n        instead of making an API call.\n        <https://developers.facebook.com/docs/facebook-login/\n        access-tokens#apptokens>\n        \"\"\"\n        if offline:\n            return \"{0}|{1}\".format(app_id, app_secret)\n        else:\n            args = {\n                \"grant_type\": \"client_credentials\",\n                \"client_id\": app_id,\n                \"client_secret\": app_secret,\n            }\n\n            return self.request(\n                \"{0}/oauth/access_token\".format(self.version), args=args\n            )[\"access_token\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_access_token_from_code(\n        self, code, redirect_uri, app_id, app_secret\n    ):\n        \"\"\"Get an access token from the \"code\" returned from an OAuth dialog.\n\n        Returns a dict containing the user-specific access token and its\n        expiration date (if applicable).\n\n        \"\"\"\n        args = {\n            \"code\": code,\n            \"redirect_uri\": redirect_uri,\n            \"client_id\": app_id,\n            \"client_secret\": app_secret,\n        }\n\n        return self.request(\n            \"{0}/oauth/access_token\".format(self.version), args\n        )", "response": "Get an access token from the code returned from an OAuth dialog."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextending the expiration time of a valid OAuth access token.", "response": "def extend_access_token(self, app_id, app_secret):\n        \"\"\"\n        Extends the expiration time of a valid OAuth access token. See\n        <https://developers.facebook.com/docs/facebook-login/access-tokens/\n        expiration-and-extension>\n\n        \"\"\"\n        args = {\n            \"client_id\": app_id,\n            \"client_secret\": app_secret,\n            \"grant_type\": \"fb_exchange_token\",\n            \"fb_exchange_token\": self.access_token,\n        }\n\n        return self.request(\n            \"{0}/oauth/access_token\".format(self.version), args=args\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef debug_access_token(self, token, app_id, app_secret):\n        args = {\n            \"input_token\": token,\n            \"access_token\": \"{0}|{1}\".format(app_id, app_secret),\n        }\n        return self.request(self.version + \"/\" + \"debug_token\", args=args)", "response": "Get information about a user access token issued by an app."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a URL to create an OAuth dialog.", "response": "def get_auth_url(self, app_id, canvas_url, perms=None, **kwargs):\n        \"\"\"Build a URL to create an OAuth dialog.\"\"\"\n        url = \"{0}{1}/{2}\".format(\n            FACEBOOK_WWW_URL, self.version, FACEBOOK_OAUTH_DIALOG_PATH\n        )\n\n        args = {\"client_id\": app_id, \"redirect_uri\": canvas_url}\n        if perms:\n            args[\"scope\"] = \",\".join(perms)\n        args.update(kwargs)\n        return url + urlencode(args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_current_user():\n\n    # Set the user in the session dictionary as a global g.user and bail out\n    # of this function early.\n    if session.get(\"user\"):\n        g.user = session.get(\"user\")\n        return\n\n    # Attempt to get the short term access token for the current user.\n    result = get_user_from_cookie(\n        cookies=request.cookies, app_id=FB_APP_ID, app_secret=FB_APP_SECRET\n    )\n\n    # If there is no result, we assume the user is not logged in.\n    if result:\n        # Check to see if this user is already in our database.\n        user = User.query.filter(User.id == result[\"uid\"]).first()\n\n        if not user:\n            # Not an existing user so get info\n            graph = GraphAPI(result[\"access_token\"])\n            profile = graph.get_object(\"me\")\n            if \"link\" not in profile:\n                profile[\"link\"] = \"\"\n\n            # Create the user and insert it into the database\n            user = User(\n                id=str(profile[\"id\"]),\n                name=profile[\"name\"],\n                profile_url=profile[\"link\"],\n                access_token=result[\"access_token\"],\n            )\n            db.session.add(user)\n        elif user.access_token != result[\"access_token\"]:\n            # If an existing user, update the access token\n            user.access_token = result[\"access_token\"]\n\n        # Add the user to the current session\n        session[\"user\"] = dict(\n            name=user.name,\n            profile_url=user.profile_url,\n            id=user.id,\n            access_token=user.access_token,\n        )\n\n    # Commit changes to the database and set the user as a global g.user\n    db.session.commit()\n    g.user = session.get(\"user\", None)", "response": "Get the currently logged in user and store it in the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding scene tree from images", "response": "def build_tree(self, directory):\r\n        '''build scene tree from images'''\r\n        confile = os.path.join(directory, 'config.yml')\r\n        conf = {}\r\n        if os.path.exists(confile):\r\n            conf = yaml.load(open(confile).read())\r\n\r\n        class node(defaultdict):\r\n            name = ''\r\n            parent = None\r\n            tmpl = None\r\n            rect = None\r\n            mask = None\r\n\r\n            def __str__(self):\r\n                obj = self\r\n                names = []\r\n                while obj.parent is not None:\r\n                    names.append(obj.name)\r\n                    obj = obj.parent\r\n                return '-'.join(names[::-1])\r\n\r\n        def tree():\r\n            return node(tree)\r\n\r\n        root = tree()\r\n        for s in os.listdir(directory):\r\n            if not s.endswith('.png') or s.endswith('_mask.png'): \r\n                continue\r\n            obj = root\r\n            for i in s[:-4].split('-'):\r\n                obj[i].name = i\r\n                obj[i].parent = obj\r\n                obj = obj[i]\r\n            obj.tmpl = cv2.imread(os.path.join(directory, s))\r\n            obj.rect = conf.get(s[:-4], {}).get('rect')\r\n            maskimg = conf.get(s[:-4], {}).get('mask')\r\n            if maskimg is not None:\r\n                maskimg = os.path.join(directory, maskimg)\r\n                if os.path.exists(maskimg):\r\n                    obj.mask = cv2.imread(maskimg)\r\n\r\n        self.tree = root\r\n        self.current_scene = []\r\n        self.confile = confile\r\n        self.conf = conf"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npacking request to delimited data", "response": "def pack(mtype, request, rid=None):\n    '''pack request to delimited data'''\n    envelope = wire.Envelope() \n    if rid is not None:\n        envelope.id = rid\n    envelope.type = mtype\n    envelope.message = request.SerializeToString()\n    data = envelope.SerializeToString()\n    data = encoder._VarintBytes(len(data)) + data\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nunpacks from delimited data", "response": "def unpack(data):\n    '''unpack from delimited data'''\n    size, position = decoder._DecodeVarint(data, 0)\n    envelope = wire.Envelope() \n    envelope.ParseFromString(data[position:position+size])\n    return envelope"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if agent is alive.", "response": "def check_stf_agent(adbprefix=None, kill=False):\n    '''return True if agent is alive.'''\n    if adbprefix is None: \n        adbprefix = ['adb']\n    command = adbprefix + ['shell', 'ps']\n    out = subprocess.check_output(command).strip()\n    out = out.splitlines()\n    if len(out) > 1:\n        first, out = out[0], out[1:]\n        idx = first.split().index('PID')\n        pid = None\n        for line in out:\n            if 'stf.agent' in line:\n                pid = line.split()[idx]\n                print 'stf.agent is running, pid is', pid\n                break\n        if pid is not None:\n            if kill:\n                print 'killing', pid\n                command = adbprefix + ['shell', 'kill', '-9', pid]\n                subprocess.call(command)\n                return False\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninsert code into the file filename.", "response": "def insert_code(filename, code, save=True, marker='# ATX CODE END'):\n    \"\"\" Auto append code \"\"\"\n    content = ''\n    found = False\n    for line in open(filename, 'rb'):\n        if not found and line.strip() == marker:\n            found = True\n            cnt = line.find(marker)\n            content += line[:cnt] + code\n        content += line\n    if not found:\n        if not content.endswith('\\n'):\n            content += '\\n'\n        content += code + marker + '\\n'\n    if save:\n        with open(filename, 'wb') as f:\n            f.write(content)\n    return content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the items for the current locale.", "response": "def _init_items(self):\n        \"\"\"\n        .---------------.\n        | Ctrl | Screen |\n        |------|        |\n        | Code |        |\n        |      |        |\n        \"\"\"\n        root = self._root\n        root.resizable(0, 0)\n\n        frm_control = tk.Frame(root, bg='#bbb')\n        frm_control.grid(column=0, row=0, padx=5, sticky=tk.NW)\n        frm_screen = tk.Frame(root, bg='#aaa')\n        frm_screen.grid(column=1, row=0)\n\n        frm_screenshot = tk.Frame(frm_control)\n        frm_screenshot.grid(column=0, row=0, sticky=tk.W)\n        tk.Label(frm_control, text='-'*30).grid(column=0, row=1, sticky=tk.EW)\n        frm_code = tk.Frame(frm_control)\n        frm_code.grid(column=0, row=2, sticky=tk.EW)\n\n        self._btn_refresh = tk.Button(frm_screenshot, textvariable=self._refresh_text, command=self._refresh_screen)\n        self._btn_refresh.grid(column=0, row=0, sticky=tk.W)\n        # tk.Button(frm_screenshot, text=\"Wakeup\", command=self._device.wakeup).grid(column=0, row=1, sticky=tk.W)\n        tk.Button(frm_screenshot, text=u\"\u4fdd\u5b58\u9009\u4e2d\u533a\u57df\", command=self._save_crop).grid(column=0, row=1, sticky=tk.W)\n        \n        # tk.Button(frm_screenshot, text=\"\u4fdd\u5b58\u622a\u5c4f\", command=self._save_screenshot).grid(column=0, row=2, sticky=tk.W)\n        frm_checkbtns = tk.Frame(frm_screenshot)\n        frm_checkbtns.grid(column=0, row=3, sticky=(tk.W, tk.E))\n        tk.Checkbutton(frm_checkbtns, text=\"Auto refresh\", variable=self._auto_refresh_var, command=self._run_check_refresh).grid(column=0, row=0, sticky=tk.W)\n        tk.Checkbutton(frm_checkbtns, text=\"UI detect\", variable=self._uiauto_detect_var).grid(column=1, row=0, sticky=tk.W)\n\n        frm_code_editor = tk.Frame(frm_code)\n        frm_code_editor.grid(column=0, row=0, sticky=(tk.W, tk.E))\n        tk.Label(frm_code_editor, text='Generated code').grid(column=0, row=0, sticky=tk.W)\n        tk.Entry(frm_code_editor, textvariable=self._gencode_text, width=30).grid(column=0, row=1, sticky=tk.W)\n        tk.Label(frm_code_editor, text='Save file name').grid(column=0, row=2, sticky=tk.W)\n        tk.Entry(frm_code_editor, textvariable=self._genfile_name, width=30).grid(column=0, row=3, sticky=tk.W)\n        tk.Label(frm_code_editor, text='Extention name').grid(column=0, row=4, sticky=tk.W)\n        tk.Entry(frm_code_editor, textvariable=self._fileext_text, width=30).grid(column=0, row=5, sticky=tk.W)\n        \n        frm_code_btns = tk.Frame(frm_code)\n        frm_code_btns.grid(column=0, row=2, sticky=(tk.W, tk.E))\n        tk.Button(frm_code_btns, text='Run', command=self._run_code).grid(column=0, row=0, sticky=tk.W)\n        self._btn_runedit = tk.Button(frm_code_btns, state=tk.DISABLED, text='Insert and Run', command=self._run_and_insert)\n        self._btn_runedit.grid(column=1, row=0, sticky=tk.W)\n        tk.Button(frm_code, text='Select File', command=self._run_selectfile).grid(column=0, row=4, sticky=tk.W)\n        tk.Label(frm_code, textvariable=self._attachfile_text).grid(column=0, row=5, sticky=tk.W)\n        tk.Button(frm_code, text='Reset', command=self._reset).grid(column=0, row=6, sticky=tk.W)\n\n        self.canvas = tk.Canvas(frm_screen, bg=\"blue\", bd=0, highlightthickness=0, relief='ridge')\n        self.canvas.grid(column=0, row=0, padx=10, pady=10)\n        self.canvas.bind(\"<Button-1>\", self._stroke_start)\n        self.canvas.bind(\"<B1-Motion>\", self._stroke_move)\n        self.canvas.bind(\"<B1-ButtonRelease>\", self._stroke_done)\n        self.canvas.bind(\"<Motion>\", self._mouse_move)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_image_position(origin='origin.png', query='query.png', outfile=None):\n    '''\n    find all image positions\n    @return None if not found else a tuple: (origin.shape, query.shape, postions)\n    might raise Exception\n    '''\n    img1 = cv2.imread(query, 0) # query image(small)\n    img2 = cv2.imread(origin, 0) # train image(big)\n\n    # Initiate SIFT detector\n    sift = cv2.SIFT()\n\n    # find the keypoints and descriptors with SIFT\n    kp1, des1 = sift.detectAndCompute(img1,None)\n    kp2, des2 = sift.detectAndCompute(img2,None)\n    print len(kp1), len(kp2)\n\n    FLANN_INDEX_KDTREE = 0\n    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n    search_params = dict(checks = 50)\n\n    # flann\n    flann = cv2.FlannBasedMatcher(index_params, search_params)\n    matches = flann.knnMatch(des1, des2, k=2)\n\n    # store all the good matches as per Lowe's ratio test.\n    good = []\n    for m,n in matches:\n        if m.distance < 0.7*n.distance:\n            good.append(m)\n    print len(kp1), len(kp2), 'good cnt:', len(good)\n\n    if len(good)*1.0/len(kp1) < 0.5:\n    #if len(good)<MIN_MATCH_COUNT:\n        print \"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT)\n        return img2.shape, img1.shape, []\n\n    queryPts = []\n    trainPts = []\n    for dm in good:\n        queryPts.append(kp1[dm.queryIdx])\n        trainPts.append(kp2[dm.trainIdx])\n\n    img3 = cv2.drawKeypoints(img1, queryPts)\n    cv2.imwrite('image/query.png', img3)\n\n    img3 = cv2.drawKeypoints(img2, trainPts)\n    point = _middlePoint(trainPts)\n    print 'position in', point\n\n    if outfile:\n        edge = 10\n        top_left = (point[0]-edge, point[1]-edge)\n        bottom_right = (point[0]+edge, point[1]+edge)\n        cv2.rectangle(img3, top_left, bottom_right, 255, 2)\n        cv2.imwrite(outfile, img3)\n    return img2.shape, img1.shape, [point]", "response": "find all image positions in a single language tree"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the name of the main activity", "response": "def main_activity(self):\n        \"\"\"\n        Returns:\n            the name of the main activity\n        \"\"\"\n        x = set()\n        y = set()\n        for item in self._dom.getElementsByTagName(\"activity\"):\n            for sitem in item.getElementsByTagName(\"action\"):\n                val = sitem.getAttribute(\"android:name\")\n                if val == \"android.intent.action.MAIN\":\n                    x.add(item.getAttribute(\"android:name\"))\n            for sitem in item.getElementsByTagName(\"category\"):\n                val = sitem.getAttribute(\"android:name\")\n                if val == \"android.intent.category.LAUNCHER\":\n                    y.add(item.getAttribute(\"android:name\"))\n        z = x.intersection(y)\n        if len(z) > 0:\n            return z.pop()\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef http_download(url, target_path):\n    r = requests.get(url, stream=True)\n    with open(target_path, 'wb') as f:\n        # shutil.copyfileobj(resp, f)\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n    return target_path", "response": "Download file from url to target_path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef radang(x, y):\r\n    '''return (radius, angle) of a vector(x, y)'''\r\n    if x == 0:\r\n        if y == 0:\r\n            return 0, 0\r\n        return abs(y), 90+180*(y<0)\r\n    if y == 0:\r\n        return abs(x), 180*(x<0)\r\n\r\n    r = math.sqrt(x*x+y*y)\r\n    a = math.degrees(math.atan(y/x))\r\n    if x < 0:\r\n        a += 180\r\n    elif y < 0:\r\n        a += 360\r\n    return r, a", "response": "returns ( radius angle ) of a vector x y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_touch_batch(self):\r\n        '''a batch syncs in about 0.001 seconds.'''\r\n        if not self._touch_batch:\r\n            return\r\n\r\n        _time = self._temp_status_time\r\n        changed = False\r\n\r\n        for (_time, _device, _type, _code, _value) in self._touch_batch:\r\n            if _code == 'ABS_MT_TRACKING_ID':\r\n                if _value == 0xffffffff:\r\n                    self._temp_status[self._curr_slot] = -INF\r\n                    changed = True\r\n                else:\r\n                    pass\r\n            elif _code == 'ABS_MT_SLOT':\r\n                self._curr_slot = _value\r\n            else:\r\n                if _code == 'ABS_MT_POSITION_X':\r\n                    self._temp_status[self._curr_slot,_X] = _value\r\n                    changed = True\r\n                elif _code == 'ABS_MT_POSITION_Y':\r\n                    self._temp_status[self._curr_slot,_Y] = _value\r\n                    changed = True\r\n                elif _code == 'ABS_MT_PRESSURE':\r\n                    self._temp_status[self._curr_slot,_PR] = _value\r\n                elif _code == 'ABS_MT_TOUCH_MAJOR':\r\n                    self._temp_status[self._curr_slot,_MJ] = _value\r\n                else:\r\n                    print 'Unknown code', _code\r\n\r\n        self._temp_status_time = _time\r\n        self._touch_batch = []\r\n        if not changed:\r\n            return\r\n\r\n        # check differences, if position changes are big enough then emit events\r\n        diff = self._temp_status - self._status\r\n        dt = self._temp_status_time - self._status_time\r\n        emitted = False\r\n        for i in range(SLOT_NUM):\r\n            arr = self._temp_status[i]\r\n            oldarr = self._status[i]\r\n            dx, dy = diff[i,_X], diff[i,_Y]\r\n            if dx > INF or dy > INF:\r\n                # touch begin\r\n                event = TouchEvent(_time, HC.TOUCH_DOWN, i, arr[_X], arr[_Y], arr[_PR], arr[_MJ])\r\n                self.emit_touch_event(event)\r\n                emitted = True\r\n            elif dx < -INF or dy < -INF:\r\n                # touch end\r\n                event = TouchEvent(_time, HC.TOUCH_UP, i, oldarr[_X], oldarr[_Y], oldarr[_PR], oldarr[_MJ])\r\n                self.emit_touch_event(event)\r\n                emitted = True\r\n            else:\r\n                r, a = radang(float(dx), float(dy))\r\n                if r > self._move_radius:\r\n                    v = r / dt\r\n                    event = TouchEvent(_time, HC.TOUCH_MOVE, i, arr[_X], arr[_Y], arr[_PR], arr[_MJ], angle=a, velocity=v)\r\n                    self.emit_touch_event(event)\r\n                    emitted = True\r\n\r\n        if not emitted:\r\n            return\r\n        self._status = self._temp_status.copy()\r\n        self._status_time = self._temp_status_time", "response": "a batch syncs in about 0.001 seconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess events and trigger time - related events", "response": "def process(self):\r\n        '''handle events and trigger time-related events'''\r\n        timediff = 0\r\n        while True:\r\n            try:\r\n                time.sleep(0.001)\r\n                event = self.queue.get_nowait()\r\n                self.handle_event(event)\r\n                if event.msg & HC.KEY_ANY:\r\n                    continue\r\n                if timediff == 0:\r\n                    timediff = time.time() - event.time\r\n                self.touches[event.slotid] = event\r\n            except Queue.Empty:\r\n                if not self.running:\r\n                    break\r\n                now = time.time() - timediff\r\n                for i in range(SLOT_NUM):\r\n                    e = self.touches[i]\r\n                    if e is None:\r\n                        continue\r\n                    if e.msg == HC.TOUCH_DOWN and now - e.time > self.long_press_delay:\r\n                        self.analyze_tracks(TouchTimeoutEvent(now, HC.TOUCH_PRESS_TIMEOUT, i))\r\n                        self.touches[i] = None\r\n                    elif e.msg == HC.TOUCH_UP and now - e.time > self.double_tap_delay:\r\n                        self.analyze_tracks(TouchTimeoutEvent(now, HC.TOUCH_FOLLOW_TIMEOUT, i))\r\n                        self.touches[i] = None\r\n                    elif e.msg == HC.TOUCH_MOVE and now - e.time > self.move_stop_delay:\r\n                        self.analyze_tracks(TouchTimeoutEvent(now, HC.TOUCH_MOVESTOP_TIMEOUT, i))\r\n                        self.touches[i] = None\r\n            except:\r\n                traceback.print_exc()\r\n\r\n        print 'process done.'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _run_nowait(self, code):\n        ''' TODO: change to no wait '''\n        print self._proc.poll()\n        encoded_code = json.dumps({'command': code, 'nowait': True})\n        output = subprocess.check_output([self._bootstrap, 'run', '--nowait', encoded_code], env=self._env)\n        return output", "response": "run code in a non - blocking mode"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake ios screenshot - Image object filename - string name of screenshot file", "response": "def screenshot(self, filename=None):\n        '''\n        Take ios screenshot\n        Args:\n            - filename(string): optional\n        Returns:\n            PIL.Image object\n        '''\n        image = self.d.screenshot()\n        if self.rotation:\n            method = getattr(Image, 'ROTATE_{}'.format(self.rotation*90))\n            image = image.transpose(method)\n        if filename:\n            image.save(filename)\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef click(self, x, y):\n        '''\n        Simulate click operation\n        Args:\n            - x (int): position of x\n            - y (int): position of y\n        Returns:\n            self\n        '''\n        self._run_nowait('target.tap({x: %d, y: %d})' % (x/self._scale, y/self._scale))\n        return self", "response": "Simulate click operation\n        Args:\n            - x (int): position of x\n            - y (int): position of y\n        Returns:\n            self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef screen(self):\r\n        # opengl windows cannot get from it's hwnd, so we use the screen\r\n        hwnd = win32gui.GetDesktopWindow()\r\n\r\n        # get window size and offset\r\n        left, top, right, bottom = self.rect\r\n        width, height = right-left, bottom-top\r\n\r\n        # the device context of the window \r\n        hdcwin = win32gui.GetWindowDC(hwnd)\r\n        # make a temporary dc\r\n        hdcmem = win32gui.CreateCompatibleDC(hdcwin)\r\n        # make a temporary bitmap in memory, this is a PyHANDLE object\r\n        hbmp = win32gui.CreateCompatibleBitmap(hdcwin, width, height)\r\n        # select bitmap for temporary dc\r\n        win32gui.SelectObject(hdcmem, hbmp)\r\n        # copy bits to temporary dc\r\n        win32gui.BitBlt(hdcmem, 0, 0, width, height, \r\n                        hdcwin, left, top, win32con.SRCCOPY)\r\n        # check the bitmap object infomation\r\n        bmp = win32gui.GetObject(hbmp)\r\n\r\n        bi = BITMAPINFOHEADER()\r\n        bi.biSize = ctypes.sizeof(BITMAPINFOHEADER)\r\n        bi.biWidth = bmp.bmWidth\r\n        bi.biHeight = bmp.bmHeight\r\n        bi.biPlanes = bmp.bmPlanes\r\n        bi.biBitCount = bmp.bmBitsPixel\r\n        bi.biCompression = 0 # BI_RGB\r\n        bi.biSizeImage = 0\r\n        bi.biXPelsPerMeter = 0\r\n        bi.biYPelsPerMeter = 0\r\n        bi.biClrUsed = 0\r\n        bi.biClrImportant = 0\r\n\r\n        # calculate total size for bits\r\n        pixel = bmp.bmBitsPixel\r\n        size = ((bmp.bmWidth * pixel + pixel - 1)/pixel) * 4 * bmp.bmHeight\r\n        buf = (ctypes.c_char * size)()\r\n\r\n        # read bits into buffer\r\n        windll.gdi32.GetDIBits(hdcmem, hbmp.handle, 0, bmp.bmHeight, buf, ctypes.byref(bi), win32con.DIB_RGB_COLORS)\r\n\r\n        # make a PIL Image\r\n        img = Image.frombuffer('RGB', (bmp.bmWidth, bmp.bmHeight), buf, 'raw', 'BGRX', 0, 1)\r\n        img = img.transpose(Image.FLIP_TOP_BOTTOM)\r\n\r\n        # cleanup\r\n        win32gui.DeleteObject(hbmp)\r\n        win32gui.DeleteObject(hdcmem)\r\n        win32gui.ReleaseDC(hwnd, hdcwin)\r\n\r\n        return img", "response": "Create a new screen with the same size and offset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef screen(self):\r\n        hwnd = win32gui.GetDesktopWindow()\r\n        left, top, right, bottom = self.rect\r\n        width, height = right-left, bottom-top\r\n        # copy bits to temporary dc\r\n        win32gui.BitBlt(self._hdcmem, 0, 0, width, height, \r\n                        self._hdcwin, left, top, win32con.SRCCOPY)\r\n        # read bits into buffer\r\n        windll.gdi32.GetDIBits(self._hdcmem, self._hbmp.handle, 0, height, self._buf, ctypes.byref(self._bi), win32con.DIB_RGB_COLORS)\r\n        # make a PIL Image\r\n        img = Image.frombuffer('RGB', (width, height), self._buf, 'raw', 'BGRX', 0, 1)\r\n        img = img.transpose(Image.FLIP_TOP_BOTTOM)\r\n        return img", "response": "Returns PIL Image of current window screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef screen_cv2(self):\r\n        hwnd = win32gui.GetDesktopWindow()\r\n        left, top, right, bottom = self.rect\r\n        width, height = right-left, bottom-top\r\n        # copy bits to temporary dc\r\n        win32gui.BitBlt(self._hdcmem, 0, 0, width, height, \r\n                        self._hdcwin, left, top, win32con.SRCCOPY)\r\n        # read bits into buffer\r\n        windll.gdi32.GetDIBits(self._hdcmem, self._hbmp.handle, 0, height, self._buf, ctypes.byref(self._bi), win32con.DIB_RGB_COLORS)\r\n        # make a cv2 Image\r\n        arr = np.fromstring(self._buf, dtype=np.uint8)\r\n        img = arr.reshape(height, width, 4)\r\n        img = img[::-1,:, 0:3]\r\n        return img", "response": "Returns cv2 Image of current window screen"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef click(self, x, y):\r\n        print 'click at', x, y\r\n        self._input_left_mouse(x, y)", "response": "Simulate click within window screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef urljoin(*urls):\r\n    return reduce(urlparse.urljoin, [u.strip('/')+'/' for u in urls if u.strip('/')], '').rstrip('/')", "response": "A version of urlparse. urljoin that uses urlparse. urljoin"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef screenshot(self):\r\n        url = urljoin(self.__device_url, \"screenshot\")\r\n        r = httpdo('GET', url)\r\n        raw_image = base64.b64decode(r.value)\r\n        return Image.open(StringIO(raw_image))", "response": "Take screenshot and return PIL. Image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef screenshot(self, filename=None):\r\n        # screen size: 1280x720\r\n        screen = self._ymc.screenshot()\r\n        if filename:\r\n            screen.save(filename)\r\n        return screen", "response": "Take a screenshot of the current process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect(*args, **kwargs):\n    connect_url = _connect_url(*args)\n    platform = kwargs.pop('platform', _detect_platform(connect_url))\n\n    cls = None\n    if platform == 'android':\n        os.environ['JSONRPC_TIMEOUT'] = \"60\" # default is 90s which is too long.\n        devcls = __import__('atx.drivers.android')\n        cls = devcls.drivers.android.AndroidDevice\n    elif platform == 'windows':\n        devcls = __import__('atx.drivers.windows')\n        cls = devcls.drivers.windows.WindowsDevice\n    elif platform == 'ios':\n        devcls = __import__('atx.drivers.ios_webdriveragent')\n        cls = devcls.drivers.ios_webdriveragent.IOSDevice\n    elif platform == 'webdriver':\n        devcls = __import__('atx.drivers.webdriver')\n        cls = devcls.drivers.webdriver.WebDriver\n    elif platform == 'dummy': # for py.test use\n        devcls = __import__('atx.drivers.dummy')\n        cls = devcls.drivers.dummy.DummyDevice\n    \n    if cls is None:\n        raise SyntaxError('Platform: %s not exists' % platform)\n\n    c = cls(connect_url, **kwargs)\n    c.platform = platform\n    return c", "response": "Connect to a device and return its object\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_once(f):\n    ''' \n    Decorator: Make sure function only call once\n    not thread safe\n\n    @run_once\n    def foo():\n        print 'bar'\n        return 1+2\n    foo()\n    foo() # 'bar' only print once\n    '''\n    def wrapper(*args, **kwargs):\n        if not wrapper.has_run:\n            wrapper.result = f(*args, **kwargs)\n            wrapper.has_run = True\n        return wrapper.result\n    wrapper.has_run = False\n    return wrapper", "response": "Decorator that makes sure function only call once"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreferences https://blog.tonyseek.com/post/open-class-in-python/ class Spam(object): pass @attach_method(Spam) def egg1(self, name): print((self, name)) spam1 = Spam() # OpenClass \u52a0\u5165\u7684\u65b9\u6cd5 egg1 \u53ef\u7528 spam1.egg1(\"Test1\") # \u8f93\u51faTest1", "response": "def attachmethod(target):\n    '''\n    Reference: https://blog.tonyseek.com/post/open-class-in-python/\n\n    class Spam(object):\n        pass\n\n    @attach_method(Spam)\n    def egg1(self, name):\n        print((self, name))\n\n    spam1 = Spam()\n    # OpenClass \u52a0\u5165\u7684\u65b9\u6cd5 egg1 \u53ef\u7528\n    spam1.egg1(\"Test1\")\n    # \u8f93\u51faTest1\n    '''\n    if isinstance(target, type):\n        def decorator(func):\n            setattr(target, func.__name__, func)\n    else:\n        def decorator(func):\n            setattr(target, func.__name__, partial(func, target))\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef go(fn):\n    '''\n    Decorator\n    '''\n    def decorator(*args, **kwargs):\n        log.info('begin run func(%s) in background', fn.__name__)\n        t = threading.Thread(target=fn, args=args, kwargs=kwargs)\n        t.setDaemon(True)\n        t.start()\n        return t\n    return decorator", "response": "Decorator to run a function in background"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a unicode string to a unicode string.", "response": "def to_string(s, encoding='utf-8'):\n    \"\"\"\n    Accept unicode(py2) or bytes(py3)\n\n    Returns:\n        py2 type: str\n        py3 type: str\n    \"\"\"\n    if six.PY2:\n        return s.encode(encoding)\n    if isinstance(s, bytes):\n        return s.decode(encoding)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef devices():\n    udids = [udid.strip() for udid in idevice('_id', '-l').splitlines() if udid.strip()]\n    return {udid: idevice('name', '-u', udid).decode('utf-8').strip() for udid in udids}", "response": "Return device dict for all devices"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake screenshot of the current instance of the instance and saves it to a file.", "response": "def screenshot(self, filename=None):\n        \"\"\"\n        Return:\n            PIL.Image\n            \n        Raises:\n            EnvironmentError\n        \"\"\"\n        tmpfile = tempfile.mktemp(prefix='atx-screencap-', suffix='.tiff')\n        try:\n            idevice(\"screenshot\", \"--udid\", self.udid, tmpfile)\n        except subprocess.CalledProcessError as e:\n            sys.exit(e.message)\n\n        try:\n            image = Image.open(tmpfile)\n            image.load()\n            if filename:\n                image.save(filename)\n            return image\n        finally:\n            if os.path.exists(tmpfile):\n                os.unlink(tmpfile)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart app by bundle_id", "response": "def start_app(self, bundle_id):\n        '''\n        Start app by bundle_id\n        Args:\n            - bundle_id(string): ex com.netease.my\n        Returns:\n            idevicedebug subprocess instance\n        '''\n        idevicedebug = must_look_exec('idevicedebug')\n\n        # run in background\n        kwargs = {'stdout': subprocess.PIPE, 'stderr': subprocess.PIPE}\n        if sys.platform != 'darwin':\n            kwargs['close_fds'] = True\n        return subprocess.Popen([idevicedebug, \"--udid\", self.udid, 'run', bundle_id], **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstall the iDevice class from the given file.", "response": "def install(self, filepath):\n        ''' TODO(ssx): not tested. '''\n        if not os.path.exists(filepath):\n            raise EnvironmentError('file \"%s\" not exists.' % filepath)\n\n        ideviceinstaller = must_look_exec('ideviceinstaller')\n        os.system(subprocess.list2cmdline([ideviceinstaller, '-u', self.udid, '-i', filepath]))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndelaying some seconds to get a new instance of the class.", "response": "def delay(self, secs):\n        \"\"\"Delay some seconds\n        Args:\n            secs: float seconds\n\n        Returns:\n            self\n        \"\"\"\n        secs = int(secs)\n        for i in reversed(range(secs)):\n            sys.stdout.write('\\r')\n            sys.stdout.write(\"sleep %ds, left %2ds\" % (secs, i+1))\n            sys.stdout.flush()\n            time.sleep(1)\n        sys.stdout.write(\"\\n\")\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if image exists in screen", "response": "def exists(self, pattern, **match_kwargs):\n        \"\"\"Check if image exists in screen\n\n        Returns:\n            If exists, return FindPoint, or\n            return None if result.confidence < self.image_match_threshold\n        \"\"\"\n        ret = self.match(pattern, **match_kwargs)\n        if ret is None:\n            return None\n        if not ret.matched:\n            return None\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwaits till pattern is found or time is out.", "response": "def wait(self, pattern, timeout=10.0, safe=False, **match_kwargs):\n        \"\"\"Wait till pattern is found or time is out (default: 10s).\"\"\"\n        t = time.time() + timeout\n        while time.time() < t:\n            ret = self.exists(pattern, **match_kwargs)\n            if ret:\n                return ret\n            time.sleep(0.2)\n        if not safe:\n            raise errors.ImageNotFoundError('Not found image %s' %(pattern,))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef click(self, x, y):\n        if x < 1 and y < 1:\n            display = self.display\n            x *= display.width\n            y *= display.height\n        return self.do_tap(x, y)", "response": "Clicks the specified location on the base screen."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _match_auto(self, screen, search_img, threshold):\n        # 1. try template first\n        ret = ac.find_template(screen, search_img)\n        if ret and ret['confidence'] > threshold:\n            return FindPoint(ret['result'], ret['confidence'], consts.IMAGE_MATCH_METHOD_TMPL, matched=True)\n\n        # 2. try sift\n        ret = ac.find_sift(screen, search_img, min_match_count=10)\n        if ret is None:\n            return None\n\n        matches, total = ret['confidence']\n        if 1.0*matches/total > 0.5: # FIXME(ssx): sift just write here\n            return FindPoint(ret['result'], ret['confidence'], consts.IMAGE_MATCH_METHOD_SIFT, matched=True)\n        return None", "response": "Try to find a match for a given image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef match_all(self, pattern):\n        pattern = self.pattern_open(pattern)\n        search_img = pattern.image\n        screen = self.region_screenshot()\n        screen = imutils.from_pillow(screen)\n        points = ac.find_all_template(screen, search_img, maxcnt=10)\n        return points", "response": "Test method not suggested to use\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match(self, pattern, screen=None, rect=None, offset=None, threshold=None, method=None):\n        pattern = self.pattern_open(pattern)\n        search_img = pattern.image\n\n        pattern_scale = self._cal_scale(pattern)\n        if pattern_scale != 1.0:\n            search_img = cv2.resize(search_img, (0, 0), \n                fx=pattern_scale, fy=pattern_scale,\n                interpolation=cv2.INTER_CUBIC)\n        \n        screen = screen or self.region_screenshot()\n        threshold = threshold or pattern.threshold or self.image_match_threshold\n\n        # handle offset if percent, ex (0.2, 0.8)\n        dx, dy = offset or pattern.offset or (0, 0)\n        dx = pattern.image.shape[1] * dx # opencv object width\n        dy = pattern.image.shape[0] * dy # opencv object height\n        dx, dy = int(dx*pattern_scale), int(dy*pattern_scale)\n\n        # image match\n        screen = imutils.from_pillow(screen) # convert to opencv image\n        if rect and isinstance(rect, tuple) and len(rect) == 4:\n            (x0, y0, x1, y1) = [int(v*pattern_scale) for v in rect]\n            (dx, dy) = dx+x0, dy+y0\n            screen = imutils.crop(screen, x0, y0, x1, y1)\n            #cv2.imwrite('cc.png', screen)\n\n        match_method = method or self.image_match_method\n        \n        ret = None\n        confidence = None\n        matched = False\n        position = None\n        if match_method == consts.IMAGE_MATCH_METHOD_TMPL: #IMG_METHOD_TMPL\n            ret = ac.find_template(screen, search_img)\n            if ret is None:\n                return None\n            confidence = ret['confidence']\n            if confidence > threshold:\n                matched = True\n            (x, y) = ret['result']\n            position = (x+dx, y+dy) # fix by offset\n        elif match_method == consts.IMAGE_MATCH_METHOD_SIFT:\n            ret = ac.find_sift(screen, search_img, min_match_count=10)\n            if ret is None:\n                return None\n            confidence = ret['confidence']\n            matches, total = confidence\n            if 1.0*matches/total > 0.5: # FIXME(ssx): sift just write here\n                matched = True\n            (x, y) = ret['result']\n            position = (x+dx, y+dy) # fix by offset\n        elif match_method == consts.IMAGE_MATCH_METHOD_AUTO:\n            fp = self._match_auto(screen, search_img, threshold)\n            if fp is None:\n                return None\n            (x, y) = fp.pos\n            position = (x+dx, y+dy)\n            return FindPoint(position, fp.confidence, fp.method, fp.matched)\n        else:\n            raise TypeError(\"Invalid image match method: %s\" %(match_method,))\n\n        (x, y) = ret['result']\n        position = (x+dx, y+dy) # fix by offset\n        if self.bounds:\n            x, y = position\n            position = (x+self.bounds.left, y+self.bounds.top)\n\n        return FindPoint(position, confidence, match_method, matched=matched)", "response": "Returns True if image position in screen matches pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the region of the screen area", "response": "def region(self, bounds):\n        \"\"\"Set region of the screen area\n        Args:\n            bounds: Bounds object\n\n        Returns:\n            A new AndroidDevice object\n\n        Raises:\n            TypeError\n        \"\"\"\n        if not isinstance(bounds, Bounds):\n            raise TypeError(\"region param bounds must be isinstance of Bounds\")\n        _d = copy.copy(self)\n        _d._bounds = bounds\n        return _d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nkeeping the current screenshot in the screen.", "response": "def keep_screen(self):\n        \"\"\"\n        Freese screenshot, so all image functions will not take images, until call free_screen()\n        \"\"\"\n        self.__last_screen = self.screenshot()\n        self.__keep_screen = True\n        inner_self = self\n\n        class _C(object):\n            def __enter__(self):\n                pass\n\n            def __exit__(self, type, value, traceback):\n                inner_self.free_screen()\n\n        return _C()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking part of the screenshot", "response": "def region_screenshot(self, filename=None):\n        \"\"\"Deprecated\n        Take part of the screenshot\n        \"\"\"\n        # warnings.warn(\"deprecated, use screenshot().crop(bounds) instead\", DeprecationWarning)\n        screen = self.__last_screen if self.__keep_screen else self.screenshot()\n        if self.bounds:\n            screen = screen.crop(self.bounds)\n        if filename:\n            screen.save(filename)\n        return screen"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking a screenshot of the current image and save it to filename.", "response": "def screenshot(self, filename=None):\n        \"\"\"\n        Take screen snapshot\n\n        Args:\n            - filename: filename where save to, optional\n\n        Returns:\n            PIL.Image object\n\n        Raises:\n            TypeError, IOError\n        \"\"\"\n        if self.__keep_screen:\n            return self.__last_screen\n        try:\n            screen = self._take_screenshot()\n        except IOError:\n            # try taks screenshot again\n            log.warn(\"warning, screenshot failed [2/1], retry again\")\n            screen = self._take_screenshot()\n        self.__last_screen = screen\n        if filename:\n            save_dir = os.path.dirname(filename) or '.'\n            if not os.path.exists(save_dir):\n                os.makedirs(save_dir)\n            screen.save(filename)\n        return screen"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef click_nowait(self, pattern, action='click', desc=None, **match_kwargs):\n        point = self.match(pattern, **match_kwargs)\n        if not point or not point.matched:\n            return None\n\n        func = getattr(self, action)\n        func(*point.pos)\n        return point", "response": "Return immediately if no image found with pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef click_exists(self, *args, **kwargs):\n        if len(args) > 0:\n            return self.click_nowait(*args, **kwargs)\n        else:\n            elem = self(**kwargs)\n            if elem.exists:\n                return elem.click()", "response": "Click when target exists\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsimulate click according image position", "response": "def click_image(self, pattern, timeout=20.0, action='click', safe=False, desc=None, delay=None, **match_kwargs):\n        \"\"\"Simulate click according image position\n\n        Args:\n            - pattern (str or Pattern): filename or an opencv image object.\n            - timeout (float): if image not found during this time, ImageNotFoundError will raise.\n            - action (str): click or long_click\n            - safe (bool): if safe is True, Exception will not raise and return None instead.\n            - method (str): image match method, choice of <template|sift>\n            - delay (float): wait for a moment then perform click\n\n        Returns:\n            None\n\n        Raises:\n            ImageNotFoundError: An error occured when img not found in current screen.\n        \"\"\"\n        pattern = self.pattern_open(pattern)\n        log.info('click image:%s %s', desc or '', pattern)\n        start_time = time.time()\n        found = False\n        point = None\n        while time.time() - start_time < timeout:\n            point = self.match(pattern, **match_kwargs)\n            if point is None:\n                sys.stdout.write('.')\n                sys.stdout.flush()\n                continue\n\n            log.debug('confidence: %s', point.confidence)\n            if not point.matched:\n                log.info('Ignore confidence: %s', point.confidence)\n                continue\n            \n            # wait for program ready\n            if delay and delay > 0:\n                self.delay(delay)\n\n            func = getattr(self, action)\n            func(*point.pos)\n\n            found = True\n            break\n        sys.stdout.write('\\n')\n\n        if not found:\n            if safe:\n                log.info(\"Image(%s) not found, safe=True, skip\", pattern)\n                return None\n            raise errors.ImageNotFoundError('Not found image %s' % pattern, point)\n\n        # FIXME(ssx): maybe this function is too complex\n        return point"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_images(path=['.']):\n    for image_dir in set(path):\n        if not os.path.isdir(image_dir):\n            continue\n        for filename in os.listdir(image_dir):\n            bname, ext = os.path.splitext(filename)\n            if ext.lower() not in VALID_IMAGE_EXTS:\n                continue\n\n            filepath = os.path.join(image_dir, filename)\n            yield strutils.decode(filepath)", "response": "Return list of image files"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist all images under path", "response": "def list_all_image(path, valid_exts=VALID_IMAGE_EXTS):\n    \"\"\"List all images under path\n\n    @return unicode list\n    \"\"\"\n    for filename in os.listdir(path):\n        bname, ext = os.path.splitext(filename)\n        if ext.lower() not in VALID_IMAGE_EXTS:\n            continue\n        filepath = os.path.join(path, filename)\n        yield strutils.decode(filepath)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlooks for the image real path, if name is None, then return all images under path. @return system encoded path string FIXME(ssx): this code is just looking wired.", "response": "def search_image(name=None, path=['.']):\n    \"\"\"\n    look for the image real path, if name is None, then return all images under path.\n\n    @return system encoded path string\n    FIXME(ssx): this code is just looking wired.\n    \"\"\"\n    name = strutils.decode(name)\n    for image_dir in path:\n        if not os.path.isdir(image_dir):\n            continue\n        image_dir = strutils.decode(image_dir)\n        image_path = os.path.join(image_dir, name)\n        if os.path.isfile(image_path):\n            return strutils.encode(image_path)\n\n        for image_path in list_all_image(image_dir):\n            if not image_name_match(name, image_path):\n                continue\n            return strutils.encode(image_path)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if the filename matches the given filename.", "response": "def filename_match(fsearch, filename, width, height):\n    '''\n    <nickname>@({width}x{height}|auto).(png|jpg|jpeg)\n    '''\n    fsearch = clean_path(fsearch)\n    filename = clean_path(filename)\n    if fsearch == filename:\n        return True\n\n    if fsearch.find('@') == -1:\n        return False\n    basename, fileext = os.path.splitext(fsearch)\n    nickname, extinfo = basename.split('@', 1)\n    if extinfo == 'auto':\n        valid_names = {}.fromkeys([\n            nickname+'@{}x{}'.format(width, height)+fileext,\n            nickname+'@{}x{}'.format(height, width)+fileext,\n            nickname+'.{}x{}'.format(width, height)+fileext,\n            nickname+'.{}x{}'.format(height, width)+fileext,\n        ])\n        if filename in valid_names:\n            return True\n    # if extinfo.find('x') != -1:\n    #     cw, ch = extinfo.split('x', 1)\n    #     if cw*width == ch*height or cw*height == ch*width:\n    #         return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npoints = nameddict('Point', ['x', 'y']) pt = Point(x=1, y=2) pt.y = 3 print pt", "response": "def nameddict(name, props):\n    \"\"\"\n    Point = nameddict('Point', ['x', 'y'])\n    pt = Point(x=1, y=2)\n    pt.y = 3\n    print pt\n    \"\"\"\n    class NamedDict(object):\n        def __init__(self, *args, **kwargs):\n            self.__store = {}.fromkeys(props)\n            if args:\n                for i, k in enumerate(props[:len(args)]):\n                    self[k] = args[i]\n            for k, v in kwargs.items():\n                self[k] = v\n\n        def __getattr__(self, key):\n            # print '+', key\n            if key.startswith('_NamedDict__'):\n                return self.__dict__[key]\n            else:\n                return self.__store[key]\n\n        def __setattr__(self, key, value):\n            # print '-', key\n            if key.startswith('_NamedDict__'):\n                object.__setattr__(self, key, value)\n            else:\n                self.__setitem__(key, value)\n                # self.__store[key] = value\n\n        def __getitem__(self, key):\n            return self.__store[key]\n\n        def __setitem__(self, key, value):\n            if key not in props:\n                raise AttributeError(\"NamedDict(%s) has no attribute %s, avaliables are %s\" % (\n                    name, key, props))\n            self.__store[key] = value\n\n        def __dict__(self):\n            return self.__store\n\n        def __str__(self):\n            return 'NamedDict(%s: %s)' % (name, str(self.__store))\n\n    return NamedDict"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns a command and return the output", "response": "def run_cmd(self, *args, **kwargs):\n        \"\"\"\n        Unix style output, already replace \\r\\n to \\n\n\n        Args:\n            - timeout (float): timeout for a command exec\n        \"\"\"\n        timeout = kwargs.pop('timeout', None)\n        p = self.raw_cmd(*args, **kwargs)\n        return p.communicate(timeout=timeout)[0].decode('utf-8').replace('\\r\\n', '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun command `adb shell`", "response": "def shell(self, *args, **kwargs):\n        \"\"\"\n        Run command `adb shell`\n        \"\"\"\n        args = ['shell'] + list(args)\n        return self.run_cmd(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves a file from the device", "response": "def remove(self, filename):\n        \"\"\" \n        Remove file from device\n        \"\"\"\n        output = self.shell('rm', filename)\n        # any output means rm failed.\n        return False if output else True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef app_uninstall(self, package_name, keep_data=False):\n        if keep_data:\n            return self.run_cmd('uninstall', '-k', package_name)\n        else:\n            return self.run_cmd('uninstall', package_name)", "response": "Uninstall the specified package from the cache"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning device width height rotation", "response": "def display(self):\n        '''\n        Return device width, height, rotation\n        '''\n        w, h = (0, 0)\n        for line in self.shell('dumpsys', 'display').splitlines():\n            m = _DISPLAY_RE.search(line, 0)\n            if not m:\n                continue\n            w = int(m.group('width'))\n            h = int(m.group('height'))\n            o = int(m.group('orientation'))\n            w, h = min(w, h), max(w, h)\n            return self.Display(w, h, o)\n\n        output = self.shell('LD_LIBRARY_PATH=/data/local/tmp', self.__minicap, '-i')\n        try:\n            data = json.loads(output)\n            (w, h, o) = (data['width'], data['height'], data['rotation']/90)\n            return self.Display(w, h, o)            \n        except ValueError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef packages(self):\n        pattern = re.compile(r'package:(/[^=]+\\.apk)=([^\\s]+)')\n        packages = []\n        for line in self.shell('pm', 'list', 'packages', '-f').splitlines():\n            m = pattern.match(line)\n            if not m:\n                continue\n            path, name = m.group(1), m.group(2)\n            packages.append(self.Package(name, path))\n        return packages", "response": "Show all packages in the system"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncapture screen with adb shell screencap", "response": "def _adb_screencap(self, scale=1.0):\n        \"\"\"\n        capture screen with adb shell screencap\n        \"\"\"\n        remote_file = tempfile.mktemp(dir='/data/local/tmp/', prefix='screencap-', suffix='.png')\n        local_file = tempfile.mktemp(prefix='atx-screencap-', suffix='.png')\n        self.shell('screencap', '-p', remote_file)\n        try:\n            self.pull(remote_file, local_file)\n            image = imutils.open_as_pillow(local_file)\n            if scale is not None and scale != 1.0:\n                image = image.resize([int(scale * s) for s in image.size], Image.BICUBIC)\n            rotation = self.rotation()\n            if rotation:\n                method = getattr(Image, 'ROTATE_{}'.format(rotation*90))\n                image = image.transpose(method)\n            return image\n        finally:\n            self.remove(remote_file)\n            os.unlink(local_file)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncapture screen with minicap with minicap", "response": "def _adb_minicap(self, scale=1.0):\n        \"\"\"\n        capture screen with minicap\n\n        https://github.com/openstf/minicap\n        \"\"\"\n        remote_file = tempfile.mktemp(dir='/data/local/tmp/', prefix='minicap-', suffix='.jpg')\n        local_file = tempfile.mktemp(prefix='atx-minicap-', suffix='.jpg')\n        (w, h, r) = self.display\n        params = '{x}x{y}@{rx}x{ry}/{r}'.format(x=w, y=h, rx=int(w*scale), ry=int(h*scale), r=r*90)\n        try:\n            self.shell('LD_LIBRARY_PATH=/data/local/tmp', self.__minicap, '-s', '-P', params, '>', remote_file)\n            self.pull(remote_file, local_file)\n            image = imutils.open_as_pillow(local_file)\n            return image\n        finally:\n            self.remove(remote_file)\n            os.unlink(local_file)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef screenshot(self, filename=None, scale=1.0, method=None):\n        image = None\n        method = method or self._screenshot_method\n        if method == 'minicap':\n            try:\n                image = self._adb_minicap(scale)\n            except Exception as e:\n                logger.warn(\"use minicap failed, fallback to screencap. error detail: %s\", e)\n                self._screenshot_method = 'screencap'\n                return self.screenshot(filename=filename, scale=scale)\n        elif method == 'screencap':\n            image = self._adb_screencap(scale)\n        else:\n            raise RuntimeError(\"No such method(%s)\" % method)\n\n        if filename:\n            image.save(filename)\n        return image", "response": "Take device screenshot and save it to filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef click(self, x, y):\n        '''\n        same as adb -s ${SERIALNO} shell input tap x y\n        FIXME(ssx): not tested on horizontal screen\n        '''\n        self.shell('input', 'tap', str(x), str(y))", "response": "same as adb - s ${SERIALNO } shell input tap x y"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_locked(self):\n        _lockScreenRE = re.compile('mShowingLockscreen=(true|false)')\n        m = _lockScreenRE.search(self.shell('dumpsys', 'window', 'policy'))\n        if m:\n            return (m.group(1) == 'true')\n        raise RuntimeError(\"Couldn't determine screen lock state\")", "response": "Returns a bool indicating if the screen is locked."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the device screen is ON.", "response": "def is_screen_on(self):\n        '''\n        Checks if the screen is ON.\n        Returns:\n            True if the device screen is ON\n        Raises:\n            RuntimeError\n        '''\n\n        _screenOnRE = re.compile('mScreenOnFully=(true|false)')\n        m = _screenOnRE.search(self.shell('dumpsys', 'window', 'policy'))\n        if m:\n            return (m.group(1) == 'true')\n        raise RuntimeError(\"Couldn't determine screen ON state\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef current_app(self):\n        # try: adb shell dumpsys activity top\n        _activityRE = re.compile(r'ACTIVITY (?P<package>[^/]+)/(?P<activity>[^/\\s]+) \\w+ pid=(?P<pid>\\d+)')\n        m = _activityRE.search(self.shell('dumpsys', 'activity', 'top'))\n        if m:\n            return dict(package=m.group('package'), activity=m.group('activity'), pid=int(m.group('pid')))\n\n        # try: adb shell dumpsys window windows\n        _focusedRE = re.compile('mFocusedApp=.*ActivityRecord{\\w+ \\w+ (?P<package>.*)/(?P<activity>.*) .*')\n        m = _focusedRE.search(self.shell('dumpsys', 'window', 'windows'))\n        if m:\n            return dict(package=m.group('package'), activity=m.group('activity'))\n        raise RuntimeError(\"Couldn't get focused app\")", "response": "Return a dict of package activity pid and active app"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_index_node(self, idx):\r\n        '''get node with iterindex `idx`'''\r\n        idx = self.node_index.index(idx)\r\n        return self.nodes[idx]", "response": "get node with iterindex idx"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self):\r\n        '''start running in background.'''\r\n        self.update_device_info()\r\n        self.get_device_status(0) # start addons.\r\n        self.hook()\r\n        self.thread = threading.Thread(target=self._run)\r\n        self.thread.start()\r\n        self.running = True", "response": "start running in background."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_device_status(self, t):\r\n        '''get device status at a given time t (within self.monitor_period)'''\r\n        data = {}\r\n        for name, (func, _, _) in self.addons.iteritems():\r\n            data[name] = func(t)\r\n        return data", "response": "get device status at a given time t"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall when user input events happens", "response": "def input_event(self, event):\r\n        '''should be called when user input events happens (from hook)'''\r\n        if not self.running or self.device is None:\r\n            return\r\n        # print 'input_event', event.time\r\n        status = self.get_device_status(event.time)\r\n        self.input_queue.put((self.input_index, event, status))\r\n        self.input_index += 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating draft from recorded frames", "response": "def analyze_frames(cls, workdir):\r\n        '''generate draft from recorded frames'''\r\n        record = cls(None, workdir)\r\n        obj = {}\r\n        with open(os.path.join(workdir, 'frames', 'frames.json')) as f:\r\n            obj = json.load(f)\r\n        record.device_info = obj['device']\r\n        record.frames = obj['frames']\r\n        record.analyze_all()\r\n        record.save()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating code from case. json", "response": "def process_casefile(cls, workdir):\r\n        '''generate code from case.json'''\r\n        record = cls(None, workdir)\r\n        obj = {}\r\n        with open(os.path.join(workdir, 'frames', 'frames.json')) as f:\r\n            obj = json.load(f)\r\n        record.device_info = obj['device']\r\n        record.frames = obj['frames']\r\n\r\n        casedir = os.path.join(workdir, 'case')\r\n        with open(os.path.join(casedir, 'case.json')) as f:\r\n            record.case_draft = json.load(f)\r\n\r\n        # remove old files\r\n        for f in os.listdir(casedir):\r\n            if f != 'case.json':\r\n                os.remove(os.path.join(casedir, f))\r\n\r\n        record.generate_script()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef driver(self, package=None, attach=True, activity=None, process=None):\r\n        app = self._d.current_app()\r\n        capabilities = {\r\n            'chromeOptions': {\r\n                'androidDeviceSerial': self._d.serial,\r\n                'androidPackage': package or app.package,\r\n                'androidUseRunningApp': attach,\r\n                'androidProcess': process or app.package,\r\n                'androidActivity': activity or app.activity,\r\n            }\r\n        }\r\n\r\n        try:\r\n            dr = webdriver.Remote('http://localhost:%d' % self._port, capabilities)\r\n        except URLError:\r\n            self._launch_webdriver()\r\n            dr = webdriver.Remote('http://localhost:%d' % self._port, capabilities)\r\n        \r\n        # always quit driver when done\r\n        atexit.register(dr.quit)\r\n        return dr", "response": "Returns a new selenium driver instance with the specified capabilities."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_original(mod_name, item_name):\r\n    if isinstance(item_name, string_types):\r\n        return _get_original(mod_name, [item_name])[0]\r\n    else:\r\n        return _get_original(mod_name, item_name)", "response": "Retrieve the original object from a module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn adb binary full path", "response": "def adb_path(cls):\n        \"\"\"return adb binary full path\"\"\"\n        if cls.__adb_cmd is None:\n            if \"ANDROID_HOME\" in os.environ:\n                filename = \"adb.exe\" if os.name == 'nt' else \"adb\"\n                adb_dir = os.path.join(os.environ[\"ANDROID_HOME\"], \"platform-tools\")\n                adb_cmd = os.path.join(adb_dir, filename)\n                if not os.path.exists(adb_cmd):\n                    raise EnvironmentError(\n                        \"Adb not found in $ANDROID_HOME/platform-tools path: %s.\" % adb_dir)\n            else:\n                import distutils\n                if \"spawn\" not in dir(distutils):\n                    import distutils.spawn\n                adb_cmd = distutils.spawn.find_executable(\"adb\")\n                if adb_cmd:\n                    adb_cmd = os.path.realpath(adb_cmd)\n                else:\n                    raise EnvironmentError(\"$ANDROID_HOME environment not set.\")\n            cls.__adb_cmd = adb_cmd\n        return cls.__adb_cmd"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a dict of attached devices. key is the device serial value is the device name. value is the device name.", "response": "def devices(self):\n        '''get a dict of attached devices. key is the device serial, value is device name.'''\n        out = self.run_cmd('devices') #subprocess.check_output([self.adb_path(), 'devices']).decode(\"utf-8\")\n        if 'adb server is out of date' in out:\n            out = self.run_cmd('devices')\n        match = \"List of devices attached\"\n        index = out.find(match)\n        if index < 0:\n            raise EnvironmentError(\"adb is not working.\")\n        return dict(re.findall(r'([^\\s]+)\\t(\\w+)', out))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting version of current adb Return example : 1. 0. 32 1. 0. 32 32", "response": "def version(self):\n        '''\n        Get version of current adb\n        Return example:\n            [u'1.0.32', u'1', u'0', u'32']\n        '''\n        '''adb version'''\n        match = re.search(r\"(\\d+)\\.(\\d+)\\.(\\d+)\", self.run_cmd(\"version\"))\n        return [match.group(i) for i in range(4)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect(self, addr):\n        '''\n        Call adb connect\n        Return true when connect success\n        '''\n        if addr.find(':') == -1:\n            addr += ':5555'\n        output = self.run_cmd('connect', addr)\n        return 'unable to connect' not in output", "response": "Connect to a specific user s domain"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nforward list - list all available keys", "response": "def forward_list(self):\n        '''\n        adb forward --list\n        TODO: not tested\n        '''\n        version = self.version()\n        if int(version[1]) <= 1 and int(version[2]) <= 0 and int(version[3]) < 31:\n            raise EnvironmentError(\"Low adb version.\")\n        lines = self.run_cmd(\"forward\", \"--list\").strip().splitlines()\n        return [line.strip().split() for line in lines]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef forward(self, serial, remote, local_port=None): #local_port, remote_port=None):\n        '''\n        Args:\n            serial (str): device serial number\n            remote (str|int): A int means device port, str can be localabstract:service-name\n            local_port (int): optional, if not given, a random port will be generated\n        \n        Returns:\n            local_port\n        \n        If forwards already exists and local_port not specified, just return current forwarded port\n        '''\n        if isinstance(remote, int):\n            remote = 'tcp:%d' % remote\n        if not local_port:\n            for s, lp, rp in self.forward_list():\n                if s == serial and rp == remote:\n                    return int(lp[4:])\n            return self.forward(serial, remote, next_local_port(self.server_host))\n        else:\n            self.raw_cmd(\"-s\", serial, \"forward\", 'tcp:%d' % local_port, remote).wait()\n            return local_port", "response": "Forward a device to another device."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nforwards device port to local cache entry.", "response": "def forward(self, device_port, local_port=None):\n        \"\"\"Forward device port to local\n        Args:\n            device_port: port inside device\n            local_port: port on PC, if this value is None, a port will random pick one.\n\n        Returns:\n            tuple, (host, local_port)\n        \"\"\"\n        port = self._adb_device.forward(device_port, local_port)\n        return (self._host, port)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the rotation of the phone or screen", "response": "def rotation(self):\n        \"\"\"\n        Rotaion of the phone\n\n        0: normal\n        1: home key on the right\n        2: home key on the top\n        3: home key on the left\n        \"\"\"\n        if self.screen_rotation in range(4):\n            return self.screen_rotation\n        return self.adb_device.rotation() or self.info['displayRotation']"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun command for example adb pull", "response": "def adb_cmd(self, command, **kwargs):\n        '''\n        Run adb command, for example: adb(['pull', '/data/local/tmp/a.png'])\n\n        Args:\n            command: string or list of string\n\n        Returns:\n            command output\n        '''\n        kwargs['timeout'] = kwargs.get('timeout', self._adb_shell_timeout)\n        if isinstance(command, list) or isinstance(command, tuple):\n            return self.adb_device.run_cmd(*list(command), **kwargs)\n        return self.adb_device.run_cmd(command, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef properties(self):\n        '''\n        Android Properties, extracted from `adb shell getprop`\n\n        Returns:\n            dict of props, for\n            example:\n\n                {'ro.bluetooth.dun': 'true'}\n        '''\n        props = {}\n        for line in self.adb_shell(['getprop']).splitlines():\n            m = _PROP_PATTERN.match(line)\n            if m:\n                props[m.group('key')] = m.group('value')\n        return props", "response": "Return a dict of Android Properties extracted from adb shell getprop"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_app(self, package_name, activity=None, stop=False):\n        '''\n        Start application\n\n        Args:\n            - package_name (string): like com.example.app1\n            - activity (string): optional, activity name\n\n        Returns time used (unit second), if activity is not None\n\n        Document: usage: adb shell am start\n            -D: enable debugging\n            -W: wait for launch to complete\n            --start-profiler <FILE>: start profiler and send results to <FILE>\n            --sampling INTERVAL: use sample profiling with INTERVAL microseconds\n                between samples (use with --start-profiler)\n            -P <FILE>: like above, but profiling stops when app goes idle\n            -R: repeat the activity launch <COUNT> times.  Prior to each repeat,\n                the top activity will be finished.\n            -S: force stop the target app before starting the activity\n            --opengl-trace: enable tracing of OpenGL functions\n            --user <USER_ID> | current: Specify which user to run as; if not\n                specified then run as the current user.\n        '''\n        _pattern = re.compile(r'TotalTime: (\\d+)')\n        if activity is None:\n            self.adb_shell(['monkey', '-p', package_name, '-c',\n                            'android.intent.category.LAUNCHER', '1'])\n        else:\n            args = ['-W']\n            if stop:\n                args.append('-S')\n            output = self.adb_shell(\n                ['am', 'start'] + args + ['-n', '%s/%s' % (package_name, activity)])\n            m = _pattern.search(output)\n            if m:\n                return int(m.group(1))/1000.0", "response": "Start application with activity name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops application with specified package name.", "response": "def stop_app(self, package_name, clear=False):\n        '''\n        Stop application\n\n        Args:\n            package_name: string like com.example.app1\n            clear: bool, remove user data\n\n        Returns:\n            None\n        '''\n        if clear:\n            self.adb_shell(['pm', 'clear', package_name])\n        else:\n            self.adb_shell(['am', 'force-stop', package_name])\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump_nodes(self):\n        xmldata = self._uiauto.dump()\n        dom = xml.dom.minidom.parseString(xmldata.encode('utf-8'))\n        root = dom.documentElement\n        nodes = root.getElementsByTagName('node')\n        ui_nodes = []\n        for node in nodes:\n            ui_nodes.append(self._parse_xml_node(node))\n        return ui_nodes", "response": "Dump current screen UI to list of UINode objects"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef type(self, s, enter=False, clear=False):\n\n        if clear:\n            self.clear_text()\n\n        self._uiauto.send_keys(s)\n\n        if enter:\n            self.keyevent('KEYCODE_ENTER')", "response": "Input some text, this method has been tested not very stable on some device.\n        \"Hi world\" maybe spell into \"H iworld\"\n\n        Args:\n            - s: string (text to input), better to be unicode\n            - enter(bool): input enter at last\n            - next(bool): perform editor action Next\n            - clear(bool): clear text before type\n            - ui_select_kwargs(**): tap then type\n\n        The android source code show that\n        space need to change to %s\n        insteresting thing is that if want to input %s, it is really unconvinent.\n        android source code can be found here.\n        https://android.googlesource.com/platform/frameworks/base/+/android-4.4.2_r1/cmds/input/src/com/android/commands/input/Input.java#159\n        app source see here: https://github.com/openatx/android-unicode"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all input methods", "response": "def input_methods(self):\n        \"\"\"\n        Get all input methods\n\n        Return example: ['com.sohu.inputmethod.sogou/.SogouIME', 'android.unicode.ime/.Utf7ImeService']\n        \"\"\"\n        imes = []\n        for line in self.adb_shell(['ime', 'list', '-s', '-a']).splitlines():\n            line = line.strip()\n            if re.match('^.+/.+$', line):\n                imes.append(line)\n        return imes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef current_ime(self):\n        ''' Get current input method '''\n        dumpout = self.adb_shell(['dumpsys', 'input_method'])\n        m = _INPUT_METHOD_RE.search(dumpout)\n        if m:\n            return m.group(1)", "response": "Get current user s iat"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open(image):\n    '''\n    Args:\n        - image: support many type. filepath or url or data:image/png:base64\n    Return:\n        Pattern\n    Raises\n        IOError\n    '''\n    if isinstance(image, six.string_types):\n        name = image\n        if name.startswith('data:image/'):\n            return _open_data_url(name)\n        if re.match(r'^https?://', name):\n            return url_to_image(name)\n        if os.path.isfile(name):\n            img = cv2.imread(name)\n            if img is None:\n                raise IOError(\"Image format error: %s\" % name)\n            return img\n        raise IOError(\"Open image(%s) not found\" % name)\n\n    return image", "response": "Returns a Pattern\n    containing the image."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen a file as PIL - low.", "response": "def open_as_pillow(filename):\n    \"\"\" This way can delete file immediately \"\"\"\n    with __sys_open(filename, 'rb') as f:\n        data = BytesIO(f.read())\n        return Image.open(data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_pillow(pil_image):\n    # convert PIL to OpenCV\n    pil_image = pil_image.convert('RGB')\n    cv2_image = np.array(pil_image)\n    # Convert RGB to BGR \n    cv2_image = cv2_image[:, :, ::-1].copy()\n    return cv2_image", "response": "Convert from PIL to OpenCV"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload the image and convert it to a NumPy array then read it into OpenCV format", "response": "def url_to_image(url, flag=cv2.IMREAD_COLOR):\n    \"\"\" download the image, convert it to a NumPy array, and then read\n    it into OpenCV format \"\"\"\n    resp = urlopen(url)\n    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n    image = cv2.imdecode(image, flag)\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind counters include pos in differences between img1 & img2 ( cv2 images", "response": "def diff_rect(img1, img2, pos=None):\n    \"\"\"find counters include pos in differences between img1 & img2 (cv2 images)\"\"\"\n    diff = cv2.absdiff(img1, img2)\n    diff = cv2.GaussianBlur(diff, (3, 3), 0)\n    edges = cv2.Canny(diff, 100, 200)\n    _, thresh = cv2.threshold(edges, 0, 255, cv2.THRESH_BINARY)\n    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n    if not contours:\n        return None\n    contours.sort(key=lambda c: len(c))\n    # no pos provide, just return the largest different area rect\n    if pos is None:\n        cnt = contours[-1]\n        x0, y0, w, h = cv2.boundingRect(cnt)\n        x1, y1 = x0+w, y0+h\n        return (x0, y0, x1, y1)\n    # else the rect should contain the pos\n    x, y = pos\n    for i in range(len(contours)):\n        cnt = contours[-1-i]\n        x0, y0, w, h = cv2.boundingRect(cnt)\n        x1, y1 = x0+w, y0+h\n        if x0 <= x <= x1 and y0 <= y <= y1:\n            return (x0, y0, x1, y1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmarks a point in the image", "response": "def mark_point(img, x, y):\n    \"\"\"\n    Mark a point\n\n    Args:\n        - img(numpy): the source image\n        - x, y(int): position\n    \"\"\"\n    overlay = img.copy()\n    output = img.copy()\n\n    alpha = 0.5\n    radius = max(5, min(img.shape[:2])//15)\n    center = int(x), int(y)\n    color = (0, 0, 255)\n\n    cv2.circle(overlay, center, radius, color, -1)\n    cv2.addWeighted(overlay, alpha, output, 1-alpha, 0, output)\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _take_screenshot(self):\n        # screen size: 1280x720\n        if self._fail_first_screenshot:\n            self._fail_first_screenshot = False\n            raise IOError(\"dummy fail screenshot\")\n        screen_path = os.path.join(__dir__, '../../tests/media/dummy_screen.png')\n        screen = Image.open(screen_path)\n        return screen", "response": "Take a screenshot of the dummy screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart an application with the specified bundle ID.", "response": "def start_app(self, bundle_id):\n        \"\"\"Start an application\n        Args:\n            - bundle_id: (string) apk bundle ID\n\n        Returns:\n            WDA session object\n        \"\"\"\n        # if self._session is not None:\n        #     self.stop_app()\n        self._bundle_id = bundle_id\n        self._session = self._wda.session(bundle_id)\n        return self._session"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef display(self):\n        w, h = self.session.window_size()\n        return Display(w*self.scale, h*self.scale)", "response": "Get screen width and height"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rotation(self):\n        rs = dict(PORTRAIT=0, LANDSCAPE=1, UIA_DEVICE_ORIENTATION_LANDSCAPERIGHT=3)\n        return rs.get(self.session.orientation, 0)", "response": "Rotation of device in USB zone"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsimulates click operation x y", "response": "def do_tap(self, x, y):\n        \"\"\"Simulate click operation\n        Args:\n            x, y(int): position\n        \"\"\"\n        rx, ry = x/self.scale, y/self.scale\n        self.session.tap(rx, ry)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsimulating swipe operation on the current log entry.", "response": "def swipe(self, x1, y1, x2, y2, duration=0.5):\n        \"\"\"Simulate swipe operation\n        Args:\n            x1, y1(int): from position\n            x2, y2(int): to position\n            duration(float): swipe duration, unit seconds\n        \"\"\"\n        scale = self.scale\n        x1, y1, x2, y2 = x1/scale, y1/scale, x2/scale, y2/scale\n        self.session.swipe(x1, y1, x2, y2, duration)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _take_screenshot(self):\n        raw_png = self._wda.screenshot()\n        img = Image.open(BytesIO(raw_png))\n        return img", "response": "Take a screenshot of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_cmd(self, *args):\n        '''adb command array\n        For example:\n            build_cmd(\"shell\", \"uptime\") will get\n            [\"adb\", \"-s\", \"xx..\", \"-P\", \"5037\", \"shell\", \"uptime\"]\n        '''\n        return [self.adb(), \"-s\", self.serial] + self.adb_host_port_options + list(args)", "response": "build adb command array"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove(self, path):\n        p = self.cmd('shell', 'rm', path)\n        stdout, stderr = p.communicate()\n        if stdout or stderr:\n            return False\n        else:\n            return True", "response": "Remove remote file from the cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts touch position to screen coordinates", "response": "def touch2screen(w, h, o, x, y):\r\n    '''convert touch position'''\r\n    if o == 0:\r\n        return x, y\r\n    elif o == 1: # landscape-right\r\n        return y, w-x\r\n    elif o == 2: # upsidedown\r\n        return w-x, h-y\r\n    elif o == 3: # landscape-left\r\n        return h-y, x\r\n    return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef listen(d, save_dir='report'):\n    ''' Depreciated '''\n    warnings.warn(\n        \"Using report.listen is deprecated, use report.Report(d, save_dir) instead.\", \n        ExtDeprecationWarning, stacklevel=2\n    )\n    Report(d, save_dir)", "response": "Deprecated. Use report. Report instead."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch_wda(self):\n        import wda\n\n        def _click(that):\n            rawx, rawy = that.bounds.center\n            x, y = self.d.scale*rawx, self.d.scale*rawy\n            screen_before = self._save_screenshot()\n            orig_click = pt.get_original(wda.Selector, 'click')\n            screen_after = self._save_screenshot()\n            self.add_step('click',\n                screen_before=screen_before,\n                screen_after=screen_after,\n                position={'x': x, 'y': y})\n            return orig_click(that)\n\n        pt.patch_item(wda.Selector, 'click', _click)", "response": "Patch the click event in the WebDriverAgent to allow for customization of the click event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef info(self, text, screenshot=None):\n        step = {\n            'time': '%.1f' % (time.time()-self.start_time,),\n            'action': 'info',\n            'description': text,\n            'success': True,\n        }   \n        if screenshot:\n            step['screenshot'] = self._take_screenshot(screenshot, name_prefix='info')\n        self.steps.append(step)", "response": "Add info step to log."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an error step to the log.", "response": "def error(self, text, screenshot=None):\n        \"\"\"\n        Args:\n            - text(str): description\n            - screenshot: Bool or PIL.Image object\n        \"\"\"\n        step = {\n            'time': '%.1f' % (time.time()-self.start_time,),\n            'action': 'error',\n            'description': text,\n            'success': False,\n        }   \n        if screenshot:\n            step['screenshot'] = self._take_screenshot(screenshot, name_prefix='error')\n        self.steps.append(step)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _take_screenshot(self, screenshot=False, name_prefix='unknown'):\n        if isinstance(screenshot, bool):\n            if not screenshot:\n                return\n            return self._save_screenshot(name_prefix=name_prefix)\n        if isinstance(screenshot, Image.Image):\n            return self._save_screenshot(screen=screenshot, name_prefix=name_prefix)\n\n        raise TypeError(\"invalid type for func _take_screenshot: \"+ type(screenshot))", "response": "This is different from _save_screenshot. It is different from _save_screenshot."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_assert(self, **kwargs):\n        # convert screenshot to relative path from <None|True|False|PIL.Image>\n        screenshot = kwargs.get('screenshot')\n        is_success = kwargs.get('success')\n        screenshot = (not is_success) if screenshot is None else screenshot\n        kwargs['screenshot'] = self._take_screenshot(screenshot=screenshot, name_prefix='assert')\n        action = kwargs.pop('action', 'assert')\n        self.add_step(action, **kwargs)\n        if not is_success:\n            message = kwargs.get('message')\n            frame, filename, line_number, function_name, lines, index = inspect.stack()[2]\n            print('Assert [%s: %d] WARN: %s' % (filename, line_number, message))\n            if not kwargs.get('safe', False):\n                raise AssertionError(message)", "response": "add_step will take screenshot and raise AssertionError"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks v1 is equals v2 and take screenshot if safe is True", "response": "def assert_equal(self, v1, v2, **kwargs):#, desc=None, screenshot=False, safe=False):\n        \"\"\" Check v1 is equals v2, and take screenshot if not equals\n        Args:\n            - desc (str): some description\n            - safe (bool): will omit AssertionError if set to True\n            - screenshot: can be type <None|True|False|PIL.Image>\n        \"\"\"\n        is_success = v1 == v2\n        if is_success:\n            message = \"assert equal success, %s == %s\" %(v1, v2)\n        else:\n            message = '%s not equal %s' % (v1, v2)\n        kwargs.update({\n            'message': message,\n            'success': is_success,\n        })\n        self._add_assert(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assert_image_exists(self, pattern, timeout=20.0, **kwargs):\n        pattern = self.d.pattern_open(pattern)\n        match_kwargs = kwargs.copy()\n        match_kwargs.pop('safe', None)\n        match_kwargs.update({\n            'timeout': timeout,\n            'safe': True,\n        })\n        res = self.d.wait(pattern, **match_kwargs)\n        is_success = res is not None\n        message = 'assert image exists'\n        if res:\n            x, y = res.pos\n            kwargs['position'] = {'x': x, 'y': y}\n            message = 'image exists\\npos %s\\nconfidence=%.2f\\nmethod=%s' % (res.pos, res.confidence, res.method)\n        else:\n            res = self.d.match(pattern)\n            if res is None:\n                message = 'Image not found'\n            else:\n                th = kwargs.get('threshold') or pattern.threshold or self.image_match_threshold\n                message = 'Matched: %s\\nPosition: %s\\nConfidence: %.2f\\nThreshold: %.2f' % (\n                    res.matched, res.pos, res.confidence, th)\n\n        kwargs['target'] = self._save_screenshot(pattern, name_prefix='target')\n        kwargs['screenshot'] = self.last_screenshot\n        kwargs.update({\n            'action': 'assert_image_exists',\n            'message': message,\n            'success': is_success,\n        })\n        self._add_assert(**kwargs)", "response": "Assert if image exists in the cache"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassert that the UI exists in the cache.", "response": "def assert_ui_exists(self, ui, **kwargs):\n        \"\"\" For Android & IOS\n        Args:\n            - ui: need have property \"exists\"\n            - desc (str): description\n            - safe (bool): will omit AssertionError if set to True\n            - screenshot: can be type <None|True|False|PIL.Image>\n            - platform (str, default:android): android | ios\n        \"\"\"\n        is_success = ui.exists\n        if is_success:\n            if kwargs.get('screenshot') is not None:\n                if self.d.platform == 'android':\n                    bounds = ui.info['bounds'] # For android only.\n                    kwargs['position'] = {\n                        'x': (bounds['left']+bounds['right'])//2,\n                        'y': (bounds['top']+bounds['bottom'])//2,\n                    }\n                elif self.d.platform == 'ios':\n                    bounds = ui.bounds # For iOS only.\n                    kwargs['position'] = {\n                        'x': self.d.scale*(bounds.x+bounds.width//2),\n                        'y': self.d.scale*(bounds.y+bounds.height//2),\n                    }\n            message = 'UI exists'\n        else:\n            message = 'UI not exists'\n        kwargs.update({\n            'message': message,\n            'success': is_success,\n        })\n        self._add_assert(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_basis(ax, arm_length=1):\n\n    ax.set_xlim3d([-1.0, 1.0])\n    ax.set_xlabel('X')\n\n    ax.set_ylim3d([-1.0, 1.0])\n    ax.set_ylabel('Y')\n\n    ax.set_zlim3d([-1.0, 1.0])\n    ax.set_zlabel('Z')\n\n    # Plot du rep\u00e8re\n    # Sa taille est relative \u00e0 la taille du bras\n    ax.plot([0, arm_length * 1.5], [0, 0], [0, 0])\n    ax.plot([0, 0], [0, arm_length * 1.5], [0, 0])\n    ax.plot([0, 0], [0, 0], [0, arm_length * 1.5])", "response": "Plot the basis of the current robot."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Rx_matrix(theta):\n    return np.array([\n        [1, 0, 0],\n        [0, np.cos(theta), -np.sin(theta)],\n        [0, np.sin(theta), np.cos(theta)]\n    ])", "response": "Rotation matrix around the X axis"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Rz_matrix(theta):\n    return np.array([\n        [np.cos(theta), -np.sin(theta), 0],\n        [np.sin(theta), np.cos(theta), 0],\n        [0, 0, 1]\n    ])", "response": "Rotation matrix around the Z axis"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a rotation matrix described by the extrinsinc roll pitch yaw coordinates", "response": "def rpy_matrix(roll, pitch, yaw):\n    \"\"\"Returns a rotation matrix described by the extrinsinc roll, pitch, yaw coordinates\"\"\"\n    return np.dot(Rz_matrix(yaw), np.dot(Ry_matrix(pitch), Rx_matrix(roll)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a rotation matrix around the given axis", "response": "def axis_rotation_matrix(axis, theta):\n    \"\"\"Returns a rotation matrix around the given axis\"\"\"\n    [x, y, z] = axis\n    c = np.cos(theta)\n    s = np.sin(theta)\n    return np.array([\n        [x**2 + (1 - x**2) * c, x * y * (1 - c) - z * s, x * z * (1 - c) + y * s],\n        [x * y * (1 - c) + z * s, y ** 2 + (1 - y**2) * c, y * z * (1 - c) - x * s],\n        [x * z * (1 - c) - y * s, y * z * (1 - c) + x * s, z**2 + (1 - z**2) * c]\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a rotation matrix around the given axis", "response": "def symbolic_axis_rotation_matrix(axis, symbolic_theta):\n    \"\"\"Returns a rotation matrix around the given axis\"\"\"\n    [x, y, z] = axis\n    c = sympy.cos(symbolic_theta)\n    s = sympy.sin(symbolic_theta)\n    return sympy.Matrix([\n        [x**2 + (1 - x**2) * c, x * y * (1 - c) - z * s, x * z * (1 - c) + y * s],\n        [x * y * (1 - c) + z * s, y ** 2 + (1 - y**2) * c, y * z * (1 - c) - x * s],\n        [x * z * (1 - c) - y * s, y * z * (1 - c) + x * s, z**2 + (1 - z**2) * c]\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_transformation_matrix(translation, orientation_matrix=np.zeros((3, 3))):\n    matrix = np.eye(4)\n\n    matrix[:-1, :-1] = orientation_matrix\n    matrix[:-1, -1] = translation\n    return matrix", "response": "Converts a tuple translation vector and orientation matrix to a transformation matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cartesian_to_homogeneous(cartesian_matrix, matrix_type=\"numpy\"):\n    dimension_x, dimension_y = cartesian_matrix.shape\n    # Square matrix\n    # Manage different types fo input matrixes\n    if matrix_type == \"numpy\":\n        homogeneous_matrix = np.eye(dimension_x + 1)\n    elif matrix_type == \"sympy\":\n        homogeneous_matrix = sympy.eye(dimension_x + 1)\n    # Add a column filled with 0 and finishing with 1 to the cartesian matrix to transform it into an homogeneous one\n    homogeneous_matrix[:-1, :-1] = cartesian_matrix\n\n    return homogeneous_matrix", "response": "Converts a cartesian matrix to an homogenous matrix"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cartesian_to_homogeneous_vectors(cartesian_vector, matrix_type=\"numpy\"):\n    dimension_x = cartesian_vector.shape[0]\n    # Vector\n    if matrix_type == \"numpy\":\n        homogeneous_vector = np.zeros(dimension_x + 1)\n        # Last item is a 1\n        homogeneous_vector[-1] = 1\n        homogeneous_vector[:-1] = cartesian_vector\n    return homogeneous_vector", "response": "Converts a cartesian vector to an homogenous vector"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pose_to_list(pose):\n    if type(pose) == geometry_msgs.msg.PoseStamped:\n        return [[\n            pose.pose.position.x, pose.pose.position.y, pose.pose.position.z\n        ], [\n            pose.pose.orientation.x, pose.pose.orientation.y,\n            pose.pose.orientation.z, pose.pose.orientation.w\n        ]]\n    elif type(pose) == geometry_msgs.msg.Pose:\n        return [[pose.position.x, pose.position.y, pose.position.z], [\n            pose.orientation.x, pose.orientation.y, pose.orientation.z,\n            pose.orientation.w\n        ]]\n    else:\n        raise Exception(\"pose_to_list: parameter of type %s unexpected\",\n                        str(type(pose)))", "response": "Converts a Pose or PoseStamped in Python list to a list of lists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a list in geometry_msgs. msg. Quaternion to a list in geometry_msgs. msg. Quaternion", "response": "def list_to_quat(quatlist):\n    \"\"\"\n    Convert a quaternion in the form of a list in geometry_msgs/Quaternion\n    :param quatlist: [x, y, z, w]\n    :return:\n    \"\"\"\n    return geometry_msgs.msg.Quaternion(\n        x=quatlist[0], y=quatlist[1], z=quatlist[2], w=quatlist[3])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_to_pose(poselist, frame_id=\"\", stamp=rospy.Time(0)):\n    p = PoseStamped()\n    p.header.frame_id = frame_id\n    p.header.stamp = stamp\n    p.pose.position.x = poselist[0][0]\n    p.pose.position.y = poselist[0][1]\n    p.pose.position.z = poselist[0][2]\n    p.pose.orientation.x = poselist[1][0]\n    p.pose.orientation.y = poselist[1][1]\n    p.pose.orientation.z = poselist[1][2]\n    p.pose.orientation.w = poselist[1][3]\n    return p", "response": "Convert a list of poses into a PoseStamped object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a Quaternion or numpy. ndarray in list [ x y z w", "response": "def quat_to_list(quat):\n    \"\"\"\n    convert a geometry_msgs/Quaternion or numpy.ndarray in list [x, y, z, w]\n    :param quat:  a geometry_msgs/Quaternion or numpy.ndarray\n    :return: the corresponding quaternion [x, y, z, w]\n    \"\"\"\n    if isinstance(quat, Quaternion):\n        return [quat.x, quat.y, quat.z, quat.w]\n    elif isinstance(quat, ndarray):\n        return [quat[0], quat[1], quat[2], quat[3]]\n    else:\n        raise Exception(\"quat_to_list expects Quaternion only but received {}\".\n                        format(str(type(quat))))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef quat_rotate(rotation, vector):\n\n    def quat_mult_point(q, w):\n        return (q[3] * w[0] + q[1] * w[2] - q[2] * w[1],\n                q[3] * w[1] + q[2] * w[0] - q[0] * w[2],\n                q[3] * w[2] + q[0] * w[1] - q[1] * w[0],\n                -q[0] * w[0] - q[1] * w[1] - q[2] * w[2])\n\n    q = quat_mult_point(rotation, vector)\n    q = tf.transformations.quaternion_multiply(\n        q, tf.transformations.quaternion_inverse(rotation))\n    return [q[0], q[1], q[2]]", "response": "Rotates a vector according to a quaternion."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncombines two transformations together", "response": "def multiply_transform(t1, t2):\n    \"\"\"\n    Combines two transformations together\n    The order is translation first, rotation then\n    :param t1: [[x, y, z], [x, y, z, w]] or matrix 4x4\n    :param t2: [[x, y, z], [x, y, z, w]] or matrix 4x4\n    :return: The combination t1-t2 in the form [[x, y, z], [x, y, z, w]] or matrix 4x4\n    \"\"\"\n    if _is_indexable(t1) and len(t1) == 2:\n        return [\n            list(quat_rotate(t1[1], t2[0]) + array(t1[0])),\n            list(tf.transformations.quaternion_multiply(t1[1], t2[1]))\n        ]\n    else:\n        return dot(t1, t2)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef multiply_transform_old(t1, t2):\n    if _is_indexable(t1) and len(t1) == 2:\n        t1m = tf.transformations.translation_matrix(t1[0])\n        r1m = tf.transformations.quaternion_matrix(t1[1])\n        m1m = dot(t1m, r1m)\n        t2m = tf.transformations.translation_matrix(t2[0])\n        r2m = tf.transformations.quaternion_matrix(t2[1])\n        m2m = dot(t2m, r2m)\n        rm = dot(m1m, m2m)\n        rt = tf.transformations.translation_from_matrix(rm)\n        rr = tf.transformations.quaternion_from_matrix(rm)\n        return [list(rt), list(rr)]\n    else:\n        return dot(t1, t2)", "response": "Combines two transformations together and returns a list of lists of lists of lists of lists of lists of lists of lists of tuples"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying a scale to a transform", "response": "def scale_transform(t, scale):\n    \"\"\"\n    Apply a scale to a transform\n    :param t: The transform [[x, y, z], [x, y, z, w]] to be scaled OR the position [x, y, z] to be scaled\n    :param scale: the scale of type float OR the scale [scale_x, scale_y, scale_z]\n    :return: The scaled\n    \"\"\"\n    if isinstance(scale, float) or isinstance(scale, int):\n        scale = [scale, scale, scale]\n\n    if _is_indexable(t[0]):\n        return [[t[0][0] * scale[0], t[0][1] * scale[1], t[0][2] * scale[2]],\n                t[1]]\n    else:\n        return [t[0] * scale[0], t[1] * scale[1], t[2] * scale[2]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the inverse transformation of t", "response": "def inverse_transform(t):\n    \"\"\"\n    Return the inverse transformation of t\n    :param t: A transform [[x, y, z], [x, y, z, w]]\n    :return: t2 such as multiply_transform_(t, t2) = [[0, 0, 0], [0, 0, 0, 1]]\n    \"\"\"\n    return [\n        quat_rotate(\n            tf.transformations.quaternion_inverse(t[1]),\n            [-t[0][0], -t[0][1], -t[0][2]]),\n        tf.transformations.quaternion_inverse(t[1])\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the inverse transformation of a m4x4 transformation", "response": "def inverse_m4x4_transform(mat):\n    \"\"\"\n    Return the inverse transformation of a m4x4 transformation\n    :param mat: A m4x4 transform\n    :return: inv_transform inverse of the tranformation\n    \"\"\"\n    inv_transform = zeros((4, 4))\n    rot = mat[:-1, :-1].T\n    inv_transform[:-1, :-1] = rot\n    inv_transform[:-1, -1] = dot(-rot, mat[:-1, -1])\n    inv_transform[-1, -1] = 1\n    return inv_transform"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_to_raw_list(poselist):\n    if not (isinstance(poselist, list) or isinstance(poselist, tuple)):\n        raise TypeError(\n            \"flatten_pose({}) does not accept this type of argument\".format(\n                str(type(poselist))))\n    return [field for pose in poselist for field in pose]", "response": "Flatten a normal pose list into a raw list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the cartesian distance between two points.", "response": "def distance(p1, p2):\n    \"\"\"\n    Cartesian distance between two PoseStamped or PoseLists\n    :param p1: point 1 (list, Pose or PoseStamped)\n    :param p2: point 2 (list, Pose or PoseStamped)\n    :return: cartesian distance (float)\n    \"\"\"\n\n    def xyz(some_pose):\n        if isinstance(some_pose, PoseStamped):\n            return some_pose.pose.position.x, some_pose.pose.position.y, some_pose.pose.position.z\n        elif isinstance(some_pose, Pose):\n            return some_pose.position.x, some_pose.position.y, some_pose.position.z\n        elif _is_indexable(some_pose[0]):\n            return some_pose[0][0], some_pose[0][1], some_pose[0][2]\n        else:\n            return some_pose[0], some_pose[1], some_pose[2]\n\n    x1, y1, z1 = xyz(p1)\n    x2, y2, z2 = xyz(p2)\n\n    x = x1 - x2\n    y = y1 - y2\n    z = z1 - z2\n    return sqrt(x * x + y * y + z * z)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the angle between two poses.", "response": "def distance_quat(p1, p2):\n    \"\"\"\n    Returns the angle between two quaternions\n    http://math.stackexchange.com/a/90098\n    :param q1, q2: two quaternions [x, y, z, w] or two poses [[x, y, z], [x, y, z, w]] or two PoseStamped\n    :return: the angle between them (radians)\n    \"\"\"\n    if isinstance(p1, PoseStamped):\n        p1 = pose_to_list(p1)\n    if isinstance(p2, PoseStamped):\n        p2 = pose_to_list(p2)\n    if _is_indexable(p1) and _is_indexable(p1[0]):\n        p1 = p1[1]\n        p2 = p2[1]\n    dotp = inner(array(p1), array(p2))\n    return arccos(2 * dotp * dotp - 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef norm(point):\n    if isinstance(point, PoseStamped):\n        x = point.pose.position.x\n        y = point.pose.position.y\n        z = point.pose.position.z\n    elif isinstance(point, Pose):\n        x = point.position.x\n        y = point.position.y\n        z = point.position.z\n    else:\n        x = point[0][0]\n        y = point[0][1]\n        z = point[0][2]\n    return sqrt(x * x + y * y + z * z)", "response": "Compute the norm between two PoseStamped or PoseLists\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the next joint in the URDF tree and return the next one.", "response": "def find_next_joint(root, current_link, next_joint_name):\n    \"\"\"\n    Find the next joint in the URDF tree\n\n    Parameters\n    ----------\n    root\n    current_link: xml.etree.ElementTree\n        The current URDF link\n    next_joint_name: str\n        Optional : The name of the next joint. If not provided, find it automatically as the first child of the link.\n    \"\"\"\n    # Find the joint attached to the link\n    has_next = False\n    next_joint = None\n    search_by_name = True\n    current_link_name = None\n\n    if next_joint_name is None:\n        # If no next joint is provided, find it automatically\n        search_by_name = False\n        current_link_name = current_link.attrib[\"name\"]\n\n    for joint in root.iter(\"joint\"):\n        # Iterate through all joints to find the good one\n        if search_by_name:\n            # Find the joint given its name\n            if joint.attrib[\"name\"] == next_joint_name:\n                has_next = True\n                next_joint = joint\n        else:\n            # Find the first joint whose parent is the current_link\n            # FIXME: We are not sending a warning when we have two children for the same link\n            # Even if this is not possible, we should ensure something coherent\n            if joint.find(\"parent\").attrib[\"link\"] == current_link_name:\n                has_next = True\n                next_joint = joint\n                break\n\n    return has_next, next_joint"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_next_link(root, current_joint, next_link_name):\n    has_next = False\n    next_link = None\n\n    # If no next link, find it automatically\n    if next_link_name is None:\n        # If the name of the next link is not provided, find it\n        next_link_name = current_joint.find(\"child\").attrib[\"link\"]\n\n    for urdf_link in root.iter(\"link\"):\n        if urdf_link.attrib[\"name\"] == next_link_name:\n            next_link = urdf_link\n            has_next = True\n    return has_next, next_link", "response": "Find the next link in the URDF tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_urdf_parameters(urdf_file, base_elements=None, last_link_vector=None, base_element_type=\"link\"):\n    tree = ET.parse(urdf_file)\n    root = tree.getroot()\n    base_elements = list(base_elements)\n    if base_elements is None:\n        base_elements = [\"base_link\"]\n    elif base_elements is []:\n        raise ValueError(\"base_elements can't be the empty list []\")\n\n    joints = []\n    links = []\n    has_next = True\n    current_joint = None\n    current_link = None\n\n    # Initialize the tree traversal\n    if base_element_type == \"link\":\n        # The first element is a link, so its (virtual) parent should be a joint\n        node_type = \"joint\"\n    elif base_element_type == \"joint\":\n        # The same as before, but swap link and joint\n        node_type = \"link\"\n    else:\n        raise ValueError(\"Unknown type: {}\".format(base_element_type))\n\n    # Parcours r\u00e9cursif de la structure de la chain\n    while has_next:\n        if len(base_elements) != 0:\n            next_element = base_elements.pop(0)\n        else:\n            next_element = None\n\n        if node_type == \"link\":\n            # Current element is a link, find child joint\n            (has_next, current_joint) = find_next_joint(root, current_link, next_element)\n            node_type = \"joint\"\n            if has_next:\n                joints.append(current_joint)\n\n        elif node_type == \"joint\":\n            # Current element is a joint, find child link\n            (has_next, current_link) = find_next_link(root, current_joint, next_element)\n            node_type = \"link\"\n            if has_next:\n                links.append(current_link)\n\n    parameters = []\n\n    # Save the joints in the good format\n    for joint in joints:\n        translation = [0, 0, 0]\n        orientation = [0, 0, 0]\n        rotation = [1, 0, 0]\n        bounds = [None, None]\n\n        origin = joint.find(\"origin\")\n        if origin is not None:\n            if origin.attrib[\"xyz\"]:\n                translation = [float(x) for x in origin.attrib[\"xyz\"].split()]\n            if origin.attrib[\"rpy\"]:\n                orientation = [float(x) for x in origin.attrib[\"rpy\"].split()]\n\n        axis = joint.find(\"axis\")\n        if axis is not None:\n            rotation = [float(x) for x in axis.attrib[\"xyz\"].split()]\n\n        limit = joint.find(\"limit\")\n        if limit is not None:\n            if limit.attrib[\"lower\"]:\n                bounds[0] = float(limit.attrib[\"lower\"])\n            if limit.attrib[\"upper\"]:\n                bounds[1] = float(limit.attrib[\"upper\"])\n\n        parameters.append(lib_link.URDFLink(\n            name=joint.attrib[\"name\"],\n            bounds=tuple(bounds),\n            translation_vector=translation,\n            orientation=orientation,\n            rotation=rotation,\n        ))\n\n    # Add last_link_vector to parameters\n    if last_link_vector is not None:\n        parameters.append(lib_link.URDFLink(\n            translation_vector=last_link_vector,\n            orientation=[0, 0, 0],\n            rotation=[1, 0, 0],\n            name=\"last_joint\"\n        ))\n\n    return parameters", "response": "Returns translated parameters from the given URDF file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_motor_parameters(json_file):\n    with open(json_file) as motor_fd:\n        global_config = json.load(motor_fd)\n\n    motors = global_config[\"motors\"]\n    # Returned dict\n    motor_config = {}\n\n    # Add motor to the config\n    for motor in motors:\n        motor_config[motor] = motors[motor]\n\n    return motor_config", "response": "Returns a dictionary with joints as keys and a description of each joint as value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _convert_angle_to_pypot(angle, joint, **kwargs):\n    angle_deg = (angle * 180 / np.pi)\n\n    if joint[\"orientation-convention\"] == \"indirect\":\n        angle_deg = -1 * angle_deg\n\n    # UGLY\n    if joint[\"name\"].startswith(\"l_shoulder_x\"):\n        angle_deg = -1 * angle_deg\n\n    angle_pypot = angle_deg - joint[\"offset\"]\n\n    return angle_pypot", "response": "Converts an angle to a PyPot - compatible format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _convert_angle_from_pypot(angle, joint, **kwargs):\n    angle_internal = angle + joint[\"offset\"]\n\n    if joint[\"orientation-convention\"] == \"indirect\":\n        angle_internal = -1 * angle_internal\n\n    # UGLY\n    if joint[\"name\"].startswith(\"l_shoulder_x\"):\n        angle_internal = -1 * angle_internal\n\n    angle_internal = (angle_internal / 180 * np.pi)\n\n    return angle_internal", "response": "Converts an angle from a PyPot - compatible format"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _convert_angle_limit(angle, joint, **kwargs):\n    angle_pypot = angle\n\n    # No need to take care of orientation\n    if joint[\"orientation\"] == \"indirect\":\n        angle_pypot = 1 * angle_pypot\n\n    # angle_pypot = angle_pypot + offset\n\n    return angle_pypot * np.pi / 180", "response": "Converts the limit angle of the PyPot JSON file to the internal format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef follow_hand(poppy, delta):\n    right_arm_position = poppy.l_arm_chain.end_effector + delta\n    poppy.r_arm_chain.goto(right_arm_position, 0.5, wait=True)", "response": "Tell the right hand to follow the left hand"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inverse_kinematic_optimization(chain, target_frame, starting_nodes_angles, regularization_parameter=None, max_iter=None):\n    # Only get the position\n    target = target_frame[:3, 3]\n\n    if starting_nodes_angles is None:\n        raise ValueError(\"starting_nodes_angles must be specified\")\n\n    # Compute squared distance to target\n    def optimize_target(x):\n        # y = np.append(starting_nodes_angles[:chain.first_active_joint], x)\n        y = chain.active_to_full(x, starting_nodes_angles)\n        squared_distance = np.linalg.norm(chain.forward_kinematics(y)[:3, -1] - target)\n        return squared_distance\n\n    # If a regularization is selected\n    if regularization_parameter is not None:\n        def optimize_total(x):\n            regularization = np.linalg.norm(x - starting_nodes_angles[chain.first_active_joint:])\n            return optimize_target(x) + regularization_parameter * regularization\n    else:\n        def optimize_total(x):\n            return optimize_target(x)\n\n    # Compute bounds\n    real_bounds = [link.bounds for link in chain.links]\n    # real_bounds = real_bounds[chain.first_active_joint:]\n    real_bounds = chain.active_from_full(real_bounds)\n\n    options = {}\n    # Manage iterations maximum\n    if max_iter is not None:\n        options[\"maxiter\"] = max_iter\n\n    # Utilisation d'une optimisation L-BFGS-B\n    res = scipy.optimize.minimize(optimize_total, chain.active_from_full(starting_nodes_angles), method='L-BFGS-B', bounds=real_bounds, options=options)\n\n    logs.manager.info(\"Inverse kinematic optimisation OK, done in {} iterations\".format(res.nit))\n\n    return chain.active_to_full(res.x, starting_nodes_angles)", "response": "Computes the inverse kinematic on the specified target with an optimization method."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the transformation matrix of the forward kinematics of the given list of joints.", "response": "def forward_kinematics(self, joints, full_kinematics=False):\n        \"\"\"Returns the transformation matrix of the forward kinematics\n\n        Parameters\n        ----------\n        joints: list\n            The list of the positions of each joint. Note : Inactive joints must be in the list.\n        full_kinematics: bool\n            Return the transformation matrices of each joint\n\n        Returns\n        -------\n        frame_matrix:\n            The transformation matrix\n        \"\"\"\n        frame_matrix = np.eye(4)\n\n        if full_kinematics:\n            frame_matrixes = []\n\n        if len(self.links) != len(joints):\n            raise ValueError(\"Your joints vector length is {} but you have {} links\".format(len(joints), len(self.links)))\n\n        for index, (link, joint_angle) in enumerate(zip(self.links, joints)):\n            # Compute iteratively the position\n            # NB : Use asarray to avoid old sympy problems\n            frame_matrix = np.dot(frame_matrix, np.asarray(link.get_transformation_matrix(joint_angle)))\n            if full_kinematics:\n                # rotation_axe = np.dot(frame_matrix, link.rotation)\n                frame_matrixes.append(frame_matrix)\n\n        # Return the matrix, or matrixes\n        if full_kinematics:\n            return frame_matrixes\n        else:\n            return frame_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the inverse kinematic of the specified target.", "response": "def inverse_kinematics(self, target, initial_position=None, **kwargs):\n        \"\"\"Computes the inverse kinematic on the specified target\n\n        Parameters\n        ----------\n        target: numpy.array\n            The frame target of the inverse kinematic, in meters. It must be 4x4 transformation matrix\n        initial_position: numpy.array\n            Optional : the initial position of each joint of the chain. Defaults to 0 for each joint\n\n        Returns\n        -------\n        The list of the positions of each joint according to the target. Note : Inactive joints are in the list.\n        \"\"\"\n        # Checks on input\n        target = np.array(target)\n        if target.shape != (4, 4):\n            raise ValueError(\"Your target must be a 4x4 transformation matrix\")\n\n        if initial_position is None:\n            initial_position = [0] * len(self.links)\n\n        return ik.inverse_kinematic_optimization(self, target, starting_nodes_angles=initial_position, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self, joints, ax, target=None, show=False):\n        from . import plot_utils\n\n        if ax is None:\n            # If ax is not given, create one\n            ax = plot_utils.init_3d_figure()\n        plot_utils.plot_chain(self, joints, ax)\n        plot_utils.plot_basis(ax, self._length)\n\n        # Plot the goal position\n        if target is not None:\n            plot_utils.plot_target(target, ax)\n        if show:\n            plot_utils.show_figure()", "response": "Plots the Chain using Matplotlib"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_urdf_file(cls, urdf_file, base_elements=None, last_link_vector=None, base_element_type=\"link\", active_links_mask=None, name=\"chain\"):\n        if base_elements is None:\n            base_elements = [\"base_link\"]\n\n        links = URDF_utils.get_urdf_parameters(urdf_file, base_elements=base_elements, last_link_vector=last_link_vector, base_element_type=base_element_type)\n        # Add an origin link at the beginning\n        return cls([link_lib.OriginLink()] + links, active_links_mask=active_links_mask, name=name)", "response": "Creates a chain from an URDF file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_internet_off(secrets_file_path):\n    while True:\n        if internet_on() is False and os.path.exists(secrets_file_path):\n            break\n        else:\n            print(\"Turn off your internet and plug in your USB to continue...\")\n            time.sleep(10)\n    return True", "response": "Checks if internet off and USB plugged in returns true. If internet off returns true. Otherwise continues to wait 10 seconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if internet on and USB unplugged returns true. Otherwise continues to wait 10 seconds.", "response": "def check_internet_on(secrets_file_path):\n    \"\"\"If internet on and USB unplugged, returns true. Else, continues to wait...\"\"\"\n    while True:\n        if internet_on() is True and not os.path.exists(secrets_file_path):\n            break\n        else:\n            print(\"Turn on your internet and unplug your USB to continue...\")\n            time.sleep(10)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef issue(self, chain):\n\n        blockchain_bytes = self.certificate_batch_handler.prepare_batch()\n\n        for attempt_number in range(0, self.max_retry):\n            try:\n                txid = self.transaction_handler.issue_transaction(blockchain_bytes)\n                self.certificate_batch_handler.finish_batch(txid, chain)\n                logging.info('Broadcast transaction with txid %s', txid)\n                return txid\n            except BroadcastError:\n                logging.warning(\n                    'Failed broadcast reattempts. Trying to recreate transaction. This is attempt number %d',\n                    attempt_number)\n        logging.error('All attempts to broadcast failed. Try rerunning issuer.')\n        raise BroadcastError('All attempts to broadcast failed. Try rerunning issuer.')", "response": "Issue the certificates on the blockchain returning the txid"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify_eth_transaction(signed_hextx, eth_data_field):\n    logging.info('verifying ethDataField value for transaction')\n    ethdata_hash = []\n    for s in signed_hextx.split('80a0'):\n        ethdata_hash.append(s)\n    ethdata_hash = ethdata_hash[1][:64]\n    result = (eth_data_field == ethdata_hash)\n    if not result:\n        error_message = 'There was a problem verifying the transaction'\n        raise UnverifiedTransactionError(error_message)\n    logging.info('verified ethDataField')", "response": "Verify that the ethDataField field in transaction is valid."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npreparing a batch of certificates for issuing.", "response": "def prepare_issuance_batch(unsigned_certs_dir, signed_certs_dir, blockchain_certs_dir, work_dir,\n                           file_extension=JSON_EXT):\n    \"\"\"\n    Prepares file system for issuing a batch of certificates. Copies inputs to work_dir, and ensures\n    that all output dirs required for processing the batch exist.\n    :param unsigned_certs_dir: input certificates\n    :param signed_certs_dir: output dir\n    :param blockchain_certs_dir: output dir\n    :param work_dir: work dir\n    :return:\n    \"\"\"\n\n    # create work dir if it doesn't already exist\n    os.makedirs(work_dir, exist_ok=True)\n\n    # create final output dirs if they don't already exist\n    os.makedirs(blockchain_certs_dir, exist_ok=True)\n    os.makedirs(signed_certs_dir, exist_ok=True)\n\n    # ensure previous processing state, if any, is cleaned up\n    for item in os.listdir(work_dir):\n        file_path = os.path.join(work_dir, item)\n        if os.path.isdir(file_path):\n            shutil.rmtree(file_path)\n\n    # define work subdirs\n    unsigned_certs_work_dir = os.path.join(work_dir, UNSIGNED_CERTIFICATES_DIR)\n    signed_certs_work_dir = os.path.join(work_dir, SIGNED_CERTIFICATES_DIR)\n    blockchain_certs_work_dir = os.path.join(work_dir, BLOCKCHAIN_CERTIFICATES_DIR)\n\n    # copy input certs to unsigned certs work subdir and create output subdirs\n    shutil.copytree(unsigned_certs_dir, unsigned_certs_work_dir)\n    os.makedirs(signed_certs_work_dir, exist_ok=True)\n    os.makedirs(blockchain_certs_work_dir, exist_ok=True)\n\n    cert_info = collections.OrderedDict()\n    input_file_pattern = str(os.path.join(unsigned_certs_work_dir, '*' + file_extension))\n\n    matches = glob2.iglob(input_file_pattern, with_matches=True)\n    if not matches:\n        logging.warning('No certificates to process')\n        raise NoCertificatesFoundError('No certificates to process')\n\n    # create certificate metadata for each certificates\n    for filename, (uid,) in sorted(matches):\n        certificate_metadata = CertificateMetadata(uid=uid,\n                                                   unsigned_certs_dir=unsigned_certs_work_dir,\n                                                   signed_certs_dir=signed_certs_work_dir,\n                                                   blockcerts_dir=blockchain_certs_work_dir,\n                                                   final_blockcerts_dir=blockchain_certs_dir,\n                                                   file_extension=file_extension)\n        cert_info[uid] = certificate_metadata\n\n    logging.info('Processing %d certificates', len(cert_info))\n    return cert_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef populate(self, node_generator):\n        for data in node_generator:\n            hashed = hash_byte_array(data)\n            self.tree.add_leaf(hashed)", "response": "Populates the tree with data from node_generator."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the blockchain data for the current level of the merkle tree.", "response": "def get_blockchain_data(self):\n        \"\"\"\n        Finalize tree and return byte array to issue on blockchain\n        :return:\n        \"\"\"\n        self.tree.make_tree()\n        merkle_root = self.tree.get_merkle_root()\n        return h2b(ensure_string(merkle_root))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a generator that returns a single merkle proof for the given transaction id.", "response": "def get_proof_generator(self, tx_id, chain=Chain.bitcoin_mainnet):\n        \"\"\"\n        Returns a generator (1-time iterator) of proofs in insertion order.\n\n        :param tx_id: blockchain transaction id\n        :return:\n        \"\"\"\n        root = ensure_string(self.tree.get_merkle_root())\n        node_count = len(self.tree.leaves)\n        for index in range(0, node_count):\n            proof = self.tree.get_proof(index)\n            proof2 = []\n\n            for p in proof:\n                dict2 = dict()\n                for key, value in p.items():\n                    dict2[key] = ensure_string(value)\n                proof2.append(dict2)\n            target_hash = ensure_string(self.tree.get_leaf(index))\n            merkle_proof = {\n                \"type\": ['MerkleProof2017', 'Extension'],\n                \"merkleRoot\": root,\n                \"targetHash\": target_hash,\n                \"proof\": proof2,\n                \"anchors\": [{\n                    \"sourceId\": to_source_id(tx_id, chain),\n                    \"type\": chain.blockchain_type.external_display_value,\n                    \"chain\": chain.external_display_value\n                }]}\n            yield merkle_proof"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts to pycoin Spendable type", "response": "def spendables_for_address(self, address):\n        \"\"\"\n        Converts to pycoin Spendable type\n        :param address:\n        :return: list of Spendables\n        \"\"\"\n        unspent_outputs = bitcoin.rpc.Proxy().listunspent(addrs=[address])\n        logging.debug('spendables_for_address %s', address)\n\n        spendables = []\n        for unspent in unspent_outputs:\n            coin_value = unspent.get('amount', 0)\n            outpoint = unspent.get('outpoint')\n            script = unspent.get('scriptPubKey')\n            previous_hash = outpoint.hash\n            previous_index = outpoint.n\n            spendables.append(Spendable(coin_value, script, previous_hash, previous_index))\n        return spendables"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_unspent_outputs(self, address):\n        logging.debug('get_unspent_outputs for address=%s', address)\n        spendables = self.spendables_for_address(bitcoin_address=address)\n        if spendables:\n            return sorted(spendables, key=lambda x: hash(x.coin_value))\n        return None", "response": "Get unspent outputs at the address\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_balance(self, address):\n        spendables = self.get_unspent_outputs(address)\n        if not spendables:\n            logging.warning('address %s has a balance of 0', address)\n            return 0\n\n        balance = sum(s.coin_value for s in spendables)\n        return balance", "response": "Get the balance available to spend at the address\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbroadcast the transaction through the configured set of providers", "response": "def broadcast_tx(self, tx):\n        \"\"\"\n        Broadcast the transaction through the configured set of providers\n\n        :param tx:\n        :return:\n        \"\"\"\n        return BitcoinServiceProviderConnector.broadcast_tx_with_chain(tx, self.bitcoin_chain, self.bitcoind)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef broadcast_tx_with_chain(tx, bitcoin_chain, bitcoind=False):\n        last_exception = None\n        final_tx_id = None\n\n        # Unlike other providers, we want to broadcast to all available apis\n        for attempt_number in range(0, MAX_BROADCAST_ATTEMPTS):\n            for method_provider in service_provider_methods('broadcast_tx',\n                                                            get_providers_for_chain(bitcoin_chain, bitcoind)):\n                try:\n                    tx_id = method_provider(tx)\n                    if tx_id:\n                        logging.info('Broadcasting succeeded with method_provider=%s, txid=%s', str(method_provider),\n                                     tx_id)\n                        if final_tx_id and final_tx_id != tx_id:\n                            logging.error(\n                                'This should never happen; fail and investigate if it does. Got conflicting tx_ids=%s and %s. Hextx=%s',\n                                final_tx_id, tx_id, tx.as_hex())\n                            raise Exception('Got conflicting tx_ids.')\n                        final_tx_id = tx_id\n                except Exception as e:\n                    logging.warning('Caught exception trying provider %s. Trying another. Exception=%s',\n                                    str(method_provider), e)\n                    last_exception = e\n            # At least 1 provider succeeded, so return\n            if final_tx_id:\n                return final_tx_id\n            else:\n                logging.warning('Broadcasting failed. Waiting before retrying. This is attempt number %d',\n                                attempt_number)\n                time.sleep(BROADCAST_RETRY_INTERVAL)\n        logging.error('Failed broadcasting through all providers')\n        logging.error(last_exception, exc_info=True)\n        raise BroadcastError(last_exception)", "response": "Broadcasts a transaction through all available APIs and returns the ID of the transaction that was broadcasted."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the size of the transaction in bytes.", "response": "def calculate_raw_tx_size(num_inputs, num_outputs):\n    \"\"\"\n    To calculate the transaction size:\n        tx_size = (num_inputs * BYTES_PER_INPUT) + (num_outputs * BYTES_PER_OUTPUT) + FIXED_EXTRA_BYTES +/- num_inputs\n\n    We'll choose the upper bound, so:\n        +/- num_inputs  =>  + num_inputs\n\n    Equation and constants are taken from http://bitcoin.stackexchange.com/a/3011.\n\n    :param num_inputs:\n    :param num_outputs:\n    :return:\n    \"\"\"\n    tx_size = (num_inputs * BYTES_PER_INPUT) + (num_outputs *\n                                                BYTES_PER_OUTPUT) + FIXED_EXTRA_BYTES + num_inputs\n    return tx_size"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_transaction_output(address, output_value):\n    bitcoin_address = CBitcoinAddress(address)\n    tx_out = CMutableTxOut(output_value, bitcoin_address.to_scriptPubKey())\n    return tx_out", "response": "Create a single transaction output"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verify_transaction(signed_hextx, op_return_value):\n    logging.info('verifying op_return value for transaction')\n    op_return_hash = signed_hextx[-72:-8]\n    result = (op_return_value == op_return_hash)\n    if not result:\n        error_message = 'There was a problem verifying the transaction'\n        raise UnverifiedTransactionError(error_message)\n    logging.info('verified OP_RETURN')", "response": "Verify OP_RETURN field in transaction\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the transaction fee for a given set of inputs and outputs.", "response": "def calculate_tx_fee(tx_cost_constants, num_inputs, num_outputs):\n    \"\"\"\n    The course grained (hard-coded value) of something like 0.0001 BTC works great for standard transactions\n    (one input, one output). However, it will cause a lag in more complex or large transactions. So we calculate the\n    raw tx fee, then take the max of the default fee and the refined cost to ensure prompt processing.\n\n    The transaction fee is:\n        fee = satoshi_per_byte * tx_size\n    :param tx_cost_constants:\n    :param num_inputs:\n    :param num_outputs:\n    :return:\n    \"\"\"\n    tx_size = calculate_raw_tx_size_with_op_return(num_inputs, num_outputs)\n    tx_fee = tx_cost_constants.satoshi_per_byte * tx_size\n    return max(tx_fee, tx_cost_constants.get_recommended_fee_coin())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef verify_message(address, message, signature):\n    bitcoin_message = BitcoinMessage(message)\n    verified = VerifyMessage(address, bitcoin_message, signature)\n    return verified", "response": "Verify the message was signed by the address\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nverifies the signature of a single object in a node.", "response": "def verify_signature(uid, signed_cert_file_name, issuing_address):\n    \"\"\"\n    Verify the certificate signature matches the expected. Double-check the uid field in the certificate and use\n    VerifyMessage to confirm that the signature in the certificate matches the issuing_address.\n\n    Raises error is verification fails.\n\n    Raises UnverifiedSignatureError if signature is invalid\n\n    :param uid:\n    :param signed_cert_file_name:\n    :param issuing_address:\n    :return:\n    \"\"\"\n\n    logging.info('verifying signature for certificate with uid=%s:', uid)\n    with open(signed_cert_file_name) as in_file:\n        signed_cert = in_file.read()\n        signed_cert_json = json.loads(signed_cert)\n        to_verify = uid\n        signature = signed_cert_json['signature']\n        verified = verify_message(issuing_address, to_verify, signature)\n        if not verified:\n            error_message = 'There was a problem with the signature for certificate uid={}'.format(uid)\n            raise UnverifiedSignatureError(error_message)\n\n        logging.info('verified signature')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a merkle proof to the certificate_metadata.", "response": "def add_proof(self, certificate_metadata, merkle_proof):\n        \"\"\"\n        :param certificate_metadata:\n        :param merkle_proof:\n        :return:\n        \"\"\"\n        certificate_json = self._get_certificate_to_issue(certificate_metadata)\n        certificate_json['signature'] = merkle_proof\n\n        with open(certificate_metadata.blockchain_cert_file_name, 'w') as out_file:\n            out_file.write(json.dumps(certificate_json))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a generator that yields the certificate data from the certificate handler.", "response": "def get_certificate_generator(self):\n        \"\"\"\n        Returns a generator (1-time iterator) of certificates in the batch\n        :return:\n        \"\"\"\n\n        for cert in self.certificates_to_issue:\n            data_to_issue = self.certificate_handler.get_byte_array_to_issue(cert)\n            yield data_to_issue"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare_batch(self):\n        \n        for cert in self.certificates_to_issue:\n            self.certificate_handler.validate_certificate(cert)\n\n        self.merkle_tree.populate(self.get_certificate_generator())\n        logging.info('here is the op_return_code data: %s', b2h(self.merkle_tree.get_blockchain_data()))\n        return self.merkle_tree.get_blockchain_data()", "response": "Prepares the blockchain for the next batch operation"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprepare the blockchain for the next batch operation.", "response": "def prepare_batch(self):\n        \"\"\"\n        Propagates exception on failure\n        :return: byte array to put on the blockchain\n        \"\"\"\n\n        # validate batch\n        for _, metadata in self.certificates_to_issue.items():\n            self.certificate_handler.validate_certificate(metadata)\n\n        # sign batch\n        with FinalizableSigner(self.secret_manager) as signer:\n            for _, metadata in self.certificates_to_issue.items():\n                self.certificate_handler.sign_certificate(signer, metadata)\n\n        self.merkle_tree.populate(self.get_certificate_generator())\n        logging.info('here is the op_return_code data: %s', b2h(self.merkle_tree.get_blockchain_data()))\n        return self.merkle_tree.get_blockchain_data()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_certificate_generator(self):\n        for uid, metadata in self.certificates_to_issue.items():\n            data_to_issue = self.certificate_handler.get_byte_array_to_issue(metadata)\n            yield data_to_issue", "response": "Returns a generator that yields the certificate data from the certificate handler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the balance in wei with some inspiration from PyWallet", "response": "def get_balance(self, address, api_token):\n        \"\"\"\n        returns the balance in wei\n        with some inspiration from PyWallet\n        \"\"\"\n        broadcast_url = self.base_url + '?module=account&action=balance'\n        broadcast_url += '&address=%s' % address\n        broadcast_url += '&tag=latest'\n        if api_token:\n            '&apikey=%s' % api_token\n        response = requests.get(broadcast_url)\n        if int(response.status_code) == 200:\n            balance = int(response.json().get('result', None))\n            logging.info('Balance check succeeded: %s', response.json())\n            return balance\n        raise BroadcastError(response.text)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_address_nonce(self, address, api_token):\n        broadcast_url = self.base_url + '?module=proxy&action=eth_getTransactionCount'\n        broadcast_url += '&address=%s' % address\n        broadcast_url += '&tag=latest'\n        if api_token:\n            '&apikey=%s' % api_token\n        response = requests.get(broadcast_url, )\n        if int(response.status_code) == 200:\n            # the int(res, 0) transforms the hex nonce to int\n            nonce = int(response.json().get('result', None), 0)\n            logging.info('Nonce check went correct: %s', response.json())\n            return nonce\n        else:\n            logging.info('response error checking nonce')\n        raise BroadcastError('Error checking the nonce through the Etherscan API. Error msg: %s', response.text)", "response": "Get the nonce of this address"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a variable to a tensor.", "response": "def _dense_var_to_tensor(self, dtype=None, name=None, as_ref=False):\n    \"\"\"Converts a variable to a tensor.\"\"\"\n    # pylint: disable=protected-access\n    if _enclosing_tpu_context() is None:\n      if hasattr(self._primary_var, '_dense_var_to_tensor'):\n        return self._primary_var._dense_var_to_tensor(dtype, name, as_ref)\n      else:\n        return ops.convert_to_tensor(self._primary_var)\n    # pylint: enable=protected-access\n    if dtype is not None and dtype != self.dtype:\n      return NotImplemented\n    if as_ref:\n      return self.handle\n    else:\n      return self.read_value()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_layer_stack(layers=gin.REQUIRED, num_layers=6):\n  return LayerStack([cls() for cls in layers] * num_layers)", "response": "Configurable layer stack.\n\n  Args:\n    layers: a list of subclasses of TransformerLayer\n    num_layers: an integer\n  Returns:\n    a LayerStack"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngins - configurable bitransformer constructor.", "response": "def make_bitransformer(\n    input_vocab_size=gin.REQUIRED,\n    output_vocab_size=gin.REQUIRED,\n    layout=None,\n    mesh_shape=None):\n  \"\"\"Gin-configurable bitransformer constructor.\n\n  In your config file you need to set the encoder and decoder layers like this:\n  encoder/make_layer_stack.layers = [\n    @transformer_layers.SelfAttention,\n    @transformer_layers.DenseReluDense,\n  ]\n  decoder/make_layer_stack.layers = [\n    @transformer_layers.SelfAttention,\n    @transformer_layers.EncDecAttention,\n    @transformer_layers.DenseReluDense,\n  ]\n\n  Args:\n    input_vocab_size: a integer\n    output_vocab_size: an integer\n    layout: optional - an input to mtf.convert_to_layout_rules\n      Some layers (e.g. MoE layers) cheat by looking at layout and mesh_shape\n    mesh_shape: optional - an input to mtf.convert_to_shape\n      Some layers (e.g. MoE layers) cheat by looking at layout and mesh_shape\n  Returns:\n    a Bitransformer\n  \"\"\"\n  with gin.config_scope(\"encoder\"):\n    encoder = Unitransformer(\n        layer_stack=make_layer_stack(),\n        input_vocab_size=input_vocab_size,\n        output_vocab_size=None,\n        autoregressive=False,\n        name=\"encoder\",\n        layout=layout,\n        mesh_shape=mesh_shape)\n  with gin.config_scope(\"decoder\"):\n    decoder = Unitransformer(\n        layer_stack=make_layer_stack(),\n        input_vocab_size=output_vocab_size,\n        output_vocab_size=output_vocab_size,\n        autoregressive=True,\n        name=\"decoder\",\n        layout=layout,\n        mesh_shape=mesh_shape)\n  return Bitransformer(encoder, decoder)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_states(self, n):\n    return self.states[len(self.new_states):len(self.new_states) + n]", "response": "Get the next n recurrent states."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the read state that was written in first_part mode.", "response": "def get_constant_state(self):\n    \"\"\"Read state that was written in \"first_part\" mode.\n\n    Returns:\n      a structure\n    \"\"\"\n    ret = self.constant_states[self.next_constant_state]\n    self.next_constant_state += 1\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nonpadding(self):\n    if self.sequence_id is None:\n      return None\n    if self.sequence_id == 1:\n      return 1\n    else:\n      return mtf.cast(\n          mtf.not_equal(self.sequence_id, 0), self.activation_dtype)", "response": "Tensor with zeros in padding positions and ones elsewhere."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing tokenwise elementwise accuracy.", "response": "def token_accuracy(labels, outputs):\n  \"\"\"Compute tokenwise (elementwise) accuracy.\n\n  Args:\n    labels: ground-truth labels, shape=(batch, seq_length)\n    outputs: predicted tokens, shape=(batch, seq_length)\n  Returns:\n    Two ops, one for getting the current average accuracy and another for\n    updating the running average estimate.\n  \"\"\"\n  weights = tf.to_float(tf.not_equal(labels, 0))\n  return tf.metrics.accuracy(labels, outputs, weights=weights)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sequence_accuracy(labels, outputs):\n  # A sequence is correct if all of the non-padded entries are correct\n  all_correct = tf.reduce_all(\n      tf.logical_or(tf.equal(labels, outputs), tf.equal(labels, 0)), axis=-1\n  )\n  return tf.metrics.mean(all_correct)", "response": "Compute the sequence - level accuracy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_operation_input_names(self, operation_name):\n    for input_tensor in self._name_to_operation(operation_name).inputs:\n      yield input_tensor.name", "response": "Generates the names of all input tensors of an operation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the names of all output tensors of an operation.", "response": "def get_operation_output_names(self, operation_name):\n    \"\"\"Generates the names of all output tensors of an operation.\n\n    Args:\n      operation_name: a string, the name of an operation in the graph.\n\n    Yields:\n      a string, the name of an output tensor.\n    \"\"\"\n    for output_tensor in self._name_to_operation(operation_name).outputs:\n      yield output_tensor.name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the tf. TensorShape of a tensor.", "response": "def get_tensor_shape(self, tensor_name):\n    \"\"\"The tf.TensorShape of a tensor.\n\n    Args:\n      tensor_name: string, the name of a tensor in the graph.\n\n    Returns:\n      a tf.TensorShape\n    \"\"\"\n    tensor = self._name_to_tensor(tensor_name)\n\n    if isinstance(tensor, mtf.Tensor):\n      return tf.TensorShape(tensor.shape.to_integer_list)\n    else:  # tf.Tensor\n      return tensor.shape"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_tensor_num_entries(self, tensor_name, partial_layout=None,\n                             mesh_dimension_to_size=None):\n    \"\"\"The number of entries in a tensor.\n\n    If partial_layout is specified, then mesh_dimension_to_size must also be. In\n    this case, the number of entries on a single device is returned.\n\n    Args:\n      tensor_name: a string, name of a tensor in the graph.\n      partial_layout: an optional {string: string}, from MTF dimension name to\n          mesh dimension name.\n      mesh_dimension_to_size: an optional {string: int}, from mesh dimension\n          name to size.\n\n    Returns:\n      an integer\n    \"\"\"\n    shape = self.get_tensor_shape(tensor_name)\n    # We don't have to worry about divisiblity issues because Mesh TensorFlow\n    # only allows evenly divisible assignments.\n    num_entries = 1\n    for dim in shape.dims:\n      num_entries = num_entries * dim.value\n\n    if not partial_layout:\n      return num_entries\n\n    for mtf_dimension_name in self.get_tensor_mtf_dimension_names(tensor_name):\n      if mtf_dimension_name not in partial_layout:\n        continue\n      mesh_dimension_name = partial_layout[mtf_dimension_name]\n      mesh_dimension_size = mesh_dimension_to_size[mesh_dimension_name]\n      num_entries = int(math.ceil(num_entries / mesh_dimension_size))\n\n    return num_entries", "response": "Returns the number of entries in a tensor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_tensor_size(self, tensor_name, partial_layout=None,\n                      mesh_dimension_to_size=None):\n    \"\"\"The size of a tensor in bytes.\n\n    If partial_layout is specified, then mesh_dimension_to_size must also be. In\n    this case, the size on a single device is returned.\n\n    Args:\n      tensor_name: a string, name of a tensor in the graph.\n      partial_layout: an optional {string: string}, from MTF dimension name to\n          mesh dimension name.\n      mesh_dimension_to_size: an optional {string: int}, from mesh dimension\n          name to size.\n\n    Returns:\n      an integer\n    \"\"\"\n    return (self.get_tensor_dtype(tensor_name).size *\n            self.get_tensor_num_entries(tensor_name, partial_layout,\n                                        mesh_dimension_to_size))", "response": "Returns the size of a tensor in bytes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_tensor_device(self, tensor_name):\n    tensor = self._name_to_tensor(tensor_name)\n    if isinstance(tensor, tf.Tensor):\n      return tensor.device\n    else:  # mtf.Tensor\n      return None", "response": "Returns the device name of a tensor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_tensor_on_canonical_device(self, tensor_name):\n    device = self.get_tensor_device(tensor_name)\n    return not device or device == self.canonical_device", "response": "Determines if the tensor is on the canonical device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_operation_device(self, operation_name):\n    operation = self._name_to_operation(operation_name)\n    if isinstance(operation, tf.Operation):\n      return operation.device\n    else:  # mtf.Operation\n      return None", "response": "Returns the device name of an operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_tensor_mtf_dimension_names(self, tensor_name):\n    tensor = self._name_to_tensor(tensor_name)\n    if isinstance(tensor, mtf.Tensor):\n      return tensor.shape.dimension_names\n    else:  # tf.Tensor\n      return []", "response": "Returns the names of Mesh TensorFlow dimensions associated with a tensor."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the names of Mesh TensorFlow dimensions associated with an operation.", "response": "def get_operation_mtf_dimension_names(self, operation_name):\n    \"\"\"The Mesh TensorFlow dimensions associated with an operation.\n\n    Args:\n      operation_name: a string, name of an operation in the graph.\n\n    Returns:\n      a set(string), the names of Mesh TensorFlow dimensions.\n    \"\"\"\n    mtf_dimension_names = set()\n    for tensor_name in self.get_operation_input_names(operation_name):\n      mtf_dimension_names.update(self.get_tensor_mtf_dimension_names(\n          tensor_name))\n    for tensor_name in self.get_operation_output_names(operation_name):\n      mtf_dimension_names.update(self.get_tensor_mtf_dimension_names(\n          tensor_name))\n    return mtf_dimension_names"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_tensor_final(self, tensor_name):\n    tensor = self._name_to_tensor(tensor_name)\n    self._final_tensors.add(tensor)", "response": "Denotes a tensor as a final output of the computation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a boolean indicating whether a tensor is a final output of the computation.", "response": "def is_tensor_final(self, tensor_name):\n    \"\"\"Whether a tensor is a final output of the computation.\n\n    Args:\n      tensor_name: a string, name of a tensor in the graph.\n\n    Returns:\n      a boolean indicating whether the tensor was a final output.\n    \"\"\"\n    tensor = self._name_to_tensor(tensor_name)\n    return tensor in self._final_tensors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_cost_graph(self, devices=None):\n    cost_graph_def = cost_graph_pb2.CostGraphDef()\n\n    for i, operation_name in enumerate(self.get_all_operation_names()):\n      node = cost_graph_def.node.add(\n          name=operation_name,\n          device=self.get_operation_device(operation_name),\n          id=i)\n      for input_name in self.get_operation_input_names(operation_name):\n        id1, id2 = self._tensor_name_to_ids[input_name]\n        node.input_info.add(preceding_node=id1, preceding_port=id2)\n\n      for output_name in self.get_operation_output_names(operation_name):\n        tensor_device = self.get_tensor_device(output_name)\n        # devices = [] is not the same as None, and tensor_device = '' is also\n        # not the same as None.\n        if devices is None or tensor_device is None or tensor_device in devices:\n          node.output_info.add(\n              size=self.get_tensor_num_entries(output_name),\n              alias_input_port=-1,\n              dtype=self.get_tensor_dtype(output_name).as_datatype_enum,\n              shape=self.get_tensor_shape(output_name).as_proto(),\n          )\n        else:\n          node.output_info.add(\n              size=0,\n              alias_input_port=-1,\n              dtype=self.get_tensor_dtype(output_name).as_datatype_enum,\n          )\n\n        # NOTE(joshuawang): Unfortunately, the CostGraphDef protobuf has final\n        # operations, not tensors. As a result, we have to declare any operation\n        # that outputs a final tensor as final, which may expand the final set\n        # of tensors to keep in memory. This issue also arises in the scheduler\n        # code we will interface with.\n        if self.is_tensor_final(output_name):\n          node.is_final = True\n\n    return cost_graph_def", "response": "Computes a CostGraphDef protobuf based on this graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the in - memory contents of the operations in the given schedule.", "response": "def compute_memory_contents_under_schedule(self, schedule):\n    \"\"\"The in-memory tensors present when executing each operation in schedule.\n\n    Simulates running operations in the order given by a schedule. Keeps track\n    of the tensors in memory at every point in time, and outputs a list (one\n    entry for each point in time) of all sets of all memory contents (i.e. a\n    frozenset of strings) ever seen in this execution.\n\n    It is assumed (but not checked) that schedule is a valid topological sort of\n    the operations in this graph.\n\n    Args:\n      schedule: A list of integer ids; the order to run operations in.\n\n    Returns:\n      a list of frozenset of strings, where the ith entry describes the tensors\n      in memory when executing operation i (where schedule[i] is an index into\n      get_all_operation_names()).\n    \"\"\"\n    out_degree = self._compute_initial_out_degree()\n\n    curr_memory_contents = set()\n    memory_contents_for_each_operation = []\n\n    for operation_id in schedule:\n      operation_name = self._operations[operation_id].name\n      # Allocate new memory to perform the computation at this node.\n      for output_name in self.get_operation_output_names(operation_name):\n        curr_memory_contents.add(output_name)\n      memory_contents_for_each_operation.append(frozenset(curr_memory_contents))\n\n      # Free any tensors which are no longer needed.\n      for output_name in self.get_operation_output_names(operation_name):\n        if out_degree[output_name] == 0:\n          curr_memory_contents.remove(output_name)\n      for input_name in self.get_operation_input_names(operation_name):\n        out_degree[input_name] -= 1\n        if out_degree[input_name] == 0:\n          curr_memory_contents.remove(input_name)\n\n    return memory_contents_for_each_operation"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _initialize_operations(self):\n    if isinstance(self._graph, tf.Graph):\n      return self._graph.get_operations()\n    elif isinstance(self._graph, mtf.Graph):\n      return self._graph.operations\n    else:\n      raise TypeError('Graph is not tf.Graph or mtf.Graph: {}'\n                      .format(type(self._graph)))", "response": "Initializer for _operations.\n\n    Raises:\n      TypeError: _graph is not a tf.Graph or mtf.Graph.\n\n    Returns:\n      a list of (tf.Operation or mtf.Operation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _initialize_operation_name_to_id(self):\n    operation_name_to_id = {}\n    for i, operation in enumerate(self._operations):\n      operation_name_to_id[operation.name] = i\n    return operation_name_to_id", "response": "Initializer for _operation_name_to_id.\n\n    Returns:\n      a {string: int}, mapping operation names to their index in _operations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _initialize_tensor_name_to_ids(self):\n    tensor_name_to_ids = {}\n    for i, operation in enumerate(self._operations):\n      for j, tensor in enumerate(operation.outputs):\n        tensor_name_to_ids[tensor.name] = (i, j)\n    return tensor_name_to_ids", "response": "Initializer for _tensor_name_to_ids.\n\n    Returns:\n      a {string: (int, int)}, mapping the name of tensor T to the index of T's\n          operation in _operations and T's index in T's operation's outputs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the tensor with the given name.", "response": "def _name_to_tensor(self, tensor_name):\n    \"\"\"The tensor with the given name.\n\n    Args:\n      tensor_name: a string, name of a tensor in the graph.\n\n    Returns:\n      a tf.Tensor or mtf.Tensor\n    \"\"\"\n    id1, id2 = self._tensor_name_to_ids[tensor_name]\n    return self._operations[id1].outputs[id2]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the initial out degree of the graph.", "response": "def _compute_initial_out_degree(self):\n    \"\"\"The number of operations which use each tensor as input.\n\n    Returns:\n      a {string, int} mapping tensor name to the number of operations which use\n      it as input, or one plus that quantity if the tensor is final.\n    \"\"\"\n    out_degree = collections.defaultdict(int)\n\n    # Pretend that final tensors have an additional degree so they are not\n    # freed.\n    for tensor_name in self.get_all_tensor_names():\n      if self.is_tensor_final(tensor_name):\n        out_degree[tensor_name] = 1\n\n    for operation_name in self.get_all_operation_names():\n      for input_name in self.get_operation_input_names(operation_name):\n        out_degree[input_name] += 1\n\n    return out_degree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef layer_norm(x, dim, epsilon=1e-6, name=\"layer_prepostprocess\"):\n  with tf.variable_scope(name + \"/layer_norm\"):\n    scale = mtf.get_variable(\n        x.mesh,\n        \"layer_norm_scale\",\n        mtf.Shape([dim]),\n        initializer=tf.ones_initializer(),\n        activation_dtype=x.dtype)\n    bias = mtf.get_variable(\n        x.mesh,\n        \"layer_norm_bias\",\n        mtf.Shape([dim]),\n        initializer=tf.zeros_initializer(),\n        activation_dtype=x.dtype)\n    reduced_shape = x.shape - dim\n    mean = mtf.reduce_mean(x, output_shape=reduced_shape)\n    variance = mtf.reduce_mean(mtf.square(x - mean), output_shape=reduced_shape)\n    norm_x = (x - mean) * mtf.rsqrt(variance + epsilon)\n    return norm_x * scale + bias", "response": "Layer normalization over dimension dim."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbatches normalization. Args: x: a mtf.Tensor whose shape contains [batch_dim, ..., dim] is_training: a boolean, whether mode is training. momentum: a floating point number, specifying batch norm decay value. epsilon: a floating point number. init_zero: a boolean, whether to initialize scale with 0's or 1's. name: a string. variable scope. Returns: a mtf.Tensor with same shape as x.", "response": "def batch_norm(x, is_training, momentum, epsilon=1e-9,\n               init_zero=False, name=None):\n  \"\"\"Batch normalization.\n\n  Args:\n    x: a mtf.Tensor whose shape contains [batch_dim, ..., dim]\n    is_training: a boolean, whether mode is training.\n    momentum: a floating point number, specifying batch norm decay value.\n    epsilon: a floating point number.\n    init_zero: a boolean, whether to initialize scale with 0's or 1's.\n    name: a string. variable scope.\n\n  Returns:\n    a mtf.Tensor with same shape as x.\n  \"\"\"\n  with tf.variable_scope(name, default_name=\"batch_norm\", values=[x]):\n    if init_zero:\n      gamma_initializer = tf.zeros_initializer()\n    else:\n      gamma_initializer = tf.ones_initializer()\n\n    norm_dim = x.shape.dims[0:3]\n    reduced_shape = x.shape - norm_dim\n\n    scale = mtf.get_variable(\n        x.mesh,\n        \"batch_norm_scale\",\n        reduced_shape,\n        initializer=gamma_initializer,\n        activation_dtype=x.dtype)\n    bias = mtf.get_variable(\n        x.mesh,\n        \"batch_norm_bias\",\n        reduced_shape,\n        initializer=tf.zeros_initializer(),\n        activation_dtype=x.dtype)\n\n    moving_mean = mtf.get_variable(\n        x.mesh, \"moving_mean\", reduced_shape,\n        initializer=tf.random_normal_initializer(stddev=1.0),\n        activation_dtype=x.dtype,\n        trainable=False)\n    moving_variance = mtf.get_variable(\n        x.mesh, \"moving_variance\",\n        reduced_shape, initializer=tf.ones_initializer(),\n        activation_dtype=x.dtype,\n        trainable=False)\n\n    # At training time, calculate mean and variance and normalize across batch\n    # dim.\n    if is_training:\n      mean = mtf.reduce_mean(x, output_shape=reduced_shape)\n      variance = mtf.reduce_mean(\n          mtf.square(x - mean), output_shape=reduced_shape)\n\n      norm_x = (x - mean) * mtf.rsqrt(variance + epsilon)\n\n      # Update running mean and running variance.\n      moving_mean = mtf.assign(\n          moving_mean, momentum * moving_mean + (1-momentum) * mean)\n      moving_variance = mtf.assign(\n          moving_variance,\n          momentum * moving_variance + (1 - momentum) * variance)\n    else:\n      # At eval and test time, use the running mean and variance.\n      norm_x = (x - moving_mean) * mtf.rsqrt(moving_variance + epsilon)\n\n    return (norm_x * scale) + bias"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sigmoid_cross_entropy_with_logits(logits, targets):\n  if logits.shape != targets.shape:\n    raise ValueError(\n        \"logits shape must equal targets shape\"\n        \"logits=%s targets=%s\" % (logits.to_string, targets.to_string))\n  x = logits\n  z = targets\n  return mtf.relu(x) - x * z + mtf.log(1 + mtf.exp(-mtf.abs(x)))", "response": "Sigmoid cross - entropy loss."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dense_relu_dense(x,\n                     hidden_channels,\n                     dropout=0.0,\n                     dropout_broadcast_dims=None,\n                     master_dtype=tf.float32,\n                     slice_dtype=tf.float32, name=None):\n  \"\"\"Hidden layer with ReLU activation followed by linear projection.\n\n  The output has the same number of channels as the input.\n\n  Args:\n    x: a mtf.Tensor\n    hidden_channels: a mtf.Dimension - channels in the hidden layer\n    dropout: an optional float\n    dropout_broadcast_dims: an optional list of mtf.Dimension\n    master_dtype: a tf.dtype\n    slice_dtype: a tf.dtype\n    name: an optional string\n\n  Returns:\n    a mtf.Tensor with the same shape as x.\n  \"\"\"\n  with tf.variable_scope(name, default_name=\"dense_relu_dense\"):\n    io_channels = x.shape.dims[-1]\n    h = dense(x, hidden_channels,\n              use_bias=False, activation=mtf.relu,\n              master_dtype=master_dtype, slice_dtype=slice_dtype, name=\"wi\")\n    if dropout != 0.0:\n      h = mtf.dropout(h, 1.0 - dropout,\n                      noise_shape=h.shape - dropout_broadcast_dims)\n    return dense(h, io_channels, use_bias=False, activation=None,\n                 master_dtype=master_dtype, slice_dtype=slice_dtype,\n                 name=\"wo\")", "response": "Hidden layer with ReLU activation followed by linear projection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhaloing exchange for keys and values for Local 1D attention.", "response": "def local_1d_halo_exchange(k, v, num_w_blocks, w_dim, mask_right):\n  \"\"\"Halo exchange for keys and values for Local 1D attention.\"\"\"\n  if num_w_blocks is not None:\n    if mask_right:\n      k = mtf.left_halo_exchange(k, num_w_blocks, w_dim, w_dim.size)\n      v = mtf.left_halo_exchange(v, num_w_blocks, w_dim, w_dim.size)\n    else:\n      k = mtf.halo_exchange(k, num_w_blocks, w_dim, w_dim.size)\n      v = mtf.halo_exchange(v, num_w_blocks, w_dim, w_dim.size)\n  else:\n    if mask_right:\n      k = mtf.pad(k, [w_dim, None], w_dim.name)\n      v = mtf.pad(v, [w_dim, None], w_dim.name)\n    else:\n      k = mtf.pad(k, [w_dim, w_dim], w_dim.name)\n      v = mtf.pad(v, [w_dim, w_dim], w_dim.name)\n  return k, v"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef masked_local_attention_1d_incremental(x,\n                                          prev_k,\n                                          prev_v,\n                                          step_num,\n                                          master_dtype=None,\n                                          slice_dtype=None,\n                                          params=None,\n                                          name=None):\n  \"\"\"Incremental local self-attention (one decode step).\n\n  Incremental version of masked_local_attention_1d()\n\n  Args:\n    x: a mtf.Tensor with shape [batch..., io_channels]\n    prev_k: mtf.Tensor with shape\n       [batch..., heads, window_length, kv_channels]\n    prev_v: mtf.Tensor with shape\n       [batch..., heads, window_length, kv_channels]\n    step_num: mtf Scalar with dtype tf.int32\n    master_dtype: a tf.dtype (deprecated)\n    slice_dtype: a tf.dtype (deprecated)\n    params: a quadruple of Tensors (see multihead_attention_params())\n    name: an optional string.\n\n  Returns:\n    y: A mtf.Tensor with shape [batch..., io_channels]\n    new_k: mtf.Tensor with shape\n       [batch..., heads, window_length, kv_channels]\n    new_v: mtf.Tensor with shape\n       [batch..., heads, window_length, kv_channels]\n\n  Raises:\n    ValueError: if the dimensions do not match.\n  \"\"\"\n  batch_dims = x.shape.dims[:-1]\n  io_channels = x.shape.dims[-1]\n  heads, window_length, kv_channels = prev_k.shape.dims[-3:]\n  with tf.variable_scope(name, default_name=\"masked_local_attention_1d\"):\n    if params is None:\n      wq, wk, wv, wo = multihead_attention_vars(\n          x.mesh, heads, io_channels, kv_channels,\n          master_dtype, slice_dtype, x.dtype)\n    else:\n      wq, wk, wv, wo = params\n    q = mtf.einsum([x, wq], mtf.Shape(batch_dims + [heads, kv_channels]))\n    k = mtf.einsum([x, wk], mtf.Shape(batch_dims + [heads, kv_channels]))\n    v = mtf.einsum([x, wv], mtf.Shape(batch_dims + [heads, kv_channels]))\n    current_position = mtf.equal(\n        mtf.range(x.mesh, window_length, dtype=tf.int32),\n        mtf.mod(step_num, window_length.size))\n    k = mtf.where(current_position, k, prev_k, output_shape=prev_k.shape)\n    v = mtf.where(current_position, v, prev_v, output_shape=prev_v.shape)\n    o = dot_product_attention(q, k, v, mask=None)\n    y = mtf.einsum([o, wo], x.shape)\n    return y, k, v", "response": "1D local self - attention."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef local_2d_halo_exchange(k, v, num_h_blocks, h_dim,\n                           num_w_blocks, w_dim, mask_right):\n  \"\"\"Halo exchange for keys and values for Local 2D attention.\"\"\"\n  for blocks_dim, block_size_dim, halo_size in [\n      (num_h_blocks, h_dim, h_dim.size),\n      (num_w_blocks, w_dim, w_dim.size)]:\n    # shape of k is [num_h_blocks, num_w_blocks, h_dim, w_dim, kv_channels]\n    if halo_size > 0:\n      if blocks_dim is not None:\n        if mask_right:\n          k = mtf.left_halo_exchange(k, blocks_dim, block_size_dim, halo_size)\n          v = mtf.left_halo_exchange(v, blocks_dim, block_size_dim, halo_size)\n        else:\n          k = mtf.halo_exchange(k, blocks_dim, block_size_dim, halo_size)\n          v = mtf.halo_exchange(v, blocks_dim, block_size_dim, halo_size)\n      else:\n        if mask_right:\n          k = mtf.pad(k, [halo_size, None], block_size_dim.name)\n          v = mtf.pad(v, [halo_size, None], block_size_dim.name)\n        else:\n          k = mtf.pad(k, [halo_size, halo_size], block_size_dim.name)\n          v = mtf.pad(v, [halo_size, halo_size], block_size_dim.name)\n  return k, v", "response": "Exchange keys and values for Local 2D attention."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndeprecate version of multihead_attention_params with combine = True.", "response": "def multihead_attention_vars(\n    mesh, heads, io_channels, kv_channels,\n    master_dtype, slice_dtype, activation_dtype):\n  \"\"\"Deprecated version of multihead_attention_params with combine=True.\"\"\"\n  return multihead_attention_params(\n      mesh, heads, io_channels, kv_channels,\n      mtf.VariableDType(master_dtype, slice_dtype, activation_dtype),\n      combine=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef multihead_attention_params(mesh, heads, io_channels, kv_channels,\n                               variable_dtype, combine=False):\n  \"\"\"Create Parameters for Multihead Attention.\n\n  If the combine flag is set to True, then we create only one variable\n  which stacks together all of the parameters.  Otherwise, we create four\n  separate variables.\n\n  Args:\n    mesh: a Mesh\n    heads: a Dimension\n    io_channels: a Dimension\n    kv_channels: a Dimension\n    variable_dtype: a mtf.VariableDType\n    combine: a boolean\n\n  Returns:\n    wq: a Tensor with shape [heads, io_channels, kv_channels]\n    wk: a Tensor with shape [heads, io_channels, kv_channels]\n    wv: a Tensor with shape [heads, io_channels, kv_channels]\n    wo: a Tensor with shape [heads, io_channels, kv_channels]\n  \"\"\"\n  qkvo = mtf.Dimension(\"qkvo\", 4)\n  qk_stddev = (io_channels.size ** -0.5) * (kv_channels.size ** -0.25)\n  v_stddev = io_channels.size ** -0.5\n  # TODO(noam): should be: o_stddev = (kv_channels.size * heads.size) ** -0.5\n  #   verify that this still works and change it.\n  o_stddev = (io_channels.size * heads.size) ** -0.5\n  if combine:\n    def qkvo_initializer(shape,\n                         dtype=None,\n                         partition_info=None,\n                         verify_shape=None):\n      del partition_info, verify_shape\n      return tf.random_normal(shape, dtype=dtype) * tf.reshape(\n          tf.cast([qk_stddev, qk_stddev, v_stddev, o_stddev],\n                  dtype or tf.float32), [4, 1, 1, 1])\n    var = mtf.get_variable(\n        mesh, \"qkvo\", mtf.Shape([qkvo, heads, io_channels, kv_channels]),\n        initializer=qkvo_initializer, dtype=variable_dtype)\n    return mtf.unstack(var, qkvo)\n  else:\n    return [mtf.get_variable(\n        mesh, name, mtf.Shape([heads, io_channels, kv_channels]),\n        initializer=tf.random_normal_initializer(stddev=stddev),\n        dtype=variable_dtype) for name, stddev in zip(\n            [\"q\", \"k\", \"v\", \"o\"],\n            [qk_stddev, qk_stddev, v_stddev, o_stddev])]", "response": "Create Parameters for Multihead Attention."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndot - product attention.", "response": "def dot_product_attention(q,\n                          k,\n                          v,\n                          mask,\n                          dropout=0.0,\n                          dropout_broadcast_dims=None,\n                          extra_logit=None):\n  \"\"\"Dot-product attention.\n\n  Args:\n    q: Tensor with shape [...., length_q, depth_k]. Typically leading dimensions\n      are [batch, heads].\n    k: Tensor with shape [..., length_kv, depth_k]. Leading dimensions must\n      match with q.\n    v: Tensor with shape [..., length_kv, depth_v] Leading dimensions must\n      match with q.\n    mask: mask Tensor (see attention_mask())\n    dropout: a float.\n    dropout_broadcast_dims: an optional list of mtf.Dimension\n    extra_logit: an optional scalar or tensor\n\n  Returns:\n    Tensor with shape [..., length_q, depth_v].\n  \"\"\"\n  length_kv = k.shape.dims[-2]\n  logits_shape = mtf.Shape(q.shape.dims[:-1] + [length_kv])\n  logits = mtf.einsum([q, k], logits_shape)\n  if mask is not None:\n    logits += mask\n  weights = mtf.softmax(logits, length_kv, extra_logit=extra_logit)\n  if dropout != 0.0:\n    weights = mtf.dropout(\n        weights, 1.0 - dropout,\n        noise_shape=weights.shape - dropout_broadcast_dims)\n  depth_v = v.shape.dims[-1]\n  outputs_shape = mtf.Shape(q.shape.dims[:-1] + [depth_v])\n  outputs = mtf.einsum([weights, v], outputs_shape)\n  return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multihead_self_attention_incremental(query_antecedent,\n                                         prev_k,\n                                         prev_v,\n                                         step_num,\n                                         master_dtype,\n                                         slice_dtype,\n                                         name=\"multihead_attention\"):\n  \"\"\"Incremental self-attention (one decode step).\n\n  In order to use only one variable containing the four weight matrices\n  packed together, we insist that the query and memory antecedents have the\n  same dimensionality (io_channels) and that the keys and values have the\n  same dimensionality (kv_channels).\n\n  Args:\n    query_antecedent: a mtf.Tensor with shape [batch..., io_channels]\n    prev_k: mtf.Tensor with shape [batch..., heads, memory_length, kv_channels]\n    prev_v: mtf.Tensor with shape [batch..., heads, memory_length, kv_channels]\n    step_num: mtf Scalar with dtype tf.int32\n    master_dtype: a tf.dtype\n    slice_dtype: a tf.dtype\n    name: an optional string.\n\n  Returns:\n    y: A mtf.Tensor with shape [batch..., io_channels]\n    new_k: mtf.Tensor with shape [batch..., heads, memory_length, kv_channels]\n    new_v: mtf.Tensor with shape [batch..., heads, memory_length, kv_channels]\n\n  Raises:\n    ValueError: if the dimensions do not match.\n  \"\"\"\n  batch_dims = query_antecedent.shape.dims[:-1]\n  io_channels = query_antecedent.shape.dims[-1]\n  heads, memory_length, kv_channels = prev_k.shape.dims[-3:]\n  with tf.variable_scope(name, default_name=\"multihead_attention\"):\n    wq, wk, wv, wo = multihead_attention_vars(\n        query_antecedent.mesh, heads, io_channels, kv_channels,\n        master_dtype, slice_dtype, query_antecedent.dtype)\n    memory_antecedent = query_antecedent\n    q = mtf.einsum(\n        [query_antecedent, wq],\n        mtf.Shape(batch_dims + [heads, kv_channels]))\n    k = mtf.einsum(\n        [memory_antecedent, wk],\n        mtf.Shape(batch_dims + [heads, kv_channels]))\n    v = mtf.einsum(\n        [memory_antecedent, wv],\n        mtf.Shape(batch_dims + [heads, kv_channels]))\n    k = prev_k + mtf.multiply(\n        k, mtf.one_hot(step_num, memory_length, dtype=prev_k.dtype),\n        output_shape=prev_k.shape)\n    v = prev_v + mtf.multiply(\n        v, mtf.one_hot(step_num, memory_length, dtype=prev_v.dtype),\n        output_shape=prev_v.shape)\n\n    mask = mtf.cast(\n        mtf.greater(mtf.range(\n            query_antecedent.mesh, memory_length, dtype=tf.int32), step_num),\n        q.dtype) * -1e9\n    o = dot_product_attention(q, k, v, mask)\n    y = mtf.einsum([o, wo], query_antecedent.shape)\n    return y, k, v", "response": "Incremental self - attention for a multihead dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbiases for encoder - decoder attention. Returns a tensor with shape [... length_dim ).", "response": "def attention_mask_ignore_padding(inputs, dtype=tf.float32):\n  \"\"\"Bias for encoder-decoder attention.\n\n  Args:\n    inputs: a mtf.Tensor with shape [..., length_dim]\n    dtype: a tf.dtype\n\n  Returns:\n    a mtf.Tensor with shape [..., memory_length_dim]\n  \"\"\"\n  inputs = rename_length_to_memory_length(inputs)\n  return mtf.cast(mtf.equal(inputs, 0), dtype) * -1e9"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef attention_mask_autoregressive(query_pos, dtype=tf.float32):\n  memory_pos = rename_length_to_memory_length(query_pos)\n  return mtf.cast(mtf.less(query_pos, memory_pos), dtype) * -1e9", "response": "Bias for self - attention where attention to the right is disallowed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbiases for attention where attention between segments is disallowed.", "response": "def attention_mask_same_segment(\n    query_segment, memory_segment=None, dtype=tf.float32):\n  \"\"\"Bias for attention where attention between segments is disallowed.\n\n  Args:\n    query_segment: a mtf.Tensor with shape [..., length_dim]\n    memory_segment: a mtf.Tensor with shape [..., memory_length_dim]\n    dtype: a tf.dtype\n\n  Returns:\n    a mtf.Tensor with shape [..., length_dim, memory_length_dim]\n  \"\"\"\n  memory_segment = rename_length_to_memory_length(\n      memory_segment or query_segment)\n  return mtf.cast(mtf.not_equal(query_segment, memory_segment), dtype) * -1e9"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbiasing for attention for local blocks where attention to right is disallowed.", "response": "def attention_bias_local_block(mesh, block_length, memory_length,\n                               dtype=tf.int32):\n  \"\"\"Bias for attention for local blocks where attention to right is disallowed.\n\n  Create the bias matrix by using two separate masks, one for the memory part\n  which doesn't overlap with the query and second which interacts with the query\n  and should be disallowed to look to the right of the current query position.\n\n  Args:\n    mesh: a MeshTensorflow object\n    block_length: a mtf.Dimension\n    memory_length: a mtf.Dimension\n    dtype: a tf.dtype\n\n  Returns:\n    a mtf.Tensor with shape [block_length, memory_length]\n  \"\"\"\n  memory_length = mtf.Dimension(memory_length.name, block_length.size)\n  memory_mask = mtf.zeros(mesh, [block_length, memory_length], dtype=dtype)\n\n  mask = mtf.cast(mtf.less(mtf.range(mesh, block_length, dtype=dtype),\n                           mtf.range(mesh, memory_length, dtype=dtype)),\n                  dtype=dtype)\n  mask = mtf.cast(\n      mtf.concat([memory_mask, mask], memory_length.name),\n      dtype=tf.float32) * -1e9\n  return mask"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbiasing for attention for local blocks where attention to right is disallowed.", "response": "def attention_bias_local_2d_block(mesh,\n                                  h_dim,\n                                  w_dim,\n                                  memory_h_dim,\n                                  memory_w_dim,\n                                  dtype=tf.int32):\n  \"\"\"Bias for attention for local blocks where attention to right is disallowed.\n\n  Create the bias matrix by using two separate masks, one for the memory part\n  which doesn't overlap with the query and second which interacts with the query\n  and should be disallowed to look to the right of the current query position.\n\n  Args:\n    mesh: a MeshTensorflow object\n    h_dim: a mtf.Dimension\n    w_dim: a mtf.Dimension\n    memory_h_dim: a mtf.Dimension\n    memory_w_dim: a mtf.Dimension\n    dtype: a tf.dtype\n\n  Returns:\n    a mtf.Tensor with shape [block_length, memory_length]\n  \"\"\"\n  memory_height = mtf.Dimension(memory_h_dim.name, h_dim.size)\n  memory_width = mtf.Dimension(memory_w_dim.name, w_dim.size)\n  mask_top_visible = mtf.zeros(mesh, [h_dim, memory_height], dtype=dtype)\n  mask_left_visible = mtf.zeros(mesh, [w_dim, memory_width], dtype=dtype)\n  mask_query = mtf.greater(\n      mtf.range(mesh, memory_height, dtype=tf.int32),\n      mtf.range(mesh, memory_width, dtype=dtype))\n  width_mask = mtf.concat([mask_left_visible, mask_query], memory_width.name)\n  mask = mtf.cast(\n      mtf.concat([mask_top_visible, width_mask], memory_height.name),\n      dtype=tf.float32) * -1e9\n  return mask"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef multiplicative_jitter(x, epsilon=1e-2):\n  if epsilon == 0:\n    return x\n  return x * mtf.random_uniform(\n      x.mesh, x.shape, minval=1.0 - epsilon, maxval=1.0+epsilon, dtype=x.dtype)", "response": "Multiply values by a random number between 1 - epsilon and 1 + epsilon."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompresses by taking group means. Arguments x a Tensor containing x. shape.", "response": "def compress_mean(x, dim, compression_factor):\n  \"\"\"Compress by taking group means.\n\n  Args:\n    x: a Tensor\n    dim: a dimension in x.shape\n    compression_factor: an integer\n\n  Returns:\n    a Tensor\n  \"\"\"\n  dims = x.shape.dims\n  pos = dims.index(dim)\n  compressed_dim = mtf.Dimension(dim.name, dim.size // compression_factor)\n  compression_factor_dim = mtf.Dimension(\n      \"compression_factor\", compression_factor)\n  new_shape = (\n      dims[:pos] + [compressed_dim, compression_factor_dim] + dims[pos + 1:])\n  x = mtf.reshape(x, new_shape)\n  x = mtf.reduce_mean(x, reduced_dim=compression_factor_dim)\n  return x"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the attention parameters for Transformer Layers.", "response": "def attention_params(context,\n                     kv_dim,\n                     num_heads,\n                     num_memory_heads=0,\n                     shared_kv=False):\n  \"\"\"Attention Parameters for Transformer Layers.\n\n  The num_heads argument indicates the number of read-heads.\n\n  For the familiar behavior described in \"Attention Is All You Need\", set\n  num_memory_heads=0.\n\n  If num_memory_heads==1, then there is only a single write-head, and multiple\n  read-heads.  This leads to faster incremental decoding, since the\n  recurrent state is smaller\n\n  If num_memory_heads > 1, then num_memory_heads indicates the number of\n  write-heads.  A fraction of the read-heads read each write-head.\n  num_memory_heads must divide num_heads. This behavior has not yet been tested.\n\n  Args:\n    context: a transformer.Context\n    kv_dim: a dimension (for key and value channels)\n    num_heads: an integer\n    num_memory_heads: an optional integer\n    shared_kv: a boolean\n  Returns:\n    an attention.AttentionParams object\n  \"\"\"\n  if num_heads == 1:\n    query_heads_dims = None\n    memory_heads_dims = None\n  elif num_memory_heads == 0:\n    query_heads_dims = [mtf.Dimension(\"heads\", num_heads)]\n    memory_heads_dims = query_heads_dims\n  elif num_memory_heads == 1:\n    query_heads_dims = [mtf.Dimension(\"heads\", num_heads)]\n    memory_heads_dims = None\n  else:\n    if num_heads % num_memory_heads != 0:\n      raise ValueError(\"num_memory_heads must divide num_heads\")\n    memory_heads_dims = [mtf.Dimension(\"heads\", num_memory_heads)]\n    query_heads_dims = memory_heads_dims + [\n        mtf.Dimension(\"query_heads\", num_heads // num_memory_heads)]\n  return attention.AttentionParams(\n      context.mesh,\n      query_input_dim=context.model_dim,\n      memory_input_dim=context.model_dim,\n      output_dim=context.model_dim,\n      key_dim=kv_dim,\n      value_dim=kv_dim,\n      query_heads_dims=query_heads_dims,\n      memory_heads_dims=memory_heads_dims,\n      variable_dtype=context.variable_dtype,\n      shared_kv=shared_kv)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_metric_fns(metric_names, labels, outputs):\n  metric_fns = {}\n  for metric_name in metric_names:\n    metric_fn_name = metric_name.split(\"/\")[-1]\n    if hasattr(metrics, metric_fn_name):\n      metric_fn = getattr(metrics, metric_fn_name)\n      metric_fns[metric_name] = metric_fn(labels, outputs)\n    else:\n      raise ValueError(\"Metric {} is not implemented\".format(metric_fn_name))\n\n  return metric_fns", "response": "Returns a dictionary of metric name to metric function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef minimize_peak_memory(graph, scheduler_alg):\n  if scheduler_alg == 'NAIVE':\n    return _minimize_peak_memory_naive(graph)\n  elif scheduler_alg == 'LIST':\n    return _minimize_peak_memory_list(graph)\n  else:\n    raise NotImplementedError('{} is not a scheduler algorithm. It should be '\n                              'one of NAIVE or LIST.'\n                              .format(scheduler_alg))", "response": "Computes a schedule to minimize peak memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nminimizing the peak memory list.", "response": "def _minimize_peak_memory_list(graph):\n  \"\"\"Computes schedule according to the greedy list heuristic.\n\n  Greedy list heuristic: schedule the operation which results in the most bytes\n  of memory being (immediately) freed.\n  TODO(joshuawang): Experiment with tiebreaking by preferring more successors.\n\n  Args:\n    graph: an mtf.auto_mtf.graph_interface.GraphInterface.\n\n  Returns:\n    an iterable of integers representing the schedule.\n  \"\"\"\n  schedule = []\n  bytes_freed = {}  # {operation_name: bytes freed}\n  users_of = collections.defaultdict(set)  # {tensor_name: set(operation_name)}\n  in_degree = collections.defaultdict(int)  # {operation_name: in degree}\n  operation_id = {}  # {operation_name: id}\n  # We want an updatable priority queue, so we use the following workaround:\n  # docs.python.org/2/library/heapq.html#priority-queue-implementation-notes\n  priority_queue = []  # (negative bytes freed, operation name)\n\n  # Set up the (greedy) topological sort.\n  for i, operation_name in enumerate(graph.get_all_operation_names()):\n    operation_id[operation_name] = i\n\n    for input_name in graph.get_operation_input_names(operation_name):\n      # Note that in _HybridGraphInterface, an operation may use a tensor twice,\n      # but we deduplicate (with respect to in_degree) so that we can later use\n      # users_of to decrement in_degree.\n      if operation_name in users_of[input_name]:\n        continue\n      users_of[input_name].add(operation_name)\n      in_degree[operation_name] += 1\n\n  for operation_name in graph.get_all_operation_names():\n    bytes_freed[operation_name] = 0\n    # For each input, this operation frees memory if it is the final consumer.\n    for input_name in graph.get_operation_input_names(operation_name):\n      if len(users_of[input_name]) == 1 and not graph.is_tensor_final(\n          input_name):\n        bytes_freed[operation_name] += graph.get_tensor_size(input_name)\n    # For each output, this operation will require additional bytes of memory\n    # (hence negative bytes freed).\n    for output_name in graph.get_operation_output_names(operation_name):\n      # If the output is used (or is final), then it eats memory.\n      if users_of[output_name] or graph.is_tensor_final(output_name):\n        bytes_freed[operation_name] -= graph.get_tensor_size(output_name)\n\n  for operation_name in graph.get_all_operation_names():\n    if in_degree[operation_name] == 0:\n      heapq.heappush(priority_queue,\n                     (-bytes_freed[operation_name], operation_name))\n\n  # Do the (greedy) topological sort.\n  while priority_queue:\n    neg_bytes_freed, operation_name = heapq.heappop(priority_queue)\n    if bytes_freed[operation_name] != -neg_bytes_freed:\n      continue\n    schedule.append(operation_id[operation_name])\n    bytes_freed[operation_name] = None\n\n    for output_name in graph.get_operation_output_names(operation_name):\n      for other_operation_name in users_of[output_name]:\n        in_degree[other_operation_name] -= 1\n        if in_degree[other_operation_name] == 0:\n          heapq.heappush(priority_queue,\n                         (-bytes_freed[other_operation_name],\n                          other_operation_name))\n\n    for input_name in graph.get_operation_input_names(operation_name):\n      if operation_name not in users_of[input_name]:\n        # Used twice by this operation and hence already removed.\n        continue\n      users_of[input_name].remove(operation_name)\n      if len(users_of[input_name]) != 1 or graph.is_tensor_final(output_name):\n        continue\n      (other_operation_name,) = users_of[input_name]\n      bytes_freed[other_operation_name] += graph.get_tensor_size(\n          input_name)\n      if in_degree[other_operation_name] > 0:\n        continue\n      # Push another copy into the priority queue with our updated value.\n      # The original copy will be ignored since it does not match bytes_freed.\n      heapq.heappush(priority_queue, (-bytes_freed[other_operation_name],\n                                      other_operation_name))\n\n  return schedule"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef layout(mtf_graph, mesh_shape, mtf_outputs=()):\n  mesh_shape = mtf.convert_to_shape(mesh_shape)\n  estimator = memory_estimator.MemoryEstimator(mtf_graph, mesh_shape,\n                                               mtf_outputs)\n  optimizer = layout_optimizer.LayoutOptimizer(estimator)\n  return mtf.convert_to_layout_rules(optimizer.solve())", "response": "Compute layout rules based on a computational graph and mesh shape."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef adafactor_decay_rate_adam(beta2):\n  t = tf.cast(tf.train.get_or_create_global_step(), tf.float32) + 1.0\n  decay = beta2 * (1.0 - tf.pow(beta2, t - 1.0)) / (1.0 - tf.pow(beta2, t))\n  return decay", "response": "Second - moment decay rate like Adam subsuming the correction factor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an Adafactor optimizer based on model hyperparameters.", "response": "def adafactor_optimizer_from_hparams(hparams, lr):\n  \"\"\"Create an Adafactor optimizer based on model hparams.\n\n  Args:\n    hparams: model hyperparameters\n    lr: learning rate scalar.\n  Returns:\n    an AdafactorOptimizer\n  Raises:\n    ValueError: on illegal values\n  \"\"\"\n  if hparams.optimizer_adafactor_decay_type == \"Adam\":\n    decay_rate = adafactor_decay_rate_adam(\n        hparams.optimizer_adafactor_beta2)\n  elif hparams.optimizer_adafactor_decay_type == \"pow\":\n    decay_rate = adafactor_decay_rate_pow(\n        hparams.optimizer_adafactor_memory_exponent)\n  else:\n    raise ValueError(\"unknown optimizer_adafactor_decay_type\")\n  return AdafactorOptimizer(\n      multiply_by_parameter_scale=(\n          hparams.optimizer_adafactor_multiply_by_parameter_scale),\n      learning_rate=lr,\n      decay_rate=decay_rate,\n      beta1=hparams.optimizer_adafactor_beta1,\n      clipping_threshold=hparams.optimizer_adafactor_clipping_threshold,\n      factored=hparams.optimizer_adafactor_factored)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply gradients to variables. Call this function externally instead of apply_grad.", "response": "def apply_grads(self, grads, variables):\n    \"\"\"Apply gradients to variables.\n\n    Call this function externally instead of apply_grad().  This causes the\n    operations to be combined, which is necessary for stacking variables\n    see mtf.rewrite_stack_variables().\n\n    Args:\n      grads: a list of Tensor\n      variables: a list of Variables\n    Returns:\n      a list of Operations\n    \"\"\"\n    ops = []\n    for grad, var in zip(grads, variables):\n      ops.extend(self.apply_grad(grad, var))\n    if not ops:\n      return ops\n    return variables[0].graph.combine_assignments(ops)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _factored_dims(self, shape):\n    if not self._factored or shape.ndims < 2:\n      return None\n    sorted_dims = sorted(shape.dims, key=lambda d: -d.size)\n    if sorted_dims[1].size < self._min_dim_size_to_factor:\n      return None\n    return sorted_dims[:2]", "response": "Returns a list of 2D Dimensions to factor over."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a boolean indicating whether this MTF dimension may be assigned to this Mesh TensorFlow dimension.", "response": "def is_valid_assignment(self, mtf_dimension_name, mesh_dimension_name):\n    \"\"\"Whether this MTF dimension may be assigned to this mesh dimension.\n\n    Args:\n      mtf_dimension_name: string, the name of a Mesh TensorFlow dimension.\n      mesh_dimension_name: string, the name of a mesh dimension.\n\n    Returns:\n      A boolean indicating whether the assignment is valid.\n    \"\"\"\n    return ((mtf_dimension_name in self._splittable_mtf_dimension_names) and\n            (self._mtf_dimension_name_to_size_gcd[mtf_dimension_name] %\n             self._mesh_dimension_name_to_size[mesh_dimension_name] == 0))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes the set of all the splittable Mesh TensorFlow dimensions.", "response": "def _initialize_splittable_dimensions(self, mtf_graph):\n    \"\"\"Initializer for self._splittable_mtf_dimension_names.\n\n    Args:\n      mtf_graph: an mtf.Graph.\n\n    Returns:\n      A set(string) of the names of Mesh TensorFlow dimensions that may be\n      assigned in a layout.\n    \"\"\"\n    all_mtf_dimension_names = set()  # set(string)\n    for mtf_operation in mtf_graph.operations:\n      for mtf_tensor in mtf_operation.outputs:\n        for mtf_dimension in mtf_tensor.shape.dims:\n          if not re.match(r\"_anonymous_\\d*\", mtf_dimension.name):\n            all_mtf_dimension_names.add(mtf_dimension.name)\n\n    unsplittable_mtf_dimension_names = set()  # set(string)\n    for mtf_operation in mtf_graph.operations:\n      unsplittable_mtf_dimension_names.update(mtf_operation.unsplittable_dims)\n\n    return all_mtf_dimension_names - unsplittable_mtf_dimension_names"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _initialize_mtf_dimension_name_to_size_gcd(self, mtf_graph):\n    mtf_dimension_name_to_size_gcd = {}\n    for mtf_operation in mtf_graph.operations:\n      for mtf_tensor in mtf_operation.outputs:\n        for mtf_dimension in mtf_tensor.shape.dims:\n          mtf_dimension_name_to_size_gcd[mtf_dimension.name] = fractions.gcd(\n              mtf_dimension_name_to_size_gcd.get(mtf_dimension.name,\n                                                 mtf_dimension.size),\n              mtf_dimension.size)\n\n    return mtf_dimension_name_to_size_gcd", "response": "Initializes the MTF dimensions to the greatest common divisor of all the sizes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the dict of names to sizes.", "response": "def _initialize_mesh_dimension_name_to_size(self, mesh_shape):\n    \"\"\"Initializer for self._mesh_dimension_name_to_size.\n\n    Args:\n      mesh_shape: an mtf.Shape.\n\n    Returns:\n      A {string: int} mapping mesh dimension names to their sizes.\n    \"\"\"\n    mesh_dimension_name_to_size = {}  # {string: int}\n    for mesh_dimension in mesh_shape.dims:\n      mesh_dimension_name_to_size[mesh_dimension.name] = mesh_dimension.size\n    return mesh_dimension_name_to_size"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the reduction of all Tensors and put the result everywhere.", "response": "def allreduce_ring_single_shard(xs, devices, reduction_fn_string=\"SUM\"):\n  \"\"\"Compute the reduction of all Tensors and put the result everywhere.\n\n  Performance-optimized for a ring of devices.\n\n  Args:\n    xs: a list of n tf.Tensors\n    devices: a list of strings\n    reduction_fn_string: \"SUM\" or \"MAX\"\n\n  Returns:\n    a list of n Tensors\n  Raises:\n    ValueError: if devices is not a list of n strings\n  \"\"\"\n  n = len(xs)\n  binary_reduction = mtf.binary_reduction_fn(reduction_fn_string)\n  assert len(devices) == n, \"devices must be a list of length len(xs)\"\n  if n == 1:\n    return xs\n  result = [None] * n\n  if n % 2 == 0:\n    left_center = n // 2 - 1\n    right_center = left_center + 1\n  else:\n    left_center = n // 2\n    right_center = left_center\n  left_sum = xs[0]\n  for i in xrange(1, left_center + 1):\n    with tf.device(devices[i]):\n      left_sum = binary_reduction(left_sum, xs[i])\n  right_sum = xs[n-1]\n  for i in reversed(xrange(left_center + 1, n - 1)):\n    with tf.device(devices[i]):\n      right_sum = binary_reduction(xs[i], right_sum)\n  with tf.device(devices[left_center]):\n    result[left_center] = binary_reduction(left_sum, right_sum)\n  if n % 2 == 0:\n    with tf.device(devices[right_center]):\n      result[right_center] = binary_reduction(left_sum, right_sum)\n  for i in reversed(xrange(left_center)):\n    with tf.device(devices[i]):\n      result[i] = tf.identity(result[i + 1])\n  for i in xrange(right_center + 1, n):\n    with tf.device(devices[i]):\n      result[i] = tf.identity(result[i - 1])\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the reduction of all Tensors and put the result everywhere.", "response": "def allreduce_ring(xs, devices, reduction_fn_string=\"SUM\"):\n  \"\"\"Compute the reduction of all Tensors and put the result everywhere.\n\n  Performance-optimized for a ring of devices.\n\n  Args:\n    xs: a list of n tf.Tensors\n    devices: a list of strings\n    reduction_fn_string: \"SUM\" or \"MAX\"\n\n  Returns:\n    a list of n Tensors\n  Raises:\n    ValueError: if devices is not a list of n strings\n  \"\"\"\n  n = len(xs)\n  if len(devices) != n:\n    raise ValueError(\"devices must be a list of length len(xs)\")\n  if n == 1:\n    return xs\n  shape = xs[0].shape.as_list()\n  # tf.logging.info(\"allreduce_ring shape = %s\" % shape)\n  size = None if None in shape else mtf.list_product(shape)\n  if size is None or size < 1024 or size % n != 0:\n    return allreduce_ring_single_shard(xs, devices, reduction_fn_string)\n\n  def _circular_shift(l, n):\n    n %= len(l)\n    return l[-n:] + l[:-n]\n  def _flatten_and_split(x):\n    # tf.reshape treats [-1] as a special value denoting 1D flattening.\n    return tf.split(tf.reshape(x, [-1]), n)\n  def _concat_and_reshape(xs):\n    return tf.reshape(tf.concat(xs, 0), shape)\n\n  # [device, shard]\n  x_split = mtf.parallel(devices, _flatten_and_split, xs)\n  x_split_t = mtf.transpose_list_of_lists(x_split)\n\n  y_split_t = []\n  for shard in xrange(n):\n    shard_xs = _circular_shift(x_split_t[shard], shard)\n    shard_devices = _circular_shift(devices, shard)\n    shard_ys = allreduce_ring_single_shard(\n        shard_xs, shard_devices, reduction_fn_string)\n    y_split_t.append(_circular_shift(shard_ys, -shard))\n  y_split = mtf.transpose_list_of_lists(y_split_t)\n  ys = mtf.parallel(devices, _concat_and_reshape, y_split)\n  return ys"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconcatenate all Tensors everywhere.", "response": "def allconcat_ring(xs, devices, concat_axis):\n  \"\"\"Concatenate all Tensors everywhere.\n\n  Performance-optimized for a ring of devices.\n\n  Args:\n    xs: a list of n tf.Tensors\n    devices: a list of n strings\n    concat_axis: an integer\n\n  Returns:\n    a list of n Tensors\n  \"\"\"\n  n = len(xs)\n  if n == 1:\n    return xs\n  # [target, source]\n  parts = [[xs[target] if target == source else None for source in xrange(n)]\n           for target in xrange(n)]\n  for distance in xrange(1, n // 2 + 1):\n    for target in xrange(n):\n      source = (target + distance) % n\n      if parts[target][source] is None:\n        with tf.device(devices[target]):\n          parts[target][source] = tf.identity(parts[(target + 1) % n][source])\n      source = (target - distance) % n\n      if parts[target][source] is None:\n        with tf.device(devices[target]):\n          parts[target][source] = tf.identity(parts[(target - 1) % n][source])\n  return mtf.parallel(devices, tf.concat, parts, axis=[concat_axis] * n)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef slicewise(self, fn, *inputs):\n    if fn == tf.add:\n      assert len(inputs) == 2\n      if isinstance(inputs[0], mtf.LazyAllreduceSum):\n        # sum of LazyAllreduceSum (keep delaying the allreduce)\n        return inputs[0] + inputs[1]\n    # convert all inputs to LaidOutTensor where possible\n    inputs = mtf.convert_args_to_laid_out_tensors(inputs)\n    inputs = [x.tensor_list if isinstance(x, self.LaidOutTensor)\n              else [x] * len(self.devices) for x in inputs]\n    ret = mtf.parallel(self.devices, fn, *inputs)\n    if isinstance(ret[0], tuple):\n      ret = mtf.transpose_list_of_lists(ret)\n      return tuple([self.LaidOutTensor(t) for t in ret])\n    else:\n      return self.LaidOutTensor(ret)", "response": "Execute a function in parallel on all slices."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall tf. Print on the first device", "response": "def Print(self, x, data, message, **kwargs):  # pylint: disable=invalid-name\n    \"\"\"call tf.Print.\n\n    Args:\n      x: a LaidOutTensor\n      data: a list of LaidOutTensor\n      message: a string\n      **kwargs: keyword arguments to tf.print\n    Returns:\n      a LaidOutTensor\n    \"\"\"\n    tf.logging.info(\"PlacementMeshImpl::Print\")\n    new_slices = x.tensor_list[:]\n    with tf.device(self._devices[0]):\n      new_slices[0] = tf.Print(\n          new_slices[0], [t for d in data for t in d.tensor_list],\n          message, **kwargs)\n    return self.LaidOutTensor(new_slices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngroups allreduce, (across the given dimensions). Args: x: a LaidOutTensor mesh_axes: a list of integers - the mesh dimensions to be reduced reduction_fn_string: \"SUM\" or \"MAX\" Returns: a LaidOutTensor", "response": "def allreduce(self, x, mesh_axes, reduction_fn_string):\n    \"\"\"Grouped allreduce, (across the given dimensions).\n\n    Args:\n      x: a LaidOutTensor\n      mesh_axes: a list of integers - the mesh dimensions to be reduced\n      reduction_fn_string: \"SUM\" or \"MAX\"\n    Returns:\n      a LaidOutTensor\n    \"\"\"\n    return self._collective_with_groups(\n        x, mesh_axes, functools.partial(\n            allreduce_ring, reduction_fn_string=reduction_fn_string))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef allconcat(self, x, mesh_axis, concat_axis):\n    return self._collective_with_groups(\n        x, [mesh_axis],\n        functools.partial(allconcat_ring, concat_axis=concat_axis))", "response": "Grouped allconcat with a single group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef alltoall(self, x, mesh_axis, split_axis, concat_axis):\n    return self._collective_with_groups(\n        x, [mesh_axis],\n        functools.partial(\n            alltoall_ring, split_axis=split_axis, concat_axis=concat_axis))", "response": "Group a LaidOutTensor into a single group of alltoall."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef receive(self, x, mesh_axis, source_pcoord):\n    x = x.to_laid_out_tensor()\n    shape = x.tensor_list[0].shape\n    dtype = x.tensor_list[0].dtype\n    def _collective_receive(tensor_list, device_list):\n      ret = []\n      for pcoord, device in enumerate(device_list):\n        with tf.device(device):\n          if source_pcoord[pcoord] is None:\n            ret.append(tf.zeros(shape, dtype))\n          else:\n            ret.append(tf.identity(tensor_list[source_pcoord[pcoord]]))\n      return ret\n    return self._collective_with_groups(\n        x, [mesh_axis], _collective_receive)", "response": "Collective receive in groups."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _collective_with_groups(self, x, mesh_axes, collective):\n    if not mesh_axes:\n      return x\n    x = x.to_laid_out_tensor()\n    if len(mesh_axes) == self.ndims:\n      return self.LaidOutTensor(collective(x.tensor_list, self._devices))\n    else:\n      groups = mtf.processor_groups(self.shape, mesh_axes)\n      ret = [None] * self.size\n      for g in groups:\n        inputs = [x.tensor_list[pnum] for pnum in g]\n        devices = [self._devices[pnum] for pnum in g]\n        reduced = collective(inputs, devices)\n        for pnum, y in zip(g, reduced):\n          ret[pnum] = y\n      return self.LaidOutTensor(ret)", "response": "Grouped collective with groups."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall a random tf operation on the LaidOutTensor.", "response": "def random(self, shape, tf_fn, kwargs):\n    \"\"\"Call a random tf operation (e.g. tf.random.uniform).\n\n    Args:\n      shape: a Shape\n      tf_fn: a function such as tf.random.uniform\n      kwargs: kwargs to pass to tf_fn, except for seed\n\n    Returns:\n      a LaidOutTensor\n    \"\"\"\n    slice_shape = self.slice_shape(shape)\n    op_seed = random.random()\n    def my_fn(pnum):\n      # seeds are necessary to make sure that slices that should have the\n      # same values actually do have the same values.\n      seed = hash(\"%s,%s\" % (op_seed, self.slice_begin(shape, pnum)))\n      return tf_fn(slice_shape, seed=seed, **kwargs)\n    return self.slicewise(my_fn, self.laid_out_pnum())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_to_tf_tensor(self, x, laid_out_x):\n    return self.combine_slices(laid_out_x.all_slices, x.shape)", "response": "Turn a Tensor into a tf. Tensor. laid_out_x"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_tf_tensor(self, x, tf_x):\n    return self.LaidOutTensor(self.make_slices(tf_x, x.shape))", "response": "Imports a tf. Tensor into a LaidOutTensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndots - product attention.", "response": "def attention(q,\n              k,\n              v,\n              memory_length_dim,\n              key_dim,\n              value_dim,\n              mask=None,\n              dropout_rate=0.0,\n              dropout_broadcast_dims=None,\n              extra_logit=None):\n  \"\"\"Dot-product attention - doesn't use positional dimensions.\n\n  key_dim is a Dimension representing the channels in the queries and keys\n  value_dim is a Dimension representing the channels in values\n  memory_length_dim is a Dimension representing the different key/value pairs.\n\n  Dimensions of q: other_query_dims + {key_dim}\n  Dimensions of k: other_memory_dims + {memory_length_dim, key_dim}\n  Dimensions of v: other_memory_dims + {memory_length_dim, value_dim}\n  other_memory_dims is a subset of other_query_dims\n\n  Typically, other_query_dims={batch, heads, length}\n  Typically, other_memory_dims={batch, heads}\n\n  Args:\n    q: a Tensor\n    k: a Tensor\n    v: a Tensor\n    memory_length_dim: a Dimension\n    key_dim: a Dimension\n    value_dim: a Dimension\n    mask: mask Tensor (see attention_mask())\n    dropout_rate: a float.\n    dropout_broadcast_dims: an optional list of mtf.Dimension\n    extra_logit: an optional scalar or tensor\n\n  Returns:\n    Tensor with shape q.shape - key_dim + value_dim\n  \"\"\"\n  logits = mtf.einsum([q, k], reduced_dims=[key_dim])\n  if mask is not None:\n    logits += mask\n  weights = mtf.softmax(logits, memory_length_dim, extra_logit=extra_logit)\n  if dropout_rate != 0.0:\n    weights = mtf.dropout(\n        weights, 1.0 - dropout_rate,\n        noise_shape=weights.shape - dropout_broadcast_dims)\n  outputs_shape = q.shape - key_dim + value_dim\n  outputs = mtf.einsum([weights, v], outputs_shape)\n  return outputs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_q(self, query_antecedent):\n    ret = mtf.einsum(\n        [query_antecedent, self.wq], reduced_dims=[self.query_input_dim])\n    if self.combine_dims:\n      ret = mtf.replace_dimensions(ret, ret.shape.dims[-1], self.q_dims)\n    return ret", "response": "Compute query Tensor q."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing key - value tensor kv.", "response": "def compute_kv(self, memory_antecedent):\n    \"\"\"Compute key/value Tensor kv.\n\n    Args:\n      memory_antecedent: a Tensor with dimensions\n        {memory_input_dim} + other_dims\n    Returns:\n      a Tensor with dimensions\n        memory_heads_dims + {key_dim} + other_dims\n    \"\"\"\n    if not self.shared_kv:\n      raise ValueError(\"compute_kv can only be called with shared_kv\")\n    ret = mtf.einsum(\n        [memory_antecedent, self.wkv], reduced_dims=[self.memory_input_dim])\n    if self.combine_dims:\n      ret = mtf.replace_dimensions(ret, ret.shape.dims[-1], self.k_dims)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute key Tensor k.", "response": "def compute_k(self, memory_antecedent):\n    \"\"\"Compute key Tensor k.\n\n    Args:\n      memory_antecedent: a Tensor with dimensions\n        {memory_input_dim} + other_dims\n    Returns:\n      a Tensor with dimensions\n        memory_heads_dims + {key_dim} + other_dims\n    \"\"\"\n    if self.shared_kv:\n      raise ValueError(\"compute_k cannot be called with shared_kv\")\n    ret = mtf.einsum(\n        [memory_antecedent, self.wk], reduced_dims=[self.memory_input_dim])\n    if self.combine_dims:\n      ret = mtf.replace_dimensions(ret, ret.shape.dims[-1], self.k_dims)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute value Tensor v.", "response": "def compute_v(self, memory_antecedent):\n    \"\"\"Compute value Tensor v.\n\n    Args:\n      memory_antecedent: a Tensor with dimensions\n        {memory_input_dim} + other_dims\n    Returns:\n      a Tensor with dimensions\n        memory_heads_dims + {value_dim} + other_dims\n    \"\"\"\n    if self.shared_kv:\n      raise ValueError(\"compute_v cannot be called with shared_kv\")\n    ret = mtf.einsum(\n        [memory_antecedent, self.wv], reduced_dims=[self.memory_input_dim])\n    if self.combine_dims:\n      ret = mtf.replace_dimensions(ret, ret.shape.dims[-1], self.v_dims)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing output of multihead attention.", "response": "def compute_output(self, o, output_shape=None):\n    \"\"\"Compute output of multihead attention.\n\n    Args:\n      o: a Tensor with dimensions\n         query_heads_dims + {value_dim} + other_dims\n      output_shape: an optional Shape\n    Returns:\n      a Tensor with shape:\n         {output_dim} + other_dims\n    \"\"\"\n    if self.combine_dims:\n      o = mtf.transpose(o, o.shape - self.o_dims + self.o_dims)\n      o = mtf.replace_dimensions(o, self.o_dims, self.wo.shape.dims[0])\n      reduced_dims = [self.wo.shape.dims[0]]\n    else:\n      reduced_dims = self.o_dims\n    return mtf.einsum(\n        [o, self.wo], output_shape=output_shape, reduced_dims=reduced_dims)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nencoding a tf. Scalar string to a tf. Tensor.", "response": "def encode_tf(self, s):\n    \"\"\"Encode a tf.Scalar string to a tf.Tensor.\n\n    This will be necessary for on-the-fly tokenization.\n\n    Args:\n      s: a tf.Scalar with dtype tf.string\n    Returns:\n      a 1d tf.Tensor with dtype tf.int32\n    \"\"\"\n    ids = subword_text_encoder_ops.subword_text_encoder_encode(\n        s, self._filepath)\n    # the c++ op apppends 1=EOS - drop it.\n    return ids[:-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a simple layer stack.", "response": "def simple_layer_stack(include_encdec_attention,\n                       num_layers=6,\n                       d_ff=2048,\n                       num_heads=8,\n                       d_kv=128,\n                       dropout_rate=0.1):\n  \"\"\"Create a layer stack.\n\n  Args:\n    include_encdec_attention: a boolean\n    num_layers: an integer\n    d_ff: an integer\n    num_heads: an integer\n    d_kv: an integer\n    dropout_rate: a float\n\n  Returns:\n    a LayerStack\n  \"\"\"\n  ret = []\n  for _ in xrange(num_layers):\n    ret.append(\n        transformer_layers.SelfAttention(\n            num_heads=num_heads,\n            key_value_size=d_kv,\n            attention_kwargs={\"dropout_rate\": dropout_rate}))\n    if include_encdec_attention:\n      ret.append(\n          transformer_layers.EncDecAttention(\n              num_heads=num_heads,\n              key_value_size=d_kv,\n              attention_kwargs={\"dropout_rate\": dropout_rate}))\n    ret.append(\n        transformer_layers.DenseReluDense(\n            hidden_size=d_ff,\n            dropout_rate=dropout_rate))\n  return transformer.LayerStack(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef toy_model(features, mesh):\n  batch_dim = mtf.Dimension('batch', FLAGS.batch_size)\n  io_dim = mtf.Dimension('io', FLAGS.io_size)\n\n  master_dtype = tf.as_dtype(FLAGS.master_dtype)\n  slice_dtype = tf.as_dtype(FLAGS.slice_dtype)\n  activation_dtype = tf.as_dtype(FLAGS.activation_dtype)\n\n  x = mtf.import_tf_tensor(mesh, features, mtf.Shape([batch_dim, io_dim]))\n  x = mtf.cast(x, activation_dtype)\n  h = x\n  for lnum in xrange(1, FLAGS.num_hidden_layers + 2):\n    if lnum + 1 == FLAGS.num_hidden_layers + 2:\n      # output layer\n      dim = io_dim\n    elif lnum % 2 == 0:\n      dim = mtf.Dimension('hidden_even', FLAGS.hidden_size)\n    else:\n      dim = mtf.Dimension('hidden_odd', FLAGS.hidden_size)\n    h = mtf.layers.dense(\n        h, dim,\n        use_bias=False,\n        master_dtype=master_dtype,\n        slice_dtype=slice_dtype,\n        name='layer_%d' % lnum)\n  y = h\n  g = tf.train.get_global_step()\n  if FLAGS.step_with_nan >= 0:\n    # Trigger NaN in the forward pass, this is used for testing whether\n    # MeshTensorFlow can handle occasional NaN value.\n    y += mtf.import_tf_tensor(\n        mesh,\n        tf.divide(\n            0.0,\n            tf.cond(tf.equal(g, FLAGS.step_with_nan), lambda: 0., lambda: 1.)),\n        mtf.Shape([]))\n\n  loss = mtf.reduce_mean(mtf.square(y - x))\n  return y, loss", "response": "A toy model implemented by mesh tensorlfow."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun a toy model on TPU.", "response": "def run_toy_model_tpu():\n  \"\"\"Run a toy model on TPU.\"\"\"\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n  iterations_per_loop = FLAGS.iterations\n  mesh_shape = mtf.convert_to_shape(FLAGS.mesh_shape)\n  config = tpu_config.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_steps=None,  # Disable the default saver\n      save_checkpoints_secs=None,  # Disable the default saver\n      log_step_count_steps=iterations_per_loop,\n      save_summary_steps=iterations_per_loop,\n      tpu_config=tpu_config.TPUConfig(\n          num_shards=mesh_shape.size,\n          iterations_per_loop=iterations_per_loop,\n          num_cores_per_replica=1,\n          per_host_input_for_training=tpu_config.InputPipelineConfig.BROADCAST))\n  classifier = tpu_estimator.TPUEstimator(\n      use_tpu=True,\n      model_fn=model_fn,\n      config=config,\n      train_batch_size=FLAGS.batch_size,\n      eval_batch_size=FLAGS.batch_size)\n  current_step = estimator_lib._load_global_step_from_checkpoint_dir(FLAGS.model_dir)  # pylint: disable=protected-access,line-too-long\n  logging.info('Current step %d', current_step)\n  if FLAGS.steps_per_checkpoint == 0:\n    classifier.train(input_fn=ToyModelInput(), max_steps=FLAGS.train_steps)\n    return\n  while current_step < FLAGS.train_steps:\n    next_checkpoint = min(current_step + FLAGS.steps_per_checkpoint,\n                          FLAGS.train_steps)\n    classifier.train(input_fn=ToyModelInput(), max_steps=next_checkpoint)\n    current_step = next_checkpoint\n    logging.info('Starting to evaluate.')\n    eval_results = classifier.evaluate(\n        input_fn=ToyModelInput(),\n        steps=156)  # since we have 10000 examples and batch_size = 64 per host\n    logging.info('Eval results: %s', eval_results)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mnist_model(image, labels, mesh):\n  batch_dim = mtf.Dimension(\"batch\", FLAGS.batch_size)\n  row_blocks_dim = mtf.Dimension(\"row_blocks\", 4)\n  col_blocks_dim = mtf.Dimension(\"col_blocks\", 4)\n  rows_dim = mtf.Dimension(\"rows_size\", 7)\n  cols_dim = mtf.Dimension(\"cols_size\", 7)\n\n  classes_dim = mtf.Dimension(\"classes\", 10)\n  one_channel_dim = mtf.Dimension(\"one_channel\", 1)\n\n  x = mtf.import_tf_tensor(\n      mesh, tf.reshape(image, [FLAGS.batch_size, 4, 7, 4, 7, 1]),\n      mtf.Shape(\n          [batch_dim, row_blocks_dim, rows_dim,\n           col_blocks_dim, cols_dim, one_channel_dim]))\n  x = mtf.transpose(x, [\n      batch_dim, row_blocks_dim, col_blocks_dim,\n      rows_dim, cols_dim, one_channel_dim])\n\n  # add some convolutional layers to demonstrate that convolution works.\n  fh_dim = mtf.Dimension(\"fh\", 9)\n  fw_dim = mtf.Dimension(\"fw\", 9)\n  filters1_dim = mtf.Dimension(\"filters1\", 16)\n  filters2_dim = mtf.Dimension(\"filters2\", 16)\n  kernel1 = mtf.get_variable(\n      mesh, \"kernel1\", [fh_dim, fw_dim, one_channel_dim, filters1_dim])\n  kernel2 = mtf.get_variable(\n      mesh, \"kernel2\", [fh_dim, fw_dim, filters1_dim, filters2_dim])\n\n  f1 = mtf.relu(mtf.conv2d_with_blocks(\n      x, kernel1, strides=[1, 1, 1, 1], padding=\"SAME\",\n      h_blocks_dim=row_blocks_dim, w_blocks_dim=col_blocks_dim))\n  f2 = mtf.relu(mtf.conv2d_with_blocks(\n      f1, kernel2, strides=[1, 1, 1, 1], padding=\"SAME\",\n      h_blocks_dim=row_blocks_dim, w_blocks_dim=col_blocks_dim))\n  x = mtf.reduce_mean(f2, reduced_dim=filters2_dim)\n\n  # add some fully-connected dense layers.\n  hidden_dim1 = mtf.Dimension(\"hidden1\", FLAGS.hidden_size)\n  hidden_dim2 = mtf.Dimension(\"hidden2\", FLAGS.hidden_size)\n\n  h1 = mtf.layers.dense(\n      x, hidden_dim1,\n      reduced_dims=x.shape.dims[-4:],\n      activation=mtf.relu, name=\"hidden1\")\n  h2 = mtf.layers.dense(\n      h1, hidden_dim2,\n      activation=mtf.relu, name=\"hidden2\")\n  logits = mtf.layers.dense(h2, classes_dim, name=\"logits\")\n  if labels is None:\n    loss = None\n  else:\n    labels = mtf.import_tf_tensor(\n        mesh, tf.reshape(labels, [FLAGS.batch_size]), mtf.Shape([batch_dim]))\n    loss = mtf.layers.softmax_cross_entropy_with_logits(\n        logits, mtf.one_hot(labels, classes_dim), classes_dim)\n    loss = mtf.reduce_mean(loss)\n  return logits, loss", "response": "The model.\n\n  Args:\n    image: tf.Tensor with shape [batch, 28*28]\n    labels: a tf.Tensor with shape [batch] and dtype tf.int32\n    mesh: a mtf.Mesh\n\n  Returns:\n    logits: a mtf.Tensor with shape [batch, 10]\n    loss: a mtf.Tensor with shape []"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_mnist():\n  mnist_classifier = tf.estimator.Estimator(\n      model_fn=model_fn,\n      model_dir=FLAGS.model_dir)\n\n  # Set up training and evaluation input functions.\n  def train_input_fn():\n    \"\"\"Prepare data for training.\"\"\"\n\n    # When choosing shuffle buffer sizes, larger sizes result in better\n    # randomness, while smaller sizes use less memory. MNIST is a small\n    # enough dataset that we can easily shuffle the full epoch.\n    ds = dataset.train(FLAGS.data_dir)\n    ds_batched = ds.cache().shuffle(buffer_size=50000).batch(FLAGS.batch_size)\n\n    # Iterate through the dataset a set number (`epochs_between_evals`) of times\n    # during each training session.\n    ds = ds_batched.repeat(FLAGS.epochs_between_evals)\n    return ds\n\n  def eval_input_fn():\n    return dataset.test(FLAGS.data_dir).batch(\n        FLAGS.batch_size).make_one_shot_iterator().get_next()\n\n  # Train and evaluate model.\n  for _ in range(FLAGS.train_epochs // FLAGS.epochs_between_evals):\n    mnist_classifier.train(input_fn=train_input_fn, hooks=None)\n    eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n    print(\"\\nEvaluation results:\\n\\t%s\\n\" % eval_results)", "response": "Run MNIST training and evaluation loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transformer_moe_layer_v1(\n    inputs, output_dim, hparams, train, variable_dtype,\n    layout=None, mesh_shape=None, nonpadding=None):\n  \"\"\"Local mixture of experts that works well on TPU.\n\n  Adapted from the paper https://arxiv.org/abs/1701.06538\n\n  Note: until the algorithm and inferface solidify, we pass in a hyperparameters\n  dictionary in order not to complicate the interface in mtf_transformer.py .\n  Once this code moves out of \"research\", we should pass the hyperparameters\n  separately.\n\n  Hyperparameters used:\n    hparams.moe_num_experts: number of experts\n    hparams.moe_hidden_size: size of hidden layer in each expert\n    hparams.moe_group_size: size of each \"group\" for gating purposes\n    hparams.moe_capacity_factor_train: a float\n    hparams.moe_capacity_factor_eval: a float\n    hparams.moe_gating: a string\n    + all hyperparmeters used by _top_2_gating()\n\n  The number of parameters in the gating network is:\n    (input_dim.size * hparams.num_experts) +\n\n  The number of parameters in the experts themselves is:\n    (hparams.num_experts\n     * (input_dim.size + output_dim.size)\n     * hparams.moe_hidden_size)\n\n  The input is n-dimensional: [<batch_and_length_dims>, input_dim], consisting\n  of the representations of all positions in a batch of sequences.\n\n  Each position of each sequence is sent to 0-2 experts.  The expert\n  choices and the combination weights are determined by a learned gating\n  function.\n\n  This function returns a small auxiliary loss that should be added to the\n  training loss of the model.  This loss helps to balance expert usage.\n  Without the loss, it is very likely that a few experts will be trained and\n  the rest will starve.\n\n  Several hacks are necessary to get around current TPU limitations:\n\n  - To ensure static shapes, we enforce (by truncation/padding)\n    that each sequence send the same number of elements to each expert.\n\n    It would make more sense to enforce this equality over the entire batch,\n    but due to our hacked-up gather-by-matmul implementation, we need to divide\n    the batch into \"groups\".  For each group, the same number of elements\n    are sent to each expert.\n\n  TODO(noam): Factor this code better.  We want to be able to substitute\n  different code for the experts themselves.\n\n  Args:\n    inputs: a mtf.Tensor with shape [<batch_dims...>, length_dim, input_dim]\n    output_dim: a mtf.Dimension (for Transformer, this is input_dim)\n    hparams: model hyperparameters\n    train: a boolean\n    variable_dtype: a mtf.VariableDType\n    layout: optional - an input to mtf.convert_to_layout_rules\n    mesh_shape: optional - an input to mtf.convert_to_shape\n    nonpadding: an optional Tensor with shape [<batch_dims>, length_dim]\n      and the same dtype as inputs, consisting of ones(nonpadding)\n      and zeros(padding).\n\n  Returns:\n    outputs: a Tensor with shape [<batch_dims...>, length_dim, output_dim]\n    loss: a mtf scalar\n\n  Raises:\n    ValueError: on unrecognized hparams.moe_gating\n  \"\"\"\n  orig_inputs = inputs\n  hidden_dim = mtf.Dimension(\"expert_hidden\", hparams.moe_hidden_size)\n  experts_dim = mtf.Dimension(\"experts\", hparams.moe_num_experts)\n\n  # We \"cheat\" here and look at the mesh shape and layout. This is to ensure\n  # that the number of groups is a multiple of the mesh dimension\n  # over which those groups are split.\n  orig_batch_dim, orig_length_dim, input_dim = orig_inputs.shape.dims\n\n  num_groups, group_size = _split_into_groups(\n      orig_batch_dim.size * orig_length_dim.size, hparams.moe_group_size,\n      mtf.tensor_dim_to_mesh_dim_size(layout, mesh_shape, orig_batch_dim))\n  group_size_dim = mtf.Dimension(\"group\", group_size)\n  batch_dim = mtf.Dimension(orig_batch_dim.name, num_groups)\n  inputs = mtf.reshape(inputs, [batch_dim, group_size_dim, input_dim])\n\n  # Each sequence sends expert_capacity positions to each expert.\n  if train:\n    capacity_factor = hparams.moe_capacity_factor_train\n  else:\n    capacity_factor = hparams.moe_capacity_factor_eval\n  expert_capacity = min(\n      group_size_dim.size,\n      int((group_size_dim.size * capacity_factor) / experts_dim.size))\n  expert_capacity_dim = mtf.Dimension(\"expert_capacity\", expert_capacity)\n\n  experts_dim_unsplit = mtf.Dimension(\"expert_unsplit\", experts_dim.size)\n  batch_dim_unsplit = mtf.Dimension(\"batch_unsplit\", batch_dim.size)\n  if nonpadding is not None:\n    nonpadding = mtf.zeros(inputs.mesh, [orig_batch_dim, orig_length_dim],\n                           dtype=inputs.dtype) + nonpadding\n    nonpadding = mtf.reshape(nonpadding, [batch_dim, group_size_dim])\n  if hparams.moe_gating == \"top_2\":\n    dispatch_tensor, combine_tensor, loss = _top_2_gating(\n        inputs=inputs,\n        outer_expert_dims=None,\n        experts_dim=experts_dim_unsplit,\n        expert_capacity_dim=expert_capacity_dim,\n        hparams=hparams,\n        train=train,\n        variable_dtype=variable_dtype,\n        importance=nonpadding)\n  else:\n    raise ValueError(\"unknown hparams.moe_gating=%s\" % hparams.moe_gating)\n\n  # put num_experts dimension first to make split easier in alltoall\n  expert_inputs = mtf.einsum([inputs, dispatch_tensor], mtf.Shape(\n      [experts_dim_unsplit, batch_dim, expert_capacity_dim, input_dim]))\n\n  expert_inputs = mtf.reshape(expert_inputs, mtf.Shape(\n      [experts_dim, batch_dim_unsplit, expert_capacity_dim, input_dim]))\n\n  # Now feed the expert inputs through the experts.\n  h = mtf.layers.dense(\n      expert_inputs, hidden_dim, expert_dims=[experts_dim],\n      activation=mtf.relu, use_bias=False,\n      variable_dtype=variable_dtype, name=\"wi\")\n  expert_output = mtf.layers.dense(\n      h, output_dim, expert_dims=[experts_dim], use_bias=False,\n      variable_dtype=variable_dtype,\n      name=\"wo\")\n\n  expert_output = mtf.reshape(expert_output, mtf.Shape(\n      [experts_dim_unsplit, batch_dim, expert_capacity_dim, input_dim]))\n\n  output = mtf.einsum([expert_output, combine_tensor], mtf.Shape(\n      [batch_dim, group_size_dim, output_dim]))\n\n  output = mtf.reshape(output, orig_inputs.shape.dims[:-1] + [output_dim])\n\n  return output, loss * hparams.moe_loss_coef", "response": "Local mixture of experts that works well on TPU."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint the solution associated with a CPA model.", "response": "def print_solution(model, solver):\n  \"\"\"Prints the solution associated with solver.\n\n  If solver has already had Solve() called on it, prints the solution. This\n  includes each variable and its assignment, along with the objective function\n  and its optimal value.\n  If solver has not had Solve() called on it, or there is no feasible solution,\n  this will probably crash.\n\n  Args:\n    model: A pywrapcp.CpModel object.\n    solver: A pywrapcp.CpSolver object.\n\n  Returns:\n    Nothing, but prints the solution associated with solver.\n  \"\"\"\n  model_proto = model.Proto()\n  response_proto = solver.ResponseProto()\n  variables_in_objective_map = {}\n  maximization = False\n  if model_proto.HasField('objective'):\n    objective = model_proto.objective\n    for i in range(len(objective.vars)):\n      variables_in_objective_map[objective.vars[i]] = objective.coeffs[i]\n    if objective.scaling_factor < 0.0:\n      maximization = True\n  variable_assignments = []\n  variables_in_objective = []\n  num_vars = len(model_proto.variables)\n  for var_index in range(num_vars):\n    if not model_proto.variables[var_index].name:\n      continue\n    variable_name = model_proto.variables[var_index].name\n    if var_index in variables_in_objective_map:\n      coefficient = variables_in_objective_map[var_index]\n      if coefficient:\n        if maximization:\n          coefficient *= -1\n        if coefficient < 0:\n          variables_in_objective.append(' - {} * {}'.format(\n              -coefficient, variable_name))\n        elif coefficient > 0:\n          variables_in_objective.append(' + {} * {}'.format(\n              coefficient, variable_name))\n    variable_assignments.append('  {} = {}\\n'.format(\n        variable_name, response_proto.solution[var_index]))\n  print(''.join(variable_assignments), end='')\n  # Strip the leading '+' if it exists.\n  if variables_in_objective and variables_in_objective[0][1] == '+':\n    variables_in_objective[0] = variables_in_objective[0][2:]\n  print('{}:{}'.format('Maximize' if maximization else 'Minimize',\n                       ''.join(variables_in_objective)))\n  print('Objective value: {}\\n'.format(solver.ObjectiveValue()))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _local_var_name(splittable_dimensions, assignment):\n  assignment_string = []\n  for splittable in sorted(splittable_dimensions):\n    if splittable in assignment:\n      assignment_string.append(\"{}:{}\".format(splittable,\n                                              assignment[splittable]))\n    else:\n      assignment_string.append(\"{}\".format(splittable))\n  return \"y_(\" + \",\".join(assignment_string) + \")\"", "response": "Returns the name of a local variable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating all ways to map splittable dimensions to mesh dimensions.", "response": "def _generate_assignments(splittable_dimensions, mesh_dimension_to_size):\n  \"\"\"Generates all ways to map splittable dimensions to mesh dimensions.\n\n  Args:\n    splittable_dimensions: a frozenset of the names of splittable dimensions.\n    mesh_dimension_to_size: a dictionary from mesh dimension name to size.\n\n  Returns:\n    A list of the valid assignments. Each assignment is a dict keyed by every\n        splittable dimension, whose value is either a mesh dimension or None.\n  \"\"\"\n  assignments = []\n  for assignment_size in six.moves.xrange(\n      1 + min(len(splittable_dimensions), len(mesh_dimension_to_size))):\n    for s_dims_chosen in itertools.combinations(splittable_dimensions,\n                                                assignment_size):\n      for m_dims_chosen in itertools.permutations(mesh_dimension_to_size,\n                                                  assignment_size):\n        assignments.append(dict(zip(s_dims_chosen, m_dims_chosen)))\n  return assignments"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute useful input data structures to ease IP construction.", "response": "def _preprocess_input(self):\n    \"\"\"Computing useful input data structures to ease IP construction.\"\"\"\n    # Compute the sets of MTF dimensions used in operations/tensors.\n\n    # a {string: frozenset(string)}, mapping operation name to MTF dimension\n    # names.\n    self._operation_name_to_mtf_dimension_set = {}\n    # a {string: frozenset(string)}, mapping tensor name to MTF dimension names.\n    self._tensor_name_to_mtf_dimension_set = {}\n\n    for operation_name in self._graph.get_all_operation_names():\n      self._operation_name_to_mtf_dimension_set[operation_name] = frozenset(\n          set(self._graph.get_operation_mtf_dimension_names(\n              operation_name)).intersection(\n                  self._layout_validator.splittable_mtf_dimension_names))\n    for tensor_name in self._graph.get_all_tensor_names():\n      self._tensor_name_to_mtf_dimension_set[tensor_name] = frozenset(\n          set(self._graph.get_tensor_mtf_dimension_names(tensor_name))\n          .intersection(self._layout_validator.splittable_mtf_dimension_names))\n\n    self._operation_mtf_dimension_sets = set(\n        self._operation_name_to_mtf_dimension_set.values())\n    self._mtf_dimension_sets = self._operation_mtf_dimension_sets | set(\n        self._tensor_name_to_mtf_dimension_set.values())\n\n    # Compute possible assignments for each set of MTF dimensions.\n    self._assignments = {}  # indexed by MTF dimension set\n    for mtf_dimension_set in self._mtf_dimension_sets:\n      self._assignments[mtf_dimension_set] = _generate_assignments(\n          mtf_dimension_set, self._layout_validator.mesh_dimension_name_to_size)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the variables of the IP.", "response": "def _initialize_variables(self):\n    \"\"\"Initializing the variables of the IP.\"\"\"\n    # Initialize global variables.\n    self._global_vars = {}  # Indexed by (MTF dimension, mesh dimension)\n    for mtf_dimension_name in (\n        self._layout_validator.splittable_mtf_dimension_names):\n      for mesh_dimension_name in (\n          self._layout_validator.mesh_dimension_name_to_size):\n        name = _global_var_name(mtf_dimension_name, mesh_dimension_name)\n        self._global_vars[(mtf_dimension_name, mesh_dimension_name)] = (\n            self._model.NewBoolVar(name))\n\n    # Initialize local variables.\n    self._local_vars = {}  # Indexed by (tensorflow dimension set), then name of\n    # assignment.\n    for mtf_dimension_set in self._mtf_dimension_sets:\n      self._local_vars[mtf_dimension_set] = {}\n      for assignment in self._assignments[mtf_dimension_set]:\n        # TODO(joshuawang): Avoid hash collision no matter what dimension names\n        # are; don't hash by this local var name, swap to using a tuple encoding\n        # of the full assignment instead.\n        name = _local_var_name(mtf_dimension_set, assignment)\n        self._local_vars[mtf_dimension_set][name] = (\n            self._model.NewBoolVar(name))\n\n    # Initialize memory variable. We need a crude upper bound on memory, so we\n    # use the total size of all tensors under the empty assignment.\n    # NOTE(joshuawang): This bound could be improved by factoring in the\n    # schedule.\n    memory_upper_bound = 0\n    for tensor_name in self._graph.get_all_tensor_names():\n      if self._graph.is_tensor_on_canonical_device(tensor_name):\n        memory_upper_bound += int(self._graph.get_tensor_size(tensor_name))\n    self._memory_var = self._model.NewIntVar(0, memory_upper_bound, \"z\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_constraints(self):\n    # Add operation constraints.\n    for mesh_dimension_name in (\n        self._layout_validator.mesh_dimension_name_to_size):\n      for mtf_dimension_set in self._operation_mtf_dimension_sets:\n        self._model.Add(\n            sum(self._global_vars[(mtf_dimension_name, mesh_dimension_name)]\n                for mtf_dimension_name in mtf_dimension_set) <= 1)\n\n    # Add global constraints.\n    for mtf_dimension_name in (\n        self._layout_validator.splittable_mtf_dimension_names):\n      self._model.Add(\n          sum(self._global_vars[(mtf_dimension_name, mesh_dimension_name)]\n              for mesh_dimension_name in (\n                  self._layout_validator.mesh_dimension_name_to_size)) <= 1)\n\n    # Add divisibility constraints.\n    for mtf_dimension_name in (\n        self._layout_validator.splittable_mtf_dimension_names):\n      for mesh_dimension_name in (\n          self._layout_validator.mesh_dimension_name_to_size):\n        if not self._layout_validator.is_valid_assignment(mtf_dimension_name,\n                                                          mesh_dimension_name):\n          self._model.Add(self._global_vars[(mtf_dimension_name,\n                                             mesh_dimension_name)] == 0)\n\n    # Add local constraints.\n    for mtf_dimension_set in self._mtf_dimension_sets:\n      self._model.Add(\n          sum(self._local_vars[mtf_dimension_set][_local_var_name(\n              mtf_dimension_set, assignment)]\n              for assignment in self._assignments[mtf_dimension_set]) == 1)\n\n    # Add local-to-global constraints.\n    for mtf_dimension_set in self._mtf_dimension_sets:\n      for assignment in self._assignments[mtf_dimension_set]:\n        name = _local_var_name(mtf_dimension_set, assignment)\n        for mtf_dimension_name in mtf_dimension_set:\n          if mtf_dimension_name in assignment:\n            mesh_dimension_name = assignment[mtf_dimension_name]\n            self._model.AddImplication(\n                self._local_vars[mtf_dimension_set][name],\n                self._global_vars[(mtf_dimension_name, mesh_dimension_name)])\n          else:\n            for mesh_dimension_name in (\n                self._layout_validator.mesh_dimension_name_to_size):\n              self._model.AddImplication(\n                  self._global_vars[(mtf_dimension_name, mesh_dimension_name)],\n                  self._local_vars[mtf_dimension_set][name].Not())\n\n    # Add memory constraints.\n    tensor_memory_sum = {}\n    for tensor_name in self._graph.get_all_tensor_names():\n      tensor_memory_sum[tensor_name] = 0\n      mtf_dimension_set = self._tensor_name_to_mtf_dimension_set[tensor_name]\n\n      if not self._graph.is_tensor_on_canonical_device(tensor_name):\n        continue\n\n      for assignment in self._assignments[mtf_dimension_set]:\n        size_under_assignment = self._graph.get_tensor_size(\n            tensor_name, assignment,\n            self._layout_validator.mesh_dimension_name_to_size)\n\n        name = _local_var_name(mtf_dimension_set, assignment)\n        tensor_memory_sum[tensor_name] += (\n            size_under_assignment * self._local_vars[mtf_dimension_set][name])\n\n    for tensor_names in self._get_memory_contents():\n      self._model.Add(\n          sum(tensor_memory_sum[tensor_name]\n              for tensor_name in tensor_names) <= self._memory_var)", "response": "Add constraints to the IP."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_memory_contents(self):\n    if self._memory_contents is not None:\n      return self._memory_contents\n\n    schedule = scheduler.minimize_peak_memory(self._graph, self._scheduler_alg)\n    self._memory_contents = self._graph.compute_memory_contents_under_schedule(\n        schedule)\n\n    return self._memory_contents", "response": "Runs the scheduler to determine the memory contents at every point in time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsolving the current integer program and returns the computed layout.", "response": "def solve(self, print_solution=False):\n    \"\"\"Solves the current integer program and returns the computed layout.\n\n    Args:\n      print_solution: An optional boolean indicating whether to print the full\n        solution in human-readable format.\n\n    Returns:\n      The computed layout (as a string).\n\n    Raises:\n      SolverError: the internal solver could not find a solution, or the\n          solution found is infeasible.\n    \"\"\"\n    # Solve and see how well the solver did.\n    self._cp_solver = cp_model.CpSolver()\n    status = self._cp_solver.Solve(self._model)\n    if status != cp_model.OPTIMAL:\n      if status == cp_model.FEASIBLE:\n        logging.warning(\"A potentially suboptimal solution was found.\")\n      else:\n        logging.error(\"Solver returned status %d.\", status)\n        raise SolverError(\"The solver could not solve the problem and returned \"\n                          \"status {}.\".format(status))\n\n    # TODO(joshuawang): Verify the solver's solution.\n    if print_solution:\n      print_cp_model_solution.print_solution(self._model, self._cp_solver)\n\n    # Reconstruct layout from solution.\n    layout = []\n    for mtf_dimension_name in (\n        self._layout_validator.splittable_mtf_dimension_names):\n      for mesh_dimension_name in (\n          self._layout_validator.mesh_dimension_name_to_size):\n        value = self._cp_solver.Value(self._global_vars[(mtf_dimension_name,\n                                                         mesh_dimension_name)])\n        if value:  # Value is integer.\n          layout.append(mtf_dimension_name + \":\" + mesh_dimension_name)\n\n    layout.sort()\n    return \";\".join(layout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nevaluating the given layout.", "response": "def evaluate_layout(self, layout):\n    \"\"\"The current objective value for the given layout.\n\n    TODO(joshuawang): The current function does not check that the given\n    layout is valid.\n\n    Args:\n      layout: a string, representing a layout to evaluate (e.g.\n          \"d_ff:m1;heads:m2\").\n\n    Returns:\n      A float, the objective value.\n    \"\"\"\n    layout_dict = {}\n    if layout:\n      for pair in layout.split(\";\"):\n        mtf_dimension_name, mesh_dimension_name = pair.split(\":\", 1)\n        if (mtf_dimension_name in\n            self._layout_validator.splittable_mtf_dimension_names):\n          layout_dict[mtf_dimension_name] = mesh_dimension_name\n        else:\n          logging.warning(\"Skipping unsplittable dimension %s.\",\n                          mtf_dimension_name)\n\n    tensor_memory = {}  # {string: float}, size of each tensor under our layout\n    for tensor_name in self._graph.get_all_tensor_names():\n      if self._graph.is_tensor_on_canonical_device(tensor_name):\n        tensor_memory[tensor_name] = self._graph.get_tensor_size(\n            tensor_name, layout_dict,\n            self._layout_validator.mesh_dimension_name_to_size)\n      else:\n        tensor_memory[tensor_name] = 0.0\n\n    peak_memory_usage = 0.0\n    for tensor_names in self._get_memory_contents():\n      memory_usage = 0.0\n      for tensor_name in tensor_names:\n        memory_usage += tensor_memory[tensor_name]\n      peak_memory_usage = max(peak_memory_usage, memory_usage)\n    return peak_memory_usage"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchooses a device for the input variable. Args: var: an Variable. Returns: The device for placing the var.", "response": "def device_function(self, var):\n    \"\"\"Choose a device for the input variable.\n\n    Args:\n      var: an Variable.\n\n    Returns:\n      The device for placing the var.\n    \"\"\"\n    if var.type not in ('Variable', 'VariableV2', 'VarHandleOp'):\n      tf.logging.debug('Place {} on last device: {}.'.format(\n          var.name, self._last_device))\n      return self._last_device\n\n    shape = tf.TensorShape(var.get_attr('shape'))\n    assert shape.num_elements() is not None\n\n    size = var.get_attr('dtype').size\n    mem, device = heapq.heappop(self._mem_device_heap)\n    mem += shape.num_elements() * size\n    heapq.heappush(self._mem_device_heap, (mem, device))\n    tf.logging.debug('Place variable {} on {} and consumes {} Bytes.'.format(\n        var.name, device, mem))\n    self._last_device = device\n\n    return device"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive sequences scores and flags compute the top k scores and sequences from sequences.", "response": "def compute_topk_scores_and_seq(sequences, scores, scores_to_gather, flags,\n                                beam_dim, prefix=\"default\"):\n  \"\"\"Given sequences and scores, will gather the top k=beam size sequences.\n\n  This function is used to grow alive, and finished. It takes sequences,\n  scores, and flags, and returns the top k from sequences, scores_to_gather,\n  and flags based on the values in scores.\n\n  This method permits easy introspection using tfdbg.  It adds two named ops\n  that are prefixed by `prefix`:\n    - _topk_seq: the tensor for topk_seq returned by this method.\n    - _topk_flags: the tensor for topk_finished_flags returned by this method.\n\n  Args:\n    sequences: Tensor of sequences that we need to gather from.\n      [batch_size, beam_size, seq_length]\n    scores: Tensor of scores for each sequence in sequences.\n      [batch_size, beam_size]. We will use these to compute the topk.\n    scores_to_gather: Tensor of scores for each sequence in sequences.\n      [batch_size, beam_size]. We will return the gathered scores from here.\n      Scores to gather is different from scores because for grow_alive, we will\n      need to return log_probs, while for grow_finished, we will need to return\n      the length penalized scores.\n    flags: Tensor of bools for sequences that say whether a sequence has reached\n      EOS or not\n    beam_dim: mtf.Dimension\n    prefix: an optional string\n  Returns:\n    Tuple of\n    (topk_seq [batch_size, beam_size, decode_length],\n     topk_gathered_scores [batch_size, beam_size],\n     topk_finished_flags[batch_size, beam_size],\n     selector)\n  \"\"\"\n  unused_batch_dim, old_beam_dim, unused_length_dim = sequences.shape.dims\n  topk_indices, _ = mtf.top_k(scores, old_beam_dim, beam_dim)\n\n  selector = mtf.one_hot(topk_indices, old_beam_dim, dtype=tf.float32)\n\n  # Gather up the highest scoring sequences.\n  # For each operation added, give it\n  # a concrete name to simplify observing these operations with tfdbg.\n  # Clients can capture these tensors by watching these node names.\n  def gather(tensor, name):\n    with tf.name_scope(prefix + name):\n      output_shape = mtf.Shape(\n          [beam_dim if d == old_beam_dim else d for d in tensor.shape.dims])\n      return mtf.gather(\n          tensor, topk_indices, old_beam_dim, output_shape=output_shape)\n  topk_seq = gather(sequences, \"_seq\")\n  topk_flags = gather(flags, \"_flags\")\n  topk_gathered_scores = gather(scores_to_gather, \"_scores\")\n  return topk_seq, topk_gathered_scores, topk_flags, selector"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef beam_search(logits_fn,\n                initial_ids,\n                alpha,\n                states=None,\n                eos_id=EOS_ID,\n                stop_early=True,\n                decode_length=None,\n                use_tpu=True,\n                dtype=tf.float32,\n                layout=None,\n                mesh_shape=None,\n                num_prefilter=2):\n  \"\"\"Beam search with length penalties.\n\n  Requires a function that can take the currently decoded symbols and return\n  the logits for the next symbol. The implementation is inspired by\n  https://arxiv.org/abs/1609.08144.\n\n  When running, the beam search steps can be visualized by using tfdbg to watch\n  the operations generating the output ids for each beam step.  These operations\n  have the pattern:\n    (alive|finished)_topk_(seq,scores)\n\n  Operations marked `alive` represent the new beam sequences that will be\n  processed in the next step.  Operations marked `finished` represent the\n  completed beam sequences, which may be padded with 0s if no beams finished.\n\n  Operations marked `seq` store the full beam sequence for the time step.\n  Operations marked `scores` store the sequence's final log scores.\n\n  The beam search steps will be processed sequentially in order, so when\n  capturing observed from these operations, tensors, clients can make\n  assumptions about which step is being recorded.\n\n  num_prefilter is a theoretically lossy shortcut around slow performance of\n  top_k on TPU on large Tensors and large k.  This option should be removed once\n  better top_k implementations on TPU are avialable.  If num_prefilter is set to\n  a nonzero value, then at each step we first compute the top num_prefilter\n  sequences per beam and then compute the top k sequences overall from among\n  those.  Empirically, there seems to be no quality difference in setting\n  num_prefilter to 2.\n\n  Args:\n    logits_fn: Interface to the model, to provide logits.\n        Should take:\n          step_num - mtf Scalar\n          ids - mtf Tensor with shape [batch, beam, length]\n        Should return:\n          logits - [batch, beam, vocab_size], dtype=dtype\n    initial_ids: a mtf.Tensor with shape [batch_dim, beam_dim, length_dim])\n    alpha: alpha for length penalty.\n    states: list of mtf.Tensor\n    eos_id: ID for end of sentence.\n    stop_early: a boolean - stop once best sequence is provably determined.\n    decode_length: a mtf Scalar of dtype tf.int32 - maximum length of decodes\n    use_tpu: a boolean\n    dtype: a tf.dtype\n    layout: an optional string\n    mesh_shape: an optional string\n    num_prefilter: an optional integer\n  Returns:\n    Tuple of\n    (decoded beams [batch, beam, length]\n     decoding probabilities [batch, beam_size])\n  \"\"\"\n  batch_dim, beam_dim, length_dim = initial_ids.shape.dims\n  batch_and_beam_dim = mtf.Dimension(\n      batch_dim.name, batch_dim.size * beam_dim.size)\n  mesh = initial_ids.mesh\n\n  batch_by_beam = mtf.Shape([batch_dim, beam_dim])\n  initial_log_probs = mtf.broadcast(\n      mtf.one_hot(\n          mtf.constant(mesh, 0, dtype=tf.int32),\n          beam_dim,\n          on_value=0.0,\n          off_value=-INF,\n          dtype=dtype),\n      batch_by_beam)\n\n  length_scalar = mtf.constant(mesh, length_dim.size, dtype=tf.int32)\n  if decode_length is None:\n    decode_length = length_scalar\n  else:\n    decode_length = mtf.minimum(decode_length, length_scalar)\n\n  alive_log_probs = initial_log_probs\n  alive_seq = initial_ids\n\n  # Finished will keep track of all the sequences that have finished so far\n  # Finished log probs will be negative infinity in the beginning\n  # finished_flags will keep track of booleans\n  finished_seq = initial_ids\n  finished_scores = mtf.constant(mesh, -INF, batch_by_beam, dtype=dtype)\n\n  # Setting the scores of the initial to negative infinity.\n  finished_flags = mtf.constant(mesh, False, batch_by_beam, tf.bool)\n\n  def grow_finished(finished_seq, finished_scores, finished_flags, curr_seq,\n                    curr_scores, curr_finished):\n    \"\"\"Given sequences and scores, will gather the top k=beam size sequences.\n\n    Args:\n      finished_seq: Current finished sequences.\n        [batch, beam, length]\n      finished_scores: scores for each of these sequences.\n        [batch, beam]\n      finished_flags: finished bools for each of these sequences.\n        [batch, beam]\n      curr_seq: current topk sequence that has been grown by one position.\n        [batch, beam, length]\n      curr_scores: scores for each of these sequences. [batch, beam]\n      curr_finished: Finished flags for each of these sequences.\n        [batch, beam]\n    Returns:\n      Tuple of\n        (Topk sequences based on scores,\n         log probs of these sequences,\n         Finished flags of these sequences,\n         None (no states))\n    \"\"\"\n\n    # Set the scores of the unfinished seq in curr_seq to large negative\n    # values\n    curr_scores += (1. - mtf.cast(curr_finished, curr_scores.dtype)) * -INF\n    unused_batch_dim, beam_dim, unused_length_dim = finished_seq.shape.dims\n    # concatenating the sequences and scores along beam axis\n    def _my_concat(a, b):\n      a = mtf.rename_dimension(a, \"beam\", \"triple_beam\")\n      b = mtf.rename_dimension(b, \"double_beam\", \"triple_beam\")\n      return mtf.concat([a, b], \"triple_beam\")\n\n    curr_finished_seq = _my_concat(finished_seq, curr_seq)\n    curr_finished_scores = _my_concat(finished_scores, curr_scores)\n    curr_finished_flags = _my_concat(finished_flags, curr_finished)\n    return compute_topk_scores_and_seq(\n        curr_finished_seq, curr_finished_scores, curr_finished_scores,\n        curr_finished_flags, beam_dim, \"grow_finished\")\n\n  def grow_alive(curr_seq, curr_scores, curr_log_probs, curr_finished):\n    \"\"\"Given sequences and scores, will gather the top k=beam size sequences.\n\n    Args:\n      curr_seq: current topk sequence that has been grown by one position.\n        [batch, beam, length]\n      curr_scores: scores for each of these sequences. [batch_size, beam_size]\n      curr_log_probs: log probs for each of these sequences.\n        [batch, beam]\n      curr_finished: Finished flags for each of these sequences.\n        [batch, beam]\n    Returns:\n      Tuple of\n        (Topk sequences based on scores,\n         log probs of these sequences,\n         Finished flags of these sequences)\n    \"\"\"\n    # Set the scores of the finished seq in curr_seq to large negative\n    # values\n    curr_scores += mtf.cast(curr_finished, curr_scores.dtype) * -INF\n    return compute_topk_scores_and_seq(curr_seq, curr_scores, curr_log_probs,\n                                       curr_finished, beam_dim,\n                                       \"grow_alive\")\n\n  def grow_topk(i, alive_seq, alive_log_probs, states=None):\n    r\"\"\"Inner beam search loop.\n\n    This function takes the current alive sequences, and grows them to topk\n    sequences where k = 2*beam. We use 2*beam because, we could have beam_size\n    number of sequences that might hit <EOS> and there will be no alive\n    sequences to continue. With 2*beam_size, this will not happen. This relies\n    on the assumption the vocab size is > beam size. If this is true, we'll\n    have at least beam_size non <EOS> extensions if we extract the next top\n    2*beam words.\n    Length penalty is given by = (5+len(decode)/6) ^ -\\alpha. Pls refer to\n    https://arxiv.org/abs/1609.08144.\n\n    Args:\n      i: loop index\n      alive_seq: Topk sequences decoded so far [batch, beam, length]\n      alive_log_probs: probabilities of these sequences. [batch, beam]\n      states: optional list of mtf.Tensor\n    Returns:\n      Tuple of\n        (Topk sequences extended by the next word,\n         The log probs of these sequences,\n         The scores with length penalty of these sequences,\n         Flags indicating which of these sequences have finished decoding,\n         list of transformed decoding states)\n    \"\"\"\n    logits, new_states = logits_fn(i, alive_seq, states)\n    batch_dim, beam_dim, vocab_dim = logits.shape.dims\n\n    # Convert logits to normalized log probs\n    candidate_log_probs = mtf.log_softmax(logits, vocab_dim)\n\n    # Multiply the probabilities by the current probabilities of the beam.\n    # (batch_size, beam_size, vocab_size) + (batch_size, beam_size, 1)\n    log_probs = candidate_log_probs + alive_log_probs\n\n    length_penalty = mtf.pow(((5. + mtf.cast(i + 1, logits.dtype)) / 6.), alpha)\n\n    # scores have shape [batch, beam, vocab]\n    curr_scores = log_probs / length_penalty\n\n    # We find the top 2k sequences to make sure we get k alive sequences.\n    #\n    # TODO(noam): This is inefficient.  We should separately compute the k\n    # finished sequences (previously alive sequences + EOS), and the top k new\n    # alive sequences.\n    double_beam = mtf.Dimension(\"double_beam\", beam_dim.size * 2)\n\n    if use_tpu and layout is not None and mesh_shape is not None:\n      # Do some partial top-k-ing first locally to avoid communication.\n      # We reshape the logits from:\n      #   [batch, beam, vocab] to\n      #   [batch, beam, major_vocab, minor_vocab]\n      # We first reduce (locally) across the minor_vocab dimension.  This makes\n      # the thing we need to broadcast smaller.\n      # This also enables our shortcut of only picking the top num_prefilter\n      #   sequences per beam per major_vocab in the first pass.\n      major_vocab_size = mtf.tensor_dim_to_mesh_dim_size(\n          layout, mesh_shape, vocab_dim)\n      major_vocab = mtf.Dimension(vocab_dim.name, major_vocab_size)\n      minor_vocab = mtf.Dimension(\n          \"minor_vocab\", vocab_dim.size // major_vocab_size)\n      curr_scores = mtf.reshape(\n          curr_scores, [batch_dim, beam_dim, major_vocab, minor_vocab])\n      prefilter = mtf.Dimension(\"prefilter\", num_prefilter or double_beam.size)\n      # shape = [batch_dim, beam_dim, major_vocab, prefilter]\n      top_minor_vocab_ids, top_scores = mtf.top_k(\n          curr_scores, reduced_dim=minor_vocab, new_dim=prefilter)\n      combined = mtf.Dimension(\n          \"combined\", beam_dim.size * major_vocab.size * prefilter.size)\n      top_scores = mtf.reshape(top_scores, [batch_dim, combined])\n      top_minor_vocab_ids = mtf.reshape(\n          top_minor_vocab_ids, [batch_dim, combined])\n      # shpae = [batch_dim, double_beam]\n      # ids are indices representing (beam, major_vocab, prefilter)\n      top_combined_ids, top_scores = mtf.top_k(\n          top_scores, reduced_dim=combined, new_dim=double_beam)\n      top_minor_vocab_ids = mtf.gather(\n          top_minor_vocab_ids, top_combined_ids, combined,\n          output_shape=[batch_dim, double_beam])\n      top_beam_index = top_combined_ids // (major_vocab.size * prefilter.size)\n      top_combined_ids -= top_beam_index * (major_vocab.size * prefilter.size)\n      top_major_vocab_ids = top_combined_ids // prefilter.size\n      top_combined_ids -= top_major_vocab_ids * prefilter.size\n      top_ids = top_major_vocab_ids * minor_vocab.size + top_minor_vocab_ids\n    else:\n      beam_and_vocab_dim = mtf.Dimension(\n          \"beam_and_vocab\", beam_dim.size * vocab_dim.size)\n      flat_shape = mtf.Shape([batch_dim, beam_and_vocab_dim])\n      # Flatten out (beam_size, vocab_size) probs into a list of possibilities\n      flat_curr_scores = mtf.reshape(\n          curr_scores, flat_shape, name=\"flatten_scores\")\n      top_ids, top_scores = mtf.top_k(\n          flat_curr_scores, reduced_dim=beam_and_vocab_dim, new_dim=double_beam)\n      # Work out what beam the top probs are in.\n      top_beam_index = top_ids // vocab_dim.size\n      top_ids %= vocab_dim.size  # Unflatten the ids\n\n    # Recovering the log probs because we will need to send them back\n    top_log_probs = top_scores * length_penalty\n\n    selector = mtf.one_hot(top_beam_index, beam_dim, dtype=tf.float32)\n\n    def my_gather(tensor):\n      return mtf.gather(\n          tensor, top_beam_index, beam_dim,\n          output_shape=mtf.Shape(\n              [double_beam if d == beam_dim else d for d in tensor.shape.dims]))\n\n    # Gather up the most probable 2*beams both for the ids and finished_in_alive\n    # bools\n    top_seq = my_gather(alive_seq)\n\n    # Append the most probable alive\n    top_seq += top_ids * mtf.one_hot(i, length_dim, dtype=tf.int32)\n    top_finished = mtf.equal(top_ids, eos_id)\n\n    return (\n        top_seq, top_log_probs, top_scores, top_finished, new_states, selector)\n\n  def inner_loop(i, alive_seq, alive_log_probs, finished_seq, finished_scores,\n                 finished_flags, *states):\n    \"\"\"Inner beam search loop.\n\n    There are three groups of tensors, alive, finished, and topk.\n    The alive group contains information about the current alive sequences\n    The topk group contains information about alive + topk current decoded words\n    the finished group contains information about finished sentences, that is,\n    the ones that have decoded to <EOS>. These are what we return.\n    The general beam search algorithm is as follows:\n    While we haven't terminated (pls look at termination condition)\n      1. Grow the current alive to get beam*2 topk sequences\n      2. Among the topk, keep the top beam_size ones that haven't reached EOS\n      into alive\n      3. Among the topk, keep the top beam_size ones have reached EOS into\n      finished\n    Repeat\n    To make things simple with using fixed size tensors, we will end\n    up inserting unfinished sequences into finished in the beginning. To stop\n    that we add -ve INF to the score of the unfinished sequence so that when a\n    true finished sequence does appear, it will have a higher score than all the\n    unfinished ones.\n\n    Args:\n      i: loop index\n      alive_seq: Topk sequences decoded so far [batch_size, beam_size, i+1]\n      alive_log_probs: probabilities of the beams. [batch_size, beam_size]\n      finished_seq: Current finished sequences.\n        [batch_size, beam_size, i+1]\n      finished_scores: scores for each of these sequences.\n        [batch_size, beam_size]\n      finished_flags: finished bools for each of these sequences.\n        [batch_size, beam_size]\n      *states: mtf Tensors\n\n    Returns:\n      Tuple of\n        (Incremented loop index\n         New alive sequences,\n         Log probs of the alive sequences,\n         New finished sequences,\n         Scores of the new finished sequences,\n         Flags indicating which sequence in finished as reached EOS,\n         dict of final decoding states)\n    \"\"\"\n    states = [mtf.replace_dimensions(\n        state, batch_and_beam_dim, [batch_dim, beam_dim]) for state in states]\n    # Each inner loop, we carry out three steps:\n    # 1. Get the current topk items.\n    # 2. Extract the ones that have finished and haven't finished\n    # 3. Recompute the contents of finished based on scores.\n    (top2k_seq, top2k_log_probs, top2k_scores, top2k_finished,\n     new_states, first_selector) = grow_topk(\n         i, alive_seq, alive_log_probs, states)\n    with tf.variable_scope(\"grow_alive\"):\n      alive_seq, alive_log_probs, _, second_selector = grow_alive(\n          top2k_seq, top2k_scores, top2k_log_probs, top2k_finished)\n    with tf.variable_scope(\"grow_finished\"):\n      finished_seq, finished_scores, finished_flags, _ = grow_finished(\n          finished_seq, finished_scores, finished_flags, top2k_seq,\n          top2k_scores, top2k_finished)\n    old_beam_dim = mtf.Dimension(\"old_beam\", beam_dim.size)\n    selector = mtf.einsum(\n        [mtf.rename_dimension(first_selector, beam_dim.name, old_beam_dim.name),\n         second_selector],\n        output_shape=[batch_dim, old_beam_dim, beam_dim])\n    gathered_states = []\n    if use_tpu and layout is not None and mesh_shape is not None:\n      # This hack combines the beam dimension with some of the batch dimension.\n      # It makes gathering faster on TPU.\n      #\n      # Instead of multiplying by a [beam, beam] selector matrix, we instead\n      # multiply by a [minor_batch*beam, minor_batch*beam] selector matrix.\n      # This is theoretically more FLOPs, but it brings the matrix size closer\n      # to the magic optimal value of 128.\n      #\n      # TODO(noam): file a bug with the XLA team to do this automatically\n      major_batch_size = mtf.tensor_dim_to_mesh_dim_size(\n          layout, mesh_shape, batch_dim)\n      major_batch = mtf.Dimension(batch_dim.name, major_batch_size)\n      minor_batch = mtf.Dimension(\n          \"minor_batch\", batch_dim.size // major_batch.size)\n      old_minor_batch = mtf.Dimension(\"old_minor_batch\", minor_batch.size)\n      old_combined = mtf.Dimension(\n          \"old_combined\", minor_batch.size * beam_dim.size)\n      combined = mtf.Dimension(\n          \"new_combined\", old_combined.size)\n      same_minor_batch = mtf.to_float(\n          mtf.equal(mtf.range(mesh, old_minor_batch, tf.float32),\n                    mtf.range(mesh, minor_batch, tf.float32)))\n      selector = mtf.reshape(\n          selector, [major_batch, minor_batch, old_beam_dim, beam_dim])\n      selector = mtf.einsum(\n          [selector, same_minor_batch],\n          output_shape=[major_batch,\n                        old_minor_batch, old_beam_dim,\n                        minor_batch, beam_dim],\n          reduced_dims=[])\n      selector = mtf.reshape(selector, [major_batch, old_combined, combined])\n      for state in new_states:\n        s = mtf.replace_dimensions(\n            state, [batch_dim, beam_dim], [major_batch, old_combined])\n        s = mtf.einsum(\n            [s, mtf.cast(selector, state.dtype)],\n            reduced_dims=[old_combined],\n            output_shape=mtf.replace_dimensions(\n                state.shape, [batch_dim, beam_dim],\n                [major_batch, combined]))\n        gathered_states.append(mtf.replace_dimensions(\n            s, [major_batch, combined], batch_and_beam_dim))\n    else:\n      for state in new_states:\n        state = mtf.einsum(\n            [mtf.rename_dimension(state, beam_dim.name, old_beam_dim.name),\n             mtf.cast(selector, state.dtype)],\n            reduced_dims=[old_beam_dim], output_shape=state.shape)\n        state = mtf.replace_dimensions(\n            state, [batch_dim, beam_dim], batch_and_beam_dim)\n        gathered_states.append(state)\n\n    return (i + 1, alive_seq, alive_log_probs, finished_seq, finished_scores,\n            finished_flags) + tuple(gathered_states)\n\n  def _is_finished(i, unused_alive_seq, alive_log_probs, unused_finished_seq,\n                   finished_scores, finished_in_finished, *unused_states):\n    \"\"\"Checking termination condition.\n\n    We terminate when we decoded up to decode_length or the lowest scoring item\n    in finished has a greater score that the highest prob item in alive divided\n    by the max length penalty\n\n    Args:\n      i: loop index\n      alive_log_probs: probabilities of the beams. [batch_size, beam_size]\n      finished_scores: scores for each of these sequences.\n        [batch_size, beam_size]\n      finished_in_finished: finished bools for each of these sequences.\n        [batch_size, beam_size]\n\n    Returns:\n      Bool.\n    \"\"\"\n    # TODO(noam): support a different decode length...\n    # decode_length = mtf.constant(mesh, length_dim.size, dtype=tf.int32)\n\n    # del alive_log_probs, finished_scores, finished_in_finished\n    # return mtf.less(i, length_dim.size)\n    if not stop_early:\n      return mtf.less(i, decode_length)\n    max_length_penalty = mtf.pow(\n        ((5. + mtf.cast(decode_length, finished_scores.dtype)) / 6.), alpha)\n    # The best possible score of the most likely alive sequence.\n    lower_bound_alive_scores = mtf.gather(\n        alive_log_probs, mtf.constant(mesh, 0, dtype=tf.int32),\n        beam_dim) / max_length_penalty\n\n    # Now to compute the lowest score of a finished sequence in finished\n    # If the sequence isn't finished, we multiply it's score by 0. since\n    # scores are all -ve, taking the min will give us the score of the lowest\n    # finished item.\n    lowest_score_of_finished_in_finished = mtf.reduce_min(\n        finished_scores * mtf.cast(finished_in_finished, finished_scores.dtype),\n        reduced_dim=beam_dim)\n\n    # If none of the sequences have finished, then the min will be 0 and\n    # we have to replace it by -ve INF if it is. The score of any seq in alive\n    # will be much higher than -ve INF and the termination condition will not\n    # be met.\n    lowest_score_of_finished_in_finished += (\n        (1. - mtf.cast(mtf.reduce_any(\n            finished_in_finished, reduced_dim=beam_dim),\n                       finished_scores.dtype)) * -INF)\n\n    bound_is_met = mtf.reduce_all(\n        mtf.greater(lowest_score_of_finished_in_finished,\n                    lower_bound_alive_scores))\n    return mtf.logical_and(\n        mtf.less(i, decode_length), mtf.logical_not(bound_is_met))\n\n  initial_step_num = mtf.constant(mesh, 0, dtype=tf.int32)\n  states = [mtf.replace_dimensions(\n      state, [batch_dim, beam_dim], batch_and_beam_dim) for state in states]\n  while_loop_inputs = [\n      initial_step_num, alive_seq, alive_log_probs, finished_seq,\n      finished_scores, finished_flags] + states\n\n  (_, alive_seq, alive_log_probs, finished_seq, finished_scores,\n   finished_flags) = mtf.while_loop(\n       _is_finished, inner_loop, while_loop_inputs,\n       num_loop_vars=None if use_tpu else 6)[:6]\n\n  # Accounting for corner case: It's possible that no sequence in alive for a\n  # particular batch item ever reached EOS. In that case, we should just copy\n  # the contents of alive for that batch item. tf.reduce_any(finished_flags, 1)\n  # if 0, means that no sequence for that batch index had reached EOS. We need\n  # to do the same for the scores as well.\n  finished_seq = mtf.where(\n      mtf.reduce_any(finished_flags, reduced_dim=beam_dim),\n      finished_seq, alive_seq)\n  finished_scores = mtf.where(\n      mtf.reduce_any(finished_flags, reduced_dim=beam_dim),\n      finished_scores, alive_log_probs)\n  return finished_seq, finished_scores", "response": "This function is used to perform a beam search on the current state of the current state of the current state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pack_and_batch(dataset, batch_size, length, pack=True):\n  if pack:\n    dataset = pack_dataset(dataset, length=length)\n  # Pad/trim length of each example to length\n  dataset = dataset.map(\n      functools.partial(trim_and_pad_all_features, length=length),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE\n  )\n  dataset = dataset.batch(batch_size, drop_remainder=False)\n  # Pad batch size of each batch to batch_size\n  dataset = dataset.map(\n      functools.partial(trim_and_pad_all_features, length=batch_size),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE\n  )\n  # Remind TensorFlow of the shape\n  dataset = dataset.map(\n      lambda x: {k: tf.reshape(v, (batch_size, length)) for k, v in x.items()},\n      num_parallel_calls=tf.data.experimental.AUTOTUNE\n  )\n  dataset = dataset.prefetch(100)\n  return dataset", "response": "Packs the input dataset into training batches and batches."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nencode from strings to token ids.", "response": "def encode_dataset(dataset, vocabulary):\n  \"\"\"Encode from strings to token ids.\n\n  Args:\n    dataset: a tf.data.Dataset with string values.\n    vocabulary: a mesh_tensorflow.transformer.Vocabulary\n  Returns:\n    a tf.data.Dataset with integer-vector values ending in EOS=1\n  \"\"\"\n  def encode(features):\n    return {k: vocabulary.encode_tf(v) for k, v in features.items()}\n  return dataset.map(encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a tensorflow_datasets dataset.", "response": "def pretokenized_tfds_dataset(dataset_name=gin.REQUIRED,\n                              text2self=gin.REQUIRED,\n                              tfds_data_dir=gin.REQUIRED,\n                              dataset_split=gin.REQUIRED,\n                              batch_size=gin.REQUIRED,\n                              sequence_length=gin.REQUIRED,\n                              vocabulary=None):\n  \"\"\"Reads a tensorflow_datasets dataset.\n\n  Args:\n    dataset_name: a string\n    text2self: a boolean\n    tfds_data_dir: a boolean\n    dataset_split: a string\n    batch_size: an integer\n    sequence_length: an integer\n    vocabulary: ignored\n  Returns:\n    a tf.data.Dataset of batches\n  \"\"\"\n  del vocabulary\n  dataset = tfds.load(\n      dataset_name,\n      split=dataset_split,\n      as_supervised=True,\n      data_dir=tfds_data_dir)\n  if dataset_split == \"train\":\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(1000)\n  def shift_and_append_eos(t):\n    # tfds encoder does not reserve an EOS token, so we need to shift\n    # in order to do so.  We also append EOS=1.\n    return tf.concat([t + 1, [1]], 0)\n  def feature_map(inputs, targets):\n    if text2self:\n      return {\"targets\": shift_and_append_eos(targets)}\n    else:\n      return {\"inputs\": shift_and_append_eos(inputs),\n              \"targets\": shift_and_append_eos(targets)}\n  dataset = dataset.map(feature_map,\n                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  return pack_and_batch(dataset, batch_size, sequence_length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef packed_parallel_tsv_dataset(filenames=gin.REQUIRED,\n                                dataset_split=gin.REQUIRED,\n                                batch_size=gin.REQUIRED,\n                                sequence_length=gin.REQUIRED,\n                                vocabulary=gin.REQUIRED,\n                                append_eos=True,\n                                shuffle_buffer_size=10000,\n                                eos_id=1):\n  \"\"\"Reads parallel tab-separated text file. One example per line.\"\"\"\n\n  dataset = tf.data.TextLineDataset(filenames)\n  if dataset_split == \"train\":\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(shuffle_buffer_size)\n\n  def _parse_fn(record):  # pylint: disable=missing-docstring\n    tokens = tf.decode_csv(\n        record,\n        record_defaults=[\"\"] * 2,\n        field_delim=\"\\t\",\n        use_quote_delim=False)\n    return {\"inputs\": tokens[0], \"targets\": tokens[1]}\n\n  def _encode_fn(features):  # pylint: disable=missing-docstring\n    inputs_vocabulary = vocabulary[0] if isinstance(vocabulary,\n                                                    tuple) else vocabulary\n    targets_vocabulary = vocabulary[1] if isinstance(vocabulary,\n                                                     tuple) else vocabulary\n    inputs_enc = inputs_vocabulary.encode_tf(features[\"inputs\"])\n    targets_enc = targets_vocabulary.encode_tf(features[\"targets\"])\n    if append_eos:\n      inputs_enc = tf.concat([tf.to_int64(inputs_enc), [eos_id]], 0)\n      targets_enc = tf.concat([tf.to_int64(targets_enc), [eos_id]], 0)\n    return {\"inputs\": inputs_enc, \"targets\": targets_enc}\n\n  dataset = dataset.map(_parse_fn)\n  dataset = dataset.map(_encode_fn)\n  return pack_and_batch(dataset, batch_size, sequence_length)", "response": "Reads parallel tab - separated text file. One example per line."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef untokenized_tfds_dataset(dataset_name=gin.REQUIRED,\n                             text2self=gin.REQUIRED,\n                             tfds_data_dir=gin.REQUIRED,\n                             dataset_split=gin.REQUIRED,\n                             batch_size=gin.REQUIRED,\n                             sequence_length=gin.REQUIRED,\n                             vocabulary=gin.REQUIRED,\n                             pack=gin.REQUIRED):\n  \"\"\"Reads a tensorflow_datasets dataset.\n\n  Returns a tf.data.Dataset containing single tokenized examples where each\n  feature ends in EOS=1.\n\n  Args:\n    dataset_name: a string\n    text2self: a boolean\n    tfds_data_dir: a boolean\n    dataset_split: a string\n    batch_size: an integer\n    sequence_length: an integer\n    vocabulary: a vocabulary.Vocabulary\n    pack: if True, multiple examples emitted by load_internal() are concatenated\n        to form one combined example.\n  Returns:\n    a tf.data.Dataset of batches\n  \"\"\"\n  dataset = tfds.load(\n      dataset_name, split=dataset_split,\n      as_supervised=True, data_dir=tfds_data_dir)\n  if dataset_split == \"train\":\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(1000)\n  dataset = supervised_to_dict(dataset, text2self)\n  dataset = encode_all_features(dataset, vocabulary)\n  return pack_and_batch(dataset, batch_size, sequence_length, pack)", "response": "Reads a tensorflow_datasets dataset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef supervised_to_dict(dataset, text2self):\n  def my_fn(inputs, targets):\n    if text2self:\n      return {\"targets\": targets}\n    else:\n      return {\"inputs\": inputs, \"targets\": targets}\n  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)", "response": "Turns a supervised dataset into a dataset with a feature dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding all features. Returns a tf. data. Dataset containing the encoded version of all features.", "response": "def encode_all_features(dataset, vocabulary):\n  \"\"\"Encode all features.\n\n  Args:\n    dataset: a tf.data.Dataset\n    vocabulary: a vocabulary.Vocabulary\n  Returns:\n    a tf.data.Dataset\n  \"\"\"\n  def my_fn(features):\n    ret = {}\n    for k, v in features.items():\n      v = vocabulary.encode_tf(v)\n      v = tf.concat([tf.to_int64(v), [1]], 0)\n      ret[k] = v\n    return ret\n  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a pre - tokenized TFRecord dataset.", "response": "def pretokenized_tfrecord_dataset(filenames,\n                                  text2self,\n                                  eos_included,\n                                  repeat,\n                                  batch_size,\n                                  sequence_length):\n  \"\"\"Reads tensor2tensor-style data files.\n\n  The dataset is defined by sets of TFRecord files of TFExample protos.\n  There should be a \"targets\" feature (a 1d tensor of integers)\n  If not text2self, there should also be an \"inputs\" feature.\n  Other features get ignored.\n\n  eos_included specifies whether the inputs and targets were written with an\n  EOS token, as in tensor2tensor\n\n  Args:\n    filenames: a list of strings\n    text2self: a boolean\n    eos_included: a boolean\n    repeat: a boolean\n    batch_size: an integer\n    sequence_length: an integer\n  Returns:\n    A tf.data.Dataset of batches\n  \"\"\"\n  dataset = tf.data.TFRecordDataset(filenames, buffer_size=64 * 1024 * 1024)\n  if repeat:\n    dataset = dataset.repeat()\n  keys = [\"targets\"] if text2self else [\"inputs\", \"targets\"]\n  def decode_example(serialized_example):\n    \"\"\"Return a dict of Tensors from a serialized tensorflow.Example.\"\"\"\n    data_fields = {}\n    data_items_to_decoders = {}\n    for k in keys:\n      data_fields[k] = tf.VarLenFeature(tf.int64)\n      data_items_to_decoders[k] = tf.contrib.slim.tfexample_decoder.Tensor(k)\n    decoder = tf.contrib.slim.tfexample_decoder.TFExampleDecoder(\n        data_fields, data_items_to_decoders)\n    decode_items = list(sorted(data_items_to_decoders))\n    decoded = decoder.decode(serialized_example, items=decode_items)\n    if not eos_included:\n      decoded = [tf.concat([v, [1]], 0) for v in decoded]\n    return dict(zip(decode_items, decoded))\n  dataset = dataset.map(decode_example,\n                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  return pack_and_batch(dataset, batch_size, sequence_length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pretokenized_t2t_dataset(dataset_name=gin.REQUIRED,\n                             text2self=False,\n                             data_dir=gin.REQUIRED,\n                             dataset_split=\"train\",\n                             batch_size=gin.REQUIRED,\n                             sequence_length=gin.REQUIRED,\n                             vocabulary=None):\n  \"\"\"Loads the Tensor2tensor dataset specified by dataset_name.\n\n  Args:\n    dataset_name: TensorFlow Datasets dataset name.\n    text2self: a boolean\n    data_dir: string, data_dir for TensorFlow Datasets\n    dataset_split: a string - \"train\" or \"dev\"\n    batch_size: an integer\n    sequence_length: an integer\n    vocabulary: ignored\n\n  Returns:\n    A tf.data.Dataset of batches\n  \"\"\"\n  del vocabulary\n  filepattern = os.path.join(\n      data_dir, dataset_name + \"-\" + dataset_split + \"-*\")\n  filenames = tf.gfile.Glob(filepattern)\n  tf.logging.info(\"Found %s files matching %s\" % (len(filenames), filepattern))\n  if not filenames:\n    raise ValueError(\"No matching files found\")\n  dataset = pretokenized_tfrecord_dataset(\n      filenames=filenames,\n      text2self=text2self,\n      eos_included=True,\n      repeat=dataset_split == \"train\",\n      batch_size=batch_size,\n      sequence_length=sequence_length)\n  if dataset_split == \"train\":\n    dataset = dataset.shuffle(1000)\n  return dataset", "response": "Loads the pretokenized Tensor2tensor dataset specified by dataset_name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pack_dataset(dataset, length, keys=None, use_custom_ops=False):\n  shapes = dataset.output_shapes\n  if keys is None:\n    keys = shapes.keys()\n  for k in keys:\n    if k not in shapes:\n      raise ValueError(\"Key %s not found in dataset.  Available keys are %s\"\n                       % (k, shapes.keys()))\n    if not shapes[k].is_compatible_with(tf.TensorShape([None])):\n      raise ValueError(\"Tensors to be packed must be one-dimensional.\")\n\n  # trim to length\n  dataset = dataset.map(lambda x: {k: x[k][:length] for k in keys},\n                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  # Setting batch_size=length ensures that the concatenated sequences (if they\n  # have length >=1) are sufficient to fill at least one packed example.\n  batch_size = length\n  dataset = dataset.padded_batch(\n      batch_size, padded_shapes={k: [-1] for k in keys})\n  if use_custom_ops and len(keys) <= 2:\n    return _pack_with_custom_ops(dataset, keys, length)\n  else:\n    return _pack_with_tf_ops(dataset, keys, length)", "response": "Packs a dataset into a separate\n."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _pack_with_tf_ops(dataset, keys, length):\n  empty_example = {}\n  for k in keys:\n    empty_example[k] = tf.zeros([0], dtype=tf.int32)\n    empty_example[k + \"_position\"] = tf.zeros([0], dtype=tf.int32)\n  keys_etc = empty_example.keys()\n\n  def write_packed_example(partial, outputs):\n    new_partial = empty_example.copy()\n    new_outputs = {}\n    for k in keys_etc:\n      new_outputs[k] = outputs[k].write(\n          outputs[k].size(),\n          tf.pad(partial[k], [[0, length - tf.size(partial[k])]]))\n    return new_partial, new_outputs\n\n  def map_fn(x):\n    \"\"\"Internal function to flat_map over.\n\n    Consumes a batch of input examples and produces a variable number of output\n    examples.\n\n    Args:\n      x: a single example\n    Returns:\n      a tf.data.Dataset\n    \"\"\"\n    partial = empty_example.copy()\n    i = tf.zeros([], dtype=tf.int32)\n    dynamic_batch_size = tf.shape(x[keys[0]])[0]\n    outputs = {}\n    for k in keys:\n      outputs[k] = tf.TensorArray(\n          tf.int32, size=0, dynamic_size=True, element_shape=[length])\n      outputs[k + \"_position\"] = tf.TensorArray(\n          tf.int32, size=0, dynamic_size=True, element_shape=[length])\n    def cond_fn(i, partial, outputs):\n      del partial, outputs\n      return i < dynamic_batch_size\n    def body_fn(i, partial, outputs):\n      \"\"\"Body function for while_loop.\n\n      Args:\n        i: integer scalar\n        partial: dictionary of Tensor (partially-constructed example)\n        outputs: dictionary of TensorArray\n      Returns:\n        A triple containing the new values of the inputs.\n      \"\"\"\n      can_append = True\n      one_example = {}\n      for k in keys:\n        val = tf.cast(x[k][i], tf.int32)\n        val = val[:tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]\n        one_example[k] = val\n      for k in keys:\n        can_append = tf.logical_and(\n            can_append,\n            tf.less_equal(\n                tf.size(partial[k]) + tf.size(one_example[k]), length))\n      def false_fn():\n        return write_packed_example(partial, outputs)\n      def true_fn():\n        return partial, outputs\n      partial, outputs = tf.cond(can_append, true_fn, false_fn)\n      new_partial = {}\n      for k in keys:\n        new_seq = one_example[k][:length]\n        new_seq_len = tf.size(new_seq)\n        new_partial[k] = tf.concat([partial[k], new_seq], 0)\n        new_partial[k + \"_position\"] = tf.concat(\n            [partial[k + \"_position\"],\n             tf.range(new_seq_len, dtype=tf.int32)], 0)\n      partial = new_partial\n      return i+1, partial, outputs\n\n    i, partial, outputs = tf.while_loop(\n        cond_fn, body_fn, (i, partial, outputs),\n        back_prop=False,\n        shape_invariants=(\n            tf.TensorShape([]),\n            {k: tf.TensorShape([None]) for k in keys_etc},\n            {k: tf.TensorShape(None) for k in keys_etc},\n            ))\n    partial, outputs = write_packed_example(partial, outputs)\n    packed = {k: outputs[k].stack() for k in keys_etc}\n    for k in keys:\n      packed[k + \"_segmentation\"] = (\n          tf.cumsum(\n              tf.cast(tf.equal(packed[k + \"_position\"], 0), tf.int32), axis=1) *\n          tf.cast(tf.not_equal(packed[k], 0), tf.int32))\n    return packed\n  dataset = dataset.map(map_fn,\n                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  return dataset.flat_map(tf.data.Dataset.from_tensor_slices)", "response": "Helper - function for packing a dataset with tf. while_loop."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _trim_and_pad(t, length):\n  t = t[:length]\n  paddings = [[0, length - tf.shape(t)[0]]] + [[0, 0]]*(t.get_shape().ndims - 1)\n  t = tf.pad(t, paddings)\n  return t", "response": "Trim and pad to the first axis of t to be of size length."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef trim_and_pad_all_features(features, length):\n  return {k: _trim_and_pad(v, length) for k, v in features.items()}", "response": "Trim and pad first dimension of all features to size length."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert input to a Dimension.", "response": "def convert_to_dimension(d):\n  \"\"\"Converts input to a Dimension.\n\n  Args:\n    d: Dimension, tuple (string, int), or None.\n\n  Returns:\n    Dimension or None.\n\n  Raises:\n    ValueError: If d cannot be converted to a Dimension.\n  \"\"\"\n  if d is None:\n    return None\n  if isinstance(d, Dimension):\n    if not isinstance(d.name, str) or not isinstance(d.size, int):\n      raise ValueError(\"Bad dimension %s\" % (d,))\n    return d\n  name, size = d\n  if isinstance(name, str) and isinstance(size, int):\n    return Dimension(name, size)\n  else:\n    raise ValueError(\"could not convert %s to Dimension\" % (d,))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert input to a Shape.", "response": "def convert_to_shape(x):\n  \"\"\"Converts input to a Shape.\n\n  Args:\n    x: Shape, str, or None.\n\n  Returns:\n    Shape or None.\n\n  Raises:\n    ValueError: If x cannot be converted to a Shape.\n  \"\"\"\n  if x is None:\n    return None\n  if isinstance(x, Shape):\n    return x\n  if isinstance(x, str):\n    x = _parse_string_to_list_of_pairs(x, seconds_to_int=True)\n  return Shape(x)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting input to a LayoutRules.", "response": "def convert_to_layout_rules(x):\n  \"\"\"Converts input to a LayoutRules.\n\n  Args:\n    x: LayoutRules, str, or set-like of string pairs.\n\n  Returns:\n    LayoutRules.\n  \"\"\"\n  if isinstance(x, LayoutRules):\n    return x\n  if isinstance(x, str):\n    x = _parse_string_to_list_of_pairs(x)\n  return LayoutRules(x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_args_to_laid_out_tensors(xs):\n  ret = []\n  for x in xs:\n    if hasattr(x, \"to_laid_out_tensor\"):\n      ret.append(x.to_laid_out_tensor())\n    else:\n      ret.append(x)\n  return ret", "response": "Convert list elements to laid - out - tensors when possible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nslice - wise call to any tensorflow function.", "response": "def slicewise(tf_fn,\n              xs,\n              output_shape=None,\n              output_dtype=None,\n              splittable_dims=None,\n              grad_function=None,\n              name=None):\n  \"\"\"Slice-wise call to any tensorflow function.\n\n  The output shape and dtype default to those of the first input.\n  splittable_dims is a list of Dimensions which can be split while keeping the\n  computation valid.\n\n  Args:\n    tf_fn: a function taking n tf.Tensors and returning a tf.Tensor\n    xs: a list of n Tensors\n    output_shape: a Shape (or list of shapes)\n    output_dtype: a dtype (or list of dtypes)\n    splittable_dims: a list of Dimensions which are ok to split\n    grad_function: an optional gradients function.  If None, use tf gradient.\n    name: an optional string\n\n  Returns:\n    a Tensor (or a tuple of Tensors)\n  \"\"\"\n  multiple_outputs = isinstance(output_dtype, list)\n  output_shapes = output_shape if multiple_outputs else [output_shape]\n  output_dtypes = output_dtype if multiple_outputs else [output_dtype]\n\n  op = SlicewiseOperation(\n      tf_fn,\n      xs,\n      [convert_to_shape(shape) or xs[0].shape for shape in output_shapes],\n      [dtype or xs[0].dtype for dtype in output_dtypes],\n      splittable_dims,\n      grad_function,\n      name=name)\n  return tuple(op.outputs) if multiple_outputs else op.outputs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cwise(tf_fn, xs, output_dtype=None, grad_function=None, name=None):\n  return slicewise(\n      tf_fn, xs, output_dtype=output_dtype, splittable_dims=xs[0].shape.dims,\n      grad_function=grad_function, name=name or \"cwise\")", "response": "Component - wise operation with no broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef binary_arguments_to_tensors(x1, x2):\n  if not isinstance(x1, Tensor) and not isinstance(x2, Tensor):\n    raise ValueError(\"at least one of x1 and x2 must be an mtf Tensor\")\n  elif isinstance(x1, Tensor) and isinstance(x2, Tensor):\n    return x1, x2\n  elif isinstance(x1, Tensor):\n    return x1, import_tf_tensor(\n        x1.mesh, tf.convert_to_tensor(x2, dtype=x1.dtype), Shape([]))\n  else:\n    return import_tf_tensor(x2.mesh, tf.convert_to_tensor(x1, dtype=x2.dtype),\n                            Shape([])), x2", "response": "Convert arguments of a binary operation to Tensors."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef minimum(x1, x2, output_shape=None, name=None):\n  output_shape = convert_to_shape(output_shape)\n  with tf.name_scope(name, default_name=\"minimum\"):\n    x1, x2 = binary_arguments_to_tensors(x1, x2)\n    return MinMaxOperation(\n        tf.minimum, x1, x2, output_shape=_infer_binary_broadcast_shape(\n            x1.shape, x2.shape, output_shape)).outputs[0]", "response": "Binary minimum with broadcsting.\n Arguments x1 x2 output_shape and name name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _reduce_helper(input_shape,\n                   output_shape,\n                   input_tensor_layout,\n                   reduction_fn_string=\"SUM\"):\n  \"\"\"Returns slicewise function and reduced mesh dimensions.\n\n  Args:\n    input_shape: a Shape\n    output_shape: a Shape\n    input_tensor_layout: a TensorLayout\n    reduction_fn_string: \"SUM\" or \"MAX\"\n  Returns:\n    reduce_slice_fn: a function from tf.Tensor to tf.Tensor\n    reduced_mesh_axes: a list of integers\n  \"\"\"\n  reduce_dims_indices = [\n      i for i, d in enumerate(input_shape.dims) if d not in output_shape.dims]\n  reduced_input_shape = Shape([\n      d for d in input_shape.dims if d in output_shape.dims])\n  perm = [reduced_input_shape.dims.index(d) for d in output_shape.dims]\n  def reduce_slice_fn(xslice):\n    ret = xslice\n    if reduce_dims_indices:\n      ret = reduction_fn(reduction_fn_string)(xslice, reduce_dims_indices)\n    if perm != list(xrange(len(perm))):\n      ret = tf.transpose(ret, perm)\n    return ret\n  reduced_mesh_axes = []\n  for i in reduce_dims_indices:\n    mesh_axis = input_tensor_layout[i]\n    if mesh_axis is not None:\n      reduced_mesh_axes.append(mesh_axis)\n  return reduce_slice_fn, reduced_mesh_axes", "response": "Returns slicewise function and reduced mesh dimensions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlikes tf.split. Args: x: a Tensor split_dim: a Dimension in x.shape.dims num_or_size_splits: either an integer dividing split_dim.size or a list of integers adding up to split_dim.size name: an optional string Returns: a list of Tensors.", "response": "def split(x, split_dim, num_or_size_splits, name=None):\n  \"\"\"Like tf.split.\n\n  Args:\n    x: a Tensor\n    split_dim: a Dimension in x.shape.dims\n    num_or_size_splits: either an integer dividing split_dim.size\n       or a list of integers adding up to split_dim.size\n    name: an optional string\n  Returns:\n    a list of Tensors.\n  \"\"\"\n  return SplitOperation(x, split_dim, num_or_size_splits, name=name).outputs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stack(xs, dim_name, axis=0, name=None):\n  ret = StackOperation(xs, dim_name, axis, name).outputs[0]\n  return ret", "response": "Stack multiple Tensors to make a new dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cumsum(x, dim, exclusive=False):\n  with tf.variable_scope(\"cumsum\"):\n    new_name = \"tmp_dim_cumsum\"\n    new_dim = Dimension(new_name, dim.size)\n    new_shape = x.shape.rename_dimension(dim.name, new_name)\n    comparator = less if exclusive else less_equal\n    m = cast(\n        comparator(mtf_range(x.mesh, dim, dtype=tf.float32),\n                   mtf_range(x.mesh, new_dim, dtype=tf.float32)), x.dtype)\n    ret = einsum([x, m], output_shape=new_shape)\n    return reshape(ret, x.shape)", "response": "Cumulative sum.\n\n  Args:\n    x: a Tensor\n    dim: a Dimension\n    exclusive: a boolean\n\n  Returns:\n    a Tensor with the same shape as x."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn slicewise function and reduced mesh dimensions.", "response": "def _einsum_helper(input_shapes, output_shape, mesh_impl):\n  \"\"\"Returns slicewise function and reduced mesh dimensions.\n\n  Assumes the output shape contains no new dimensions.\n\n  Args:\n    input_shapes: a list of Shapes\n    output_shape: a Shape\n    mesh_impl: a MeshImpl\n  Returns:\n    einsum_slice_fn: a function from tf.Tensors to tf.Tensor\n    reduced_mesh_axes: a list of integers\n  \"\"\"\n  input_shape_union = _shape_union(input_shapes)\n  total_num_dims = input_shape_union.ndims\n  # list of input shapes that contain all dimensions.\n  full_shapes = [\n      s for s in input_shapes + [output_shape] if s.ndims == total_num_dims]\n  full_shape = full_shapes[0] if full_shapes else input_shape_union\n  reduce_slice_fn, reduced_mesh_axes = _reduce_helper(\n      full_shape, output_shape, mesh_impl.tensor_layout(full_shape))\n  def einsum_slice_fn_naive(*slices):\n    # naive einsum implementation where we broadcast all inputs to the full\n    # shape, multiply componentwise, then reduce.\n    return reduce_slice_fn(functools.reduce(tf.multiply, [\n        _expand_dims(x, input_shape, full_shape)\n        for x, input_shape in zip(slices, input_shapes)]))\n  if full_shapes:\n    # it is not wasteful of space to broadcast fully and then reduce.\n    # this helps to avoid some inefficient GPU implementations.\n    einsum_slice_fn = einsum_slice_fn_naive\n  else:\n    # call tf.einsum\n    equation = _einsum_equation(input_shapes, output_shape)\n    def einsum_slice_fn(*slices):\n      if slices[0].dtype.is_floating:\n        return mesh_impl.einsum(equation, *slices)\n      else:\n        return einsum_slice_fn_naive(*slices)\n  return einsum_slice_fn, reduced_mesh_axes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shift(x, offset, dim, wrap, name=None):\n  return ShiftOperation(x, offset, dim, wrap, name=name).outputs[0]", "response": "Shift x right by + offset in dimension dim."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_laid_out_tensor(mesh, laid_out_tensor, shape, name=None):\n  return ImportLaidOutTensorOperation(\n      mesh, laid_out_tensor, convert_to_shape(shape), name=name).outputs[0]", "response": "Import a laid_out_tensor.\n\n  For expert users.\n  The input must be laid out appropriately given the eventual MeshImpl,\n  and layout.\n\n  Args:\n    mesh: a Mesh\n    laid_out_tensor: a LaidOutTensor\n    shape: a mtf.Shape\n    name: an optional string\n\n  Returns:\n   a mtf.Tensor"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new variable or retrieve an already - created one.", "response": "def get_variable(mesh, name, shape, dtype=tf.float32,\n                 master_dtype=None, slice_dtype=None, activation_dtype=None,\n                 initializer=None, trainable=True,\n                 **kwargs):\n  \"\"\"Create a new variable or retrieve an already-created one.\n\n  Args:\n    mesh: a Mesh\n    name: a string (uses the existing tf.variable_scope())\n    shape: a Shape\n    dtype: a VariableDType or a tf.DType\n    master_dtype: an optional tf.DType (deprecated - use dtype arg)\n    slice_dtype: an optional tf.DType (deprecated - use dtype arg)\n    activation_dtype: an optional tf.DType (deprecated - use dtype arg)\n    initializer: an optional tf initializer function\n    trainable: a boolean\n    **kwargs: additional keyword arguments to tf.get_variable\n\n  Returns:\n    a Tensor with the given shape and dtype equal to dtype.activation_dtype\n  \"\"\"\n  if dtype is None:\n    dtype = VariableDType(master_dtype, slice_dtype, activation_dtype)\n  elif isinstance(dtype, tf.DType):\n    dtype = VariableDType(\n        master_dtype or dtype, slice_dtype or dtype, activation_dtype or dtype)\n  elif not isinstance(dtype, VariableDType):\n    raise ValueError(\"dtype should be a tf.dtype or a mtf.VariableDType\")\n  scope_name = tf.get_variable_scope().name\n  if scope_name:\n    full_name = scope_name + \"/\" + name\n  else:\n    full_name = name\n  if full_name in mesh.graph.name_to_variable:\n    var = mesh.graph.name_to_variable[full_name]\n  else:\n    var = Variable(\n        mesh, name, convert_to_shape(shape), dtype, initializer, trainable,\n        **kwargs)\n    if var.name != full_name:\n      raise ValueError(\n          \"Expected var.name == full_name.  %s vs %s\" % (var.name, full_name))\n    mesh.graph.name_to_variable[full_name] = var\n  return var.outputs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign a new value to a variable.", "response": "def assign(var, new_val, assign_fn=assign_slice):\n  \"\"\"Assign a new value to a variable.\n\n  Args:\n    var: either a Variable operation or its output Tensor.\n    new_val: a Tensor\n    assign_fn: a function from\n        (mtf.Variable, tf.Variable, tf.Tensor) -> tf.Operation\n  Returns:\n    an Operation\n  Raises:\n    ValueError: if var is not a Variable and var.operation is not a Variable\n  \"\"\"\n  if isinstance(var, Tensor):\n    var = var.operation\n  if not isinstance(var, Variable):\n    raise ValueError(\"var must be a mtf.Variable or its output Tensor.\")\n  return Assign([var], [new_val], assign_fn=assign_fn)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint a tensor with a message.", "response": "def Print(x, data, message, **kwargs):  # pylint: disable=invalid-name\n  \"\"\"Call tf.Print.\n\n  Args:\n    x: a Tensor.\n    data: a list of Tensor\n    message: a string\n    **kwargs: keyword arguments to tf.Print\n  Returns:\n    a Tensor which is identical in value to x\n  \"\"\"\n  return PrintOperation(x, data, message, **kwargs).outputs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rename_dimension(x, old_name, new_name):\n  return reshape(x, x.shape.rename_dimension(old_name, new_name))", "response": "Reshape a Tensor x renaming one dimension."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace_dimensions(tensor_or_shape, old_dim_or_dims, new_dim_or_dims):\n  if isinstance(tensor_or_shape, Tensor):\n    return reshape(tensor_or_shape, replace_dimensions(\n        tensor_or_shape.shape, old_dim_or_dims, new_dim_or_dims))\n  if not isinstance(tensor_or_shape, Shape):\n    raise ValueError(\n        \"tensor_or_shape must be a Tensor or Shape got %s\" % (tensor_or_shape,))\n  in_dims = tensor_or_shape.dims\n  if isinstance(old_dim_or_dims, Dimension):\n    old_dim_or_dims = [old_dim_or_dims]\n  if isinstance(new_dim_or_dims, Dimension):\n    new_dim_or_dims = [new_dim_or_dims]\n  if not isinstance(old_dim_or_dims, list) or not old_dim_or_dims:\n    raise ValueError(\n        \"old_dim_or_dims must be a Dimension or a list of Dimension got %s\"\n        % (old_dim_or_dims,))\n  if not isinstance(new_dim_or_dims, list) or not new_dim_or_dims:\n    raise ValueError(\n        \"new_dim_or_dims must be a Dimension or a list of Dimension got %s\"\n        % (new_dim_or_dims,))\n  try:\n    positions = [in_dims.index(d) for d in old_dim_or_dims]\n    pos = positions[0]\n    if positions != list(range(pos, pos + len(positions))):\n      raise ValueError()\n  except ValueError:\n    raise ValueError(\n        \"old_dim_or_dims must be a subsequence of the input's dimensions\"\n        \" old_dim_or_dims=%s input's dimensions=%s\" %\n        (old_dim_or_dims, in_dims))\n  return Shape(in_dims[:pos] + new_dim_or_dims +\n               in_dims[pos + len(old_dim_or_dims):])", "response": "Replace dimensions in a Tensor or Shape."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef matmul(a, b, output_shape=None, reduced_dims=None, name=None):\n  return einsum(\n      [a, b], output_shape=output_shape, reduced_dims=reduced_dims, name=name)", "response": "A version of scipy. sparse. matrix. matmul that supports both arrays and lists."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _reduction_output_shape(x, output_shape, reduced_dim):\n  if output_shape is None:\n    if reduced_dim is None:\n      return Shape([])\n    else:\n      if reduced_dim not in x.shape.dims:\n        raise ValueError(\n            \"reduced_dim=%s not in x.shape.dims=%s\" % (reduced_dim, x.shape))\n      return x.shape - reduced_dim\n  if reduced_dim is not None:\n    if [reduced_dim] != [d for d in x.shape.dims if d not in output_shape.dims]:\n      raise ValueError(\n          \"reduced_dim contradicts output_shape:\"\n          \"x=%s output_shape=%s reduced_dim=%s\" %\n          (x, output_shape, reduced_dim))\n  return output_shape", "response": "Helper function to reduce_sum etc."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reduce_sum(x,\n               disable_positional_args=None,\n               output_shape=None,\n               reduced_dim=None,\n               name=None):\n  \"\"\"Reduction on 1 or more axes.\n\n  If reduced_dim is present, then only that dimension is reduced out.\n  Alternatively, specify output_shape.\n  Do not specify both reduced_dim and output_shape.\n  If neither is specified, then all dimensions are reduced out.\n\n  Args:\n    x: a Tensor\n    disable_positional_args: None\n    output_shape: an optional Shape.  Must be a subsequence of x.shape.\n    reduced_dim: a mtf.Dimension\n    name: an optional string\n  Returns:\n    a Tensor\n  \"\"\"\n  output_shape = convert_to_shape(output_shape)\n  reduced_dim = convert_to_dimension(reduced_dim)\n  assert disable_positional_args is None\n  output_shape = _reduction_output_shape(x, output_shape, reduced_dim)\n  if output_shape == x.shape:\n    return x\n  return ReduceOperation(x, output_shape, \"SUM\", name=name).outputs[0]", "response": "Reduces the tensor x by summing the output_shape and reduced_dim."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreduce the mean of a tensor x.", "response": "def reduce_mean(x,\n                disable_positional_args=None,\n                output_shape=None,\n                reduced_dim=None,\n                name=None):\n  \"\"\"Reduction on 1 or more axes.\n\n  If reduced_dim is present, then only that dimension is reduced out.\n  Alternatively, specify output_shape.\n  Do not specify both reduced_dim and output_shape.\n  If neither is specified, then all dimensions are reduced out.\n\n  Args:\n    x: a Tensor\n    disable_positional_args: None\n    output_shape: an optional Shape. Must be a subsequence of x.shape.\n    reduced_dim: a mtf.Dimension\n    name: an optional string\n\n  Returns:\n    a Tensor\n  \"\"\"\n  output_shape = convert_to_shape(output_shape)\n  reduced_dim = convert_to_dimension(reduced_dim)\n  assert disable_positional_args is None\n  output_shape = _reduction_output_shape(x, output_shape, reduced_dim)\n  with tf.variable_scope(name, default_name=\"reduce_mean\"):\n    if output_shape == x.shape:\n      return x\n    return reduce_sum(\n        x, output_shape=output_shape) * (output_shape.size / x.shape.size)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreducing the tensor x by the maximum value of x.", "response": "def reduce_max(x,\n               disable_positional_args=None,\n               output_shape=None,\n               reduced_dim=None,\n               name=None):\n  \"\"\"Reduction on 1 or more axes.\n\n  Args:\n    x: a Tensor\n    disable_positional_args: None\n    output_shape: an optional Shape.  Must be a subsequence of x.shape.\n    reduced_dim: an optional Dimension\n    name: an optional string\n  Returns:\n    a Tensor\n  \"\"\"\n  output_shape = convert_to_shape(output_shape)\n  reduced_dim = convert_to_dimension(reduced_dim)\n  assert disable_positional_args is None\n  output_shape = _reduction_output_shape(x, output_shape, reduced_dim)\n  if output_shape is None:\n    output_shape = Shape([])\n  if output_shape == x.shape:\n    return x\n  return ReduceOperation(\n      x, output_shape, \"MAX\", name=name or \"reduce_max\").outputs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlikes tf. top_k. This operation returns two tensors with the same shape and shape as x.", "response": "def top_k(x, reduced_dim, new_dim, dtype=tf.int32, name=None):\n  \"\"\"Like tf.top_k.\n\n  This operation returns two tensors with the same shape.  The output shape\n  is identical to the shape of x, except that reduced_dim is replaced by\n  new_dim.\n\n  Args:\n    x: a Tensor\n    reduced_dim: a Dimension in x.shape.dims.\n    new_dim: a Dimension.  The size determines k.\n    dtype: optional dtype for indices.\n    name: optional string.\n  Returns:\n    indices: a Tensor with given dtype.\n    values: a Tensor with same type as x.\n  \"\"\"\n  reduced_dim = convert_to_dimension(reduced_dim)\n  new_dim = convert_to_dimension(new_dim)\n  indices = []\n  values = []\n  k = new_dim.size\n  with tf.name_scope(name, default_name=\"top_k\"):\n    for i in xrange(k):\n      max_index, max_val = top_1(x, reduced_dim, dtype)\n      indices.append(max_index)\n      values.append(max_val)\n      if i + 1 < k:\n        x += one_hot(max_index, reduced_dim, on_value=-1e9, dtype=x.dtype)\n  axis = x.shape.dims.index(reduced_dim)\n  return stack(indices, new_dim.name, axis), stack(values, new_dim.name, axis)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(x1, x2, output_shape=None, name=None):\n  output_shape = convert_to_shape(output_shape)\n  if not isinstance(x2, Tensor):\n    return ScalarAddOperation(x1, x2).outputs[0]\n  with tf.name_scope(name, default_name=\"add\"):\n    x1, x2 = binary_arguments_to_tensors(x1, x2)\n    return AddOperation(\n        x1, x2, output_shape=_infer_binary_broadcast_shape(\n            x1.shape, x2.shape, output_shape)).outputs[0]", "response": "Binary addition with broadcsting.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sub(x1, x2, output_shape=None, name=None):\n  output_shape = convert_to_shape(output_shape)\n  if not isinstance(x2, Tensor):\n    return ScalarAddOperation(x1, -x2).outputs[0]\n  with tf.name_scope(name, default_name=\"sub\"):\n    x1, x2 = binary_arguments_to_tensors(x1, x2)\n    return add(x1, negative(x2), output_shape=output_shape)", "response": "Binary subtraction with broadcsting.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mtf_slice(x, begin, size, slice_dim_name, name=None):\n  return SliceOperation(\n      x, begin, size, slice_dim_name, name=name).outputs[0]", "response": "Slice operation.\n\n  Call externally as mtf.slice()\n\n  Args:\n    x: a list of Tensors\n    begin: integer, where to begin slicing from along the axis\n    size: integer, size to slice from axis.\n    slice_dim_name: string, dimension name of slicing axis.\n    name: an optional string\n  Returns:\n    a Tensor with shape extended by output_shape for the last axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nslices operation. Args: x: a list of Tensors paddings: list of integers of size 2, padding size before and after for dim. dim_name: string, name for the padding dim name: an optional string Returns: a Tensor with shape extended by output_shape for the last axis.", "response": "def pad(x, paddings, dim_name, name=None):\n  \"\"\"Slice operation.\n\n  Args:\n    x: a list of Tensors\n    paddings: list of integers of size 2, padding size before and after for dim.\n    dim_name: string, name for the padding dim\n    name: an optional string\n  Returns:\n    a Tensor with shape extended by output_shape for the last axis.\n  \"\"\"\n  return PadOperation(\n      x, paddings, dim_name, name=name).outputs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef one_hot(indices, output_dim, on_value=1.0,\n            off_value=0.0, dtype=tf.float32, name=None):\n  \"\"\"One hot operation.\n\n  TODO(noam): Is there a good reason we need a special mtf.Operation here?\n  We could just use some code like this:\n  cast(equal(indices, mtf_range(indices.mesh, output_dim, dtype=indices.dtype)),\n       dtype)\n\n  Args:\n    indices: a Tensor\n    output_dim: a Dimension\n    on_value: Value taken when indices are on at a location, default 1\n    off_value: Value taken when indices are off at a location, default 0\n    dtype: a tf.DType\n    name: an optional string\n  Returns:\n    a Tensor with shape extended by output_dim for the last axis.\n  \"\"\"\n  return OneHotOperation(\n      indices, output_dim, on_value, off_value, dtype, name=name).outputs[0]", "response": "One hot operation.\n\n  TODO(noam): Is there a good reason we need a special mtf.Operation here?\n  We could just use some code like this:\n  cast(equal(indices, mtf_range(indices.mesh, output_dim, dtype=indices.dtype)),\n       dtype)\n\n  Args:\n    indices: a Tensor\n    output_dim: a Dimension\n    on_value: Value taken when indices are on at a location, default 1\n    off_value: Value taken when indices are off at a location, default 0\n    dtype: a tf.DType\n    name: an optional string\n  Returns:\n    a Tensor with shape extended by output_dim for the last axis."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gradients(ys, xs, grad_ys=None):\n  graph = ys[0].graph\n  if not grad_ys:\n    grad_ys = [Constant(y.mesh, 1.0, y.shape, y.dtype).outputs[0] for y in ys]\n  # figure out what Tensors are downstream of xs\n  downstream = set(xs)\n  for op in graph.operations:\n    if op.has_gradient:\n      if set(op.inputs) & downstream:\n        downstream |= set(op.outputs)\n  tensor_to_gradient = dict(zip(ys, grad_ys))\n  for op in graph.operations[::-1]:\n    grad_outputs = [tensor_to_gradient.get(out) for out in op.outputs]\n    if op.has_gradient and any(grad_outputs) and (set(op.inputs) & downstream):\n      with tf.variable_scope(op.name + \"/gradients\"):\n        input_grads = op.gradient(grad_outputs)\n        for inp, grad in zip(op.inputs, input_grads):\n          if inp in downstream and grad is not None:\n            if inp in tensor_to_gradient:\n              tensor_to_gradient[inp] += grad\n            else:\n              tensor_to_gradient[inp] = grad\n  return [tensor_to_gradient.get(x, None) for x in xs]", "response": "Compute gradients in dtf.\n\n Returns a list of gradients in dtf.\n\n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninfer the shape of a binary op with broadcasting.", "response": "def _infer_binary_broadcast_shape(shape1, shape2, given_output_shape=None):\n  \"\"\"Infer shape of the output of a binary op with broadcasting.\n\n  If the output shape is not given with given_output_shape, then we check\n  to see if one of the shapes is a subsequence of the other one, and we\n  return the one that is the supersequence.  Otherwise, we list the dimensions\n  of shape1, followed by all new dimensions in shape2.\n\n  Args:\n    shape1: a Shape\n    shape2: a Shape\n    given_output_shape: an optional Shape\n  Returns:\n    a Shape\n  \"\"\"\n  shape1 = convert_to_shape(shape1)\n  shape2 = convert_to_shape(shape2)\n  given_output_shape = convert_to_shape(given_output_shape)\n  if given_output_shape is not None:\n    return given_output_shape\n  if is_subsequence(shape1.dims, shape2.dims):\n    return shape2\n  if is_subsequence(shape2.dims, shape1.dims):\n    return shape1\n  return Shape(\n      shape1.dims + [d for d in shape2.dims if d not in shape1.dims])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexpand dimensions and transpose if necessary.", "response": "def _expand_dims(x, input_shape, output_shape):\n  \"\"\"Expand dimensions and transpose if necessary.\n\n  Args:\n    x: a tf.Tensor\n    input_shape: a Shape\n    output_shape: a Shape whose dimensions are a superset of\n      those in input_shape\n\n  Returns:\n    a tf.Tensor\n  \"\"\"\n  verify_no_new_dims([output_shape], input_shape)\n  if input_shape == output_shape or input_shape.ndims == 0:\n    return x\n  perm = [input_shape.dims.index(d) for d in output_shape.dims\n          if d in input_shape.dims]\n  x = tf.transpose(x, perm)\n  for i, d in enumerate(output_shape.dims):\n    if d not in input_shape.dims:\n      x = tf.expand_dims(x, i)\n  return x"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nturning shapes into an einsum equation.", "response": "def _einsum_equation(input_shapes, output_shape):\n  \"\"\"Turn shapes into an einsum equation.\n\n  e.g. \"ij,jk->ik\"\n\n  Args:\n    input_shapes: a list of Shapes\n    output_shape: a Shape\n  Returns:\n    a string\n  \"\"\"\n  ret = []\n  next_letter = ord(\"a\")\n  dim_to_letter = {}\n  for shape_num, shape in enumerate(input_shapes + [output_shape]):\n    if shape_num == len(input_shapes):\n      ret.append(\"->\")\n    elif shape_num > 0:\n      ret.append(\",\")\n    for d in shape.dims:\n      if d not in dim_to_letter:\n        dim_to_letter[d] = chr(next_letter)\n        next_letter += 1\n      ret.append(dim_to_letter[d])\n\n  return \"\".join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_subsequence(short_seq, long_seq):\n  if not short_seq:\n    return True\n  pos = 0\n  for x in long_seq:\n    if pos == len(short_seq):\n      return True\n    if short_seq[pos] == x:\n      pos += 1\n  if pos == len(short_seq):\n    return True\n  return False", "response": "Is short_seq a subsequence of long_seq?"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying that all dimensions in the output are in at least one input.", "response": "def verify_no_new_dims(input_shapes, output_shape):\n  \"\"\"Verifies that all dimensions in the output are in at least one input.\n\n  Args:\n    input_shapes: a list of Shapes\n    output_shape: a Shape\n  Raises:\n    ValueError: if there are new dimensions in the output.\n  \"\"\"\n  all_input_dims = set(sum([s.dims for s in input_shapes], []))\n  all_output_dims = set(output_shape.dims)\n  if not all_output_dims.issubset(all_input_dims):\n    raise ValueError(\n        \"No new dimensions allowed in output\"\n        \" input_shapes = %s output_shape= %s\"\n        % ([s.dims for s in input_shapes], output_shape.dims))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pnum_to_processor_coordinates(mesh_shape, pnum):\n  ret = []\n  for dimsize in mesh_shape.to_integer_list[::-1]:\n    ret.append(pnum % dimsize)\n    pnum //= dimsize\n  return ret[::-1]", "response": "Returns the coordinates of a processor in the mesh."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninverse of pnum_to_processor_coordinates. Args: mesh_shape: a Shape coord: a list of integers with length len(mesh_shape) Returns: an integer less than len(mesh_shape)", "response": "def processor_coordinates_to_pnum(mesh_shape, coord):\n  \"\"\"Inverse of pnum_to_processor_coordinates.\n\n  Args:\n    mesh_shape: a Shape\n    coord: a list of integers with length len(mesh_shape)\n\n  Returns:\n    an integer less than len(mesh_shape)\n  \"\"\"\n  ret = 0\n  multiplier = 1\n  for c, d in zip(coord[::-1], mesh_shape.to_integer_list[::-1]):\n    ret += multiplier * c\n    multiplier *= d\n  return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pnum_to_group(mesh_shape, group_dims, pnum):\n  coord = pnum_to_processor_coordinates(mesh_shape, pnum)\n  remaining_shape = Shape(\n      [d for i, d in enumerate(mesh_shape) if i not in group_dims])\n  remaining_coord = [d for i, d in enumerate(coord) if i not in group_dims]\n  return processor_coordinates_to_pnum(remaining_shape, remaining_coord)", "response": "Return the number of the processor that is grouped over the given processor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngroups of processors which differ only in the given dimensions.", "response": "def processor_groups(mesh_shape, group_dims):\n  \"\"\"Groups of processors which differ only in the given dimensions.\n\n  Args:\n    mesh_shape: a Shape\n    group_dims: a list of integers\n\n  Returns:\n    a list of lists of integers (processor numbers)\n  \"\"\"\n  group_numbers = [\n      pnum_to_group(mesh_shape, group_dims, pnum)\n      for pnum in xrange(mesh_shape.size)]\n  ret = []\n  for pnum, g in enumerate(group_numbers):\n    while len(ret) <= g:\n      ret.append([])\n    ret[g].append(pnum)\n  return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef log_softmax(x, reduced_dim, extra_logit=None, name=None):\n  return x - reduce_logsumexp(\n      x, reduced_dim, extra_logit=extra_logit, name=name)", "response": "Log - softmax of a Tensor x."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a 1d mesh tensor with a range from [ 0 dim. size ).", "response": "def mtf_range(mesh, dim, dtype, name=None):\n  \"\"\"Create a 1d mesh tensor with a range from [0, dim.size).\n\n  Call externally as mtf.range()\n\n  Args:\n    mesh: a Mesh\n    dim: a Dimension\n    dtype: a tf.DType\n    name: an optional string\n\n  Returns:\n    a Tensor\n  \"\"\"\n  dim = convert_to_dimension(dim)\n  with tf.variable_scope(name, default_name=\"range\"):\n    if dtype == tf.bfloat16:\n      # tf.range(dtype=bfloat16) gives the wrong shape.\n      # TODO(noam): report the bug.\n      tf_range = tf.cast(tf.range(dim.size), tf.bfloat16)\n    else:\n      tf_range = tf.range(dim.size, dtype=dtype)\n    return import_tf_tensor(mesh, tf_range, shape=Shape([dim]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pretty_print_counters(counters):\n  totals = collections.defaultdict(int)\n  for (name, val) in counters:\n    prefixes = [name[:i] for i in xrange(len(name)) if name[i] == \"/\"] + [name]\n    for p in prefixes:\n      totals[p] += val\n  parts = []\n  for name, val in sorted(six.iteritems(totals)):\n    parts.append(\" \" * name.count(\"/\") + \"%s: %.3g\" % (name, val))\n  return \"\\n\".join(parts)", "response": "Pretty print a list of counters hierarchically."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_string_to_list_of_pairs(s, seconds_to_int=False):\n  ret = []\n  for p in [s.split(\":\") for s in re.sub(\"[,.;]\", \" \", s).split()]:\n    if len(p) != 2:\n      raise ValueError(\"bad input to _parse_string_to_list_of_pairs %s\" % s)\n    if seconds_to_int:\n      ret.append((p[0], int(p[1])))\n    else:\n      ret.append(tuple(p))\n  return ret", "response": "r Parses a string into a list of pairs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall a function once on each device.", "response": "def parallel(devices, fn, *args, **kwargs):\n  \"\"\"Call a function once on each device.\n\n  Args:\n    devices: a list of n devices\n    fn: a function\n    *args: arguments, each of which is a list of length n\n    **kwargs: keyword-args, each of which is a list of length n\n  Returns:\n    a list of length n\n  Raises:\n    ValueError: if the arguments are not all lists of length n\n  \"\"\"\n  if not isinstance(devices, list):\n    raise ValueError(\"devices must be a list\")\n  for x in list(args) + list(six.itervalues(kwargs)):\n    if not isinstance(x, list) or len(x) != len(devices):\n      raise ValueError(\n          \"Argument not a list with same length as devices \"\n          \"arg=%s devices=%s\" % (x, devices))\n  ret = []\n  for i, device in enumerate(devices):\n    with tf.device(device):\n      with tf.variable_scope(\"parallel_%d\" % i):\n        my_args = [x[i] for x in args]\n        my_kwargs = {k: v[i] for k, v in six.iteritems(kwargs)}\n        ret.append(fn(*my_args, **my_kwargs))\n  return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dropout(x, keep_prob, noise_shape=None, name=None):\n  noise_shape = convert_to_shape(noise_shape)\n  if noise_shape is None:\n    noise_shape = x.shape\n  with tf.variable_scope(name, default_name=\"dropout\"):\n    if keep_prob == 1.0:\n      return x\n    noise = cast(less(random_uniform(\n        x.mesh, noise_shape, dtype=x.dtype), keep_prob), x.dtype)\n    noise /= keep_prob\n    return x * noise", "response": "Dropout layer.\n Arguments x is a Tensor containing noise and keep_prob is a float between 0. 0 and 1. 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlog the sizes and shapes of variables and the total size.", "response": "def log_variable_sizes(var_list, tag, verbose=True, mesh_to_impl=None):\n  \"\"\"Log the sizes and shapes of variables, and the total size.\n\n  Args:\n    var_list: a list of variables; defaults to trainable_variables\n    tag: a string; defaults to \"Trainable Variables\"\n    verbose: bool, if True, log every weight; otherwise, log total size only.\n    mesh_to_impl: an optional map from Mesh to MeshImpl\n  \"\"\"\n  if not var_list:\n    return\n\n  name_to_var = {v.name: v for v in var_list}\n  total_size = 0\n  total_slice_size = 0\n  for v_name in sorted(list(name_to_var)):\n    v = name_to_var[v_name]\n    v_size = v.shape.size\n    if mesh_to_impl is not None:\n      slice_size = mesh_to_impl[v.mesh].slice_size(v.shape)\n    else:\n      slice_size = 0\n    total_slice_size += slice_size\n    if verbose:\n      tf.logging.info(\n          \"Variable %s size %s slice_size %s %s\",\n          v.name.ljust(60),\n          str(v_size).ljust(12),\n          str(slice_size).ljust(12),\n          str(v.shape).ljust(60))\n      if isinstance(v, StackedVariable):\n        for n in v.original_names:\n          tf.logging.info(\"    \" + n)\n    total_size += v_size\n  tf.logging.info(\"%s count: %s  Total size: %s  Total slice_size: %s\",\n                  tag.ljust(30), str(len(var_list)).ljust(6),\n                  str(total_size).ljust(15),\n                  str(total_slice_size).ljust(15))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwhile Loop. See comments above for WhileLoopOperation num_loop_vars is a hack for the multi-gpu setup. In this case, loops are generally slow, as all loop variables are placed on device. By setting num_loop_vars=k, then all of the loop variables except for the first k are handled as mtf Variables instead of loop variables, using explicit updates and control dependencies. In this case, we only return the first num_loop_vars outputs. Do not use this option on TPU, since it is unnecessary and also produces incorrect results, since xla does not respect control dependencies. Args: cond_fn: a function from n Tensors to scalar boolean Tensor body_fn: a function from n Tensors to list of n Tensors inputs: a list of n Tensors num_loop_vars: an optional integer. has_accumulators: a boolean **kwargs: additional kwargs passed to tf.while_loop Returns: a list of n Tensors.", "response": "def while_loop(cond_fn, body_fn, inputs, num_loop_vars=None,\n               has_accumulators=False, **kwargs):\n  \"\"\"While Loop.\n\n  See comments above for WhileLoopOperation\n\n  num_loop_vars is a hack for the multi-gpu setup.  In this case, loops\n  are generally slow, as all loop variables are placed on device.  By setting\n  num_loop_vars=k, then all of the loop variables except for the first k\n  are handled as mtf Variables instead of loop variables, using explicit\n  updates and control dependencies.  In this case, we only return the\n  first num_loop_vars outputs.  Do not use this option on TPU, since it\n  is unnecessary and also produces incorrect results, since xla does not\n  respect control dependencies.\n\n  Args:\n    cond_fn: a function from n Tensors to scalar boolean Tensor\n    body_fn: a function from n Tensors to list of n Tensors\n    inputs: a list of n Tensors\n    num_loop_vars: an optional integer.\n    has_accumulators: a boolean\n    **kwargs: additional kwargs passed to tf.while_loop\n\n  Returns:\n    a list of n Tensors.\n  \"\"\"\n  if num_loop_vars is None:\n    return WhileLoopOperation(cond_fn, body_fn, inputs, tf_kwargs=kwargs,\n                              has_accumulators=has_accumulators).outputs\n  # Turn all loop vars except for the first ones into non-loop vars.\n  # see comments in docstring.\n  assert num_loop_vars > 0\n  extra_inputs = inputs[num_loop_vars:]\n  my_vars = []\n  for i, x in enumerate(extra_inputs):\n    my_vars.append(get_variable(\n        x.mesh, \"loop_var_%d\" % i,\n        x.shape, initializer=tf.zeros_initializer(),\n        dtype=x.dtype,\n        collections=[tf.GraphKeys.LOCAL_VARIABLES]))\n  my_vars = tuple(my_vars)\n  first_input = depend(\n      inputs[0], [assign(var, x) for var, x in zip(my_vars, extra_inputs)])\n  inputs = [first_input] + inputs[1:num_loop_vars]\n  def my_cond_fn(*inputs):\n    return cond_fn(*(inputs + my_vars))\n  def my_body_fn(*inputs):\n    outputs = tuple(body_fn(*(inputs + my_vars)))\n    extra_outputs = outputs[num_loop_vars:]\n    first_output = depend(\n        outputs[0], [assign(var, x) for var, x in zip(my_vars, extra_outputs)])\n    outputs = (first_output,) + outputs[1:num_loop_vars]\n    return outputs\n  return WhileLoopOperation(\n      my_cond_fn, my_body_fn, inputs, tf_kwargs=kwargs,\n      has_accumulators=has_accumulators).outputs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _tf_flatten_batch_dims(x, num_nonbatch_dims):\n  shape = x.shape.as_list()\n  assert None not in shape\n  new_shape = ([list_product(shape[:-num_nonbatch_dims])]\n               + shape[-num_nonbatch_dims:])\n  if new_shape != shape:\n    x = tf.reshape(x, new_shape)\n  return x", "response": "Flatten all but last num_nonbatch_dims into one dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tf_restore_batch_dims(x, num_nonbatch_dims, prototype):\n  assert x.shape.ndims == 1 + num_nonbatch_dims\n  new_shape = (\n      prototype.shape.as_list()[:-num_nonbatch_dims] + x.shape.as_list()[1:])\n  assert None not in new_shape\n  if new_shape != x.shape.as_list():\n    x = tf.reshape(x, new_shape)\n  return x", "response": "Reverse op of _tf_flatten_batch_dims."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconcatenates each block with the margins of adjacent blocks.", "response": "def halo_exchange(x, blocks_dim, block_size_dim, halo_size, wrap=False):\n  \"\"\"Concat each block with the margins of adjacent blocks.\n\n  Get left and right blocks_dim and concatenate along block_size_dim.\n\n  Args:\n    x: a Tensor.\n    blocks_dim: a Dimension in x.shape\n    block_size_dim: a Dimension in x.shape\n    halo_size: an integer\n    wrap: a boolean\n\n  Returns:\n    a Tensor with the same shape as x, other than in block_size_dim, whose\n    size is increased by 2*halo_size.\n  \"\"\"\n  if halo_size == 0:\n    return x\n\n  block_size = block_size_dim.size\n  partial_size = halo_size % block_size\n  num_complete_blocks = halo_size // block_size\n  parts = [x]\n\n  for i in xrange(1, num_complete_blocks + 1):\n    parts = ([shift(x, i, blocks_dim, wrap)] + parts +\n             [shift(x, -i, blocks_dim, wrap)])\n  if partial_size > 0:\n    left_margin = mtf_slice(x, 0, partial_size, block_size_dim.name)\n    right_margin = mtf_slice(\n        x, block_size_dim.size - partial_size, partial_size,\n        block_size_dim.name)\n    parts = (\n        [shift(right_margin, num_complete_blocks + 1, blocks_dim, wrap)]\n        + parts +\n        [shift(left_margin, -(num_complete_blocks + 1), blocks_dim, wrap)])\n  return concat(parts, block_size_dim.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef conv2d_with_blocks(\n    conv_input,\n    conv_filter,\n    strides,\n    padding,\n    h_blocks_dim=None,\n    w_blocks_dim=None,\n    name=None):\n  \"\"\"conv2d operation with spatial partitioning.\n\n  Spatial partitioning is implemented by decomposing the image into blocks.\n  Block dimensions represented as h_blocks_dim and w_blocks_dim can be split\n  along the mesh axis. If split, then we do a halo exchange where each block\n  receives the part of the image from its left and right neighbors necessary to\n  do the convolution. Exchange can involve complete or partial blocks depending\n  on the filter height and width.\n\n  Currently, only \"SAME\" padding with dilation rate of 1 is supported.\n\n  Args:\n    conv_input: a Tensor of shape\n      [batch, h_blocks_dim, w_blocks_dim, h_dim, w_dim, in_channels_dim]\n    conv_filter: a Tensor of shape\n      [filter_height, filter_width, in_channels_dim, out_channels_dim]\n    strides: A list of ints. 1-D tensor of length 4.\n    padding: string, \"SAME\". The type of padding algorithm to use.\n      Valid is not currently supported.\n    h_blocks_dim: Dimension representing number of height blocks.\n    w_blocks_dim: Dimension representing number of height blocks.\n    name: A name for the operation (optional).\n\n  Returns:\n    A Tensor of shape\n      [batch, h_blocks_dim, w_blocks_dim, h_dim, w_dim, out_channels_dim]\n  \"\"\"\n  filter_h_dim, filter_w_dim = conv_filter.shape.dims[:2]\n  assert filter_h_dim.size % 2 == 1\n  assert filter_w_dim.size % 2 == 1\n  h_dim, w_dim = conv_input.shape.dims[-3:-1]\n\n  # If h_blocks_dim and w_blocks_dim is not split, directly call conv2d.\n  if h_blocks_dim is None and w_blocks_dim is None:\n    return conv2d(conv_input, conv_filter, strides, padding, name)\n\n  # Padding 'VALID' is not supported yet.\n  if padding != \"SAME\":\n    raise NotImplementedError(\"conv2d_with_blocks requires padding=SAME\")\n\n  # Halo exchange for h_blocks and w_blocks.\n  for blocks_dim, block_size_dim, halo_size in [\n      (h_blocks_dim, h_dim, filter_h_dim.size // 2),\n      (w_blocks_dim, w_dim, filter_w_dim.size // 2)]:\n    if halo_size > 0:\n      if blocks_dim is not None:\n        conv_input = halo_exchange(\n            conv_input, blocks_dim, block_size_dim, halo_size)\n      else:\n        conv_input = pad(\n            conv_input, [halo_size, halo_size], block_size_dim.name)\n  return conv2d(conv_input, conv_filter, strides, \"VALID\", name)", "response": "conv2d operation with spatial partitioning.\n\n  Spatial partitioning is implemented by decomposing the image into blocks.\n  Block dimensions represented as h_blocks_dim and w_blocks_dim can be split\n  along the mesh axis. If split, then we do a halo exchange where each block\n  receives the part of the image from its left and right neighbors necessary to\n  do the convolution. Exchange can involve complete or partial blocks depending\n  on the filter height and width.\n\n  Currently, only \"SAME\" padding with dilation rate of 1 is supported.\n\n  Args:\n    conv_input: a Tensor of shape\n      [batch, h_blocks_dim, w_blocks_dim, h_dim, w_dim, in_channels_dim]\n    conv_filter: a Tensor of shape\n      [filter_height, filter_width, in_channels_dim, out_channels_dim]\n    strides: A list of ints. 1-D tensor of length 4.\n    padding: string, \"SAME\". The type of padding algorithm to use.\n      Valid is not currently supported.\n    h_blocks_dim: Dimension representing number of height blocks.\n    w_blocks_dim: Dimension representing number of height blocks.\n    name: A name for the operation (optional).\n\n  Returns:\n    A Tensor of shape\n      [batch, h_blocks_dim, w_blocks_dim, h_dim, w_dim, out_channels_dim]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tensor_dim_to_mesh_dim_size(layout, mesh_shape, tensor_dim):\n  layout_rules = convert_to_layout_rules(layout)\n  mesh_shape = convert_to_shape(mesh_shape)\n  mesh_axis = layout_rules.tensor_dimension_to_mesh_axis(tensor_dim, mesh_shape)\n  if mesh_axis is None:\n    return 1\n  else:\n    return mesh_shape.dims[mesh_axis].size", "response": "This function returns the size of the tensor dimension in a mesh."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef serialize_training_step(features, model_fn, batch_dim, num_splits):\n  for v in features.values():\n    mesh = v.mesh\n    graph = v.graph\n  microbatch_dim = Dimension(\"microbatch\", num_splits)\n  smaller_batch_dim = Dimension(batch_dim.name, batch_dim.size // num_splits)\n  cache = {}\n  def select(t, microbatch_num):\n    return gather(\n        replace_dimensions(t, batch_dim, [smaller_batch_dim, microbatch_dim]),\n        microbatch_num, microbatch_dim)\n  def cond_fn(microbatch_num):\n    return less(microbatch_num, num_splits)\n  def body_fn(microbatch_num):\n    \"\"\"Body function for mtf.while_loop.\n\n    Args:\n      microbatch_num: a mtf Scalar\n    Returns:\n      a list of mtf Tensors\n    \"\"\"\n    my_features = {}\n    for k, v in six.iteritems(features):\n      my_features[k] = select(v, microbatch_num)\n    outputs = model_fn(my_features)\n    grads = gradients(\n        [outputs[\"loss\"]], [v.outputs[0] for v in graph.trainable_variables])\n    output_keys = outputs.keys()\n    cache[\"output_keys\"] = output_keys\n    ret = []\n    ret.append(microbatch_num + 1)\n    # The rest of the returned values are \"accumulators\" that get summed\n    # across all microbatches.\n    for t in outputs.values():\n      if smaller_batch_dim in t.shape:\n        # The output contains a batch dimension, so we want to concatenate\n        # across microbatches.\n        # Here we pad the tensor for each microbatch - summing will complete\n        #  the concatenation.\n        t = einsum(\n            [t, one_hot(microbatch_num, microbatch_dim, dtype=t.dtype)],\n            output_shape=replace_dimensions(\n                t.shape, smaller_batch_dim,\n                [smaller_batch_dim, microbatch_dim]))\n        t = replace_dimensions(\n            t, [smaller_batch_dim, microbatch_dim], batch_dim)\n        ret.append(t)\n      else:\n        # There is no batch dimension.  Sum across all microbatches.\n        ret.append(t)\n    # we also want to sum the gradients.\n    ret.extend(grads)\n    return ret\n  while_out = while_loop(\n      cond_fn, body_fn, [constant(mesh, 0, dtype=tf.int32)],\n      has_accumulators=True)\n  num_outputs = len(cache[\"output_keys\"])\n  combined_outputs = {}\n  for k, v in zip(cache[\"output_keys\"], while_out[1:1 + num_outputs]):\n    combined_outputs[k] = v\n  combined_grads = while_out[1 + num_outputs:]\n  return combined_grads, combined_outputs", "response": "This function is used to serialize a training step."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the tensor axis corresponding to the given cumulative product.", "response": "def cumprod_to_tensor_axis(self, cumprod):\n    \"\"\"Maximum tensor axis i such that self.cumprod[i] == cumprod, or None.\"\"\"\n    try:\n      return len(self) - 1 - self.cumprod[::-1].index(cumprod)\n    except ValueError:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename_dimension(self, old_name, new_name):\n    if old_name not in self.dimension_names:\n      raise ValueError(\"Shape %s does not have dimension named %s\"\n                       % (self, old_name))\n    return Shape(\n        [Dimension(new_name, d.size) if d.name == old_name else d\n         for d in self.dims])", "response": "Returns a copy where one dimension is renamed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a copy where one dimension has a different size.", "response": "def resize_dimension(self, name, new_size):\n    \"\"\"Returns a copy where one dimension has a different size.\"\"\"\n    if name not in self.dimension_names:\n      raise ValueError(\"Shape %s does not have dimension named %s\"\n                       % (self, name))\n    return Shape(\n        [Dimension(name, new_size) if d.name == name else d\n         for d in self.dims])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tensor_dimension_to_mesh_axis(self, tensor_dimension, mesh_shape):\n    val = [i for i, mesh_dimension in enumerate(mesh_shape)\n           if (tensor_dimension.name, mesh_dimension.name) in self._pairs]\n    if len(val) > 1:\n      raise ValueError(\n          \"Tensor dimension maps to multiple mesh dimensions\"\n          \" tensor_dimension=%s mesh_shape=%s layout=%s\"\n          % (tensor_dimension, mesh_shape, self._pairs))\n    return val[0] if val else None", "response": "Returns the index of the mesh axis associated with a tensor dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tensor_layout(self, tensor_shape, mesh_shape):\n    ret = [self.tensor_dimension_to_mesh_axis(d, mesh_shape)\n           for d in tensor_shape]\n    not_nones = [a for a in ret if a is not None]\n    if len(not_nones) != len(set(not_nones)):\n      raise ValueError(\n          \"Two Tensor Dimensions may not map to the same Mesh Dimension:\"\n          \" layout=%s tensor_shape=%s mesh_shape=%s \" %\n          (self, tensor_shape, mesh_shape))\n    return TensorLayout(ret)", "response": "Computes a TensorLayout given a Tensor Shape and a Mesh Shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mesh_axis_to_tensor_axis(self, mesh_ndims):\n    ta2ma = self._tensor_axis_to_mesh_axis\n    return tuple(\n        [ta2ma.index(mesh_axis) if mesh_axis in ta2ma else None\n         for mesh_axis in xrange(mesh_ndims)])", "response": "For each mesh axis which Tensor axis maps to it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unique_name(self, name, mark_as_used=True):\n    scope_name = tf.get_variable_scope().name\n    if scope_name:\n      name = scope_name + \"/\" + name\n\n    # As in TensorFlow, treat names as case insensitive when deciding whether\n    # they are in use.\n    name_key = name.lower()\n    i = self._names_in_use.get(name_key, 0)\n    if mark_as_used:\n      self._names_in_use[name_key] = i + 1\n    if i > 0:\n      base_name_key = name_key\n      while name_key in self._names_in_use:\n        name_key = \"%s_%d\" % (base_name_key, i)\n        i += 1\n      if mark_as_used:\n        self._names_in_use[name_key] = 1\n      name = \"%s_%d\" % (name, i-1)\n\n    return name", "response": "Like tf. Graph. unique_name returns a unique name for name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rewrite_stack_variables(self,\n                              max_combined_variable_size=2 ** 30,\n                              max_combined_slice_size=2 ** 27,\n                              mesh_to_impl=None):\n    \"\"\"Rewrite the current graph to combine variables.\n\n    This helps speed up graph construction times in the case of large meshes\n    and large numbers of variables.\n\n    This function should be called after graph construction  (it is called by\n    default in the Lowering constuctor).\n\n    When we find a set of variables with the same shape/dtype/etc, we replace\n    them with one StackedVariable and an \"unstack\" operation.  The\n    StackedVariable has multiple master variables (so as to maintain checkpiont\n    compatibility), but only one slice variable per device.  We point the inputs\n    of later operations to the outputs of the \"unstack\" operations, instead of\n    the outputs of the defunct single variables.\n\n    In order for variables to be combinable, they must be set in the same Assign\n    operation(s) - so it is necessary to call mtf.grouped_assign() from the\n    optimizer instead of many separate calls to mtf.assign().  The assign\n    operations get rewritten to set the appropriate stacked variables.\n\n    TODO(noam): Combining to larger sizes seems to cause errors on TPU.\n      debug this.  Perhaps we should try to keep the combined master variables\n      on the same device.\n\n    Args:\n      max_combined_variable_size: an integer\n      max_combined_slice_size: an integer\n      mesh_to_impl: an optional dictionary from Mesh to MeshImpl\n    \"\"\"\n    # pylint: disable=protected-access\n    all_variables = self._all_variables\n    operations = self._operations\n    self._operations = []\n    self._all_variables = []\n    self._trainable_variables = []\n    # We can only stack varaibles which share the same set of assignment\n    # operations.\n    var_to_assign_ops = collections.defaultdict(str)\n    for op in operations:\n      if isinstance(op, Assign):\n        for v in op._variables:\n          var_to_assign_ops[v.name] += op.name + \", \"\n    # Two variables with the same \"key\" can be stacked together.\n    def var_key(v):\n      return str([v.mesh,\n                  v.shape,\n                  str(v.dtype.__dict__),\n                  v.trainable,\n                  var_to_assign_ops[v]])\n    key_to_vars = collections.defaultdict(collections.deque)\n    for v in all_variables:\n      key_to_vars[var_key(v)].append(v)\n    individual_to_stacked = {}\n    for op in operations:\n      if isinstance(op, StackedVariable):\n        raise ValueError(\"stack_variables() should not be called twice.\")\n      elif isinstance(op, Variable):\n        if op.name in individual_to_stacked:\n          continue\n        similar_vars = key_to_vars[var_key(op)]\n        num_to_stack = len(similar_vars)\n        if max_combined_variable_size is not None:\n          num_to_stack = min(\n              num_to_stack, max_combined_variable_size // op.shape.size)\n        if mesh_to_impl is not None:\n          mesh_impl = mesh_to_impl[op.mesh]\n          if mesh_impl.size == 1:\n            num_to_stack = 1  # no point in stacking for single processors.\n          slice_size = mesh_impl.slice_size(op.shape)\n          num_to_stack = min(\n              num_to_stack, max_combined_slice_size // slice_size)\n        num_to_stack = max(1, num_to_stack)\n        to_stack = [similar_vars.popleft() for _ in xrange(num_to_stack)]\n        if num_to_stack > 1:\n          stacked_var = StackedVariable(to_stack)\n          stack_dim = stacked_var.shape.dims[0]\n          unstacked = unstack(stacked_var.outputs[0], stack_dim)\n          unstack_op = unstacked[0].operation\n          # replace the output Tensors of the unstack operation with the\n          # Tensors which were the outputs of the original variable operations.\n          # Later operations use these Tensors as inputs.\n          unstack_op._outputs = [v.outputs[0] for v in to_stack]\n          for t in unstack_op._outputs:\n            t._operation = unstack_op\n          for idx, v in enumerate(to_stack):\n            individual_to_stacked[v.name] = stacked_var, idx\n        else:\n          assert op == to_stack[0]\n          self._operations.append(op)\n          self._all_variables.append(op)\n          if op.trainable:\n            self._trainable_variables.append(op)\n      else:\n        if isinstance(op, Assign):\n          # Rewrite the grouped assignment to stack up the values and then\n          # assign to the stacked variables.\n          new_variables = []\n          new_values = []\n          var_to_val = dict(zip([v.name for v in op._variables], op._inputs))\n          for var, val in zip(op._variables, op._inputs):\n            if var.name in individual_to_stacked:\n              stacked_var, pos = individual_to_stacked[var.name]\n              if pos == 0:\n                vals = [var_to_val[n] for n in stacked_var.original_names]\n                new_variables.append(stacked_var)\n                new_values.append(\n                    stack(vals, stacked_var.shape.dims[0].name, 0))\n            else:\n              new_variables.append(var)\n              new_values.append(val)\n          op._variables = new_variables\n          op._inputs = new_values\n        self._operations.append(op)", "response": "Rewrite the current graph to combine variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrewrites the current graph to combine Assign operations into grouped Assigns.", "response": "def combine_assignments(self, assignments):\n    \"\"\"Rewrite the current graph to combine \"Assign\" operations.\n\n    Combine similar Assign operations into grouped Assign operations.\n    This is useful when using the rewrite_stack_variables() optimization,\n    since variables can only be stacked if they are present in the same set\n    of Assign operations.\n\n    This function takes a list of Assign operations and returns a possibly\n    shorter list of Assign operations.  The input Assignment operations\n    are removed from the graph and become invalid.\n\n    Args:\n      assignments: a list of Assign objects\n    Returns:\n      a list of Assign objects\n    \"\"\"\n    group_by_fn = collections.defaultdict(list)\n    for a in assignments:\n      if not isinstance(a, Assign):\n        raise ValueError(\"ops should be instances of mtf.Assign\")\n      group_by_fn[a.assign_fn].append(a)\n    assignments_set = set(assignments)\n    self._operations = [\n        op for op in self._operations if op not in assignments_set]\n    ret = []\n    for fn, ops in six.iteritems(group_by_fn):\n      variables = []\n      values = []\n      for a in ops:\n        variables.extend(a.variables)\n        values.extend(a.inputs)\n      ret.append(Assign(variables, values, fn))\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_to_tf_tensor(self, x):\n    mesh_impl = self.mesh_impl(x)\n    return mesh_impl.export_to_tf_tensor(\n        x, self.tensors[x].to_laid_out_tensor())", "response": "Turn a Tensor into a tf. Tensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef autostack(self):\n    num_slices = 0\n    for v in self.graph.all_variables:\n      num_slices += self.mesh_to_impl[v.mesh].size\n    if num_slices >= 2 ** 16:\n      # Startup times are slow with lots of variable slices.\n      # Perform more aggressive stacking\n      max_combined_slice_size = 2 ** 27\n    else:\n      # Stacking hurts memory utilization - only stack small variables.\n      max_combined_slice_size = 2 ** 16\n    self.graph.rewrite_stack_variables(\n        mesh_to_impl=self.mesh_to_impl,\n        max_combined_slice_size=max_combined_slice_size)", "response": "Rewrite graph to combine similarly - prepended variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a TensorLayout for a Tensor or Shape.", "response": "def tensor_layout(self, arg):\n    \"\"\"Compute TensorLayout for a Tensor or a Shape.\n\n    Args:\n      arg: Tensor or Shape.\n\n    Returns:\n      TensorLayout.\n    \"\"\"\n    if isinstance(arg, Tensor):\n      arg = arg.shape\n    return self.layout_rules.tensor_layout(arg, self.shape)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mesh_axis_to_cumprod(self, tensor_shape):\n    tensor_layout = self.tensor_layout(tensor_shape)\n    ma2ta = tensor_layout.mesh_axis_to_tensor_axis(self.ndims)\n    ta2cumprod = tensor_shape.cumprod\n    return [None if ta is None else ta2cumprod[ta] for ta in ma2ta]", "response": "For each mesh axis give the product of previous tensor axes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef slice_shape(self, tensor_shape):\n    tensor_layout = self.tensor_layout(tensor_shape)\n    ret = []\n    for tensor_dim, mesh_axis in zip(\n        tensor_shape, tensor_layout.tensor_axis_to_mesh_axis):\n      if mesh_axis is None:\n        ret.append(tensor_dim.size)\n      else:\n        mesh_dim = self.shape[mesh_axis]\n        if tensor_dim.size % mesh_dim.size != 0:\n          raise ValueError(\n              \"Tensor dimension size not divisible by mesh dimension size:\"\n              \" tensor_shape=%s tensor_layout=%s\"\n              % (tensor_shape, tensor_layout))\n        ret.append(tensor_dim.size // mesh_dim.size)\n    return ret", "response": "Shape of each slice of the Tensor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef slice_begin(self, tensor_shape, pnum):\n    tensor_layout = self.tensor_layout(tensor_shape)\n    coordinates = pnum_to_processor_coordinates(self.shape, pnum)\n    ret = []\n    for dim_size, mesh_axis in zip(\n        tensor_shape.to_integer_list, tensor_layout.tensor_axis_to_mesh_axis):\n      if mesh_axis is None:\n        ret.append(0)\n      else:\n        ret.append(\n            dim_size // self.shape[mesh_axis].size * coordinates[mesh_axis])\n    return ret", "response": "Begin position for the tensor slice for the given processor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling tf.Print. Args: x: LaidOutTensor. data: list of LaidOutTensor. message: str. **kwargs: keyword arguments to tf.print. Returns: LaidOutTensor.", "response": "def Print(self, x, data, message, **kwargs):  # pylint: disable=invalid-name\n    \"\"\"Calls tf.Print.\n\n    Args:\n      x: LaidOutTensor.\n      data: list of LaidOutTensor.\n      message: str.\n      **kwargs: keyword arguments to tf.print.\n\n    Returns:\n      LaidOutTensor.\n    \"\"\"\n    del data, message, kwargs\n    tf.logging.warning(\"Warning - mtf.Print not implemented for this mesh type\")\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninversing of allconcat - split each slice and keep only one piece of it.", "response": "def allsplit(self, x, mesh_axis, split_axis, which=None):\n    \"\"\"Inverse of allconcat - split each slice and keep only one piece of it.\n\n    The number of ways to split is the number of processors in the group.\n    The part that is kept corresponds to the processor's index in the group.\n\n    Args:\n      x: LaidOutTensor.\n      mesh_axis: int, the mesh axis along which to split.\n      split_axis: int, the Tensor axis along which to split.\n      which: an optional LaidOutTensor of integer scalars. Selects the slice to\n        to keep, instead of the coordinate.\n\n    Returns:\n      LaidOutTensor.\n    \"\"\"\n    if which is None:\n      which = self.laid_out_pcoord(mesh_axis)\n    num_splits = self.shape[mesh_axis].size\n    def my_fn(x, which):\n      slice_begin = [\n          dimsize // num_splits * which if i == split_axis else 0\n          for i, dimsize in enumerate(x.shape.as_list())]\n      slice_size = [\n          dimsize // num_splits if i == split_axis else dimsize\n          for i, dimsize in enumerate(x.shape.as_list())]\n      return tf.slice(x, slice_begin, slice_size)\n    return self.slicewise(my_fn, x, which)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreceives the slice from processor pcoord - offset.", "response": "def shift_by_n_processors(self, x, mesh_axis, offset, wrap):\n    \"\"\"Receive the slice from processor pcoord - offset.\n\n    Args:\n      x: a LaidOutTensor\n      mesh_axis: an integer\n      offset: an integer\n      wrap: a boolean. If True, then wrap around. Otherwise, pad with zeros.\n    \"\"\"\n    n = self.shape[mesh_axis].size\n    source_pcoord = []\n    for i in xrange(n):\n      c = i - offset\n      if c != c % n:\n        if wrap:\n          c = c % n\n        else:\n          c = None\n      source_pcoord.append(c)\n    return self.receive(x, mesh_axis, source_pcoord)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef laid_out_pcoord(self, mesh_axis):\n    divisor = list_product(self.shape.to_integer_list[mesh_axis + 1:])\n    modulus = self.shape[mesh_axis].size\n    def my_fn(pnum):\n      return (pnum // divisor) % modulus\n    return self.slicewise(my_fn, self.laid_out_pnum())", "response": "Returns a LaidOutTensor containing the processor coordinate."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef broadcast_impl(self, old_slices, old_shape, new_shape):\n    new_slice_shape = self.slice_shape(new_shape)\n    def tf_fn(x):\n      return (tf.zeros(new_slice_shape, dtype=x.dtype) +\n              _expand_dims(x, old_shape, new_shape))\n    return self.slicewise(tf_fn, old_slices)", "response": "Implementation of a broadcast operation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nturn a single tf. Tensor into a list of slices one for each processor.", "response": "def make_slices(self, tf_tensor, tensor_shape):\n    \"\"\"Turns a single tf.Tensor into a list of slices, one for each processor.\n\n    Args:\n      tf_tensor: tf.Tensor.\n      tensor_shape: Shape.\n\n    Returns:\n      list of tf.tensor with length self.size.\n    \"\"\"\n    tensor_layout = self.tensor_layout(tensor_shape)\n    slice_shape = self.slice_shape(tensor_shape)\n    def my_fn(pnum):\n      if tensor_layout.is_fully_replicated:\n        return tf_tensor\n      else:\n        slice_begin = self.slice_begin(tensor_shape, pnum)\n        return tf.slice(tf_tensor, slice_begin, slice_shape)\n\n    return parallel([tf_tensor.device] * self.size, my_fn,\n                    list(xrange(self.size)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nturns a list of slices into a single tensor.", "response": "def combine_slices(self, slices, tensor_shape, device=None):\n    \"\"\"Turns a set of slices into a single tensor.\n\n    Args:\n      slices: list of tf.Tensor with length self.size.\n      tensor_shape: Shape.\n      device: optional str. If absent, we use the devices of the slices.\n\n    Returns:\n      tf.Tensor.\n    \"\"\"\n    if tensor_shape.ndims == 0:\n      return slices[0]\n\n    ret = slices[:]\n    tensor_layout = self.tensor_layout(tensor_shape)\n    for mesh_dim, tensor_axis in zip(\n        self.shape, tensor_layout.mesh_axis_to_tensor_axis(self.ndims)):\n      slice_size = len(ret) // mesh_dim.size\n      if tensor_axis is None:\n        ret = ret[:slice_size]\n      else:\n        if device:\n          devices = [device] * slice_size\n        else:\n          devices = [ret[i].device for i in xrange(slice_size)]\n        concat_inputs = []\n        for i in xrange(slice_size):\n          concat_inputs.append(\n              [ret[i + slice_size * j] for j in xrange(mesh_dim.size)])\n        ret = parallel(\n            devices, tf.concat, concat_inputs,\n            axis=[tensor_axis] * len(devices))\n    assert len(ret) == 1\n    return ret[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _initialize_splittable_and_unsplittable_dims(\n      self, default_splittability, exception_dims_iterable=None):\n    \"\"\"Initializer for splittable_dims and unsplittable_dims.\n\n    Helper method to categorize all dimensions in the input/output tensors as\n    either splittable or unsplittable.\n\n    Args:\n      default_splittability: a string which is either \"splittable\" or\n        \"unsplittable\".\n      exception_dims_iterable: an optional iterable of names of dimensions\n        which are exceptions to the default splittability.\n\n    Returns:\n      splittable_dims and unsplittable_dims, two frozensets of names of\n        dimensions (strings)\n\n    Raises:\n      ValueError: default_splittability is not one of \"splittable\" or\n        \"unsplittable\".\n    \"\"\"\n    default_dims = set()\n    exception_dims = set()\n    if exception_dims_iterable:\n      exception_dims.update(exception_dims_iterable)\n\n    for t in itertools.chain(self.inputs, self.outputs):\n      for dim_name in t.shape.dimension_names:\n        if dim_name not in exception_dims:\n          default_dims.add(dim_name)\n\n    if default_splittability == \"splittable\":\n      return frozenset(default_dims), frozenset(exception_dims)\n    elif default_splittability == \"unsplittable\":\n      return frozenset(exception_dims), frozenset(default_dims)\n    else:\n      raise ValueError(\"default_splittability should be either \\\"splittable\\\" \"\n                       \"or \\\"unsplittable\\\" but was {}\"\n                       .format(default_splittability))", "response": "Helper method to categorize all dimensions in the input and output tensors as splittable_dims and unsplittable_dims."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlower the ReshapeOperation. Reshaping can require collective communication between processors. We haven't yet implemented all possible reshapes. We try to handle the common cases here - otherwise we raise a NotImplementedError. Args: lowering: a Lowering Raises: NotImplementedError: if we haven't covered this case", "response": "def lower(self, lowering):\n    \"\"\"Lower the ReshapeOperation.\n\n    Reshaping can require collective communication between processors.\n    We haven't yet implemented all possible reshapes.  We try to handle the\n    common cases here - otherwise we raise a NotImplementedError.\n\n    Args:\n      lowering: a Lowering\n    Raises:\n      NotImplementedError: if we haven't covered this case\n    \"\"\"\n    old_shape = self.inputs[0].shape\n    new_shape = self.outputs[0].shape\n    mesh_impl = lowering.mesh_impl(self)\n    slices = lowering.tensors[self.inputs[0]]\n    mesh_axis_to_cumprod_old = mesh_impl.mesh_axis_to_cumprod(old_shape)\n    mesh_axis_to_cumprod_new = mesh_impl.mesh_axis_to_cumprod(new_shape)\n    # Figure out what needs to be done for different mesh-axes\n    mesh_axes_allsplit = []\n    mesh_axes_allconcat = []\n    mesh_axes_alltoall = []\n    for mesh_axis, (old_cumprod, new_cumprod) in enumerate(\n        zip(mesh_axis_to_cumprod_old, mesh_axis_to_cumprod_new)):\n      if new_cumprod != old_cumprod:\n        if old_cumprod is None:\n          # split in new layout but not in old layout - we need an allsplit\n          mesh_axes_allsplit.append(mesh_axis)\n        elif new_cumprod is None:\n          # split in old layout but not in new layout - we need an allconcat\n          mesh_axes_allconcat.append(mesh_axis)\n        else:\n          # split differently in old and new layouts - we need an alltoall\n          mesh_axes_alltoall.append(mesh_axis)\n\n    laid_out_size = mesh_impl.laid_out_size(old_shape)\n\n    for mesh_axis in mesh_axes_allsplit:\n      tensor_axis = old_shape.cumprod_to_tensor_axis(\n          mesh_axis_to_cumprod_new[mesh_axis])\n      if tensor_axis is None:\n        # TODO(noam): try to handle this case\n        raise NotImplementedError(\n            \"Try first reshaping to insert a new tf dimension,\"\n            \" then changing layout. input_shape=%s output_shape=%s\"\n            % (self.inputs[0].shape, self.outputs[0].shape))\n      slices = mesh_impl.allsplit(slices, mesh_axis, tensor_axis)\n      laid_out_size //= mesh_impl.shape[mesh_axis].size\n    for mesh_axis in mesh_axes_alltoall:\n      split_tensor_axis = old_shape.cumprod_to_tensor_axis(\n          mesh_axis_to_cumprod_new[mesh_axis])\n      if split_tensor_axis is None:\n        # TODO(noam): try to handle this case\n        raise NotImplementedError(\n            \"Try first reshaping to insert a new tf dimension,\"\n            \" then changing layout. input_shape=%s output_shape=%s\"\n            % (self.inputs[0].shape, self.outputs[0].shape))\n      concat_tensor_axis = old_shape.cumprod_to_tensor_axis(\n          mesh_axis_to_cumprod_old[mesh_axis])\n      assert concat_tensor_axis is not None\n      slices = mesh_impl.alltoall(\n          slices, mesh_axis, split_tensor_axis, concat_tensor_axis)\n      lowering.add_counter(\n          \"alltoall/%s/reshape_op\" % mesh_axis, laid_out_size)\n\n    for mesh_axis in mesh_axes_allconcat:\n      tensor_axis = old_shape.cumprod_to_tensor_axis(\n          mesh_axis_to_cumprod_old[mesh_axis])\n      assert tensor_axis is not None\n      slices = mesh_impl.allconcat(slices, mesh_axis, tensor_axis)\n      laid_out_size *= mesh_impl.shape[mesh_axis].size\n      lowering.add_counter(\n          \"allconcat/%s/reshape_op\" % mesh_axis, laid_out_size)\n    # now reshape the slices\n    old_slice_shape = mesh_impl.slice_shape(old_shape)\n    new_slice_shape = mesh_impl.slice_shape(new_shape)\n    if new_slice_shape != old_slice_shape:\n      def reshape_fn(x):\n        return tf.reshape(x, new_slice_shape)\n      slices = mesh_impl.slicewise(reshape_fn, slices)\n    lowering.set_tensor_lowering(self.outputs[0], slices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_variable_dtype(\n    master_dtype=tf.bfloat16,\n    slice_dtype=tf.float32,\n    activation_dtype=tf.float32):\n  \"\"\"Datatypes to use for the run.\n\n  Args:\n    master_dtype: string, datatype for checkpoints\n      keep this the same between training and eval/inference\n    slice_dtype: string, datatype for variables in memory\n      must be tf.float32 for training\n    activation_dtype: string, datatype for activations\n      less memory usage if tf.bfloat16 but possible numerical issues\n  Returns:\n    a mtf.VariableDtype\n  \"\"\"\n  return mtf.VariableDType(\n      master_dtype=tf.as_dtype(master_dtype),\n      slice_dtype=tf.as_dtype(slice_dtype),\n      activation_dtype=tf.as_dtype(activation_dtype))", "response": "Returns the dtype for the variable in the checkpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_model(model_type=\"bitransformer\",\n                input_vocab_size=gin.REQUIRED,\n                output_vocab_size=gin.REQUIRED,\n                layout_rules=None,\n                mesh_shape=None):\n  \"\"\"Build a transformer model.\n\n  Currently, three types of models are supported:\n\n  \"bitransformer\": The traditional encoder-decoder architecture from\n     \"attention is all you need\".  Requires a non-text2self dataset.\n\n  \"lm\": an autoregressive language model (one layer stack).  This is similar\n     to the decoder part of a bitransformer, but with no attention over an\n     encoder, since there is no encoder.  Requires a text2self dataset,\n     with targets, but no inputs.\n\n  \"aligned\": a non-autoregressive single-stack model (like BERT).  Requires\n     a non-text2self dataset with inputs and targets.  The targets are\n     aligned with the inputs.\n\n  Args:\n    model_type: a string - \"bitransformer\", \"lm\" or \"aligned\"\n    input_vocab_size: an integer\n    output_vocab_size: an integer\n    layout_rules: optional - an input to mtf.convert_to_layout_rules\n    mesh_shape: optional - an input to mtf.convert_to_shape\n  Returns:\n    a Unitransformer or Bitransformer\n  \"\"\"\n  if model_type == \"bitransformer\":\n    return transformer.make_bitransformer(\n        input_vocab_size=input_vocab_size,\n        output_vocab_size=output_vocab_size,\n        mesh_shape=mesh_shape,\n        layout=layout_rules)\n  elif model_type == \"lm\" or model_type == \"aligned\":\n    return transformer.Unitransformer(\n        autoregressive=model_type == \"lm\",\n        layer_stack=transformer.make_layer_stack(),\n        input_vocab_size=input_vocab_size,\n        output_vocab_size=output_vocab_size,\n        mesh_shape=mesh_shape,\n        layout=layout_rules)\n  else:\n    raise ValueError(\"unknown model_type\")", "response": "Builds a new transformer model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a TPUEstimator model function.", "response": "def tpu_estimator_model_fn(model_type,\n                           transformer_model,\n                           model_dir,\n                           use_tpu,\n                           mesh_shape,\n                           layout_rules,\n                           batch_size,\n                           sequence_length,\n                           autostack,\n                           metric_names):\n  \"\"\"Create a TPUEstimator model function.\n\n  Args:\n    model_type: a string\n    transformer_model: a transformer.Unitransformer or transformer.Bitransformer\n    model_dir: a string\n    use_tpu: a boolean\n    mesh_shape: a mtf.Shape\n    layout_rules: a mtf.LayoutRules\n    batch_size: an integer\n    sequence_length: an integer\n    autostack: a boolean\n    metric_names: list of strings giving the metric names. If None, then\n      computes padded_neg_log_perplexity\n\n  Returns:\n    a function to be passed to TPUEstimator\n  \"\"\"\n  def my_model_fn(features, labels, mode, params=None, config=None):\n    \"\"\"Estimator model function.\n\n    Args:\n      features: input features dictionary\n      labels: ignored\n      mode: a tf.estimator.ModeKeys\n      params: something\n      config: something\n\n    Returns:\n      something\n    \"\"\"\n    del labels, config\n    global_step = tf.train.get_global_step()\n    if use_tpu:\n      ctx = params[\"context\"]\n      num_hosts = ctx.num_hosts\n      host_placement_fn = ctx.tpu_host_placement_function\n      device_list = [host_placement_fn(host_id=t) for t in range(num_hosts)]\n      # TODO(ylc): Better estimation of replica cache size?\n      replica_cache_size = 300 * 1000000  # 300M per replica\n      # Worker 0 caches all the TPU binaries.\n      worker0_mem = replica_cache_size * ctx.num_replicas\n      devices_memeory_usage = [worker0_mem] + [0] * (num_hosts - 1)\n      var_placer = mtf.utils.BalancedVariablePlacer(device_list,\n                                                    devices_memeory_usage)\n      mesh_devices = [\"\"] * mesh_shape.size\n      mesh_impl = mtf.simd_mesh_impl.SimdMeshImpl(\n          mesh_shape, layout_rules, mesh_devices, ctx.device_assignment)\n    else:\n      var_placer = None\n      mesh_devices = [\"\"] * mesh_shape.size\n      mesh_impl = mtf.placement_mesh_impl.PlacementMeshImpl(\n          mesh_shape, layout_rules, mesh_devices)\n\n    graph = mtf.Graph()\n    mesh = mtf.Mesh(graph, \"my_mesh\", var_placer)\n\n    def _import_feature(key, allow_missing=False):\n      \"\"\"Import a feature from the features dictionary into a mtf.Tensor.\n\n      Args:\n        key: a string\n        allow_missing: a boolean\n\n      Returns:\n        a mtf.Tensor with dtype int32 and shape [batch_dim, length_dim]\n      \"\"\"\n      batch_dim = mtf.Dimension(\"batch\", batch_size)\n      length_dim = mtf.Dimension(\"length\", sequence_length)\n      mtf_shape = mtf.Shape([batch_dim, length_dim])\n      if key not in features:\n        if allow_missing:\n          return None\n        else:\n          raise ValueError(\n              \"feature not found %s - features %s = \" % (key, features))\n      tf.logging.info(\"Import feature %s: %s\" % (key, features[key]))\n\n      x = tf.to_int32(features[key])\n      if not use_tpu:\n        x = tf.Print(\n            x, [x], \"import feature %s\" % key, summarize=1000, first_n=1)\n      return mtf.import_fully_replicated(mesh, x, mtf_shape, name=key)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      inputs = _import_feature(\"inputs\")\n      if isinstance(transformer_model, transformer.Unitransformer):\n        mtf_samples = transformer_model.sample_autoregressive(\n            inputs, variable_dtype=get_variable_dtype())\n      elif isinstance(transformer_model, transformer.Bitransformer):\n        mtf_samples = transformer_model.decode(\n            inputs, variable_dtype=get_variable_dtype())\n      else:\n        raise ValueError(\"unrecognized class\")\n      mtf_samples = mtf.anonymize(mtf_samples)\n      lowering = mtf.Lowering(graph, {mesh: mesh_impl}, autostack=autostack)\n      outputs = lowering.export_to_tf_tensor(mtf_samples)\n      predictions = {\"outputs\": outputs}\n      return tpu_estimator.TPUEstimatorSpec(\n          mode=tf.estimator.ModeKeys.PREDICT,\n          predictions=predictions,\n          prediction_hooks=[mtf.MtfRestoreHook(lowering)])\n\n    targets = _import_feature(\"targets\")\n    anon_targets = mtf.anonymize(targets)\n    if model_type == \"lm\":\n      _, length_dim = targets.shape\n      inputs = mtf.shift(targets, offset=1, dim=length_dim, wrap=False)\n    else:\n      inputs = _import_feature(\"inputs\")\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n      if isinstance(transformer_model, transformer.Unitransformer):\n        mtf_samples = transformer_model.sample_autoregressive(\n            inputs, variable_dtype=get_variable_dtype())\n      elif isinstance(transformer_model, transformer.Bitransformer):\n        mtf_samples = transformer_model.decode(\n            inputs, variable_dtype=get_variable_dtype())\n      else:\n        raise ValueError(\"unrecognized class\")\n      mtf_samples = mtf.anonymize(mtf_samples)\n      lowering = mtf.Lowering(graph, {mesh: mesh_impl}, autostack=autostack)\n      outputs = lowering.export_to_tf_tensor(mtf_samples)\n      labels = lowering.export_to_tf_tensor(anon_targets)\n      restore_hook = mtf.MtfRestoreHook(lowering)\n\n      # metric_names becomes locally scoped if we simply assign\n      # [\"padded_neg_log_perplexity\"] to it conditioned on if it's None.\n      local_metric_names = metric_names or [\"token_accuracy\"]\n\n      def metric_fn(labels, outputs):\n        return metric_utils.get_metric_fns(\n            local_metric_names, labels, outputs\n        )\n      eval_metrics = (metric_fn, [labels, outputs])\n      return tpu_estimator.TPUEstimatorSpec(\n          tf.estimator.ModeKeys.EVAL,\n          # Unfortunately TPUEstimatorSpec requires us to provide a value for\n          # loss when in EVAL mode. Since we are sampling or decoding from the\n          # model, we don't have a loss to report.\n          loss=tf.constant(0.),\n          evaluation_hooks=[restore_hook],\n          eval_metrics=eval_metrics)\n\n    if isinstance(transformer_model, transformer.Unitransformer):\n      position_kwargs = dict(\n          sequence_id=_import_feature(\"targets_segmentation\", True),\n          position=_import_feature(\"targets_position\", True),\n      )\n    elif isinstance(transformer_model, transformer.Bitransformer):\n      position_kwargs = dict(\n          encoder_sequence_id=_import_feature(\"inputs_segmentation\", True),\n          decoder_sequence_id=_import_feature(\"targets_segmentation\", True),\n          encoder_position=_import_feature(\"inputs_position\", True),\n          decoder_position=_import_feature(\"targets_position\", True),\n      )\n    else:\n      raise ValueError(\"unrecognized class\")\n\n    logits, loss = transformer_model.call_simple(\n        inputs=inputs,\n        targets=targets,\n        compute_loss=True,\n        mode=mode,\n        variable_dtype=get_variable_dtype(),\n        **position_kwargs)\n\n    if use_tpu and logits is not None:\n      logits = mtf.anonymize(logits)\n\n    # TRAIN mode\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      var_grads = mtf.gradients(\n          [loss], [v.outputs[0] for v in graph.trainable_variables])\n      optimizer = mtf.optimize.AdafactorOptimizer()\n      update_ops = optimizer.apply_grads(var_grads, graph.trainable_variables)\n\n    lowering = mtf.Lowering(graph, {mesh: mesh_impl}, autostack=autostack)\n\n    tf_loss = lowering.export_to_tf_tensor(loss)\n    tf_loss = tf.to_float(tf_loss)\n    if not use_tpu:\n      tf_loss = tf.Print(tf_loss, [tf_loss, tf.train.get_global_step()],\n                         \"step, tf_loss\")\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      tf_update_ops = [lowering.lowered_operation(op) for op in update_ops]\n      tf_update_ops.append(tf.assign_add(global_step, 1))\n      train_op = tf.group(tf_update_ops)\n\n    with mtf.utils.outside_all_rewrites():\n      # Copy master variables to slices. Must be called first.\n      restore_hook = mtf.MtfRestoreHook(lowering)\n      saver = tf.train.Saver(\n          tf.global_variables(),\n          sharded=True,\n          max_to_keep=10,\n          keep_checkpoint_every_n_hours=2,\n          defer_build=False,\n          save_relative_paths=True)\n      tf.add_to_collection(tf.GraphKeys.SAVERS, saver)\n      saver_listener = mtf.MtfCheckpointSaverListener(lowering)\n      saver_hook = tf.train.CheckpointSaverHook(\n          model_dir, save_steps=1000, saver=saver, listeners=[saver_listener])\n      gin_config_saver_hook = gin.tf.GinConfigSaverHook(\n          model_dir, summarize_config=True)\n\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        if use_tpu:\n          return tpu_estimator.TPUEstimatorSpec(\n              mode=tf.estimator.ModeKeys.TRAIN,\n              loss=tf_loss,\n              train_op=train_op,\n              training_hooks=[\n                  restore_hook,\n                  saver_hook,\n                  gin_config_saver_hook,\n              ])\n        else:\n          return tf.estimator.EstimatorSpec(\n              tf.estimator.ModeKeys.TRAIN,\n              loss=tf_loss,\n              train_op=train_op,\n              training_chief_hooks=[\n                  restore_hook,\n                  saver_hook,\n                  gin_config_saver_hook,\n              ])\n\n  return my_model_fn"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode_from_file(estimator,\n                     vocabulary,\n                     model_type,\n                     batch_size,\n                     sequence_length,\n                     checkpoint_path=\"\",\n                     input_filename=gin.REQUIRED,\n                     output_filename=gin.REQUIRED,\n                     eos_id=1):\n  \"\"\"Decode from a text file.\n\n  Args:\n    estimator: a TPUEstimator\n    vocabulary: a mtf.transformer.vocabulary.Vocabulary\n    model_type: a string\n    batch_size: an integer\n    sequence_length: an integer (maximum decode length)\n    checkpoint_path: an optional string\n    input_filename: a string\n    output_filename: a string\n    eos_id: EOS id\n  \"\"\"\n  with tf.gfile.Open(input_filename) as f:\n    text = f.read()\n  records = text.split(\"\\n\")\n  inputs = [record.strip() for record in records]\n  # Strip the last empty line.\n  if not inputs[-1]:\n    inputs.pop()\n  n = len(inputs)\n  # encode all inputs\n  all_input_ids = []\n  for line in inputs:\n    ids = inputs_vocabulary(vocabulary).encode(line.strip())\n    if model_type != \"lm\":\n      # for text2self problems, the inputs represent a partial sequence\n      # to be continued, and should not be terminated by EOS.\n      # for sequence-to-sequence problems, the input needs to be EOS-terminated\n      ids += [eos_id]\n    if len(ids) > sequence_length:\n      ids = ids[:sequence_length]\n    else:\n      ids.extend([0] * (sequence_length - len(ids)))\n      all_input_ids.append(ids)\n      # pad to make an integral number of batches\n  all_input_ids.extend([all_input_ids[0]] * (-n % batch_size))\n  padded_n = len(all_input_ids)\n  all_input_ids = np.array(all_input_ids, dtype=np.int32)\n\n  def input_fn(params):\n    del params\n    dataset = tf.data.Dataset.from_tensor_slices({\"inputs\": all_input_ids})\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    return dataset\n\n  result_iter = estimator.predict(input_fn, checkpoint_path=checkpoint_path)\n  vocab_size = targets_vocabulary(vocabulary).vocab_size\n  decodes = []\n  for i, result in enumerate(result_iter):\n    output_ids = clean_decodes(list(result[\"outputs\"]), vocab_size)\n    output_string = targets_vocabulary(vocabulary).decode(\n        [int(x) for x in output_ids])\n    decodes.append(output_string)\n    if i & (i - 1) == 0:\n      if i < len(inputs):\n        # LOG every power of 2, don't log if it's padded input i >= len(inputs)\n        tf.logging.info(\"decode %d input = %s\" % (i, inputs[i]))\n        tf.logging.info(\"          output = %s\" % output_string)\n\n  # BUG WORKAROUND - on TF1.13 and earlier, the output for each batch is\n  # repeated a number of times equal to the number of cores.\n  if len(decodes) == padded_n:\n    tf.logging.info(\"number of decodes matches number of inputs\")\n  elif len(decodes) % padded_n == 0:\n    num_cores = len(decodes) // padded_n\n    tf.logging.info(\"output is repeated num_cores times - removing extras\")\n\n    def keep(i):\n      return i % (batch_size * num_cores) < batch_size\n\n    decodes = [d for i, d in enumerate(decodes) if keep(i)]\n  else:\n    raise ValueError(\"unexpected number of outputs\")\n  output_file = tf.gfile.Open(output_filename, \"w\")\n  decodes = decodes[:n]\n  for d in decodes:\n    output_file.write(d)\n    output_file.write(\"\\n\")\n  output_file.close()", "response": "Decode from a text file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clean_decodes(ids, vocab_size, eos_id=1):\n  ret = []\n  for i in ids:\n    if i == eos_id:\n      break\n    if i >= vocab_size:\n      break\n    ret.append(int(i))\n  return ret", "response": "Stop at EOS or padding or OOV."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef auto_batch_size(sequence_length,\n                    mesh_shape,\n                    layout_rules,\n                    tokens_per_split=2048):\n  \"\"\"Automatically compute batch size.\n\n  Args:\n    sequence_length: an integer\n    mesh_shape: an input to mtf.convert_to_shape()\n    layout_rules: an input to mtf.convert_to_layout_rules()\n    tokens_per_split: an integer\n  Returns:\n    an integer\n  \"\"\"\n  num_splits = mtf.tensor_dim_to_mesh_dim_size(\n      layout_rules, mesh_shape, mtf.Dimension(\"batch\", 0))\n  ret = max(1, tokens_per_split // sequence_length) * num_splits\n  tf.logging.info(\n      \"AUTO_BATCH_SIZE tokens_per_split=%s num_splits=%s\"\n      \" sequence_length=%s batch_size=%s\"\n      % (tokens_per_split, num_splits, sequence_length, ret))\n  return ret", "response": "Automatically compute batch size."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(tpu_job_name,\n        tpu,\n        gcp_project,\n        tpu_zone,\n        model_dir,\n        model_type=\"bitransformer\",\n        vocabulary=gin.REQUIRED,\n        train_dataset_fn=None,\n        eval_dataset_fn=None,\n        dataset_split=\"train\",\n        autostack=True,\n        checkpoint_path=\"\",\n        mode=\"train\",\n        iterations_per_loop=100,\n        save_checkpoints_steps=1000,\n        eval_steps=10,\n        train_steps=1000000,\n        batch_size=auto_batch_size,\n        sequence_length=gin.REQUIRED,\n        mesh_shape=gin.REQUIRED,\n        layout_rules=gin.REQUIRED,\n        get_components_fn=None):\n  \"\"\"Run training/eval/inference.\n\n  Args:\n    tpu_job_name: string, name of TPU worker binary\n    tpu: string, the Cloud TPU to use for training\n    gcp_project: string, project name for the Cloud TPU-enabled project\n    tpu_zone: string, GCE zone where the Cloud TPU is located in\n    model_dir: string, estimator model_dir\n    model_type: a string - either \"bitransformer\", \"lm\" or \"aligned\"\n    vocabulary: a vocabulary.Vocabulary or\n      (inputs_vocabulary, targets_vocabulary) tuple.\n    train_dataset_fn: A function returning a tf.data.Dataset. Must be provided\n      for mode=train\n    eval_dataset_fn: A function returning a tf.data.Dataset. Must be provided\n      for model=eval\n    dataset_split: a string\n    autostack: boolean, internally combine variables\n    checkpoint_path: a string - which checkpoint to load for inference\n    mode: string, train/evaluate/infer\n    iterations_per_loop: integer, steps per train loop\n    save_checkpoints_steps: integer, steps per checkpoint\n    eval_steps: integer, number of evaluation steps\n    train_steps: Total number of training steps.\n    batch_size: An integer or a function with the same signature as\n      auto_batch_size().  Mini-batch size for the training. Note that this is\n      the global batch size and not the per-shard batch size.\n    sequence_length: an integer\n    mesh_shape: an input to mtf.convert_to_shape()\n    layout_rules: an input to mtf.convert_to_layout_rules()\n    get_components_fn: an optional function that gets a list of tuples of\n      (metric_names, component) for each component. Required if mode is\n      \"continuous_eval\"\n  \"\"\"\n  if not isinstance(batch_size, int):\n    batch_size = batch_size(sequence_length, mesh_shape, layout_rules)\n\n  tf.logging.info(\"mode=%s\" % mode,)\n  tf.logging.info(\"batch_size=%s\" % batch_size,)\n  tf.logging.info(\"sequence_length=%s\" % sequence_length,)\n  tf.logging.info(\"mesh_shape=%s\" % mesh_shape,)\n  tf.logging.info(\"layout_rules=%s\" % layout_rules,)\n\n  if mode == \"train\" and dataset_split != \"train\":\n    raise ValueError(\"mode==\\\"train\\\" requires dataset_split==\\\"train\\\"\")\n\n  mesh_shape = mtf.convert_to_shape(mesh_shape)\n  layout_rules = mtf.convert_to_layout_rules(layout_rules)\n\n  cluster = tf.contrib.cluster_resolver.TPUClusterResolver(\n      tpu if (tpu) else \"\", zone=tpu_zone, project=gcp_project)\n\n  tf.logging.info(\n      \"Building TPUConfig with tpu_job_name={}\".format(tpu_job_name)\n  )\n  my_tpu_config = tpu_config.TPUConfig(\n      tpu_job_name=tpu_job_name,\n      iterations_per_loop=iterations_per_loop,\n      num_cores_per_replica=1,\n      per_host_input_for_training=tpu_config.InputPipelineConfig.BROADCAST,\n  )\n\n  run_config = tpu_config.RunConfig(\n      cluster=cluster,\n      model_dir=model_dir,\n      save_checkpoints_steps=save_checkpoints_steps,\n      tpu_config=my_tpu_config)\n\n  transformer_model = build_model(\n      model_type=model_type,\n      input_vocab_size=inputs_vocabulary(vocabulary).vocab_size,\n      output_vocab_size=targets_vocabulary(vocabulary).vocab_size,\n      layout_rules=layout_rules,\n      mesh_shape=mesh_shape)\n\n  model_fn = tpu_estimator_model_fn(\n      model_type=model_type,\n      transformer_model=transformer_model,\n      model_dir=model_dir,\n      use_tpu=tpu,\n      mesh_shape=mesh_shape,\n      layout_rules=layout_rules,\n      batch_size=batch_size,\n      sequence_length=sequence_length,\n      autostack=autostack,\n      metric_names=None)\n\n  estimator = tpu_estimator.TPUEstimator(\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=batch_size,\n      eval_batch_size=batch_size,\n      predict_batch_size=batch_size,\n      use_tpu=tpu,\n      export_to_tpu=False,\n      params={})\n\n  if mode == \"train\":\n    if train_dataset_fn is None:\n      raise ValueError(\"Must provide train_dataset_fn through gin for train.\")\n    def input_fn(params):\n      del params\n      dataset = train_dataset_fn(batch_size=batch_size,\n                                 sequence_length=sequence_length,\n                                 vocabulary=vocabulary,\n                                 dataset_split=dataset_split)\n      return dataset\n\n    estimator.train(input_fn=input_fn, max_steps=train_steps)\n  elif mode == \"continuous_eval\":\n    if get_components_fn is None:\n      raise ValueError(\"Must provide get_components_fn through gin for eval.\")\n    if eval_dataset_fn is None:\n      raise ValueError(\"Must provide eval_dataset_fn through gin for eval.\")\n    metrics_inputs = get_components_fn()\n    for _ in tf.contrib.training.checkpoints_iterator(estimator.model_dir):\n      for metric_names, component in metrics_inputs:\n        tf.logging.info(\"Evaluating {}\".format(component.__dict__))\n        tf.logging.info(\"on split {}\".format(dataset_split))\n        # Prepend eval tag and split name to metric names\n        metric_names = [\n            \"eval/{}/{}\".format(dataset_split, n) for n in metric_names\n        ]\n        # Regenerate the estimator\n        model_fn = tpu_estimator_model_fn(\n            model_type=model_type,\n            transformer_model=transformer_model,\n            model_dir=model_dir,\n            use_tpu=tpu,\n            mesh_shape=mesh_shape,\n            layout_rules=layout_rules,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            autostack=autostack,\n            metric_names=metric_names)\n        estimator = tpu_estimator.TPUEstimator(\n            model_fn=model_fn,\n            config=run_config,\n            train_batch_size=batch_size,\n            eval_batch_size=batch_size,\n            predict_batch_size=batch_size,\n            use_tpu=tpu,\n            export_to_tpu=False,\n            params={})\n        def input_fn(params):\n          del params\n          dataset = eval_dataset_fn(component,  # pylint: disable=cell-var-from-loop\n                                    batch_size=batch_size,\n                                    sequence_length=sequence_length,\n                                    vocabulary=vocabulary,\n                                    dataset_split=dataset_split,\n                                    pack=False)\n          return dataset\n\n        eval_args = {\"eval\": (input_fn, eval_steps)}\n        _ = evaluate(estimator, eval_args)\n\n  elif mode == \"infer\":\n    decode_from_file(\n        estimator,\n        vocabulary=vocabulary,\n        model_type=model_type,\n        batch_size=batch_size,\n        sequence_length=sequence_length,\n        checkpoint_path=checkpoint_path)\n  else:\n    raise ValueError(\n        \"unknown mode %s - must be train/evaluate/continuous_eval/infer\" % mode)", "response": "This function runs training and eval on a binary TPU."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun evaluation on the latest model checkpoint and logs to tensorboard.", "response": "def evaluate(estimator, eval_args):\n  \"\"\"Runs evaluation on the latest model checkpoint & logs to tensorboard.\n\n  Args:\n    estimator: A tf.Estimator object.\n    eval_args: Dictionary of {eval_name: (input_fn, eval_steps)} where eval_name\n      is the name of the evaluation set, e.g. \"train\" or \"val\", input_fn is an\n      input function returning a tuple (features, labels), and eval_steps is the\n      number of steps for which to evaluate the model. If None, evaluates until\n      input_fn raises an end-of-input exception.\n\n  Returns:\n    A dict of metric values from the evaluation. May be empty, e.g. if the\n    training job has not yet saved a checkpoint or the checkpoint is deleted by\n    the time the TPU worker initializes.\n  \"\"\"\n  values = {}  # Default return value if evaluation fails.\n\n  checkpoint_path = estimator.latest_checkpoint()\n  if not checkpoint_path:\n    # This is expected if the training job has not yet saved a checkpoint.\n    return values\n\n  tf.logging.info(\"Starting evaluation on checkpoint %s\", checkpoint_path)\n  for eval_name in eval_args:\n    input_fn, eval_steps = eval_args[eval_name]\n    metric_values = estimator.evaluate(\n        input_fn,\n        steps=eval_steps,\n        name=eval_name,\n        checkpoint_path=checkpoint_path)\n    for key, val in metric_values.iteritems():\n      values[eval_name + \"/\" + key] = val\n\n  tf.logging.info(values)\n  return values"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ring_2d(m, n):\n  if m == 1:\n    return [(0, i) for i in range(n)]\n  if n == 1:\n    return [(i, 0) for i in range(m)]\n  if m % 2 != 0:\n    tf.logging.warning(\"Odd dimension\")\n    return [(i % m, i // m) for i in range(n * m)]\n  ret = [(0, 0)]\n  for i in range(m // 2):\n    for j in range(1, n):\n      ret.append((2 * i, j))\n    for j in range(n-1, 0, -1):\n      ret.append((2 * i + 1, j))\n  for i in range(m-1, 0, -1):\n    ret.append((i, 0))\n  return ret", "response": "Ring - order of a mxn mesh.\n Args : m n Returns a list of mxn pairs\n Args : n m Returns a list of mxn pairs\n Args : m n Returns a list of mxn pairs\n Returns a list of mxn pairs\n Returns a list of mxn pairs\n Returns a list of mxn pairs\n Returns a list of mxn pairs\n "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tile_2d(physical_shape, tile_shape,\n            outer_name=\"outer\",\n            inner_name=\"inner\",\n            cores_name=None):\n  \"\"\"2D tiling of a 3d physical mesh.\n\n  The \"outer\" mesh dimension corresponds to which tile.\n  The \"inner\" mesh dimension corresponds to the position within a tile\n  of processors.\n\n  Optionally, if cores_name is specified, then a 3 dimensional logical mesh\n  is returned, with the third dimension representing the two different\n  cores within a chip.  If cores_name is not specified, then the\n  cores-in-a-chip dimension is folded into the inner dimension.\n\n  TODO(noam): explain this better.\n\n  Example:\n\n  tile_2d(physical_shape=[8, 16, 2], tile_shape=[4, 4])\n\n  The \"inner\" dimension has size 4x4x2=32 and corresponds to the position\n  within a 4x4 tile of processors.\n\n  The \"outer\" dimension has size 8/4 * 16/4 = 8, and corresponds to the 8\n  tiles in the mesh.\n\n  Args:\n    physical_shape: a triple of integers [X, Y, cores]\n    tile_shape: a pair\n    outer_name: a string\n    inner_name: a string\n    cores_name: an optional string\n\n  Returns:\n    mesh_shape: a mtf.Shape\n    logical_to_physical: a list\n  \"\"\"\n  logical_to_physical = []\n  p0, p1, p2 = physical_shape\n  t0, t1 = tile_shape\n  tile_ring = _ring_2d(t0, t1)\n  tiles_ring = _ring_2d(p0 // t0, p1 // t1)\n  for logical_pnum in range(p0 * p1 * p2):\n    core_on_chip = logical_pnum % p2\n    logical_chip_num = logical_pnum // p2\n    logical_pos_in_tile = logical_chip_num % (t0 * t1)\n    logical_tile_num = logical_chip_num // (t0 * t1)\n    tile_i, tile_j = tile_ring[logical_pos_in_tile]\n    tiles_i, tiles_j = tiles_ring[logical_tile_num]\n    physical_pnum = core_on_chip + p2 * (\n        tile_i * p1 + tile_j +\n        tiles_i * p1 * t0 + tiles_j * t1)\n    logical_to_physical.append(physical_pnum)\n  assert sorted(logical_to_physical) == list(range(p0 * p1  * p2))\n  tile_size = t0 * t1 * p2\n  num_tiles = p0 * p1 // (t0 * t1)\n  if cores_name:\n    mesh_shape = mtf.Shape(\n        [mtf.Dimension(outer_name, int(num_tiles)),\n         mtf.Dimension(inner_name, int(t0 * t1)),\n         mtf.Dimension(cores_name, int(p2))])\n  else:\n    mesh_shape = mtf.Shape(\n        [mtf.Dimension(outer_name, int(num_tiles)),\n         mtf.Dimension(inner_name, int(tile_size))])\n  return mesh_shape, logical_to_physical", "response": "2D tiling of a 3d physical mesh."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_group_assignment(self, mesh_axes):\n\n    partitioning = {}\n    for logical_pnum in xrange(self.size):\n      group = mtf.pnum_to_group(self.shape, mesh_axes, logical_pnum)\n      if group not in partitioning:\n        partitioning[group] = []\n      partitioning[group].append(self.l2p(logical_pnum))\n    group_assignment = []\n    for group, physical_pnums in partitioning.items():\n      group_assignment.append(physical_pnums)\n    return group_assignment", "response": "Create group assignment for XLA cross replica ops."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef allreduce(self, x, mesh_axes, reduction_fn_string):\n    if not mesh_axes:\n      return x\n    x = x.to_laid_out_tensor()\n    if reduction_fn_string == \"SUM\":\n      group_assignment = self._create_group_assignment(mesh_axes)\n      group_size = len(group_assignment[0])\n      tf_in = x.one_slice\n      dtype = tf_in.dtype\n      if dtype == tf.float32:\n        cast_to_float32 = False\n      elif dtype == tf.bfloat16:\n        cast_to_float32 = (\n            group_size > self._allreduce_in_bfloat16_max_group_size)\n      else:\n        tf.logging.info(\"Casting %s to float32 for allreduce\" % tf_in.dtype)\n        cast_to_float32 = True\n      if cast_to_float32:\n        tf_in = tf.cast(tf_in, tf.float32)\n      tf_out = tpu_ops.cross_replica_sum(tf_in, group_assignment)\n      if cast_to_float32:\n        tf_out = tf.cast(tf_out, dtype)\n      return self.LaidOutTensor([tf_out])\n    else:\n      for axis in mesh_axes:\n        x = self.allconcat(x, axis, 0, stack=True)\n        x = self.LaidOutTensor(\n            [mtf.reduction_fn(reduction_fn_string)(x.one_slice, 0)])\n      return x", "response": "Grouped allreduce, (summed across the given dimensions).\n\n    Args:\n      x: a LaidOutTensor\n      mesh_axes: a list of integers\n      reduction_fn_string: \"SUM\"\n    Returns:\n      a LaidOutTensor\n    Raises:\n      ValueError: if the reduction is not yet implemented."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngroups allconcat with a single tensor.", "response": "def allconcat(self, x, mesh_axis, concat_axis, stack=False):\n    \"\"\"Grouped allconcat (like MPI allgather followed by concat).\n\n    TODO(noam): inefficient - replace with a XLA allconcat when available\n\n    Args:\n      x: a LaidOutTensor\n      mesh_axis: an integer - the mesh axis along which to group\n      concat_axis: an integer (the Tensor axis along which to concatenate)\n      stack: a boolean - whether to stack instead of concat\n    Returns:\n      a LaidOutTensor\n    \"\"\"\n    x = x.to_laid_out_tensor()\n    coord = self.laid_out_pcoord(mesh_axis)\n    t = x.one_slice\n    old_shape = t.shape.as_list()\n    num_parts = self.shape[mesh_axis].size\n    t = tf.expand_dims(t, concat_axis)\n    t *= tf.reshape(\n        tf.one_hot(coord.one_slice, num_parts, dtype=t.dtype),\n        [num_parts if i == concat_axis else 1\n         for i in xrange(len(old_shape) + 1)])\n    if not stack:\n      new_shape = old_shape[:]\n      new_shape[concat_axis] *= num_parts\n      t = tf.reshape(t, new_shape)\n    return self.allreduce(self.LaidOutTensor([t]), [mesh_axis], \"SUM\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef alltoall(self, x, mesh_axis, split_axis, concat_axis):\n    x = x.to_laid_out_tensor()\n    t = x.one_slice\n    group_assignment = self._create_group_assignment([mesh_axis])\n    dtype = t.dtype\n    if dtype == tf.float32:\n      # There seems to be a bug with float32 alltoall.\n      # Do it in bfloat16 until the bug is fixed.\n      # TODO(noam): file a bug\n      t = tf.to_bfloat16(t)\n    t = tpu_ops.all_to_all(\n        t,\n        concat_dimension=concat_axis,\n        split_dimension=split_axis,\n        split_count=len(group_assignment[0]),\n        group_assignment=group_assignment)\n    t = tf.cast(t, dtype)\n    x = self.LaidOutTensor([t])\n    return x", "response": "Grouped alltoall with splitting and concatenation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef receive(self, x, mesh_axis, source_pcoord):\n    x = x.to_laid_out_tensor()\n    t = x.one_slice\n    source_target_pairs = []\n\n    for pnum in xrange(self.size):\n      coord = mtf.pnum_to_processor_coordinates(self.shape, pnum)\n      k = coord[mesh_axis]\n      if source_pcoord[k] is not None:\n        coord[mesh_axis] = source_pcoord[k]\n        target_pnum = mtf.processor_coordinates_to_pnum(self.shape, coord)\n        source_target_pairs.append(\n            [self.l2p(pnum),\n             self.l2p(target_pnum)])\n\n    if t.dtype in [tf.float32, tf.bfloat16, tf.int32]:\n      return tpu_ops.collective_permute(t, source_target_pairs)\n\n    # If t is not one of the allowed types, cast and cast back.\n    return tf.cast(tpu_ops.collective_permute(\n        tf.cast(t, tf.float32), source_target_pairs), t.dtype)", "response": "Collective receive in groups."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef slice(self, tf_tensor, tensor_shape):\n    tensor_layout = self.tensor_layout(tensor_shape)\n\n    if tensor_layout.is_fully_replicated:\n      return self.LaidOutTensor([tf_tensor])\n    else:\n      slice_shape = self.slice_shape(tensor_shape)\n      slice_begins = [\n          self.slice_begin(tensor_shape, pnum) for pnum in xrange(self.size)\n      ]\n      slice_begins_tensor = tf.stack(slice_begins)\n      # slice on source device\n      selected_slice_begin = tf.gather(slice_begins_tensor, self.pnum_tensor)\n      return self.LaidOutTensor(\n          [tf.slice(tf_tensor, selected_slice_begin, slice_shape)])", "response": "Slice out the corresponding part of tensor given the pnum variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a function in parallel on all slices.", "response": "def slicewise(self, fn, *inputs):\n    \"\"\"Execute a function in parallel on all slices.\n\n    Args:\n      fn: a function from tf.Tensors to tf.Tensor or a tuple of tf.Tensors.\n      *inputs: a list of inputs.  Each input is either a LaidOutTensor or\n        is convertible to a tf.Tensor.\n    Returns:\n      a LaidOutTensor, or a tuple of LaidOutTensors if fn returns a tuple.\n    \"\"\"\n    if fn == tf.add:\n      assert len(inputs) == 2\n      if isinstance(inputs[0], mtf.LazyAllreduceSum):\n        # sum of LazyAllreduceSum (keep delaying the allreduce)\n        return inputs[0] + inputs[1]\n    # convert all inputs to LaidOutTensor where possible\n    inputs = mtf.convert_args_to_laid_out_tensors(inputs)\n    ret = fn(*[\n        x.one_slice if isinstance(x, self.LaidOutTensor) else x\n        for x in inputs])\n    if isinstance(ret, tuple):\n      return tuple([self.LaidOutTensor([t]) for t in ret])\n    else:\n      return self.LaidOutTensor([ret])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef random(self, shape, tf_fn, kwargs):\n    # TODO(noam): can we make things better with stateless_random?\n    slice_shape = self.slice_shape(shape)\n    x = tf_fn(slice_shape, **kwargs)\n    # TPU does not have seeds enabled.  Sync up the\n    # random choices by zeroing out all but the first core per group of\n    # identical slices, then allreducing by group.\n    layout = self.tensor_layout(shape)\n    # we need to sync across these axes.\n    mesh_axes = [i for i in xrange(self.ndims)\n                 if i not in layout.tensor_axis_to_mesh_axis]\n    multiplier = 1.0\n    for axis in mesh_axes:\n      multiplier *= tf.cast(\n          tf.equal(self.laid_out_pcoord(axis).one_slice, 0), x.dtype)\n    x *= multiplier\n    x = self.LaidOutTensor([x])\n    x = self.allreduce(x, mesh_axes, \"SUM\")\n    return x", "response": "Call a random tf operation on the TPU."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nturn a Tensor into a LaidOutTensor.", "response": "def export_to_tf_tensor(self, x, laid_out_x):\n    \"\"\"Turn a Tensor into a tf.Tensor.\n\n    Args:\n      x: a Tensor\n      laid_out_x: a LaidOutTensor\n    Returns:\n      a tf.Tensor\n    \"\"\"\n    tensor_layout = self.tensor_layout(x.shape)\n    if not tensor_layout.is_fully_replicated:\n      raise NotImplementedError(\n          \"SimdMeshImpl only supports export_to_tf_tensor of fully-replicated \"\n          \"Tensors.  Try reshaping to new dimension names. \"\n          \" x.shape = %s tensor_layout=%s\"\n          % (x.shape, tensor_layout))\n    return laid_out_x.one_slice"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read32(bytestream):\n  dt = np.dtype(np.uint32).newbyteorder('>')\n  return np.frombuffer(bytestream.read(4), dtype=dt)[0]", "response": "Read 4 bytes from bytestream as an unsigned 32 - bit integer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_image_file_header(filename):\n  with tf.gfile.Open(filename, 'rb') as f:\n    magic = read32(f)\n    read32(f)  # num_images, unused\n    rows = read32(f)\n    cols = read32(f)\n    if magic != 2051:\n      raise ValueError('Invalid magic number %d in MNIST file %s' % (magic,\n                                                                     f.name))\n    if rows != 28 or cols != 28:\n      raise ValueError(\n          'Invalid MNIST file %s: Expected 28x28 images, found %dx%d' %\n          (f.name, rows, cols))", "response": "Validate that filename corresponds to images for the MNIST dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that filename corresponds to labels for the MNIST dataset.", "response": "def check_labels_file_header(filename):\n  \"\"\"Validate that filename corresponds to labels for the MNIST dataset.\"\"\"\n  with tf.gfile.Open(filename, 'rb') as f:\n    magic = read32(f)\n    read32(f)  # num_items, unused\n    if magic != 2049:\n      raise ValueError('Invalid magic number %d in MNIST file %s' % (magic,\n                                                                     f.name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads a file from the MNIST dataset if not already done.", "response": "def download(directory, filename):\n  \"\"\"Download (and unzip) a file from the MNIST dataset if not already done.\"\"\"\n  filepath = os.path.join(directory, filename)\n  if tf.gfile.Exists(filepath):\n    return filepath\n  if not tf.gfile.Exists(directory):\n    tf.gfile.MakeDirs(directory)\n  url = 'http://yann.lecun.com/exdb/mnist/' + filename + '.gz'\n  _, zipped_filepath = tempfile.mkstemp(suffix='.gz')\n  print('Downloading %s to %s' % (url, zipped_filepath))\n  urllib.request.urlretrieve(url, zipped_filepath)\n  with gzip.open(zipped_filepath, 'rb') as f_in, \\\n      tf.gfile.Open(filepath, 'wb') as f_out:\n    shutil.copyfileobj(f_in, f_out)\n  os.remove(zipped_filepath)\n  return filepath"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading and parse MNIST dataset.", "response": "def dataset(directory, images_file, labels_file):\n  \"\"\"Download and parse MNIST dataset.\"\"\"\n\n  images_file = download(directory, images_file)\n  labels_file = download(directory, labels_file)\n\n  check_image_file_header(images_file)\n  check_labels_file_header(labels_file)\n\n  def decode_image(image):\n    # Normalize from [0, 255] to [0.0, 1.0]\n    image = tf.decode_raw(image, tf.uint8)\n    image = tf.cast(image, tf.float32)\n    image = tf.reshape(image, [784])\n    return image / 255.0\n\n  def decode_label(label):\n    label = tf.decode_raw(label, tf.uint8)  # tf.string -> [tf.uint8]\n    label = tf.reshape(label, [])  # label is a scalar\n    return tf.to_int32(label)\n\n  images = tf.data.FixedLengthRecordDataset(\n      images_file, 28 * 28, header_bytes=16).map(decode_image)\n  labels = tf.data.FixedLengthRecordDataset(\n      labels_file, 1, header_bytes=8).map(decode_label)\n  return tf.data.Dataset.zip((images, labels))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsampling from a one - dimensional finite pmf", "response": "def sample_discrete(distn,size=[],dtype=np.int32):\n    'samples from a one-dimensional finite pmf'\n    distn = np.atleast_1d(distn)\n    assert (distn >=0).all() and distn.ndim == 1\n    if (0 == distn).all():\n        return np.random.randint(distn.shape[0],size=size)\n    cumvals = np.cumsum(distn)\n    return np.sum(np.array(random(size))[...,na] * cumvals[-1] > cumvals, axis=-1,dtype=dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsamples log probability array along specified axis", "response": "def sample_discrete_from_log(p_log,axis=0,dtype=np.int32):\n    'samples log probability array along specified axis'\n    cumvals = np.exp(p_log - np.expand_dims(p_log.max(axis),axis)).cumsum(axis) # cumlogaddexp\n    thesize = np.array(p_log.shape)\n    thesize[axis] = 1\n    randvals = random(size=thesize) * \\\n            np.reshape(cumvals[[slice(None) if i is not axis else -1\n                for i in range(p_log.ndim)]],thesize)\n    return np.sum(randvals > cumvals,axis=axis,dtype=dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef used_states(self):\n        'a list of the used states in the order they appear'\n        c = itertools.count()\n        canonical_ids = collections.defaultdict(lambda: next(c))\n        for s in self.states_list:\n            for state in s.stateseq:\n                canonical_ids[state]\n        return list(map(operator.itemgetter(0),\n                sorted(canonical_ids.items(),key=operator.itemgetter(1))))", "response": "a list of the used states in the order they appear"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the BIC on the passed data.", "response": "def BIC(self,data=None):\n        '''\n        BIC on the passed data. If passed data is None (default), calculates BIC\n        on the model's assigned data\n        '''\n        # NOTE: in principle this method computes the BIC only after finding the\n        # maximum likelihood parameters (or, of course, an EM fixed-point as an\n        # approximation!)\n        assert data is None and len(self.states_list) > 0, 'Must have data to get BIC'\n        if data is None:\n            return -2*sum(self.log_likelihood(s.data).sum() for s in self.states_list) + \\\n                        self.num_parameters() * np.log(\n                                sum(s.data.shape[0] for s in self.states_list))\n        else:\n            return -2*self.log_likelihood(data) + self.num_parameters() * np.log(data.shape[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_gaussian_2D(mu, lmbda, color='b', centermarker=True,label='',alpha=1.,ax=None,artists=None):\n    '''\n    Plots mean and cov ellipsoid into current axes. Must be 2D. lmbda is a covariance matrix.\n    '''\n    assert len(mu) == 2\n    ax = ax if ax else plt.gca()\n\n    # TODO use artists!\n\n    t = np.hstack([np.arange(0,2*np.pi,0.01),0])\n    circle = np.vstack([np.sin(t),np.cos(t)])\n    ellipse = np.dot(np.linalg.cholesky(lmbda),circle)\n\n    if artists is None:\n        point = ax.scatter([mu[0]],[mu[1]],marker='D',color=color,s=4,alpha=alpha) \\\n                if centermarker else None\n        line, = ax.plot(ellipse[0,:] + mu[0], ellipse[1,:] + mu[1],linestyle='-',\n                linewidth=2,color=color,label=label,alpha=alpha)\n    else:\n        line, point = artists\n        if centermarker:\n            point.set_offsets(np.atleast_2d(mu))\n        line.set_xdata(ellipse[0,:] + mu[0])\n        line.set_ydata(ellipse[1,:] + mu[1])\n        line.set_alpha(alpha)\n        line.set_color(color)\n\n    return line, point", "response": "Plots a 2D Gaussian distribution."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot a ndim gaussian projected onto 2D vecs", "response": "def plot_gaussian_projection(mu, lmbda, vecs, **kwargs):\n    '''\n    Plots a ndim gaussian projected onto 2D vecs, where vecs is a matrix whose two columns\n    are the subset of some orthonomral basis (e.g. from PCA on samples).\n    '''\n    return plot_gaussian_2D(project_data(mu,vecs),project_ellipsoid(lmbda,vecs),**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlike the function in scipy. stats but with an axis argument and works on arrays", "response": "def scoreatpercentile(data,per,axis=0):\n    'like the function in scipy.stats but with an axis argument and works on arrays'\n    a = np.sort(data,axis=axis)\n    idx = per/100. * (data.shape[axis]-1)\n\n    if (idx % 1 == 0):\n        return a[[slice(None) if ii != axis else idx for ii in range(a.ndim)]]\n    else:\n        lowerweight = 1-(idx % 1)\n        upperweight = (idx % 1)\n        idx = int(np.floor(idx))\n        return lowerweight * a[[slice(None) if ii != axis else idx for ii in range(a.ndim)]] \\\n                + upperweight * a[[slice(None) if ii != axis else idx+1 for ii in range(a.ndim)]]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef top_eigenvector(A,niter=1000,force_iteration=False):\n    '''\n    assuming the LEFT invariant subspace of A corresponding to the LEFT\n    eigenvalue of largest modulus has geometric multiplicity of 1 (trivial\n    Jordan block), returns the vector at the intersection of that eigenspace and\n    the simplex\n\n    A should probably be a ROW-stochastic matrix\n\n    probably uses power iteration\n    '''\n    n = A.shape[0]\n    np.seterr(invalid='raise',divide='raise')\n    if n <= 25 and not force_iteration:\n        x = np.repeat(1./n,n)\n        x = np.linalg.matrix_power(A.T,niter).dot(x)\n        x /= x.sum()\n        return x\n    else:\n        x1 = np.repeat(1./n,n)\n        x2 = x1.copy()\n        for itr in range(niter):\n            np.dot(A.T,x1,out=x2)\n            x2 /= x2.sum()\n            x1,x2 = x2,x1\n            if np.linalg.norm(x1-x2) < 1e-8:\n                break\n        return x1", "response": "returns the vector at the intersection of the simplex A with the top eigenvalue of largest modulus has geometric multiplicity of 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a random HTTP content type.", "response": "def content_type(self, mime_type: Optional[MimeType] = None) -> str:\n        \"\"\"Get a random HTTP content type.\n\n        :return: Content type.\n\n        :Example:\n            Content-Type: application/json\n        \"\"\"\n        fmt = self.__file.mime_type(type_=mime_type)\n        return 'Content-Type: {}'.format(fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ip_v4(self, with_port: bool = False) -> str:\n        ip = '.'.join(str(self.random.randint(0, 255)) for _ in range(4))\n\n        if with_port:\n            ip += ':{}'.format(self.port())\n\n        return ip", "response": "Generate a random IPv4 address."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a random IPv6 address.", "response": "def ip_v6(self) -> str:\n        \"\"\"Generate a random IPv6 address.\n\n        :return: Random IPv6 address.\n\n        :Example:\n            2001:c244:cf9d:1fb1:c56d:f52c:8a04:94f3\n        \"\"\"\n        ipv6 = IPv6Address(\n            self.random.randint(\n                0, 2 ** 128 - 1,\n            ),\n        )\n        return str(ipv6)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a random MAC address.", "response": "def mac_address(self) -> str:\n        \"\"\"Generate a random MAC address.\n\n        :return: Random MAC address.\n\n        :Example:\n            00:16:3e:25:e7:b1\n        \"\"\"\n        mac_hex = [\n            0x00, 0x16, 0x3e,\n            self.random.randint(0x00, 0x7f),\n            self.random.randint(0x00, 0xff),\n            self.random.randint(0x00, 0xff),\n        ]\n        mac = map(lambda x: '%02x' % x, mac_hex)\n        return ':'.join(mac)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef image_placeholder(width: Union[int, str] = 1920,\n                          height: Union[int, str] = 1080) -> str:\n        \"\"\"Generate a link to the image placeholder.\n\n        :param width: Width of image.\n        :param height: Height of image.\n        :return: URL to image placeholder.\n        \"\"\"\n        url = 'http://placehold.it/{width}x{height}'\n        return url.format(width=width, height=height)", "response": "Generate a link to the image placeholder."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a random stock image.", "response": "def stock_image(width: Union[int, str] = 1920,\n                    height: Union[int, str] = 1080,\n                    keywords: Optional[List[str]] = None,\n                    writable: bool = False) -> Union[str, bytes]:\n        \"\"\"Generate random stock image (JPEG) hosted on Unsplash.\n\n        .. note:: This method required an active HTTP connection.\n\n        :param width: Width of the image.\n        :param height: Height of the image.\n        :param keywords: List of search keywords.\n        :param writable: Return image as sequence ob bytes.\n        :return: Link to the image.\n        \"\"\"\n        api = 'https://source.unsplash.com/{}x{}?{}'\n\n        if keywords is not None:\n            keywords_str = ','.join(keywords)\n        else:\n            keywords_str = ''\n\n        url = api.format(width, height, keywords_str)\n\n        try:\n            response = urllib.request.urlopen(url)\n            if writable:\n                return response.read()\n            url = response.geturl()\n            return url\n        except urllib.error.URLError:\n            raise urllib.error.URLError(\n                'Required an active HTTP connection')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a list of hashtags.", "response": "def hashtags(self, quantity: int = 4) -> Union[str, list]:\n        \"\"\"Generate a list of hashtags.\n\n        :param quantity: The quantity of hashtags.\n        :return: The list of hashtags.\n        :raises NonEnumerableError: if category is not in Hashtag.\n\n        :Example:\n            ['#love', '#sky', '#nice']\n        \"\"\"\n        tags = ['#' + self.random.choice(HASHTAGS)\n                for _ in range(quantity)]\n\n        if int(quantity) == 1:\n            return tags[0]\n\n        return tags"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a random home page.", "response": "def home_page(self, tld_type: Optional[TLDType] = None) -> str:\n        \"\"\"Generate a random home page.\n\n        :param tld_type: TLD type.\n        :return: Random home page.\n\n        :Example:\n            http://www.fontir.info\n        \"\"\"\n        resource = self.random.choice(USERNAMES)\n        domain = self.top_level_domain(\n            tld_type=tld_type,\n        )\n\n        return 'http://www.{}{}'.format(\n            resource, domain)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a random top level domain.", "response": "def top_level_domain(self, tld_type: Optional[TLDType] = None) -> str:\n        \"\"\"Return random top level domain.\n\n        :param tld_type: Enum object DomainType\n        :return: Top level domain.\n        :raises NonEnumerableError: if tld_type not in DomainType.\n        \"\"\"\n        key = self._validate_enum(item=tld_type, enum=TLDType)\n        return self.random.choice(TLD[key])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a random network protocol name.", "response": "def network_protocol(self, layer: Optional[Layer] = None) -> str:\n        \"\"\"Get a random network protocol form OSI model.\n\n        :param layer: Enum object Layer.\n        :return: Protocol name.\n\n        :Example:\n            AMQP\n        \"\"\"\n        key = self._validate_enum(item=layer, enum=Layer)\n        protocols = NETWORK_PROTOCOLS[key]\n        return self.random.choice(protocols)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating random port. :param port_range: Range enum object. :return: Port number. :raises NonEnumerableError: if port_range is not in PortRange. :Example: 8080", "response": "def port(self, port_range: PortRange = PortRange.ALL) -> int:\n        \"\"\"Generate random port.\n\n        :param port_range: Range enum object.\n        :return: Port number.\n        :raises NonEnumerableError: if port_range is not in PortRange.\n\n        :Example:\n            8080\n        \"\"\"\n        if port_range and port_range in PortRange:\n            return self.random.randint(*port_range.value)\n        else:\n            raise NonEnumerableError(PortRange)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a truck model.", "response": "def truck(self, model_mask: str = '#### @@') -> str:\n        \"\"\"Generate a truck model.\n\n        :param model_mask: Mask of truck model. Here '@' is a\n            placeholder of characters and '#' is a placeholder of digits.\n        :return: Dummy truck model.\n\n        :Example:\n            Caledon-966O.\n        \"\"\"\n        return '{}-{}'.format(\n            self.random.choice(TRUCKS),\n            self.random.custom_code(model_mask),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef airplane(self, model_mask: str = '###') -> str:\n        model = self.random.custom_code(mask=model_mask)\n        plane = self.random.choice(AIRPLANES)\n        return '{} {}'.format(plane, model)", "response": "Generate a dummy airplane model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget vehicle registration code of country.", "response": "def vehicle_registration_code(self, locale: Optional[str] = None) -> str:\n        \"\"\"Get vehicle registration code of country.\n\n        :param locale: Registration code for locale (country).\n        :return: Vehicle registration code.\n        \"\"\"\n        if locale:\n            return VRC_BY_LOCALES[locale]\n\n        return self.random.choice(VR_CODES)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bulk_create_datetimes(date_start: DateTime,\n                              date_end: DateTime, **kwargs) -> List[DateTime]:\n        \"\"\"Bulk create datetime objects.\n\n        This method creates list of datetime objects from\n        ``date_start`` to ``date_end``.\n\n        You can use the following keyword arguments:\n\n        * ``days``\n        * ``hours``\n        * ``minutes``\n        * ``seconds``\n        * ``microseconds``\n\n        See datetime module documentation for more:\n        https://docs.python.org/3.7/library/datetime.html#timedelta-objects\n\n\n        :param date_start: Begin of the range.\n        :param date_end: End of the range.\n        :param kwargs: Keyword arguments for datetime.timedelta\n        :return: List of datetime objects\n        :raises: ValueError: When ``date_start``/``date_end`` not passed and\n            when ``date_start`` larger than ``date_end``.\n        \"\"\"\n        dt_objects = []\n\n        if not date_start and not date_end:\n            raise ValueError('You must pass date_start and date_end')\n\n        if date_end < date_start:\n            raise ValueError('date_start can not be larger than date_end')\n\n        while date_start <= date_end:\n            date_start += timedelta(**kwargs)\n            dt_objects.append(date_start)\n\n        return dt_objects", "response": "Bulk create datetime objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef week_date(self, start: int = 2017, end: int = 2018) -> str:\n        year = self.year(start, end)\n        week = self.random.randint(1, 52)\n        return '{year}-W{week}'.format(\n            year=year,\n            week=week,\n        )", "response": "Get week number with year."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a random day of the week.", "response": "def day_of_week(self, abbr: bool = False) -> str:\n        \"\"\"Get a random day of week.\n\n        :param abbr: Abbreviated day name.\n        :return: Day of the week.\n        \"\"\"\n        key = 'abbr' if abbr else 'name'\n        days = self._data['day'].get(key)\n        return self.random.choice(days)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef month(self, abbr: bool = False) -> str:\n        key = 'abbr' if abbr else 'name'\n        months = self._data['month'].get(key)\n        return self.random.choice(months)", "response": "Get a random month name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef year(self, minimum: int = 1990, maximum: int = 2050) -> int:\n        return self.random.randint(minimum, maximum)", "response": "Generate a random year."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a random periodicity string.", "response": "def periodicity(self) -> str:\n        \"\"\"Get a random periodicity string.\n\n        :return: Periodicity.\n        \"\"\"\n        periodicity = self._data['periodicity']\n        return self.random.choice(periodicity)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef date(self, start: int = 2000, end: int = 2019) -> Date:\n        year = self.random.randint(start, end)\n        month = self.random.randint(1, 12)\n        day = self.random.randint(1, monthrange(year, month)[1])\n        date_object = date(year, month, day)\n        return date_object", "response": "Generate random date object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a random date string.", "response": "def formatted_date(self, fmt: str = '', **kwargs) -> str:\n        \"\"\"Generate random date as string.\n\n        :param fmt: The format of date, if None then use standard\n            accepted in the current locale.\n        :param kwargs: Keyword arguments for :meth:`~Datetime.date()`\n        :return: Formatted date.\n        \"\"\"\n        date_obj = self.date(**kwargs)\n\n        if not fmt:\n            fmt = self._data['formats'].get('date')\n\n        return date_obj.strftime(fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef time(self) -> Time:\n        random_time = time(\n            self.random.randint(0, 23),\n            self.random.randint(0, 59),\n            self.random.randint(0, 59),\n            self.random.randint(0, 999999),\n        )\n        return random_time", "response": "Generate a random time object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef formatted_time(self, fmt: str = '') -> str:\n        time_obj = self.time()\n\n        if not fmt:\n            fmt = self._data['formats'].get('time')\n        return time_obj.strftime(fmt)", "response": "Generate a string formatted time."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate random datetime. :param start: Minimum value of year. :param end: Maximum value of year. :param timezone: Set custom timezone (pytz required). :return: Datetime", "response": "def datetime(self, start: int = 2000, end: int = 2035,\n                 timezone: Optional[str] = None) -> DateTime:\n        \"\"\"Generate random datetime.\n\n        :param start: Minimum value of year.\n        :param end: Maximum value of year.\n        :param timezone: Set custom timezone (pytz required).\n        :return: Datetime\n        \"\"\"\n        datetime_obj = datetime.combine(\n            date=self.date(start, end),\n            time=self.time(),\n        )\n        if timezone:\n            if not pytz:\n                raise ImportError('Timezones are supported only with pytz')\n            tz = pytz.timezone(timezone)\n            datetime_obj = tz.localize(datetime_obj)\n\n        return datetime_obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef formatted_datetime(self, fmt: str = '', **kwargs) -> str:\n        dt_obj = self.datetime(**kwargs)\n\n        if not fmt:\n            date_fmt = self._data['formats'].get('date')\n            time_fmt = self._data['formats'].get('time')\n            fmt = '{} {}'.format(date_fmt, time_fmt)\n\n        return dt_obj.strftime(fmt)", "response": "Generate a datetime string in human readable format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a random timestamp.", "response": "def timestamp(self, posix: bool = True, **kwargs) -> Union[str, int]:\n        \"\"\"Generate random timestamp.\n\n        :param posix: POSIX time.\n        :param kwargs: Kwargs for :meth:`~Datetime.datetime()`.\n        :return: Timestamp.\n        \"\"\"\n        stamp = self.datetime(**kwargs)\n\n        if posix:\n            return timegm(stamp.utctimetuple())\n\n        return stamp.strftime('%Y-%m-%dT%H:%M:%SZ')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates random UUID. :param version: UUID version. :return: UUID", "response": "def uuid(self, version: int = None) -> str:\n        \"\"\"Generate random UUID.\n\n        :param version: UUID version.\n        :return: UUID\n        \"\"\"\n        bits = self.random.getrandbits(128)\n        return str(uuid.UUID(int=bits, version=version))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates random hash. To change hashing algorithm, pass parameter ``algorithm`` with needed value of the enum object :class:`~mimesis.enums.Algorithm` :param algorithm: Enum object :class:`~mimesis.enums.Algorithm`. :return: Hash. :raises NonEnumerableError: if algorithm is not supported.", "response": "def hash(self, algorithm: Algorithm = None) -> str:  # noqa: A003\n        \"\"\"Generate random hash.\n\n        To change hashing algorithm, pass parameter ``algorithm``\n        with needed value of the enum object :class:`~mimesis.enums.Algorithm`\n\n        :param algorithm: Enum object :class:`~mimesis.enums.Algorithm`.\n        :return: Hash.\n        :raises NonEnumerableError: if algorithm is not supported.\n        \"\"\"\n        key = self._validate_enum(algorithm, Algorithm)\n\n        if hasattr(hashlib, key):\n            fn = getattr(hashlib, key)\n            return fn(self.uuid().encode()).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate pseudo mnemonic phrase.", "response": "def mnemonic_phrase(self, length: int = 12) -> str:\n        \"\"\"Generate pseudo mnemonic phrase.\n\n        :param length: Number of words.\n        :return: Mnemonic code.\n        \"\"\"\n        words = self.__words['normal']\n        return ' '.join(self.random.choice(words) for _ in range(length))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind all files of all locales.", "response": "def initialize_options(self):\n        \"\"\"Find all files of all locales.\"\"\"\n        self.paths = []\n        self.separators = (',', ':')\n        self.data_dir = join(here, 'mimesis', 'data')\n        self.before_total = 0\n        self.after_total = 0\n\n        for root, _, files in os.walk(self.data_dir):\n            for file in sorted(files):\n                if splitext(file)[1] == '.json':\n                    self.paths.append(join(\n                        relpath(root, self.data_dir), file))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        for rel_path in sorted(self.paths):\n            file_path = join(self.data_dir, rel_path)\n            self.minify(file_path)\n\n        after = self.size_of(self.after_total)\n        before = self.size_of(self.before_total)\n        saved = self.size_of(self.before_total - self.after_total)\n\n        template = '\\nTotal: ' \\\n                   '\\033[92m{}\\033[0m -> \\033[92m{}\\033[0m. ' \\\n                   'Compressed: \\033[92m{}\\033[0m\\n'\n\n        print(template.format(before, after, saved))", "response": "Start json minimizer and exit when all json files were minimized."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a random snippet of CSS.", "response": "def css(self) -> str:\n        \"\"\"Generate a random snippet of CSS.\n\n        :return: CSS.\n        \"\"\"\n        selector = self.random.choice(CSS_SELECTORS)\n        css_sel = '{}{}'.format(selector, self.__text.word())\n\n        cont_tag = self.random.choice(list(HTML_CONTAINER_TAGS.keys()))\n        mrk_tag = self.random.choice(HTML_MARKUP_TAGS)\n\n        base = '{}'.format(self.random.choice([cont_tag, mrk_tag, css_sel]))\n        props = '; '.join(\n            [self.css_property() for _ in range(self.random.randint(1, 6))])\n        return '{} {{{}}}'.format(base, props)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a random CSS property.", "response": "def css_property(self) -> str:\n        \"\"\"Generate a random snippet of CSS that assigns value to a property.\n\n        :return: CSS property.\n\n        :Examples:\n            'background-color: #f4d3a1'\n        \"\"\"\n        prop = self.random.choice(list(CSS_PROPERTIES.keys()))\n        val = CSS_PROPERTIES[prop]\n\n        if isinstance(val, list):\n            val = self.random.choice(val)\n        elif val == 'color':\n            val = self.__text.hex_color()\n        elif val == 'size':\n            val = '{}{}'.format(self.random.randint(1, 99),\n                                self.random.choice(CSS_SIZE_UNITS))\n\n        return '{}: {}'.format(prop, val)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef html(self) -> str:\n        tag_name = self.random.choice(list(HTML_CONTAINER_TAGS))\n        tag_attributes = list(HTML_CONTAINER_TAGS[tag_name])  # type: ignore\n        k = self.random.randint(1, len(tag_attributes))\n\n        selected_attrs = self.random.sample(tag_attributes, k=k)\n\n        attrs = []\n        for attr in selected_attrs:\n            attrs.append('{}=\"{}\"'.format(\n                attr, self.html_attribute_value(tag_name, attr)))\n\n        html_result = '<{tag} {attrs}>{content}</{tag}>'\n        return html_result.format(\n            tag=tag_name,\n            attrs=' '.join(attrs),\n            content=self.__text.sentence(),\n        )", "response": "Generate a random HTML tag with text inside and some attrs set."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef html_attribute_value(self, tag: str = None,\n                             attribute: str = None) -> str:\n        \"\"\"Generate random value for specified HTML tag attribute.\n\n        :param tag: An HTML tag.\n        :param attribute: An attribute of the specified tag.\n        :return: An attribute.\n        :raises NotImplementedError: if tag is unsupported.\n        \"\"\"\n        if not tag:\n            tag = self.random.choice(\n                list(HTML_CONTAINER_TAGS.keys()),\n            )\n        if not attribute:\n            attribute = self.random.choice(\n                list(HTML_CONTAINER_TAGS[tag]),  # type: ignore\n            )\n\n        try:\n            value = HTML_CONTAINER_TAGS[tag][attribute]  # type: ignore\n        except KeyError:\n            raise NotImplementedError(\n                'Tag {} or attribute {} is not supported'.format(\n                    tag, attribute))\n\n        if isinstance(value, list):\n            value = self.random.choice(value)\n        elif value == 'css':\n            value = self.css_property()\n        elif value == 'word':\n            value = self.__text.word()\n        elif value == 'url':\n            value = self.__inet.home_page()\n        else:\n            raise NotImplementedError(\n                'Attribute type {} is not implemented'.format(value))\n        return value", "response": "Generate random value for specified HTML tag attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef version(self, calver: bool = False, pre_release: bool = False) -> str:\n        # TODO: Optimize\n        version = '{}.{}.{}'\n        major, minor, patch = self.random.randints(3, 0, 10)\n\n        if calver:\n            if minor == 0:\n                minor += 1\n\n            if patch == 0:\n                patch += 1\n            major = self.random.randint(2016, 2018)\n            return version.format(major, minor, patch)\n\n        version = '{}.{}.{}'.format(major, minor, patch)\n\n        if pre_release:\n            suffixes = ('alpha', 'beta', 'rc')\n            suffix = self.random.choice(suffixes)\n            number = self.random.randint(1, 11)\n            return '{}-{}.{}'.format(version, suffix, number)\n\n        return version", "response": "Generate version number.\n\n        :param calver: Calendar versioning.\n        :param pre_release: Pre-release.\n        :return: Version.\n\n        :Example:\n            0.2.1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chemical_element(self, name_only: bool = True) -> Union[dict, str]:\n        elements = self._data['chemical_element']\n        nm, sm, an = self.random.choice(elements).split('|')\n\n        if not name_only:\n            return {\n                'name': nm.strip(),\n                'symbol': sm.strip(),\n                'atomic_number': an.strip(),\n            }\n\n        return nm.strip()", "response": "Generate a random chemical element."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a random CPF.", "response": "def cpf(self, with_mask: bool = True) -> str:\n        \"\"\"Get a random CPF.\n\n        :param with_mask: Use CPF mask (###.###.###-##).\n        :returns: Random CPF.\n\n        :Example:\n            001.137.297-40\n        \"\"\"\n        def get_verifying_digit_cpf(cpf, peso):\n            \"\"\"Calculate the verifying digit for the CPF.\n\n            :param cpf: List of integers with the CPF.\n            :param peso: Integer with the weight for the modulo 11 calculate.\n            :returns: The verifying digit for the CPF.\n            \"\"\"\n            soma = 0\n            for index, digit in enumerate(cpf):\n                soma += digit * (peso - index)\n            resto = soma % 11\n            if resto == 0 or resto == 1 or resto >= 11:\n                return 0\n            return 11 - resto\n\n        cpf_without_dv = [self.random.randint(0, 9) for _ in range(9)]\n        first_dv = get_verifying_digit_cpf(cpf_without_dv, 10)\n\n        cpf_without_dv.append(first_dv)\n        second_dv = get_verifying_digit_cpf(cpf_without_dv, 11)\n        cpf_without_dv.append(second_dv)\n\n        cpf = ''.join([str(i) for i in cpf_without_dv])\n\n        if with_mask:\n            return cpf[:3] + '.' + cpf[3:6] + '.' + cpf[6:9] + '-' + cpf[9:]\n        return cpf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cnpj(self, with_mask: bool = True) -> str:\n        def get_verifying_digit_cnpj(cnpj, peso):\n            \"\"\"Calculate the verifying digit for the CNPJ.\n\n            :param cnpj: List of integers with the CNPJ.\n            :param peso: Integer with the weight for the modulo 11 calculate.\n            :returns: The verifying digit for the CNPJ.\n            \"\"\"\n            soma = 0\n            if peso == 5:\n                peso_list = [5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n            elif peso == 6:\n                peso_list = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n            for i, _ in enumerate(cnpj):\n                soma += peso_list[i] * cnpj[i]\n            resto = soma % 11\n            if resto < 2:\n                return 0\n            return 11 - resto\n\n        cnpj_without_dv = [self.random.randint(0, 9) for _ in range(12)]\n\n        first_dv = get_verifying_digit_cnpj(cnpj_without_dv, 5)\n        cnpj_without_dv.append(first_dv)\n\n        second_dv = get_verifying_digit_cnpj(cnpj_without_dv, 6)\n        cnpj_without_dv.append(second_dv)\n\n        cnpj = ''.join([str(i) for i in cnpj_without_dv])\n\n        if with_mask:\n            return '{}.{}.{}/{}-{}'.format(cnpj[:2], cnpj[2:5],\n                                           cnpj[5:8], cnpj[8:12], cnpj[12:])\n        return cnpj", "response": "Get a random CNPJ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef romanized(locale: str = '') -> Callable:\n    def romanized_deco(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                # String can contain ascii symbols, digits and\n                # punctuation symbols.\n                alphabet = {s: s for s in\n                            letters + digits + punctuation}\n                alphabet.update(data.ROMANIZATION_DICT[locale])\n                # Add common cyrillic letters\n                alphabet.update(data.COMMON_LETTERS)\n            except KeyError:\n                raise UnsupportedLocale(locale)\n            result = func(*args, **kwargs)\n            txt = ''.join([alphabet[i] for i in result if i in alphabet])\n            return txt\n\n        return wrapper\n\n    return romanized_deco", "response": "Romanize the Cyrillic text."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_sentence(self) -> str:\n        sentences = self._data['sentence']\n        sentence = [\n            self.random.choice(sentences[k]) for k\n            in ('head', 'p1', 'p2', 'tail')\n        ]\n        return '{0} {1} {2} {3}'.format(*sentence)", "response": "Generate a sentence from the parts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patronymic(self, gender: Gender = None) -> str:\n        gender = self._validate_enum(gender, Gender)\n        patronymics = self._data['patronymic'][gender]\n        return self.random.choice(patronymics)", "response": "Generate random patronymic name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate random series of passport.", "response": "def passport_series(self, year: int = None) -> str:\n        \"\"\"Generate random series of passport.\n\n        :param year: Year of manufacture.\n        :type year: int or None\n        :return: Series.\n\n        :Example:\n            02 15.\n        \"\"\"\n        if not year:\n            year = self.random.randint(10, 18)\n\n        region = self.random.randint(1, 99)\n        return '{:02d} {}'.format(region, year)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating snils with special algorithm.", "response": "def snils(self) -> str:\n        \"\"\"Generate snils with special algorithm.\n\n        :return: SNILS.\n\n        :Example:\n            41917492600.\n        \"\"\"\n        numbers = []\n        control_codes = []\n\n        for i in range(0, 9):\n            numbers.append(self.random.randint(0, 9))\n\n        for i in range(9, 0, -1):\n            control_codes.append(numbers[9 - i] * i)\n\n        control_code = sum(control_codes)\n        code = ''.join(str(number) for number in numbers)\n\n        if control_code in (100, 101):\n            snils = code + '00'\n            return snils\n\n        if control_code < 100:\n            snils = code + str(control_code)\n            return snils\n\n        if control_code > 101:\n            control_code = control_code % 101\n            if control_code == 100:\n                control_code = 0\n            snils = code + '{:02}'.format(control_code)\n            return snils"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating random but valid INN.", "response": "def inn(self) -> str:\n        \"\"\"Generate random, but valid ``INN``.\n\n        :return: INN.\n        \"\"\"\n        def control_sum(nums: list, t: str) -> int:\n            digits = {\n                'n2': [7, 2, 4, 10, 3, 5, 9, 4, 6, 8],\n                'n1': [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8],\n            }\n            number = 0\n            length = digits[t]\n            for i in range(0, len(length)):\n                number += nums[i] * length[i]\n            return number % 11 % 10\n\n        numbers = []\n        for x in range(0, 10):\n            numbers.append(self.random.randint(1 if x == 0 else 0, 9))\n\n        n2 = control_sum(numbers, 'n2')\n        numbers.append(n2)\n        n1 = control_sum(numbers, 'n1')\n        numbers.append(n1)\n        return ''.join([str(x) for x in numbers])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ogrn(self) -> str:\n        numbers = []\n        for _ in range(0, 12):\n            numbers.append(self.random.randint(1 if _ == 0 else 0, 9))\n\n        ogrn = ''.join([str(x) for x in numbers])\n        check_sum = str(int(ogrn) % 11 % 10)\n\n        return '{}{}'.format(ogrn, check_sum)", "response": "Generate random valid OGRN."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bic(self) -> str:\n        country_code = '04'\n        code = '{:02}'.format(self.random.randint(1, 10))\n        bank_number = '{:02}'.format(self.random.randint(0, 99))\n        bank_office = '{:03}'.format(self.random.randint(50, 999))\n        bic = country_code + code + bank_number + bank_office\n        return bic", "response": "Generate random ``BIC`` (Bank ID Code).\n\n        :return: BIC.\n\n        :Example:\n            044025575."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef kpp(self) -> str:\n        tax_codes = [\n            '7700', '7800', '5000', '0100',\n            '0200', '0300', '0500', '0600',\n            '0700', '0800', '0900', '1000',\n            '1100', '1200', '1300', '1400',\n            '1500', '1600', '1700', '1800',\n            '1900', '2000', '2100', '2200',\n            '2300', '2400', '2500', '2600',\n            '2700', '2800', '2900', '3000',\n            '3100', '3200', '3300', '3400',\n            '3500', '3600', '3700', '3800',\n            '3900', '4000', '4100', '4900',\n            '5100', '5200', '5300', '5400',\n            '5500', '5600', '5700', '5800',\n            '5900', '6000', '6100', '6200',\n            '6300', '6400', '6500', '6600',\n            '6700', '6800', '6900', '7000',\n            '7100', '7200', '7300', '7400',\n            '7500', '7600', '7900', '8600',\n            '8700', '8900', '9100', '9200',\n            '9800', '9900', '9901', '9951',\n            '9952', '9953', '9954', '9955',\n            '9956', '9957', '9958', '9959',\n            '9961', '9962', '9965', '9966',\n            '9971', '9972', '9973', '9974',\n            '9975', '9976', '9977', '9979',\n            '9998',\n        ]\n\n        tax_code = tax_codes[self.random.randint(0, len(tax_codes) - 1)]\n        reg_code = '{:02}'.format(self.random.randint(1, 99))\n        reg_number = '{:03}'.format(self.random.randint(1, 999))\n        kpp = tax_code + reg_code + reg_number\n        return kpp", "response": "Generate random ``KPP``.\n\n        :return: 'KPP'.\n\n        :Example:\n            560058652."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cpr(self) -> str:\n        day = '{:02d}'.format(self.random.randint(1, 31))\n        month = '{:02d}'.format(self.random.randint(1, 12))\n        year = '{:02d}'.format(self.random.randint(0, 99))\n        serial_number = '{:04d}'.format(self.random.randint(0, 9999))\n\n        cpr_nr = '{}{}{}{}'.format(day, month, year, serial_number)\n\n        return cpr_nr", "response": "Generate a random CPR number."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cpu_frequency(self) -> str:\n        return '{}GHz'.format(\n            self.random.uniform(\n                a=1.5,\n                b=4.3,\n                precision=1,\n            ),\n        )", "response": "Get a random frequency of CPU."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a list of random float numbers.", "response": "def floats(self, n: int = 2) -> List[float]:\n        \"\"\"Generate a list of random float numbers.\n\n        :param n: Raise 10 to the 'n' power.\n        :return: The list of floating-point numbers.\n        \"\"\"\n        nums = [self.random.random()\n                for _ in range(10 ** int(n))]\n        return nums"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef integers(self, start: int = 0, end: int = 10,\n                 length: int = 10) -> List[int]:\n        \"\"\"Generate a list of random integers.\n\n        Integers can be negative or positive numbers.\n        .. note: You can use both positive and negative numbers.\n\n        :param start: Start.\n        :param end: End.\n        :param length: Length of list.\n        :return: List of integers.\n\n        :Example:\n            [-20, -19, -18, -17]\n        \"\"\"\n        return self.random.randints(\n            length, start, end)", "response": "Generate a list of random integers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a list of prime numbers from start to end.", "response": "def primes(start: int = 1, end: int = 999) -> List[int]:\n        \"\"\"Generate a list of prime numbers.\n\n        :param start: First value of range.\n        :param end: Last value of range.\n        :return: A list of prime numbers from start to end.\n        \"\"\"\n        # TODO: It should generate random primes with passed length.\n        sieve_size = (end // 2 - 1) if end % 2 == 0 else (end // 2)\n        sieve = [True] * sieve_size\n\n        primes = []  # list of primes\n        # add 2 to the list if it's in the given range\n        if end >= 2:\n            primes.append(2)\n        for i in range(sieve_size):\n            if sieve[i]:\n                value_at_i = i * 2 + 3\n                primes.append(value_at_i)\n                for j in range(i, sieve_size, value_at_i):\n                    sieve[j] = False\n\n        chop_index = 0\n        for i in range(len(primes)):\n            if primes[i] >= start:\n                chop_index = i\n                break\n        return primes[chop_index:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef digit(self, to_bin: bool = False) -> Union[str, int]:\n        digit = self.random.randint(0, 9)\n\n        if to_bin:\n            return bin(digit)\n\n        return digit", "response": "Get a random digit."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a random number between minimum and maximum.", "response": "def between(self, minimum: int = 1, maximum: int = 1000) -> int:\n        \"\"\"Generate a random number between minimum and maximum.\n\n        :param minimum: Minimum of range.\n        :param maximum: Maximum of range.\n        :return: Number.\n        \"\"\"\n        return self.random.randint(minimum, maximum)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef age(self, minimum: int = 16, maximum: int = 66) -> int:\n        age = self.random.randint(minimum, maximum)\n        self._store['age'] = age\n        return age", "response": "Get a random integer value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef work_experience(self, working_start_age: int = 22) -> int:\n        age = self._store['age']\n        if age == 0:\n            age = self.age()\n\n        return max(age - working_start_age, 0)", "response": "Get a work experience."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef name(self, gender: Optional[Gender] = None) -> str:\n        key = self._validate_enum(gender, Gender)\n        names = self._data['names'].get(key)\n        return self.random.choice(names)", "response": "Generate a random name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a random surname.", "response": "def surname(self, gender: Optional[Gender] = None) -> str:\n        \"\"\"Generate a random surname.\n\n        :param gender: Gender's enum object.\n        :return: Surname.\n\n        :Example:\n            Smith.\n        \"\"\"\n        surnames = self._data['surnames']\n\n        # Surnames separated by gender.\n        if isinstance(surnames, dict):\n            key = self._validate_enum(gender, Gender)\n            surnames = surnames[key]\n\n        return self.random.choice(surnames)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef title(self, gender: Optional[Gender] = None,\n              title_type: Optional[TitleType] = None) -> str:\n        \"\"\"Generate a random title for name.\n\n        You can generate random prefix or suffix\n        for name using this method.\n\n        :param gender: The gender.\n        :param title_type: TitleType enum object.\n        :return: The title.\n        :raises NonEnumerableError: if gender or title_type in incorrect format.\n\n        :Example:\n            PhD.\n        \"\"\"\n        gender_key = self._validate_enum(gender, Gender)\n        title_key = self._validate_enum(title_type, TitleType)\n\n        titles = self._data['title'][gender_key][title_key]\n        return self.random.choice(titles)", "response": "Generate a random title for name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef full_name(self, gender: Optional[Gender] = None,\n                  reverse: bool = False) -> str:\n        \"\"\"Generate a random full name.\n\n        :param reverse: Return reversed full name.\n        :param gender: Gender's enum object.\n        :return: Full name.\n\n        :Example:\n            Johann Wolfgang.\n        \"\"\"\n        if gender is None:\n            gender = get_random_item(Gender, rnd=self.random)\n\n        if gender and isinstance(gender, Gender):\n            gender = gender\n        else:\n            raise NonEnumerableError(Gender)\n\n        fmt = '{1} {0}' if reverse else '{0} {1}'\n        return fmt.format(\n            self.name(gender),\n            self.surname(gender),\n        )", "response": "Generate a random full name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a random username for a specific template.", "response": "def username(self, template: Optional[str] = None) -> str:\n        \"\"\"Generate username by template.\n\n        Supported template placeholders: (U, l, d)\n\n        Supported separators: (-, ., _)\n\n        Template must contain at least one \"U\" or \"l\" placeholder.\n\n        If template is None one of the following templates is used:\n        ('U_d', 'U.d', 'U-d', 'UU-d', 'UU.d', 'UU_d',\n        'ld', 'l-d', 'Ud', 'l.d', 'l_d', 'default')\n\n        :param template: Template.\n        :return: Username.\n        :raises ValueError: If template is not supported.\n\n        :Example:\n            Celloid1873\n        \"\"\"\n        MIN_DATE = 1800\n        MAX_DATE = 2070\n        DEFAULT_TEMPLATE = 'l.d'\n\n        templates = ('U_d', 'U.d', 'U-d', 'UU-d', 'UU.d', 'UU_d',\n                     'ld', 'l-d', 'Ud', 'l.d', 'l_d', 'default')\n\n        if template is None:\n            template = self.random.choice(templates)\n\n        if template == 'default':\n            template = DEFAULT_TEMPLATE\n\n        if not re.fullmatch(r'[Ul\\.\\-\\_d]*[Ul]+[Ul\\.\\-\\_d]*', template):\n            raise ValueError(\n                \"Template '{}' is not supported.\".format(template))\n\n        tags = re.findall(r'[Uld\\.\\-\\_]', template)\n\n        username = ''\n        for tag in tags:\n            if tag == 'U':\n                username += self.random.choice(USERNAMES).capitalize()\n            elif tag == 'l':\n                username += self.random.choice(USERNAMES)\n            elif tag == 'd':\n                username += str(self.random.randint(MIN_DATE, MAX_DATE))\n            elif tag in '-_.':\n                username += tag\n\n        return username"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef password(self, length: int = 8, hashed: bool = False) -> str:\n        text = ascii_letters + digits + punctuation\n        password = ''.join([self.random.choice(text) for _ in range(length)])\n\n        if hashed:\n            md5 = hashlib.md5()\n            md5.update(password.encode())\n            return md5.hexdigest()\n        else:\n            return password", "response": "Generate a password or hash of password."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a random email.", "response": "def email(self, domains: Union[tuple, list] = None) -> str:\n        \"\"\"Generate a random email.\n\n        :param domains: List of custom domains for emails.\n        :type domains: list or tuple\n        :return: Email address.\n\n        :Example:\n            foretime10@live.com\n        \"\"\"\n        if not domains:\n            domains = EMAIL_DOMAINS\n\n        domain = self.random.choice(domains)\n        name = self.username(template='ld')\n        return '{name}{domain}'.format(\n            name=name,\n            domain=domain,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef social_media_profile(self,\n                             site: Optional[SocialNetwork] = None) -> str:\n        \"\"\"Generate profile for random social network.\n\n        :return: Profile in some network.\n\n        :Example:\n            http://facebook.com/some_user\n        \"\"\"\n        key = self._validate_enum(site, SocialNetwork)\n        website = SOCIAL_NETWORKS[key]\n        url = 'https://www.' + website\n        return url.format(self.username())", "response": "Generate social media profile for random social network."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a random gender.", "response": "def gender(self, iso5218: bool = False,\n               symbol: bool = False) -> Union[str, int]:\n        \"\"\"Get a random gender.\n\n        Get a random title of gender, code for the representation\n        of human sexes is an international standard that defines a\n        representation of human sexes through a language-neutral single-digit\n        code or symbol of gender.\n\n        :param iso5218:\n            Codes for the representation of human sexes is an international\n            standard (0 - not known, 1 - male, 2 - female, 9 - not applicable).\n        :param symbol: Symbol of gender.\n        :return: Title of gender.\n\n        :Example:\n            Male\n        \"\"\"\n        if iso5218:\n            return self.random.choice([0, 1, 2, 9])\n\n        if symbol:\n            return self.random.choice(GENDER_SYMBOLS)\n\n        return self.random.choice(self._data['gender'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a random height in M ( Meter.", "response": "def height(self, minimum: float = 1.5, maximum: float = 2.0) -> str:\n        \"\"\"Generate a random height in M (Meter).\n\n        :param minimum: Minimum value.\n        :param float maximum: Maximum value.\n        :return: Height.\n\n        :Example:\n            1.85.\n        \"\"\"\n        h = self.random.uniform(minimum, maximum)\n        return '{:0.2f}'.format(h)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a random weight in Kg.", "response": "def weight(self, minimum: int = 38, maximum: int = 90) -> int:\n        \"\"\"Generate a random weight in Kg.\n\n        :param minimum: min value\n        :param maximum: max value\n        :return: Weight.\n\n        :Example:\n            48.\n        \"\"\"\n        weight = self.random.randint(minimum, maximum)\n        return weight"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sexual_orientation(self, symbol: bool = False) -> str:\n        if symbol:\n            return self.random.choice(SEXUALITY_SYMBOLS)\n\n        sexuality = self._data['sexuality']\n        return self.random.choice(sexuality)", "response": "Get a random sexual orientation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a random job.", "response": "def occupation(self) -> str:\n        \"\"\"Get a random job.\n\n        :return: The name of job.\n\n        :Example:\n            Programmer.\n        \"\"\"\n        jobs = self._data['occupation']\n        return self.random.choice(jobs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a random political views.", "response": "def political_views(self) -> str:\n        \"\"\"Get a random political views.\n\n        :return: Political views.\n\n        :Example:\n            Liberal.\n        \"\"\"\n        views = self._data['political_views']\n        return self.random.choice(views)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef worldview(self) -> str:\n        views = self._data['worldview']\n        return self.random.choice(views)", "response": "Get a random worldview."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a random views on.", "response": "def views_on(self) -> str:\n        \"\"\"Get a random views on.\n\n        :return: Views on.\n\n        :Example:\n            Negative.\n        \"\"\"\n        views = self._data['views_on']\n        return self.random.choice(views)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a random nationality.", "response": "def nationality(self, gender: Optional[Gender] = None) -> str:\n        \"\"\"Get a random nationality.\n\n        :param gender: Gender.\n        :return: Nationality.\n\n        :Example:\n            Russian\n        \"\"\"\n        nationalities = self._data['nationality']\n\n        # Separated by gender\n        if isinstance(nationalities, dict):\n            key = self._validate_enum(gender, Gender)\n            nationalities = nationalities[key]\n\n        return self.random.choice(nationalities)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a random university name.", "response": "def university(self) -> str:\n        \"\"\"Get a random university.\n\n        :return: University name.\n\n        :Example:\n            MIT.\n        \"\"\"\n        universities = self._data['university']\n        return self.random.choice(universities)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef academic_degree(self) -> str:\n        degrees = self._data['academic_degree']\n        return self.random.choice(degrees)", "response": "Get a random academic degree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a random language.", "response": "def language(self) -> str:\n        \"\"\"Get a random language.\n\n        :return: Random language.\n\n        :Example:\n            Irish.\n        \"\"\"\n        languages = self._data['language']\n        return self.random.choice(languages)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a random phone number.", "response": "def telephone(self, mask: str = '', placeholder: str = '#') -> str:\n        \"\"\"Generate a random phone number.\n\n        :param mask: Mask for formatting number.\n        :param placeholder: A placeholder for a mask (default is #).\n        :return: Phone number.\n\n        :Example:\n            +7-(963)-409-11-22.\n        \"\"\"\n        if not mask:\n            code = self.random.choice(CALLING_CODES)\n            default = '{}-(###)-###-####'.format(code)\n            masks = self._data.get('telephone_fmt', [default])\n            mask = self.random.choice(masks)\n\n        return self.random.custom_code(mask=mask, digit=placeholder)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef avatar(self, size: int = 256) -> str:\n        url = 'https://api.adorable.io/avatars/{0}/{1}.png'\n        return url.format(size, self.password(hashed=True))", "response": "Generate a random avatar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef identifier(self, mask: str = '##-##/##') -> str:\n        return self.random.custom_code(mask=mask)", "response": "Generate a random identifier by mask."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef custom_size(self, minimum: int = 40, maximum: int = 62) -> int:\n        return self.random.randint(minimum, maximum)", "response": "Generate clothing size using custom format."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates a checksum for a number using the Luhn algorithm.", "response": "def luhn_checksum(num: str) -> str:\n    \"\"\"Calculate a checksum for num using the Luhn algorithm.\n\n    :param num: The number to calculate a checksum for as a string.\n    :return: Checksum for number.\n    \"\"\"\n    check = 0\n    for i, s in enumerate(reversed(num)):\n        sx = int(s)\n        sx = sx * 2 if i % 2 == 0 else sx\n        sx = sx - 9 if sx > 9 else sx\n        check += sx\n    return str(check * 9 % 10)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading image and save in local machine.", "response": "def download_image(url: str = '', save_path: str = '',\n                   unverified_ctx: bool = False) -> Union[None, str]:\n    \"\"\"Download image and save in current directory on local machine.\n\n    :param url: URL to image.\n    :param save_path: Saving path.\n    :param unverified_ctx: Create unverified context.\n    :return: Path to downloaded image.\n    :rtype: str or None\n    \"\"\"\n    if unverified_ctx:\n        ssl._create_default_https_context = ssl._create_unverified_context\n\n    if url:\n        image_name = url.rsplit('/')[-1]\n\n        splitted_name = image_name.rsplit('.')\n        if len(splitted_name) < 2:\n            image_name = '{}.jpg'.format(uuid4())\n        else:\n            image_name = '{}.{}'.format(uuid4(), splitted_name[-1])\n        full_image_path = path.join(save_path, image_name)\n        request.urlretrieve(url, full_image_path)\n        return full_image_path\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating random valid 10 - digit NIP.", "response": "def nip(self) -> str:\n        \"\"\"Generate random valid 10-digit NIP.\n\n        :return: Valid 10-digit NIP\n        \"\"\"\n        nip_digits = [int(d) for d in str(self.random.randint(101, 998))]\n        nip_digits += [self.random.randint(0, 9) for _ in range(6)]\n        nip_coefficients = (6, 5, 7, 2, 3, 4, 5, 6, 7)\n        sum_v = sum([nc * nd for nc, nd in\n                     zip(nip_coefficients, nip_digits)])\n\n        checksum_digit = sum_v % 11\n        if checksum_digit > 9:\n            return self.nip()\n        nip_digits.append(checksum_digit)\n        return ''.join(str(d) for d in nip_digits)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pesel(self, birth_date: DateTime = None,\n              gender: Gender = None) -> str:\n        \"\"\"Generate random 11-digit PESEL.\n\n        :param birth_date: Initial birth date (optional)\n        :param gender: Gender of person\n        :return: Valid 11-digit PESEL\n        \"\"\"\n        date_object = birth_date\n        if not date_object:\n            date_object = Datetime().datetime(1940, 2018)\n\n        year = date_object.date().year\n        month = date_object.date().month\n        day = date_object.date().day\n        pesel_digits = [int(d) for d in str(year)][-2:]\n\n        if 1800 <= year <= 1899:\n            month += 80\n        elif 2000 <= year <= 2099:\n            month += 20\n        elif 2100 <= year <= 2199:\n            month += 40\n        elif 2200 <= year <= 2299:\n            month += 60\n\n        pesel_digits += [int(d) for d in '{:02d}'.format(month)]\n        pesel_digits += [int(d) for d in '{:02d}'.format(day)]\n        series_number = self.random.randint(0, 999)\n        pesel_digits += [int(d) for d in '{:03d}'.format(series_number)]\n\n        if gender == Gender.MALE:\n            gender_digit = self.random.choice((1, 3, 5, 7, 9))\n        elif gender == Gender.FEMALE:\n            gender_digit = self.random.choice((0, 2, 4, 6, 8))\n        else:\n            gender_digit = self.random.choice(range(10))\n\n        pesel_digits.append(gender_digit)\n        pesel_coeffs = (9, 7, 3, 1, 9, 7, 3, 1, 9, 7)\n        sum_v = sum([nc * nd for nc, nd in\n                     zip(pesel_coeffs, pesel_digits)])\n        checksum_digit = sum_v % 10\n        pesel_digits.append(checksum_digit)\n        return ''.join(str(d) for d in pesel_digits)", "response": "Generate a random 11 - digit PESEL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef regon(self) -> str:\n        regon_coeffs = (8, 9, 2, 3, 4, 5, 6, 7)\n        regon_digits = [self.random.randint(0, 9) for _ in range(8)]\n        sum_v = sum([nc * nd for nc, nd in\n                     zip(regon_coeffs, regon_digits)])\n        checksum_digit = sum_v % 11\n        if checksum_digit > 9:\n            checksum_digit = 0\n        regon_digits.append(checksum_digit)\n        return ''.join(str(d) for d in regon_digits)", "response": "Generate random valid 9 - digit REGON."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a random tracking number.", "response": "def tracking_number(self, service: str = 'usps') -> str:\n        \"\"\"Generate random tracking number.\n\n        Supported services: USPS, FedEx and UPS.\n\n        :param str service: Post service.\n        :return: Tracking number.\n        \"\"\"\n        service = service.lower()\n\n        if service not in ('usps', 'fedex', 'ups'):\n            raise ValueError('Unsupported post service')\n\n        services = {\n            'usps': (\n                '#### #### #### #### ####',\n                '@@ ### ### ### US',\n            ),\n            'fedex': (\n                '#### #### ####',\n                '#### #### #### ###',\n            ),\n            'ups': (\n                '1Z@####@##########',\n            ),\n        }\n        mask = self.random.choice(services[service])  # type: ignore\n        return self.random.custom_code(mask=mask)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a random but valid SSN.", "response": "def ssn(self) -> str:\n        \"\"\"Generate a random, but valid SSN.\n\n        :returns: SSN.\n\n        :Example:\n            569-66-5801\n        \"\"\"\n        area = self.random.randint(1, 899)\n        if area == 666:\n            area = 665\n\n        return '{:03}-{:02}-{:04}'.format(\n            area,\n            self.random.randint(1, 99),\n            self.random.randint(1, 9999),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef personality(self, category: str = 'mbti') -> Union[str, int]:\n        mbtis = ('ISFJ', 'ISTJ', 'INFJ', 'INTJ',\n                 'ISTP', 'ISFP', 'INFP', 'INTP',\n                 'ESTP', 'ESFP', 'ENFP', 'ENTP',\n                 'ESTJ', 'ESFJ', 'ENFJ', 'ENTJ')\n\n        if category.lower() == 'rheti':\n            return self.random.randint(1, 10)\n\n        return self.random.choice(mbtis)", "response": "Generate a type of personality."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unit(self, name: Optional[UnitName] = None, symbol=False):\n        result = self._validate_enum(item=name, enum=UnitName)\n\n        if symbol:\n            return result[1]\n        return result[0]", "response": "Get unit name.\n\n        :param name: Enum object UnitName.\n        :param symbol: Return only symbol\n        :return: Unit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prefix(self, sign: Optional[PrefixSign] = None,\n               symbol: bool = False) -> str:\n        \"\"\"Get a random prefix for the International System of Units.\n\n        :param sign: Sing of number.\n        :param symbol: Return symbol of prefix.\n        :return: Prefix for SI.\n        :raises NonEnumerableError: if sign is not supported.\n\n        :Example:\n            mega\n        \"\"\"\n        prefixes = SI_PREFIXES_SYM if \\\n            symbol else SI_PREFIXES\n\n        key = self._validate_enum(item=sign, enum=PrefixSign)\n        return self.random.choice(prefixes[key])", "response": "Get a random prefix for the International System of Units."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a random but valid BSN.", "response": "def bsn(self) -> str:\n        \"\"\"Generate a random, but valid ``Burgerservicenummer``.\n\n        :returns: Random BSN.\n\n        :Example:\n            255159705\n        \"\"\"\n        def _is_valid_bsn(number: str) -> bool:\n            total = 0\n            multiplier = 9\n\n            for char in number:\n                multiplier = -multiplier if multiplier == 1 else multiplier\n                total += int(char) * multiplier\n                multiplier -= 1\n\n            result = total % 11 == 0\n            return result\n\n        a, b = (100000000, 999999999)\n        sample = str(self.random.randint(a, b))\n\n        while not _is_valid_bsn(sample):\n            sample = str(self.random.randint(a, b))\n\n        return sample"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a custom provider to Generic object.", "response": "def add_provider(self, cls: Type[BaseProvider]) -> None:\n        \"\"\"Add a custom provider to Generic() object.\n\n        :param cls: Custom provider.\n        :return: None\n        :raises TypeError: if cls is not class.\n        \"\"\"\n        if inspect.isclass(cls):\n            if not issubclass(cls, BaseProvider):\n                raise TypeError('The provider must be a '\n                                'subclass of BaseProvider')\n            try:\n                meta = getattr(cls, 'Meta')\n                name = getattr(meta, 'name')\n            except AttributeError:\n                name = cls.__name__.lower()\n            setattr(self, name, cls(seed=self.seed))\n        else:\n            raise TypeError('The provider must be a class')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_providers(self, *providers: Type[BaseProvider]) -> None:\n        for provider in providers:\n            self.add_provider(provider)", "response": "Add a lot of custom providers to Generic object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, iterations: int = 1) -> List[JSON]:\n        return [self.schema() for _ in range(iterations)]", "response": "Return filled schema.\n\n        Create a list of a filled schemas with elements in\n        an amount of **iterations**.\n\n        :param iterations: Amount of iterations.\n        :return: List of willed schemas."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate the enum parameter of method in subclasses of BaseProvider.", "response": "def _validate_enum(self, item: Any, enum: Any) -> Any:\n        \"\"\"Validate enum parameter of method in subclasses of BaseProvider.\n\n        :param item: Item of enum object.\n        :param enum: Enum object.\n        :return: Value of item.\n        :raises NonEnumerableError: if ``item`` not in ``enum``.\n        \"\"\"\n        if item is None:\n            result = get_random_item(enum, self.random)\n        elif item and isinstance(item, enum):\n            result = item\n        else:\n            raise NonEnumerableError(enum)\n\n        return result.value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _setup_locale(self, locale: str = locales.DEFAULT_LOCALE) -> None:\n        if not locale:\n            locale = locales.DEFAULT_LOCALE\n\n        locale = locale.lower()\n        if locale not in locales.SUPPORTED_LOCALES:\n            raise UnsupportedLocale(locale)\n\n        self.locale = locale", "response": "Set up locale after pre - check."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npulls the content from the JSON and memorize one.", "response": "def pull(self, datafile: str = ''):\n        \"\"\"Pull the content from the JSON and memorize one.\n\n        Opens JSON file ``file`` in the folder ``data/locale``\n        and get content from the file and memorize ones using lru_cache.\n\n        :param datafile: The name of file.\n        :return: The content of the file.\n        :raises UnsupportedLocale: if locale is not supported.\n        \"\"\"\n        locale = self.locale\n        data_dir = self._data_dir\n\n        if not datafile:\n            datafile = self._datafile\n\n        def get_data(locale_name: str) -> JSON:\n            \"\"\"Pull JSON data from file.\n\n            :param locale_name: Locale name.\n            :return: Content of JSON file as dict.\n            \"\"\"\n            file_path = Path(data_dir).joinpath(locale_name, datafile)\n            with open(file_path, 'r', encoding='utf8') as f:\n                return json.load(f)\n\n        separator = locales.LOCALE_SEPARATOR\n\n        master_locale = locale.split(separator).pop(0)\n        data = get_data(master_locale)\n\n        if separator in locale:\n            data = self._update_dict(data, get_data(locale))\n\n        self._data = data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides current locale with passed and pull data for new locale.", "response": "def _override_locale(self, locale: str = locales.DEFAULT_LOCALE) -> None:\n        \"\"\"Overrides current locale with passed and pull data for new locale.\n\n        :param locale: Locale\n        :return: Nothing.\n        \"\"\"\n        self.locale = locale\n        self.pull.cache_clear()\n        self.pull()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef override_locale(self, locale: str = locales.EN,\n                        ) -> Generator['BaseDataProvider', None, None]:\n        \"\"\"Context manager which allows overriding current locale.\n\n        Temporarily overrides current locale for\n        locale-dependent providers.\n\n        :param locale: Locale.\n        :return: Provider with overridden locale.\n        \"\"\"\n        try:\n            origin_locale = self.locale\n            self._override_locale(locale)\n            try:\n                yield self\n            finally:\n                self._override_locale(origin_locale)\n        except AttributeError:\n            raise ValueError('\u00ab{}\u00bb has not locale dependent'.format(\n                self.__class__.__name__))", "response": "Context manager which allows overriding current locale for overriding current locale for locale - dependent providers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert decimal number to DMS format.", "response": "def _dd_to_dms(num: float, _type: str) -> str:\n        \"\"\"Convert decimal number to DMS format.\n\n        :param num: Decimal number.\n        :param _type: Type of number.\n        :return: Number in DMS format.\n        \"\"\"\n        degrees = int(num)\n        minutes = int((num - degrees) * 60)\n        seconds = (num - degrees - minutes / 60) * 3600.00\n        seconds = round(seconds, 3)\n        result = [abs(i) for i in (degrees, minutes, seconds)]\n\n        direction = ''\n        if _type == 'lg':\n            direction = 'W' if degrees < 0 else 'E'\n        elif _type == 'lt':\n            direction = 'S' if degrees < 0 else 'N'\n\n        return ('{}\u00ba{}\\'{:.3f}\"' + direction).format(*result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef street_number(self, maximum: int = 1400) -> str:\n        return str(self.random.randint(1, maximum))", "response": "Generate a random street number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef address(self) -> str:\n        fmt = self._data['address_fmt']\n\n        st_num = self.street_number()\n        st_name = self.street_name()\n\n        if self.locale in SHORTENED_ADDRESS_FMT:\n            return fmt.format(\n                st_num=st_num,\n                st_name=st_name,\n            )\n\n        if self.locale == 'ja':\n            return fmt.format(\n                self.random.choice(self._data['city']),\n                # Generate list of random integers\n                # in amount of 3, from 1 to 100.\n                *self.random.randints(amount=3, a=1, b=100),\n            )\n\n        return fmt.format(\n            st_num=st_num,\n            st_name=st_name,\n            st_sfx=self.street_suffix(),\n\n        )", "response": "Generate a random full address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef state(self, abbr: bool = False) -> str:\n        return self.random.choice(\n            self._data['state']['abbr' if abbr else 'name'])", "response": "Get a random administrative district of country."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a random country code in the given format.", "response": "def country_code(self, fmt: Optional[CountryCode] = CountryCode.A2) -> str:\n        \"\"\"Get a random code of country.\n\n        Default format is :attr:`~enums.CountryCode.A2` (ISO 3166-1-alpha2),\n        you can change it by passing parameter ``fmt`` with enum object\n        :class:`~enums.CountryCode`.\n\n        :param fmt: Enum object CountryCode.\n        :return: Country code in selected format.\n        :raises KeyError: if fmt is not supported.\n        \"\"\"\n        key = self._validate_enum(fmt, CountryCode)\n        return self.random.choice(COUNTRY_CODES[key])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a random float number.", "response": "def _get_fs(self, key: str, dms: bool = False) -> Union[str, float]:\n        \"\"\"Get float number.\n\n        :param key: Key (`lt` or `lg`).\n        :param dms: DMS format.\n        :return: Float number\n        \"\"\"\n        # Default range is a range of longitude.\n        rng = (-90, 90) if key == 'lt' else (-180, 180)\n        result = self.random.uniform(*rng, precision=6)\n\n        if dms:\n            return self._dd_to_dms(result, key)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a random value of latitude.", "response": "def latitude(self, dms: bool = False) -> Union[str, float]:\n        \"\"\"Generate a random value of latitude.\n\n        :param dms: DMS format.\n        :return: Value of longitude.\n        \"\"\"\n        return self._get_fs('lt', dms)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef coordinates(self, dms: bool = False) -> dict:\n        return {\n            'longitude': self._get_fs('lg', dms),\n            'latitude': self._get_fs('lt', dms),\n        }", "response": "Generate random geo coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef continent(self, code: bool = False) -> str:\n        codes = CONTINENT_CODES if \\\n            code else self._data['continent']\n\n        return self.random.choice(codes)", "response": "Get a random continent name or continent code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef company_type(self, abbr: bool = False) -> str:\n        key = 'abbr' if abbr else 'title'\n        return self.random.choice(\n            self._data['company']['type'][key],\n        )", "response": "Get a random type of business entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copyright(self) -> str:  # noqa: A003\n        return '\u00a9 {}, {}'.format(\n            self.company(),\n            self.company_type(abbr=True),\n        )", "response": "Generate a random copyright."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef currency_iso_code(self, allow_random: bool = False) -> str:\n        if allow_random:\n            return self.random.choice(CURRENCY_ISO_CODES)\n        else:\n            return self._data['currency-code']", "response": "Get ISO code for current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef price(self, minimum: float = 10.00,\n              maximum: float = 1000.00) -> str:\n        \"\"\"Generate a random price.\n\n        :param minimum: Max value of price.\n        :param maximum: Min value of price.\n        :return: Price.\n        \"\"\"\n        price = self.random.uniform(minimum, maximum, precision=2)\n        return '{0} {1}'.format(price, self.currency_symbol())", "response": "Generate a random price."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef price_in_btc(self, minimum: float = 0, maximum: float = 2) -> str:\n        return '{} BTC'.format(\n            self.random.uniform(\n                minimum,\n                maximum,\n                precision=7,\n            ),\n        )", "response": "Generate random price in BTC."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a random fiscal code.", "response": "def fiscal_code(self, gender: Optional[Gender] = None) -> str:\n        \"\"\"Return a random fiscal code.\n\n        :param gender: Gender's enum object.\n        :return: Fiscal code.\n\n        Example:\n            RSSMRA66R05D612U\n        \"\"\"\n        code = ''.join(self.random.choices(string.ascii_uppercase, k=6))\n\n        code += self.random.custom_code(mask='##')\n\n        month_codes = self._data['fiscal_code']['month_codes']\n        code += self.random.choice(month_codes)\n\n        birth_day = self.random.randint(101, 131)\n        self._validate_enum(gender, Gender)\n        if gender == Gender.FEMALE:\n            birth_day += 40\n        code += str(birth_day)[1:]\n\n        city_letters = self._data['fiscal_code']['city_letters']\n        code += self.random.choice(city_letters)\n        code += self.random.custom_code(mask='###@')\n\n        return code"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_random_item(enum: Any, rnd: Optional[Random] = None) -> Any:\n    if rnd and isinstance(rnd, Random):\n        return rnd.choice(list(enum))\n    return random_module.choice(list(enum))", "response": "Get random item of enum object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a list of random integers.", "response": "def randints(self, amount: int = 3,\n                 a: int = 1, b: int = 100) -> List[int]:\n        \"\"\"Generate list of random integers.\n\n        :param amount: Amount of elements.\n        :param a: Minimum value of range.\n        :param b: Maximum value of range.\n        :return: List of random integers.\n        :raises ValueError: if amount less or equal to zero.\n        \"\"\"\n        if amount <= 0:\n            raise ValueError('Amount out of range.')\n\n        return [int(self.random() * (b - a)) + a\n                for _ in range(amount)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef urandom(*args: Any, **kwargs: Any) -> bytes:\n        return os.urandom(*args, **kwargs)", "response": "Return a bytes object containing random bytes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef schoice(self, seq: str, end: int = 10) -> str:\n        return ''.join(self.choice(list(seq))\n                       for _ in range(end))", "response": "Choice function which returns string created from sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef custom_code(self, mask: str = '@###',\n                    char: str = '@', digit: str = '#') -> str:\n        \"\"\"Generate custom code using ascii uppercase and random integers.\n\n        :param mask: Mask of code.\n        :param char: Placeholder for characters.\n        :param digit: Placeholder for digits.\n        :return: Custom code.\n        \"\"\"\n        char_code = ord(char)\n        digit_code = ord(digit)\n        code = bytearray(len(mask))\n\n        def random_int(a: int, b: int) -> int:\n            b = b - a\n            return int(self.random() * b) + a\n\n        _mask = mask.encode()\n        for i, p in enumerate(_mask):\n            if p == char_code:\n                a = random_int(65, 91)  # A-Z\n            elif p == digit_code:\n                a = random_int(48, 58)  # 0-9\n            else:\n                a = p\n            code[i] = a\n        return code.decode()", "response": "Generate custom code using ascii uppercase and random integers.\n\n        :param mask: Mask of code.\n        :param char: Placeholder for characters.\n        :param digit: Placeholder for digits.\n        :return: Custom code."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a random number in the range [ a b ) depending on rounding.", "response": "def uniform(self, a: float, b: float, precision: int = 15) -> float:\n        \"\"\"Get a random number in the range [a, b) or [a, b] depending on rounding.\n\n        :param a: Minimum value.\n        :param b: Maximum value.\n        :param precision: Round a number to a given\n            precision in decimal digits, default is 15.\n        \"\"\"\n        return round(a + (b - a) * self.random(), precision)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a random user.", "response": "def user(self) -> str:\n        \"\"\"Generate a random user.\n\n        :return: Path to user.\n\n        :Example:\n            /home/oretha\n        \"\"\"\n        user = self.random.choice(USERNAMES)\n        user = user.capitalize() if 'win' in self.platform else user.lower()\n        return str(self._pathlib_home / user)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef users_folder(self) -> str:\n        user = self.user()\n        folder = self.random.choice(FOLDERS)\n        return str(self._pathlib_home / user / folder)", "response": "Generate a random path to user s folders."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a random path to development directory.", "response": "def dev_dir(self) -> str:\n        \"\"\"Generate a random path to development directory.\n\n        :return: Path.\n\n        :Example:\n            /home/sherrell/Development/Python\n        \"\"\"\n        user = self.user()\n        folder = self.random.choice(['Development', 'Dev'])\n        stack = self.random.choice(PROGRAMMING_LANGS)\n        return str(self._pathlib_home / user / folder / stack)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef project_dir(self) -> str:\n        dev_dir = self.dev_dir()\n        project = self.random.choice(PROJECT_NAMES)\n        return str(self._pathlib_home / dev_dir / project)", "response": "Generate a random path to project directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __sub(self, string: str = '') -> str:\n        replacer = self.random.choice(['_', '-'])\n        return re.sub(r'\\s+', replacer, string.strip())", "response": "Replace spaces in string.\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extension(self, file_type: Optional[FileType] = None) -> str:\n        key = self._validate_enum(item=file_type, enum=FileType)\n        extensions = EXTENSIONS[key]\n        return self.random.choice(extensions)", "response": "Get a random file extension from list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a random mime type from list.", "response": "def mime_type(self, type_: Optional[MimeType] = None) -> str:\n        \"\"\"Get a random mime type from list.\n\n        :param type_: Enum object MimeType.\n        :return: Mime type.\n        \"\"\"\n        key = self._validate_enum(item=type_, enum=MimeType)\n        types = MIME_TYPES[key]\n        return self.random.choice(types)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting size of file.", "response": "def size(self, minimum: int = 1, maximum: int = 100) -> str:\n        \"\"\"Get size of file.\n\n        :param minimum: Maximum value.\n        :param maximum: Minimum value.\n        :return: Size of file.\n\n        :Example:\n            56 kB\n        \"\"\"\n        num = self.random.randint(minimum, maximum)\n        unit = self.random.choice(\n            ['bytes', 'kB', 'MB', 'GB', 'TB'])\n\n        return '{num} {unit}'.format(\n            num=num,\n            unit=unit,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a random file name with some extension.", "response": "def file_name(self, file_type: Optional[FileType] = None) -> str:\n        \"\"\"Get a random file name with some extension.\n\n        :param file_type: Enum object FileType\n        :return: File name.\n\n        :Example:\n            legislative.txt\n        \"\"\"\n        name = self.__text.word()\n        ext = self.extension(file_type)\n\n        return '{name}{ext}'.format(\n            name=self.__sub(name),\n            ext=ext,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a random noun in German.", "response": "def noun(self, plural: bool = False) -> str:\n        \"\"\"Return a random noun in German.\n\n        :param plural: Return noun in plural.\n        :return: Noun.\n        \"\"\"\n        key = 'plural' if \\\n            plural else 'noun'\n\n        return self.random.choice(self._data[key])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bitcoin_address(self) -> str:\n        type_ = self.random.choice(['1', '3'])\n        letters = string.ascii_letters + string.digits\n        return type_ + ''.join(\n            self.random.choice(letters) for _ in range(33))", "response": "Generate a random bitcoin address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a random Ethereum address.", "response": "def ethereum_address(self) -> str:\n        \"\"\"Generate a random Ethereum address.\n\n        .. Note: The address will look like Ethereum address,\n        but keep in mind that it is not the valid address.\n\n        :return: Ethereum address.\n\n        :Example:\n            0xe8ece9e6ff7dba52d4c07d37418036a89af9698d\n        \"\"\"\n        bits = self.random.getrandbits(160)\n        address = bits.to_bytes(20, byteorder='big')\n        return '0x' + address.hex()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a random credit card number.", "response": "def credit_card_number(self, card_type: Optional[CardType] = None) -> str:\n        \"\"\"Generate a random credit card number.\n\n        :param card_type: Issuing Network. Default is Visa.\n        :return: Credit card number.\n        :raises NotImplementedError: if cart_type is not supported.\n\n        :Example:\n            4455 5299 1152 2450\n        \"\"\"\n        length = 16\n        regex = re.compile(r'(\\d{4})(\\d{4})(\\d{4})(\\d{4})')\n\n        if card_type is None:\n            card_type = get_random_item(CardType, rnd=self.random)\n\n        if card_type == CardType.VISA:\n            number = self.random.randint(4000, 4999)\n        elif card_type == CardType.MASTER_CARD:\n            number = self.random.choice([\n                self.random.randint(2221, 2720),\n                self.random.randint(5100, 5599),\n            ])\n        elif card_type == CardType.AMERICAN_EXPRESS:\n            number = self.random.choice([34, 37])\n            length = 15\n            regex = re.compile(r'(\\d{4})(\\d{6})(\\d{5})')\n        else:\n            raise NonEnumerableError(CardType)\n\n        str_num = str(number)\n        while len(str_num) < length - 1:\n            str_num += self.random.choice(string.digits)\n\n        groups = regex.search(  # type: ignore\n            str_num + luhn_checksum(str_num),\n        ).groups()\n        card = ' '.join(groups)\n        return card"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a random expiration date for a credit card.", "response": "def credit_card_expiration_date(self, minimum: int = 16,\n                                    maximum: int = 25) -> str:\n        \"\"\"Generate a random expiration date for credit card.\n\n        :param minimum: Date of issue.\n        :param maximum: Maximum of expiration_date.\n        :return: Expiration date of credit card.\n\n        :Example:\n            03/19.\n        \"\"\"\n        month = self.random.randint(1, 12)\n        year = self.random.randint(minimum, maximum)\n        return '{0:02d}/{1}'.format(month, year)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate credit card owner.", "response": "def credit_card_owner(self, gender: Optional[Gender] = None) -> dict:\n        \"\"\"Generate credit card owner.\n\n        :param gender: Gender of credit card owner.\n        :type gender: Gender's enum object.\n        :return:\n        \"\"\"\n        owner = {\n            'credit_card': self.credit_card_number(),\n            'expiration_date': self.credit_card_expiration_date(),\n            'owner': self.__person.full_name(gender=gender).upper(),\n        }\n        return owner"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef alphabet(self, lower_case: bool = False) -> list:\n        case = 'uppercase' if \\\n            not lower_case else 'lowercase'\n\n        alpha = self._data['alphabet'].get(case)\n        return alpha", "response": "Get an alphabet for current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef level(self) -> str:\n        levels = self._data['level']\n        return self.random.choice(levels)", "response": "Generate a random level of danger or something else."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate the text. :param quantity: Quantity of sentences. :return: Text.", "response": "def text(self, quantity: int = 5) -> str:\n        \"\"\"Generate the text.\n\n        :param quantity: Quantity of sentences.\n        :return: Text.\n        \"\"\"\n        text = ''\n        for _ in range(quantity):\n            text += ' ' + self.random.choice(self._data['text'])\n        return text.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef words(self, quantity: int = 5) -> List[str]:\n        words = self._data['words'].get('normal')\n        words_list = [self.random.choice(words) for _ in range(quantity)]\n        return words_list", "response": "Generate lis of the random words."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a random swear word.", "response": "def swear_word(self) -> str:\n        \"\"\"Get a random swear word.\n\n        :return: Swear word.\n\n        :Example:\n            Damn.\n        \"\"\"\n        bad_words = self._data['words'].get('bad')\n        return self.random.choice(bad_words)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a random quote from movie.", "response": "def quote(self) -> str:\n        \"\"\"Get a random quote.\n\n        :return: Quote from movie.\n\n        :Example:\n            \"Bond... James Bond.\"\n        \"\"\"\n        quotes = self._data['quotes']\n        return self.random.choice(quotes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef color(self) -> str:\n        colors = self._data['color']\n        return self.random.choice(colors)", "response": "Get a random name of color."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _hex_to_rgb(color: str) -> Tuple[int, ...]:\n        if color.startswith('#'):\n            color = color.lstrip('#')\n        return tuple(int(color[i:i + 2], 16) for i in (0, 2, 4))", "response": "Convert hex color to RGB format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a random hex color.", "response": "def hex_color(self, safe: bool = False) -> str:\n        \"\"\"Generate a random hex color.\n\n        :param safe: Get safe Flat UI hex color.\n        :return: Hex color code.\n\n        :Example:\n            #d8346b\n        \"\"\"\n        if safe:\n            return self.random.choice(SAFE_COLORS)\n\n        return '#{:06x}'.format(\n            self.random.randint(0x000000, 0xffffff))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rgb_color(self, safe: bool = False) -> Tuple[int, ...]:\n        color = self.hex_color(safe)\n        return self._hex_to_rgb(color)", "response": "Generate a random rgb color tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a random answer in current language.", "response": "def answer(self) -> str:\n        \"\"\"Get a random answer in current language.\n\n        :return: An answer.\n\n        :Example:\n            No\n        \"\"\"\n        answers = self._data['answers']\n        return self.random.choice(answers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a random ISSN.", "response": "def issn(self, mask: str = '####-####') -> str:\n        \"\"\"Generate a random ISSN.\n\n        :param mask: Mask of ISSN.\n        :return: ISSN.\n        \"\"\"\n        return self.random.custom_code(mask=mask)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate ISBN for current locale.", "response": "def isbn(self, fmt: Optional[ISBNFormat] = None,\n             locale: str = 'en') -> str:\n        \"\"\"Generate ISBN for current locale.\n\n        To change ISBN format, pass parameter ``fmt`` with needed value of\n        the enum object :class:`~mimesis.enums.ISBNFormat`\n\n        :param fmt: ISBN format.\n        :param locale: Locale code.\n        :return: ISBN.\n        :raises NonEnumerableError: if fmt is not enum ISBNFormat.\n        \"\"\"\n        fmt_value = self._validate_enum(item=fmt, enum=ISBNFormat)\n        mask = ISBN_MASKS[fmt_value].format(\n            ISBN_GROUPS[locale])\n        return self.random.custom_code(mask)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating EAN. To change EAN format, pass parameter ``fmt`` with needed value of the enum object :class:`~mimesis.enums.EANFormat`. :param fmt: Format of EAN. :return: EAN. :raises NonEnumerableError: if fmt is not enum EANFormat.", "response": "def ean(self, fmt: Optional[EANFormat] = None) -> str:\n        \"\"\"Generate EAN.\n\n        To change EAN format, pass parameter ``fmt`` with needed value of\n        the enum object :class:`~mimesis.enums.EANFormat`.\n\n        :param fmt: Format of EAN.\n        :return: EAN.\n        :raises NonEnumerableError: if fmt is not enum EANFormat.\n        \"\"\"\n        key = self._validate_enum(\n            item=fmt,\n            enum=EANFormat,\n        )\n        mask = EAN_MASKS[key]\n        return self.random.custom_code(mask=mask)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a random IMEI.", "response": "def imei(self) -> str:\n        \"\"\"Generate a random IMEI.\n\n        :return: IMEI.\n        \"\"\"\n        num = self.random.choice(IMEI_TACS)\n        num = num + str(self.random.randint(100000, 999999))\n        return num + luhn_checksum(num)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pin(self, mask: str = '####') -> str:\n        return self.random.custom_code(mask=mask)", "response": "Generate a random PIN code."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the SAS Data Set currently exist and False otherwise.", "response": "def exist(self, table: str, libref: str = \"\") -> bool:\n        \"\"\"\n        Does the SAS data set currently exist\n\n        :param table: the name of the SAS Data Set\n        :param libref: the libref for the Data Set, defaults to WORK, or USER if assigned\n        :return: Boolean True it the Data Set exists and False if it does not\n        :rtype: bool\n        \"\"\"\n        return self._io.exist(table, libref)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new instance of the SASets class.", "response": "def sasets(self) -> 'SASets':\n        \"\"\"\n        This methods creates a SASets object which you can use to run various analytics.\n        See the sasets.py module.\n        :return: sasets object\n        \"\"\"\n        if not self._loaded_macros:\n            self._loadmacros()\n            self._loaded_macros = True\n        return SASets(self)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sasstat(self) -> 'SASstat':\n        if not self._loaded_macros:\n            self._loadmacros()\n            self._loaded_macros = True\n\n        return SASstat(self)", "response": "Returns a SASstat object which can be used to run various analytics."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sasml(self) -> 'SASml':\n        if not self._loaded_macros:\n            self._loadmacros()\n            self._loaded_macros = True\n\n        return SASml(self)", "response": "Returns a SASML object which can be used to run various analytics."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sasqc(self) -> 'SASqc':\n        if not self._loaded_macros:\n            self._loadmacros()\n            self._loaded_macros = True\n\n        return SASqc(self)", "response": "Returns a SASqc object which can be used to run various analytics."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a SASutil object which can be used to run various analytics.", "response": "def sasutil(self) -> 'SASutil':\n        \"\"\"\n        This methods creates a SASutil object which you can use to run various analytics.\n        See the sasutil.py module.\n\n        :return: sasutil object\n        \"\"\"\n        if not self._loaded_macros:\n            self._loadmacros()\n            self._loaded_macros = True\n\n        return SASutil(self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the SAS macros at the start of the session", "response": "def _loadmacros(self):\n        \"\"\"\n        Load the SAS macros at the start of the session\n\n        :return:\n        \"\"\"\n        macro_path = os.path.dirname(os.path.realpath(__file__))\n        fd = os.open(macro_path + '/' + 'libname_gen.sas', os.O_RDONLY)\n        code = b'options nosource;\\n'\n        code += os.read(fd, 32767)\n        code += b'\\noptions source;'\n\n        self._io._asubmit(code.decode(), results='text')\n        os.close(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_csv(self, file: str, table: str = '_csv', libref: str = '', results: str = '',\n                 opts: dict = None) -> 'SASdata':\n        \"\"\"\n        :param file: either the OS filesystem path of the file, or HTTP://... for a url accessible file\n        :param table: the name of the SAS Data Set to create\n        :param libref: the libref for the SAS Data Set being created. Defaults to WORK, or USER if assigned\n        :param results: format of results, SASsession.results is default, PANDAS, HTML or TEXT are the alternatives\n        :param opts: a dictionary containing any of the following Proc Import options(datarow, delimiter, getnames, guessingrows)\n        :return: SASdata object\n        \"\"\"\n        opts = opts if opts is not None else {}\n\n        if results == '':\n            results = self.results\n\n        self._io.read_csv(file, table, libref, self.nosub, opts)\n\n        if self.exist(table, libref):\n            return SASdata(self, libref, table, results)\n        else:\n            return None", "response": "Reads the SAS Data Set from a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef df2sd(self, df: 'pd.DataFrame', table: str = '_df', libref: str = '',\n              results: str = '', keep_outer_quotes: bool = False) -> 'SASdata':\n        \"\"\"\n        This is an alias for 'dataframe2sasdata'. Why type all that?\n\n        :param df: :class:`pandas.DataFrame` Pandas Data Frame to import to a SAS Data Set\n        :param table: the name of the SAS Data Set to create\n        :param libref: the libref for the SAS Data Set being created. Defaults to WORK, or USER if assigned\n        :param results: format of results, SASsession.results is default, PANDAS, HTML or TEXT are the alternatives\n        :param keep_outer_quotes: the defualt is for SAS to strip outer quotes from delimitted data. This lets you keep them\n        :return: SASdata object\n        \"\"\"\n        return self.dataframe2sasdata(df, table, libref, results, keep_outer_quotes)", "response": "Converts a Pandas Data Frame to a SAS Data Set"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dataframe2sasdata(self, df: 'pd.DataFrame', table: str = '_df', libref: str = '',\n                          results: str = '', keep_outer_quotes: bool = False) -> 'SASdata':\n        \"\"\"\n        This method imports a Pandas Data Frame to a SAS Data Set, returning the SASdata object for the new Data Set.\n\n        :param df: Pandas Data Frame to import to a SAS Data Set\n        :param table: the name of the SAS Data Set to create\n        :param libref: the libref for the SAS Data Set being created. Defaults to WORK, or USER if assigned\n        :param results: format of results, SASsession.results is default, PANDAS, HTML or TEXT are the alternatives\n        :param keep_outer_quotes: the defualt is for SAS to strip outer quotes from delimitted data. This lets you keep them\n        :return: SASdata object\n        \"\"\"\n        if libref != '':\n           if libref.upper() not in self.assigned_librefs():\n              print(\"The libref specified is not assigned in this SAS Session.\")\n              return None\n\n        if results == '':\n            results = self.results\n        if self.nosub:\n            print(\"too complicated to show the code, read the source :), sorry.\")\n            return None\n        else:\n            self._io.dataframe2sasdata(df, table, libref, keep_outer_quotes)\n\n        if self.exist(table, libref):\n            return SASdata(self, libref, table, results)\n        else:\n            return None", "response": "This method imports a Pandas Data Frame to a SAS Data Set and returns a SASdata object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _dsopts(self, dsopts):\n        opts = ''\n        fmat = ''\n        if len(dsopts):\n            for key in dsopts:\n                if len(str(dsopts[key])):\n                    if key == 'where':\n                        if isinstance(dsopts[key], str):\n                            opts += 'where=(' + dsopts[key] + ') '\n                        elif isinstance(dsopts[key], list):\n                            opts += 'where=(' + \" and \".join(dsopts[key]) + ') '\n                        else:\n                            raise TypeError(\"Bad key type. {} must be a str or list type\".format(key))\n\n                    elif key == 'drop':\n                        opts += 'drop='\n                        if isinstance(dsopts[key], list):\n                            for var in dsopts[key]:\n                                opts += var + ' '\n                        else:\n                            opts += dsopts[key] + ' '\n                    elif key == 'keep':\n                        opts += 'keep='\n                        if isinstance(dsopts[key], list):\n                            for var in dsopts[key]:\n                                opts += var + ' '\n                        else:\n                            opts += dsopts[key] + ' '\n                    elif key == 'obs':\n                        opts += 'obs=' + str(dsopts[key]) + ' '\n\n                    elif key == 'firstobs':\n                        opts += 'firstobs=' + str(dsopts[key]) + ' '\n\n                    elif key == 'format':\n                        if isinstance(dsopts[key], str):\n                            fmat = 'format ' + dsopts[key] + ';'\n                        elif isinstance(dsopts[key], dict):\n                            fmat = 'format '\n                            for k, v in dsopts[key].items():\n                                fmat += ' '.join((k, v)) + ' '\n                            fmat += ';'\n                        else:\n                            raise TypeError(\"Bad key type. {} must be a str or dict type\".format(key))\n\n            if len(opts):\n                opts = '(' + opts + ')'\n                if len(fmat) > 0:\n                    opts += ';\\n\\t' + fmat\n            elif len(fmat) > 0:\n                opts = ';\\n\\t' + fmat\n        return opts", "response": "This function returns the SAS data set options as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a string containing the options for the process Import command.", "response": "def _impopts(self, opts):\n        \"\"\"\n        :param opts: a dictionary containing any of the following Proc Import options(datarow, delimiter, getnames, guessingrows):\n\n            - datarow      is a number\n            - delimiter    is a character\n            - getnames     is a boolean\n            - guessingrows is a numbers or the string 'MAX'\n\n            .. code-block:: python\n\n                             {'datarow'     : 2\n                              'delimiter'   : ',''\n                              'getnames'    : True\n                              'guessingrows': 20\n                             }\n        :return: str\n        \"\"\"\n        optstr = ''\n\n        if len(opts):\n            for key in opts:\n                if len(str(opts[key])):\n                    if key == 'datarow':\n                        optstr += 'datarow=' + str(opts[key]) + ';'\n                    elif key == 'delimiter':\n                        optstr += 'delimiter='\n                        optstr += \"'\" + '%02x' % ord(opts[key].encode(self._io.sascfg.encoding)) + \"'x; \"\n                    elif key == 'getnames':\n                        optstr += 'getnames='\n                        if opts[key]:\n                            optstr += 'YES; '\n                        else:\n                            optstr += 'NO; '\n                    elif key == 'guessingrows':\n                        optstr += 'guessingrows='\n                        if opts[key] == 'MAX':\n                            optstr += 'MAX; '\n                        else:\n                            optstr += str(opts[key]) + '; '\n        return optstr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _expopts(self, opts):\n        optstr = ''\n\n        if len(opts):\n            for key in opts:\n                if len(str(opts[key])):\n                    if key == 'delimiter':\n                        optstr += 'delimiter='\n                        optstr += \"'\" + '%02x' % ord(opts[key].encode(self._io.sascfg.encoding)) + \"'x; \"\n                    elif key == 'putnames':\n                        optstr += 'putnames='\n                        if opts[key]:\n                            optstr += 'YES; '\n                        else:\n                            optstr += 'NO; '\n        return optstr", "response": "Return a string containing the options for the process export"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign a value to a macro varable", "response": "def symput(self, name, value):\n        \"\"\"\n        :param name:  name of the macro varable to set:\n        :param value: python variable to use for the value to assign to the macro variable:\n\n            - name    is a character\n            - value   is a variable that can be resolved to a string\n\n        \"\"\"\n        ll = self.submit(\"%let \" + name + \"=%NRBQUOTE(\" + str(value) + \");\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef symget(self, name):\n        ll = self.submit(\"%put \" + name + \"=&\" + name + \";\\n\")\n\n        l2 = ll['LOG'].rpartition(name + \"=\")\n        l2 = l2[2].partition(\"\\n\")\n        try:\n            var = int(l2[0])\n        except:\n            try:\n                var = float(l2[0])\n            except:\n                var = l2[0]\n\n        return var", "response": "get the name of the macro varable"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disconnect(self):\n        if self.sascfg.mode != 'IOM':\n            res = \"This method is only available with the IOM access method\"\n        else:\n            res = self._io.disconnect()\n        return res", "response": "Disconnects an IOM session to allow for reconnecting when switching networks\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef assigned_librefs(self):\n\n        code = \"\"\"\n        data _null_; retain libref; retain cobs 1; \n           set sashelp.vlibnam end=last;\n           if cobs EQ 1 then\n              put \"LIBREFSSTART\";\n           cobs = 2;\n           if libref NE libname then\n              put \"LIBREF=\" libname;\n           libref = libname;\n           if last then\n              put \"LIBREFSEND\";\n        run;\n        \"\"\"\n\n        if self.nosub:\n            print(code)\n            return None\n        else:\n           ll = self.submit(code, results='text')\n\n        librefs = []\n        log = ll['LOG'].rpartition('LIBREFSEND')[0].rpartition('LIBREFSSTART')\n                                                                                  \n        for i in range(log[2].count('LIBREF=')):                                    \n           log = log[2].partition('LIBREF=')[2].partition('\\n')                    \n           librefs.append(log[0].strip())                                                         \n                                                                                  \n        return librefs", "response": "This method returns the list of currently assigned librefs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dirlist(self, path):\n\n        code = \"\"\"\n        data _null_;\n         spd = '\"\"\" + path + \"\"\"';\n         rc  = filename('saspydir', spd);\n         did = dopen('saspydir');\n\n         if did > 0 then\n            do;\n               memcount = dnum(did);\n               put 'MEMCOUNT=' memcount;\n               do while (memcount > 0);\n                  name = dread(did, memcount);\n                  memcount = memcount - 1;\n\n                  qname = spd || '\"\"\"+self.hostsep+\"\"\"' || name;\n\n                  rc = filename('saspydq', qname);\n                  dq = dopen('saspydq');\n                  if dq NE 0 then\n                     do;\n                        dname = strip(name) || '\"\"\"+self.hostsep+\"\"\"';\n                        put 'DIR=' dname;\n                        rc = dclose(dq);\n                     end;\n                  else\n                     put 'FILE=' name;\n               end;\n\n            put 'MEMEND';\n            rc = dclose(did);\n            end;\n         else\n            do;\n               put 'MEMCOUNT=0';\n               put 'MEMEND';\n           end;\n\n         rc = filename('saspydq');\n         rc = filename('saspydir');\n        run;\n        \"\"\"\n\n        if self.nosub:\n            print(code)\n            return None\n        else:\n           ll = self.submit(code, results='text')\n\n        dirlist = []\n\n        l2 = ll['LOG'].rpartition(\"MEMCOUNT=\")[2].partition(\"\\n\")\n        memcount = int(l2[0])\n\n        l3 = l2[2].rpartition(\"MEMEND\")[0]\n\n        for row in l3.split(sep='\\n'):\n            i = row.partition('=')\n            if i[0] in ['FILE', 'DIR']:\n                dirlist.append(i[2])\n\n        if memcount != len(dirlist):\n            print(\"Some problem parsing list. Should be \" + str(memcount) + \" entries but got \" + str(\n                len(dirlist)) + \" instead.\")\n\n        return dirlist", "response": "This method returns a list of SAS files in the directory specified by path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hpforest(self, data: ['SASdata', str] = None,\n                 freq: str = None,\n                 id: str = None,\n                 input: [str, list, dict] = None,\n                 save: str = None,\n                 score: [str, bool, 'SASdata'] = True,\n                 target: [str, list, dict] = None,\n                 procopts: str = None,\n                 stmtpassthrough: str = None,\n                 **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the HPFOREST procedure\n\n        Documentation link:\n        https://support.sas.com/documentation/solutions/miner/emhp/14.1/emhpprcref.pdf\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm freq: The freq variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable. This parameter is required\n        :parm save: The save variable can only be a string type.\n        :parm score: The score variable can only be a string type.\n        :parm target: The target variable can be a string, list or dict type. It refers to the dependent, y, or label variable. This parameter is required\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the HPFOREST procedure in the SAS server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef submit(self, code: str, results: str =\"html\", prompt: dict = None) -> dict:\n      '''\n      This method is used to submit any SAS code. It returns the Log and Listing as a python dictionary.\n      code    - the SAS statements you want to execute\n      results - format of results, HTML is default, TEXT is the alternative\n      prompt  - dict of names:flags to prompt for; create macro variables (used in submitted code), then keep or delete\n                The keys are the names of the macro variables and the boolean flag is to either hide what you type and delete\n                the macros, or show what you type and keep the macros (they will still be available later)\n                for example (what you type for pw will not be displayed, user and dsname will):\n\n                results = sas.submit(\n                   \"\"\"\n                   libname tera teradata server=teracop1 user=&user pw=&pw;\n                   proc print data=tera.&dsname (obs=10); run;\n                   \"\"\" ,\n                   prompt = {'user': False, 'pw': True, 'dsname': False}\n                   )\n\n      Returns - a Dict containing two keys:values, [LOG, LST]. LOG is text and LST is 'results' (HTML or TEXT)\n\n      NOTE: to view HTML results in the ipykernel, issue: from IPython.display import HTML  and use HTML() instead of print()\n      i.e,: results = sas.submit(\"data a; x=1; run; proc print;run')\n            print(results['LOG'])\n            HTML(results['LST'])\n      '''\n      prompt = prompt if prompt is not None else {}\n\n      odsopen  = b\"ods listing close;ods \"+self.sascfg.output.encode()+ \\\n                 b\" (id=saspy_internal) file=stdout options(bitmap_mode='inline') device=svg style=\"+self._sb.HTML_Style.encode()+ \\\n                 b\"; ods graphics on / outputfmt=png;\\n\"\n      odsclose = b\"ods \"+self.sascfg.output.encode()+b\" (id=saspy_internal) close;ods listing;\\n\"\n      ods      = True;\n      mj       = b\";*\\';*\\\";*/;\"\n      lstf     = b''\n      logf     = b''\n      bail     = False\n      eof      = 5\n      bc       = False\n      done     = False\n      logn     = self._logcnt()\n      logcodei = \"%put E3969440A681A24088859985\" + logn + \";\"\n      logcodeo = b\"\\nE3969440A681A24088859985\" + logn.encode()\n      pcodei   = ''\n      pcodeiv  = ''\n      pcodeo   = ''\n\n      if self.pid == None:\n         self._sb.SASpid = None\n         print(\"No SAS process attached. SAS process has terminated unexpectedly.\")\n         return dict(LOG=\"No SAS process attached. SAS process has terminated unexpectedly.\", LST='')\n\n      rc = os.waitid(os.P_PID, self.pid, os.WEXITED | os.WNOHANG)\n      if rc != None:\n         self.pid = None\n         self._sb.SASpid = None\n         return dict(LOG='SAS process has terminated unexpectedly. Pid State= '+str(rc), LST='')\n\n      # to cover the possibility of an _asubmit w/ lst output not read; no known cases now; used to be __flushlst__()\n      # removing this and adding comment in _asubmit to use _getlst[txt] so this will never be necessary; delete later\n      #while(len(self.stdout.read1(4096)) > 0):\n      #   continue\n\n      if results.upper() != \"HTML\":\n         ods = False\n\n      if len(prompt):\n         pcodei += 'options nosource nonotes;\\n'\n         pcodeo += 'options nosource nonotes;\\n'\n         for key in prompt:\n            gotit = False\n            while not gotit:\n               var = self.sascfg._prompt('Please enter value for macro variable '+key+' ', pw=prompt[key])\n               if var is None:\n                  raise KeyboardInterrupt\n               if len(var) > 0:\n                  gotit = True\n               else:\n                  print(\"Sorry, didn't get a value for that variable.\")\n            if prompt[key]:\n               pcodei += '%let '+key+'='+var+';\\n'\n               pcodeo += '%symdel '+key+';\\n'\n            else:\n               pcodeiv += '%let '+key+'='+var+';\\n'\n         pcodei += 'options source notes;\\n'\n         pcodeo += 'options source notes;\\n'\n\n      if ods:\n         self.stdin.write(odsopen)\n\n      pgm  = mj+b'\\n'+pcodei.encode(self.sascfg.encoding)+pcodeiv.encode(self.sascfg.encoding)\n      pgm += code.encode(self.sascfg.encoding)+b'\\n'+pcodeo.encode(self.sascfg.encoding)+b'\\n'+mj\n      out  = self.stdin.write(pgm)\n\n      if ods:\n         self.stdin.write(odsclose)\n\n      out = self.stdin.write(b'\\n'+logcodei.encode(self.sascfg.encoding)+b'\\n')\n      self.stdin.flush()\n\n      while not done:\n         try:\n             while True:\n                 rc = os.waitid(os.P_PID, self.pid, os.WEXITED | os.WNOHANG)\n                 if rc is not None:\n                     log = b''\n                     try:\n                        log = self.stderr.read1(4096)\n                        if len(log) > 0:\n                            logf += log\n                        self._log += logf.decode(self.sascfg.encoding, errors='replace')\n                     except:\n                        pass\n                     self.pid = None\n                     self._sb.SASpid = None\n                     return dict(LOG='SAS process has terminated unexpectedly. Pid State= ' +\n                                 str(rc)+'\\n'+logf.decode(self.sascfg.encoding, errors='replace'), LST='')\n                 if bail:\n                     eof -= 1\n                 if eof < 0:\n                     break\n                 if ods:\n                    lst = self.stdout.read1(4096)\n                 else:\n                    lst = self.stdout.read1(4096)\n                 if len(lst) > 0:\n                     lstf += lst\n                 else:\n                     log = self.stderr.read1(4096)\n                     if len(log) > 0:\n                         logf += log\n                         if logf.count(logcodeo) >= 1:\n                             bail = True\n                         if not bail and bc:\n                             self.stdin.write(odsclose+logcodei.encode(self.sascfg.encoding) + b'\\n')\n                             self.stdin.flush()\n                             bc = False\n             done = True\n\n         except (ConnectionResetError):\n             log = ''\n             try:\n                log = self.stderr.read1(4096)\n                if len(log) > 0:\n                   logf += log\n                self._log += logf.decode(self.sascfg.encoding, errors='replace')\n             except:\n                pass\n             rc = 0\n             rc = os.waitpid(self.pid, 0)\n             self.pid = None\n             self._sb.SASpid = None\n             log = logf.partition(logcodeo)[0]+b'\\nConnection Reset: SAS process has terminated unexpectedly. Pid State= '+str(rc).encode()+b'\\n'+logf\n             return dict(LOG=log.encode(), LST='')\n\n         except (KeyboardInterrupt, SystemExit):\n             print('Exception caught!')\n             ll = self._breakprompt(logcodeo)\n\n             if ll.get('ABORT', False):\n                return ll\n\n             logf += ll['LOG']\n             lstf += ll['LST']\n             bc    = ll['BC']\n\n             if not bc:\n                print('Exception handled :)\\n')\n             else:\n                print('Exception ignored, continuing to process...\\n')\n\n             self.stdin.write(odsclose+logcodei.encode(self.sascfg.encoding)+b'\\n')\n             self.stdin.flush()\n\n      if ods:\n         try:\n            lstf = lstf.decode()\n         except UnicodeDecodeError:\n            try:\n               lstf = lstf.decode(self.sascfg.encoding)\n            except UnicodeDecodeError:\n               lstf = lstf.decode(errors='replace')\n      else:\n         lstf = lstf.decode(self.sascfg.encoding, errors='replace')\n\n      logf = logf.decode(self.sascfg.encoding, errors='replace')\n\n      trip = lstf.rpartition(\"/*]]>*/\")\n      if len(trip[1]) > 0 and len(trip[2]) < 100:\n         lstf = ''\n\n      self._log += logf\n      final = logf.partition(logcodei)\n      z = final[0].rpartition(chr(10))\n      prev = '%08d' %  (self._log_cnt - 1)\n      zz = z[0].rpartition(\"\\nE3969440A681A24088859985\" + prev +'\\n')\n      logd = zz[2].replace(mj.decode(self.sascfg.encoding), '')\n\n      lstd = lstf.replace(chr(12), chr(10)).replace('<body class=\"c body\">',\n                                                    '<body class=\"l body\">').replace(\"font-size: x-small;\",\n                                                                                     \"font-size:  normal;\")\n      return dict(LOG=logd, LST=lstd)", "response": "This method is used to submit a SAS code to the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the SAS Data Set exists and returns True if it does not exist and False otherwise.", "response": "def exist(self, table: str, libref: str =\"\") -> bool:\n      \"\"\"\n      table  - the name of the SAS Data Set\n      libref - the libref for the Data Set, defaults to WORK, or USER if assigned\n\n      Returns True it the Data Set exists and False if it does not\n      \"\"\"\n      code  = \"data _null_; e = exist('\"\n      if len(libref):\n         code += libref+\".\"\n      code += table+\"');\\n\"\n      code += \"v = exist('\"\n      if len(libref):\n         code += libref+\".\"\n      code += table+\"', 'VIEW');\\n if e or v then e = 1;\\n\"\n      code += \"te='TABLE_EXISTS='; put te e;run;\"\n\n      ll = self.submit(code, \"text\")\n\n      l2 = ll['LOG'].rpartition(\"TABLE_EXISTS= \")\n      l2 = l2[2].partition(\"\\n\")\n      exists = int(l2[0])\n\n      return bool(exists)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_csv(self, file: str, table: str, libref: str =\"\", nosub: bool =False, opts: dict = None) -> '<SASdata object>':\n      opts = opts if opts is not None else {}\n      code  = \"filename x \"\n\n      if file.lower().startswith(\"http\"):\n         code += \"url \"\n\n      code += \"\\\"\"+file+\"\\\";\\n\"\n      code += \"proc import datafile=x out=\"\n      if len(libref):\n         code += libref+\".\"\n\n      code += table+\" dbms=csv replace; \"+self._sb._impopts(opts)+\" run;\"\n\n      if nosub:\n         print(code)\n      else:\n         ll = self.submit(code, \"text\")", "response": "This method will import a CSV file into a SAS Data Set and return the SASdata object that represents it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upload_slow(self, localfile: str, remotefile: str, overwrite: bool = True, permission: str = '', **kwargs):\n      valid = self._sb.file_info(remotefile, quiet = True)\n\n      if valid is None:\n         remf = remotefile\n      else:\n         if valid == {}:\n            remf = remotefile + self._sb.hostsep + localfile.rpartition(os.sep)[2]\n         else:\n            remf = remotefile\n            if overwrite == False:\n               return {'Success' : False, \n                       'LOG'     : \"File \"+str(remotefile)+\" exists and overwrite was set to False. Upload was stopped.\"}\n\n      try:\n         fd = open(localfile, 'rb')\n      except OSError as e:\n         return {'Success' : False, \n                 'LOG'     : \"File \"+str(localfile)+\" could not be opened. Error was: \"+str(e)}\n\n      code = \"\"\"\n         filename saspydir '\"\"\"+remf+\"\"\"' recfm=F encoding=binary lrecl=1 permission='\"\"\"+permission+\"\"\"';\n         data _null_;\n         file saspydir; \n         infile datalines;\n         input;\n         lin = length(_infile_);\n         outdata = inputc(_infile_, '$hex.', lin);\n         lout = lin/2;\n         put outdata $varying80. lout; \n         datalines4;\"\"\"\n\n      buf = fd.read1(40)\n      if len(buf):\n         self._asubmit(code, \"text\")\n      else:\n         code = \"\"\"\n            filename saspydir '\"\"\"+remf+\"\"\"' recfm=F encoding=binary lrecl=1 permission='\"\"\"+permission+\"\"\"';\n            data _null_;\n            fid = fopen('saspydir', 'O');\n            if fid then\n               rc = fclose(fid);\n            run;\\n\"\"\"\n\n         ll = self.submit(code, 'text')\n         fd.close()\n         return {'Success' : True, \n                 'LOG'     : ll['LOG']}\n\n      while len(buf):\n         buf2 = ''\n         for i in range(len(buf)):\n            buf2 += '%02x' % buf[i]\n         self.stdin.write(buf2.encode()+b'\\n')\n         buf = fd.read1(40)\n\n      self._asubmit(\";;;;\", \"text\")\n      ll = self.submit(\"run;\\nfilename saspydir;\", 'text')\n      fd.close()\n\n      return {'Success' : True, \n              'LOG'     : ll['LOG']}", "response": "This method uploads a local file to the SAS server and returns a tuple of the status and the name of the uploaded file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download(self, localfile: str, remotefile: str, overwrite: bool = True, **kwargs):\n      valid = self._sb.file_info(remotefile, quiet = True)\n\n      if valid is None:\n         return {'Success' : False, \n                 'LOG'     : \"File \"+str(remotefile)+\" does not exist.\"}\n\n      if valid == {}:\n         return {'Success' : False, \n                 'LOG'     : \"File \"+str(remotefile)+\" is a directory.\"}\n\n      if os.path.isdir(localfile):\n         locf = localfile + os.sep + remotefile.rpartition(self._sb.hostsep)[2]\n      else:\n         locf = localfile\n\n      try:\n         fd = open(locf, 'wb')\n      except OSError as e:\n         return {'Success' : False, \n                 'LOG'     : \"File \"+str(locf)+\" could not be opened. Error was: \"+str(e)}\n\n      port =  kwargs.get('port', 0)\n\n      if port==0 and self.sascfg.tunnel:\n         # we are using a tunnel; default to that port\n         port = self.sascfg.tunnel\n\n      try:\n         sock = socks.socket()\n         if self.sascfg.tunnel:\n            sock.bind(('localhost', port))\n         else:\n            sock.bind(('', port))\n         port = sock.getsockname()[1]\n      except OSError:\n         return {'Success' : False, \n                 'LOG'     : \"Error try to open a socket in the download method. Call failed.\"}\n\n      if self.sascfg.ssh:\n         if not self.sascfg.tunnel:\n            host = self.sascfg.hostip #socks.gethostname()\n         else:\n            host = 'localhost'\n      else:\n         host = ''\n\n      code = \"\"\"\n         filename saspydir '\"\"\"+remotefile+\"\"\"' recfm=F encoding=binary lrecl=4096;\n         filename sock socket '\"\"\"+host+\"\"\":\"\"\"+str(port)+\"\"\"' recfm=S encoding=binary;\n         data _null_;\n         file sock;\n         infile saspydir;\n         input;\n         put _infile_;\n         run;\\n\"\"\"\n\n      sock.listen(1)\n      self._asubmit(code, 'text')\n\n      datar = b''\n\n      newsock = (0,0)\n      try:\n         newsock = sock.accept()\n         while True:\n            data = newsock[0].recv(4096)\n\n            if len(data):\n               datar += data\n            else:\n               if len(datar):\n                  fd.write(datar)\n               break\n            if len(datar) > 8300:\n               fd.write(datar[:8192])\n               datar = datar[8192:]\n      except:\n         if newsock[0]:\n            newsock[0].shutdown(socks.SHUT_RDWR)\n            newsock[0].close()\n         sock.close()\n         fd.close()\n         ll = self.submit(\"filename saspydir;\", 'text')\n         return {'Success' : False, \n                 'LOG'     : \"Download was interupted. Returning the SAS log:\\n\\n\"+ll['LOG']}\n\n      newsock[0].shutdown(socks.SHUT_RDWR)\n      newsock[0].close()\n      sock.close()\n\n      fd.flush()\n      fd.close()\n\n      ll = self.submit(\"filename saspydir;\", 'text')\n      return {'Success' : True, \n              'LOG'     : ll['LOG']}", "response": "This method downloads a remote file from the SAS servers file system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sasdata2dataframe(self, table: str, libref: str ='', dsopts: dict = None, rowsep: str = '\\x01', colsep: str = '\\x02', **kwargs) -> '<Pandas Data Frame object>':\n      dsopts = dsopts if dsopts is not None else {}\n\n      method = kwargs.pop('method', None)\n      if method and method.lower() == 'csv':\n         return self.sasdata2dataframeCSV(table, libref, dsopts, **kwargs)\n\n      port =  kwargs.get('port', 0)\n\n      if port==0 and self.sascfg.tunnel:\n         # we are using a tunnel; default to that port\n         port = self.sascfg.tunnel\n\n      datar = \"\"\n\n      if libref:\n         tabname = libref+\".\"+table\n      else:\n         tabname = table\n\n      code  = \"proc sql; create view sasdata2dataframe as select * from \"+tabname+self._sb._dsopts(dsopts)+\";quit;\\n\"\n      code += \"data _null_; file STDERR;d = open('sasdata2dataframe');\\n\"\n      code += \"lrecl = attrn(d, 'LRECL'); nvars = attrn(d, 'NVARS');\\n\"\n      code += \"lr='LRECL='; vn='VARNUMS='; vl='VARLIST='; vt='VARTYPE=';\\n\"\n      code += \"put lr lrecl; put vn nvars; put vl;\\n\"\n      code += \"do i = 1 to nvars; var = varname(d, i); put var; end;\\n\"\n      code += \"put vt;\\n\"\n      code += \"do i = 1 to nvars; var = vartype(d, i); put var; end;\\n\"\n      code += \"run;\"\n\n      ll = self.submit(code, \"text\")\n\n      l2 = ll['LOG'].rpartition(\"LRECL= \")\n      l2 = l2[2].partition(\"\\n\")\n      lrecl = int(l2[0])\n\n      l2 = l2[2].partition(\"VARNUMS= \")\n      l2 = l2[2].partition(\"\\n\")\n      nvars = int(l2[0])\n\n      l2 = l2[2].partition(\"\\n\")\n      varlist = l2[2].split(\"\\n\", nvars)\n      del varlist[nvars]\n\n      l2 = l2[2].partition(\"VARTYPE=\")\n      l2 = l2[2].partition(\"\\n\")\n      vartype = l2[2].split(\"\\n\", nvars)\n      del vartype[nvars]\n\n      topts             = dict(dsopts)\n      topts['obs']      = 0\n      topts['firstobs'] = ''\n      \n      code  = \"data work._n_u_l_l_;output;run;\\n\"\n      code += \"data _null_; set \"+tabname+self._sb._dsopts(topts)+\" work._n_u_l_l_;put 'FMT_CATS=';\\n\"\n\n      for i in range(nvars):\n         code += \"_tom = vformatn('\"+varlist[i]+\"'n);put _tom;\\n\"\n      code += \"run;\\nproc delete data=work._n_u_l_l_;run;\"\n      \n      ll = self.submit(code, \"text\")\n      \n      l2 = ll['LOG'].rpartition(\"FMT_CATS=\")\n      l2 = l2[2].partition(\"\\n\")\n      varcat = l2[2].split(\"\\n\", nvars)\n      del varcat[nvars]\n\n      try:\n         sock = socks.socket()\n         if self.sascfg.tunnel:\n            sock.bind(('localhost', port))\n         else:\n            sock.bind(('', port))\n         port = sock.getsockname()[1]\n      except OSError:\n         print('Error try to open a socket in the sasdata2dataframe method. Call failed.')\n         return None\n\n      if self.sascfg.ssh:\n         if not self.sascfg.tunnel:\n            host = self.sascfg.hostip #socks.gethostname()\n         else:\n            host = 'localhost'\n      else:\n         host = ''\n\n      rdelim = \"'\"+'%02x' % ord(rowsep.encode(self.sascfg.encoding))+\"'x\"\n      cdelim = \"'\"+'%02x' % ord(colsep.encode(self.sascfg.encoding))+\"'x \"\n\n      code  = \"\"\n      code += \"filename sock socket '\"+host+\":\"+str(port)+\"' lrecl=\"+str(self.sascfg.lrecl)+\" recfm=v termstr=LF;\\n\"\n      code += \" data _null_; set \"+tabname+self._sb._dsopts(dsopts)+\";\\n file sock dlm=\"+cdelim+\"; put \"\n      for i in range(nvars):\n         code += \"'\"+varlist[i]+\"'n \"\n         if vartype[i] == 'N':\n            if varcat[i] in self._sb.sas_date_fmts:\n               code += 'E8601DA10. '+cdelim\n            else:\n               if varcat[i] in self._sb.sas_time_fmts:\n                  code += 'E8601TM15.6 '+cdelim\n               else:\n                  if varcat[i] in self._sb.sas_datetime_fmts:\n                     code += 'E8601DT26.6 '+cdelim\n                  else:\n                     code += 'best32. '+cdelim\n         if not (i < (len(varlist)-1)):\n            code += rdelim\n      code += \"; run;\\n\"\n\n      sock.listen(1)\n      self._asubmit(code, 'text')\n\n      r     = []\n      df    = None\n      datar = b''\n      trows = kwargs.get('trows', None)\n      if not trows:\n         trows = 100000\n\n      newsock = (0,0)\n      try:\n         newsock = sock.accept()\n         while True:\n            data = newsock[0].recv(4096)\n\n            if len(data):\n               datar += data\n            else:\n               break\n\n            data  = datar.rpartition(colsep.encode()+rowsep.encode()+b'\\n')\n            datap = data[0]+data[1]\n            datar = data[2]\n\n            datap = datap.decode(self.sascfg.encoding, errors='replace')\n            for i in datap.split(sep=colsep+rowsep+'\\n'):\n               if i != '':\n                  r.append(tuple(i.split(sep=colsep)))\n\n            if len(r) > trows:\n               tdf = pd.DataFrame.from_records(r, columns=varlist)\n\n               for i in range(nvars):\n                  if vartype[i] == 'N':\n                     if varcat[i] not in self._sb.sas_date_fmts + self._sb.sas_time_fmts + self._sb.sas_datetime_fmts:\n                        if tdf.dtypes[tdf.columns[i]].kind not in ('f','u','i','b','B','c','?'):\n                           tdf[varlist[i]] = pd.to_numeric(tdf[varlist[i]], errors='coerce')\n                     else:\n                        if tdf.dtypes[tdf.columns[i]].kind not in ('M'):\n                           tdf[varlist[i]] = pd.to_datetime(tdf[varlist[i]], errors='coerce')\n                  else:\n                     tdf[varlist[i]].replace(' ', np.NaN, True)\n\n               if df is not None:\n                  df = df.append(tdf, ignore_index=True)\n               else:\n                  df = tdf\n               r = []\n      except:\n         print(\"sasdata2dataframe was interupted. Trying to return the saslog instead of a data frame.\")\n         if newsock[0]:\n            newsock[0].shutdown(socks.SHUT_RDWR)\n            newsock[0].close()\n         sock.close()\n         ll = self.submit(\"\", 'text')\n         return ll['LOG']\n\n      newsock[0].shutdown(socks.SHUT_RDWR)\n      newsock[0].close()\n      sock.close()\n\n      if len(r) > 0 or df is None:\n         tdf = pd.DataFrame.from_records(r, columns=varlist)\n\n         for i in range(nvars):\n            if vartype[i] == 'N':\n               if varcat[i] not in self._sb.sas_date_fmts + self._sb.sas_time_fmts + self._sb.sas_datetime_fmts:\n                  if tdf.dtypes[tdf.columns[i]].kind not in ('f','u','i','b','B','c','?'):\n                     tdf[varlist[i]] = pd.to_numeric(tdf[varlist[i]], errors='coerce')\n               else:\n                  if tdf.dtypes[tdf.columns[i]].kind not in ('M'):\n                     tdf[varlist[i]] = pd.to_datetime(tdf[varlist[i]], errors='coerce')\n            else:\n               tdf[varlist[i]].replace(' ', np.NaN, True)\n\n         if df is not None:\n            df = df.append(tdf, ignore_index=True)\n         else:\n            df = tdf\n\n      return df", "response": "This method exports the SAS Data Set to a Pandas Data Frame."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sasdata2dataframeCSV(self, table: str, libref: str ='', dsopts: dict = None, tempfile: str=None, tempkeep: bool=False, **kwargs) -> '<Pandas Data Frame object>':\n      dsopts = dsopts if dsopts is not None else {}\n\n      port =  kwargs.get('port', 0)\n\n      if port==0 and self.sascfg.tunnel:\n         # we are using a tunnel; default to that port\n         port = self.sascfg.tunnel\n\n      if libref:\n         tabname = libref+\".\"+table\n      else:\n         tabname = table\n\n      tmpdir  = None\n\n      if tempfile is None:\n         tmpdir = tf.TemporaryDirectory()\n         tmpcsv = tmpdir.name+os.sep+\"tomodsx\"\n      else:\n         tmpcsv  = tempfile\n\n      code  = \"proc sql; create view sasdata2dataframe as select * from \"+tabname+self._sb._dsopts(dsopts)+\";quit;\\n\"\n      code += \"data _null_; file STDERR;d = open('sasdata2dataframe');\\n\"\n      code += \"lrecl = attrn(d, 'LRECL'); nvars = attrn(d, 'NVARS');\\n\"\n      code += \"lr='LRECL='; vn='VARNUMS='; vl='VARLIST='; vt='VARTYPE=';\\n\"\n      code += \"put lr lrecl; put vn nvars; put vl;\\n\"\n      code += \"do i = 1 to nvars; var = varname(d, i); put var; end;\\n\"\n      code += \"put vt;\\n\"\n      code += \"do i = 1 to nvars; var = vartype(d, i); put var; end;\\n\"\n      code += \"run;\"\n\n      ll = self.submit(code, \"text\")\n\n      l2 = ll['LOG'].rpartition(\"LRECL= \")\n      l2 = l2[2].partition(\"\\n\")\n      lrecl = int(l2[0])\n\n      l2 = l2[2].partition(\"VARNUMS= \")\n      l2 = l2[2].partition(\"\\n\")\n      nvars = int(l2[0])\n\n      l2 = l2[2].partition(\"\\n\")\n      varlist = l2[2].split(\"\\n\", nvars)\n      del varlist[nvars]\n\n      l2 = l2[2].partition(\"VARTYPE=\")\n      l2 = l2[2].partition(\"\\n\")\n      vartype = l2[2].split(\"\\n\", nvars)\n      del vartype[nvars]\n\n      topts             = dict(dsopts)\n      topts['obs']      = 0\n      topts['firstobs'] = ''\n\n      code  = \"data work._n_u_l_l_;output;run;\\n\"\n      code += \"data _null_; set \"+tabname+self._sb._dsopts(topts)+\" work._n_u_l_l_;put 'FMT_CATS=';\\n\"\n\n      for i in range(nvars):\n         code += \"_tom = vformatn('\"+varlist[i]+\"'n);put _tom;\\n\"\n      code += \"run;\\nproc delete data=work._n_u_l_l_;run;\"\n\n      ll = self.submit(code, \"text\")\n\n      l2 = ll['LOG'].rpartition(\"FMT_CATS=\")\n      l2 = l2[2].partition(\"\\n\")\n      varcat = l2[2].split(\"\\n\", nvars)\n      del varcat[nvars]\n\n      if self.sascfg.ssh:\n         try:\n            sock = socks.socket()\n            if self.sascfg.tunnel:\n               sock.bind(('localhost', port))\n            else:\n               sock.bind(('', port))\n            port = sock.getsockname()[1]\n         except OSError:\n            print('Error try to open a socket in the sasdata2dataframe method. Call failed.')\n            return None\n\n         if not self.sascfg.tunnel:\n            host = self.sascfg.hostip  #socks.gethostname()\n         else:\n            host = 'localhost'\n         code  = \"filename sock socket '\"+host+\":\"+str(port)+\"' lrecl=\"+str(self.sascfg.lrecl)+\" recfm=v encoding='utf-8';\\n\"\n      else:\n         host = ''\n         code = \"filename sock '\"+tmpcsv+\"' lrecl=\"+str(self.sascfg.lrecl)+\" recfm=v encoding='utf-8';\\n\"\n\n      code += \"data sasdata2dataframe / view=sasdata2dataframe; set \"+tabname+self._sb._dsopts(dsopts)+\";\\nformat \"\n\n      for i in range(nvars):\n         if vartype[i] == 'N':\n            code += \"'\"+varlist[i]+\"'n \"\n            if varcat[i] in self._sb.sas_date_fmts:\n               code += 'E8601DA10. '\n            else:\n               if varcat[i] in self._sb.sas_time_fmts:\n                  code += 'E8601TM15.6 '\n               else:\n                  if varcat[i] in self._sb.sas_datetime_fmts:\n                     code += 'E8601DT26.6 '\n                  else:\n                     code += 'best32. '\n      code += \";\\n run;\\n\"\n      ll = self.submit(code, \"text\")\n\n      dts = kwargs.pop('dtype', '')\n      if dts == '':\n         dts = {}\n         for i in range(nvars):\n            if vartype[i] == 'N':\n               if varcat[i] not in self._sb.sas_date_fmts + self._sb.sas_time_fmts + self._sb.sas_datetime_fmts:\n                  dts[varlist[i]] = 'float'\n               else:\n                  dts[varlist[i]] = 'str'\n            else:\n               dts[varlist[i]] = 'str'\n\n      code  = ''\n      #code += \"options nosource;\\n\"\n      code  = \"proc export data=sasdata2dataframe outfile=sock dbms=csv replace; run\\n;\"\n      #code += \"options source;\\n\"\n\n      if self.sascfg.ssh:\n         csv = open(tmpcsv, mode='wb')\n         sock.listen(1)\n         self._asubmit(code, 'text')\n\n         newsock = (0,0)\n         try:\n            newsock = sock.accept()\n            while True:\n               data = newsock[0].recv(4096)\n\n               if not len(data):\n                  break\n\n               csv.write(data)\n         except:\n            print(\"sasdata2dataframe was interupted. Trying to return the saslog instead of a data frame.\")\n            if newsock[0]:\n               newsock[0].shutdown(socks.SHUT_RDWR)\n               newsock[0].close()\n            sock.close()\n            ll = self.submit(\"\", 'text')\n            return ll['LOG']\n\n         newsock[0].shutdown(socks.SHUT_RDWR)\n         newsock[0].close()\n         sock.close()\n         ll = self.submit(\"\", 'text')\n\n         csv.close()\n         df = pd.read_csv(tmpcsv, index_col=False, engine='c', dtype=dts, **kwargs)\n      else:\n         ll = self.submit(code, \"text\")\n         df = pd.read_csv(tmpcsv, index_col=False, engine='c', dtype=dts, **kwargs)\n\n      if tmpdir:\n         tmpdir.cleanup()\n      else:\n         if not tempkeep:\n            os.remove(tmpcsv)\n\n      for i in range(nvars):\n         if varcat[i] in self._sb.sas_date_fmts + self._sb.sas_time_fmts + self._sb.sas_datetime_fmts:\n            df[varlist[i]] = pd.to_datetime(df[varlist[i]], errors='coerce')\n\n      return df", "response": "This method exports the SAS Data Set to a Pandas Data Frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timeseries(self, data: ['SASdata', str] = None,\n                   by: str = None,\n                   corr: str = None,\n                   crosscorr: str = None,\n                   crossvar: str = None,\n                   decomp: str = None,\n                   id: str = None,\n                   out: [str, 'SASdata'] = None,\n                   season: str = None,\n                   trend: str = None,\n                   var: str = None,\n                   procopts: str = None,\n                   stmtpassthrough: str = None,\n                   **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the TIMESERIES procedure\n\n        Documentation link:\n        http://support.sas.com/documentation/cdl//en/etsug/68148/HTML/default/viewer.htm#etsug_timeseries_syntax.htm\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm by: The by variable can only be a string type.\n        :parm corr: The corr variable can only be a string type.\n        :parm crosscorr: The crosscorr variable can only be a string type.\n        :parm crossvar: The crossvar variable can only be a string type.\n        :parm decomp: The decomp variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm out: The out variable can be a string or SASdata type.\n        :parm season: The season variable can only be a string type.\n        :parm trend: The trend variable can only be a string type.\n        :parm var: The var variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method returns the SAS data object that represents the time series of the current SAS process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef arima(self, data: ['SASdata', str] = None,\n              by: str = None,\n              estimate: str = None,\n              forecast: str = None,\n              identify: str = None,\n              out: [str, 'SASdata'] = None,\n              outlier: str = None,\n              procopts: str = None,\n              stmtpassthrough: str = None,\n              **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the ARIMA procedure\n\n        Documentation link:\n        http://support.sas.com/documentation/cdl//en/etsug/68148/HTML/default/viewer.htm#etsug_arima_syntax.htm\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm by: The by variable can only be a string type.\n        :parm estimate: The estimate variable can only be a string type.\n        :parm forecast: The forecast variable can only be a string type.\n        :parm identify: The identify variable can only be a string type.\n        :parm out: The out variable can be a string or SASdata type.\n        :parm outlier: The outlier variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the ARIMA procedure to get the current ARIMA result for the current ARIMA process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef esm(self, data: ['SASdata', str] = None,\n            by: str = None,\n            forecast: str = None,\n            id: str = None,\n            out: [str, 'SASdata'] = None,\n            procopts: str = None,\n            stmtpassthrough: str = None,\n            **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the ESM procedure\n\n        Documentation link:\n        http://support.sas.com/documentation/cdl//en/etsug/68148/HTML/default/viewer.htm#etsug_esm_syntax.htm\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm by: The by variable can only be a string type.\n        :parm forecast: The forecast variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm out: The out variable can be a string or SASdata type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method returns the SAS results object for the specified set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef x11(self, data: ['SASdata', str] = None,\n            arima: str = None,\n            by: [str, list] = None,\n            id: [str, list] = None,\n            macurves: str = None,\n            monthly: str = None,\n            output: [str, bool, 'SASdata'] = None,\n            pdweights: str = None,\n            quarterly: str = None,\n            sspan: str = None,\n            tables: str = None,\n            var: str = None,\n            procopts: [str, list] = None,\n            stmtpassthrough: [str, list] = None,\n            **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the X11 procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=etsug&docsetTarget=etsug_x11_syntax.htm&locale=en\n\n        Either the MONTHLY or QUARTERLY statement must be specified, depending on the type of time series data you have.\n        The PDWEIGHTS and MACURVES statements can be used only with the MONTHLY statement. The TABLES statement controls\n        the printing of tables, while the OUTPUT statement controls the creation of the OUT= data set.\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm arima: The arima variable can only be a string type.\n        :parm by: The by variable can be a string or list type.\n        :parm id: The id variable can be a string or list type.\n        :parm macurves: The macurves variable can only be a string type.\n        :parm monthly: The monthly variable can only be a string type.\n        :parm output: The output variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm pdweights: The pdweights variable can only be a string type.\n        :parm quarterly: The quarterly variable can only be a string type.\n        :parm sspan: The sspan variable can only be a string type.\n        :parm tables: The tables variable can only be a string type.\n        :parm var: The var variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the X11 method of the SAS server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expand(self, data: ['SASdata', str] = None,\n               by: [str, list] = None,\n               convert: str = None,\n               id: [str, list] = None,\n               procopts: str = None,\n               stmtpassthrough: str = None,\n               **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the EXPAND procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=etsug&docsetTarget=etsug_expand_syntax.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm by: The by variable can be a string or list type.\n        :parm convert: The convert variable can only be a string type.\n        :parm id: The id variable can be a string or list type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method expands the SASdata object and returns a SASresult object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hpsplit(self, data: ['SASdata', str] = None,\n                cls: [str, list] = None,\n                code: str = None,\n                grow: str = None,\n                id: str = None,\n                input: [str, list, dict] = None,\n                model: str = None,\n                out: [str, bool, 'SASdata'] = None,\n                partition: str = None,\n                performance: str = None,\n                prune: str = None,\n                rules: str = None,\n                target: [str, list, dict] = None,\n                procopts: str = None,\n                stmtpassthrough: str = None,\n                **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the HPSPLIT procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=stathpug&docsetTarget=stathpug_hpsplit_syntax.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm cls: The cls variable can be a string or list type. It refers to the categorical, or nominal variables.\n        :parm code: The code variable can only be a string type.\n        :parm grow: The grow variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm model: The model variable can only be a string type.\n        :parm out: The out variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm partition: The partition variable can only be a string type.\n        :parm performance: The performance variable can only be a string type.\n        :parm prune: The prune variable can only be a string type.\n        :parm rules: The rules variable can only be a string type.\n        :parm target: The target variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to split the SAS data into two sets of variables. The function returns a dictionary containing the keys and values for each variable in the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mixed(self, data: ['SASdata', str] = None,\n              by: str = None,\n              cls: [str, list] = None,\n              code: str = None,\n              contrast: str = None,\n              estimate: str = None,\n              id: str = None,\n              lsmeans: str = None,\n              model: str = None,\n              out: [str, bool, 'SASdata'] = None,\n              random: str = None,\n              repeated: str = None,\n              slice: str = None,\n              weight: str = None,\n              procopts: str = None,\n              stmtpassthrough: str = None,\n              **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the MIXED procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=statug&docsetTarget=statug_mixed_syntax.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm by: The by variable can only be a string type.\n        :parm cls: The cls variable can be a string or list type. It refers to the categorical, or nominal variables.\n        :parm code: The code variable can only be a string type.\n        :parm contrast: The contrast variable can only be a string type.\n        :parm estimate: The estimate variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm lsmeans: The lsmeans variable can only be a string type.\n        :parm model: The model variable can only be a string type.\n        :parm out: The out variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm random: The random variable can only be a string type.\n        :parm repeated: The repeated variable can only be a string type.\n        :parm slice: The slice variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n\n        \"\"\"", "response": "This method is used to call the MIXED procedure on the SAS server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef glm(self, data: ['SASdata', str] = None,\n            absorb: str = None,\n            by: str = None,\n            cls: [str, list] = None,\n            contrast: str = None,\n            estimate: str = None,\n            freq: str = None,\n            id: str = None,\n            lsmeans: str = None,\n            manova: str = None,\n            means: str = None,\n            model: str = None,\n            out: [str, bool, 'SASdata'] = None,\n            random: str = None,\n            repeated: str = None,\n            test: str = None,\n            weight: str = None,\n            procopts: str = None,\n            stmtpassthrough: str = None,\n            **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the GLM procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=statug&docsetTarget=statug_glm_syntax.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm absorb: The absorb variable can only be a string type.\n        :parm by: The by variable can only be a string type.\n        :parm cls: The cls variable can be a string or list type. It refers to the categorical, or nominal variables.\n        :parm contrast: The contrast variable can only be a string type.\n        :parm estimate: The estimate variable can only be a string type.\n        :parm freq: The freq variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm lsmeans: The lsmeans variable can only be a string type.\n        :parm manova: The manova variable can only be a string type.\n        :parm means: The means variable can only be a string type.\n        :parm model: The model variable can only be a string type.\n        :parm out: The out variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm random: The random variable can only be a string type.\n        :parm repeated: The repeated variable can only be a string type.\n        :parm test: The test variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the GLM procedure in the base class of the base class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hplogistic(self, data: ['SASdata', str] = None,\n                   by: str = None,\n                   cls: [str, list] = None,\n                   code: str = None,\n                   freq: str = None,\n                   id: str = None,\n                   model: str = None,\n                   out: [str, bool, 'SASdata'] = None,\n                   partition: str = None,\n                   score: [str, bool, 'SASdata'] = True,\n                   selection: str = None,\n                   weight: str = None,\n                   procopts: str = None,\n                   stmtpassthrough: str = None,\n                   **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the HPLOGISTIC procedure\n\n        Documentation link.\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=stathpug&docsetTarget=stathpug_hplogistic_toc.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm by: The by variable can only be a string type.\n        :parm cls: The cls variable can be a string or list type. It refers to the categorical, or nominal variables.\n        :parm code: The code variable can only be a string type.\n        :parm freq: The freq variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm model: The model variable can only be a string type.\n        :parm out: The out variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm partition: The partition variable can only be a string type.\n        :parm score: The score variable can only be a string type.\n        :parm selection: The selection variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the HPLOGISTIC procedure in the SAS server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef phreg(self, data: ['SASdata', str] = None,\n              assess: str = None,\n              bayes: str = None,\n              by: str = None,\n              cls: [str, list] = None,\n              contrast: str = None,\n              effect: str = None,\n              estimate: str = None,\n              freq: str = None,\n              hazardratio: str = None,\n              id: str = None,\n              lsmeans: str = None,\n              lsmestimate: str = None,\n              model: str = None,\n              out: [str, bool, 'SASdata'] = None,\n              random: str = None,\n              roc: str = None,\n              slice: str = None,\n              store: str = None,\n              strata: str = None,\n              test: str = None,\n              weight: str = None,\n              procopts: str = None,\n              stmtpassthrough: str = None,\n              **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the PHREG procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=statug&docsetTarget=statug_phreg_syntax.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm assess: The assess variable can only be a string type.\n        :parm bayes: The bayes variable can only be a string type.\n        :parm by: The by variable can only be a string type.\n        :parm cls: The cls variable can be a string or list type. It refers to the categorical, or nominal variables.\n        :parm contrast: The contrast variable can only be a string type.\n        :parm effect: The effect variable can only be a string type.\n        :parm estimate: The estimate variable can only be a string type.\n        :parm freq: The freq variable can only be a string type.\n        :parm hazardratio: The hazardratio variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm lsmeans: The lsmeans variable can only be a string type.\n        :parm lsmestimate: The lsmestimate variable can only be a string type.\n        :parm model: The model variable can only be a string type.\n        :parm out: The out variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm random: The random variable can only be a string type.\n        :parm roc: The roc variable can only be a string type.\n        :parm slice: The slice variable can only be a string type.\n        :parm store: The store variable can only be a string type.\n        :parm strata: The strata variable can only be a string type.\n        :parm test: The test variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the PHREG procedure in the SAS server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef macontrol(self, data: ['SASdata', str] = None,\n                  ewmachart: str = None,\n                  machart: str = None,\n                  procopts: str = None,\n                  stmtpassthrough: str = None,\n                  **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the MACONTROL procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=qcug&docsetTarget=qcug_macontrol_toc.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm ewmachart: The ewmachart variable can only be a string type.\n        :parm machart: The machart variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the MACONTROL procedure on the SAS server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shewhart(self, data: ['SASdata', str] = None,\n                 boxchart: str = None,\n                 cchart: str = None,\n                 irchart: str = None,\n                 mchart: str = None,\n                 mrchart: str = None,\n                 npchart: str = None,\n                 pchart: str = None,\n                 rchart: str = None,\n                 schart: str = None,\n                 uchart: str = None,\n                 xrchart: str = None,\n                 xschart: str = None,\n                 procopts: str = None,\n                 stmtpassthrough: str = None,\n                 **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the SHEWHART procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=qcug&docsetTarget=qcug_shewhart_toc.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm boxchart: The boxchart variable can only be a string type.\n        :parm cchart: The cchart variable can only be a string type.\n        :parm irchart: The irchart variable can only be a string type.\n        :parm mchart: The mchart variable can only be a string type.\n        :parm mrchart: The mrchart variable can only be a string type.\n        :parm npchart: The npchart variable can only be a string type.\n        :parm pchart: The pchart variable can only be a string type.\n        :parm rchart: The rchart variable can only be a string type.\n        :parm schart: The schart variable can only be a string type.\n        :parm uchart: The uchart variable can only be a string type.\n        :parm xrchart: The xrchart variable can only be a string type.\n        :parm xschart: The xschart variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is called to generate the SHEWHART object for a specific set of items in the current cluster. The data parameter can only be a string or an array of SASdata objects. The data parameter can only be a string or an array of SASdata objects. The keyword arguments are passed to the function as keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_results(self, results: str):\n        if results.upper() == \"HTML\":\n            self.HTML = 1\n        else:\n            self.HTML = 0\n        self.results = results", "response": "This method sets the results attribute for the SASdata object. It stays in effect till changed\n        results - set the default result type for this SASdata object Pandas HTML or TEXT"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _returnPD(self, code, tablename, **kwargs):\n        libref = kwargs.get('libref','work')\n        ll = self.sas._io.submit(code)\n        check, errorMsg = self._checkLogForError(ll['LOG'])\n        if not check:\n            raise ValueError(\"Internal code execution failed: \" + errorMsg)\n        if isinstance(tablename, str):\n            pd = self.sas.sasdata2dataframe(tablename, libref)\n            self.sas._io.submit(\"proc delete data=%s.%s; run;\" % (libref, tablename))\n        elif isinstance(tablename, list):\n            pd = dict()\n            for t in tablename:\n                # strip leading '_' from names and capitalize for dictionary labels\n                if self.sas.exist(t, libref):\n                   pd[t.replace('_', '').capitalize()] = self.sas.sasdata2dataframe(t, libref)\n                self.sas._io.submit(\"proc delete data=%s.%s; run;\" % (libref, t))\n        else:\n            raise SyntaxError(\"The tablename must be a string or list %s was submitted\" % str(type(tablename)))\n\n        return pd", "response": "Private function to take a SAS code normally to create a table generate Pandas Data Frame and cleanup."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisplaying the first n rows of a table", "response": "def head(self, obs=5):\n        \"\"\"\n        display the first n rows of a table\n\n        :param obs: the number of rows of the table that you want to display. The default is 5\n        :return:\n        \"\"\"\n        topts = dict(self.dsopts)\n        topts['obs'] = obs\n        code = \"proc print data=\" + self.libref + '.' + self.table + self.sas._dsopts(topts) + \";run;\"\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        if self.results.upper() == 'PANDAS':\n            code = \"data _head ; set %s.%s %s; run;\" % (self.libref, self.table, self.sas._dsopts(topts))\n            return self._returnPD(code, '_head')\n        else:\n            ll = self._is_valid()\n            if self.HTML:\n                if not ll:\n                    ll = self.sas._io.submit(code)\n                if not self.sas.batch:\n                    self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n                else:\n                    return ll\n            else:\n                if not ll:\n                    ll = self.sas._io.submit(code, \"text\")\n                if not self.sas.batch:\n                    print(ll['LST'])\n                else:\n                    return ll"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tail(self, obs=5):\n        code = \"proc sql;select count(*) format best32. into :lastobs from \" + self.libref + '.' + self.table + self._dsopts() + \";%put lastobs=&lastobs tom;quit;\"\n\n        nosub = self.sas.nosub\n        self.sas.nosub = False\n\n        le = self._is_valid()\n        if not le:\n            ll = self.sas.submit(code, \"text\")\n\n            lastobs = ll['LOG'].rpartition(\"lastobs=\")\n            lastobs = lastobs[2].partition(\" tom\")\n            lastobs = int(lastobs[0])\n        else:\n            lastobs = obs\n\n        firstobs = lastobs - (obs - 1)\n        if firstobs < 1:\n            firstobs = 1\n\n        topts = dict(self.dsopts)\n        topts['obs'] = lastobs\n        topts['firstobs'] = firstobs\n\n        code = \"proc print data=\" + self.libref + '.' + self.table + self.sas._dsopts(topts) + \";run;\"\n\n        self.sas.nosub = nosub\n        if self.sas.nosub:\n            print(code)\n            return\n\n        if self.results.upper() == 'PANDAS':\n            code = \"data _tail ; set %s.%s %s; run;\" % (self.libref, self.table, self.sas._dsopts(topts))\n            return self._returnPD(code, '_tail')\n        else:\n            if self.HTML:\n                if not le:\n                    ll = self.sas._io.submit(code)\n                else:\n                    ll = le\n                if not self.sas.batch:\n                    self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n                else:\n                    return ll\n            else:\n                if not le:\n                    ll = self.sas._io.submit(code, \"text\")\n                else:\n                    ll = le\n                if not self.sas.batch:\n                    print(ll['LST'])\n                else:\n                    return ll", "response": "display the last n rows of a table"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef obs(self):\n        code = \"proc sql;select count(*) format best32. into :lastobs from \" + self.libref + '.' + self.table + self._dsopts() + \";%put lastobs=&lastobs tom;quit;\"\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        le = self._is_valid()\n        if not le:\n            ll = self.sas.submit(code, \"text\")\n\n            lastobs = ll['LOG'].rpartition(\"lastobs=\")\n            lastobs = lastobs[2].partition(\" tom\")\n            lastobs = int(lastobs[0])\n        else:\n            print(\"The SASdata object is not valid. The table doesn't exist in this SAS session at this time.\")\n            lastobs = None\n\n        return lastobs", "response": "get the number of observations for your SASdata object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef partition(self, var: str = '', fraction: float = .7, seed: int = 9878, kfold: int = 1,\n                  out: 'SASdata' = None, singleOut: bool = True) -> object:\n        \"\"\"\n        Partition a sas data object using SRS sampling or if a variable is specified then\n        stratifying with respect to that variable\n\n        :param var: variable(s) for stratification. If multiple then space delimited list\n        :param fraction: fraction to split\n        :param seed: random seed\n        :param kfold: number of k folds\n        :param out: the SAS data object\n        :param singleOut: boolean to return single table or seperate tables\n        :return: Tuples or SAS data object\n        \"\"\"\n        # loop through for k folds cross-validation\n        i = 1\n        # initialize code string so that loops work\n        code = ''\n        # Make sure kfold was an integer\n        try:\n            k = int(kfold)\n        except ValueError:\n            print(\"Kfold must be an integer\")\n        if out is None:\n            out_table = self.table\n            out_libref = self.libref\n        elif not isinstance(out, str):\n            out_table = out.table\n            out_libref = out.libref\n        else:\n            try:\n                out_table = out.split('.')[1]\n                out_libref = out.split('.')[0]\n            except IndexError:\n                out_table = out\n                out_libref = 'work'\n        while i <= k:\n            # get the list of variables\n            if k == 1:\n                code += \"proc hpsample data=%s.%s %s out=%s.%s %s samppct=%s seed=%s Partition;\\n\" % (\n                    self.libref, self.table, self._dsopts(), out_libref, out_table, self._dsopts(), fraction * 100,\n                    seed)\n            else:\n                seed += 1\n                code += \"proc hpsample data=%s.%s %s out=%s.%s %s samppct=%s seed=%s partition PARTINDNAME=_cvfold%s;\\n\" % (\n                    self.libref, self.table, self._dsopts(), out_libref, out_table, self._dsopts(), fraction * 100,\n                    seed, i)\n\n            # Get variable info for stratified sampling\n            if len(var) > 0:\n                if i == 1:\n                    num_string = \"\"\"\n                        data _null_; file LOG;\n                          d = open('{0}.{1}');\n                          nvars = attrn(d, 'NVARS'); \n                          put 'VARLIST=';\n                          do i = 1 to nvars; \n                             vart = vartype(d, i);\n                             var  = varname(d, i);\n                             if vart eq 'N' then\n                                put var; end;\n                             put 'VARLISTend=';\n                        run;\n                        \"\"\"\n                    # ignore teach_me_SAS mode to run contents\n                    nosub = self.sas.nosub\n                    self.sas.nosub = False\n                    ll = self.sas.submit(num_string.format(self.libref, self.table + self._dsopts()))\n                    self.sas.nosub = nosub\n                    l2 = ll['LOG'].partition(\"VARLIST=\\n\")\n                    l2 = l2[2].rpartition(\"VARLISTend=\\n\")\n                    numlist1 = l2[0].split(\"\\n\")\n\n                    # check if var is in numlist1\n                    if isinstance(var, str):\n                        tlist = var.split()\n                    elif isinstance(var, list):\n                        tlist = var\n                    else:\n                        raise SyntaxError(\"var must be a string or list you submitted: %s\" % str(type(var)))\n                if set(numlist1).isdisjoint(tlist):\n                    if isinstance(var, str):\n                        code += \"class _character_;\\ntarget %s;\\nvar _numeric_;\\n\" % var\n                    else:\n                        code += \"class _character_;\\ntarget %s;\\nvar _numeric_;\\n\" % \" \".join(var)\n                else:\n                    varlist = [x for x in numlist1 if x not in tlist]\n                    varlist.extend([\"_cvfold%s\" % j for j in range(1, i) if k > 1 and i > 1])\n                    code += \"class %s _character_;\\ntarget %s;\\nvar %s;\\n\" % (var, var, \" \".join(varlist))\n\n            else:\n                code += \"class _character_;\\nvar _numeric_;\\n\"\n            code += \"run;\\n\"\n            i += 1\n        # split_code is used if singleOut is False it generates the needed SAS code to break up the kfold partition set.\n        split_code = ''\n        if not singleOut:\n            split_code += 'DATA '\n            for j in range(1, k + 1):\n                split_code += \"\\t%s.%s%s_train(drop=_Partind_ _cvfold:)\\n\" % (out_libref, out_table, j)\n                split_code += \"\\t%s.%s%s_score(drop=_Partind_ _cvfold:)\\n\" % (out_libref, out_table, j)\n            split_code += ';\\n \\tset %s.%s;\\n' % (out_libref, out_table)\n            for z in range(1, k + 1):\n                split_code += \"\\tif _cvfold%s = 1 or _partind_ = 1 then output %s.%s%s_train;\\n\" % (z, out_libref, out_table, z)\n                split_code += \"\\telse output %s.%s%s_score;\\n\" % (out_libref, out_table, z)\n            split_code += 'run;'\n        runcode = True\n        if self.sas.nosub:\n            print(code + '\\n\\n' + split_code)\n            runcode = False\n        ll = self._is_valid()\n        if ll:\n            runcode = False\n        if runcode:\n            ll = self.sas.submit(code + split_code, \"text\")\n            elog = []\n            for line in ll['LOG'].splitlines():\n                if line.startswith('ERROR'):\n                    elog.append(line)\n            if len(elog):\n                raise RuntimeError(\"\\n\".join(elog))\n            if not singleOut:\n                outTableList = []\n                if k == 1:\n                    return (self.sas.sasdata(out_table + str(k) + \"_train\", out_libref, dsopts=self._dsopts()),\n                            self.sas.sasdata(out_table + str(k) + \"_score\", out_libref, dsopts=self._dsopts()))\n\n                for j in range(1, k + 1):\n                    outTableList.append((self.sas.sasdata(out_table + str(j) + \"_train\", out_libref, dsopts=self._dsopts()),\n                                               self.sas.sasdata(out_table + str(j) + \"_score\", out_libref, dsopts=self._dsopts())))\n                return outTableList\n            if out:\n                if not isinstance(out, str):\n                    return out\n                else:\n                    return self.sas.sasdata(out_table, out_libref, self.results)\n            else:\n                return self", "response": "This function partitions a SAS data object using SRS sampling or a random seed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay metadata about the table. size number of rows columns and their data type...", "response": "def contents(self):\n        \"\"\"\n        display metadata about the table. size, number of rows, columns and their data type ...\n\n        :return: output\n        \"\"\"\n        code = \"proc contents data=\" + self.libref + '.' + self.table + self._dsopts() + \";run;\"\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        ll = self._is_valid()\n        if self.results.upper() == 'PANDAS':\n            code  = \"proc contents data=%s.%s %s ;\" % (self.libref, self.table, self._dsopts())\n            code += \"ods output Attributes=work._attributes;\"\n            code += \"ods output EngineHost=work._EngineHost;\"\n            code += \"ods output Variables=work._Variables;\"\n            code += \"ods output Sortedby=work._Sortedby;\"\n            code += \"run;\"\n            return self._returnPD(code, ['_attributes', '_EngineHost', '_Variables', '_Sortedby'])\n\n        else:\n            if self.HTML:\n                if not ll:\n                    ll = self.sas._io.submit(code)\n                if not self.sas.batch:\n                    self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n                else:\n                    return ll\n            else:\n                if not ll:\n                    ll = self.sas._io.submit(code, \"text\")\n                if not self.sas.batch:\n                    print(ll['LST'])\n                else:\n                    return ll"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay metadata about the table size number of rows columns and data type", "response": "def columnInfo(self):\n        \"\"\"\n        display metadata about the table, size, number of rows, columns and their data type\n        \"\"\"\n        code = \"proc contents data=\" + self.libref + '.' + self.table + ' ' + self._dsopts() + \";ods select Variables;run;\"\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        if self.results.upper() == 'PANDAS':\n            code = \"proc contents data=%s.%s %s ;ods output Variables=work._variables ;run;\" % (self.libref, self.table, self._dsopts())\n            pd = self._returnPD(code, '_variables')\n            pd['Type'] = pd['Type'].str.rstrip()\n            return pd\n\n        else:\n            ll = self._is_valid()\n            if self.HTML:\n                if not ll:\n                    ll = self.sas._io.submit(code)\n                if not self.sas.batch:\n                    self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n                else:\n                    return ll\n            else:\n                if not ll:\n                    ll = self.sas._io.submit(code, \"text\")\n                if not self.sas.batch:\n                    print(ll['LST'])\n                else:\n                    return ll"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisplays the column info on a SAS data object.", "response": "def info(self):\n        \"\"\"\n        Display the column info on a SAS data object\n\n        :return: Pandas data frame\n        \"\"\"\n        if self.results.casefold() != 'pandas':\n            print(\"The info method only works with Pandas results\")\n            return None\n        info_code = \"\"\"\n        data work._statsInfo ;\n            do rows=0 by 1 while( not last ) ;\n                set {0}.{1}{2} end=last;\n                array chrs _character_ ;\n                array nums _numeric_ ;\n                array ccounts(999) _temporary_ ;\n                array ncounts(999) _temporary_ ;\n                do over chrs;\n                    ccounts(_i_) + missing(chrs) ;\n                end;\n                do over nums;\n                    ncounts(_i_) + missing(nums);\n                end;   \n            end ;\n            length Variable $32 type $8. ;\n            Do over chrs;\n                Type = 'char';\n                Variable = vname(chrs) ;\n                N = rows;\n                Nmiss = ccounts(_i_) ;\n                Output ;\n            end ;\n            Do over nums;\n                Type = 'numeric';\n                Variable = vname(nums) ;\n                N = rows;\n                Nmiss = ncounts(_i_) ;\n                if variable ^= 'rows' then output;\n            end ;\n            stop;\n            keep Variable N NMISS Type ;\n        run;\n        \"\"\"\n        if self.sas.nosub:\n            print(info_code.format(self.libref, self.table, self._dsopts()))\n            return None\n        info_pd = self._returnPD(info_code.format(self.libref, self.table, self._dsopts()), '_statsInfo')\n        info_pd = info_pd.iloc[:, :]\n        info_pd.index.name = None\n        info_pd.name = None\n        return info_pd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimputing missing values for a SASdata object.", "response": "def impute(self, vars: dict, replace: bool = False, prefix: str = 'imp_', out: 'SASdata' = None) -> 'SASdata':\n        \"\"\"\n        Imputes missing values for a SASdata object.\n\n        :param vars: a dictionary in the form of {'varname':'impute type'} or {'impute type':'[var1, var2]'}\n        :param replace:\n        :param prefix:\n        :param out:\n        :return:\n        \"\"\"\n        outstr = ''\n        if out:\n            if isinstance(out, str):\n                fn = out.partition('.')\n                if fn[1] == '.':\n                    out_libref = fn[0]\n                    out_table = fn[2]\n                else:\n                    out_libref = ''\n                    out_table = fn[0]\n            else:\n                out_libref = out.libref\n                out_table = out.table\n            outstr = \"out=%s.%s\" % (out_libref, out_table)\n\n        else:\n            out_table = self.table\n            out_libref = self.libref\n\n        # get list of variables and types\n        varcode = \"data _null_; d = open('\" + self.libref + \".\" + self.table + \"');\\n\"\n        varcode += \"nvars = attrn(d, 'NVARS');\\n\"\n        varcode += \"vn='VARNUMS='; vl='VARLIST='; vt='VARTYPE=';\\n\"\n        varcode += \"put vn nvars; put vl;\\n\"\n        varcode += \"do i = 1 to nvars; var = varname(d, i); put var; end;\\n\"\n        varcode += \"put vt;\\n\"\n        varcode += \"do i = 1 to nvars; var = vartype(d, i); put var; end;\\n\"\n        varcode += \"run;\"\n        print(varcode)\n        ll = self.sas._io.submit(varcode, \"text\")\n        l2 = ll['LOG'].rpartition(\"VARNUMS= \")\n        l2 = l2[2].partition(\"\\n\")\n        nvars = int(float(l2[0]))\n        l2 = l2[2].partition(\"\\n\")\n        varlist = l2[2].upper().split(\"\\n\", nvars)\n        del varlist[nvars]\n        l2 = l2[2].partition(\"VARTYPE=\")\n        l2 = l2[2].partition(\"\\n\")\n        vartype = l2[2].split(\"\\n\", nvars)\n        del vartype[nvars]\n        varListType = dict(zip(varlist, vartype))\n\n        # process vars dictionary to generate code\n        ## setup default statements\n        sql = \"proc sql;\\n  select\\n\"\n        sqlsel = ' %s(%s),\\n'\n        sqlinto = ' into\\n'\n        if len(out_libref)>0 :\n            ds1 = \"data \" + out_libref + \".\" + out_table + \"; set \" + self.libref + \".\" + self.table + self._dsopts() + \";\\n\"\n        else:\n            ds1 = \"data \" + out_table + \"; set \" + self.libref + \".\" + self.table + self._dsopts() + \";\\n\"\n        dsmiss = 'if missing({0}) then {1} = {2};\\n'\n        if replace:\n            dsmiss = prefix+'{1} = {0}; if missing({0}) then %s{1} = {2};\\n' % prefix\n\n        modesql = ''\n        modeq = \"proc sql outobs=1;\\n  select %s, count(*) as freq into :imp_mode_%s, :imp_mode_freq\\n\"\n        modeq += \"  from %s where %s is not null group by %s order by freq desc, %s;\\nquit;\\n\"\n\n        # pop the values key because it needs special treatment\n        contantValues = vars.pop('value', None)\n        if contantValues is not None:\n            if not all(isinstance(x, tuple) for x in contantValues):\n                raise SyntaxError(\"The elements in the 'value' key must be tuples\")\n            for t in contantValues:\n                if varListType.get(t[0].upper()) == \"N\":\n                    ds1 += dsmiss.format((t[0], t[0], t[1]))\n                else:\n                    ds1 += dsmiss.format(t[0], t[0], '\"' + str(t[1]) + '\"')\n        for key, values in vars.items():\n            if key.lower() in ['midrange', 'random']:\n                for v in values:\n                    sql += sqlsel % ('max', v)\n                    sql += sqlsel % ('min', v)\n                    sqlinto += ' :imp_max_' + v + ',\\n'\n                    sqlinto += ' :imp_min_' + v + ',\\n'\n                    if key.lower() == 'midrange':\n                        ds1 += dsmiss.format(v, v, '(&imp_min_' + v + '.' + ' + ' + '&imp_max_' + v + '.' + ') / 2')\n                    elif key.lower() == 'random':\n                        # random * (max - min) + min\n                        ds1 += dsmiss.format(v, v, '(&imp_max_' + v + '.' + ' - ' + '&imp_min_' + v + '.' + ') * ranuni(0)' + '+ &imp_min_' + v + '.')\n                    else:\n                        raise SyntaxError(\"This should not happen!!!!\")\n            else:\n                for v in values:\n                    sql += sqlsel % (key, v)\n                    sqlinto += ' :imp_' + v + ',\\n'\n                    if key.lower == 'mode':\n                        modesql += modeq % (v, v, self.libref + \".\" + self.table + self._dsopts() , v, v, v)\n                    if varListType.get(v.upper()) == \"N\":\n                        ds1 += dsmiss.format(v, v, '&imp_' + v + '.')\n                    else:\n                        ds1 += dsmiss.format(v, v, '\"&imp_' + v + '.\"')\n\n        if len(sql) > 20:\n            sql = sql.rstrip(', \\n') + '\\n' + sqlinto.rstrip(', \\n') + '\\n  from ' + self.libref + '.' + self.table + self._dsopts() + ';\\nquit;\\n'\n        else:\n            sql = ''\n        ds1 += 'run;\\n'\n\n        if self.sas.nosub:\n            print(modesql + sql + ds1)\n            return None\n        ll = self.sas.submit(modesql + sql + ds1)\n        return self.sas.sasdata(out_table, libref=out_libref, results=self.results, dsopts=self._dsopts())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsorting the SAS Data Set by a given variable.", "response": "def sort(self, by: str, out: object = '', **kwargs) -> 'SASdata':\n        \"\"\"\n        Sort the SAS Data Set\n\n        :param by: REQUIRED variable to sort by (BY <DESCENDING> variable-1 <<DESCENDING> variable-2 ...>;)\n        :param out: OPTIONAL takes either a string 'libref.table' or 'table' which will go to WORK or USER\n            if assigned or a sas data object'' will sort in place if allowed\n        :param kwargs:\n        :return: SASdata object if out= not specified, or a new SASdata object for out= when specified\n\n        :Example:\n\n        #. wkcars.sort('type')\n        #. wkcars2 = sas.sasdata('cars2')\n        #. wkcars.sort('cylinders', wkcars2)\n        #. cars2=cars.sort('DESCENDING origin', out='foobar')\n        #. cars.sort('type').head()\n        #. stat_results = stat.reg(model='horsepower = Cylinders EngineSize', by='type', data=wkcars.sort('type'))\n        #. stat_results2 = stat.reg(model='horsepower = Cylinders EngineSize', by='type', data=wkcars.sort('type','work.cars'))\n        \"\"\"\n        outstr = ''\n        options = ''\n        if out:\n            if isinstance(out, str):\n                fn = out.partition('.')\n                if fn[1] == '.':\n                    libref = fn[0]\n                    table = fn[2]\n                    outstr = \"out=%s.%s\" % (libref, table)\n                else:\n                    libref = ''\n                    table = fn[0]\n                    outstr = \"out=\" + table\n            else:\n                libref = out.libref\n                table = out.table\n                outstr = \"out=%s.%s\" % (out.libref, out.table)\n\n        if 'options' in kwargs:\n            options = kwargs['options']\n\n        code = \"proc sort data=%s.%s%s %s %s ;\\n\" % (self.libref, self.table, self._dsopts(), outstr, options)\n        code += \"by %s;\" % by\n        code += \"run\\n;\"\n        runcode = True\n        if self.sas.nosub:\n            print(code)\n            runcode = False\n\n        ll = self._is_valid()\n        if ll:\n            runcode = False\n        if runcode:\n            ll = self.sas.submit(code, \"text\")\n            elog = []\n            for line in ll['LOG'].splitlines():\n                if line.startswith('ERROR'):\n                    elog.append(line)\n            if len(elog):\n                raise RuntimeError(\"\\n\".join(elog))\n        if out:\n            if not isinstance(out, str):\n                return out\n            else:\n                return self.sas.sasdata(table, libref, self.results)\n        else:\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef score(self, file: str = '', code: str = '', out: 'SASdata' = None) -> 'SASdata':\n        if out is not None:\n            outTable = out.table\n            outLibref = out.libref\n        else:\n            outTable = self.table\n            outLibref = self.libref\n        codestr = code\n        code = \"data %s.%s%s;\" % (outLibref, outTable, self._dsopts())\n        code += \"set %s.%s%s;\" % (self.libref, self.table, self._dsopts())\n        if len(file)>0:\n            code += '%%include \"%s\";' % file\n        else:\n            code += \"%s;\" %codestr\n        code += \"run;\"\n\n        if self.sas.nosub:\n            print(code)\n            return None\n\n        ll = self._is_valid()\n        if not ll:\n            html = self.HTML\n            self.HTML = 1\n            ll = self.sas._io.submit(code)\n            self.HTML = html\n        if not self.sas.batch:\n            self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n        else:\n            return ll", "response": "This method updates the SAS Data object with a model score file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_df(self, method: str = 'MEMORY', **kwargs) -> 'pd.DataFrame':\n        ll = self._is_valid()\n        if ll:\n            print(ll['LOG'])\n            return None\n        else:\n            return self.sas.sasdata2dataframe(self.table, self.libref, self.dsopts, method, **kwargs)", "response": "Export this SAS Data Set to a Pandas Data Frame"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexporting this SAS Data Set to a Pandas Data Frame via CSV file", "response": "def to_df_CSV(self, tempfile: str=None, tempkeep: bool=False, **kwargs) -> 'pd.DataFrame':\n        \"\"\"\n        Export this SAS Data Set to a Pandas Data Frame via CSV file\n\n        :param tempfile: [optional] an OS path for a file to use for the local CSV file; default it a temporary file that's cleaned up\n        :param tempkeep: if you specify your own file to use with tempfile=, this controls whether it's cleaned up after using it\n        :param kwargs:\n        :return: Pandas data frame\n        :rtype: 'pd.DataFrame'\n        \"\"\"\n        return self.to_df(method='CSV', tempfile=tempfile, tempkeep=tempkeep, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_json(self, pretty: bool = False, sastag: bool = False, **kwargs) -> str:\n        code = \"filename file1 temp;\"\n        code += \"proc json out=file1\"\n        if pretty:\n            code += \" pretty \"\n        if not sastag:\n            code += \" nosastags \"\n        code +=\";\\n export %s.%s %s;\\n run;\" % (self.libref, self.table, self._dsopts())\n\n        if self.sas.nosub:\n            print(code)\n            return None\n\n        ll = self._is_valid()\n        runcode = True\n        if ll:\n            runcode = False\n        if runcode:\n            ll = self.sas.submit(code, \"text\")\n            elog = []\n            fpath=''\n            for line in ll['LOG'].splitlines():\n                if line.startswith('JSONFilePath:'):\n                    fpath = line[14:]\n                if line.startswith('ERROR'):\n                    elog.append(line)\n            if len(elog):\n                raise RuntimeError(\"\\n\".join(elog))\n            if len(fpath):\n                with open(fpath, 'r') as myfile:\n                    json_str = myfile.read()\n                return json_str", "response": "Export this SAS Data Set to a JSON object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef heatmap(self, x: str, y: str, options: str = '', title: str = '',\n                label: str = '') -> object:\n        \"\"\"\n        Documentation link: http://support.sas.com/documentation/cdl/en/grstatproc/67909/HTML/default/viewer.htm#n0w12m4cn1j5c6n12ak64u1rys4w.htm\n\n        :param x: x variable\n        :param y: y variable\n        :param options: display options (string)\n        :param title: graph title\n        :param label:\n        :return:\n        \"\"\"\n        code = \"proc sgplot data=%s.%s %s;\" % (self.libref, self.table, self._dsopts())\n        if len(options):\n            code += \"\\n\\theatmap x=%s y=%s / %s;\" % (x, y, options)\n        else:\n            code += \"\\n\\theatmap x=%s y=%s;\" % (x, y)\n\n        if len(label) > 0:\n            code += \" LegendLABEL='\" + label + \"'\"\n        code += \";\\n\"\n        if len(title) > 0:\n            code += \"\\ttitle '%s';\\n\" % title\n        code += \"run;\\ntitle;\"\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        ll = self._is_valid()\n        if not ll:\n            html = self.HTML\n            self.HTML = 1\n            ll = self.sas._io.submit(code)\n            self.HTML = html\n        if not self.sas.batch:\n            self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n        else:\n            return ll", "response": "This function returns the heatmap of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hist(self, var: str, title: str = '',\n             label: str = '') -> object:\n        \"\"\"\n        This method requires a numeric column (use the contents method to see column types) and generates a histogram.\n\n        :param var: the NUMERIC variable (column) you want to plot\n        :param title: an optional Title for the chart\n        :param label: LegendLABEL= value for sgplot\n        :return:\n        \"\"\"\n        code = \"proc sgplot data=\" + self.libref + '.' + self.table + self._dsopts()\n        code += \";\\n\\thistogram \" + var + \" / scale=count\"\n        if len(label) > 0:\n            code += \" LegendLABEL='\" + label + \"'\"\n        code += \";\\n\"\n        if len(title) > 0:\n            code += '\\ttitle \"' + title + '\";\\n'\n        code += \"\\tdensity \" + var + ';\\nrun;\\n' + 'title;'\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        ll = self._is_valid()\n        if not ll:\n            html = self.HTML\n            self.HTML = 1\n            ll = self.sas._io.submit(code)\n            self.HTML = html\n        if not self.sas.batch:\n            self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n        else:\n            return ll", "response": "This method generates a histogram of the numeric variable var."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef top(self, var: str, n: int = 10, order: str = 'freq', title: str = '') -> object:\n        code = \"proc freq data=%s.%s %s order=%s noprint;\" % (self.libref, self.table, self._dsopts(), order)\n        code += \"\\n\\ttables %s / out=tmpFreqOut;\" % var\n        code += \"\\nrun;\"\n        if len(title) > 0:\n            code += '\\ttitle \"' + title + '\";\\n'\n        code += \"proc print data=tmpFreqOut(obs=%s); \\nrun;\" % n\n        code += 'title;'\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        ll = self._is_valid()\n        if self.results.upper() == 'PANDAS':\n            code = \"proc freq data=%s.%s%s order=%s noprint;\" % (self.libref, self.table, self._dsopts(), order)\n            code += \"\\n\\ttables %s / out=tmpFreqOut;\" % var\n            code += \"\\nrun;\"\n            code += \"\\ndata tmpFreqOut; set tmpFreqOut(obs=%s); run;\" % n\n            return self._returnPD(code, 'tmpFreqOut')\n        else:\n            if self.HTML:\n                if not ll:\n                    ll = self.sas._io.submit(code)\n                if not self.sas.batch:\n                    self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n                else:\n                    return ll\n            else:\n                if not ll:\n                    ll = self.sas._io.submit(code, \"text\")\n                if not self.sas.batch:\n                    print(ll['LST'])\n                else:\n                    return ll", "response": "Return the most commonly occuring items in a table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef series(self, x: str, y: list, title: str = '') -> object:\n\n        code = \"proc sgplot data=\" + self.libref + '.' + self.table + self._dsopts() + \";\\n\"\n        if len(title) > 0:\n            code += '\\ttitle \"' + title + '\";\\n'\n\n        if isinstance(y, list):\n            num = len(y)\n        else:\n            num = 1\n            y = [y]\n\n        for i in range(num):\n            code += \"\\tseries x=\" + x + \" y=\" + str(y[i]) + \";\\n\"\n\n        code += 'run;\\n' + 'title;'\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        ll = self._is_valid()\n        if not ll:\n            html = self.HTML\n            self.HTML = 1\n            ll = self.sas._io.submit(code)\n            self.HTML = html\n        if not self.sas.batch:\n            self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n        else:\n            return ll", "response": "This method plots a series of x y coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hpimpute(self, data: ['SASdata', str] = None,\n                 code: str = None,\n                 freq: str = None,\n                 id: str = None,\n                 impute: str = None,\n                 input: [str, list, dict] = None,\n                 performance: str = None,\n                 procopts: str = None,\n                 stmtpassthrough: str = None,\n                 **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the HPIMPUTE procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=prochp&docsetTarget=prochp_hpimpute_toc.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm code: The code variable can only be a string type.\n        :parm freq: The freq variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm impute: The impute variable can only be a string type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm performance: The performance variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to HPIMPUTE the SAS resource store for a specific set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hpbin(self, data: ['SASdata', str] = None,\n              code: str = None,\n              freq: str = None,\n              id: [str, list] = None,\n              input: [str, list, dict] = None,\n              performance: str = None,\n              target: [str, list, dict] = None,\n              procopts: str = None,\n              stmtpassthrough: str = None,\n              **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the HPBIN procedure.\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=prochp&docsetTarget=prochp_hpbin_syntax.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required..\n        :parm code: The code variable can only be a string type.\n        :parm freq: The freq variable can only be a string type.\n        :parm id: The id variable can be a string or list type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm performance: The performance variable can only be a string type.\n        :parm target: The target variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the HPBIN procedure. It is used to call the HPBIN procedure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef univariate(self, data: ['SASdata', str] = None,\n                   by: [str, list] = None,\n                   cdfplot: str = None,\n                   cls: [str, list] = None,\n                   freq: str = None,\n                   histogram: str = None,\n                   id: [str, list] = None,\n                   inset: str = None,\n                   output: [str, bool, 'SASdata'] = None,\n                   ppplot: str = None,\n                   probplot: str = None,\n                   qqplot: str = None,\n                   var: str = None,\n                   weight: str = None,\n                   procopts: str = None,\n                   stmtpassthrough: str = None,\n                   **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the UNIVARIATE procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=procstat&docsetTarget=procstat_univariate_syntax.htm&locale=en\n\n        The PROC UNIVARIATE statement invokes the procedure. The VAR statement specifies the numeric variables to be analyzed, and it is required if the OUTPUT statement is used to save summary statistics in an output data set. If you do not use the VAR statement, all numeric variables in the data set are analyzed. The plot statements (CDFPLOT, HISTOGRAM, PPPLOT, PROBPLOT, and QQPLOT) create graphical displays, and the INSET statement enhances these displays by adding a table of summary statistics directly on the graph. You can specify one or more of each of the plot statements, the INSET statement, and the OUTPUT statement. If you use a VAR statement, the variables listed in a plot statement must be a subset of the variables listed in the VAR statement.\n\n        You can specify a BY statement to obtain separate analyses for each BY group. The FREQ statement specifies a variable whose values provide the frequency for each observation. The ID statement specifies one or more variables to identify the extreme observations. The WEIGHT statement specifies a variable whose values are used to weight certain statistics.\n\n        You can use a CLASS statement to specify one or two variables that group the data into classification levels. The analysis is carried out for each combination of levels in the input data set, or within each BY group if you also specify a BY statement. You can use the CLASS statement with plot statements to create comparative displays, in which each cell contains a plot for one combination of classification levels.\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm by: The by variable can be a string or list type.\n        :parm cdfplot: The cdfplot variable can only be a string type.\n        :parm cls: The cls variable can be a string or list type. It refers to the categorical, or nominal variables.\n        :parm freq: The freq variable can only be a string type.\n        :parm histogram: The histogram variable can only be a string type.\n        :parm id: The id variable can be a string or list type.\n        :parm inset: The inset variable can only be a string type.\n        :parm output: The output variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm ppplot: The ppplot variable can only be a string type.\n        :parm probplot: The probplot variable can only be a string type.\n        :parm qqplot: The qqplot variable can only be a string type.\n        :parm var: The var variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "Python method to call the UNIVARIATE procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=procstat&docsetTarget=procstat_univariate_syntax.htm&locale=en\n\n        The PROC UNIVARIATE statement invokes the procedure. The VAR statement specifies the numeric variables to be analyzed, and it is required if the OUTPUT statement is used to save summary statistics in an output data set. If you do not use the VAR statement, all numeric variables in the data set are analyzed. The plot statements (CDFPLOT, HISTOGRAM, PPPLOT, PROBPLOT, and QQPLOT) create graphical displays, and the INSET statement enhances these displays by adding a table of summary statistics directly on the graph. You can specify one or more of each of the plot statements, the INSET statement, and the OUTPUT statement. If you use a VAR statement, the variables listed in a plot statement must be a subset of the variables listed in the VAR statement.\n\n        You can specify a BY statement to obtain separate analyses for each BY group. The FREQ statement specifies a variable whose values provide the frequency for each observation. The ID statement specifies one or more variables to identify the extreme observations. The WEIGHT statement specifies a variable whose values are used to weight certain statistics.\n\n        You can use a CLASS statement to specify one or two variables that group the data into classification levels. The analysis is carried out for each combination of levels in the input data set, or within each BY group if you also specify a BY statement. You can use the CLASS statement with plot statements to create comparative displays, in which each cell contains a plot for one combination of classification levels.\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm by: The by variable can be a string or list type.\n        :parm cdfplot: The cdfplot variable can only be a string type.\n        :parm cls: The cls variable can be a string or list type. It refers to the categorical, or nominal variables.\n        :parm freq: The freq variable can only be a string type.\n        :parm histogram: The histogram variable can only be a string type.\n        :parm id: The id variable can be a string or list type.\n        :parm inset: The inset variable can only be a string type.\n        :parm output: The output variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm ppplot: The ppplot variable can only be a string type.\n        :parm probplot: The probplot variable can only be a string type.\n        :parm qqplot: The qqplot variable can only be a string type.\n        :parm var: The var variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _makeProcCallMacro(self, objtype: str, objname: str, data: ['SASdata', str] = None, args: dict = None) -> str:\n        plot = ''\n        outmeth = ''\n        procopts = args.pop('procopts', '')\n        # Set ODS graphic generation to True by default\n        ODSGraphics = args.get('ODSGraphics', True)\n\n        # The different SAS products vary slightly in plotting and out methods.\n        # this block sets the options correctly for plotting and output statements\n        if self.sasproduct.lower() == 'stat' and not ('ODSGraphics' in args.keys() or ODSGraphics == False):\n            plot = 'plot=all'\n        if self.sasproduct.lower() == 'qc':\n            pass\n        if self.sasproduct.lower() == 'ets' and not ('ODSGraphics' in args.keys() or ODSGraphics == False):\n            outmeth = 'out'\n            plot = 'plot=all'\n        if self.sasproduct.lower() == 'em':\n\n            pass\n        if self.sasproduct.lower() == 'vddml':\n            outmeth = 'out'\n        if self.sasproduct.lower() == 'util':\n            outmeth = 'out'\n            if objtype.lower() =='univariate' and not ('ODSGraphics' in args.keys() or ODSGraphics == False):\n                plot = 'plot'\n                outmeth = ''\n        outds = args.pop('out', None)\n        if outds == None:\n            outds = args.pop('output', None)\n        outcodegen = Codegen.new('out', outds)\n        outcodegen.outmeth = outmeth\n        outcodegen.objname = objname\n        outstr = outcodegen.codestmt\n        self.logger.debug(\"product caller: \" + self.sasproduct.lower())\n        debug_code= ''\n        code = \"%macro proccall(d);\\n\"\n        # resolve issues withe Proc options, out= and plots=\n        # The procopts statement should be in every procedure as a way to pass arbitrary options to the procedures\n        if 'outmeth' in args:\n            outmeth = args['outmeth']\n        if 'plot' in args:\n            plot = args['plot']\n        if len(outmeth) and not outds == None:\n            #outstr = outds.libref + '.' + outds.table\n            code += \"proc %s data=%s.%s%s %s %s=%s %s ;\\n\" % (\n                objtype, data.libref, data.table, data._dsopts(), plot, outmeth, outstr, procopts)\n        else:\n            code += \"proc %s data=%s.%s%s %s %s ;\\n\" % (\n            objtype, data.libref, data.table, data._dsopts(), plot, procopts)\n            if outds is not None:\n                args['output'] = outds\n        self.logger.debug(\"args value: \" + str(args))\n        self.logger.debug(\"args type: \" + str(type(args)))\n\n        # this list is largely alphabetical but there are exceptions in order to\n        # satisfy the order needs of the statements for the procedures\n        # as an example...\n        # http://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_glm_syntax.htm#statug.glm.glmpostable\n\n        uoargs = {}\n        orderedargs = {}\n        keyorder = ['by', 'input', 'target', 'cls', 'model', 'output']\n        for k, v in args.items():\n            if k in keyorder:\n                orderedargs[k] = v\n            else:\n                uoargs[k] = v\n        orderedargs = OrderedDict(sorted(orderedargs.items(), key=lambda i: keyorder.index(i[0])))\n        for k, v in uoargs.items():\n            orderedargs[k] = v\n            orderedargs.move_to_end(k)\n        for key, value in orderedargs.items():\n            gen = Codegen.new(key, value)\n            gen.objtype = objtype\n            gen.data = data\n            gen.outmeth = outmeth\n            gen.objname = objname\n            code += gen.codestmt\n            if gen.debug is not None:\n                debug_code += gen.debug\n\n        code += \"run; quit; %mend;\\n\"\n        code += \"%%mangobj(%s,%s,%s);\" % (objname, objtype, data.table)\n        if self.logger.level == 10:\n            print(\"Proc code submission:\\n \" + str(code))\n            print(\"\\n\\n\\n\" + debug_code)\n        return code", "response": "This method generates the SAS code needed to execute a particular Python object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _charlist(self, data) -> list:\n        # Get list of character variables to add to nominal list\n        char_string = \"\"\"\n        data _null_; file LOG;\n          d = open('{0}.{1}');\n          nvars = attrn(d, 'NVARS');\n          put 'VARLIST=';\n          do i = 1 to nvars;\n             vart = vartype(d, i);\n             var  = varname(d, i);\n             if vart eq 'C' then\n                put var; end;\n          put 'VARLISTend=';\n        run;\n        \"\"\"\n        # ignore teach_me_SAS mode to run contents\n        nosub = self.sas.nosub\n        self.sas.nosub = False\n        ll = self.sas.submit(char_string.format(data.libref, data.table + data._dsopts()))\n        self.sas.nosub = nosub\n        l2 = ll['LOG'].partition(\"VARLIST=\\n\")\n        l2 = l2[2].rpartition(\"VARLISTend=\\n\")\n        charlist1 = l2[0].split(\"\\n\")\n        del charlist1[len(charlist1) - 1]\n        charlist1 = [x.casefold() for x in charlist1]\n        return charlist1", "response": "Private method to return the list of character variables in a SAS Data set that are of type char\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking the target key from kwargs and processes it to aid in the generation of a model statement.", "response": "def _target_stmt(self, stmt: object) -> tuple:\n        \"\"\"\n        takes the target key from kwargs and processes it to aid in the generation of a model statement\n        :param stmt: str, list, or dict that contains the model information.\n        :return: tuple of strings one for the class statement one for the model statements\n        \"\"\"\n        # make sure target is a single variable extra split to account for level= option\n        code = ''\n        cls = ''\n        if isinstance(stmt, str):\n            if len(stmt.split('/')[0].split()) == 1:\n                code += \"%s\" % (stmt)\n            else:\n                raise SyntaxError(\n                    \"ERROR in code submission. TARGET can only have one variable and you submitted: %s\" % stmt)\n        elif isinstance(stmt, list):\n            if len(stmt) == 1:\n                code += \"%s\" % str(stmt[0])\n            else:\n                raise SyntaxError(\"The target list must have exactly one member\")\n        elif isinstance(stmt, dict):\n            try:\n                # check there there is only one target:\n                length = 0\n                try:\n                    length += len([stmt['nominal'], stmt['interval']])\n                except KeyError:\n                    try:\n                        length += len([stmt['nominal']])\n                    except KeyError:\n                        try:\n                            length += len([stmt['interval']])\n                        except KeyError:\n                            raise\n                if length == 1:\n                    if 'interval' in stmt.keys():\n                        if isinstance(stmt['interval'], str):\n                            code += \"%s\" % stmt['interval']\n                        if isinstance(stmt['interval'], list):\n                            code += \"%s\" % \" \".join(stmt['interval'])\n                    if 'nominal' in stmt.keys():\n                        if isinstance(stmt['nominal'], str):\n                            code += \"%s\" % stmt['nominal']\n                            cls += \"%s\" % stmt['nominal']\n\n                        if isinstance(stmt['nominal'], list):\n                            code += \"%s\" % \" \".join(stmt['nominal'])\n                            cls += \"%s\" % \" \".join(stmt['nominal'])\n                else:\n                    raise SyntaxError\n            except SyntaxError:\n                print(\"SyntaxError: TARGET can only have one variable\")\n            except KeyError:\n                print(\"KeyError: Proper keys not found for TARGET dictionary: %s\" % stmt.keys())\n        else:\n            raise SyntaxError(\"TARGET is in an unknown format: %s\" % str(stmt))\n        return (code, cls)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking the input key from kwargs and processes it to aid in the generation of a model statement.", "response": "def _input_stmt(self, stmt: object) -> tuple:\n        \"\"\"\n        takes the input key from kwargs and processes it to aid in the generation of a model statement\n        :param stmt: str, list, or dict that contains the model information.\n        :return: tuple of strings one for the class statement one for the model statements\n        \"\"\"\n        code = ''\n        cls = ''\n        if isinstance(stmt, str):\n            code += \"%s \" % (stmt)\n        elif isinstance(stmt, dict):\n            try:\n                if 'interval' in stmt.keys():\n                    if isinstance(stmt['interval'], str):\n                        code += \"%s \" % stmt['interval']\n                    if isinstance(stmt['interval'], list):\n                        code += \"%s \" % \" \".join(stmt['interval'])\n                if 'nominal' in stmt.keys():\n                    if isinstance(stmt['nominal'], str):\n                        code += \"%s \" % stmt['nominal']\n                        cls += \"%s \" % stmt['nominal']\n                    if isinstance(stmt['nominal'], list):\n                        code += \"%s \" % \" \".join(stmt['nominal'])\n                        cls += \"%s \" % \" \".join(stmt['nominal'])\n            except:\n                raise SyntaxError(\"Proper Keys not found for INPUT dictionary: %s\" % stmt.keys())\n        elif isinstance(stmt, list):\n            if len(stmt) == 1:\n                code += \"%s\" % str(stmt[0])\n            elif len(stmt) > 1:\n                code += \"%s\" % \" \".join(stmt)\n            else:\n                raise SyntaxError(\"The input list has no members\")\n        else:\n            raise SyntaxError(\"INPUT is in an unknown format: %s\" % str(stmt))\n        return (code, cls)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _stmt_check(self, req: set, legal: set, stmt: dict) -> dict:\n        # debug the argument list\n        if self.logger.level == 10:\n            for k, v in stmt.items():\n                print(\"Key: \" + k + \", Value: \" + str(v) + \", Type: \" + str(type(v)))\n\n        # required statements\n        reqSet = req\n        if len(reqSet):\n            self.logger.debug(\"reqSet: {}\".format(reqSet))\n            missing_set = reqSet.difference(set(stmt.keys()))\n            if missing_set:\n                if not stmt.get(\n                        'score'):  # till we handle either/or required. proc can be called more than one way w/ diff requirements\n                    raise SyntaxError(\n                        \"You are missing %d required statements:\\n%s\" % (len(missing_set), str(missing_set)))\n\n        # legal statements\n        legalSet = legal\n        if len(legalSet):\n            self.logger.debug(\"legalSet: {}\".format(legalSet))\n            if len(reqSet):\n                totSet = legalSet | reqSet\n            else:\n                totSet = legalSet\n            generalSet = {'ODSGraphics', 'stmtpassthrough', 'targOpts', 'procopts'}\n            extraSet = set(stmt.keys() - generalSet).difference(totSet)  # find keys not in legal or required sets\n            if extraSet:\n                self.logger.debug(\"extraSet: {}\".format(extraSet))\n                for item in extraSet:\n                    stmt.pop(item, None)\n                warnings.warn(\n                    \"The following {} statements are invalid and will be ignored:\\n{}\".format(len(extraSet), extraSet))\n        self.logger.debug(\"stmt: {}\".format(stmt))\n        return stmt", "response": "This method checks to make sure that the proc has all required statements and removes any extra statements that are not in legal."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef factmac(self, data: ['SASdata', str] = None,\n                autotune: str = None,\n                code: str = None,\n                display: str = None,\n                displayout: str = None,\n                id: str = None,\n                input: [str, list, dict] = None,\n                output: [str, bool, 'SASdata'] = None,\n                savestate: str = None,\n                target: [str, list, dict] = None,\n                procopts: str = None,\n                stmtpassthrough: str = None,\n                **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the FACTMAC procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?docsetId=casml&docsetTarget=casml_factmac_syntax.htm&docsetVersion=8.3&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm autotune: The autotune variable can only be a string type.\n        :parm code: The code variable can only be a string type.\n        :parm display: The display variable can only be a string type.\n        :parm displayout: The displayout variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm output: The output variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm savestate: The savestate variable can only be a string type.\n        :parm target: The target variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the FACTMAC method of the FACTMAC module. It is used to generate the SAS data object for the current SAS module."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fastknn(self, data: ['SASdata', str] = None,\n                display: str = None,\n                displayout: str = None,\n                id: str = None,\n                input: [str, list, dict] = None,\n                output: [str, bool, 'SASdata'] = None,\n                procopts: str = None,\n                stmtpassthrough: str = None,\n                **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the FASTKNN procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?docsetId=casml&docsetTarget=casml_fastknn_toc.htm&docsetVersion=8.3&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm display: The display variable can only be a string type.\n        :parm displayout: The displayout variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm output: The output variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the FASTKNN procedure in the SAS server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nnet(self, data: ['SASdata', str] = None,\n             architecture: str = None,\n             autotune: str = None,\n             code: str = None,\n             crossvalidation: str = None,\n             hidden: [str, int] = None,\n             input: [str, list, dict] = None,\n             optimization: str = None,\n             output: [str, bool, 'SASdata'] = None,\n             partition: str = None,\n             target: [str, list, dict] = None,\n             train: [str, dict] = None,\n             weight: str = None,\n             procopts: str = None,\n             stmtpassthrough: str = None,\n             **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the HPNEURAL procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?docsetId=casml&docsetTarget=casml_nnet_toc.htm&docsetVersion=8.3&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm architecture: The architecture variable can only be a string type.\n        :parm autotune: The autotune variable can only be a string type.\n        :parm code: The code variable can only be a string type.\n        :parm crossvalidation: The crossvalidation variable can only be a string type.\n        :parm hidden: The hidden variable can only be a string type. This statement is required if there is a Train statement and the architecture is not GLIM.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm optimization: The optimization variable can only be a string type.\n        :parm output: The output variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm partition: The partition variable can only be a string type.\n        :parm target: The target variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm train: The train variable can be a string or dict type. This parameter is required\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the HPNEURAL nnet procedure on the SAS server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef svdd(self, data: ['SASdata', str] = None,\n             code: str = None,\n             id: str = None,\n             input: [str, list, dict] = None,\n             kernel: str = None,\n             savestate: str = None,\n             solver: str = None,\n             weight: str = None,\n             procopts: str = None,\n             stmtpassthrough: str = None,\n             **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the SVDD procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?docsetId=casml&docsetTarget=casml_svdd_toc.htm&docsetVersion=8.3&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm code: The code variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm kernel: The kernel variable can only be a string type.\n        :parm savestate: The savestate variable can only be a string type.\n        :parm solver: The solver variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"", "response": "This method is used to call the SVDD procedure in the SAS server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _breakprompt(self, eos):\n        found = False\n        logf  = b''\n        lstf  = b''\n        bc    = False\n\n        if self.pid is None:\n            self._sb.SASpid = None\n            return dict(LOG=b\"No SAS process attached. SAS process has terminated unexpectedly.\", LST=b'', ABORT=True)\n\n        if True:\n           response = self.sascfg._prompt(\n                     \"SAS attention handling is not yet supported over IOM. Please enter (T) to terminate SAS or (C) to continue.\")\n           while True:\n              if response is None or response.upper() == 'C':\n                 return dict(LOG=b'', LST=b'', BC=True)\n              if response.upper() == 'T':\n                 break\n              response = self.sascfg._prompt(\"Please enter (T) to terminate SAS or (C) to continue.\")\n\n        if os.name == 'nt':\n           self.pid.kill()\n        else:\n           interrupt = signal.SIGINT\n           os.kill(self.pid, interrupt)\n           sleep(.25)\n\n        self.pid = None\n        self._sb.SASpid = None\n        return dict(LOG=b\"SAS process terminated\", LST=b'', ABORT=True)\n\n\n\n\n        \"\"\"\n        while True:\n            rc = os.waitid(os.P_PID, self.pid, os.WEXITED | os.WNOHANG)\n            if rc is not None:\n                self.pid = None\n                self._sb.SASpid = None\n                outrc = str(rc)\n                return dict(LOG='SAS process has terminated unexpectedly. Pid State= '+outrc, LST='', ABORT=True)\n\n            lst = self.stdout.read1(4096).decode(errors='replace')\n            lstf += lst\n            if len(lst) > 0:\n                lsts = lst.rpartition('Select:')\n                if lsts[0] != '' and lsts[1] != '':\n                    found = True\n                    query = lsts[1] + lsts[2].rsplit('\\n?')[0] + '\\n'\n                    print('Processing interrupt\\nAttn handler Query is\\n\\n' + query)\n                    response = self.sascfg._prompt(\"Please enter your Response: \")\n                    self.stdin[0].send(response.encode() + b'\\n')\n                    if (response == 'C' or response == 'c') and query.count(\"C. Cancel\") >= 1:\n                       bc = True\n                       break\n                else:\n                    lsts = lst.rpartition('Press')\n                    if lsts[0] != '' and lsts[1] != '':\n                        query = lsts[1] + lsts[2].rsplit('\\n?')[0] + '\\n'\n                        print('Secondary Query is:\\n\\n' + query)\n                        response = self.sascfg._prompt(\"Please enter your Response: \")\n                        self.stdin[0].send(response.encode() + b'\\n')\n                        if (response == 'N' or response == 'n') and query.count(\"N to continue\") >= 1:\n                           bc = True\n                           break\n                    else:\n                        #print(\"******************No 'Select' or 'Press' found in lst=\")\n                        pass\n            else:\n                log = self.stderr[0].recv(4096).decode(errors='replace')\n                logf += log\n                self._log += log\n\n                if log.count(eos) >= 1:\n                    print(\"******************Found end of step. No interrupt processed\")\n                    found = True\n\n                if found:\n                    break\n\n            sleep(.25)\n\n        lstr = lstf\n        logr = logf\n        return dict(LOG=logr, LST=lstr, BC=bc)\n        \"\"\"", "response": "This function is used to break the prompt process until the process is terminated."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disconnect(self):\n\n      if not self.sascfg.reconnect:\n         return \"Disconnecting and then reconnecting to this workspaceserver has been disabled. Did not disconnect\"\n\n      pgm = b'\\n'+b'tom says EOL=DISCONNECT                      \\n'\n      self.stdin[0].send(pgm)\n\n      while True:\n         try:\n            log = self.stderr[0].recv(4096).decode(errors='replace')\n         except (BlockingIOError):\n            log = b''\n\n         if len(log) > 0:\n            if log.count(\"DISCONNECT\") >= 1:\n               break\n\n      return log.rstrip(\"DISCONNECT\")", "response": "Disconnects an IOM session and returns a string containing the disconnect log"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upload(self, localfile: str, remotefile: str, overwrite: bool = True, permission: str = '', **kwargs):\n      valid = self._sb.file_info(remotefile, quiet = True)\n\n      if valid is None:\n         remf = remotefile\n      else:\n         if valid == {}:\n            remf = remotefile + self._sb.hostsep + localfile.rpartition(os.sep)[2]\n         else:\n            remf = remotefile\n            if overwrite == False:\n               return {'Success' : False, \n                       'LOG'     : \"File \"+str(remotefile)+\" exists and overwrite was set to False. Upload was stopped.\"}\n\n      try:\n         fd = open(localfile, 'rb')\n      except OSError as e:\n         return {'Success' : False, \n                 'LOG'     : \"File \"+str(localfile)+\" could not be opened. Error was: \"+str(e)}\n\n      fsize = os.path.getsize(localfile)\n\n      if fsize > 0:\n         code = \"filename _sp_updn '\"+remf+\"' recfm=N permission='\"+permission+\"';\"\n         ll = self.submit(code, 'text')\n         log1 = ll['LOG']\n\n         self.stdin[0].send(str(fsize).encode()+b'tom says EOL=UPLOAD                          \\n')\n\n         while True:\n            buf  = fd.read1(32768)\n            sent = 0\n            send = len(buf)\n            blen = send\n            if blen == 0:\n               break\n            while send:\n               try:\n                  sent = 0\n                  sent = self.stdout[0].send(buf[blen-send:blen])\n               except (BlockingIOError):\n                  pass\n               send -= sent\n               \n         code = \"filename _sp_updn;\"\n      else:\n         log1 = ''\n         code = \"\"\"\n            filename _sp_updn '\"\"\"+remf+\"\"\"' recfm=F encoding=binary lrecl=1 permission='\"\"\"+permission+\"\"\"';\n            data _null_;\n            fid = fopen('_sp_updn', 'O');\n            if fid then\n               rc = fclose(fid);\n            run;\n            filename _sp_updn;\n            \"\"\"\n\n      ll2 = self.submit(code, 'text')\n      fd.close()\n\n      return {'Success' : True, \n              'LOG'     : log1+ll2['LOG']}", "response": "This method uploads a local file to the SAS server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dataframe2sasdata(self, df: '<Pandas Data Frame object>', table: str ='a', \n                         libref: str =\"\", keep_outer_quotes: bool=False):\n      \"\"\"\n      This method imports a Pandas Data Frame to a SAS Data Set, returning the SASdata object for the new Data Set.\n      df      - Pandas Data Frame to import to a SAS Data Set\n      table   - the name of the SAS Data Set to create\n      libref  - the libref for the SAS Data Set being created. Defaults to WORK, or USER if assigned\n      keep_outer_quotes - for character columns, have SAS keep any outer quotes instead of stripping them off.\n      \"\"\"\n      input  = \"\"\n      card   = \"\"\n      format = \"\"\n      length = \"\"\n      dts    = []\n      ncols  = len(df.columns)\n\n      for name in range(ncols):\n         input += \"'\"+str(df.columns[name])+\"'n \"\n         if df.dtypes[df.columns[name]].kind in ('O','S','U','V'):\n            try:\n               col_l = df[df.columns[name]].astype(str).apply(self._getbytelen).max()\n            except Exception as e:\n               print(\"Transcoding error encountered.\")\n               print(\"DataFrame contains characters that can't be transcoded into the SAS session encoding.\\n\"+str(e))\n               return None\n            if col_l == 0:\n               col_l = 8\n            length += \" '\"+str(df.columns[name])+\"'n $\"+str(col_l)\n            if keep_outer_quotes:\n               input  += \"~ \"\n            dts.append('C')\n         else:\n            if df.dtypes[df.columns[name]].kind in ('M'):\n               length += \" '\"+str(df.columns[name])+\"'n 8\"\n               input  += \":E8601DT26.6 \"\n               format += \"'\"+str(df.columns[name])+\"'n E8601DT26.6 \"\n               dts.append('D')\n            else:\n               length += \" '\"+str(df.columns[name])+\"'n 8\"\n               if df.dtypes[df.columns[name]] == 'bool':\n                  dts.append('B')\n               else:\n                  dts.append('N')\n\n      code = \"data \"\n      if len(libref):\n         code += libref+\".\"\n      code += table+\";\\n\"\n      if len(length):\n         code += \"length \"+length+\";\\n\"\n      if len(format):\n         code += \"format \"+format+\";\\n\"\n      code += \"infile datalines delimiter='03'x DSD STOPOVER;\\ninput @;\\nif _infile_ = '' then delete;\\ninput \"+input+\";\\ndatalines4;\"\n      self._asubmit(code, \"text\")\n\n      code = \"\"\n      for row in df.itertuples(index=False):\n         card  = \"\"\n         for col in range(ncols):\n            var = str(row[col])\n\n            if   dts[col] == 'N' and var == 'nan':\n               var = '.'\n            elif dts[col] == 'C' and var == 'nan':\n               var = ' '\n            elif dts[col] == 'B':\n               var = str(int(row[col]))\n            elif dts[col] == 'D':\n               if var == 'nan':\n                  var = '.'\n               else:\n                  var = str(row[col].to_datetime64())[:26]\n\n            card += var\n            if col < (ncols-1):\n               card += chr(3)\n         code += card+\"\\n\"\n         if len(code) > 4000:\n            self._asubmit(code, \"text\")\n            code = \"\"\n\n      self._asubmit(code+\";;;;\", \"text\")\n      ll = self.submit(\"run;\", 'text')\n      return", "response": "This method imports a Pandas Data Frame into a SAS Data Set and returns a SASdata object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a PROC TABULATE statement in canonical SASpy way.", "response": "def execute_table(self, _output_type, **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        executes a PROC TABULATE statement \n\n        You must specify an output type to use this method, of 'HTML', 'text', or 'Pandas'.\n        There are three convenience functions for generating specific output; see:\n            .text_table()\n            .table()\n            .to_dataframe()\n\n        :param _output_type: style of output to use\n        :param left: the query for the left side of the table\n        :param top: the query for the top of the table\n        :return:\n        \"\"\"\n\n        left = kwargs.pop('left', None)\n        top = kwargs.pop('top', None)\n        sets = dict(classes=set(), vars=set())\n        left._gather(sets)\n        if top: top._gather(sets)\n\n        table = top \\\n                and '%s, %s' % (str(left), str(top)) \\\n                or str(left)\n\n        proc_kwargs = dict(\n            cls=' '.join(sets['classes']),\n            var=' '.join(sets['vars']),\n            table=table\n        )\n\n        # permit additional valid options if passed; for now, just 'where'\n        proc_kwargs.update(kwargs)\n\n        # we can't easily use the SASProcCommons approach for submiting, \n        # since this is merely an output / display proc for us;\n        # but we can at least use it to check valid options in the canonical saspy way\n        required_options = {'cls', 'var', 'table'}\n        allowed_options = {'cls', 'var', 'table', 'where'}\n        verifiedKwargs = sp.sasproccommons.SASProcCommons._stmt_check(self, required_options, allowed_options,\n                                                                      proc_kwargs)\n\n        if (_output_type == 'Pandas'):\n            # for pandas, use the out= directive\n            code = \"proc tabulate data=%s.%s %s out=temptab;\\n\" % (\n            self.data.libref, self.data.table, self.data._dsopts())\n        else:\n            code = \"proc tabulate data=%s.%s %s;\\n\" % (self.data.libref, self.data.table, self.data._dsopts())\n\n        # build the code\n        for arg, value in verifiedKwargs.items():\n            code += \"  %s %s;\\n\" % (arg == 'cls' and 'class' or arg, value)\n        code += \"run;\"\n\n        # teach_me_SAS\n        if self.sas.nosub:\n            print(code)\n            return\n\n        # submit the code\n        ll = self.data._is_valid()\n\n        if _output_type == 'HTML':\n            if not ll:\n                html = self.data.HTML\n                self.data.HTML = 1\n                ll = self.sas._io.submit(code)\n                self.data.HTML = html\n            if not self.sas.batch:\n                self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n                check, errorMsg = self.data._checkLogForError(ll['LOG'])\n                if not check:\n                    raise ValueError(\"Internal code execution failed: \" + errorMsg)\n            else:\n                return ll\n\n        elif _output_type == 'text':\n            if not ll:\n                html = self.data.HTML\n                self.data.HTML = 1\n                ll = self.sas._io.submit(code, 'text')\n                self.data.HTML = html\n            print(ll['LST'])\n            return\n\n        elif _output_type == 'Pandas':\n            return self.to_nested_dataframe(code)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a section a subsection and some text to the document.", "response": "def fill_document(doc):\n    \"\"\"Add a section, a subsection and some text to the document.\n\n    :param doc: the document\n    :type doc: :class:`pylatex.document.Document` instance\n    \"\"\"\n    with doc.create(Section('A section')):\n        doc.append('Some regular text and some ')\n        doc.append(italic('italic text. '))\n\n        with doc.create(Subsection('A subsection')):\n            doc.append('Also some crazy characters: $&#{}')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dumps_content(self, **kwargs):\n\n        return dumps_list(self, escape=self.escape,\n                          token=self.content_separator, **kwargs)", "response": "Represent the container as a string in LaTeX syntax."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes sure packages get propagated.", "response": "def _propagate_packages(self):\n        \"\"\"Make sure packages get propagated.\"\"\"\n\n        for item in self.data:\n            if isinstance(item, LatexObject):\n                if isinstance(item, Container):\n                    item._propagate_packages()\n                for p in item.packages:\n                    self.packages.add(p)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a LaTeX object to the current container", "response": "def create(self, child):\n        \"\"\"Add a LaTeX object to current container, context-manager style.\n\n        Args\n        ----\n        child: `~.Container`\n            An object to be added to the current container\n        \"\"\"\n\n        prev_data = self.data\n        self.data = child.data  # This way append works appends to the child\n\n        yield child  # allows with ... as to be used as well\n\n        self.data = prev_data\n        self.append(child)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrepresent the environment as a string in LaTeX syntax.", "response": "def dumps(self):\n        \"\"\"Represent the environment as a string in LaTeX syntax.\n\n        Returns\n        -------\n        str\n            A LaTeX string representing the environment.\n        \"\"\"\n\n        content = self.dumps_content()\n        if not content.strip() and self.omit_if_empty:\n            return ''\n\n        string = ''\n\n        # Something other than None needs to be used as extra arguments, that\n        # way the options end up behind the latex_name argument.\n        if self.arguments is None:\n            extra_arguments = Arguments()\n        else:\n            extra_arguments = self.arguments\n\n        begin = Command('begin', self.start_arguments, self.options,\n                        extra_arguments=extra_arguments)\n        begin.arguments._positional_args.insert(0, self.latex_name)\n        string += begin.dumps() + self.content_separator\n\n        string += content + self.content_separator\n\n        string += Command('end', self.latex_name).dumps()\n\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dumps(self):\n\n        content = self.dumps_content()\n\n        if not content.strip() and self.omit_if_empty:\n            return ''\n\n        string = ''\n\n        start = Command(self.latex_name, arguments=self.arguments,\n                        options=self.options)\n\n        string += start.dumps() + '{ \\n'\n\n        if content != '':\n            string += content + '\\n}'\n        else:\n            string += '}'\n\n        return string", "response": "rConvert the container to a string in latex syntax."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrepresents the section as a string in LaTeX syntax.", "response": "def dumps(self):\n        \"\"\"Represent the section as a string in LaTeX syntax.\n\n        Returns\n        -------\n        str\n\n        \"\"\"\n\n        if not self.numbering:\n            num = '*'\n        else:\n            num = ''\n\n        string = Command(self.latex_name + num, self.title).dumps()\n        if self.label is not None:\n            string += '%\\n' + self.label.dumps()\n        string += '%\\n' + self.dumps_content()\n\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_table_width(table_spec):\n\n    # Remove things like {\\bfseries}\n    cleaner_spec = re.sub(r'{[^}]*}', '', table_spec)\n\n    # Remove X[] in tabu environments so they dont interfere with column count\n    cleaner_spec = re.sub(r'X\\[(.*?(.))\\]', r'\\2', cleaner_spec)\n    spec_counter = Counter(cleaner_spec)\n\n    return sum(spec_counter[l] for l in COLUMN_LETTERS)", "response": "Calculate the width of a table based on its LaTeX column specification."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrepresenting the content of the table in LaTeX syntax.", "response": "def dumps_content(self, **kwargs):\n        r\"\"\"Represent the content of the tabular in LaTeX syntax.\n\n        This adds the top and bottomrule when using a booktabs style tabular.\n\n        Args\n        ----\n        \\*\\*kwargs:\n            Arguments that can be passed to `~.dumps_list`\n\n        Returns\n        -------\n        string:\n            A LaTeX string representing the\n        \"\"\"\n\n        content = ''\n        if self.booktabs:\n            content += '\\\\toprule%\\n'\n\n        content += super().dumps_content(**kwargs)\n\n        if self.booktabs:\n            content += '\\\\bottomrule%\\n'\n\n        return NoEscape(content)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_hline(self, start=None, end=None, *, color=None,\n                  cmidruleoption=None):\n        r\"\"\"Add a horizontal line to the table.\n\n        Args\n        ----\n        start: int\n            At what cell the line should begin\n        end: int\n            At what cell the line should end\n        color: str\n            The hline color.\n        cmidruleoption: str\n            The option to be used for the booktabs cmidrule, i.e. the ``x`` in\n            ``\\cmidrule(x){1-3}``.\n        \"\"\"\n        if self.booktabs:\n            hline = 'midrule'\n            cline = 'cmidrule'\n            if cmidruleoption is not None:\n                cline += '(' + cmidruleoption + ')'\n        else:\n            hline = 'hline'\n            cline = 'cline'\n\n        if color is not None:\n            if not self.color:\n                self.packages.append(Package('xcolor', options='table'))\n                self.color = True\n            color_command = Command(command=\"arrayrulecolor\", arguments=color)\n            self.append(color_command)\n\n        if start is None and end is None:\n            self.append(Command(hline))\n        else:\n            if start is None:\n                start = 1\n            elif end is None:\n                end = self.width\n\n            self.append(Command(cline,\n                                dumps_list([start, NoEscape('-'), end])))", "response": "r Add a horizontal line to the table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a row of cells to the table.", "response": "def add_row(self, *cells, color=None, escape=None, mapper=None,\n                strict=True):\n        \"\"\"Add a row of cells to the table.\n\n        Args\n        ----\n        cells: iterable, such as a `list` or `tuple`\n            There's two ways to use this method. The first method is to pass\n            the content of each cell as a separate argument. The second method\n            is to pass a single argument that is an iterable that contains each\n            contents.\n        color: str\n            The name of the color used to highlight the row\n        mapper: callable or `list`\n            A function or a list of functions that should be called on all\n            entries of the list after converting them to a string,\n            for instance bold\n        strict: bool\n            Check for correct count of cells in row or not.\n        \"\"\"\n\n        if len(cells) == 1 and _is_iterable(cells):\n            cells = cells[0]\n\n        if escape is None:\n            escape = self.escape\n\n        # Propagate packages used in cells\n        for c in cells:\n            if isinstance(c, LatexObject):\n                for p in c.packages:\n                    self.packages.add(p)\n\n        # Count cell contents\n        cell_count = 0\n\n        for c in cells:\n            if isinstance(c, MultiColumn):\n                cell_count += c.size\n            else:\n                cell_count += 1\n\n        if strict and cell_count != self.width:\n            msg = \"Number of cells added to table ({}) \" \\\n                \"did not match table width ({})\".format(cell_count, self.width)\n            raise TableRowSizeError(msg)\n\n        if color is not None:\n            if not self.color:\n                self.packages.append(Package(\"xcolor\", options='table'))\n                self.color = True\n            color_command = Command(command=\"rowcolor\", arguments=color)\n            self.append(color_command)\n\n        self.append(dumps_list(cells, escape=escape, token='&',\n                    mapper=mapper) + NoEscape(r'\\\\'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrepresenting the multicolumn as a string in LaTeX syntax.", "response": "def dumps(self):\n        \"\"\"Represent the multicolumn as a string in LaTeX syntax.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        args = [self.size, self.align, self.dumps_content()]\n        string = Command(self.latex_name, args).dumps()\n\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dumps(self):\n\n        _s = super().dumps()\n\n        # Tabu tables support a unusual syntax:\n        # \\begin{tabu} spread 0pt {<col format...>}\n        #\n        # Since this syntax isn't common, it doesn't make\n        # sense to support it in the baseclass (e.g., Environment)\n        # rather, here we fix the LaTeX string post-hoc\n        if self._preamble:\n            if _s.startswith(r\"\\begin{longtabu}\"):\n                _s = _s[:16] + self._preamble + _s[16:]\n            elif _s.startswith(r\"\\begin{tabu}\"):\n                _s = _s[:12] + self._preamble + _s[12:]\n            else:\n                raise TableError(\"Can't apply preamble to Tabu table \"\n                                 \"(unexpected initial command sequence)\")\n\n        return _s", "response": "Turn the tabu object into a string in Latex format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrend the table header which will appear on every page.", "response": "def end_table_header(self):\n        r\"\"\"End the table header which will appear on every page.\"\"\"\n\n        if self.header:\n            msg = \"Table already has a header\"\n            raise TableError(msg)\n\n        self.header = True\n\n        self.append(Command(r'endhead'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef end_table_footer(self):\n\n        if self.foot:\n            msg = \"Table already has a foot\"\n            raise TableError(msg)\n\n        self.foot = True\n\n        self.append(Command('endfoot'))", "response": "r End the table foot which will appear on every page."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef escape_latex(s):\n\n    if isinstance(s, NoEscape):\n        return s\n\n    return NoEscape(''.join(_latex_special_chars.get(c, c) for c in str(s)))", "response": "r Escape characters that are special in latex."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfixes the filename for use in LaTeX.", "response": "def fix_filename(path):\n    r\"\"\"Fix filenames for use in LaTeX.\n\n    Latex has problems if there are one or more points in the filename, thus\n    'abc.def.jpg' will be changed to '{abc.def}.jpg'\n\n    Args\n    ----\n    filename : str\n        The filen name to be changed.\n\n    Returns\n    -------\n    str\n        The new filename.\n\n    Examples\n    --------\n    >>> fix_filename(\"foo.bar.pdf\")\n    '{foo.bar}.pdf'\n    >>> fix_filename(\"/etc/local/foo.bar.pdf\")\n    '/etc/local/{foo.bar}.pdf'\n    >>> fix_filename(\"/etc/local/foo.bar.baz/document.pdf\")\n    '/etc/local/foo.bar.baz/document.pdf'\n    >>> fix_filename(\"/etc/local/foo.bar.baz/foo~1/document.pdf\")\n    '\\detokenize{/etc/local/foo.bar.baz/foo~1/document.pdf}'\n    \"\"\"\n\n    path_parts = path.split('/' if os.name == 'posix' else '\\\\')\n    dir_parts = path_parts[:-1]\n\n    filename = path_parts[-1]\n    file_parts = filename.split('.')\n\n    if len(file_parts) > 2:\n        filename = '{' + '.'.join(file_parts[0:-1]) + '}.' + file_parts[-1]\n\n    dir_parts.append(filename)\n    fixed_path = '/'.join(dir_parts)\n\n    if '~' in fixed_path:\n        fixed_path = r'\\detokenize{' + fixed_path + '}'\n\n    return fixed_path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dumps_list(l, *, escape=True, token='%\\n', mapper=None, as_content=True):\n    strings = (_latex_item_to_string(i, escape=escape, as_content=as_content)\n               for i in l)\n\n    if mapper is not None:\n        if not isinstance(mapper, list):\n            mapper = [mapper]\n\n        for m in mapper:\n            strings = [m(s) for s in strings]\n        strings = [_latex_item_to_string(s) for s in strings]\n\n    return NoEscape(token.join(strings))", "response": "r Try to generate a LaTeX string of a list that can contain anything."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _latex_item_to_string(item, *, escape=False, as_content=False):\n\n    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        else:\n            return item.dumps()\n    elif not isinstance(item, str):\n        item = str(item)\n\n    if escape:\n        item = escape_latex(item)\n\n    return item", "response": "Use the render method when possible otherwise uses str."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bold(s, *, escape=True):\n\n    if escape:\n        s = escape_latex(s)\n\n    return NoEscape(r'\\textbf{' + s + '}')", "response": "r Make a string appear bold in LaTeX formatting."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef auto_change_docstring(app, what, name, obj, options, lines):\n    if what == 'module' and name.startswith('pylatex'):\n        lines.insert(0, len(name) * '=')\n        lines.insert(0, name)\n\n    hits = 0\n    for i, line in enumerate(lines.copy()):\n        if line.endswith('\\\\'):\n            lines[i - hits] += lines.pop(i + 1 - hits)\n            hits += 1", "response": "r Make some automatic changes to docstrings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_invalid_char(s):\n\n    s = ''.join([i if ord(i) >= 32 and ord(i) < 127 else '' for i in s])\n    s = s.translate(dict.fromkeys(map(ord, \"_%~#\\\\{}\\\":\")))\n    return s", "response": "Remove invalid and dangerous characters from a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds an item to the list.", "response": "def add_item(self, label, s):\n        \"\"\"Add an item to the list.\n\n        Args\n        ----\n        label: str\n            Description of the item.\n        s: str or `~.LatexObject`\n            The item itself.\n        \"\"\"\n        self.append(Command('item', options=label))\n        self.append(s)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_image(self, filename, *, width=NoEscape(r'0.8\\textwidth'),\n                  placement=NoEscape(r'\\centering')):\n        \"\"\"Add an image to the figure.\n\n        Args\n        ----\n        filename: str\n            Filename of the image.\n        width: str\n            The width of the image\n        placement: str\n            Placement of the figure, `None` is also accepted.\n\n        \"\"\"\n\n        if width is not None:\n            if self.escape:\n                width = escape_latex(width)\n\n            width = 'width=' + str(width)\n\n        if placement is not None:\n            self.append(placement)\n\n        self.append(StandAloneGraphic(image_options=width,\n                                      filename=fix_filename(filename)))", "response": "Add an image to the figure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the plot. Returns ------- str The basename with which the plot has been saved.", "response": "def _save_plot(self, *args, extension='pdf', **kwargs):\n        \"\"\"Save the plot.\n\n        Returns\n        -------\n        str\n            The basename with which the plot has been saved.\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        tmp_path = make_temp_dir()\n        filename = '{}.{}'.format(str(uuid.uuid4()), extension.strip('.'))\n        filepath = posixpath.join(tmp_path, filename)\n\n        plt.savefig(filepath, *args, **kwargs)\n        return filepath"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the current Matplotlib plot to the figure.", "response": "def add_plot(self, *args, extension='pdf', **kwargs):\n        \"\"\"Add the current Matplotlib plot to the figure.\n\n        The plot that gets added is the one that would normally be shown when\n        using ``plt.show()``.\n\n        Args\n        ----\n        args:\n            Arguments passed to plt.savefig for displaying the plot.\n        extension : str\n            extension of image file indicating figure file type\n        kwargs:\n            Keyword arguments passed to plt.savefig for displaying the plot. In\n            case these contain ``width`` or ``placement``, they will be used\n            for the same purpose as in the add_image command. Namely the width\n            and placement of the generated plot in the LaTeX document.\n        \"\"\"\n\n        add_image_kwargs = {}\n\n        for key in ('width', 'placement'):\n            if key in kwargs:\n                add_image_kwargs[key] = kwargs.pop(key)\n\n        filename = self._save_plot(*args, extension=extension, **kwargs)\n\n        self.add_image(filename, **add_image_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_image(self, filename, *, width=NoEscape(r'\\linewidth'),\n                  placement=None):\n        \"\"\"Add an image to the subfigure.\n\n        Args\n        ----\n        filename: str\n            Filename of the image.\n        width: str\n            Width of the image in LaTeX terms.\n        placement: str\n            Placement of the figure, `None` is also accepted.\n        \"\"\"\n\n        super().add_image(filename, width=width, placement=placement)", "response": "Add an image to the subfigure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef escape(self):\n        if self._escape is not None:\n            return self._escape\n        if self._default_escape is not None:\n            return self._default_escape\n        return True", "response": "Determine whether or not to escape content of this class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _repr_values(self):\n        def getattr_better(obj, field):\n            try:\n                return getattr(obj, field)\n            except AttributeError as e:\n                try:\n                    return getattr(obj, '_' + field)\n                except AttributeError:\n                    raise e\n\n        return (getattr_better(self, attr) for attr in self._repr_attributes)", "response": "Return values that are to be shown in repr string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the attributes that should be part of the repr string.", "response": "def _repr_attributes(self):\n        \"\"\"Return attributes that should be part of the repr string.\"\"\"\n        if self._repr_attributes_override is None:\n            # Default to init arguments\n            attrs = getfullargspec(self.__init__).args[1:]\n            mapping = self._repr_attributes_mapping\n            if mapping:\n                attrs = [mapping[a] if a in mapping else a for a in attrs]\n            return attrs\n\n        return self._repr_attributes_override"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the name of the class used in LaTeX.", "response": "def latex_name(self):\n        \"\"\"Return the name of the class used in LaTeX.\n\n        It can be `None` when the class doesn't have a name.\n        \"\"\"\n        star = ('*' if self._star_latex_name else '')\n        if self._latex_name is not None:\n            return self._latex_name + star\n        return self.__class__.__name__.lower() + star"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_tex(self, filepath):\n\n        with open(filepath + '.tex', 'w', encoding='utf-8') as newf:\n            self.dump(newf)", "response": "Generate a. tex file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dumps_as_content(self):\n\n        string = self.dumps()\n\n        if self.separate_paragraph or self.begin_paragraph:\n            string = '\\n\\n' + string.lstrip('\\n')\n\n        if self.separate_paragraph or self.end_paragraph:\n            string = string.rstrip('\\n') + '\\n\\n'\n\n        return string", "response": "Create a string representation of the object as content."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrepresenting the document as a string in LaTeX syntax.", "response": "def dumps(self):\n        \"\"\"Represent the document as a string in LaTeX syntax.\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        head = self.documentclass.dumps() + '%\\n'\n        head += self.dumps_packages() + '%\\n'\n        head += dumps_list(self.variables) + '%\\n'\n        head += dumps_list(self.preamble) + '%\\n'\n\n        return head + '%\\n' + super().dumps()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_pdf(self, filepath=None, *, clean=True, clean_tex=True,\n                     compiler=None, compiler_args=None, silent=True):\n        \"\"\"Generate a pdf file from the document.\n\n        Args\n        ----\n        filepath: str\n            The name of the file (without .pdf), if it is `None` the\n            ``default_filepath`` attribute will be used.\n        clean: bool\n            Whether non-pdf files created that are created during compilation\n            should be removed.\n        clean_tex: bool\n            Also remove the generated tex file.\n        compiler: `str` or `None`\n            The name of the LaTeX compiler to use. If it is None, PyLaTeX will\n            choose a fitting one on its own. Starting with ``latexmk`` and then\n            ``pdflatex``.\n        compiler_args: `list` or `None`\n            Extra arguments that should be passed to the LaTeX compiler. If\n            this is None it defaults to an empty list.\n        silent: bool\n            Whether to hide compiler output\n        \"\"\"\n\n        if compiler_args is None:\n            compiler_args = []\n\n        filepath = self._select_filepath(filepath)\n        filepath = os.path.join('.', filepath)\n\n        cur_dir = os.getcwd()\n        dest_dir = os.path.dirname(filepath)\n        basename = os.path.basename(filepath)\n\n        if basename == '':\n            basename = 'default_basename'\n\n        os.chdir(dest_dir)\n\n        self.generate_tex(basename)\n\n        if compiler is not None:\n            compilers = ((compiler, []),)\n        else:\n            latexmk_args = ['--pdf']\n\n            compilers = (\n                ('latexmk', latexmk_args),\n                ('pdflatex', [])\n            )\n\n        main_arguments = ['--interaction=nonstopmode', basename + '.tex']\n\n        os_error = None\n\n        for compiler, arguments in compilers:\n            command = [compiler] + arguments + compiler_args + main_arguments\n\n            try:\n                output = subprocess.check_output(command,\n                                                 stderr=subprocess.STDOUT)\n            except (OSError, IOError) as e:\n                # Use FileNotFoundError when python 2 is dropped\n                os_error = e\n\n                if os_error.errno == errno.ENOENT:\n                    # If compiler does not exist, try next in the list\n                    continue\n                raise\n            except subprocess.CalledProcessError as e:\n                # For all other errors print the output and raise the error\n                print(e.output.decode())\n                raise\n            else:\n                if not silent:\n                    print(output.decode())\n\n            if clean:\n                try:\n                    # Try latexmk cleaning first\n                    subprocess.check_output(['latexmk', '-c', basename],\n                                            stderr=subprocess.STDOUT)\n                except (OSError, IOError, subprocess.CalledProcessError) as e:\n                    # Otherwise just remove some file extensions.\n                    extensions = ['aux', 'log', 'out', 'fls',\n                                  'fdb_latexmk']\n\n                    for ext in extensions:\n                        try:\n                            os.remove(basename + '.' + ext)\n                        except (OSError, IOError) as e:\n                            # Use FileNotFoundError when python 2 is dropped\n                            if e.errno != errno.ENOENT:\n                                raise\n                rm_temp_dir()\n\n            if clean_tex:\n                os.remove(basename + '.tex')  # Remove generated tex file\n\n            # Compilation has finished, so no further compilers have to be\n            # tried\n            break\n\n        else:\n            # Notify user that none of the compilers worked.\n            raise(CompilerError(\n                'No LaTex compiler was found\\n' +\n                'Either specify a LaTex compiler ' +\n                'or make sure you have latexmk or pdfLaTex installed.'\n            ))\n\n        os.chdir(cur_dir)", "response": "Generate a pdf file from the document."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a choice between filepath and self. default_filepath.", "response": "def _select_filepath(self, filepath):\n        \"\"\"Make a choice between ``filepath`` and ``self.default_filepath``.\n\n        Args\n        ----\n        filepath: str\n            the filepath to be compared with ``self.default_filepath``\n\n        Returns\n        -------\n        str\n            The selected filepath\n        \"\"\"\n\n        if filepath is None:\n            return self.default_filepath\n        else:\n            if os.path.basename(filepath) == '':\n                filepath = os.path.join(filepath, os.path.basename(\n                    self.default_filepath))\n            return filepath"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_color(self, name, model, description):\n\n        if self.color is False:\n            self.packages.append(Package(\"color\"))\n            self.color = True\n\n        self.preamble.append(Command(\"definecolor\", arguments=[name,\n                                                               model,\n                                                               description]))", "response": "r Add a color that can be used throughout the document."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef change_length(self, parameter, value):\n\n        self.preamble.append(UnsafeCommand('setlength',\n                                           arguments=[parameter, value]))", "response": "r Change the length of a certain parameter to a certain value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef change(self, **kwargs):\n\n        old_attrs = {}\n\n        for k, v in kwargs.items():\n            old_attrs[k] = getattr(self, k, v)\n            setattr(self, k, v)\n\n        yield self  # allows with ... as ...\n\n        for k, v in old_attrs.items():\n            setattr(self, k, v)", "response": "Override some attributes of the config in a specific context."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a hashable key representing the command.", "response": "def __key(self):\n        \"\"\"Return a hashable key, representing the command.\n\n        Returns\n        -------\n        tuple\n        \"\"\"\n\n        return (self.latex_name, self.arguments, self.options,\n                self.extra_arguments)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrepresent the command as a string in LaTeX syntax. Returns the LaTeX formatted command", "response": "def dumps(self):\n        \"\"\"Represent the command as a string in LaTeX syntax.\n\n        Returns\n        -------\n        str\n            The LaTeX formatted command\n        \"\"\"\n\n        options = self.options.dumps()\n        arguments = self.arguments.dumps()\n\n        if self.extra_arguments is None:\n            return r'\\{command}{options}{arguments}'\\\n                .format(command=self.latex_name, options=options,\n                        arguments=arguments)\n\n        extra_arguments = self.extra_arguments.dumps()\n\n        return r'\\{command}{arguments}{options}{extra_arguments}'\\\n            .format(command=self.latex_name, arguments=arguments,\n                    options=options, extra_arguments=extra_arguments)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat the parameters. The formatting is done using the three arguments suplied to this function. Arguments --------- prefix: str separator: str suffix: str Returns ------- str", "response": "def _format_contents(self, prefix, separator, suffix):\n        \"\"\"Format the parameters.\n\n        The formatting is done using the three arguments suplied to this\n        function.\n\n        Arguments\n        ---------\n        prefix: str\n        separator: str\n        suffix: str\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        params = self._list_args_kwargs()\n\n        if len(params) <= 0:\n            return ''\n\n        string = prefix + dumps_list(params, escape=self.escape,\n                                     token=separator) + suffix\n\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _list_args_kwargs(self):\n\n        params = []\n        params.extend(self._positional_args)\n        params.extend(['{k}={v}'.format(k=k, v=v) for k, v in\n                       self._key_value_args.items()])\n\n        return params", "response": "Make a list of strings representing al parameters.\n        Returns\n        ------- list\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dumps_content(self):\n\n        import numpy as np\n\n        string = ''\n        shape = self.matrix.shape\n\n        for (y, x), value in np.ndenumerate(self.matrix):\n            if x:\n                string += '&'\n            string += str(value)\n\n            if x == shape[1] - 1 and y != shape[0] - 1:\n                string += r'\\\\' + '%\\n'\n\n        super().dumps_content()\n\n        return string", "response": "Return a string representing the matrix in LaTeX syntax."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a TikZCoordinate object from a string representation.", "response": "def from_str(cls, coordinate):\n        \"\"\"Build a TikZCoordinate object from a string.\"\"\"\n\n        m = cls._coordinate_str_regex.match(coordinate)\n\n        if m is None:\n            raise ValueError('invalid coordinate string')\n\n        if m.group(1) == '++':\n            relative = True\n        else:\n            relative = False\n\n        return TikZCoordinate(\n            float(m.group(2)), float(m.group(4)), relative=relative)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dumps(self):\n\n        ret_str = []\n        ret_str.append(Command('node', options=self.options).dumps())\n\n        if self.handle is not None:\n            ret_str.append('({})'.format(self.handle))\n\n        if self._node_position is not None:\n            ret_str.append('at {}'.format(str(self._position)))\n\n        if self._node_text is not None:\n            ret_str.append('{{{text}}};'.format(text=self._node_text))\n        else:\n            ret_str.append('{};')\n\n        return ' '.join(ret_str)", "response": "Return string representation of the node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an anchor point of the node if it exists.", "response": "def get_anchor_point(self, anchor_name):\n        \"\"\"Return an anchor point of the node, if it exists.\"\"\"\n\n        if anchor_name in self._possible_anchors:\n            return TikZNodeAnchor(self.handle, anchor_name)\n        else:\n            try:\n                anchor = int(anchor_name.split('_')[1])\n            except:\n                anchor = None\n\n            if anchor is not None:\n                return TikZNodeAnchor(self.handle, str(anchor))\n\n        raise ValueError('Invalid anchor name: \"{}\"'.format(anchor_name))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dumps(self):\n\n        ret_str = self.path_type\n\n        if self.options is not None:\n            ret_str += self.options.dumps()\n\n        return ret_str", "response": "Return path command representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns representation of the path command.", "response": "def dumps(self):\n        \"\"\"Return representation of the path command.\"\"\"\n\n        ret_str = []\n        for item in self._arg_list:\n            if isinstance(item, TikZUserPath):\n                ret_str.append(item.dumps())\n            elif isinstance(item, TikZCoordinate):\n                ret_str.append(item.dumps())\n            elif isinstance(item, str):\n                ret_str.append(item)\n\n        return ' '.join(ret_str)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dumps(self):\n\n        ret_str = [Command('path', options=self.options).dumps()]\n\n        ret_str.append(self.path.dumps())\n\n        return ' '.join(ret_str) + ';'", "response": "Return a representation for the command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dumps(self):\n\n        string = Command('addplot', options=self.options).dumps()\n\n        if self.coordinates is not None:\n            string += ' coordinates {%\\n'\n\n            if self.error_bar is None:\n                for x, y in self.coordinates:\n                    # ie: \"(x,y)\"\n                    string += '(' + str(x) + ',' + str(y) + ')%\\n'\n\n            else:\n                for (x, y), (e_x, e_y) in zip(self.coordinates,\n                                              self.error_bar):\n                    # ie: \"(x,y) +- (e_x,e_y)\"\n                    string += '(' + str(x) + ',' + str(y) + \\\n                        ') +- (' + str(e_x) + ',' + str(e_y) + ')%\\n'\n\n            string += '};%\\n%\\n'\n\n        elif self.func is not None:\n            string += '{' + self.func + '};%\\n%\\n'\n\n        if self.name is not None:\n            string += Command('addlegendentry', self.name).dumps()\n\n        super().dumps()\n\n        return string", "response": "Represent the plot as a string in LaTeX syntax."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a section a subsection and some text to the document.", "response": "def fill_document(self):\n        \"\"\"Add a section, a subsection and some text to the document.\"\"\"\n        with self.create(Section('A section')):\n            self.append('Some regular text and some ')\n            self.append(italic('italic text. '))\n\n            with self.create(Subsection('A subsection')):\n                self.append('Also some crazy characters: $&#{}')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef constant_cost_withGradients(x):\n    return np.ones(x.shape[0])[:,None], np.zeros(x.shape)", "response": "Constant cost function used by default."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npredicting the time cost of evaluating the function at x.", "response": "def _cost_gp(self,x):\n        \"\"\"\n        Predicts the time cost of evaluating the function at x.\n        \"\"\"\n        m, _, _, _ = self.cost_model.predict_withGradients(x)\n        return np.exp(m)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cost_gp_withGradients(self,x):\n        m, _, dmdx, _= self.cost_model.predict_withGradients(x)\n        return np.exp(m), np.exp(m)*dmdx", "response": "Predicts the time cost and its gradient of evaluating the function at x."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the cost model for the time cost at the input locations.", "response": "def update_cost_model(self, x, cost_x):\n        \"\"\"\n        Updates the GP used to handle the cost.\n\n        param x: input of the GP for the cost model.\n        param x_cost: values of the time cost at the input locations.\n        \"\"\"\n\n        if self.cost_type == 'evaluation_time':\n            cost_evals = np.log(np.atleast_2d(np.asarray(cost_x)).T)\n\n            if self.num_updates == 0:\n                X_all = x\n                costs_all = cost_evals\n            else:\n                X_all = np.vstack((self.cost_model.model.X,x))\n                costs_all = np.vstack((self.cost_model.model.Y,cost_evals))\n\n            self.num_updates += 1\n            self.cost_model.updateModel(X_all, costs_all, None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _hammer_function_precompute(self,x0, L, Min, model):\n        if x0 is None: return None, None\n        if len(x0.shape)==1: x0 = x0[None,:]\n        m = model.predict(x0)[0]\n        pred = model.predict(x0)[1].copy()\n        pred[pred<1e-16] = 1e-16\n        s = np.sqrt(pred)\n        r_x0 = (m-Min)/L\n        s_x0 = s/L\n        r_x0 = r_x0.flatten()\n        s_x0 = s_x0.flatten()\n        return r_x0, s_x0", "response": "Pre - computes the parameters of a penalizer centered at x0."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _hammer_function(self, x,x0,r_x0, s_x0):\n        '''\n        Creates the function to define the exclusion zones\n        '''\n        return norm.logcdf((np.sqrt((np.square(np.atleast_2d(x)[:,None,:]-np.atleast_2d(x0)[None,:,:])).sum(-1))- r_x0)/s_x0)", "response": "Creates the function to define the exclusion zones"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _penalized_acquisition(self, x,  model, X_batch, r_x0, s_x0):\n        '''\n        Creates a penalized acquisition function using 'hammer' functions around the points collected in the batch\n\n        .. Note:: the penalized acquisition is always mapped to the log space. This way gradients can be computed additively and are more stable.\n        '''\n        fval = -self.acq.acquisition_function(x)[:,0]\n\n        if self.transform=='softplus':\n            fval_org = fval.copy()\n            fval[fval_org>=40.] = np.log(fval_org[fval_org>=40.])\n            fval[fval_org<40.] = np.log(np.log1p(np.exp(fval_org[fval_org<40.])))\n        elif self.transform=='none':\n            fval = np.log(fval+1e-50)\n\n        fval = -fval\n        if X_batch is not None:\n            h_vals = self._hammer_function(x, X_batch, r_x0, s_x0)\n            fval += -h_vals.sum(axis=-1)\n        return fval", "response": "Creates a penalized acquisition function using hammer functions around the points collected in the batch and returns the log - space of the penalized acquisition function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the Hammer function for the penalizer at any x.", "response": "def _d_hammer_function(self, x, X_batch, r_x0, s_x0):\n        \"\"\"\n        Computes the value of the penalizer (centered at x_0) at any x.\n        \"\"\"\n        dx = np.atleast_2d(x)[:,None,:]-np.atleast_2d(X_batch)[None,:,:]\n        nm = np.sqrt((np.square(dx)).sum(-1))\n        z = (nm- r_x0)/s_x0\n        h_func = norm.cdf(z)\n\n        d = 1./(s_x0*np.sqrt(2*np.pi)*h_func)*np.exp(-np.square(z)/2)/nm\n        d[h_func<1e-50] = 0.\n        d = d[:,:,None]\n        return d.sum(axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the acquisition function at x.", "response": "def acquisition_function(self, x):\n        \"\"\"\n        Returns the value of the acquisition function at x.\n        \"\"\"\n\n        return self._penalized_acquisition(x, self.model, self.X_batch, self.r_x0, self.s_x0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef d_acquisition_function(self, x):\n        x = np.atleast_2d(x)\n\n        if self.transform=='softplus':\n            fval = -self.acq.acquisition_function(x)[:,0]\n            scale = 1./(np.log1p(np.exp(fval))*(1.+np.exp(-fval)))\n        elif self.transform=='none':\n            fval = -self.acq.acquisition_function(x)[:,0]\n            scale = 1./fval\n        else:\n            scale = 1.\n\n        if self.X_batch is None:\n            _, grad_acq_x = self.acq.acquisition_function_withGradients(x)\n            return scale*grad_acq_x\n        else:\n            _, grad_acq_x = self.acq.acquisition_function_withGradients(x)\n            return scale*grad_acq_x  - self._d_hammer_function(x, self.X_batch, self.r_x0, self.s_x0)", "response": "Returns the gradient of the acquisition function at x."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the acquisition function and its gradient at x.", "response": "def acquisition_function_withGradients(self, x):\n        \"\"\"\n        Returns the acquisition function and its its gradient at x.\n        \"\"\"\n        aqu_x      = self.acquisition_function(x)\n        aqu_x_grad = self.d_acquisition_function(x)\n        return aqu_x, aqu_x_grad"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef acquisition_function(self,x):\n        f_acqu = self._compute_acq(x)\n        cost_x, _ = self.cost_withGradients(x)\n        return -(f_acqu*self.space.indicator_constraints(x))/cost_x", "response": "Returns the acquisition function of the given time series."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef acquisition_function_withGradients(self, x):\n        f_acqu,df_acqu = self._compute_acq_withGradients(x)\n        cost_x, cost_grad_x = self.cost_withGradients(x)\n        f_acq_cost = f_acqu/cost_x\n        df_acq_cost = (df_acqu*cost_x - f_acqu*cost_grad_x)/(cost_x**2)\n        return -f_acq_cost*self.space.indicator_constraints(x), -df_acq_cost*self.space.indicator_constraints(x)", "response": "Returns the acquisition function and the gradient of the acquisition and weights it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noptimize the acquisition function.", "response": "def optimize(self, duplicate_manager=None):\n        \"\"\"\n        Optimizes the acquisition function (uses a flag from the model to use gradients or not).\n        \"\"\"\n        if not self.analytical_gradient_acq:\n            out = self.optimizer.optimize(f=self.acquisition_function, duplicate_manager=duplicate_manager)\n        else:\n            out = self.optimizer.optimize(f=self.acquisition_function, f_df=self.acquisition_function_withGradients, duplicate_manager=duplicate_manager)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the parameters of the class that are required to evaluate the function.", "response": "def _update_parameters(self):\n        \"\"\"\n        Update parameters of the acquisition required to evaluate the function. In particular:\n            * Sample representer points repr_points\n            * Compute their log values repr_points_log\n            * Compute belief locations logP\n        \"\"\"\n        self.repr_points, self.repr_points_log = self.sampler.get_samples(self.num_repr_points, self.proposal_function, self.burn_in_steps)\n\n        if np.any(np.isnan(self.repr_points_log)) or np.any(np.isposinf(self.repr_points_log)):\n            raise RuntimeError(\"Sampler generated representer points with invalid log values: {}\".format(self.repr_points_log))\n\n        # Removing representer points that have 0 probability of being the minimum (corresponding to log probability being minus infinity)\n        idx_to_remove = np.where(np.isneginf(self.repr_points_log))[0]\n        if len(idx_to_remove) > 0:\n            idx = list(set(range(self.num_repr_points)) - set(idx_to_remove))\n            self.repr_points = self.repr_points[idx, :]\n            self.repr_points_log = self.repr_points_log[idx]\n\n        # We predict with the noise as we need to make sure that var is indeed positive definite.\n        mu, _ = self.model.predict(self.repr_points)\n        # we need a vector\n        mu = np.ndarray.flatten(mu)\n        var = self.model.predict_covariance(self.repr_points)\n        \n        self.logP, self.dlogPdMu, self.dlogPdSigma, self.dlogPdMudMu = epmgp.joint_min(mu, var, with_derivatives=True)\n        # add a second dimension to the array\n        self.logP = np.reshape(self.logP, (self.logP.shape[0], 1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _innovations(self, x):\n        \n        \n        '''\n        The notation differs from the paper. The representer points\n        play the role of x*, the test input x is X. The Omega term is applied\n        in the calling function _compute_acq. Another difference is that we\n        ignore the noise as in the original Matlab implementation:\n        https://github.com/ProbabilisticNumerics/entropy-search/blob/master/matlab/GP_innovation_local.m\n        '''\n        \n        # Get the standard deviation at x without noise\n        _, stdev_x = self.model.predict(x, with_noise=False)\n\n        # Compute the variance between the test point x and the representer points\n        sigma_x_rep = self.model.get_covariance_between_points(self.repr_points, x)\n        dm_rep = sigma_x_rep / stdev_x\n\n        # Compute the deterministic innovation for the variance\n        dv_rep = -dm_rep.dot(dm_rep.T)\n        return dm_rep, dv_rep", "response": "Compute the expected change in mean and variance at the representer points."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_integrated_acquisition(acquisition,x):\n    '''\n    Used to compute the acquisition function when samples of the hyper-parameters have been generated (used in GP_MCMC model).\n\n    :param acquisition: acquisition function with GpyOpt model type GP_MCMC.\n    :param x: location where the acquisition is evaluated.\n    '''\n\n    acqu_x = 0\n\n    for i in range(acquisition.model.num_hmc_samples):\n        acquisition.model.model.kern[:] = acquisition.model.hmc_samples[i,:]\n        acqu_x += acquisition.acquisition_function(x)\n\n    acqu_x = acqu_x/acquisition.model.num_hmc_samples\n    return acqu_x", "response": "Compute the acquisition function for the given acquisition function with GpyOpt model type GP_MCMC."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse to compute the acquisition function with gradients when samples of the hyper - parameters have been generated.", "response": "def compute_integrated_acquisition_withGradients(acquisition,x):\n    '''\n    Used to compute the acquisition function with gradients when samples of the hyper-parameters have been generated (used in GP_MCMC model).\n\n    :param acquisition: acquisition function with GpyOpt model type GP_MCMC.\n    :param x: location where the acquisition is evaluated.\n    '''\n\n    acqu_x = 0\n    d_acqu_x = 0\n\n    for i in range(acquisition.model.num_hmc_samples):\n        acquisition.model.model.kern[:] = acquisition.model.hmc_samples[i,:]\n        acqu_x_sample, d_acqu_x_sample = acquisition.acquisition_function_withGradients(x)\n        acqu_x += acqu_x_sample\n        d_acqu_x += d_acqu_x_sample\n\n    acqu_x = acqu_x/acquisition.model.num_hmc_samples\n    d_acqu_x = d_acqu_x/acquisition.model.num_hmc_samples\n\n    return acqu_x, d_acqu_x"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the best current guess from a vector.", "response": "def best_guess(f,X):\n    '''\n    Gets the best current guess from a vector.\n    :param f: function to evaluate.\n    :param X: locations.\n    '''\n    n = X.shape[0]\n    xbest = np.zeros(n)\n    for i in range(n):\n        ff = f(X[0:(i+1)])\n        xbest[i] = ff[np.argmin(ff)]\n    return xbest"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef samples_multidimensional_uniform(bounds,num_data):\n    '''\n    Generates a multidimensional grid uniformly distributed.\n    :param bounds: tuple defining the box constraints.\n    :num_data: number of data points to generate.\n\n    '''\n    dim = len(bounds)\n    Z_rand = np.zeros(shape=(num_data,dim))\n    for k in range(0,dim): Z_rand[:,k] = np.random.uniform(low=bounds[k][0],high=bounds[k][1],size=num_data)\n    return Z_rand", "response": "Generates a multidimensional grid uniformly distributed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_moments(model,x):\n    '''\n    Moments (mean and sdev.) of a GP model at x\n\n    '''\n    input_dim = model.X.shape[1]\n    x = reshape(x,input_dim)\n    fmin = min(model.predict(model.X)[0])\n    m, v = model.predict(x)\n    s = np.sqrt(np.clip(v, 0, np.inf))\n    return (m,s, fmin)", "response": "Returns the mean and sdev of a GP model at x"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_d_moments(model,x):\n    '''\n    Gradients with respect to x of the moments (mean and sdev.) of the GP\n    :param model: GPy model.\n    :param x: location where the gradients are evaluated.\n    '''\n    input_dim = model.input_dim\n    x = reshape(x,input_dim)\n    _, v = model.predict(x)\n    dmdx, dvdx = model.predictive_gradients(x)\n    dmdx = dmdx[:,:,0]\n    dsdx = dvdx / (2*np.sqrt(v))\n    return (dmdx, dsdx)", "response": "Returns the d_moments and dsdx of the GP\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_quantiles(acquisition_par, fmin, m, s):\n    '''\n    Quantiles of the Gaussian distribution useful to determine the acquisition function values\n    :param acquisition_par: parameter of the acquisition function\n    :param fmin: current minimum.\n    :param m: vector of means.\n    :param s: vector of standard deviations.\n    '''\n    if isinstance(s, np.ndarray):\n        s[s<1e-10] = 1e-10\n    elif s< 1e-10:\n        s = 1e-10\n    u = (fmin - m - acquisition_par)/s\n    phi = np.exp(-0.5 * u**2) / np.sqrt(2*np.pi)\n    Phi = 0.5 * erfc(-u / np.sqrt(2))\n    return (phi, Phi, u)", "response": "Returns the quantiles of the Gaussian distribution useful to determine the acquisition function values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a vector whose components i are the minimum or maximum of Y[:i ]", "response": "def best_value(Y,sign=1):\n    '''\n    Returns a vector whose components i are the minimum (default) or maximum of Y[:i]\n    '''\n    n = Y.shape[0]\n    Y_best = np.ones(n)\n    for i in range(n):\n        if sign == 1:\n            Y_best[i]=Y[:(i+1)].min()\n        else:\n            Y_best[i]=Y[:(i+1)].max()\n    return Y_best"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spawn(f):\n    '''\n    Function for parallel evaluation of the acquisition function\n    '''\n    def fun(pipe,x):\n        pipe.send(f(x))\n        pipe.close()\n    return fun", "response": "Returns a function that runs the given function in parallel."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the evaluation of a function f and the time per evaluation", "response": "def evaluate_function(f,X):\n    '''\n    Returns the evaluation of a function *f* and the time per evaluation\n    '''\n    num_data, dim_data = X.shape\n    Y_eval = np.zeros((num_data, dim_data))\n    Y_time = np.zeros((num_data, 1))\n    for i in range(num_data):\n        time_zero = time.time()\n        Y_eval[i,:] = f(X[i,:])\n        Y_time[i,:] = time.time() - time_zero\n    return Y_eval, Y_time"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef values_to_array(input_values):\n    '''\n    Transforms a values of int, float and tuples to a column vector numpy array\n    '''\n    if type(input_values)==tuple:\n        values = np.array(input_values).reshape(-1,1)\n    elif type(input_values) == np.ndarray:\n        values = np.atleast_2d(input_values)\n    elif type(input_values)==int or type(input_values)==float or type(np.int64):\n        values = np.atleast_2d(np.array(input_values))\n    else:\n        print('Type to transform not recognized')\n    return values", "response": "Transforms a values of int float and tuples to a column vector numpy array\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_values(values1,values2):\n    '''\n    Merges two numpy arrays by calculating all possible combinations of rows\n    '''\n    array1 = values_to_array(values1)\n    array2 = values_to_array(values2)\n\n    if array1.size == 0:\n        return array2\n    if array2.size == 0:\n        return array1\n\n    merged_array = []\n    for row_array1 in array1:\n        for row_array2 in array2:\n            merged_row = np.hstack((row_array1,row_array2))\n            merged_array.append(merged_row)\n    return np.atleast_2d(merged_array)", "response": "Merges two numpy arrays by calculating all possible combinations of rows\n    and returns the resulting array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize(Y, normalization_type='stats'):\n    Y = np.asarray(Y, dtype=float)\n\n    if np.max(Y.shape) != Y.size:\n        raise NotImplementedError('Only 1-dimensional arrays are supported.')\n\n    # Only normalize with non null sdev (divide by zero). For only one\n    # data point both std and ptp return 0.\n    if normalization_type == 'stats':\n        Y_norm = Y - Y.mean()\n        std = Y.std()\n        if std > 0:\n            Y_norm /= std\n    elif normalization_type == 'maxmin':\n        Y_norm = Y - Y.min()\n        y_range = np.ptp(Y)\n        if y_range > 0:\n            Y_norm /= y_range\n            # A range of [-1, 1] is more natural for a zero-mean GP\n            Y_norm = 2 * (Y_norm - 0.5)\n    else:\n        raise ValueError('Unknown normalization type: {}'.format(normalization_type))\n\n    return Y_norm", "response": "Normalizes the vector Y using statistics or its range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select_acquisition(name):\n    '''\n    Acquisition selector\n    '''\n    if name == 'EI':\n        return AcquisitionEI\n    elif name == 'EI_MCMC':\n        return AcquisitionEI_MCMC\n    elif name == 'LCB':\n        return AcquisitionLCB\n    elif name == 'LCB_MCMC':\n        return AcquisitionLCB_MCMC\n    elif name == 'MPI':\n        return AcquisitionMPI\n    elif name == 'MPI_MCMC':\n        return AcquisitionMPI_MCMC\n    elif name == 'LP':\n        return AcquisitionLP\n    elif name == 'ES':\n        return AcquisitionEntropySearch\n    else:\n        raise Exception('Invalid acquisition selected.')", "response": "Select an acquisition by its name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multigrid(bounds, points_count):\n    if len(bounds)==1:\n        return np.linspace(bounds[0][0], bounds[0][1], points_count).reshape(points_count, 1)\n    x_grid_rows = np.meshgrid(*[np.linspace(b[0], b[1], points_count) for b in bounds])\n    x_grid_columns = np.vstack([x.flatten(order='F') for x in x_grid_rows]).T\n    return x_grid_columns", "response": "Generates a multidimensional lattice for a set of points."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_samples(self, init_points_count):\n\n        init_points_count = self._adjust_init_points_count(init_points_count)\n        samples = np.empty((init_points_count, self.space.dimensionality))\n\n        # Use random design to fill non-continuous variables\n        random_design = RandomDesign(self.space)\n        random_design.fill_noncontinous_variables(samples)\n\n        if self.space.has_continuous():\n            X_design = multigrid(self.space.get_continuous_bounds(), self.data_per_dimension)\n            samples[:,self.space.get_continuous_dims()] = X_design\n\n        return samples", "response": "This method returns a list of samples for the current set of entries."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_samples_with_constraints(self, init_points_count):\n        samples = np.empty((0, self.space.dimensionality))\n\n        while samples.shape[0] < init_points_count:\n            domain_samples = self.get_samples_without_constraints(init_points_count)\n            valid_indices = (self.space.indicator_constraints(domain_samples) == 1).flatten()\n            if sum(valid_indices) > 0:\n                valid_samples = domain_samples[valid_indices,:]\n                samples = np.vstack((samples,valid_samples))\n\n        return samples[0:init_points_count,:]", "response": "Draw random samples and only save those that satisfy constraints\n        Finish when required number of samples is generated"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fill_noncontinous_variables(self, samples):\n        init_points_count = samples.shape[0]\n\n        for (idx, var) in enumerate(self.space.space_expanded):\n            if isinstance(var, DiscreteVariable) or isinstance(var, CategoricalVariable) :\n                sample_var = np.atleast_2d(np.random.choice(var.domain, init_points_count))\n                samples[:,idx] = sample_var.flatten()\n\n            # sample in the case of bandit variables\n            elif isinstance(var, BanditVariable):\n                # Bandit variable is represented by a several adjacent columns in the samples array\n                idx_samples = np.random.randint(var.domain.shape[0], size=init_points_count)\n                bandit_idx = np.arange(idx, idx + var.domain.shape[1])\n                samples[:, bandit_idx] = var.domain[idx_samples,:]", "response": "Fill sample values to non - continuous variables in place\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_obj(self,space):\n\n        \"\"\"\n        Imports the acquisition function.\n        \"\"\"\n        obj_func = self.obj_func\n        \n        from ..core.task import SingleObjective\n        return SingleObjective(obj_func, self.config['resources']['cores'], space=space, unfold_args=True)", "response": "Returns the object in the given space"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_space(self):\n        assert 'space' in self.config, 'The search space is NOT configured!'\n        \n        space_config = self.config['space']\n        constraint_config = self.config['constraints']\n        from ..core.task.space import Design_space\n        return Design_space.fromConfig(space_config, constraint_config)", "response": "Imports the domain. availabe. Design_space class"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport the model. Imports the model. Imports the model.", "response": "def _get_model(self):\n        \"\"\"\n        Imports the model.\n        \"\"\"\n\n        from copy import deepcopy\n        model_args = deepcopy(self.config['model'])\n        del model_args['type']\n        from ..models import select_model\n        \n        return select_model(self.config['model']['type']).fromConfig(model_args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimporting the acquisition object", "response": "def _get_acquisition(self, model, space):\n        \"\"\"\n        Imports the acquisition\n        \"\"\"\n\n        from copy import deepcopy        \n        acqOpt_config = deepcopy(self.config['acquisition']['optimizer'])\n        acqOpt_name = acqOpt_config['name']\n        del acqOpt_config['name']\n        \n        from ..optimization import AcquisitionOptimizer\n        acqOpt = AcquisitionOptimizer(space, acqOpt_name, **acqOpt_config)\n        from ..acquisitions import select_acquisition\n        return select_acquisition(self.config['acquisition']['type']).fromConfig(model, space, acqOpt, None, self.config['acquisition'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimporting the evaluator Imports the evaluator", "response": "def _get_acq_evaluator(self, acq):\n        \"\"\"\n        Imports the evaluator\n        \"\"\"\n\n        from ..core.evaluators import select_evaluator\n        from copy import deepcopy\n        eval_args = deepcopy(self.config['acquisition']['evaluator'])\n        del eval_args['type']\n        return select_evaluator(self.config['acquisition']['evaluator']['type'])(acq, **eval_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_stop(self, iters, elapsed_time, converged):\n\n        r_c = self.config['resources']\n        \n        stop = False\n        if converged==0:\n            stop=True\n        if r_c['maximum-iterations'] !='NA' and iters>= r_c['maximum-iterations']:\n            stop = True\n        if r_c['max-run-time'] != 'NA' and elapsed_time/60.>= r_c['max-run-time']:\n            stop = True\n        return stop", "response": "Check if the stopping criterion is satisfied."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the optimization using the previously loaded elements.", "response": "def run(self):\n        \"\"\"\n        Runs the optimization using the previously loaded elements.\n        \"\"\"\n\n        space = self._get_space()\n        obj_func = self._get_obj(space)\n        model = self._get_model()\n        acq = self._get_acquisition(model, space)\n        acq_eval = self._get_acq_evaluator(acq)\n                \n        from ..experiment_design import initial_design\n        X_init = initial_design(self.config['initialization']['type'], space, self.config['initialization']['num-eval'])\n\n        from ..methods import ModularBayesianOptimization\n        bo = ModularBayesianOptimization(model, space, obj_func, acq, acq_eval, X_init)\n                \n        bo.run_optimization(max_iter = self.config['resources']['maximum-iterations'], max_time = self.config['resources']['max-run-time'] if self.config['resources']['max-run-time']!=\"NA\" else np.inf,\n                              eps = self.config['resources']['tolerance'], verbosity=True)        \n        return bo"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_optimizer(optimizer, x0, f=None, df=None, f_df=None, duplicate_manager=None, context_manager=None, space=None):\n    x0 = np.atleast_2d(x0)\n\n    ## --- Compute a new objective that inputs non context variables but that takes into account the values of the context ones.\n    ## --- It does nothing if no context is passed\n    problem = OptimizationWithContext(x0=x0, f=f, df=df, f_df=f_df, context_manager=context_manager)\n\n\n    if context_manager:\n        add_context = lambda x : context_manager._expand_vector(x)\n    else:\n        add_context = lambda x : x\n\n    if duplicate_manager and duplicate_manager.is_unzipped_x_duplicate(x0):\n        raise ValueError(\"The starting point of the optimizer cannot be a duplicate.\")\n\n    ## --- Optimize point\n    optimized_x, _ = optimizer.optimize(problem.x0_nocontext, problem.f_nocontext, problem.df_nocontext, problem.f_df_nocontext)\n\n    ## --- Add context and round according to the type of variables of the design space\n    suggested_x_with_context = add_context(optimized_x)\n    suggested_x_with_context_rounded = space.round_optimum(suggested_x_with_context)\n\n    ## --- Run duplicate_manager\n    if duplicate_manager and duplicate_manager.is_unzipped_x_duplicate(suggested_x_with_context_rounded):\n        suggested_x, suggested_fx = x0, np.atleast_2d(f(x0))\n    else:\n        suggested_x, suggested_fx = suggested_x_with_context_rounded, f(suggested_x_with_context_rounded)\n\n    return suggested_x, suggested_fx", "response": "Applies an optimizer to a single object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef choose_optimizer(optimizer_name, bounds):\n        if optimizer_name == 'lbfgs':\n            optimizer = OptLbfgs(bounds)\n\n        elif optimizer_name == 'DIRECT':\n            optimizer = OptDirect(bounds)\n\n        elif optimizer_name == 'CMA':\n            optimizer = OptCma(bounds)\n        else:\n            raise InvalidVariableNameError('Invalid optimizer selected.')\n\n        return optimizer", "response": "Selects the type of local optimizer based on the name of the optimizer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noptimizes a local optimizer.", "response": "def optimize(self, x0, f=None, df=None, f_df=None):\n        \"\"\"\n        :param x0: initial point for a local optimizer.\n        :param f: function to optimize.\n        :param df: gradient of the function to optimize.\n        :param f_df: returns both the function to optimize and its gradient.\n        \"\"\"\n        # Based on the documentation of DIRECT, it does not seem we can pass through an initial point x0\n        try:\n            from DIRECT import solve\n            def DIRECT_f_wrapper(f):\n                def g(x, user_data):\n                    return f(np.array([x])), 0\n                return g\n            lB = np.asarray(self.bounds)[:,0]\n            uB = np.asarray(self.bounds)[:,1]\n            x,_,_ = solve(DIRECT_f_wrapper(f),lB,uB, maxT=self.maxiter)\n            return np.atleast_2d(x), f(np.atleast_2d(x))\n        except ImportError:\n            print(\"Cannot find DIRECT library, please install it to use this option.\")\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noptimizes the local optimizer.", "response": "def optimize(self, x0, f=None, df=None, f_df=None):\n        \"\"\"\n        :param x0: initial point for a local optimizer.\n        :param f: function to optimize.\n        :param df: gradient of the function to optimize.\n        :param f_df: returns both the function to optimize and its gradient.\n        \"\"\"\n\n        if len(self.bounds) == 1:\n            raise IndexError(\"CMA does not work in problems of dimension 1.\")\n\n        try:\n            import cma\n            def CMA_f_wrapper(f):\n                def g(x):\n                    return f(np.array([x]))\n                return g\n            lB = np.asarray(self.bounds)[:,0]\n            uB = np.asarray(self.bounds)[:,1]\n            x = cma.fmin(CMA_f_wrapper(f), x0, 0.6, options={\"bounds\":[lB, uB], \"verbose\":-1})[0]\n            return np.atleast_2d(x), f(np.atleast_2d(x))\n        except ImportError:\n            print(\"Cannot find cma library, please install it to use this option.\")\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef f_nc(self,x):\n        '''\n        Wrapper of *f*: takes an input x with size of the noncontext dimensions\n        expands it and evaluates the entire function.\n        '''\n        x = np.atleast_2d(x)\n        xx = self.context_manager._expand_vector(x)\n        if x.shape[0] == 1:\n            return self.f(xx)[0]\n        else:\n            return self.f(xx)", "response": "Wrapper of f * f * takes an input x with size of the noncontext dimensions expands it and evaluates the entire function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef df_nc(self,x):\n        '''\n        Wrapper of the derivative of *f*: takes an input x with size of the not\n        fixed dimensions expands it and evaluates the gradient of the entire function.\n        '''\n        x = np.atleast_2d(x)\n        xx = self.context_manager._expand_vector(x)\n        _, df_nocontext_xx = self.f_df(xx)\n        df_nocontext_xx = df_nocontext_xx[:,np.array(self.context_manager.noncontext_index)]\n        return df_nocontext_xx", "response": "Wrapper of the derivative of f * x"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef f_df_nc(self,x):\n        '''\n        Wrapper of the derivative of *f*: takes an input x with size of the not\n        fixed dimensions expands it and evaluates the gradient of the entire function.\n        '''\n        x = np.atleast_2d(x)\n        xx = self.context_manager._expand_vector(x)\n        f_nocontext_xx , df_nocontext_xx = self.f_df(xx)\n        df_nocontext_xx = df_nocontext_xx[:,np.array(self.context_manager.noncontext_index)]\n        return f_nocontext_xx, df_nocontext_xx", "response": "Wrapper of the derivative of f * x with size of the not\n        fixed dimensions expands it and evaluates the gradient of the entire function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef evaluator_creator(self, evaluator_type, acquisition, batch_size, model_type, model, space, acquisition_optimizer):\n        acquisition_transformation = self.kwargs.get('acquisition_transformation','none')\n\n        if batch_size == 1 or evaluator_type == 'sequential':\n            return Sequential(acquisition)\n\n        elif batch_size >1 and (evaluator_type == 'random' or evaluator_type is  None):\n            return RandomBatch(acquisition, batch_size)\n\n        elif batch_size >1 and evaluator_type == 'thompson_sampling':\n            return ThompsonBatch(acquisition, batch_size)\n\n        elif evaluator_type == 'local_penalization':\n            if model_type not in ['GP', 'sparseGP', 'GP_MCMC', 'warpedGP']:\n                raise InvalidConfigError('local_penalization evaluator can only be used with GP models')\n\n            if not isinstance(acquisition, AcquisitionLP):\n                acquisition_lp = AcquisitionLP(model, space, acquisition_optimizer, acquisition, acquisition_transformation)\n            return LocalPenalization(acquisition_lp, batch_size)", "response": "Create an evaluator instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef acquisition_creator(self, acquisition_type, model, space, acquisition_optimizer, cost_withGradients):\n\n        \"\"\"\n        Acquisition chooser from the available options. Extra parameters can be passed via **kwargs.\n        \"\"\"\n        acquisition_type = acquisition_type\n        model = model\n        space = space\n        acquisition_optimizer = acquisition_optimizer\n        cost_withGradients = cost_withGradients\n        acquisition_jitter = self.kwargs.get('acquisition_jitter',0.01)\n        acquisition_weight = self.kwargs.get('acquisition_weight',2)\n\n        # --- Choose the acquisition\n        if acquisition_type is  None or acquisition_type =='EI':\n            return AcquisitionEI(model, space, acquisition_optimizer, cost_withGradients, acquisition_jitter)\n\n        elif acquisition_type =='EI_MCMC':\n            return AcquisitionEI_MCMC(model, space, acquisition_optimizer, cost_withGradients, acquisition_jitter)\n\n        elif acquisition_type =='MPI':\n            return AcquisitionMPI(model, space, acquisition_optimizer, cost_withGradients, acquisition_jitter)\n\n        elif acquisition_type =='MPI_MCMC':\n            return AcquisitionMPI_MCMC(model, space, acquisition_optimizer, cost_withGradients, acquisition_jitter)\n\n        elif acquisition_type =='LCB':\n            return AcquisitionLCB(model, space, acquisition_optimizer, None, acquisition_weight)\n\n        elif acquisition_type =='LCB_MCMC':\n            return AcquisitionLCB_MCMC(model, space, acquisition_optimizer, None, acquisition_weight)\n\n        else:\n            raise Exception('Invalid acquisition selected.')", "response": "Returns an acquisition instance based on the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef model_creator(self, model_type, exact_feval, space):\n        model_type = model_type\n        exact_feval = exact_feval\n        space = space\n\n        kernel = self.kwargs.get('kernel',None)\n        ARD = self.kwargs.get('ARD',False)\n        verbosity_model = self.kwargs.get('verbosity_model',False)\n        noise_var = self.kwargs.get('noise_var',None)\n        model_optimizer_type = self.kwargs.get('model_optimizer_type','lbfgs')\n        max_iters = self.kwargs.get('max_iters',1000)\n        optimize_restarts = self.kwargs.get('optimize_restarts',5)\n\n        # --------\n        # --- Initialize GP model with MLE on the parameters\n        # --------\n        if model_type == 'GP' or model_type == 'sparseGP':\n            if model_type == 'GP':\n                sparse = False\n            if model_type == 'sparseGP':\n                sparse = True\n            optimize_restarts = self.kwargs.get('optimize_restarts',5)\n            num_inducing = self.kwargs.get('num_inducing',10)\n            return GPModel(kernel, noise_var, exact_feval, model_optimizer_type, max_iters, optimize_restarts, sparse, num_inducing, verbosity_model, ARD)\n\n        # --------\n        # --- Initialize GP model with MCMC on the parameters\n        # --------\n        elif model_type == 'GP_MCMC':\n            n_samples = self.kwargs.get('n_samples',10)\n            n_burnin = self.kwargs.get('n_burnin',100)\n            subsample_interval = self.kwargs.get('subsample_interval',10)\n            step_size = self.kwargs.get('step_size',1e-1)\n            leapfrog_steps = self.kwargs.get('leapfrog_steps',20)\n            return GPModel_MCMC(kernel, noise_var, exact_feval, n_samples, n_burnin, subsample_interval, step_size, leapfrog_steps, verbosity_model)\n\n        # --------\n        # --- Initialize RF: values taken from default in scikit-learn\n        # --------\n        elif model_type =='RF':\n            return RFModel(verbose=verbosity_model)\n\n        # --------\n        # --- Initialize WapedGP in the outputs\n        # --------\n        elif model_type =='warpedGP':\n            return WarpedGPModel()\n\n        # --------\n        # --- Initialize WapedGP in the inputs\n        # --------\n        elif model_type == 'input_warped_GP':\n            if 'input_warping_function_type' in self.kwargs:\n                if self.kwargs['input_warping_function_type'] != \"kumar_warping\":\n                    print(\"Only support kumar_warping for input!\")\n\n            # Only support Kumar warping now, setting it to None will use default Kumar warping\n            input_warping_function = None\n            optimize_restarts = self.kwargs.get('optimize_restarts',5)\n            return InputWarpedGPModel(space, input_warping_function, kernel, noise_var,\n                                      exact_feval, model_optimizer_type, max_iters,\n                                      optimize_restarts, verbosity_model, ARD)", "response": "Create a new instance of the model class based on the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the GP - Lower Confidence Bound", "response": "def _compute_acq(self, x):\n        \"\"\"\n        Computes the GP-Lower Confidence Bound \n        \"\"\"\n        m, s = self.model.predict(x)   \n        f_acqu = -m + self.exploration_weight * s\n        return f_acqu"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the GP - Lower Confidence Bound and its derivative ArcGIS derivative ArcGIS", "response": "def _compute_acq_withGradients(self, x):\n        \"\"\"\n        Computes the GP-Lower Confidence Bound and its derivative\n        \"\"\"\n        m, s, dmdx, dsdx = self.model.predict_withGradients(x) \n        f_acqu = -m + self.exploration_weight * s       \n        df_acqu = -dmdx + self.exploration_weight * dsdx\n        return f_acqu, df_acqu"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a variable from a dictionary descriptor", "response": "def create_variable(descriptor):\n    \"\"\"\n    Creates a variable from a dictionary descriptor\n    \"\"\"\n    if descriptor['type'] == 'continuous':\n        return ContinuousVariable(descriptor['name'], descriptor['domain'], descriptor.get('dimensionality', 1))\n    elif descriptor['type'] == 'bandit':\n        return BanditVariable(descriptor['name'], descriptor['domain'], descriptor.get('dimensionality', None))  # bandits variables cannot be repeated\n    elif descriptor['type'] == 'discrete':\n        return DiscreteVariable(descriptor['name'], descriptor['domain'], descriptor.get('dimensionality', 1))\n    elif descriptor['type'] == 'categorical':\n        return CategoricalVariable(descriptor['name'], descriptor['domain'], descriptor.get('dimensionality', 1))\n    else:\n        raise InvalidConfigError('Unknown variable type ' + descriptor['type'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expand(self):\n        expanded_variables = []\n        for i in range(self.dimensionality):\n            one_d_variable = deepcopy(self)\n            one_d_variable.dimensionality = 1\n            if self.dimensionality > 1:\n                one_d_variable.name = '{}_{}'.format(self.name, i+1)\n            else:\n                one_d_variable.name = self.name\n            one_d_variable.dimensionality_in_model = 1\n            expanded_variables.append(one_d_variable)\n        return expanded_variables", "response": "Builds a list of single dimensional variables representing current variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef round(self, value_array):\n\n        min_value = self.domain[0]\n        max_value = self.domain[1]\n\n        rounded_value = value_array[0]\n        if rounded_value < min_value:\n            rounded_value = min_value\n        elif rounded_value > max_value:\n            rounded_value = max_value\n\n        return [rounded_value]", "response": "Round the value of the key to the nearest value in the domain."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef round(self, value_array):\n        distances = np.linalg.norm(np.array(self.domain) - value_array, axis=1)\n        idx = np.argmin(distances)\n        return [self.domain[idx]]", "response": "Finds the closest point in the domain by selecting the closest point in the domain"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef round(self, value_array):\n        value = value_array[0]\n        rounded_value = self.domain[0]\n\n        for domain_value in self.domain:\n            if np.abs(domain_value - value) < np.abs(rounded_value - value):\n                rounded_value = domain_value\n\n        return [rounded_value]", "response": "Rounds a discrete variable by selecting the closest point in the domain."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef round(self, value_array):\n\n        rounded_values = np.zeros(value_array.shape)\n        rounded_values[np.argmax(value_array)] = 1\n        return rounded_values", "response": "Rounds a categorical variable by setting to one the max of the given vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noptimizes the input function f df f_df returns both the function to optimize and its gradient.", "response": "def optimize(self, f=None, df=None, f_df=None, duplicate_manager=None):\n        \"\"\"\n        Optimizes the input function.\n\n        :param f: function to optimize.\n        :param df: gradient of the function to optimize.\n        :param f_df: returns both the function to optimize and its gradient.\n\n        \"\"\"\n        self.f = f\n        self.df = df\n        self.f_df = f_df\n\n        ## --- Update the optimizer, in case context has beee passed.\n        self.optimizer = choose_optimizer(self.optimizer_name, self.context_manager.noncontext_bounds)\n\n        ## --- Selecting the anchor points and removing duplicates\n        if self.type_anchor_points_logic == max_objective_anchor_points_logic:\n            anchor_points_generator = ObjectiveAnchorPointsGenerator(self.space, random_design_type, f)\n        elif self.type_anchor_points_logic == thompson_sampling_anchor_points_logic:\n            anchor_points_generator = ThompsonSamplingAnchorPointsGenerator(self.space, sobol_design_type, self.model)\n\n        ## -- Select the anchor points (with context)\n        anchor_points = anchor_points_generator.get(duplicate_manager=duplicate_manager, context_manager=self.context_manager)\n\n        ## --- Applying local optimizers at the anchor points and update bounds of the optimizer (according to the context)\n        optimized_points = [apply_optimizer(self.optimizer, a, f=f, df=None, f_df=f_df, duplicate_manager=duplicate_manager, context_manager=self.context_manager, space = self.space) for a in anchor_points]\n        x_min, fx_min = min(optimized_points, key=lambda t:t[1])\n\n        #x_min, fx_min = min([apply_optimizer(self.optimizer, a, f=f, df=None, f_df=f_df, duplicate_manager=duplicate_manager, context_manager=self.context_manager, space = self.space) for a in anchor_points], key=lambda t:t[1])\n\n        return x_min, fx_min"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _expand_vector(self,x):\n        '''\n        Takes a value x in the subspace of not fixed dimensions and expands it with the values of the fixed ones.\n        :param x: input vector to be expanded by adding the context values\n        '''\n        x = np.atleast_2d(x)\n        x_expanded = np.zeros((x.shape[0],self.space.model_dimensionality))\n        x_expanded[:,np.array(self.noncontext_index).astype(int)]  = x\n        x_expanded[:,np.array(self.context_index).astype(int)]  = self.context_value\n        return x_expanded", "response": "Expands the input vector x by adding the context values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform the evaluation of the objective at x.", "response": "def evaluate(self, x):\n        \"\"\"\n        Performs the evaluation of the objective at x.\n        \"\"\"\n\n        if self.n_procs == 1:\n            f_evals, cost_evals = self._eval_func(x)\n        else:\n            try:\n                f_evals, cost_evals = self._syncronous_batch_evaluation(x)\n            except:\n                if not hasattr(self, 'parallel_error'):\n                    print('Error in parallel computation. Fall back to single process!')\n                else:\n                    self.parallel_error = True\n                f_evals, cost_evals = self._eval_func(x)\n\n        return f_evals, cost_evals"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms sequential evaluations of the function at x.", "response": "def _eval_func(self, x):\n        \"\"\"\n        Performs sequential evaluations of the function at x (single location or batch). The computing time of each\n        evaluation is also provided.\n        \"\"\"\n        cost_evals = []\n        f_evals     = np.empty(shape=[0, 1])\n\n        for i in range(x.shape[0]):\n            st_time    = time.time()\n            rlt = self.func(np.atleast_2d(x[i]))\n            f_evals     = np.vstack([f_evals,rlt])\n            cost_evals += [time.time()-st_time]\n        return f_evals, cost_evals"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates the function a x.", "response": "def _syncronous_batch_evaluation(self,x):\n        \"\"\"\n        Evaluates the function a x, where x can be a single location or a batch. The evaluation is performed in parallel\n        according to the number of accessible cores.\n        \"\"\"\n        from multiprocessing import Process, Pipe\n\n        # --- parallel evaluation of the function\n        divided_samples = [x[i::self.n_procs] for i in range(self.n_procs)]\n        pipe = [Pipe() for i in range(self.n_procs)]\n        proc = [Process(target=spawn(self._eval_func),args=(c,k)) for k,(p,c) in zip(divided_samples,pipe)]\n        [p.start() for p in proc]\n        [p.join() for p in proc]\n\n        # --- time of evaluation is set to constant (=1). This is one of the hypothesis of synchronous batch methods.\n        f_evals = np.zeros((x.shape[0],1))\n        cost_evals = np.ones((x.shape[0],1))\n        i = 0\n        for (p,c) in pipe:\n            f_evals[i::self.n_procs] = p.recv()[0] # throw away costs\n            i += 1\n        return f_evals, cost_evals"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the batch of data for the current context.", "response": "def compute_batch(self, duplicate_manager=None,context_manager=None):\n        \"\"\"\n        Selects the new location to evaluate the objective.\n        \"\"\"\n        x, _ = self.acquisition.optimize(duplicate_manager=duplicate_manager)\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_acq(self,x):\n        means, stds = self.model.predict(x)\n        f_acqu = 0\n        for m,s in zip(means, stds):\n            f_acqu += -m + self.exploration_weight * s\n        return f_acqu/(len(means))", "response": "Compute the approximate GP - Lower Confidence Bound"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the approximate GP - Lower Confidence Bound and its derivative ArcGIS - like Confidence Bound and its derivative ArcGIS - like Confidence Bound and its derivative ArcGIS - like Confidence Bound", "response": "def _compute_acq_withGradients(self, x):\n        \"\"\"\n        Integrated GP-Lower Confidence Bound and its derivative\n        \"\"\"\n        means, stds, dmdxs, dsdxs = self.model.predict_withGradients(x)\n        f_acqu = None\n        df_acqu = None\n        for m, s, dmdx, dsdx in zip(means, stds, dmdxs, dsdxs):  \n            f = -m + self.exploration_weight * s        \n            df = -dmdx + self.exploration_weight * dsdx\n            if f_acqu is None:\n                f_acqu = f        \n                df_acqu = df\n            else:\n                f_acqu += f\n                df_acqu += df\n        return f_acqu/(len(means)), df_acqu/(len(means))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bounds_to_space(bounds):\n    space = []\n    for k in range(len(bounds)):\n        space += [{'name': 'var_'+str(k+1), 'type': 'continuous', 'domain':bounds[k], 'dimensionality':1}]\n    return space", "response": "Takes as input a list of tuples with bounds and creates a dictionary that contains the class Design_space for each variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _expand_config_space(self):\n        self.config_space_expanded = []\n\n        for variable in self.config_space:\n            variable_dic = variable.copy()\n            if 'dimensionality' in variable_dic.keys():\n                dimensionality = variable_dic['dimensionality']\n                variable_dic['dimensionality'] = 1\n                variables_set = [variable_dic.copy() for d in range(dimensionality)]\n                k=1\n                for variable in variables_set:\n                    variable['name'] = variable['name'] + '_'+str(k)\n                    k+=1\n                self.config_space_expanded += variables_set\n            else:\n                self.config_space_expanded += [variable_dic]", "response": "Expands the config input space into a list of diccionaries one for each variable_dic\n        in which the dimensionality is always one."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes and saves the index location of each variable in the objectives space and in the model space.", "response": "def _compute_variables_indices(self):\n        \"\"\"\n        Computes and saves the index location of each variable (as a list) in the objectives\n        space and in the model space. If no categorical variables are available, these two are\n        equivalent.\n        \"\"\"\n\n        counter_objective = 0\n        counter_model     = 0\n\n        for variable in self.space_expanded:\n            variable.set_index_in_objective([counter_objective])\n            counter_objective +=1\n\n            if variable.type is not 'categorical':\n                variable.set_index_in_model([counter_model])\n                counter_model +=1\n            else:\n                num_categories = len(variable.domain)\n                variable.set_index_in_model(list(range(counter_model,counter_model + num_categories)))\n                counter_model +=num_categories"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_variables_dic(self):\n        self.name_to_variable = {}\n        for variable in self.space_expanded:\n            self.name_to_variable[variable.name] = variable", "response": "Creates the dictionary of variable names to values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _translate_space(self, space):\n        self.space = []\n        self.dimensionality = 0\n        self.has_types = d = {t: False for t in self.supported_types}\n\n        for i, d in enumerate(space):\n            descriptor = deepcopy(d)\n            descriptor['name'] = descriptor.get('name', 'var_' + str(i))\n            descriptor['type'] = descriptor.get('type', 'continuous')\n            if 'domain' not in descriptor:\n                raise InvalidConfigError('Domain attribute is missing for variable ' + descriptor['name'])\n            variable = create_variable(descriptor)\n            self.space.append(variable)\n            self.dimensionality += variable.dimensionality\n            self.has_types[variable.type] = True\n\n        # Check if there are any bandit and non-bandit variables together in the space\n        if any(v.is_bandit() for v in self.space) and any(not v.is_bandit() for v in self.space):\n            raise InvalidConfigError('Invalid mixed domain configuration. Bandit variables cannot be mixed with other types.')", "response": "Translates a list of dictionaries into internal list of variables"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexpands the config space and the user - defined variables.", "response": "def _expand_space(self):\n        \"\"\"\n        Creates an internal list where the variables with dimensionality larger than one are expanded.\n        This list is the one that is used internally to do the optimization.\n        \"\"\"\n\n        ## --- Expand the config space\n        self._expand_config_space()\n\n        ## --- Expand the space\n        self.space_expanded = []\n        for variable in self.space:\n            self.space_expanded += variable.expand()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef objective_to_model(self, x_objective):\n        ''' This function serves as interface between objective input vectors and\n        model input vectors'''\n\n        x_model = []\n\n        for k in range(self.objective_dimensionality):\n            variable = self.space_expanded[k]\n            new_entry = variable.objective_to_model(x_objective[0,k])\n            x_model += new_entry\n\n        return x_model", "response": "This function serves as interface between objective input vectors and the model input vectors."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the bounds of all the variables in the domain that are in the model.", "response": "def get_bounds(self):\n        \"\"\"\n        Extracts the bounds of all the inputs of the domain of the *model*\n        \"\"\"\n        bounds = []\n\n        for variable in self.space_expanded:\n            bounds += variable.get_bounds()\n\n        return bounds"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_subspace(self, dims):\n        '''\n        Extracts subspace from the reference of a list of variables in the inputs\n        of the model.\n        '''\n        subspace = []\n        k = 0\n        for variable in self.space_expanded:\n            if k in dims:\n                subspace.append(variable)\n            k += variable.dimensionality_in_model\n        return subspace", "response": "Returns a list of subspace of the specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn array of ones and zeros indicating if x is within the constraints", "response": "def indicator_constraints(self,x):\n        \"\"\"\n        Returns array of ones and zeros indicating if x is within the constraints\n        \"\"\"\n        x = np.atleast_2d(x)\n        I_x = np.ones((x.shape[0],1))\n        if self.constraints is not None:\n            for d in self.constraints:\n                try:\n                    exec('constraint = lambda x:' + d['constraint'], globals())\n                    ind_x = (constraint(x) <= 0) * 1\n                    I_x *= ind_x.reshape(x.shape[0],1)\n                except:\n                    print('Fail to compile the constraint: ' + str(d))\n                    raise\n        return I_x"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef input_dim(self):\n        n_cont = len(self.get_continuous_dims())\n        n_disc = len(self.get_discrete_dims())\n        return n_cont + n_disc", "response": "Returns the number of input dimensions of the domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef round_optimum(self, x):\n        x = np.array(x)\n        if not ((x.ndim == 1) or (x.ndim == 2 and x.shape[0] == 1)):\n            raise ValueError(\"Unexpected dimentionality of x. Got {}, expected (1, N) or (N,)\".format(x.ndim))\n\n        if x.ndim == 2:\n            x = x[0]\n\n        x_rounded = []\n        value_index = 0\n        for variable in self.space_expanded:\n            var_value = x[value_index : value_index + variable.dimensionality_in_model]\n            var_value_rounded = variable.round(var_value)\n\n            x_rounded.append(var_value_rounded)\n            value_index += variable.dimensionality_in_model\n\n        return np.atleast_2d(np.concatenate(x_rounded))", "response": "Rounds some value x to a feasible value in the design space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_continuous_bounds(self):\n        bounds = []\n        for d in self.space:\n            if d.type == 'continuous':\n                bounds.extend([d.domain]*d.dimensionality)\n        return bounds", "response": "Extracts the bounds of the continuous variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the dimension of the continuous components of the domain.", "response": "def get_continuous_dims(self):\n        \"\"\"\n        Returns the dimension of the continuous components of the domain.\n        \"\"\"\n        continuous_dims = []\n        for i in range(self.dimensionality):\n            if self.space_expanded[i].type == 'continuous':\n                continuous_dims += [i]\n        return continuous_dims"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_discrete_grid(self):\n        sets_grid = []\n        for d in self.space:\n            if d.type == 'discrete':\n                sets_grid.extend([d.domain]*d.dimensionality)\n        return np.array(list(itertools.product(*sets_grid)))", "response": "Computes a Numpy array with the grid of points that results after crossing possible outputs of the discreteCOOKIE sets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_discrete_dims(self):\n        discrete_dims = []\n        for i in range(self.dimensionality):\n            if self.space_expanded[i].type == 'discrete':\n                discrete_dims += [i]\n        return discrete_dims", "response": "Returns the dimension of the discrete components of the domain."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the arms of the bandit.", "response": "def get_bandit(self):\n        \"\"\"\n        Extracts the arms of the bandit if any.\n        \"\"\"\n        arms_bandit = []\n        for d in self.space:\n            if d.type == 'bandit':\n                arms_bandit += tuple(map(tuple, d.domain))\n        return np.asarray(arms_bandit)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the model given some input data X and Y.", "response": "def _create_model(self, X, Y):\n        \"\"\"\n        Creates the model given some input data X and Y.\n        \"\"\"\n        from sklearn.ensemble import RandomForestRegressor\n        self.X = X\n        self.Y = Y\n        self.model = RandomForestRegressor(bootstrap = self.bootstrap,\n                                           criterion = self.criterion,\n                                           max_depth = self.max_depth,\n                                           max_features = self.max_features,\n                                           max_leaf_nodes = self.max_leaf_nodes,\n                                           min_samples_leaf = self.min_samples_leaf,\n                                           min_samples_split = self.min_samples_split,\n                                           min_weight_fraction_leaf = self.min_weight_fraction_leaf,\n                                           n_estimators = self.n_estimators,\n                                           n_jobs = self.n_jobs,\n                                           oob_score = self.oob_score,\n                                           random_state = self.random_state,\n                                           verbose = self.verbose,\n                                           warm_start = self.warm_start)\n\n        #self.model = RandomForestRegressor()\n        self.model.fit(X,Y.flatten())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef updateModel(self, X_all, Y_all, X_new, Y_new):\n        self.X = X_all\n        self.Y = Y_all\n        if self.model is None:\n            self._create_model(X_all, Y_all)\n        else:\n            self.model.fit(X_all, Y_all.flatten())", "response": "Updates the model with new observations."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npredicts with the model. Returns posterior means and standard deviations at X.", "response": "def predict(self, X):\n        \"\"\"\n        Predictions with the model. Returns posterior means and standard deviations at X.\n        \"\"\"\n        X = np.atleast_2d(X)\n        m = np.empty(shape=(0,1))\n        s = np.empty(shape=(0,1))\n\n        for k in range(X.shape[0]):\n            preds = []\n            for pred in self.model.estimators_:\n                preds.append(pred.predict(X[k,:])[0])\n            m = np.vstack((m ,np.array(preds).mean()))\n            s = np.vstack((s ,np.array(preds).std()))\n        return m, s"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the Integrated Expected Improvement of a single object.", "response": "def _compute_acq(self,x):\n        \"\"\"\n        Integrated Expected Improvement\n        \"\"\"\n        means, stds = self.model.predict(x)\n        fmins = self.model.get_fmin()\n        f_acqu = 0\n        for m,s,fmin in zip(means, stds, fmins):\n            _, Phi, _ = get_quantiles(self.jitter, fmin, m, s)\n            f_acqu += Phi\n        return f_acqu/len(means)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the integral of expected improvement and derivative of the object x.", "response": "def _compute_acq_withGradients(self, x):\n        \"\"\"\n        Integrated Expected Improvement and its derivative\n        \"\"\"\n        means, stds, dmdxs, dsdxs = self.model.predict_withGradients(x)\n        fmins = self.model.get_fmin()\n        f_acqu = None\n        df_acqu = None\n        for m, s, fmin, dmdx, dsdx in zip(means, stds, fmins, dmdxs, dsdxs):\n            phi, Phi, u = get_quantiles(self.jitter, fmin, m, s)\n            f = Phi\n            df = -(phi/s)* (dmdx + dsdx * u)\n            if f_acqu is None:\n                f_acqu = f\n                df_acqu = df\n            else:\n                f_acqu += f\n                df_acqu += df\n        return f_acqu/(len(means)), df_acqu/(len(means))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_acquisition(bounds,input_dim,model,Xdata,Ydata,acquisition_function,suggested_sample, filename = None):\n    '''\n    Plots of the model and the acquisition function in 1D and 2D examples.\n    '''\n\n    # Plots in dimension 1\n    if input_dim ==1:\n        # X = np.arange(bounds[0][0], bounds[0][1], 0.001)\n        # X = X.reshape(len(X),1)\n        # acqu = acquisition_function(X)\n        # acqu_normalized = (-acqu - min(-acqu))/(max(-acqu - min(-acqu))) # normalize acquisition\n        # m, v = model.predict(X.reshape(len(X),1))\n        # plt.ioff()\n        # plt.figure(figsize=(10,5))\n        # plt.subplot(2, 1, 1)\n        # plt.plot(X, m, 'b-', label=u'Posterior mean',lw=2)\n        # plt.fill(np.concatenate([X, X[::-1]]), \\\n        #         np.concatenate([m - 1.9600 * np.sqrt(v),\n        #                     (m + 1.9600 * np.sqrt(v))[::-1]]), \\\n        #         alpha=.5, fc='b', ec='None', label='95% C. I.')\n        # plt.plot(X, m-1.96*np.sqrt(v), 'b-', alpha = 0.5)\n        # plt.plot(X, m+1.96*np.sqrt(v), 'b-', alpha=0.5)\n        # plt.plot(Xdata, Ydata, 'r.', markersize=10, label=u'Observations')\n        # plt.axvline(x=suggested_sample[len(suggested_sample)-1],color='r')\n        # plt.title('Model and observations')\n        # plt.ylabel('Y')\n        # plt.xlabel('X')\n        # plt.legend(loc='upper left')\n        # plt.xlim(*bounds)\n        # grid(True)\n        # plt.subplot(2, 1, 2)\n        # plt.axvline(x=suggested_sample[len(suggested_sample)-1],color='r')\n        # plt.plot(X,acqu_normalized, 'r-',lw=2)\n        # plt.xlabel('X')\n        # plt.ylabel('Acquisition value')\n        # plt.title('Acquisition function')\n        # grid(True)\n        # plt.xlim(*bounds)\n\n        x_grid = np.arange(bounds[0][0], bounds[0][1], 0.001)\n        x_grid = x_grid.reshape(len(x_grid),1)\n        acqu = acquisition_function(x_grid)\n        acqu_normalized = (-acqu - min(-acqu))/(max(-acqu - min(-acqu)))\n        m, v = model.predict(x_grid)\n\n\n        model.plot_density(bounds[0], alpha=.5)\n\n        plt.plot(x_grid, m, 'k-',lw=1,alpha = 0.6)\n        plt.plot(x_grid, m-1.96*np.sqrt(v), 'k-', alpha = 0.2)\n        plt.plot(x_grid, m+1.96*np.sqrt(v), 'k-', alpha=0.2)\n\n        plt.plot(Xdata, Ydata, 'r.', markersize=10)\n        plt.axvline(x=suggested_sample[len(suggested_sample)-1],color='r')\n        factor = max(m+1.96*np.sqrt(v))-min(m-1.96*np.sqrt(v))\n\n        plt.plot(x_grid,0.2*factor*acqu_normalized-abs(min(m-1.96*np.sqrt(v)))-0.25*factor, 'r-',lw=2,label ='Acquisition (arbitrary units)')\n        plt.xlabel('x')\n        plt.ylabel('f(x)')\n        plt.ylim(min(m-1.96*np.sqrt(v))-0.25*factor,  max(m+1.96*np.sqrt(v))+0.05*factor)\n        plt.axvline(x=suggested_sample[len(suggested_sample)-1],color='r')\n        plt.legend(loc='upper left')\n\n\n        if filename!=None:\n            savefig(filename)\n        else:\n            plt.show()\n\n    if input_dim ==2:\n        X1 = np.linspace(bounds[0][0], bounds[0][1], 200)\n        X2 = np.linspace(bounds[1][0], bounds[1][1], 200)\n        x1, x2 = np.meshgrid(X1, X2)\n        X = np.hstack((x1.reshape(200*200,1),x2.reshape(200*200,1)))\n        acqu = acquisition_function(X)\n        acqu_normalized = (-acqu - min(-acqu))/(max(-acqu - min(-acqu)))\n        acqu_normalized = acqu_normalized.reshape((200,200))\n        m, v = model.predict(X)\n        plt.figure(figsize=(15,5))\n        plt.subplot(1, 3, 1)\n        plt.contourf(X1, X2, m.reshape(200,200),100)\n        plt.plot(Xdata[:,0], Xdata[:,1], 'r.', markersize=10, label=u'Observations')\n        plt.colorbar()\n        plt.xlabel('X1')\n        plt.ylabel('X2')\n        plt.title('Posterior mean')\n        plt.axis((bounds[0][0],bounds[0][1],bounds[1][0],bounds[1][1]))\n        ##\n        plt.subplot(1, 3, 2)\n        plt.plot(Xdata[:,0], Xdata[:,1], 'r.', markersize=10, label=u'Observations')\n        plt.contourf(X1, X2, np.sqrt(v.reshape(200,200)),100)\n        plt.colorbar()\n        plt.xlabel('X1')\n        plt.ylabel('X2')\n        plt.title('Posterior sd.')\n        plt.axis((bounds[0][0],bounds[0][1],bounds[1][0],bounds[1][1]))\n        ##\n        plt.subplot(1, 3, 3)\n        plt.contourf(X1, X2, acqu_normalized,100)\n        plt.colorbar()\n        plt.plot(suggested_sample[:,0],suggested_sample[:,1],'k.', markersize=10)\n        plt.xlabel('X1')\n        plt.ylabel('X2')\n        plt.title('Acquisition function')\n        plt.axis((bounds[0][0],bounds[0][1],bounds[1][0],bounds[1][1]))\n        if filename!=None:\n            savefig(filename)\n        else:\n            plt.show()", "response": "Plot the acquisition function and the model and the observations."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots the convergence of standard Bayesian optimization algorithms", "response": "def plot_convergence(Xdata,best_Y, filename = None):\n    '''\n    Plots to evaluate the convergence of standard Bayesian optimization algorithms\n    '''\n    n = Xdata.shape[0]\n    aux = (Xdata[1:n,:]-Xdata[0:n-1,:])**2\n    distances = np.sqrt(aux.sum(axis=1))\n\n    ## Distances between consecutive x's\n    plt.figure(figsize=(10,5))\n    plt.subplot(1, 2, 1)\n    plt.plot(list(range(n-1)), distances, '-ro')\n    plt.xlabel('Iteration')\n    plt.ylabel('d(x[n], x[n-1])')\n    plt.title('Distance between consecutive x\\'s')\n    grid(True)\n\n    # Estimated m(x) at the proposed sampling points\n    plt.subplot(1, 2, 2)\n    plt.plot(list(range(n)),best_Y,'-o')\n    plt.title('Value of the best selected sample')\n    plt.xlabel('Iteration')\n    plt.ylabel('Best y')\n    grid(True)\n\n    if filename!=None:\n        savefig(filename)\n    else:\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef estimate_L(model,bounds,storehistory=True):\n    def df(x,model,x0):\n        x = np.atleast_2d(x)\n        dmdx,_ = model.predictive_gradients(x)\n        res = np.sqrt((dmdx*dmdx).sum(1)) # simply take the norm of the expectation of the gradient\n        return -res\n\n    samples = samples_multidimensional_uniform(bounds,500)\n    samples = np.vstack([samples,model.X])\n    pred_samples = df(samples,model,0)\n    x0 = samples[np.argmin(pred_samples)]\n    res = scipy.optimize.minimize(df,x0, method='L-BFGS-B',bounds=bounds, args = (model,x0), options = {'maxiter': 200})\n    minusL = res.fun[0][0]\n    L = -minusL\n    if L<1e-7: L=10  ## to avoid problems in cases in which the model is flat.\n    return L", "response": "Estimate the Lipschitz constant of f by taking maximizing the norm of the expectation of the gradient of the gradient of f."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_batch(self, duplicate_manager=None, context_manager=None):\n        from ...acquisitions import AcquisitionLP\n        assert isinstance(self.acquisition, AcquisitionLP)\n\n        self.acquisition.update_batches(None,None,None)\n\n        # --- GET first element in the batch\n        X_batch = self.acquisition.optimize()[0]\n        k=1\n\n        if self.batch_size >1:\n            # ---------- Approximate the constants of the the method\n            L = estimate_L(self.acquisition.model.model,self.acquisition.space.get_bounds())\n            Min = self.acquisition.model.model.Y.min()\n\n        # --- GET the remaining elements\n        while k<self.batch_size:\n            self.acquisition.update_batches(X_batch,L,Min)\n            new_sample = self.acquisition.optimize()[0]\n            X_batch = np.vstack((X_batch,new_sample))\n            k +=1\n\n        # --- Back to the non-penalized acquisition\n        self.acquisition.update_batches(None,None,None)\n        return X_batch", "response": "Computes the elements of the batch sequentially by penalizing the acquisition."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nevaluate all notebooks in given directory and prints errors if any.", "response": "def check_notebooks_for_errors(notebooks_directory):\n    ''' Evaluates all notebooks in given directory and prints errors, if any '''\n    print(\"Checking notebooks in directory {} for errors\".format(notebooks_directory))\n\n    failed_notebooks_count = 0\n    for file in os.listdir(notebooks_directory):\n        if file.endswith(\".ipynb\"):\n            print(\"Checking notebook \" + file)\n            full_file_path = os.path.join(notebooks_directory, file)\n            output, errors = run_notebook(full_file_path)\n            if errors is not None and len(errors) > 0:\n                failed_notebooks_count += 1\n                print(\"Errors in notebook \" + file)\n                print(errors)\n\n    if failed_notebooks_count == 0:\n        print(\"No errors found in notebooks under \" + notebooks_directory)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a notebook via nbconvert and collect output.", "response": "def run_notebook(notebook_path):\n    \"\"\"Execute a notebook via nbconvert and collect output.\n       :returns (parsed nb object, execution errors)\n    \"\"\"\n    dirname, __ = os.path.split(notebook_path)\n    os.chdir(dirname)\n    with tempfile.NamedTemporaryFile(suffix=\".ipynb\") as fout:\n        args = [\"jupyter-nbconvert\", \"--to\", \"notebook\", \"--execute\", \"--allow-errors\",\n          \"--ExecutePreprocessor.timeout=300\",\n          \"--output\", fout.name, notebook_path]\n        try:\n            subprocess.check_call(args)\n        except subprocess.CalledProcessError as e:\n            if e.returncode == 1:\n              # print the message and ignore error with code 1 as this indicates there were errors in the notebook\n              print(e.output)\n              pass\n            else:\n              # all other codes indicate some other problem, rethrow\n              raise\n\n        fout.seek(0)\n        nb = nbformat.read(fout, nbformat.current_nbformat)\n\n    errors = [output for cell in nb.cells if \"outputs\" in cell\n                     for output in cell[\"outputs\"]\\\n                     if output.output_type == \"error\"]\n\n    return nb, errors"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_model(self, X, Y):\n\n        # --- define kernel\n        self.input_dim = X.shape[1]\n        if self.kernel is None:\n            kern = GPy.kern.Matern52(self.input_dim, variance=1., ARD=self.ARD) #+ GPy.kern.Bias(self.input_dim)\n        else:\n            kern = self.kernel\n            self.kernel = None\n\n        # --- define model\n        noise_var = Y.var()*0.01 if self.noise_var is None else self.noise_var\n\n        if not self.sparse:\n            self.model = GPy.models.GPRegression(X, Y, kernel=kern, noise_var=noise_var)\n        else:\n            self.model = GPy.models.SparseGPRegression(X, Y, kernel=kern, num_inducing=self.num_inducing)\n\n        # --- restrict variance if exact evaluations of the objective\n        if self.exact_feval:\n            self.model.Gaussian_noise.constrain_fixed(1e-6, warning=False)\n        else:\n            # --- We make sure we do not get ridiculously small residual noise variance\n            self.model.Gaussian_noise.constrain_bounded(1e-9, 1e6, warning=False)", "response": "Creates the model given some input data X and Y."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef updateModel(self, X_all, Y_all, X_new, Y_new):\n        if self.model is None:\n            self._create_model(X_all, Y_all)\n        else:\n            self.model.set_XY(X_all, Y_all)\n\n        # WARNING: Even if self.max_iters=0, the hyperparameters are bit modified...\n        if self.max_iters > 0:\n            # --- update the model maximizing the marginal likelihood.\n            if self.optimize_restarts==1:\n                self.model.optimize(optimizer=self.optimizer, max_iters = self.max_iters, messages=False, ipython_notebook=False)\n            else:\n                self.model.optimize_restarts(num_restarts=self.optimize_restarts, optimizer=self.optimizer, max_iters = self.max_iters, verbose=self.verbose)", "response": "Updates the model with new observations."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict(self, X, with_noise=True):\n        m, v = self._predict(X, False, with_noise)\n        # We can take the square root because v is just a diagonal matrix of variances\n        return m, np.sqrt(v)", "response": "Predicts posterior means and standard deviations at X."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef predict_covariance(self, X, with_noise=True):\n        _, v = self._predict(X, True, with_noise)\n        return v", "response": "Predicts the covariance matric for points in X."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict_withGradients(self, X):\n        if X.ndim==1: X = X[None,:]\n        m, v = self.model.predict(X)\n        v = np.clip(v, 1e-10, np.inf)\n        dmdx, dvdx = self.model.predictive_gradients(X)\n        dmdx = dmdx[:,:,0]\n        dsdx = dvdx / (2*np.sqrt(v))\n\n        return m, np.sqrt(v), dmdx, dsdx", "response": "Predict the mean standard deviation mean gradient and standard deviation gradient at X."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a safe copy of the model.", "response": "def copy(self):\n        \"\"\"\n        Makes a safe copy of the model.\n        \"\"\"\n        copied_model = GPModel(kernel = self.model.kern.copy(),\n                            noise_var=self.noise_var,\n                            exact_feval=self.exact_feval,\n                            optimizer=self.optimizer,\n                            max_iters=self.max_iters,\n                            optimize_restarts=self.optimize_restarts,\n                            verbose=self.verbose,\n                            ARD=self.ARD)\n\n        copied_model._create_model(self.model.X,self.model.Y)\n        copied_model.updateModel(self.model.X,self.model.Y, None, None)\n        return copied_model"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_model(self, X, Y):\n\n        # --- define kernel\n        self.input_dim = X.shape[1]\n        if self.kernel is None:\n            kern = GPy.kern.RBF(self.input_dim, variance=1.)\n        else:\n            kern = self.kernel\n            self.kernel = None\n\n        # --- define model\n        noise_var = Y.var()*0.01 if self.noise_var is None else self.noise_var\n        self.model = GPy.models.GPRegression(X, Y, kernel=kern, noise_var=noise_var)\n\n        # --- Define prior on the hyper-parameters for the kernel (for integrated acquisitions)\n        self.model.kern.set_prior(GPy.priors.Gamma.from_EV(2.,4.))\n        self.model.likelihood.variance.set_prior(GPy.priors.Gamma.from_EV(2.,4.))\n\n        # --- Restrict variance if exact evaluations of the objective\n        if self.exact_feval:\n            self.model.Gaussian_noise.constrain_fixed(1e-6, warning=False)\n        else:\n            self.model.Gaussian_noise.constrain_positive(warning=False)", "response": "Creates the model given some input data X and Y."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef updateModel(self, X_all, Y_all, X_new, Y_new):\n\n        if self.model is None:\n            self._create_model(X_all, Y_all)\n        else:\n            self.model.set_XY(X_all, Y_all)\n\n        # update the model generating hmc samples\n        self.model.optimize(max_iters = 200)\n        self.model.param_array[:] = self.model.param_array * (1.+np.random.randn(self.model.param_array.size)*0.01)\n        self.hmc = GPy.inference.mcmc.HMC(self.model, stepsize=self.step_size)\n        ss = self.hmc.sample(num_samples=self.n_burnin + self.n_samples* self.subsample_interval, hmc_iters=self.leapfrog_steps)\n        self.hmc_samples = ss[self.n_burnin::self.subsample_interval]", "response": "Updates the model with new observations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict(self, X):\n\n        if X.ndim==1: X = X[None,:]\n        ps = self.model.param_array.copy()\n        means = []\n        stds = []\n        for s in self.hmc_samples:\n            if self.model._fixes_ is None:\n                self.model[:] = s\n            else:\n                self.model[self.model._fixes_] = s\n            self.model._trigger_params_changed()\n            m, v = self.model.predict(X)\n            means.append(m)\n            stds.append(np.sqrt(np.clip(v, 1e-10, np.inf)))\n        self.model.param_array[:] = ps\n        self.model._trigger_params_changed()\n        return means, stds", "response": "Predicts the posterior means and standard deviations at X."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the location where the posterior mean takes its minimal value.", "response": "def get_fmin(self):\n        \"\"\"\n        Returns the location where the posterior mean is takes its minimal value.\n        \"\"\"\n        ps = self.model.param_array.copy()\n        fmins = []\n        for s in self.hmc_samples:\n            if self.model._fixes_ is None:\n                self.model[:] = s\n            else:\n                self.model[self.model._fixes_] = s\n            self.model._trigger_params_changed()\n            fmins.append(self.model.predict(self.model.X)[0].min())\n        self.model.param_array[:] = ps\n        self.model._trigger_params_changed()\n\n        return fmins"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict_withGradients(self, X):\n        if X.ndim==1: X = X[None,:]\n        ps = self.model.param_array.copy()\n        means = []\n        stds = []\n        dmdxs = []\n        dsdxs = []\n        for s in self.hmc_samples:\n            if self.model._fixes_ is None:\n                self.model[:] = s\n            else:\n                self.model[self.model._fixes_] = s\n            self.model._trigger_params_changed()\n            m, v = self.model.predict(X)\n            std = np.sqrt(np.clip(v, 1e-10, np.inf))\n            dmdx, dvdx = self.model.predictive_gradients(X)\n            dmdx = dmdx[:,:,0]\n            dsdx = dvdx / (2*std)\n            means.append(m)\n            stds.append(std)\n            dmdxs.append(dmdx)\n            dsdxs.append(dsdx)\n        self.model.param_array[:] = ps\n        self.model._trigger_params_changed()\n        return means, stds, dmdxs, dsdxs", "response": "Predicts the mean standard deviation mean gradient and standard deviation gradient at X."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a safe copy of the model.", "response": "def copy(self):\n        \"\"\"\n        Makes a safe copy of the model.\n        \"\"\"\n\n        copied_model = GPModel( kernel = self.model.kern.copy(),\n                                noise_var= self.noise_var ,\n                                exact_feval= self.exact_feval,\n                                n_samples = self.n_samples,\n                                n_burnin = self.n_burnin,\n                                subsample_interval = self.subsample_interval,\n                                step_size = self.step_size,\n                                leapfrog_steps= self.leapfrog_steps,\n                                verbose= self.verbose)\n\n        copied_model._create_model(self.model.X,self.model.Y)\n        copied_model.updateModel(self.model.X,self.model.Y, None, None)\n        return copied_model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef joint_min(mu, var, with_derivatives=False, **kwargs):\n\n    logP = np.zeros(mu.shape)\n    D = mu.shape[0]\n    if with_derivatives:\n        dlogPdMu = np.zeros((D, D))\n        dlogPdSigma = np.zeros((D, int(0.5 * D * (D + 1))))\n        dlogPdMudMu = np.zeros((D, D, D))\n    for i in range(mu.shape[0]):\n\n        # logP[k] ) self._min_faktor(mu, var, 0)\n        a = min_faktor(mu, var, i)\n\n        logP[i] = next(a)\n        if with_derivatives:\n            dlogPdMu[i, :] = next(a).T\n            dlogPdMudMu[i, :, :] = next(a)\n            dlogPdSigma[i, :] = next(a).T\n\n    logP[np.isinf(logP)] = -500\n    # re-normalize at the end, to smooth out numerical imbalances:\n    logPold = logP\n    Z = np.sum(np.exp(logPold))\n    maxLogP = np.max(logP)\n    s = maxLogP + np.log(np.sum(np.exp(logP - maxLogP)))\n    s = maxLogP if np.isinf(s) else s\n\n    logP = logP - s\n    if not with_derivatives:\n        return logP\n\n    dlogPdMuold = dlogPdMu\n    dlogPdSigmaold = dlogPdSigma\n    dlogPdMudMuold = dlogPdMudMu\n    # adjust derivatives, too. This is a bit tedious.\n    Zm = sum(np.rot90((np.exp(logPold) * np.rot90(dlogPdMuold, 1)), 3)) / Z\n    Zs = sum(np.rot90((np.exp(logPold) * np.rot90(dlogPdSigmaold, 1)), 3)) / Z\n\n    dlogPdMu = dlogPdMuold - Zm\n    dlogPdSigma = dlogPdSigmaold - Zs\n\n    ff = np.einsum('ki,kj->kij', dlogPdMuold, dlogPdMuold)\n    gg = np.einsum('kij,k->ij', dlogPdMudMuold + ff, np.exp(logPold)) / Z\n    Zij = Zm.T * Zm\n    adds = np.reshape(-gg + Zij, (1, D, D))\n    dlogPdMudMu = dlogPdMudMuold + adds\n    return logP, dlogPdMu, dlogPdSigma, dlogPdMudMu", "response": "Compute the minimum probability of every given point to be the minimum one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log_relative_gauss(z):\n    if z < -6:\n        return 1, -1.0e12, -1\n    if z > 6:\n        return 0, 0, 1\n    else:\n        logphi = -0.5 * (z * z + l2p)\n        logPhi = np.log(.5 * special.erfc(-z / sq2))\n        e = np.exp(logphi - logPhi)\n    return e, logPhi, 0", "response": "log_relative_gauss - returns the log of the relative Gauss - relative log of the given time z"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_objective(config):\n\n    assert 'prjpath' in config\n    assert 'main-file' in config, \"The problem file ('main-file') is missing!\"\n    \n    os.chdir(config['prjpath'])\n    if config['language'].lower()=='python':\n        assert config['main-file'].endswith('.py'), 'The python problem file has to end with .py!'\n        import imp\n        m = imp.load_source(config['main-file'][:-3], os.path.join(config['prjpath'],config['main-file']))\n        func = m.__dict__[config['main-file'][:-3]]\n    return func", "response": "Loads the objective function from a. json file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_samples(self, init_points_count, criterion='center'):\n        samples = np.empty((init_points_count, self.space.dimensionality))\n\n        # Use random design to fill non-continuous variables\n        random_design = RandomDesign(self.space)\n        random_design.fill_noncontinous_variables(samples)\n\n        if self.space.has_continuous():\n            bounds = self.space.get_continuous_bounds()\n            lower_bound = np.asarray(bounds)[:,0].reshape(1, len(bounds))\n            upper_bound = np.asarray(bounds)[:,1].reshape(1, len(bounds))\n            diff = upper_bound - lower_bound\n\n            from pyDOE import lhs\n            X_design_aux = lhs(len(self.space.get_continuous_bounds()), init_points_count, criterion=criterion)\n            I = np.ones((X_design_aux.shape[0], 1))\n            X_design = np.dot(I, lower_bound) + X_design_aux * np.dot(I, diff)\n\n            samples[:, self.space.get_continuous_dims()] = X_design\n\n        return samples", "response": "Generates required amount of sample points for the current cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_acq(self, x):\n        m, s = self.model.predict(x)\n        fmin = self.model.get_fmin()\n        phi, Phi, u = get_quantiles(self.jitter, fmin, m, s)\n        f_acqu = s * (u * Phi + phi)\n        return f_acqu", "response": "Computes the Expected Improvement per unit of cost for the given time series x"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the Expected Improvement and its derivative", "response": "def _compute_acq_withGradients(self, x):\n        \"\"\"\n        Computes the Expected Improvement and its derivative (has a very easy derivative!)\n        \"\"\"\n        fmin = self.model.get_fmin()\n        m, s, dmdx, dsdx = self.model.predict_withGradients(x)\n        phi, Phi, u = get_quantiles(self.jitter, fmin, m, s)\n        f_acqu = s * (u * Phi + phi)\n        df_acqu = dsdx * phi - Phi * dmdx\n        return f_acqu, df_acqu"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if there is a duplicate entry in the zipped_x", "response": "def is_unzipped_x_duplicate(self, unzipped_x):\n        \"\"\"\n        param: unzipped_x: configuration assumed to be unzipped\n        \"\"\"\n        return self.is_zipped_x_duplicate(self.space.zip_inputs(np.atleast_2d(unzipped_x)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nzip x into a tuple", "response": "def zip_and_tuple(self, x):\n        \"\"\"\n        convenient helper\n        :param x: input configuration in the model space\n        :return: zipped x as a tuple\n        \"\"\"\n        return tuple(self.space.zip_inputs(np.atleast_2d(x)).flatten())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the loaded method configuration with default values.", "response": "def update_config(config_new, config_default):\n\n    '''\n    Updates the loaded method configuration with default values.\n    '''\n    if any([isinstance(v, dict) for v in list(config_new.values())]):\n        for k,v in list(config_new.items()):\n            if isinstance(v,dict) and k in config_default:\n                update_config(config_new[k],config_default[k])\n            else:\n                config_default[k] = v\n    else:\n        config_default.update(config_new)\n    return config_default"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parser(input_file_path='config.json'):\n    '''\n    Parser for the .json file containing the configuration of the method.\n    '''\n\n    # --- Read .json file\n    try:\n        with open(input_file_path, 'r') as config_file:\n            config_new = json.load(config_file)\n            config_file.close()\n    except:\n        raise Exception('Config file \"'+input_file_path+'\" not loaded properly. Please check it an try again.')\n\n    import copy\n    options = update_config(config_new, copy.deepcopy(default_config))\n\n    return options", "response": "Parses the. json file containing the configuration of the method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_samples(self, n_samples, log_p_function, burn_in_steps=50):\n        restarts = initial_design('random', self.space, n_samples)\n        sampler = emcee.EnsembleSampler(n_samples, self.space.input_dim(), log_p_function)\n        samples, samples_log, _ = sampler.run_mcmc(restarts, burn_in_steps)\n\n        # make sure we have an array of shape (n samples, space input dim)\n        if len(samples.shape) == 1:\n            samples = samples.reshape(-1, 1)\n        samples_log = samples_log.reshape(-1, 1)\n\n        return samples, samples_log", "response": "Generates samples.\n\n        Parameters:\n            n_samples - number of samples to generate\n            log_p_function - a function that returns log density for a specific sample\n            burn_in_steps - number of burn-in steps for sampling\n\n        Returns a tuple of two array: (samples, log_p_function values for samples)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns a single optimization step and return the next set of locations to evaluate the objective.", "response": "def suggest_next_locations(self, context = None, pending_X = None, ignored_X = None):\n        \"\"\"\n        Run a single optimization step and return the next locations to evaluate the objective.\n        Number of suggested locations equals to batch_size.\n\n        :param context: fixes specified variables to a particular context (values) for the optimization run (default, None).\n        :param pending_X: matrix of input configurations that are in a pending state (i.e., do not have an evaluation yet) (default, None).\n        :param ignored_X: matrix of input configurations that the user black-lists, i.e., those configurations will not be suggested again (default, None).\n        \"\"\"\n        self.model_parameters_iterations = None\n        self.num_acquisitions = 0\n        self.context = context\n        self._update_model(self.normalization_type)\n\n        suggested_locations = self._compute_next_evaluations(pending_zipped_X = pending_X, ignored_zipped_X = ignored_X)\n\n        return suggested_locations"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the Bayesian Optimization loop.", "response": "def run_optimization(self, max_iter = 0, max_time = np.inf,  eps = 1e-8, context = None, verbosity=False, save_models_parameters= True, report_file = None, evaluations_file = None, models_file=None):\n        \"\"\"\n        Runs Bayesian Optimization for a number 'max_iter' of iterations (after the initial exploration data)\n\n        :param max_iter: exploration horizon, or number of acquisitions. If nothing is provided optimizes the current acquisition.\n        :param max_time: maximum exploration horizon in seconds.\n        :param eps: minimum distance between two consecutive x's to keep running the model.\n        :param context: fixes specified variables to a particular context (values) for the optimization run (default, None).\n        :param verbosity: flag to print the optimization results after each iteration (default, False).\n        :param report_file: file to which the results of the optimization are saved (default, None).\n        :param evaluations_file: file to which the evalations are saved (default, None).\n        :param models_file: file to which the model parameters are saved (default, None).\n        \"\"\"\n\n        if self.objective is None:\n            raise InvalidConfigError(\"Cannot run the optimization loop without the objective function\")\n\n        # --- Save the options to print and save the results\n        self.verbosity = verbosity\n        self.save_models_parameters = save_models_parameters\n        self.report_file = report_file\n        self.evaluations_file = evaluations_file\n        self.models_file = models_file\n        self.model_parameters_iterations = None\n        self.context = context\n\n        # --- Check if we can save the model parameters in each iteration\n        if self.save_models_parameters == True:\n            if not (isinstance(self.model, GPyOpt.models.GPModel) or isinstance(self.model, GPyOpt.models.GPModel_MCMC)):\n                print('Models printout after each iteration is only available for GP and GP_MCMC models')\n                self.save_models_parameters = False\n\n        # --- Setting up stop conditions\n        self.eps = eps\n        if  (max_iter is None) and (max_time is None):\n            self.max_iter = 0\n            self.max_time = np.inf\n        elif (max_iter is None) and (max_time is not None):\n            self.max_iter = np.inf\n            self.max_time = max_time\n        elif (max_iter is not None) and (max_time is None):\n            self.max_iter = max_iter\n            self.max_time = np.inf\n        else:\n            self.max_iter = max_iter\n            self.max_time = max_time\n\n        # --- Initial function evaluation and model fitting\n        if self.X is not None and self.Y is None:\n            self.Y, cost_values = self.objective.evaluate(self.X)\n            if self.cost.cost_type == 'evaluation_time':\n                self.cost.update_cost_model(self.X, cost_values)\n\n        # --- Initialize iterations and running time\n        self.time_zero = time.time()\n        self.cum_time  = 0\n        self.num_acquisitions = 0\n        self.suggested_sample = self.X\n        self.Y_new = self.Y\n\n        # --- Initialize time cost of the evaluations\n        while (self.max_time > self.cum_time):\n            # --- Update model\n            try:\n                self._update_model(self.normalization_type)\n            except np.linalg.linalg.LinAlgError:\n                break\n\n            if (self.num_acquisitions >= self.max_iter\n                    or (len(self.X) > 1 and self._distance_last_evaluations() <= self.eps)):\n                break\n\n            self.suggested_sample = self._compute_next_evaluations()\n\n            # --- Augment X\n            self.X = np.vstack((self.X,self.suggested_sample))\n\n            # --- Evaluate *f* in X, augment Y and update cost function (if needed)\n            self.evaluate_objective()\n\n            # --- Update current evaluation time and function evaluations\n            self.cum_time = time.time() - self.time_zero\n            self.num_acquisitions += 1\n\n            if verbosity:\n                print(\"num acquisition: {}, time elapsed: {:.2f}s\".format(\n                    self.num_acquisitions, self.cum_time))\n\n        # --- Stop messages and execution time\n        self._compute_results()\n\n        # --- Print the desired result in files\n        if self.report_file is not None:\n            self.save_report(self.report_file)\n        if self.evaluations_file is not None:\n            self.save_evaluations(self.evaluations_file)\n        if self.models_file is not None:\n            self.save_models(self.models_file)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _print_convergence(self):\n\n        if self.verbosity:\n            if (self.num_acquisitions == self.max_iter) and (not self.initial_iter):\n                print('   ** Maximum number of iterations reached **')\n                return 1\n            elif (self._distance_last_evaluations() < self.eps) and (not self.initial_iter):\n                print('   ** Two equal location selected **')\n                return 1\n            elif (self.max_time < self.cum_time) and not (self.initial_iter):\n                print('   ** Evaluation time reached **')\n                return 0\n\n            if self.initial_iter:\n                print('** GPyOpt Bayesian Optimization class initialized successfully **')\n                self.initial_iter = False", "response": "Prints the reason why the optimization stopped."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef evaluate_objective(self):\n        self.Y_new, cost_new = self.objective.evaluate(self.suggested_sample)\n        self.cost.update_cost_model(self.suggested_sample, cost_new)\n        self.Y = np.vstack((self.Y,self.Y_new))", "response": "Evaluate the objective and update the cost model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the optimum and its value.", "response": "def _compute_results(self):\n        \"\"\"\n        Computes the optimum and its value.\n        \"\"\"\n        self.Y_best = best_value(self.Y)\n        self.x_opt = self.X[np.argmin(self.Y),:]\n        self.fx_opt = np.min(self.Y)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the distance between the last two evaluations.", "response": "def _distance_last_evaluations(self):\n        \"\"\"\n        Computes the distance between the last two evaluations.\n        \"\"\"\n        if self.X.shape[0] < 2:\n            # less than 2 evaluations\n            return np.inf\n        return np.sqrt(np.sum((self.X[-1, :] - self.X[-2, :]) ** 2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the location of the next evaluation.", "response": "def _compute_next_evaluations(self, pending_zipped_X=None, ignored_zipped_X=None):\n        \"\"\"\n        Computes the location of the new evaluation (optimizes the acquisition in the standard case).\n        :param pending_zipped_X: matrix of input configurations that are in a pending state (i.e., do not have an evaluation yet).\n        :param ignored_zipped_X: matrix of input configurations that the user black-lists, i.e., those configurations will not be suggested again.\n        :return:\n        \"\"\"\n\n        ## --- Update the context if any\n        self.acquisition.optimizer.context_manager = ContextManager(self.space, self.context)\n\n        ### --- Activate de_duplication\n        if self.de_duplication:\n            duplicate_manager = DuplicateManager(space=self.space, zipped_X=self.X, pending_zipped_X=pending_zipped_X, ignored_zipped_X=ignored_zipped_X)\n        else:\n            duplicate_manager = None\n\n        ### We zip the value in case there are categorical variables\n        return self.space.zip_inputs(self.evaluator.compute_batch(duplicate_manager=duplicate_manager, context_manager= self.acquisition.optimizer.context_manager))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_model(self, normalization_type='stats'):\n        if self.num_acquisitions % self.model_update_interval == 0:\n\n            # input that goes into the model (is unziped in case there are categorical variables)\n            X_inmodel = self.space.unzip_inputs(self.X)\n\n            # Y_inmodel is the output that goes into the model\n            if self.normalize_Y:\n                Y_inmodel = normalize(self.Y, normalization_type)\n            else:\n                Y_inmodel = self.Y\n\n            self.model.updateModel(X_inmodel, Y_inmodel, None, None)\n\n        # Save parameters of the model\n        self._save_model_parameter_values()", "response": "Updates the model and saves the parameters of the model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot the acquisition function and the model.", "response": "def plot_acquisition(self,filename=None):\n        \"\"\"\n        Plots the model and the acquisition function.\n            if self.input_dim = 1: Plots data, mean and variance in one plot and the acquisition function in another plot\n            if self.input_dim = 2: as before but it separates the mean and variance of the model in two different plots\n        :param filename: name of the file where the plot is saved\n        \"\"\"\n        if self.model.model is None:\n            from copy import deepcopy\n            model_to_plot = deepcopy(self.model)\n            if self.normalize_Y:\n                Y = normalize(self.Y, self.normalization_type)\n            else:\n                Y = self.Y\n            model_to_plot.updateModel(self.X, Y, self.X, Y)\n        else:\n            model_to_plot = self.model\n\n        return plot_acquisition(self.acquisition.space.get_bounds(),\n                                model_to_plot.model.X.shape[1],\n                                model_to_plot.model,\n                                model_to_plot.model.X,\n                                model_to_plot.model.Y,\n                                self.acquisition.acquisition_function,\n                                self.suggest_next_locations(),\n                                filename)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_report(self, report_file= None):\n\n        with open(report_file,'w') as file:\n            import GPyOpt\n            import time\n\n            file.write('-----------------------------' + ' GPyOpt Report file ' + '-----------------------------------\\n')\n            file.write('GPyOpt Version ' + str(GPyOpt.__version__) + '\\n')\n            file.write('Date and time:               ' + time.strftime(\"%c\")+'\\n')\n            if self.num_acquisitions==self.max_iter:\n                file.write('Optimization completed:      ' +'YES, ' + str(self.X.shape[0]).strip('[]') + ' samples collected.\\n')\n                file.write('Number initial samples:      ' + str(self.initial_design_numdata) +' \\n')\n            else:\n                file.write('Optimization completed:      ' +'NO,' + str(self.X.shape[0]).strip('[]') + ' samples collected.\\n')\n                file.write('Number initial samples:      ' + str(self.initial_design_numdata) +' \\n')\n\n            file.write('Tolerance:                   ' + str(self.eps) + '.\\n')\n            file.write('Optimization time:           ' + str(self.cum_time).strip('[]') +' seconds.\\n')\n\n            file.write('\\n')\n            file.write('--------------------------------' + ' Problem set up ' + '------------------------------------\\n')\n            file.write('Problem name:                ' + self.objective_name +'\\n')\n            file.write('Problem dimension:           ' + str(self.space.dimensionality) +'\\n')\n            file.write('Number continuous variables  ' + str(len(self.space.get_continuous_dims()) ) +'\\n')\n            file.write('Number discrete variables    ' + str(len(self.space.get_discrete_dims())) +'\\n')\n            file.write('Number bandits               ' + str(self.space.get_bandit().shape[0]) +'\\n')\n            file.write('Noiseless evaluations:       ' + str(self.exact_feval) +'\\n')\n            file.write('Cost used:                   ' + self.cost.cost_type +'\\n')\n            file.write('Constraints:                  ' + str(self.constraints==True) +'\\n')\n\n            file.write('\\n')\n            file.write('------------------------------' + ' Optimization set up ' + '---------------------------------\\n')\n            file.write('Normalized outputs:          ' + str(self.normalize_Y) + '\\n')\n            file.write('Model type:                  ' + str(self.model_type).strip('[]') + '\\n')\n            file.write('Model update interval:       ' + str(self.model_update_interval) + '\\n')\n            file.write('Acquisition type:            ' + str(self.acquisition_type).strip('[]') + '\\n')\n            file.write('Acquisition optimizer:       ' + str(self.acquisition_optimizer.optimizer_name).strip('[]') + '\\n')\n\n            file.write('Acquisition type:            ' + str(self.acquisition_type).strip('[]') + '\\n')\n            if hasattr(self, 'acquisition_optimizer') and hasattr(self.acquisition_optimizer, 'optimizer_name'):\n                file.write('Acquisition optimizer:       ' + str(self.acquisition_optimizer.optimizer_name).strip('[]') + '\\n')\n            else:\n                file.write('Acquisition optimizer:       None\\n')\n            file.write('Evaluator type (batch size): ' + str(self.evaluator_type).strip('[]') + ' (' + str(self.batch_size) + ')' + '\\n')\n            file.write('Cores used:                  ' + str(self.num_cores) + '\\n')\n\n            file.write('\\n')\n            file.write('---------------------------------' + ' Summary ' + '------------------------------------------\\n')\n            file.write('Value at minimum:            ' + str(min(self.Y)).strip('[]') +'\\n')\n            file.write('Best found minimum location: ' + str(self.X[np.argmin(self.Y),:]).strip('[]') +'\\n')\n\n            file.write('----------------------------------------------------------------------------------------------\\n')\n            file.close()", "response": "Saves a report with the main results of the optimization."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves the evaluations at each iteration of the optimization.", "response": "def save_evaluations(self, evaluations_file = None):\n        \"\"\"\n        Saves  evaluations at each iteration of the optimization\n\n        :param evaluations_file: name of the file in which the results are saved.\n        \"\"\"\n        iterations = np.array(range(1, self.Y.shape[0] + 1))[:, None]\n        results = np.hstack((iterations, self.Y, self.X))\n        header = ['Iteration', 'Y'] + ['var_' + str(k) for k in range(1, self.X.shape[1] + 1)]\n\n        data = [header] + results.tolist()\n        self._write_csv(evaluations_file, data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the model parameters at each iteration of the optimization.", "response": "def save_models(self, models_file):\n        \"\"\"\n        Saves model parameters at each iteration of the optimization\n\n        :param models_file: name of the file or a file buffer, in which the results are saved.\n        \"\"\"\n        if self.model_parameters_iterations is None:\n            raise ValueError(\"No iterations have been carried out yet and hence no iterations of the BO can be saved\")\n\n        iterations = np.array(range(1,self.model_parameters_iterations.shape[0]+1))[:,None]\n        results = np.hstack((iterations,self.model_parameters_iterations))\n        header = ['Iteration'] + self.model.get_model_parameters_names()\n\n        data = [header] + results.tolist()\n        self._write_csv(models_file, data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the choice of X and Y based on the selected initial design and number of points selected.", "response": "def _init_design_chooser(self):\n        \"\"\"\n        Initializes the choice of X and Y based on the selected initial design and number of points selected.\n        \"\"\"\n\n        # If objective function was not provided, we require some initial sample data\n        if self.f is None and (self.X is None or self.Y is None):\n            raise InvalidConfigError(\"Initial data for both X and Y is required when objective function is not provided\")\n\n        # Case 1:\n        if self.X is None:\n            self.X = initial_design(self.initial_design_type, self.space, self.initial_design_numdata)\n            self.Y, _ = self.objective.evaluate(self.X)\n        # Case 2\n        elif self.X is not None and self.Y is None:\n            self.Y, _ = self.objective.evaluate(self.X)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a test function that will return True when the given datetime matches the given parameters set forth in .", "response": "def crontab(minute='*', hour='*', day='*', month='*', day_of_week='*'):\n    \"\"\"\n    Convert a \"crontab\"-style set of parameters into a test function that will\n    return True when the given datetime matches the parameters set forth in\n    the crontab.\n\n    For day-of-week, 0=Sunday and 6=Saturday.\n\n    Acceptable inputs:\n    * = every distinct value\n    */n = run every \"n\" times, i.e. hours='*/4' == 0, 4, 8, 12, 16, 20\n    m-n = run every time m..n\n    m,n = run on m and n\n    \"\"\"\n    validation = (\n        ('m', month, range(1, 13)),\n        ('d', day, range(1, 32)),\n        ('w', day_of_week, range(8)), # 0-6, but also 7 for Sunday.\n        ('H', hour, range(24)),\n        ('M', minute, range(60))\n    )\n    cron_settings = []\n\n    for (date_str, value, acceptable) in validation:\n        settings = set([])\n\n        if isinstance(value, int):\n            value = str(value)\n\n        for piece in value.split(','):\n            if piece == '*':\n                settings.update(acceptable)\n                continue\n\n            if piece.isdigit():\n                piece = int(piece)\n                if piece not in acceptable:\n                    raise ValueError('%d is not a valid input' % piece)\n                elif date_str == 'w':\n                    piece %= 7\n                settings.add(piece)\n\n            else:\n                dash_match = dash_re.match(piece)\n                if dash_match:\n                    lhs, rhs = map(int, dash_match.groups())\n                    if lhs not in acceptable or rhs not in acceptable:\n                        raise ValueError('%s is not a valid input' % piece)\n                    elif date_str == 'w':\n                        lhs %= 7\n                        rhs %= 7\n                    settings.update(range(lhs, rhs + 1))\n                    continue\n\n                # Handle stuff like */3, */6.\n                every_match = every_re.match(piece)\n                if every_match:\n                    if date_str == 'w':\n                        raise ValueError('Cannot perform this kind of matching'\n                                         ' on day-of-week.')\n                    interval = int(every_match.groups()[0])\n                    settings.update(acceptable[::interval])\n\n        cron_settings.append(sorted(list(settings)))\n\n    def validate_date(timestamp):\n        _, m, d, H, M, _, w, _, _ = timestamp.timetuple()\n\n        # fix the weekday to be sunday=0\n        w = (w + 1) % 7\n\n        for (date_piece, selection) in zip((m, d, w, H, M), cron_settings):\n            if date_piece not in selection:\n                return False\n\n        return True\n\n    return validate_date"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put_if_empty(self, key, value):\n        if self.has_data_for_key(key):\n            return False\n        self.put_data(key, value)\n        return True", "response": "Atomically write data only if the key is not already set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_naive(dt):\n    tt = dt.utctimetuple()\n    ts = calendar.timegm(tt)\n    local_tt = time.localtime(ts)\n    return datetime.datetime(*local_tt[:6])", "response": "Makes an aware datetime. datetime naive in local time zone."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sleep_for_interval(self, start_ts, nseconds):\n        sleep_time = nseconds - (time.time() - start_ts)\n        if sleep_time <= 0:\n            return\n        self._logger.debug('Sleeping for %s', sleep_time)\n        # Recompute time to sleep to improve accuracy in case the process was\n        # pre-empted by the kernel while logging.\n        sleep_time = nseconds - (time.time() - start_ts)\n        if sleep_time > 0:\n            time.sleep(sleep_time)", "response": "Sleep for a given number of seconds."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_process(self, process, name):\n        def _run():\n            process.initialize()\n            try:\n                while not self.stop_flag.is_set():\n                    process.loop()\n            except KeyboardInterrupt:\n                pass\n            except:\n                self._logger.exception('Process %s died!', name)\n        return self.environment.create_process(_run, name)", "response": "Create a process with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts all consumer processes and register signal handlers.", "response": "def start(self):\n        \"\"\"\n        Start all consumer processes and register signal handlers.\n        \"\"\"\n        if self.huey.immediate:\n            raise ConfigurationError(\n                'Consumer cannot be run with Huey instances where immediate '\n                'is enabled. Please check your configuration and ensure that '\n                '\"huey.immediate = False\".')\n        # Log startup message.\n        self._logger.info('Huey consumer started with %s %s, PID %s at %s',\n                          self.workers, self.worker_type, os.getpid(),\n                          self.huey._get_timestamp())\n        self._logger.info('Scheduler runs every %s second(s).',\n                          self.scheduler_interval)\n        self._logger.info('Periodic tasks are %s.',\n                          'enabled' if self.periodic else 'disabled')\n\n        self._set_signal_handlers()\n\n        msg = ['The following commands are available:']\n        for command in self.huey._registry._registry:\n            msg.append('+ %s' % command)\n\n        self._logger.info('\\n'.join(msg))\n\n        # We'll temporarily ignore SIGINT and SIGHUP (so that it is inherited\n        # by the child-processes). Once the child processes are created, we\n        # restore the handler.\n        original_sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n        if hasattr(signal, 'SIGHUP'):\n            original_sighup_handler = signal.signal(signal.SIGHUP, signal.SIG_IGN)\n\n        self.scheduler.start()\n        for _, worker_process in self.worker_threads:\n            worker_process.start()\n\n        signal.signal(signal.SIGINT, original_sigint_handler)\n        if hasattr(signal, 'SIGHUP'):\n            signal.signal(signal.SIGHUP, original_sighup_handler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops the current instance of the class.", "response": "def stop(self, graceful=False):\n        \"\"\"\n        Set the stop-flag.\n\n        If `graceful=True`, this method blocks until the workers to finish\n        executing any tasks they might be currently working on.\n        \"\"\"\n        self.stop_flag.set()\n        if graceful:\n            self._logger.info('Shutting down gracefully...')\n            try:\n                for _, worker_process in self.worker_threads:\n                    worker_process.join()\n            except KeyboardInterrupt:\n                self._logger.info('Received request to shut down now.')\n            else:\n                self._logger.info('All workers have stopped.')\n        else:\n            self._logger.info('Shutting down')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_worker_health(self):\n        self._logger.debug('Checking worker health.')\n        workers = []\n        restart_occurred = False\n        for i, (worker, worker_t) in enumerate(self.worker_threads):\n            if not self.environment.is_alive(worker_t):\n                self._logger.warning('Worker %d died, restarting.', i + 1)\n                worker = self._create_worker()\n                worker_t = self._create_process(worker, 'Worker-%d' % (i + 1))\n                worker_t.start()\n                restart_occurred = True\n            workers.append((worker, worker_t))\n\n        if restart_occurred:\n            self.worker_threads = workers\n        else:\n            self._logger.debug('Workers are up and running.')\n\n        if not self.environment.is_alive(self.scheduler):\n            self._logger.warning('Scheduler died, restarting.')\n            scheduler = self._create_scheduler()\n            self.scheduler = self._create_process(scheduler, 'Scheduler')\n            self.scheduler.start()\n        else:\n            self._logger.debug('Scheduler is up and running.')\n\n        return not restart_occurred", "response": "Check the health of the worker processes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn list of purchasable and not entitled products", "response": "def list(self):\n        \"\"\" return list of purchasable and not entitled products\"\"\"\n        mylist = []\n        for prod in self.product_list:\n            if self.purchasable(prod) and not self.entitled(prod):\n                mylist.append(prod)\n        return mylist"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the 2 high tides for the day.", "response": "def _find_tide_info(predictions):\n    \"\"\"\n     Algorithm to find the 2 high tides for the day, the first of which is smaller and occurs\n     mid-day, the second of which is larger and typically in the evening.\n    \"\"\"\n\n    last_prediction = None\n    first_high_tide = None\n    second_high_tide = None\n    low_tide = None\n    first_tide_done = False\n    for prediction in predictions:\n        if last_prediction is None:\n            last_prediction = prediction\n            continue\n        if last_prediction['v'] < prediction['v']:\n            if not first_tide_done:\n                first_high_tide = prediction\n            else:\n                second_high_tide = prediction\n        else:  # we're decreasing\n            if not first_tide_done and first_high_tide is not None:\n                first_tide_done = True\n            elif second_high_tide is not None:\n                break  # we're decreasing after having found the 2nd tide. We're done.\n            if first_tide_done:\n                low_tide = prediction\n        last_prediction = prediction\n\n    fmt = '%Y-%m-%d %H:%M'\n    parse = datetime.datetime.strptime\n    tideinfo = TideInfo()\n    tideinfo.first_high_tide_time = parse(first_high_tide['t'], fmt)\n    tideinfo.first_high_tide_height = float(first_high_tide['v'])\n    tideinfo.second_high_tide_time = parse(second_high_tide['t'], fmt)\n    tideinfo.second_high_tide_height = float(second_high_tide['v'])\n    tideinfo.low_tide_time = parse(low_tide['t'], fmt)\n    tideinfo.low_tide_height = float(low_tide['v'])\n    return tideinfo"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a Play Directive to begin playback and replace current and enqueued streams.", "response": "def play(self, stream_url, offset=0, opaque_token=None):\n        \"\"\"Sends a Play Directive to begin playback and replace current and enqueued streams.\"\"\"\n\n        self._response['shouldEndSession'] = True\n        directive = self._play_directive('REPLACE_ALL')\n        directive['audioItem'] = self._audio_item(stream_url=stream_url, offset=offset, opaque_token=opaque_token)\n        self._response['directives'].append(directive)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enqueue(self, stream_url, offset=0, opaque_token=None):\n        directive = self._play_directive('ENQUEUE')\n        audio_item = self._audio_item(stream_url=stream_url,\n                                      offset=offset,\n                                      push_buffer=False,\n                                      opaque_token=opaque_token)\n        audio_item['stream']['expectedPreviousToken'] = current_stream.token\n\n        directive['audioItem'] = audio_item\n        self._response['directives'].append(directive)\n        return self", "response": "Adds a stream to the queue. Does not impact the currently playing stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef play_next(self, stream_url=None, offset=0, opaque_token=None):\n\n        directive = self._play_directive('REPLACE_ENQUEUED')\n        directive['audioItem'] = self._audio_item(stream_url=stream_url, offset=offset, opaque_token=opaque_token)\n        self._response['directives'].append(directive)\n        return self", "response": "Replace all streams in the queue but does not impact the currently playing stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a Play Directive to resume playback at the paused offset", "response": "def resume(self):\n        \"\"\"Sends Play Directive to resume playback at the paused offset\"\"\"\n        directive = self._play_directive('REPLACE_ALL')\n        directive['audioItem'] = self._audio_item()\n        self._response['directives'].append(directive)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds an audioItem and updates current_stream", "response": "def _audio_item(self, stream_url=None, offset=0, push_buffer=True, opaque_token=None):\n        \"\"\"Builds an AudioPlayer Directive's audioItem and updates current_stream\"\"\"\n        audio_item = {'stream': {}}\n        stream = audio_item['stream']\n\n        # existing stream\n        if not stream_url:\n            # stream.update(current_stream.__dict__)\n            stream['url'] = current_stream.url\n            stream['token'] = current_stream.token\n            stream['offsetInMilliseconds'] = current_stream.offsetInMilliseconds\n\n        # new stream\n        else:\n            stream['url'] = stream_url\n            stream['token'] = opaque_token or str(uuid.uuid4())\n            stream['offsetInMilliseconds'] = offset\n\n        if push_buffer:  # prevents enqueued streams from becoming current_stream\n            push_stream(stream_cache, context['System']['user']['userId'], stream)\n        return audio_item"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear_queue(self, stop=False):\n\n        directive = {}\n        directive['type'] = 'AudioPlayer.ClearQueue'\n        if stop:\n            directive['clearBehavior'] = 'CLEAR_ALL'\n        else:\n            directive['clearBehavior'] = 'CLEAR_ENQUEUED'\n\n        self._response['directives'].append(directive)\n        return self", "response": "Clears queued streams and optionally stops current stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding our instance of Ask navigating Local s and possible blueprints.", "response": "def find_ask():\n    \"\"\"\n    Find our instance of Ask, navigating Local's and possible blueprints.\n\n    Note: This only supports returning a reference to the first instance\n    of Ask found.\n    \"\"\"\n    if hasattr(current_app, 'ask'):\n        return getattr(current_app, 'ask')\n    else:\n        if hasattr(current_app, 'blueprints'):\n            blueprints = getattr(current_app, 'blueprints')\n            for blueprint_name in blueprints:\n                if hasattr(blueprints[blueprint_name], 'ask'):\n                    return getattr(blueprints[blueprint_name], 'ask')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init_app(self, app, path='templates.yaml'):\n        if self._route is None:\n            raise TypeError(\"route is a required argument when app is not None\")\n\n        self.app = app\n        \n        app.ask = self\n\n        app.add_url_rule(self._route, view_func=self._flask_view_func, methods=['POST'])\n        app.jinja_loader = ChoiceLoader([app.jinja_loader, YamlLoader(app, path)])", "response": "Initializes the Ask app by setting the configuration variables loading templates and maps the route to a flask view."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing a Flask Blueprint with the given path.", "response": "def init_blueprint(self, blueprint, path='templates.yaml'):\n        \"\"\"Initialize a Flask Blueprint, similar to init_app, but without the access\n        to the application config.\n\n        Keyword Arguments:\n            blueprint {Flask Blueprint} -- Flask Blueprint instance to initialize (Default: {None})\n            path {str} -- path to templates yaml file, relative to Blueprint (Default: {'templates.yaml'})\n        \"\"\"\n        if self._route is not None:\n            raise TypeError(\"route cannot be set when using blueprints!\")\n\n        # we need to tuck our reference to this Ask instance into the blueprint object and find it later!\n        blueprint.ask = self\n\n        # BlueprintSetupState.add_url_rule gets called underneath the covers and\n        # concats the rule string, so we should set to an empty string to allow\n        # Blueprint('blueprint_api', __name__, url_prefix=\"/ask\") to result in\n        # exposing the rule at \"/ask\" and not \"/ask/\".\n        blueprint.add_url_rule(\"\", view_func=self._flask_view_func, methods=['POST'])\n        blueprint.jinja_loader = ChoiceLoader([YamlLoader(blueprint, path)])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef session_ended(self, f):\n        self._session_ended_view_func = f\n\n        @wraps(f)\n        def wrapper(*args, **kw):\n            self._flask_view_func(*args, **kw)\n        return f", "response": "Decorator that registers a function as the session_ended view function and renders the response for the end of the session."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef intent(self, intent_name, mapping={}, convert={}, default={}):\n        def decorator(f):\n            self._intent_view_funcs[intent_name] = f\n            self._intent_mappings[intent_name] = mapping\n            self._intent_converts[intent_name] = convert\n            self._intent_defaults[intent_name] = default\n\n            @wraps(f)\n            def wrapper(*args, **kw):\n                self._flask_view_func(*args, **kw)\n            return f\n        return decorator", "response": "Decorator for Alexa IntentRequest and provides the slot parameters to the decorated function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef display_element_selected(self, f):\n        self._display_element_selected_func = f\n\n        @wraps(f)\n        def wrapper(*args, **kw):\n            self._flask_view_func(*args, **kw)\n        return f", "response": "Decorator that registers a function as the display_element_selected view function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_purchase_completed(self, mapping={'payload': 'payload','name':'name','status':'status','token':'token'}, convert={}, default={}):\n        def decorator(f):\n            self._intent_view_funcs['Connections.Response'] = f\n            self._intent_mappings['Connections.Response'] = mapping\n            self._intent_converts['Connections.Response'] = convert\n            self._intent_defaults['Connections.Response'] = default\n            @wraps(f)\n            def wrapper(*args, **kwargs):\n                self._flask_view_func(*args, **kwargs)\n            return f\n        return decorator", "response": "Decorator for Alexa Connections. Response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_aws_lambda(self, event):\n\n        # We are guaranteed to be called by AWS as a Lambda function does not\n        # expose a public facing interface.\n        self.app.config['ASK_VERIFY_REQUESTS'] = False\n\n        # Convert an environment variable to a WSGI \"bytes-as-unicode\" string\n        enc, esc = sys.getfilesystemencoding(), 'surrogateescape'\n        def unicode_to_wsgi(u):\n            return u.encode(enc, esc).decode('iso-8859-1')\n\n        # Create a WSGI-compatible environ that can be passed to the\n        # application. It is loaded with the OS environment variables,\n        # mandatory CGI-like variables, as well as the mandatory WSGI\n        # variables.\n        environ = {k: unicode_to_wsgi(v) for k, v in os.environ.items()}\n        environ['REQUEST_METHOD'] = 'POST'\n        environ['PATH_INFO'] = '/'\n        environ['SERVER_NAME'] = 'AWS-Lambda'\n        environ['SERVER_PORT'] = '80'\n        environ['SERVER_PROTOCOL'] = 'HTTP/1.0'\n        environ['wsgi.version'] = (1, 0)\n        environ['wsgi.url_scheme'] = 'http'\n        environ['wsgi.errors'] = sys.stderr\n        environ['wsgi.multithread'] = False\n        environ['wsgi.multiprocess'] = False\n        environ['wsgi.run_once'] = True\n\n        # Convert the event provided by the AWS Lambda handler to a JSON\n        # string that can be read as the body of a HTTP POST request.\n        body = json.dumps(event)\n        environ['CONTENT_TYPE'] = 'application/json'\n        environ['CONTENT_LENGTH'] = len(body)\n        \n        PY3 = sys.version_info[0] == 3\n        \n        if PY3:\n            environ['wsgi.input'] = io.StringIO(body)\n        else:\n            environ['wsgi.input'] = io.BytesIO(body)\n\n        # Start response is a required callback that must be passed when\n        # the application is invoked. It is used to set HTTP status and\n        # headers. Read the WSGI spec for details (PEP3333).\n        headers = []\n        def start_response(status, response_headers, _exc_info=None):\n            headers[:] = [status, response_headers]\n\n        # Invoke the actual Flask application providing our environment,\n        # with our Alexa event as the body of the HTTP request, as well\n        # as the callback function above. The result will be an iterator\n        # that provides a serialized JSON string for our Alexa response.\n        result = self.app(environ, start_response)\n        try:\n            if not headers:\n                raise AssertionError(\"start_response() not called by WSGI app\")\n\n            output = b\"\".join(result)\n            if not headers[0].startswith(\"2\"):\n                raise AssertionError(\"Non-2xx from app: hdrs={}, body={}\".format(headers, output))\n\n            # The Lambda handler expects a Python object that can be\n            # serialized as JSON, so we need to take the already serialized\n            # JSON and deserialize it.\n            return json.loads(output)\n\n        finally:\n            # Per the WSGI spec, we need to invoke the close method if it\n            # is implemented on the result object.\n            if hasattr(result, 'close'):\n                result.close()", "response": "Invoke the Flask Ask application and return the output generated by the AWS Lambda function handler function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_timestamp(timestamp):\n        if timestamp:\n            try:\n                return aniso8601.parse_datetime(timestamp)\n            except AttributeError:\n                # raised by aniso8601 if raw_timestamp is not valid string\n                # in ISO8601 format\n                try:\n                    return datetime.utcfromtimestamp(timestamp)\n                except:\n                    # relax the timestamp a bit in case it was sent in millis\n                    return datetime.utcfromtimestamp(timestamp/1000)\n\n        raise ValueError('Invalid timestamp value! Cannot parse from either ISO8601 string or UTC timestamp.')", "response": "Parse a given timestamp value raising ValueError if None or Flasey\n        is not a string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide appropiate parameters to the intent functions.", "response": "def _map_intent_to_view_func(self, intent):\n        \"\"\"Provides appropiate parameters to the intent functions.\"\"\"\n        if intent.name in self._intent_view_funcs:\n            view_func = self._intent_view_funcs[intent.name]\n        elif self._default_intent_view_func is not None:\n            view_func = self._default_intent_view_func\n        else:\n            raise NotImplementedError('Intent \"{}\" not found and no default intent specified.'.format(intent.name))\n\n        PY3 = sys.version_info[0] == 3\n        if PY3:\n            argspec = inspect.getfullargspec(view_func)\n        else:\n            argspec = inspect.getargspec(view_func)\n            \n        arg_names = argspec.args\n        arg_values = self._map_params_to_view_args(intent.name, arg_names)\n\n        return partial(view_func, *arg_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprovide appropriate parameters to the on_playback functions.", "response": "def _map_player_request_to_func(self, player_request_type):\n        \"\"\"Provides appropriate parameters to the on_playback functions.\"\"\"\n        # calbacks for on_playback requests are optional\n        view_func = self._intent_view_funcs.get(player_request_type, lambda: None)\n\n        argspec = inspect.getargspec(view_func)\n        arg_names = argspec.args\n        arg_values = self._map_params_to_view_args(player_request_type, arg_names)\n\n        return partial(view_func, *arg_values)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _map_purchase_request_to_func(self, purchase_request_type):\n        \n        if purchase_request_type in self._intent_view_funcs:\n            view_func = self._intent_view_funcs[purchase_request_type]\n        else:\n            raise NotImplementedError('Request type \"{}\" not found and no default view specified.'.format(purchase_request_type)) \n\n        argspec = inspect.getargspec(view_func)\n        arg_names = argspec.args\n        arg_values = self._map_params_to_view_args(purchase_request_type, arg_names)\n\n        print('_map_purchase_request_to_func', arg_names, arg_values, view_func, purchase_request_type)\n        return partial(view_func, *arg_values)", "response": "Provides appropriate parameters to the on_purchase functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npushes a stream onto the stack of user s internal cache.", "response": "def push_stream(cache, user_id, stream):\n    \"\"\"\n    Push a stream onto the stream stack in cache.\n\n    :param cache: werkzeug BasicCache-like object\n    :param user_id: id of user, used as key in cache\n    :param stream: stream object to push onto stack\n\n    :return: True on successful update,\n             False if failed to update,\n             None if invalid input was given\n    \"\"\"\n    stack = cache.get(user_id)\n    if stack is None:\n        stack = []\n    if stream:\n        stack.append(stream)\n        return cache.set(user_id, stack)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npop an item off the stack in the cache.", "response": "def pop_stream(cache, user_id):\n    \"\"\"\n    Pop an item off the stack in the cache. If stack\n    is empty after pop, it deletes the stack.\n\n    :param cache: werkzeug BasicCache-like object\n    :param user_id: id of user, used as key in cache\n\n    :return: top item from stack, otherwise None\n    \"\"\"\n    stack = cache.get(user_id)\n    if stack is None:\n        return None\n\n    result = stack.pop()\n\n    if len(stack) == 0:\n        cache.delete(user_id)\n    else:\n        cache.set(user_id, stack)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef top_stream(cache, user_id):\n    if not user_id:\n        return None\n    \n    stack = cache.get(user_id)\n    if stack is None:\n        return None\n    return stack.pop()", "response": "Return the top item in the cache or None if no such item is available."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps the given XML content into a new XML document.", "response": "def wrap(content, version, account=None):\n    \"\"\"\n    Generate the necessary boilerplate XML for a raw SOAP request. The XML is specific to the server version.\n    ExchangeImpersonation allows to act as the user we want to impersonate.\n    \"\"\"\n    envelope = create_element('s:Envelope', nsmap=ns_translation)\n    header = create_element('s:Header')\n    requestserverversion = create_element('t:RequestServerVersion', Version=version)\n    header.append(requestserverversion)\n    if account:\n        if account.access_type == IMPERSONATION:\n            exchangeimpersonation = create_element('t:ExchangeImpersonation')\n            connectingsid = create_element('t:ConnectingSID')\n            add_xml_child(connectingsid, 't:PrimarySmtpAddress', account.primary_smtp_address)\n            exchangeimpersonation.append(connectingsid)\n            header.append(exchangeimpersonation)\n        timezonecontext = create_element('t:TimeZoneContext')\n        timezonedefinition = create_element('t:TimeZoneDefinition', Id=account.default_timezone.ms_id)\n        timezonecontext.append(timezonedefinition)\n        header.append(timezonecontext)\n    envelope.append(header)\n    body = create_element('s:Body')\n    body.append(content)\n    envelope.append(body)\n    return xml_to_str(envelope, encoding=DEFAULT_ENCODING, xml_declaration=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_auth_instance(credentials, auth_type):\n    model = AUTH_TYPE_MAP[auth_type]\n    if model is None:\n        return None\n    username = credentials.username\n    if auth_type == NTLM and credentials.type == credentials.EMAIL:\n        username = '\\\\' + username\n    if auth_type == GSSAPI:\n        # Kerberos auth relies on credentials supplied via a ticket available externally to this library\n        return model()\n    return model(username=username, password=credentials.password)", "response": "Returns an auth instance suitable for the requests package."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming autodiscover dance and return the primary SMTP address of the account and a Protocol on success.", "response": "def discover(email, credentials):\n    \"\"\"\n    Performs the autodiscover dance and returns the primary SMTP address of the account and a Protocol on success. The\n    autodiscover and EWS server might not be the same, so we use a different Protocol to do the autodiscover request,\n    and return a hopefully-cached Protocol to the callee.\n    \"\"\"\n    log.debug('Attempting autodiscover on email %s', email)\n    if not isinstance(credentials, Credentials):\n        raise ValueError(\"'credentials' %r must be a Credentials instance\" % credentials)\n    domain = get_domain(email)\n    # We may be using multiple different credentials and changing our minds on TLS verification. This key combination\n    # should be safe.\n    autodiscover_key = (domain, credentials)\n    # Use lock to guard against multiple threads competing to cache information\n    log.debug('Waiting for _autodiscover_cache_lock')\n    with _autodiscover_cache_lock:\n        # Don't recurse while holding the lock!\n        log.debug('_autodiscover_cache_lock acquired')\n        if autodiscover_key in _autodiscover_cache:\n            protocol = _autodiscover_cache[autodiscover_key]\n            if not isinstance(protocol, AutodiscoverProtocol):\n                raise ValueError('Unexpected autodiscover cache contents: %s' % protocol)\n            log.debug('Cache hit for domain %s credentials %s: %s', domain, credentials, protocol.server)\n            try:\n                # This is the main path when the cache is primed\n                return _autodiscover_quick(credentials=credentials, email=email, protocol=protocol)\n            except AutoDiscoverFailed:\n                # Autodiscover no longer works with this domain. Clear cache and try again after releasing the lock\n                del _autodiscover_cache[autodiscover_key]\n            except AutoDiscoverRedirect as e:\n                log.debug('%s redirects to %s', email, e.redirect_email)\n                if email.lower() == e.redirect_email.lower():\n                    raise_from(AutoDiscoverCircularRedirect('Redirect to same email address: %s' % email), None)\n                # Start over with the new email address after releasing the lock\n                email = e.redirect_email\n        else:\n            log.debug('Cache miss for domain %s credentials %s', domain, credentials)\n            log.debug('Cache contents: %s', _autodiscover_cache)\n            try:\n                # This eventually fills the cache in _autodiscover_hostname\n                return _try_autodiscover(hostname=domain, credentials=credentials, email=email)\n            except AutoDiscoverRedirect as e:\n                if email.lower() == e.redirect_email.lower():\n                    raise_from(AutoDiscoverCircularRedirect('Redirect to same email address: %s' % email), None)\n                log.debug('%s redirects to %s', email, e.redirect_email)\n                # Start over with the new email address after releasing the lock\n                email = e.redirect_email\n    log.debug('Released autodiscover_cache_lock')\n    # We fell out of the with statement, so either cache was filled by someone else, or autodiscover redirected us to\n    # another email address. Start over after releasing the lock.\n    return discover(email=email, credentials=credentials)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the individual parts of a field path that may apart from the fieldname have label and subfield parts.", "response": "def split_field_path(field_path):\n    \"\"\"Return the individual parts of a field path that may, apart from the fieldname, have label and subfield parts.\n    Examples:\n        'start' -> ('start', None, None)\n        'phone_numbers__PrimaryPhone' -> ('phone_numbers', 'PrimaryPhone', None)\n        'physical_addresses__Home__street' -> ('physical_addresses', 'Home', 'street')\n    \"\"\"\n    if not isinstance(field_path, string_types):\n        raise ValueError(\"Field path %r must be a string\" % field_path)\n    search_parts = field_path.split('__')\n    field = search_parts[0]\n    try:\n        label = search_parts[1]\n    except IndexError:\n        label = None\n    try:\n        subfield = search_parts[2]\n    except IndexError:\n        subfield = None\n    return field, label, subfield"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_server_timezone(self, timezones, for_year):\n        candidates = set()\n        for tz_id, tz_name, tz_periods, tz_transitions, tz_transitions_groups in timezones:\n            candidate = self.from_server_timezone(tz_periods, tz_transitions, tz_transitions_groups, for_year)\n            if candidate == self:\n                log.debug('Found exact candidate: %s (%s)', tz_id, tz_name)\n                # We prefer this timezone over anything else. Return immediately.\n                return tz_id\n            # Reduce list based on base bias and standard / daylight bias values\n            if candidate.bias != self.bias:\n                continue\n            if candidate.standard_time is None:\n                if self.standard_time is not None:\n                    continue\n            else:\n                if self.standard_time is None:\n                    continue\n                if candidate.standard_time.bias != self.standard_time.bias:\n                    continue\n            if candidate.daylight_time is None:\n                if self.daylight_time is not None:\n                    continue\n            else:\n                if self.daylight_time is None:\n                    continue\n                if candidate.daylight_time.bias != self.daylight_time.bias:\n                    continue\n            log.debug('Found candidate with matching biases: %s (%s)', tz_id, tz_name)\n            candidates.add(tz_id)\n        if not candidates:\n            raise ValueError('No server timezones match this timezone definition')\n        if len(candidates) == 1:\n            log.info('Could not find an exact timezone match for %s. Selecting the best candidate', self)\n        else:\n            log.warning('Could not find an exact timezone match for %s. Selecting a random candidate', self)\n        return candidates.pop()", "response": "Returns the Microsoft timezone ID corresponding to this timezone definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecreases the session pool size in response to error messages from the server requesting to rate - limit requests. We increase by one session per call.", "response": "def decrease_poolsize(self):\n        \"\"\"Decreases the session pool size in response to error messages from the server requesting to rate-limit\n        requests. We decrease by one session per call.\n        \"\"\"\n        # Take a single session from the pool and discard it. We need to protect this with a lock while we are changing\n        # the pool size variable, to avoid race conditions. We must keep at least one session in the pool.\n        if self._session_pool_size <= 1:\n            raise SessionPoolMinSizeReached('Session pool size cannot be decreased further')\n        with self._session_pool_lock:\n            if self._session_pool_size <= 1:\n                log.debug('Session pool size was decreased in another thread')\n                return\n            log.warning('Lowering session pool size from %s to %s', self._session_pool_size,\n                        self._session_pool_size - 1)\n            self.get_session().close()\n            self._session_pool_size -= 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if value is a list - like object.", "response": "def is_iterable(value, generators_allowed=False):\n    \"\"\"\n    Checks if value is a list-like object. Don't match generators and generator-like objects here by default, because\n    callers don't necessarily guarantee that they only iterate the value once. Take care to not match string types and\n    bytes.\n\n    :param value: any type of object\n    :param generators_allowed: if True, generators will be treated as iterable\n    :return: True or False\n    \"\"\"\n    if generators_allowed:\n        if not isinstance(value, string_types + (bytes,)) and hasattr(value, '__iter__'):\n            return True\n    else:\n        if isinstance(value, (tuple, list, set)):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit an iterable into chunks of size chunksize.", "response": "def chunkify(iterable, chunksize):\n    \"\"\"\n    Splits an iterable into chunks of size ``chunksize``. The last chunk may be smaller than ``chunksize``.\n    \"\"\"\n    from .queryset import QuerySet\n    if hasattr(iterable, '__getitem__') and not isinstance(iterable, QuerySet):\n        # tuple, list. QuerySet has __getitem__ but that evaluates the entire query greedily. We don't want that here.\n        for i in range(0, len(iterable), chunksize):\n            yield iterable[i:i + chunksize]\n    else:\n        # generator, set, map, QuerySet\n        chunk = []\n        for i in iterable:\n            chunk.append(i)\n            if len(chunk) == chunksize:\n                yield chunk\n                chunk = []\n        if chunk:\n            yield chunk"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if an iterable is empty and returns status and the rewinded iterable", "response": "def peek(iterable):\n    \"\"\"\n    Checks if an iterable is empty and returns status and the rewinded iterable\n    \"\"\"\n    from .queryset import QuerySet\n    if isinstance(iterable, QuerySet):\n        # QuerySet has __len__ but that evaluates the entire query greedily. We don't want that here. Instead, peek()\n        # should be called on QuerySet.iterator()\n        raise ValueError('Cannot peek on a QuerySet')\n    if hasattr(iterable, '__len__'):\n        # tuple, list, set\n        return not iterable, iterable\n    # generator\n    try:\n        first = next(iterable)\n    except StopIteration:\n        return True, iterable\n    # We can't rewind a generator. Instead, chain the first element and the rest of the generator\n    return False, itertools.chain([first], iterable)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nserializes an XML tree. Returns unicode if encoding is None. Otherwise we return encoded bytes.", "response": "def xml_to_str(tree, encoding=None, xml_declaration=False):\n    \"\"\"Serialize an XML tree. Returns unicode if 'encoding' is None. Otherwise, we return encoded 'bytes'.\"\"\"\n    if xml_declaration and not encoding:\n        raise ValueError(\"'xml_declaration' is not supported when 'encoding' is None\")\n    if encoding:\n        return tostring(tree, encoding=encoding, xml_declaration=True)\n    return tostring(tree, encoding=text_type, xml_declaration=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post_ratelimited(protocol, session, url, headers, data, allow_redirects=False, stream=False):\n    thread_id = get_ident()\n    wait = 10  # seconds\n    retry = 0\n    redirects = 0\n    # In Python 2, we want this to be a 'str' object so logging doesn't break (all formatting arguments are 'str').\n    # We activated 'unicode_literals' at the top of this file, so it would be a 'unicode' object unless we convert\n    # to 'str' explicitly. This is a no-op for Python 3.\n    log_msg = str('''\\\nRetry: %(retry)s\nWaited: %(wait)s\nTimeout: %(timeout)s\nSession: %(session_id)s\nThread: %(thread_id)s\nAuth type: %(auth)s\nURL: %(url)s\nHTTP adapter: %(adapter)s\nAllow redirects: %(allow_redirects)s\nStreaming: %(stream)s\nResponse time: %(response_time)s\nStatus code: %(status_code)s\nRequest headers: %(request_headers)s\nResponse headers: %(response_headers)s\nRequest data: %(xml_request)s\nResponse data: %(xml_response)s\n''')\n    log_vals = dict(\n        retry=retry,\n        wait=wait,\n        timeout=protocol.TIMEOUT,\n        session_id=session.session_id,\n        thread_id=thread_id,\n        auth=session.auth,\n        url=url,\n        adapter=session.get_adapter(url),\n        allow_redirects=allow_redirects,\n        stream=stream,\n        response_time=None,\n        status_code=None,\n        request_headers=headers,\n        response_headers=None,\n        xml_request=data,\n        xml_response=None,\n    )\n    try:\n        while True:\n            _back_off_if_needed(protocol.credentials.back_off_until)\n            log.debug('Session %s thread %s: retry %s timeout %s POST\\'ing to %s after %ss wait', session.session_id,\n                      thread_id, retry, protocol.TIMEOUT, url, wait)\n            d_start = time_func()\n            # Always create a dummy response for logging purposes, in case we fail in the following\n            r = DummyResponse(url=url, headers={}, request_headers=headers)\n            try:\n                r = session.post(url=url, headers=headers, data=data, allow_redirects=False, timeout=protocol.TIMEOUT,\n                                 stream=stream)\n            except CONNECTION_ERRORS as e:\n                log.debug('Session %s thread %s: connection error POST\\'ing to %s', session.session_id, thread_id, url)\n                r = DummyResponse(url=url, headers={'TimeoutException': e}, request_headers=headers)\n            finally:\n                log_vals.update(\n                    retry=retry,\n                    wait=wait,\n                    session_id=session.session_id,\n                    url=str(r.url),\n                    response_time=time_func() - d_start,\n                    status_code=r.status_code,\n                    request_headers=r.request.headers,\n                    response_headers=r.headers,\n                    xml_response='[STREAMING]' if stream else r.content,\n                )\n            log.debug(log_msg, log_vals)\n            if _may_retry_on_error(r, protocol, wait):\n                log.info(\"Session %s thread %s: Connection error on URL %s (code %s). Cool down %s secs\",\n                         session.session_id, thread_id, r.url, r.status_code, wait)\n                time.sleep(wait)  # Increase delay for every retry\n                retry += 1\n                wait *= 2\n                session = protocol.renew_session(session)\n                continue\n            if r.status_code in (301, 302):\n                if stream:\n                    r.close()\n                url, redirects = _redirect_or_fail(r, redirects, allow_redirects)\n                continue\n            break\n    except (RateLimitError, RedirectError) as e:\n        log.warning(e.value)\n        protocol.retire_session(session)\n        raise\n    except Exception as e:\n        # Let higher layers handle this. Add full context for better debugging.\n        log.error(str('%s: %s\\n%s'), e.__class__.__name__, str(e), log_msg % log_vals)\n        protocol.retire_session(session)\n        raise\n    if r.status_code == 500 and r.content and is_xml(r.content):\n        # Some genius at Microsoft thinks it's OK to send a valid SOAP response as an HTTP 500\n        log.debug('Got status code %s but trying to parse content anyway', r.status_code)\n    elif r.status_code != 200:\n        protocol.retire_session(session)\n        try:\n            _raise_response_errors(r, protocol, log_msg, log_vals)  # Always raises an exception\n        finally:\n            if stream:\n                r.close()\n    log.debug('Session %s thread %s: Useful response from %s', session.session_id, thread_id, url)\n    return r, session", "response": "This function is used to make POST requests on a throttled batch of requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call(self, items, additional_fields, shape):\n        return self._pool_requests(payload_func=self.get_payload, **dict(\n            items=items,\n            additional_fields=additional_fields,\n            shape=shape,\n        ))", "response": "This method is used to call the get method of the object that is used to retrieve the items in an account that correspond to a list of ID s."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef call(self, additional_fields, restriction, order_fields, shape, query_string, depth, calendar_view, max_items,\n             offset):\n        \"\"\"\n        Find items in an account.\n\n        :param additional_fields: the extra fields that should be returned with the item, as FieldPath objects\n        :param restriction: a Restriction object for\n        :param order_fields: the fields to sort the results by\n        :param shape: The set of attributes to return\n        :param query_string: a QueryString object\n        :param depth: How deep in the folder structure to search for items\n        :param calendar_view: If set, returns recurring calendar items unfolded\n        :param max_items: the max number of items to return\n        :param offset: the offset relative to the first item in the item collection. Usually 0.\n        :return: XML elements for the matching items\n        \"\"\"\n        return self._paged_call(payload_func=self.get_payload, max_items=max_items, **dict(\n            additional_fields=additional_fields,\n            restriction=restriction,\n            order_fields=order_fields,\n            query_string=query_string,\n            shape=shape,\n            depth=depth,\n            calendar_view=calendar_view,\n            page_size=self.chunk_size,\n            offset=offset,\n        ))", "response": "This method will call the get_payload method of the item collection API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef call(self, additional_fields, restriction, shape, depth, max_items, offset):\n        from .folders import Folder\n        roots = {f.root for f in self.folders}\n        if len(roots) != 1:\n            raise ValueError('FindFolder must be called with folders in the same root hierarchy (%r)' % roots)\n        root = roots.pop()\n        for elem in self._paged_call(payload_func=self.get_payload, max_items=max_items, **dict(\n            additional_fields=additional_fields,\n            restriction=restriction,\n            shape=shape,\n            depth=depth,\n            page_size=self.chunk_size,\n            offset=offset,\n        )):\n            if isinstance(elem, Exception):\n                yield elem\n                continue\n            yield Folder.from_xml(elem=elem, root=root)", "response": "Find subfolders of a folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the full information for a folder.", "response": "def call(self, folders, additional_fields, shape):\n        \"\"\"\n        Takes a folder ID and returns the full information for that folder.\n\n        :param folders: a list of Folder objects\n        :param additional_fields: the extra fields that should be returned with the folder, as FieldPath objects\n        :param shape: The set of attributes to return\n        :return: XML elements for the folders, in stable order\n        \"\"\"\n        # We can't easily find the correct folder class from the returned XML. Instead, return objects with the same\n        # class as the folder instance it was requested with.\n        from .folders import Folder, DistinguishedFolderId, RootOfHierarchy\n        folders_list = list(folders)  # Convert to a list, in case 'folders' is a generator\n        for folder, elem in zip(folders_list, self._get_elements(payload=self.get_payload(\n            folders=folders,\n            additional_fields=additional_fields,\n            shape=shape,\n        ))):\n            if isinstance(elem, Exception):\n                yield elem\n                continue\n            if isinstance(folder, RootOfHierarchy):\n                f = folder.from_xml(elem=elem, account=self.account)\n            elif isinstance(folder, Folder):\n                f = folder.from_xml(elem=elem, root=folder.root)\n            elif isinstance(folder, DistinguishedFolderId):\n                # We don't know the root, so assume account.root.\n                for folder_cls in self.account.root.WELLKNOWN_FOLDERS:\n                    if folder_cls.DISTINGUISHED_FOLDER_ID == folder.id:\n                        break\n                else:\n                    raise ValueError('Unknown distinguished folder ID: %s', folder.id)\n                f = folder_cls.from_xml(elem=elem, root=self.account.root)\n            else:\n                # 'folder' is a generic FolderId instance. We don't know the root so assume account.root.\n                f = Folder.from_xml(elem=elem, root=self.account.root)\n            if isinstance(folder, DistinguishedFolderId):\n                f.is_distinguished = True\n            elif isinstance(folder, Folder) and folder.is_distinguished:\n                f.is_distinguished = True\n            yield f"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call(self, folder, additional_fields, restriction, order_fields, shape, query_string, depth, max_items, offset):\n        from .items import Persona, ID_ONLY\n        personas = self._paged_call(payload_func=self.get_payload, max_items=max_items, **dict(\n            folder=folder,\n            additional_fields=additional_fields,\n            restriction=restriction,\n            order_fields=order_fields,\n            query_string=query_string,\n            shape=shape,\n            depth=depth,\n            page_size=self.chunk_size,\n            offset=offset,\n        ))\n        if shape == ID_ONLY and additional_fields is None:\n            for p in personas:\n                yield p if isinstance(p, Exception) else Persona.id_from_xml(p)\n        else:\n            for p in personas:\n                yield p if isinstance(p, Exception) else Persona.from_xml(p, account=self.account)", "response": "This method returns the XML elements of the items in an account."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_payload(self, items):\n        from .properties import ParentFolderId\n        uploaditems = create_element('m:%s' % self.SERVICE_NAME)\n        itemselement = create_element('m:Items')\n        uploaditems.append(itemselement)\n        for parent_folder, data_str in items:\n            item = create_element('t:Item', CreateAction='CreateNew')\n            parentfolderid = ParentFolderId(parent_folder.id, parent_folder.changekey)\n            set_xml_value(item, parentfolderid, version=self.account.version)\n            add_xml_child(item, 't:Data', data_str)\n            itemselement.append(item)\n        return uploaditems", "response": "Returns the XML payload for the given items to given account\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef view(self, start, end, max_items=None, *args, **kwargs):\n        qs = QuerySet(self).filter(*args, **kwargs)\n        qs.calendar_view = CalendarView(start=start, end=end, max_items=max_items)\n        return qs", "response": "Returns a new QuerySet with the specified start and end timespan and optional arguments."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_items(self, q, shape=ID_ONLY, depth=SHALLOW, additional_fields=None, order_fields=None,\n                   calendar_view=None, page_size=None, max_items=None, offset=0):\n        \"\"\"\n        Private method to call the FindItem service\n\n        :param q: a Q instance containing any restrictions\n        :param shape: controls whether to return (id, chanegkey) tuples or Item objects. If additional_fields is\n               non-null, we always return Item objects.\n        :param depth: controls the whether to return soft-deleted items or not.\n        :param additional_fields: the extra properties we want on the return objects. Default is no properties. Be\n               aware that complex fields can only be fetched with fetch() (i.e. the GetItem service).\n        :param order_fields: the SortOrder fields, if any\n        :param calendar_view: a CalendarView instance, if any\n        :param page_size: the requested number of items per page\n        :param max_items: the max number of items to return\n        :param offset: the offset relative to the first item in the item collection\n        :return: a generator for the returned item IDs or items\n        \"\"\"\n        if shape not in SHAPE_CHOICES:\n            raise ValueError(\"'shape' %s must be one of %s\" % (shape, SHAPE_CHOICES))\n        if depth not in ITEM_TRAVERSAL_CHOICES:\n            raise ValueError(\"'depth' %s must be one of %s\" % (depth, ITEM_TRAVERSAL_CHOICES))\n        if not self.folders:\n            log.debug('Folder list is empty')\n            return\n        if additional_fields:\n            for f in additional_fields:\n                self.validate_item_field(field=f)\n            for f in additional_fields:\n                if f.field.is_complex:\n                    raise ValueError(\"find_items() does not support field '%s'. Use fetch() instead\" % f.field.name)\n        if calendar_view is not None and not isinstance(calendar_view, CalendarView):\n            raise ValueError(\"'calendar_view' %s must be a CalendarView instance\" % calendar_view)\n\n        # Build up any restrictions\n        if q.is_empty():\n            restriction = None\n            query_string = None\n        elif q.query_string:\n            restriction = None\n            query_string = Restriction(q, folders=self.folders, applies_to=Restriction.ITEMS)\n        else:\n            restriction = Restriction(q, folders=self.folders, applies_to=Restriction.ITEMS)\n            query_string = None\n        log.debug(\n            'Finding %s items in folders %s (shape: %s, depth: %s, additional_fields: %s, restriction: %s)',\n            self.folders,\n            self.account,\n            shape,\n            depth,\n            additional_fields,\n            restriction.q if restriction else None,\n        )\n        items = FindItem(account=self.account, folders=self.folders, chunk_size=page_size).call(\n            additional_fields=additional_fields,\n            restriction=restriction,\n            order_fields=order_fields,\n            shape=shape,\n            query_string=query_string,\n            depth=depth,\n            calendar_view=calendar_view,\n            max_items=calendar_view.max_items if calendar_view else max_items,\n            offset=offset,\n        )\n        if shape == ID_ONLY and additional_fields is None:\n            for i in items:\n                yield i if isinstance(i, Exception) else Item.id_from_xml(i)\n        else:\n            for i in items:\n                if isinstance(i, Exception):\n                    yield i\n                else:\n                    yield Folder.item_model_from_tag(i.tag).from_xml(elem=i, account=self.account)", "response": "Private method to find items in a folder list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the distinguished folder for this folder class.", "response": "def get_distinguished(cls, account):\n        \"\"\"Gets the distinguished folder for this folder class\"\"\"\n        if not cls.DISTINGUISHED_FOLDER_ID:\n            raise ValueError('Class %s must have a DISTINGUISHED_FOLDER_ID value' % cls)\n        folders = list(FolderCollection(\n            account=account,\n            folders=[cls(account=account, name=cls.DISTINGUISHED_FOLDER_ID, is_distinguished=True)]\n        ).get_folders()\n        )\n        if not folders:\n            raise ErrorFolderNotFound('Could not find distinguished folder %s' % cls.DISTINGUISHED_FOLDER_ID)\n        if len(folders) != 1:\n            raise ValueError('Expected result length 1, but got %s' % folders)\n        folder = folders[0]\n        if isinstance(folder, Exception):\n            raise folder\n        if folder.__class__ != cls:\n            raise ValueError(\"Expected 'folder' %r to be a %s instance\" % (folder, cls))\n        return folder"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef folder_cls_from_folder_name(cls, folder_name, locale):\n        for folder_cls in cls.WELLKNOWN_FOLDERS + NON_DELETEABLE_FOLDERS:\n            if folder_name.lower() in folder_cls.localized_names(locale):\n                return folder_cls\n        raise KeyError()", "response": "Returns the folder class that matches a localized folder name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a custom extended property in this item class so they can be accessed just like any other attribute.", "response": "def register(cls, attr_name, attr_cls):\n        \"\"\"\n        Register a custom extended property in this item class so they can be accessed just like any other attribute\n        \"\"\"\n        if not cls.INSERT_AFTER_FIELD:\n            raise ValueError('Class %s is missing INSERT_AFTER_FIELD value' % cls)\n        try:\n            cls.get_field_by_fieldname(attr_name)\n        except InvalidField:\n            pass\n        else:\n            raise ValueError(\"'%s' is already registered\" % attr_name)\n        if not issubclass(attr_cls, ExtendedProperty):\n            raise ValueError(\"%r must be a subclass of ExtendedProperty\" % attr_cls)\n        # Check if class attributes are properly defined\n        attr_cls.validate_cls()\n        # ExtendedProperty is not a real field, but a placeholder in the fields list. See\n        #   https://msdn.microsoft.com/en-us/library/office/aa580790(v=exchg.150).aspx\n        #\n        # Find the correct index for the new extended property, and insert.\n        field = ExtendedPropertyField(attr_name, value_cls=attr_cls)\n        cls.add_field(field, insert_after=cls.INSERT_AFTER_FIELD)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deregister(cls, attr_name):\n        try:\n            field = cls.get_field_by_fieldname(attr_name)\n        except InvalidField:\n            raise ValueError(\"'%s' is not registered\" % attr_name)\n        if not isinstance(field, ExtendedPropertyField):\n            raise ValueError(\"'%s' is not registered as an ExtendedProperty\" % attr_name)\n        cls.remove_field(field)", "response": "De - register an extended property that has been registered with register"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an attachment or a list of attachments to this item.", "response": "def attach(self, attachments):\n        \"\"\"Add an attachment, or a list of attachments, to this item. If the item has already been saved, the\n        attachments will be created on the server immediately. If the item has not yet been saved, the attachments will\n        be created on the server the item is saved.\n\n        Adding attachments to an existing item will update the changekey of the item.\n        \"\"\"\n        if not is_iterable(attachments, generators_allowed=True):\n            attachments = [attachments]\n        for a in attachments:\n            if not a.parent_item:\n                a.parent_item = self\n            if self.id and not a.attachment_id:\n                # Already saved object. Attach the attachment server-side now\n                a.attach()\n            if a not in self.attachments:\n                self.attachments.append(a)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetaches an item or a list of attachments from this item.", "response": "def detach(self, attachments):\n        \"\"\"Remove an attachment, or a list of attachments, from this item. If the item has already been saved, the\n        attachments will be deleted on the server immediately. If the item has not yet been saved, the attachments will\n        simply not be created on the server the item is saved.\n\n        Removing attachments from an existing item will update the changekey of the item.\n        \"\"\"\n        if not is_iterable(attachments, generators_allowed=True):\n            attachments = [attachments]\n        for a in attachments:\n            if a.parent_item is not self:\n                raise ValueError('Attachment does not belong to this item')\n            if self.id:\n                # Item is already created. Detach  the attachment server-side now\n                a.detach()\n            if a in self.attachments:\n                self.attachments.remove(a)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, folder):\n        if not self.account:\n            raise ValueError('%s must have an account' % self.__class__.__name__)\n        res = self.account.bulk_create(items=[self], folder=folder, message_disposition=SAVE_ONLY)\n        if res and isinstance(res[0], Exception):\n            raise res[0]\n        res = list(self.account.fetch(res)) # retrieve result\n        if res and isinstance(res[0], Exception):\n            raise res[0]\n        return res[0]", "response": "Save the item to the given folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef back_off_until(self):\n        if self._back_off_until is None:\n            return None\n        with self._back_off_lock:\n            if self._back_off_until is None:\n                return None\n            if self._back_off_until < datetime.datetime.now():\n                self._back_off_until = None  # The backoff value has expired. Reset\n                return None\n            return self._back_off_until", "response": "Returns the back off value as a datetime. Resets the current back off value if it has expired."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning ISO 8601 format to satisfy xs : datetime as interpreted by EWS.", "response": "def ewsformat(self):\n        \"\"\"\n        ISO 8601 format to satisfy xs:datetime as interpreted by EWS. Examples:\n            2009-01-15T13:45:56Z\n            2009-01-15T13:45:56+01:00\n        \"\"\"\n        if not self.tzinfo:\n            raise ValueError('EWSDateTime must be timezone-aware')\n        if self.tzinfo.zone == 'UTC':\n            return self.strftime('%Y-%m-%dT%H:%M:%SZ')\n        return self.replace(microsecond=0).isoformat()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a map from the CLDR database to the CLDR server.", "response": "def generate_map(timeout=10):\n    \"\"\" Helper method to update the map if the CLDR database is updated \"\"\"\n    r = requests.get(CLDR_WINZONE_URL, timeout=timeout)\n    if r.status_code != 200:\n        raise ValueError('Unexpected response: %s' % r)\n    tz_map = {}\n    for e in to_xml(r.content).find('windowsZones').find('mapTimezones').findall('mapZone'):\n        for location in e.get('type').split(' '):\n            if e.get('territory') == DEFAULT_TERRITORY or location not in tz_map:\n                # Prefer default territory. This is so MS_TIMEZONE_TO_PYTZ_MAP maps from MS timezone ID back to the\n                # \"preferred\" region/location timezone name.\n                tz_map[location] = e.get('other'), e.get('territory')\n    return tz_map"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run():\n    obj = CompareGOsCli()\n    obj.write(obj.kws.get('xlsx'), obj.kws.get('ofile'), obj.kws.get('verbose', False))", "response": "Compare two or more GO IDs. Best done using sections."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sortby(self, ntd):\n        if 'reldepth' in self.grprobj.gosubdag.prt_attr['flds']:\n            return [ntd.NS, -1*ntd.dcnt, ntd.reldepth]\n        else:\n            return [ntd.NS, -1*ntd.dcnt, ntd.depth]", "response": "Return function for sorting."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_nts_sorted(self, hdrgo_prt, hdrgos, hdrgo_sort):\n        nts_flat = []\n        self.get_sorted_hdrgo2usrgos(hdrgos, nts_flat, hdrgo_prt, hdrgo_sort)\n        return nts_flat", "response": "Return a flat list of grouped and sorted GO terms."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning GO IDs sorting using go2nt s namedtuple.", "response": "def get_sorted_hdrgo2usrgos(self, hdrgos, flat_list=None, hdrgo_prt=True, hdrgo_sort=True):\n        \"\"\"Return GO IDs sorting using go2nt's namedtuple.\"\"\"\n        # Return user-specfied sort or default sort of header and user GO IDs\n        sorted_hdrgos_usrgos = []\n        h2u_get = self.grprobj.hdrgo2usrgos.get\n        # Sort GO group headers using GO info in go2nt\n        hdr_go2nt = self._get_go2nt(hdrgos)\n        if hdrgo_sort is True:\n            hdr_go2nt = sorted(hdr_go2nt.items(), key=lambda t: self.hdrgo_sortby(t[1]))\n        for hdrgo_id, hdrgo_nt in hdr_go2nt:\n            if flat_list is not None:\n                if hdrgo_prt or hdrgo_id in self.grprobj.usrgos:\n                    flat_list.append(hdrgo_nt)\n            # Sort user GOs which are under the current GO header\n            usrgos_unsorted = h2u_get(hdrgo_id)\n            if usrgos_unsorted:\n                usrgo2nt = self._get_go2nt(usrgos_unsorted)\n                usrgont_sorted = sorted(usrgo2nt.items(), key=lambda t: self.usrgo_sortby(t[1]))\n                usrgos_sorted, usrnts_sorted = zip(*usrgont_sorted)\n                if flat_list is not None:\n                    flat_list.extend(usrnts_sorted)\n                sorted_hdrgos_usrgos.append((hdrgo_id, usrgos_sorted))\n            else:\n                sorted_hdrgos_usrgos.append((hdrgo_id, []))\n        return cx.OrderedDict(sorted_hdrgos_usrgos)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_go2nt(self, goids):\n        go2nt_all = self.grprobj.go2nt\n        return {go:go2nt_all[go] for go in goids}", "response": "Get go2nt for given goids."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_hdrgo_sortby(self, hdrgo_sortby, sortby):\n        if hdrgo_sortby is not None:\n            return hdrgo_sortby\n        if sortby is not None:\n            return sortby\n        return self.sortby", "response": "Initialize header sort function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_gos(fout_png, goids, obo_dag, *args, **kws):\n    engine = kws['engine'] if 'engine' in kws else 'pydot'\n    godagsmall = OboToGoDagSmall(goids=goids, obodag=obo_dag).godag\n    godagplot = GODagSmallPlot(godagsmall, *args, **kws)\n    godagplot.plt(fout_png, engine)", "response": "Plots a list of GO ids and the GO DAG."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_goid2goobj(fout_png, goid2goobj, *args, **kws):\n    engine = kws['engine'] if 'engine' in kws else 'pydot'\n    godagsmall = OboToGoDagSmall(goid2goobj=goid2goobj).godag\n    godagplot = GODagSmallPlot(godagsmall, *args, **kws)\n    godagplot.plt(fout_png, engine)", "response": "Create a plot of paths from GO IDs and GO objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a list of GOEA results plot result GOs up to top.", "response": "def plot_results(fout_png, goea_results, *args, **kws):\n    \"\"\"Given a list of GOEA results, plot result GOs up to top.\"\"\"\n    if \"{NS}\" not in fout_png:\n        plt_goea_results(fout_png, goea_results, *args, **kws)\n    else:\n        # Plot separately by NS: BP, MF, CC\n        ns2goea_results = cx.defaultdict(list)\n        for rec in goea_results:\n            ns2goea_results[rec.NS].append(rec)\n        for ns_name, ns_res in ns2goea_results.items():\n            png = fout_png.format(NS=ns_name)\n            plt_goea_results(png, ns_res, *args, **kws)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plt_goea_results(fout_png, goea_results, *args, **kws):\n    engine = kws['engine'] if 'engine' in kws else 'pydot'\n    godagsmall = OboToGoDagSmall(goea_results=goea_results).godag\n    godagplot = GODagSmallPlot(godagsmall, *args, goea_results=goea_results, **kws)\n    godagplot.plt(fout_png, engine)", "response": "Plot a single page of GOEA results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_study_items_max(self):\n        if self.study_items is None:\n            return None\n        if self.study_items is True:\n            return None\n        if isinstance(self.study_items, int):\n            return self.study_items\n        return None", "response": "User can limit the number of genes printed in a GO term."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize GO2 result set.", "response": "def _init_go2res(**kws):\n        \"\"\"Initialize GOEA results.\"\"\"\n        if 'goea_results' in kws:\n            return {res.GO:res for res in kws['goea_results']}\n        if 'go2nt' in kws:\n            return kws['go2nt']"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _init_pval_name(**kws):\n        if 'pval_name' in kws:\n            return kws['pval_name']\n        if 'goea_results' in kws:\n            goea = kws['goea_results']\n            if goea:\n                return \"p_{M}\".format(M=goea[0].method_flds[0].fieldname)", "response": "Initialize pvalue attribute name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _init_goid2color(self):\n        goid2color = {}\n        # 1. colors based on p-value override colors based on source GO\n        if self.go2res is not None:\n            alpha2col = self.pltvars.alpha2col\n            pval_name = self.pval_name\n            for goid, res in self.go2res.items():\n                pval = getattr(res, pval_name, None)\n                if pval is not None:\n                    for alpha, color in alpha2col.items():\n                        if pval <= alpha and res.study_count != 0:\n                            if goid not in goid2color:\n                                goid2color[goid] = color\n        # 2. GO source color\n        color = self.pltvars.key2col['go_sources']\n        for goid in self.godag.go_sources:\n            if goid not in goid2color:\n                goid2color[goid] = color\n        # 3. Level-01 GO color\n        color = self.pltvars.key2col['level_01']\n        for goid, goobj in self.godag.go2obj.items():\n            if goobj.level == 1:\n                if goid not in goid2color:\n                    goid2color[goid] = color\n        return goid2color", "response": "Set colors of GO terms based on source GO IDs and alpha - value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plt(self, fout_img, engine=\"pydot\"):\n        if engine == \"pydot\":\n            self._plt_pydot(fout_img)\n        elif engine == \"pygraphviz\":\n            raise Exception(\"TO BE IMPLEMENTED SOON: ENGINE pygraphvis\")\n        else:\n            raise Exception(\"UNKNOWN ENGINE({E})\".format(E=engine))", "response": "Plot using pydot graphviz or GML."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot using the pydot graphics engine.", "response": "def _plt_pydot(self, fout_img):\n        \"\"\"Plot using the pydot graphics engine.\"\"\"\n        dag = self._get_pydot_graph()\n        img_fmt = os.path.splitext(fout_img)[1][1:]\n        dag.write(fout_img, format=img_fmt)\n        self.log.write(\"  {GO_USR:>3} usr {GO_ALL:>3} GOs  WROTE: {F}\\n\".format(\n            F=fout_img,\n            GO_USR=len(self.godag.go_sources),\n            GO_ALL=len(self.godag.go2obj)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a DAG return a pydot digraph object.", "response": "def _get_pydot_graph(self):\n        \"\"\"Given a DAG, return a pydot digraph object.\"\"\"\n        rel = \"is_a\"\n        pydot = self._get_pydot()\n        # Initialize empty dag\n        dag = pydot.Dot(label=self.title, graph_type='digraph', dpi=\"{}\".format(self.dpi))\n        # Initialize nodes\n        go2node = self._get_go2pydotnode()\n        # Add nodes to graph\n        for node in go2node.values():\n            dag.add_node(node)\n        # Add edges to graph\n        rel2col = self.pltvars.rel2col\n        for src, tgt in self.godag.get_edges():\n            dag.add_edge(pydot.Edge(\n                go2node[tgt], go2node[src],\n                shape=\"normal\",\n                color=rel2col[rel],\n                dir=\"back\")) # invert arrow direction for obo dag convention\n        return dag"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_pydot(self):\n        if self.pydot:\n            return self.pydot\n        self.pydot = __import__(\"pydot\")\n        return self.pydot", "response": "Return pydot package. Load pydot if necessary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string to be printed in a GO term box.", "response": "def _get_node_text(self, goid, goobj):\n        \"\"\"Return a string to be printed in a GO term box.\"\"\"\n        txt = []\n        # Header line: \"GO:0036464 L04 D06\"\n        txt.append(self.pltvars.fmthdr.format(\n            GO=goobj.id.replace(\"GO:\", \"GO\"),\n            level=goobj.level,\n            depth=goobj.depth))\n        # GO name line: \"cytoplamic ribonucleoprotein\"\n        name = goobj.name.replace(\",\", \"\\n\")\n        txt.append(name)\n        # study info line: \"24 genes\"\n        study_txt = self._get_study_txt(goid)\n        if study_txt is not None:\n            txt.append(study_txt)\n        # return text string\n        return \"\\n\".join(txt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_study_txt(self, goid):\n        if self.go2res is not None:\n            res = self.go2res.get(goid, None)\n            if res is not None:\n                if self.study_items is not None:\n                    return self._get_item_str(res)\n                else:\n                    return self.pltvars.fmtres.format(\n                        study_count=res.study_count)", "response": "Get GO text from GOEA study."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_item_str(self, res):\n        npl = self.pltvars.items_p_line  # Number of items Per Line\n        prt_items = sorted([self.__get_genestr(itemid) for itemid in res.study_items])\n        prt_multiline = [prt_items[i:i+npl] for i in range(0, len(prt_items), npl)]\n        num_items = len(prt_items)\n        if self.study_items_max is None:\n            genestr = \"\\n\".join([\", \".join(str(e) for e in sublist) for sublist in prt_multiline])\n            return \"{N}) {GENES}\".format(N=num_items, GENES=genestr)\n        else:\n            if num_items <= self.study_items_max:\n                strs = [\", \".join(str(e) for e in sublist) for sublist in prt_multiline]\n                genestr = \"\\n\".join([\", \".join(str(e) for e in sublist) for sublist in prt_multiline])\n                return genestr\n            else:\n                short_list = prt_items[:self.study_items_max]\n                short_mult = [short_list[i:i+npl] for i in range(0, len(short_list), npl)]\n                short_str = \"\\n\".join([\", \".join(str(e) for e in sublist) for sublist in short_mult])\n                return \"\".join([\"{N} genes; \".format(N=num_items), short_str, \"...\"])", "response": "Return a string representation of the items in any of these formats."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a geneid return the string geneid or a gene symbol.", "response": "def __get_genestr(self, itemid):\n        \"\"\"Given a geneid, return the string geneid or a gene symbol.\"\"\"\n        if self.id2symbol is not None:\n            symbol = self.id2symbol.get(itemid, None)\n            if symbol is not None:\n                return symbol\n        if isinstance(itemid, int):\n            return str(itemid)\n        return itemid"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints list of namedtuples into a table using prtfmt.", "response": "def prt_txt(prt, data_nts, prtfmt=None, nt_fields=None, **kws):\n    \"\"\"Print list of namedtuples into a table using prtfmt.\"\"\"\n    lines = get_lines(data_nts, prtfmt, nt_fields, **kws)\n    if lines:\n        for line in lines:\n            prt.write(line)\n    else:\n        sys.stdout.write(\"      0 items. NOT WRITING\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint list of namedtuples into a table using prtfmt.", "response": "def get_lines(data_nts, prtfmt=None, nt_fields=None, **kws):\n    \"\"\"Print list of namedtuples into a table using prtfmt.\"\"\"\n    lines = []\n    # optional keyword args: prt_if sort_by\n    if prtfmt is None:\n        prtfmt = mk_fmtfld(data_nts[0], kws.get('joinchr', ' '), kws.get('eol', '\\n'))\n    # if nt_fields arg is None, use fields from prtfmt string.\n    if nt_fields is not None:\n        _chk_flds_fmt(nt_fields, prtfmt)\n    if 'sort_by' in kws:\n        data_nts = sorted(data_nts, key=kws['sort_by'])\n    prt_if = kws.get('prt_if', None)\n    for data_nt in data_nts:\n        if prt_if is None or prt_if(data_nt):\n            lines.append(prtfmt.format(**data_nt._asdict()))\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prt_nts(data_nts, prtfmt=None, prt=sys.stdout, nt_fields=None, **kws):\n    prt_txt(prt, data_nts, prtfmt, nt_fields, **kws)", "response": "Print list of namedtuples into a table using prtfmt."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wr_xlsx(fout_xlsx, data_xlsx, **kws):\n    from goatools.wr_tbl_class import WrXlsx\n    # optional keyword args: fld2col_widths hdrs prt_if sort_by fld2fmt prt_flds\n    items_str = kws.get(\"items\", \"items\") if \"items\" not in kws else kws[\"items\"]\n    if data_xlsx:\n        # Open xlsx file\n        xlsxobj = WrXlsx(fout_xlsx, data_xlsx[0]._fields, **kws)\n        worksheet = xlsxobj.add_worksheet()\n        # Write title (optional) and headers.\n        row_idx = xlsxobj.wr_title(worksheet)\n        row_idx = xlsxobj.wr_hdrs(worksheet, row_idx)\n        row_idx_data0 = row_idx\n        # Write data\n        row_idx = xlsxobj.wr_data(data_xlsx, row_idx, worksheet)\n        # Close xlsx file\n        xlsxobj.workbook.close()\n        sys.stdout.write(\"  {N:>5} {ITEMS} WROTE: {FOUT}\\n\".format(\n            N=row_idx-row_idx_data0, ITEMS=items_str, FOUT=fout_xlsx))\n    else:\n        sys.stdout.write(\"      0 {ITEMS}. NOT WRITING {FOUT}\\n\".format(\n            ITEMS=items_str, FOUT=fout_xlsx))", "response": "Write a spreadsheet into a xlsx file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites xlsx file containing section names followed by lines of namedtuple data.", "response": "def wr_xlsx_sections(fout_xlsx, xlsx_data, **kws):\n    \"\"\"Write xlsx file containing section names followed by lines of namedtuple data.\"\"\"\n    from goatools.wr_tbl_class import WrXlsx\n    items_str = \"items\" if \"items\" not in kws else kws[\"items\"]\n    prt_hdr_min = 10\n    num_items = 0\n    if xlsx_data:\n        # Basic data checks\n        assert len(xlsx_data[0]) == 2, \"wr_xlsx_sections EXPECTED: [(section, nts), ...\"\n        assert xlsx_data[0][1], \\\n            \"wr_xlsx_sections EXPECTED SECTION({S}) LIST TO HAVE DATA\".format(S=xlsx_data[0][0])\n        # Open xlsx file and write title (optional) and headers.\n        xlsxobj = WrXlsx(fout_xlsx, xlsx_data[0][1][0]._fields, **kws)\n        worksheet = xlsxobj.add_worksheet()\n        row_idx = xlsxobj.wr_title(worksheet)\n        hdrs_wrote = False\n        # Write data\n        for section_text, data_nts in xlsx_data:\n            num_items += len(data_nts)\n            fmt = xlsxobj.wbfmtobj.get_fmt_section()\n            row_idx = xlsxobj.wr_row_mergeall(worksheet, section_text, fmt, row_idx)\n            if hdrs_wrote is False or len(data_nts) > prt_hdr_min:\n                row_idx = xlsxobj.wr_hdrs(worksheet, row_idx)\n                hdrs_wrote = True\n            row_idx = xlsxobj.wr_data(data_nts, row_idx, worksheet)\n        # Close xlsx file\n        xlsxobj.workbook.close()\n        sys.stdout.write(\"  {N:>5} {ITEMS} WROTE: {FOUT} ({S} sections)\\n\".format(\n            N=num_items, ITEMS=items_str, FOUT=fout_xlsx, S=len(xlsx_data)))\n    else:\n        sys.stdout.write(\"      0 {ITEMS}. NOT WRITING {FOUT}\\n\".format(\n            ITEMS=items_str, FOUT=fout_xlsx))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a file of tab - separated table data", "response": "def wr_tsv(fout_tsv, tsv_data, **kws):\n    \"\"\"Write a file of tab-separated table data\"\"\"\n    items_str = \"items\" if \"items\" not in kws else kws[\"items\"]\n    if tsv_data:\n        ifstrm = sys.stdout if fout_tsv is None else open(fout_tsv, 'w')\n        num_items = prt_tsv(ifstrm, tsv_data, **kws)\n        if fout_tsv is not None:\n            sys.stdout.write(\"  {N:>5} {ITEMS} WROTE: {FOUT}\\n\".format(\n                N=num_items, ITEMS=items_str, FOUT=fout_tsv))\n            ifstrm.close()\n    else:\n        sys.stdout.write(\"      0 {ITEMS}. NOT WRITING {FOUT}\\n\".format(\n            ITEMS=items_str, FOUT=fout_tsv))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prt_tsv_sections(prt, tsv_data, **kws):\n    prt_hdr_min = 10  # Print hdr on the 1st section and for any following 'large' sections\n    num_items = 0\n    if tsv_data:\n        # Basic data checks\n        assert len(tsv_data[0]) == 2, \"wr_tsv_sections EXPECTED: [(section, nts), ...\"\n        assert tsv_data[0][1], \\\n            \"wr_tsv_sections EXPECTED SECTION({S}) LIST TO HAVE DATA\".format(S=tsv_data[0][0])\n        hdrs_wrote = False\n        sep = \"\\t\" if 'sep' not in kws else kws['sep']\n        prt_flds = kws['prt_flds'] if 'prt_flds' in kws else tsv_data[0]._fields\n        fill = sep*(len(prt_flds) - 1)\n        # Write data\n        for section_text, data_nts in tsv_data:\n            prt.write(\"{SEC}{FILL}\\n\".format(SEC=section_text, FILL=fill))\n            if hdrs_wrote is False or len(data_nts) > prt_hdr_min:\n                prt_tsv_hdr(prt, data_nts, **kws)\n                hdrs_wrote = True\n            num_items += prt_tsv_dat(prt, data_nts, **kws)\n        return num_items\n    else:\n        return 0", "response": "Write tsv file containing section names followed by lines of namedtuple data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint tab - separated table headers and data.", "response": "def prt_tsv(prt, data_nts, **kws):\n    \"\"\"Print tab-separated table headers and data\"\"\"\n    # User-controlled printing options\n    prt_tsv_hdr(prt, data_nts, **kws)\n    return prt_tsv_dat(prt, data_nts, **kws)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting tab - separated table headers.", "response": "def prt_tsv_hdr(prt, data_nts, **kws):\n    \"\"\"Print tab-separated table headers\"\"\"\n    sep = \"\\t\" if 'sep' not in kws else kws['sep']\n    flds_all = data_nts[0]._fields\n    hdrs = get_hdrs(flds_all, **kws)\n    prt.write(\"# {}\\n\".format(sep.join(hdrs)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prt_tsv_dat(prt, data_nts, **kws):\n    sep = \"\\t\" if 'sep' not in kws else kws['sep']\n    fld2fmt = None if 'fld2fmt' not in kws else kws['fld2fmt']\n    if 'sort_by' in kws:\n        data_nts = sorted(data_nts, key=kws['sort_by'])\n    prt_if = kws['prt_if'] if 'prt_if' in kws else None\n    prt_flds = kws['prt_flds'] if 'prt_flds' in kws else data_nts[0]._fields\n    items = 0\n    for nt_data_row in data_nts:\n        if prt_if is None or prt_if(nt_data_row):\n            if fld2fmt is not None:\n                row_fld_vals = [(fld, getattr(nt_data_row, fld)) for fld in prt_flds]\n                row_vals = _fmt_fields(row_fld_vals, fld2fmt)\n            else:\n                row_vals = [getattr(nt_data_row, fld) for fld in prt_flds]\n            prt.write(\"{}\\n\".format(sep.join(str(d) for d in row_vals)))\n            items += 1\n    return items", "response": "Print tab - separated table data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fmt_fields(fld_vals, fld2fmt):\n    vals = []\n    for fld, val in fld_vals:\n        if fld in fld2fmt:\n            val = fld2fmt[fld].format(val)\n        vals.append(val)\n    return vals", "response": "Format specific fields in a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that all fields in the namedtuple have corresponding data in the namedtuple.", "response": "def _chk_flds_fmt(nt_fields, prtfmt):\n    \"\"\"Check that all fields in the prtfmt have corresponding data in the namedtuple.\"\"\"\n    fmtflds = get_fmtflds(prtfmt)\n    missing_data = set(fmtflds).difference(set(nt_fields))\n    # All data needed for print is present, return.\n    if not missing_data:\n        return\n    #raise Exception('MISSING DATA({M}).'.format(M=\" \".join(missing_data)))\n    msg = ['CANNOT PRINT USING: \"{PF}\"'.format(PF=prtfmt.rstrip())]\n    for fld in fmtflds:\n        errmrk = \"\" if fld in nt_fields else \"ERROR-->\"\n        msg.append(\"  {ERR:8} {FLD}\".format(ERR=errmrk, FLD=fld))\n    raise Exception('\\n'.join(msg))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prt_txt_hdr(prt, prtfmt):\n    tblhdrs = get_fmtfldsdict(prtfmt)\n    # If needed, reformat for format_string for header, which has strings, not floats.\n    hdrfmt = re.sub(r':(\\d+)\\.\\S+}', r':\\1}', prtfmt)\n    hdrfmt = re.sub(r':(0+)(\\d+)}', r':\\2}', hdrfmt)\n    prt.write(\"#{}\".format(hdrfmt.format(**tblhdrs)))", "response": "Print header for text report."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a namedtuple return a format_field string.", "response": "def mk_fmtfld(nt_item, joinchr=\" \", eol=\"\\n\"):\n    \"\"\"Given a namedtuple, return a format_field string.\"\"\"\n    fldstrs = []\n    # Default formats based on fieldname\n    fld2fmt = {\n        'hdrgo' : lambda f: \"{{{FLD}:1,}}\".format(FLD=f),\n        'dcnt' : lambda f: \"{{{FLD}:6,}}\".format(FLD=f),\n        'level' : lambda f: \"L{{{FLD}:02,}}\".format(FLD=f),\n        'depth' : lambda f: \"D{{{FLD}:02,}}\".format(FLD=f),\n    }\n    for fld in nt_item._fields:\n        if fld in fld2fmt:\n            val = fld2fmt[fld](fld)\n        else:\n            val = \"{{{FLD}}}\".format(FLD=fld)\n        fldstrs.append(val)\n    return \"{LINE}{EOL}\".format(LINE=joinchr.join(fldstrs), EOL=eol)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, fout_xlsx=None, fout_txt=None, verbose=False):\n        sortby = self._get_fncsortnt(self.objgrpd.grprobj.gosubdag.prt_attr['flds'])\n        kws_sort = {'sortby' if verbose else 'section_sortby': sortby}\n        sortobj = Sorter(self.objgrpd.grprobj, **kws_sort)\n        # KWS: hdrgo_prt=True section_prt=None top_n=None use_sections=True\n        # RET: {sortobj, sections, hdrgo_prt} or {sortobj flat hdrgo_prt}\n        desc2nts = sortobj.get_desc2nts_fnc(\n            hdrgo_prt=verbose,\n            section_prt=True,\n            top_n=None,\n            use_sections=True)\n        # print('FFFF', desc2nts['flds'])\n        # Write user GO IDs in sections\n        objgowr = WrXlsxSortedGos(\"init\", sortobj, self.objgrpd.ver_list)\n        if fout_xlsx is not None:\n            kws_xlsx = {'shade_hdrgos':verbose}\n            if not verbose:\n                kws_xlsx['prt_flds'] = [f for f in desc2nts['flds'] if f not in self.excl_flds]\n            objgowr.wr_xlsx_nts(fout_xlsx, desc2nts, **kws_xlsx)\n            fout_desc = '{BASE}_desc.txt'.format(BASE=os.path.splitext(fout_xlsx)[0])\n            self._wr_ver_n_key(fout_desc, verbose)\n        if fout_txt is not None:\n            self._wr_txt_nts(fout_txt, desc2nts, objgowr, verbose)\n        if fout_xlsx is None and fout_txt is None:\n            self._prt_ver_n_key(sys.stdout, verbose)\n            prtfmt = self._get_prtfmt(objgowr, verbose)\n            summary_dct = objgowr.prt_txt_desc2nts(sys.stdout, desc2nts, prtfmt)\n            self._prt_ver_n_key(sys.stdout, verbose)\n            if summary_dct:\n                print(\"\\n{N} GO IDs in {S} sections\".format(\n                    N=desc2nts['num_items'], S=desc2nts['num_sections']))", "response": "Write the GO IDs in a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget print format containing markers.", "response": "def _get_prtfmt(self, objgowr, verbose):\n        \"\"\"Get print format containing markers.\"\"\"\n        prtfmt = objgowr.get_prtfmt('fmt')\n        prtfmt = prtfmt.replace('# ', '')\n        # print('PPPPPPPPPPP', prtfmt)\n        if not verbose:\n            prtfmt = prtfmt.replace('{hdr1usr01:2}', '')\n            prtfmt = prtfmt.replace('{childcnt:3} L{level:02} ', '')\n            prtfmt = prtfmt.replace('{num_usrgos:>4} uGOs ', '')\n            prtfmt = prtfmt.replace('{D1:5} {REL} {rel}', '')\n            prtfmt = prtfmt.replace('R{reldepth:02} ', '')\n        # print('PPPPPPPPPPP', prtfmt)\n        marks = ''.join(['{{{}}}'.format(nt.hdr) for nt in self.go_ntsets])\n        return '{MARKS} {PRTFMT}'.format(MARKS=marks, PRTFMT=prtfmt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a sort function for sorting header GO IDs found in sections.", "response": "def _get_fncsortnt(flds):\n        \"\"\"Return a sort function for sorting header GO IDs found in sections.\"\"\"\n        if 'tinfo' in flds:\n            return lambda ntgo: [ntgo.NS, -1*ntgo.tinfo, ntgo.depth, ntgo.alt]\n        if 'dcnt' in flds:\n            return lambda ntgo: [ntgo.NS, -1*ntgo.dcnt, ntgo.depth, ntgo.alt]\n        return lambda ntgo: [ntgo.NS, -1*ntgo.depth, ntgo.alt]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite grouped and sorted GO IDs to a file.", "response": "def _wr_txt_nts(self, fout_txt, desc2nts, objgowr, verbose):\n        \"\"\"Write grouped and sorted GO IDs to GOs.\"\"\"\n        with open(fout_txt, 'w') as prt:\n            self._prt_ver_n_key(prt, verbose)\n            prt.write('\\n\\n')\n            prt.write('# ----------------------------------------------------------------\\n')\n            prt.write('# - Sections and GO IDs\\n')\n            prt.write('# ----------------------------------------------------------------\\n')\n            prtfmt = self._get_prtfmt(objgowr, verbose)\n            summary_dct = objgowr.prt_txt_desc2nts(prt, desc2nts, prtfmt)\n            if summary_dct:\n                print(\"  {N:>5} GO IDs WROTE: {FOUT} ({S} sections)\".format(\n                    N=desc2nts['num_items'], FOUT=fout_txt, S=desc2nts['num_sections']))\n            else:\n                print(\"  WROTE: {TXT}\".format(TXT=fout_txt))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites GO DAG version and key indicating presence of GO ID in a list.", "response": "def _wr_ver_n_key(self, fout_txt, verbose):\n        \"\"\"Write GO DAG version and key indicating presence of GO ID in a list.\"\"\"\n        with open(fout_txt, 'w') as prt:\n            self._prt_ver_n_key(prt, verbose)\n            print('               WROTE: {TXT}'.format(TXT=fout_txt))"}
